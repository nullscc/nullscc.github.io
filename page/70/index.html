
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/70/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.SD_2023_08_12" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/12/cs.SD_2023_08_12/" class="article-date">
  <time datetime="2023-08-12T15:00:00.000Z" itemprop="datePublished">2023-08-12</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/12/cs.SD_2023_08_12/">cs.SD - 2023-08-12</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Alternative-Pseudo-Labeling-for-Semi-Supervised-Automatic-Speech-Recognition"><a href="#Alternative-Pseudo-Labeling-for-Semi-Supervised-Automatic-Speech-Recognition" class="headerlink" title="Alternative Pseudo-Labeling for Semi-Supervised Automatic Speech Recognition"></a>Alternative Pseudo-Labeling for Semi-Supervised Automatic Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06547">http://arxiv.org/abs/2308.06547</a></li>
<li>repo_url: None</li>
<li>paper_authors: Han Zhu, Dongji Gao, Gaofeng Cheng, Daniel Povey, Pengyuan Zhang, Yonghong Yan</li>
<li>for: 提高自动语音识别器的性能在半监督学习中，当 Label 缺乏时</li>
<li>methods: 提议一种新的替代 pseudo-labeling 框架，包括一种通用的 CTC 损失函数、一种 confidence-based 错误检测方法和一种自动调整 threshold 方法</li>
<li>results: 对比 traditional CTC 损失函数和 confidence-based 错误检测方法，提议的替代 pseudo-labeling 框架可以更好地处理含有错误 tokens 的 pseudo-Label，并且不需要手动调整 threshold<details>
<summary>Abstract</summary>
When labeled data is insufficient, semi-supervised learning with the pseudo-labeling technique can significantly improve the performance of automatic speech recognition. However, pseudo-labels are often noisy, containing numerous incorrect tokens. Taking noisy labels as ground-truth in the loss function results in suboptimal performance. Previous works attempted to mitigate this issue by either filtering out the nosiest pseudo-labels or improving the overall quality of pseudo-labels. While these methods are effective to some extent, it is unrealistic to entirely eliminate incorrect tokens in pseudo-labels. In this work, we propose a novel framework named alternative pseudo-labeling to tackle the issue of noisy pseudo-labels from the perspective of the training objective. The framework comprises several components. Firstly, a generalized CTC loss function is introduced to handle noisy pseudo-labels by accepting alternative tokens in the positions of incorrect tokens. Applying this loss function in pseudo-labeling requires detecting incorrect tokens in the predicted pseudo-labels. In this work, we adopt a confidence-based error detection method that identifies the incorrect tokens by comparing their confidence scores with a given threshold, thus necessitating the confidence score to be discriminative. Hence, the second proposed technique is the contrastive CTC loss function that widens the confidence gap between the correctly and incorrectly predicted tokens, thereby improving the error detection ability. Additionally, obtaining satisfactory performance with confidence-based error detection typically requires extensive threshold tuning. Instead, we propose an automatic thresholding method that uses labeled data as a proxy for determining the threshold, thus saving the pain of manual tuning.
</details>
<details>
<summary>摘要</summary>
当标注数据短缺时，半超vised学习采用pseudo-标签技术可以显著提高自动语音识别的性能。然而，pseudo-标签经常含有许多错误的token。将含有错误token的pseudo-标签作为真实标签在损失函数中使用会导致优化性能下降。前一些工作尝试了通过过滤 pseudo-标签中最含糟糕的token或提高总体pseudo-标签质量来缓解这个问题。虽然这些方法有一定的效果，但是完全消除pseudo-标签中的错误token是不现实的。在这种情况下，我们提出了一种新的框架名为代理 pseudo-标签。该框架包括以下几个组成部分。首先，我们引入一种通用的CTC损失函数，可以处理含有错误token的pseudo-标签。在使用这种损失函数进行pseudo-标签时，需要检测pseudo-标签中的错误token。在这种情况下，我们采用一种 confidence-based 错误检测方法，通过比较错误token的信任分数与一个给定的阈值，以确定错误token的存在。因此，第二个提出的技术是增强CTC损失函数，以增强错误检测的能力。此外，通过 confidence-based 错误检测获得良好性能通常需要进行广泛的阈值调整。而我们提出的自动阈值调整方法，通过使用标注数据作为代理，自动地调整阈值，从而避免了手动调整的痛苦。
</details></li>
</ul>
<hr>
<h2 id="BigWavGAN-A-Wave-To-Wave-Generative-Adversarial-Network-for-Music-Super-Resolution"><a href="#BigWavGAN-A-Wave-To-Wave-Generative-Adversarial-Network-for-Music-Super-Resolution" class="headerlink" title="BigWavGAN: A Wave-To-Wave Generative Adversarial Network for Music Super-Resolution"></a>BigWavGAN: A Wave-To-Wave Generative Adversarial Network for Music Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06483">http://arxiv.org/abs/2308.06483</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yenan Zhang, Hiroshi Watanabe</li>
<li>for: 这个论文目的是提高音乐超解析（SR）领域中深度神经网络（DNN）的性能。</li>
<li>methods: 这个论文使用了大型DNN模型，并结合了State-Of-The-Art（SOTA）的激励函数和对抗训练策略。它的权衡器包括多尺度权衡器（MSD）和多分辨率权衡器（MRD）。</li>
<li>results: 对于音乐SR问题，BigWavGAN模型表现出色，超过了基eline模型和State-Of-The-Art（SOTA）音乐SR模型。它还能够处理异常数据，并且有较好的总体化能力。<details>
<summary>Abstract</summary>
Generally, Deep Neural Networks (DNNs) are expected to have high performance when their model size is large. However, large models failed to produce high-quality results commensurate with their scale in music Super-Resolution (SR). We attribute this to that DNNs cannot learn information commensurate with their size from standard mean square error losses. To unleash the potential of large DNN models in music SR, we propose BigWavGAN, which incorporates Demucs, a large-scale wave-to-wave model, with State-Of-The-Art (SOTA) discriminators and adversarial training strategies. Our discriminator consists of Multi-Scale Discriminator (MSD) and Multi-Resolution Discriminator (MRD). During inference, since only the generator is utilized, there are no additional parameters or computational resources required compared to the baseline model Demucs. Objective evaluation affirms the effectiveness of BigWavGAN in music SR. Subjective evaluations indicate that BigWavGAN can generate music with significantly high perceptual quality over the baseline model. Notably, BigWavGAN surpasses the SOTA music SR model in both simulated and real-world scenarios. Moreover, BigWavGAN represents its superior generalization ability to address out-of-distribution data. The conducted ablation study reveals the importance of our discriminators and training strategies. Samples are available on the demo page.
</details>
<details>
<summary>摘要</summary>
通常情况下，深度神经网络（DNNs）预期会在模型大小增加时表现出色。然而，大型模型在音乐超分解（SR）中并没有达到预期的高质量效果。我们认为这是因为DNNs无法从标准方差误差损失中学习足够的信息。为了解放大型DNN模型在音乐SR中的潜力，我们提出了BigWavGAN，它将大规模涉及的wave-to-wave模型Demucs融合到了领先的推误器和对抗训练策略中。我们的推误器包括多尺度推误器（MSD）和多分辨率推误器（MRD）。在推理过程中，由于只有生成器被使用，因此没有额外的参数或计算资源的需求，与基线模型Demucs相比。对象评估表明BigWavGAN在音乐SR中的效果非常高。主观评估表明BigWavGAN可以生成具有显著高媒体质量的音乐，比基线模型高。此外，BigWavGAN在实际和 simulate 的情况下都能够超越领先的音乐SR模型。此外，BigWavGAN在处理异常数据的能力方面表现出了superior的普适性。进行的ablation研究表明我们的推误器和训练策略的重要性。样例可以在 demo 页面中找到。
</details></li>
</ul>
<hr>
<h2 id="Bilingual-Streaming-ASR-with-Grapheme-units-and-Auxiliary-Monolingual-Loss"><a href="#Bilingual-Streaming-ASR-with-Grapheme-units-and-Auxiliary-Monolingual-Loss" class="headerlink" title="Bilingual Streaming ASR with Grapheme units and Auxiliary Monolingual Loss"></a>Bilingual Streaming ASR with Grapheme units and Auxiliary Monolingual Loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06327">http://arxiv.org/abs/2308.06327</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Soleymanpour, Mahmoud Al Ismail, Fahimeh Bahmaninezhad, Kshitiz Kumar, Jian Wu</li>
<li>for: 支持英语为次要地区的半自动语音识别（ASR）设置</li>
<li>methods: 使用全双语对照模型、双流Transformer模型、并行编码结构和语言标识（LID）损失</li>
<li>results: 提高英语混合码能力，对代码混合ES和IT应用进行大规模训练和测试，并显示出优于LID损失的特点<details>
<summary>Abstract</summary>
We introduce a bilingual solution to support English as secondary locale for most primary locales in hybrid automatic speech recognition (ASR) settings. Our key developments constitute: (a) pronunciation lexicon with grapheme units instead of phone units, (b) a fully bilingual alignment model and subsequently bilingual streaming transformer model, (c) a parallel encoder structure with language identification (LID) loss, (d) parallel encoder with an auxiliary loss for monolingual projections. We conclude that in comparison to LID loss, our proposed auxiliary loss is superior in specializing the parallel encoders to respective monolingual locales, and that contributes to stronger bilingual learning. We evaluate our work on large-scale training and test tasks for bilingual Spanish (ES) and bilingual Italian (IT) applications. Our bilingual models demonstrate strong English code-mixing capability. In particular, the bilingual IT model improves the word error rate (WER) for a code-mix IT task from 46.5% to 13.8%, while also achieving a close parity (9.6%) with the monolingual IT model (9.5%) over IT tests.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种双语解决方案，以支持英语为次要地区的多地点自动语音识别（ASR）设置。我们的关键发展包括：(a) 使用字节单位 вместоPhone单位的发音词典。(b)一个完全双语对应模型和随后的双语流transformer模型。(c)一个并行编码结构，并且添加语言标识（LID）损失。(d)并行编码器，并且添加一个辅助损失来特化到各自的单语言本地。我们结合了这些发展，并进行了大规模的训练和测试任务，以评估我们的方法在双语西班牙（ES）和双语意大利（IT）应用中的性能。我们的双语模型在英语混合码中表现出色，特别是双语IT模型在一个混合IT任务中，从46.5%降低到13.8%，同时也与单语意大利模型（9.5%）在意大利测试上凑平。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/12/cs.SD_2023_08_12/" data-id="clpxp044a00xzfm88axoa1osz" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_08_12" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/12/cs.CV_2023_08_12/" class="article-date">
  <time datetime="2023-08-12T13:00:00.000Z" itemprop="datePublished">2023-08-12</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/12/cs.CV_2023_08_12/">cs.CV - 2023-08-12</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Cyclic-Test-Time-Adaptation-on-Monocular-Video-for-3D-Human-Mesh-Reconstruction"><a href="#Cyclic-Test-Time-Adaptation-on-Monocular-Video-for-3D-Human-Mesh-Reconstruction" class="headerlink" title="Cyclic Test-Time Adaptation on Monocular Video for 3D Human Mesh Reconstruction"></a>Cyclic Test-Time Adaptation on Monocular Video for 3D Human Mesh Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06554">http://arxiv.org/abs/2308.06554</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hygenie1228/cycleadapt_release">https://github.com/hygenie1228/cycleadapt_release</a></li>
<li>paper_authors: Hyeongjin Nam, Daniel Sungho Jung, Yeonguk Oh, Kyoung Mu Lee</li>
<li>for:  addresses the domain gap problem in 3D human mesh reconstruction by proposing a cyclic adaptation method that leverages both 2D and 3D evidence.</li>
<li>methods:  the proposed method consists of two networks: a human mesh reconstruction network (HMRNet) and a human motion denoising network (MDNet), which are cyclically adapted given a test video. The 3D supervision targets generated by MDNet are used to fully supervise HMRNet, reducing the reliance on 2D evidence.</li>
<li>results:  the proposed method achieves state-of-the-art performance compared to previous test-time adaptation methods, demonstrating the effectiveness of the cyclic adaptation scheme in addressing the domain gap problem.<details>
<summary>Abstract</summary>
Despite recent advances in 3D human mesh reconstruction, domain gap between training and test data is still a major challenge. Several prior works tackle the domain gap problem via test-time adaptation that fine-tunes a network relying on 2D evidence (e.g., 2D human keypoints) from test images. However, the high reliance on 2D evidence during adaptation causes two major issues. First, 2D evidence induces depth ambiguity, preventing the learning of accurate 3D human geometry. Second, 2D evidence is noisy or partially non-existent during test time, and such imperfect 2D evidence leads to erroneous adaptation. To overcome the above issues, we introduce CycleAdapt, which cyclically adapts two networks: a human mesh reconstruction network (HMRNet) and a human motion denoising network (MDNet), given a test video. In our framework, to alleviate high reliance on 2D evidence, we fully supervise HMRNet with generated 3D supervision targets by MDNet. Our cyclic adaptation scheme progressively elaborates the 3D supervision targets, which compensate for imperfect 2D evidence. As a result, our CycleAdapt achieves state-of-the-art performance compared to previous test-time adaptation methods. The codes are available at https://github.com/hygenie1228/CycleAdapt_RELEASE.
</details>
<details>
<summary>摘要</summary>
尽管最近的3D人体渲染技术得到了进步，但域外差问题仍然是主要挑战。一些先前的工作通过测试时适应来解决域外差问题，但高度依赖于2D证据（例如2D人体关键点）的适应会导致两个主要问题。首先，2D证据引入深度不确定性，阻碍学习准确的3D人体几何学。其次，2D证据在测试时可能受到噪声或部分损失，这会导致错误的适应。为解决以上问题，我们介绍了CyclesAdapt，它将两个网络——人体渲染网络（HMRNet）和人体动作净化网络（MDNet）——在测试视频基础上进行循环适应。在我们的框架中，为了减少依赖于2D证据，我们完全supervise HMRNet 的生成3D目标，使其能够学习准确的3D人体几何学。我们的循环适应方案逐渐填充3D目标，以补做受到噪声或部分损失的2D证据。因此，我们的CyclesAdapt可以与之前的测试时适应方法相比，实现最新的表现。代码可以在https://github.com/hygenie1228/CycleAdapt_RELEASE 中找到。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-Vision-Transformer-from-the-View-of-Path-Ensemble"><a href="#Revisiting-Vision-Transformer-from-the-View-of-Path-Ensemble" class="headerlink" title="Revisiting Vision Transformer from the View of Path Ensemble"></a>Revisiting Vision Transformer from the View of Path Ensemble</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06548">http://arxiv.org/abs/2308.06548</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuning Chang, Pichao Wang, Hao Luo, Fan Wang, Mike Zheng Shou</li>
<li>for: 本文提出了一种新的视点，认为 transformer 层可以被看作是多个并行的路径 ensemble network。</li>
<li>methods: 将传统的多头自注意力（MSA）和Feed Forward Network（FFN）替换为三个并行的路径，并使用 identify connection 将这些路径转换为明确的多路ensemble network。</li>
<li>results: 通过调查每个路径对最终预测的影响，发现一些路径甚至会降低性能。因此，提出了路径裁剪和 EnsembleScale 技术来优化路径组合，以便允许短路专注提供高质量表示。此外，通过自馈沟通来增强 paths 服务后续路径的表示。<details>
<summary>Abstract</summary>
Vision Transformers (ViTs) are normally regarded as a stack of transformer layers. In this work, we propose a novel view of ViTs showing that they can be seen as ensemble networks containing multiple parallel paths with different lengths. Specifically, we equivalently transform the traditional cascade of multi-head self-attention (MSA) and feed-forward network (FFN) into three parallel paths in each transformer layer. Then, we utilize the identity connection in our new transformer form and further transform the ViT into an explicit multi-path ensemble network. From the new perspective, these paths perform two functions: the first is to provide the feature for the classifier directly, and the second is to provide the lower-level feature representation for subsequent longer paths. We investigate the influence of each path for the final prediction and discover that some paths even pull down the performance. Therefore, we propose the path pruning and EnsembleScale skills for improvement, which cut out the underperforming paths and re-weight the ensemble components, respectively, to optimize the path combination and make the short paths focus on providing high-quality representation for subsequent paths. We also demonstrate that our path combination strategies can help ViTs go deeper and act as high-pass filters to filter out partial low-frequency signals. To further enhance the representation of paths served for subsequent paths, self-distillation is applied to transfer knowledge from the long paths to the short paths. This work calls for more future research to explain and design ViTs from new perspectives.
</details>
<details>
<summary>摘要</summary>
视transformer（ViT）通常被看作是一 stack of transformer层。在这项工作中，我们提出了一种新的视图，显示了ViT可以被看作是一个多路网络，每个层包含多个平行的路径。 Specifically, we可以将传统的多头自注意（MSA）和Feed-Forward Network（FFN）转化为每个transformer层中的三个平行路径。然后，我们利用我们新的transformer形式中的标识连接，并将ViT转化为一个显式多路ensemble网络。从这种新的视角来看，这些路径在两个功能：第一是提供分类器所需的特征，第二是提供后续更长的路径所需的下一个特征表示。我们调查每个路径对最终预测的影响，发现一些路径甚至会降低性能。因此，我们提出了路径剔除和EnsembleScale技巧来优化路径组合，即将不良表现的路径剔除，并重新权重ensemble组件。我们还证明了我们的路径组合策略可以帮助ViT深入探索，并作为高通过滤器来过滤部分低频信号。为了进一步增强路径服务后续路径的表示，我们应用了自适应知识传递，将长路径中的知识传递给短路径。这项工作呼吁了更多的未来研究，以解释和设计ViT从新的视角。
</details></li>
</ul>
<hr>
<h2 id="SegPrompt-Boosting-Open-world-Segmentation-via-Category-level-Prompt-Learning"><a href="#SegPrompt-Boosting-Open-world-Segmentation-via-Category-level-Prompt-Learning" class="headerlink" title="SegPrompt: Boosting Open-world Segmentation via Category-level Prompt Learning"></a>SegPrompt: Boosting Open-world Segmentation via Category-level Prompt Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06531">http://arxiv.org/abs/2308.06531</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aim-uofa/segprompt">https://github.com/aim-uofa/segprompt</a></li>
<li>paper_authors: Muzhi Zhu, Hengtao Li, Hao Chen, Chengxiang Fan, Weian Mao, Chenchen Jing, Yifan Liu, Chunhua Shen</li>
<li>for: 提高closed-set实例分割模型对未知类别的检测能力</li>
<li>methods: 使用类别信息进行训练 Mechanism，提高模型对已知和未知类别的检测能力</li>
<li>results: 在新的开放世界数据集上，SegPrompt可以提高总和未知检测性能by 5.6%和6.1%，而无需影响推理效率。在 existed cross-dataset transfer和强烈监督设置下，我们的方法也得到了5.5%和12.3%的相对改进。<details>
<summary>Abstract</summary>
Current closed-set instance segmentation models rely on pre-defined class labels for each mask during training and evaluation, largely limiting their ability to detect novel objects. Open-world instance segmentation (OWIS) models address this challenge by detecting unknown objects in a class-agnostic manner. However, previous OWIS approaches completely erase category information during training to keep the model's ability to generalize to unknown objects. In this work, we propose a novel training mechanism termed SegPrompt that uses category information to improve the model's class-agnostic segmentation ability for both known and unknown categories. In addition, the previous OWIS training setting exposes the unknown classes to the training set and brings information leakage, which is unreasonable in the real world. Therefore, we provide a new open-world benchmark closer to a real-world scenario by dividing the dataset classes into known-seen-unseen parts. For the first time, we focus on the model's ability to discover objects that never appear in the training set images.   Experiments show that SegPrompt can improve the overall and unseen detection performance by 5.6% and 6.1% in AR on our new benchmark without affecting the inference efficiency. We further demonstrate the effectiveness of our method on existing cross-dataset transfer and strongly supervised settings, leading to 5.5% and 12.3% relative improvement.
</details>
<details>
<summary>摘要</summary>
当前的闭erset实例分割模型依赖于在训练和评估中预先定义的类标签，这限制了它们的能力检测新的对象。开放世界实例分割（OWIS）模型解决了这个挑战，它在无类别情况下检测未知对象。然而，前一些OWIS方法完全抹除了类型信息在训练中，以保持模型对未知类型的泛化能力。在这种情况下，我们提出了一种新的训练机制，称为SegPrompt，它使用类型信息来提高模型在已知和未知类型之间的无类别分割能力。此外，前一些OWIS训练设置会泄露信息，这不符合实际世界的情况。因此，我们提供了一个更加真实的开放世界 benchmark，将数据集分为已知、未seen和未知三部分。我们首次关注模型能够在训练集图像中不出现的对象检测能力。实验结果显示，SegPrompt可以在AR上提高总和未seen检测性能5.6%和6.1%，而不影响推理效率。我们还证明我们的方法在现有的跨数据集转移和强烈监督设置下有5.5%和12.3%的相对改进。
</details></li>
</ul>
<hr>
<h2 id="BEV-DG-Cross-Modal-Learning-under-Bird’s-Eye-View-for-Domain-Generalization-of-3D-Semantic-Segmentation"><a href="#BEV-DG-Cross-Modal-Learning-under-Bird’s-Eye-View-for-Domain-Generalization-of-3D-Semantic-Segmentation" class="headerlink" title="BEV-DG: Cross-Modal Learning under Bird’s-Eye View for Domain Generalization of 3D Semantic Segmentation"></a>BEV-DG: Cross-Modal Learning under Bird’s-Eye View for Domain Generalization of 3D Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06530">http://arxiv.org/abs/2308.06530</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miaoyu Li, Yachao Zhang, Xu MA, Yanyun Qu, Yun Fu</li>
<li>for: 这篇论文旨在提高频率域执行3D semantic segmentation的预测性和灵活性，并且在新的频率域中进行预测，而不需要训练数据集。</li>
<li>methods: 这篇论文提出了一种基于鸟瞰看的cross-modal learning架构，具有更高的错误耐受性和稳定性，并且可以实现频率域内的预测。</li>
<li>results: 这篇论文透过三个不同的3D数据集进行评估，结果显示BEV-DG在所有设定中具有显著的性能优势，与现有的竞争者相比，BEV-DG的性能优势为10%左右。<details>
<summary>Abstract</summary>
Cross-modal Unsupervised Domain Adaptation (UDA) aims to exploit the complementarity of 2D-3D data to overcome the lack of annotation in a new domain. However, UDA methods rely on access to the target domain during training, meaning the trained model only works in a specific target domain. In light of this, we propose cross-modal learning under bird's-eye view for Domain Generalization (DG) of 3D semantic segmentation, called BEV-DG. DG is more challenging because the model cannot access the target domain during training, meaning it needs to rely on cross-modal learning to alleviate the domain gap. Since 3D semantic segmentation requires the classification of each point, existing cross-modal learning is directly conducted point-to-point, which is sensitive to the misalignment in projections between pixels and points. To this end, our approach aims to optimize domain-irrelevant representation modeling with the aid of cross-modal learning under bird's-eye view. We propose BEV-based Area-to-area Fusion (BAF) to conduct cross-modal learning under bird's-eye view, which has a higher fault tolerance for point-level misalignment. Furthermore, to model domain-irrelevant representations, we propose BEV-driven Domain Contrastive Learning (BDCL) with the help of cross-modal learning under bird's-eye view. We design three domain generalization settings based on three 3D datasets, and BEV-DG significantly outperforms state-of-the-art competitors with tremendous margins in all settings.
</details>
<details>
<summary>摘要</summary>
cross-modal无监督领域适应（UDA）目标是利用2D-3D数据的补充性来缺乏目标领域的标注。然而，UDA方法需要训练时有Target领域的存在，因此训练的模型只能在特定的Target领域中工作。为了解决这个问题，我们提出了基于鸟瞰视的cross-modal学习 для领域总结（DG）的3D语义分割，称为BEV-DG。DG比UDA更加困难，因为模型在训练时无法访问目标领域，因此它需要通过cross-modal学习来减少领域差距。由于3D语义分割需要每个点的分类，现有的cross-modal学习是直接进行点对点的，这是 projection between pixels and points 的不一致敏感。为此，我们的方法是通过cross-modal学习下鸟瞰视模型化领域无关表示，使用BEV-based Area-to-area Fusion（BAF）来进行cross-modal学习，这种方法具有更高的错误忍容度。此外，我们还提出了基于鸟瞰视的BEV-driven Domain Contrastive Learning（BDCL），通过cross-modal学习来模型领域无关表示。我们设计了基于三个3D数据集的三个领域总结设置，BEV-DG在所有设置中都以很大的优势超越了当前的竞争对手。
</details></li>
</ul>
<hr>
<h2 id="Seed-Feature-Maps-based-CNN-Models-for-LEO-Satellite-Remote-Sensing-Services"><a href="#Seed-Feature-Maps-based-CNN-Models-for-LEO-Satellite-Remote-Sensing-Services" class="headerlink" title="Seed Feature Maps-based CNN Models for LEO Satellite Remote Sensing Services"></a>Seed Feature Maps-based CNN Models for LEO Satellite Remote Sensing Services</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06515">http://arxiv.org/abs/2308.06515</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhichao Lu, Chuntao Ding, Shangguang Wang, Ran Cheng, Felix Juefei-Xu, Vishnu Naresh Boddeti</li>
<li>for: 这篇研究是为了提出一个基于ground-station server的框架，以实现高性能的卷积神经网络模型在低地球轨道（LEO）卫星上的快速遥测图像处理。</li>
<li>methods: 本研究使用了一个基于seed feature map的框架，具体是每个层的卷积神经网络模型仅包含一个可学习的特征图（seed feature map），并通过特定规律生成其他特征图。此外，这个框架还使用了Random Hyperparameter Generation（RHG）技术，实现在LEO卫星上更新卷积神经网络模型。</li>
<li>results: 实验结果显示，提出的框架可以与现有的State-of-the-art方法相比，在ISPRS Vaihingen、ISPRS Potsdam、UAVid和LoveDA等数据集上实现更高的mIoU，特别是在UAVid数据集上，SineFM-based模型的mIoU高于UNetFormer，仅使用3.3倍少的参数和2.2倍少的FLOPs。<details>
<summary>Abstract</summary>
Deploying high-performance convolutional neural network (CNN) models on low-earth orbit (LEO) satellites for rapid remote sensing image processing has attracted significant interest from industry and academia. However, the limited resources available on LEO satellites contrast with the demands of resource-intensive CNN models, necessitating the adoption of ground-station server assistance for training and updating these models. Existing approaches often require large floating-point operations (FLOPs) and substantial model parameter transmissions, presenting considerable challenges. To address these issues, this paper introduces a ground-station server-assisted framework. With the proposed framework, each layer of the CNN model contains only one learnable feature map (called the seed feature map) from which other feature maps are generated based on specific rules. The hyperparameters of these rules are randomly generated instead of being trained, thus enabling the generation of multiple feature maps from the seed feature map and significantly reducing FLOPs. Furthermore, since the random hyperparameters can be saved using a few random seeds, the ground station server assistance can be facilitated in updating the CNN model deployed on the LEO satellite. Experimental results on the ISPRS Vaihingen, ISPRS Potsdam, UAVid, and LoveDA datasets for semantic segmentation services demonstrate that the proposed framework outperforms existing state-of-the-art approaches. In particular, the SineFM-based model achieves a higher mIoU than the UNetFormer on the UAVid dataset, with 3.3x fewer parameters and 2.2x fewer FLOPs.
</details>
<details>
<summary>摘要</summary>
deploying high-performance convolutional neural network (CNN) models on low-earth orbit (LEO) satellites for rapid remote sensing image processing has attracted significant interest from industry and academia. However, the limited resources available on LEO satellites contrast with the demands of resource-intensive CNN models, necessitating the adoption of ground-station server assistance for training and updating these models. existing approaches often require large floating-point operations (FLOPs) and substantial model parameter transmissions, presenting considerable challenges. to address these issues, this paper introduces a ground-station server-assisted framework. with the proposed framework, each layer of the CNN model contains only one learnable feature map (called the seed feature map) from which other feature maps are generated based on specific rules. the hyperparameters of these rules are randomly generated instead of being trained, thus enabling the generation of multiple feature maps from the seed feature map and significantly reducing FLOPs. furthermore, since the random hyperparameters can be saved using a few random seeds, the ground station server assistance can be facilitated in updating the CNN model deployed on the LEO satellite. experimental results on the ISPRS Vaihingen, ISPRS Potsdam, UAVid, and LoveDA datasets for semantic segmentation services demonstrate that the proposed framework outperforms existing state-of-the-art approaches. in particular, the SineFM-based model achieves a higher mIoU than the UNetFormer on the UAVid dataset, with 3.3x fewer parameters and 2.2x fewer FLOPs.
</details></li>
</ul>
<hr>
<h2 id="Out-of-distribution-multi-view-auto-encoders-for-prostate-cancer-lesion-detection"><a href="#Out-of-distribution-multi-view-auto-encoders-for-prostate-cancer-lesion-detection" class="headerlink" title="Out-of-distribution multi-view auto-encoders for prostate cancer lesion detection"></a>Out-of-distribution multi-view auto-encoders for prostate cancer lesion detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06481">http://arxiv.org/abs/2308.06481</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alvaro Fernandez-Quilez, Linas Vidziunas, Ørjan Kløvfjell Thoresen, Ketil Oppedal, Svein Reidar Kjosavik, Trygve Eftestøl</li>
<li>for: 这篇论文目的是为了提出一种基于对外域检测的潜在医疗影像识别方法，并且运用不同T2w方向的多条流进行检测，以提高肝癌潜在病变检测的精确度。</li>
<li>methods: 本论文使用的方法包括对外域检测和多条流方法，以探索肝癌潜在病变检测的可能性。</li>
<li>results: 本论文的结果显示，使用多条流方法可以提高肝癌潜在病变检测的精确度，并且在一个公共可用数据集上获得了更高的检测精确度（AUC），具体为73.1%和82.3%之间。<details>
<summary>Abstract</summary>
Traditional deep learning (DL) approaches based on supervised learning paradigms require large amounts of annotated data that are rarely available in the medical domain. Unsupervised Out-of-distribution (OOD) detection is an alternative that requires less annotated data. Further, OOD applications exploit the class skewness commonly present in medical data. Magnetic resonance imaging (MRI) has proven to be useful for prostate cancer (PCa) diagnosis and management, but current DL approaches rely on T2w axial MRI, which suffers from low out-of-plane resolution. We propose a multi-stream approach to accommodate different T2w directions to improve the performance of PCa lesion detection in an OOD approach. We evaluate our approach on a publicly available data-set, obtaining better detection results in terms of AUC when compared to a single direction approach (73.1 vs 82.3). Our results show the potential of OOD approaches for PCa lesion detection based on MRI.
</details>
<details>
<summary>摘要</summary>
传统的深度学习（DL）方法基于指导学习 paradigma需要大量的标注数据，而这些数据在医疗领域很难获得。不supervised Out-of-distribution（OOD）检测是一种alternative，它需要更少的标注数据。另外，OOD应用可以利用医疗数据的类偏好。核磁共振成像（MRI）已经证明是肠癌（PCa）诊断和管理的有用工具，但当前的DL方法仅仅采用T2w极向MRI，这会受到低外平面分辨率的限制。我们提议一种多流程approach来满足不同的T2w方向，以提高PCa患部检测的性能。我们对公共可用数据集进行评估，并获得了与单向approach相比的更好的检测结果（AUC=73.1 vs AUC=82.3）。我们的结果表明OOD方法在MRI上进行PCa患部检测具有潜在的应用前景。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-multi-view-data-without-annotations-for-prostate-MRI-segmentation-A-contrastive-approach"><a href="#Leveraging-multi-view-data-without-annotations-for-prostate-MRI-segmentation-A-contrastive-approach" class="headerlink" title="Leveraging multi-view data without annotations for prostate MRI segmentation: A contrastive approach"></a>Leveraging multi-view data without annotations for prostate MRI segmentation: A contrastive approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06477">http://arxiv.org/abs/2308.06477</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tim Nikolass Lindeijer, Tord Martin Ytredal, Trygve Eftestøl, Tobias Nordström, Fredrik Jäderling, Martin Eklund, Alvaro Fernandez-Quilez</li>
<li>for: 提高 automatic prostate segmentation 的精度和可靠性，使用 multi-view MRI 数据和 contrastive learning 技术。</li>
<li>methods: 提posed 一种 triplet encoder and single decoder network 基于 U-Net，称为 tU-Net (triplet U-Net)，可以利用不需要注意力的 sagittal 和 coronal 视图来提高 segmentation 的精度。</li>
<li>results: tU-Net 显示在 dice score 指标上 statistically 提高了精度 (91.25+-0.52% 比 86.40+-1.50%,P&lt;.001)，并且在不同视图的数据上进行了可靠的总体骨骼变换。<details>
<summary>Abstract</summary>
An accurate prostate delineation and volume characterization can support the clinical assessment of prostate cancer. A large amount of automatic prostate segmentation tools consider exclusively the axial MRI direction in spite of the availability as per acquisition protocols of multi-view data. Further, when multi-view data is exploited, manual annotations and availability at test time for all the views is commonly assumed. In this work, we explore a contrastive approach at training time to leverage multi-view data without annotations and provide flexibility at deployment time in the event of missing views. We propose a triplet encoder and single decoder network based on U-Net, tU-Net (triplet U-Net). Our proposed architecture is able to exploit non-annotated sagittal and coronal views via contrastive learning to improve the segmentation from a volumetric perspective. For that purpose, we introduce the concept of inter-view similarity in the latent space. To guide the training, we combine a dice score loss calculated with respect to the axial view and its manual annotations together with a multi-view contrastive loss. tU-Net shows statistical improvement in dice score coefficient (DSC) with respect to only axial view (91.25+-0.52% compared to 86.40+-1.50%,P<.001). Sensitivity analysis reveals the volumetric positive impact of the contrastive loss when paired with tU-Net (2.85+-1.34% compared to 3.81+-1.88%,P<.001). Further, our approach shows good external volumetric generalization in an in-house dataset when tested with multi-view data (2.76+-1.89% compared to 3.92+-3.31%,P=.002), showing the feasibility of exploiting non-annotated multi-view data through contrastive learning whilst providing flexibility at deployment in the event of missing views.
</details>
<details>
<summary>摘要</summary>
通过增强多视图数据的利用，我们提出了一种基于对照学习的三元Encoder-单元网络（tU-Net），用于提高肾脏细分。我们在训练时使用了非标注的架子视图和仰视图，通过对照学习来利用这些视图，从而提高 segmentation 的精度。为了引导训练，我们组合了axial视图和其手动注释的 dice score 损失函数，以及多视图对照损失函数。 results 表明，tU-Net 比只使用axial视图的情况提高了 dice score 系数（DSC）（91.25±0.52% vs 86.40±1.50%,P<0.001）。另外，我们的方法还在不同的混合率下进行了敏感性分析，发现对照学习损失函数对于与 tU-Net 结合使用时产生的卷积效应具有 Statistical significance（2.85±1.34% vs 3.81±1.88%,P<0.001）。此外，我们的方法还在一个自有的数据集上进行了 external volumetric 一致性测试，并发现在使用多视图数据时，tU-Net 的性能较好（2.76±1.89% vs 3.92±3.31%,P=.002），这表明了我们的方法可以在实际应用中利用非标注的多视图数据进行对照学习，并且在部署时可以避免 missing views 的问题。
</details></li>
</ul>
<hr>
<h2 id="Tiny-and-Efficient-Model-for-the-Edge-Detection-Generalization"><a href="#Tiny-and-Efficient-Model-for-the-Edge-Detection-Generalization" class="headerlink" title="Tiny and Efficient Model for the Edge Detection Generalization"></a>Tiny and Efficient Model for the Edge Detection Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06468">http://arxiv.org/abs/2308.06468</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xavysp/teed">https://github.com/xavysp/teed</a></li>
<li>paper_authors: Xavier Soria, Yachuan Li, Mohammad Rouhani, Angel D. Sappa</li>
<li>for: 本文 targets at addressing the issue of edge detection in computer vision, with the objectives of simplicity, efficiency, and generalization.</li>
<li>methods: 本文提出了一种轻量级卷积神经网络（TEED），具有只有58K参数，比现状态 искусственный智能模型少。通过在BIPED dataset上训练，可以在less than 30分钟内完成训练，每个epoch仅需less than 5分钟。</li>
<li>results: 本文的提出的模型可以快速 converges within the first few epochs，并且预测的边映射具有高质量。此外，本文还提出了一个新的测试数据集，用于测试边检测模型的通用性。I hope this helps!<details>
<summary>Abstract</summary>
Most high-level computer vision tasks rely on low-level image operations as their initial processes. Operations such as edge detection, image enhancement, and super-resolution, provide the foundations for higher level image analysis. In this work we address the edge detection considering three main objectives: simplicity, efficiency, and generalization since current state-of-the-art (SOTA) edge detection models are increased in complexity for better accuracy. To achieve this, we present Tiny and Efficient Edge Detector (TEED), a light convolutional neural network with only $58K$ parameters, less than $0.2$% of the state-of-the-art models. Training on the BIPED dataset takes $less than 30 minutes$, with each epoch requiring $less than 5 minutes$. Our proposed model is easy to train and it quickly converges within very first few epochs, while the predicted edge-maps are crisp and of high quality. Additionally, we propose a new dataset to test the generalization of edge detection, which comprises samples from popular images used in edge detection and image segmentation. The source code is available in https://github.com/xavysp/TEED.
</details>
<details>
<summary>摘要</summary>
大多数高级计算机视觉任务都基于低级图像操作作为初始过程。操作如图像提高、图像增强和超分辨率，为更高级图像分析提供基础。在这项工作中，我们考虑了三个主要目标：简单、高效和泛化，因为当前状态体系（SOTA）的边检测模型在精度方面增加了复杂度。为达到这个目标，我们提出了简单高效的边检测器（TEED），这是一个具有58000个参数的轻量级卷积神经网络，比状态体系模型少了99.8%的参数。在BIPE dataset上训练时间只需要少于30分钟，每个epoch只需要少于5分钟。我们的提出的模型轻松训练，快速 converges，并且预测的边映射具有高质量。此外，我们还提出了一个新的测试泛化边检测的数据集，该数据集包括流行的图像used in edge detection和图像分类中的样本。源代码可以在https://github.com/xavysp/TEED上获取。
</details></li>
</ul>
<hr>
<h2 id="Improved-YOLOv8-Detection-Algorithm-in-Security-Inspection-Image"><a href="#Improved-YOLOv8-Detection-Algorithm-in-Security-Inspection-Image" class="headerlink" title="Improved YOLOv8 Detection Algorithm in Security Inspection Image"></a>Improved YOLOv8 Detection Algorithm in Security Inspection Image</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06452">http://arxiv.org/abs/2308.06452</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liyao Lu</li>
<li>for: 本研究旨在解决X射线图像检测中的重叠检测对象、假阳性货物检测和检测失败问题。</li>
<li>methods: 本研究提出了基于YOLOv8s的改进X射线财物检测算法CSS-YOLO。</li>
<li>results: 实验结果表明，CSS-YOLO算法能够提高检测精度，降低假阳性率和 missed detection 率，提高安全检查效果。<details>
<summary>Abstract</summary>
Security inspection is the first line of defense to ensure the safety of people's lives and property, and intelligent security inspection is an inevitable trend in the future development of the security inspection industry. Aiming at the problems of overlapping detection objects, false detection of contraband, and missed detection in the process of X-ray image detection, an improved X-ray contraband detection algorithm CSS-YOLO based on YOLOv8s is proposed.
</details>
<details>
<summary>摘要</summary>
安全检查是人们生命和财产安全的首列防御，未来安全检查行业的发展将具有智能化特点。面临检测对象重叠、质控违禁品和检测失败等问题，我们提出了基于YOLOv8s的改进X射线质控检测算法CSS-YOLO。
</details></li>
</ul>
<hr>
<h2 id="TongueSAM-An-Universal-Tongue-Segmentation-Model-Based-on-SAM-with-Zero-Shot"><a href="#TongueSAM-An-Universal-Tongue-Segmentation-Model-Based-on-SAM-with-Zero-Shot" class="headerlink" title="TongueSAM: An Universal Tongue Segmentation Model Based on SAM with Zero-Shot"></a>TongueSAM: An Universal Tongue Segmentation Model Based on SAM with Zero-Shot</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06444">http://arxiv.org/abs/2308.06444</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cshan-github/tonguesam">https://github.com/cshan-github/tonguesam</a></li>
<li>paper_authors: Shan Cao, Qunsheng Ruan, Qingfeng Wu</li>
<li>for: 本研究旨在提出一种通用的舌部分 segmentation模型，以解决现有的舌部分 segmentation方法在不同舌型图像上表现 mediocre 的问题。</li>
<li>methods: 本研究使用的是一种名为 SAM（Segment Anything Model）的大规模预训练交互分割模型，该模型具有强大的零shot泛化能力。通过应用 SAM 到舌部分分割，可以实现零shot 分割不同类型的舌型图像。此外，本研究还使用了一种基于对象检测的 Prompt Generator，以实现一个端到端自动化的舌部分分割方法。</li>
<li>results: 实验表明，TongueSAM 在不同舌部分分割数据集上表现出色，特别是在零shot 下表现。此外，TongueSAM 可以 direct 应用于其他数据集无需 fine-tuning。据我们知道，这是首次应用大规模预训练模型于舌部分分割。研究成果和预训练模型将在：<a target="_blank" rel="noopener" href="https://github.com/cshan-github/TongueSAM">https://github.com/cshan-github/TongueSAM</a> 上公布。<details>
<summary>Abstract</summary>
Tongue segmentation serves as the primary step in automated TCM tongue diagnosis, which plays a significant role in the diagnostic results. Currently, numerous deep learning based methods have achieved promising results. However, most of these methods exhibit mediocre performance on tongues different from the training set. To address this issue, this paper proposes a universal tongue segmentation model named TongueSAM based on SAM (Segment Anything Model). SAM is a large-scale pretrained interactive segmentation model known for its powerful zero-shot generalization capability. Applying SAM to tongue segmentation enables the segmentation of various types of tongue images with zero-shot. In this study, a Prompt Generator based on object detection is integrated into SAM to enable an end-to-end automated tongue segmentation method. Experiments demonstrate that TongueSAM achieves exceptional performance across various of tongue segmentation datasets, particularly under zero-shot. TongueSAM can be directly applied to other datasets without fine-tuning. As far as we know, this is the first application of large-scale pretrained model for tongue segmentation. The project and pretrained model of TongueSAM be publiced in :https://github.com/cshan-github/TongueSAM.
</details>
<details>
<summary>摘要</summary>
叙述分割 serves as the primary step in automated TCM tongue diagnosis, which plays a significant role in the diagnostic results. Currently, numerous deep learning based methods have achieved promising results. However, most of these methods exhibit mediocre performance on tongues different from the training set. To address this issue, this paper proposes a universal tongue segmentation model named TongueSAM based on SAM (Segment Anything Model). SAM is a large-scale pretrained interactive segmentation model known for its powerful zero-shot generalization capability. Applying SAM to tongue segmentation enables the segmentation of various types of tongue images with zero-shot. In this study, a Prompt Generator based on object detection is integrated into SAM to enable an end-to-end automated tongue segmentation method. Experiments demonstrate that TongueSAM achieves exceptional performance across various of tongue segmentation datasets, particularly under zero-shot. TongueSAM can be directly applied to other datasets without fine-tuning. As far as we know, this is the first application of large-scale pretrained model for tongue segmentation. The project and pretrained model of TongueSAM be publiced in :https://github.com/cshan-github/TongueSAM.Here's the translation in Traditional Chinese: tonguesegmentation serves as the primary step in automated TCM tongue diagnosis, which plays a significant role in the diagnostic results. Currently, numerous deep learning based methods have achieved promising results. However, most of these methods exhibit mediocre performance on tongues different from the training set. To address this issue, this paper proposes a universal tongue segmentation model named TongueSAM based on SAM (Segment Anything Model). SAM is a large-scale pretrained interactive segmentation model known for its powerful zero-shot generalization capability. Applying SAM to tongue segmentation enables the segmentation of various types of tongue images with zero-shot. In this study, a Prompt Generator based on object detection is integrated into SAM to enable an end-to-end automated tongue segmentation method. Experiments demonstrate that TongueSAM achieves exceptional performance across various of tongue segmentation datasets, particularly under zero-shot. TongueSAM can be directly applied to other datasets without fine-tuning. As far as we know, this is the first application of large-scale pretrained model for tongue segmentation. The project and pretrained model of TongueSAM be publiced in :https://github.com/cshan-github/TongueSAM.
</details></li>
</ul>
<hr>
<h2 id="Distributionally-Robust-Optimization-and-Invariant-Representation-Learning-for-Addressing-Subgroup-Underrepresentation-Mechanisms-and-Limitations"><a href="#Distributionally-Robust-Optimization-and-Invariant-Representation-Learning-for-Addressing-Subgroup-Underrepresentation-Mechanisms-and-Limitations" class="headerlink" title="Distributionally Robust Optimization and Invariant Representation Learning for Addressing Subgroup Underrepresentation: Mechanisms and Limitations"></a>Distributionally Robust Optimization and Invariant Representation Learning for Addressing Subgroup Underrepresentation: Mechanisms and Limitations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06434">http://arxiv.org/abs/2308.06434</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nilesh Kumar, Ruby Shrestha, Zhiyuan Li, Linwei Wang</li>
<li>For: This paper aims to address the issue of spurious correlation due to subgroup underrepresentation in medical image classification, specifically by exploring the use of robust optimization to learn invariant representations.* Methods: The paper proposes a novel approach that leverages robust optimization to facilitate the learning of invariant representations, and evaluates the effectiveness of this approach through a comprehensive study.* Results: The proposed approach is shown to improve the performance of classifiers on underrepresented subgroups, while maintaining high average and worst-group performance, compared to existing methods such as generalized reweighting and naive invariant representation learning.<details>
<summary>Abstract</summary>
Spurious correlation caused by subgroup underrepresentation has received increasing attention as a source of bias that can be perpetuated by deep neural networks (DNNs). Distributionally robust optimization has shown success in addressing this bias, although the underlying working mechanism mostly relies on upweighting under-performing samples as surrogates for those underrepresented in data. At the same time, while invariant representation learning has been a powerful choice for removing nuisance-sensitive features, it has been little considered in settings where spurious correlations are caused by significant underrepresentation of subgroups. In this paper, we take the first step to better understand and improve the mechanisms for debiasing spurious correlation due to subgroup underrepresentation in medical image classification. Through a comprehensive evaluation study, we first show that 1) generalized reweighting of under-performing samples can be problematic when bias is not the only cause for poor performance, while 2) naive invariant representation learning suffers from spurious correlations itself. We then present a novel approach that leverages robust optimization to facilitate the learning of invariant representations at the presence of spurious correlations. Finetuned classifiers utilizing such representation demonstrated improved abilities to reduce subgroup performance disparity, while maintaining high average and worst-group performance.
</details>
<details>
<summary>摘要</summary>
假设对于小分支的参数不足，导致深度神经网络（DNNs）中的伪正相关。 Distributionally robust optimization 已经在解决这种偏见方面取得成功，但是其主要运作机制是通过增重下performing samples 作为没有在数据中受到代表的样本。 在这篇研究中，我们对对于小分支参数不足导致的伪正相关的推导和改进方法进行了首次的研究。我们首先显示了以下两个结果：1）通过增重下performing samples 并不一定能够解决伪正相关，而2）简单的对称表现学习方法本身受到了伪正相关的影响。我们然后提出了一种新的方法，利用Robust optimization 来促进对伪正相关的推导。我们继续调整这些表现，以便在存在伪正相关的情况下维持高的平均和最差分支性能。
</details></li>
</ul>
<hr>
<h2 id="Learn-Single-horizon-Disease-Evolution-for-Predictive-Generation-of-Post-therapeutic-Neovascular-Age-related-Macular-Degeneration"><a href="#Learn-Single-horizon-Disease-Evolution-for-Predictive-Generation-of-Post-therapeutic-Neovascular-Age-related-Macular-Degeneration" class="headerlink" title="Learn Single-horizon Disease Evolution for Predictive Generation of Post-therapeutic Neovascular Age-related Macular Degeneration"></a>Learn Single-horizon Disease Evolution for Predictive Generation of Post-therapeutic Neovascular Age-related Macular Degeneration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06432">http://arxiv.org/abs/2308.06432</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhan Zhang, Kun Huang, Mingchao Li, Songtao Yuan, Qiang Chen</li>
<li>for: 预测 age-related macular degeneration (nAMD) 疾病进程和效果。</li>
<li>methods: 提posed a single-horizon disease evolution network (SHENet)，使用 feature encoder、graph evolution module 和 feature decoder，并通过 adversarial training 确保疾病进程学习的有效性。</li>
<li>results: 与其他生成方法相比，SHENet 生成的 SD-OCT 图像质量最高，同时保持结构保护和内容预测。 Qualitative evaluations 也表明 SHENet 的视觉效果较好。<details>
<summary>Abstract</summary>
Most of the existing disease prediction methods in the field of medical image processing fall into two classes, namely image-to-category predictions and image-to-parameter predictions. Few works have focused on image-to-image predictions. Different from multi-horizon predictions in other fields, ophthalmologists prefer to show more confidence in single-horizon predictions due to the low tolerance of predictive risk. We propose a single-horizon disease evolution network (SHENet) to predictively generate post-therapeutic SD-OCT images by inputting pre-therapeutic SD-OCT images with neovascular age-related macular degeneration (nAMD). In SHENet, a feature encoder converts the input SD-OCT images to deep features, then a graph evolution module predicts the process of disease evolution in high-dimensional latent space and outputs the predicted deep features, and lastly, feature decoder recovers the predicted deep features to SD-OCT images. We further propose an evolution reinforcement module to ensure the effectiveness of disease evolution learning and obtain realistic SD-OCT images by adversarial training. SHENet is validated on 383 SD-OCT cubes of 22 nAMD patients based on three well-designed schemes based on the quantitative and qualitative evaluations. Compared with other generative methods, the generative SD-OCT images of SHENet have the highest image quality. Besides, SHENet achieves the best structure protection and content prediction. Qualitative evaluations also demonstrate that SHENet has a better visual effect than other methods. SHENet can generate post-therapeutic SD-OCT images with both high prediction performance and good image quality, which has great potential to help ophthalmologists forecast the therapeutic effect of nAMD.
</details>
<details>
<summary>摘要</summary>
现有的疾病预测方法在医学图像处理领域主要分为两类：图像到类别预测和图像到参数预测，其中少数工作关注到图像到图像预测。与其他多个预测horizon不同，眼科医生更偏好单个预测horizon，因为预测风险的低忍性。我们提出了单个预测疾病演化网络（SHENet），用于预测基于前治疗SD-OCT图像的后治疗SD-OCT图像。在SHENet中，一个特征编码器将输入SD-OCT图像转化为深度特征，然后一个图像演化模块预测疾病演化过程在高维潜在空间中，并输出预测的深度特征。最后，特征解码器重建预测的深度特征为SD-OCT图像。我们还提出了演化增强模块，以确保疾病演化学习的有效性并获得真实的SD-OCT图像，通过对抗训练。SHENet在383个SD-OCT立方体上的22例nAMD患者基于三种有效的方案进行验证，并通过量化和质量评估。与其他生成方法相比，SHENet生成的SD-OCT图像的生成质量最高。此外，SHENet保持了最佳的结构保护和内容预测。质量评估还表明，SHENet在视觉效果方面表现更好。SHENet可以预测nAMD后治疗SD-OCT图像，具有高预测性和良好的图像质量，这对眼科医生预测nAMD治疗效果具有很大潜力。
</details></li>
</ul>
<hr>
<h2 id="M-M-Tackling-False-Positives-in-Mammography-with-a-Multi-view-and-Multi-instance-Learning-Sparse-Detector"><a href="#M-M-Tackling-False-Positives-in-Mammography-with-a-Multi-view-and-Multi-instance-Learning-Sparse-Detector" class="headerlink" title="M&amp;M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector"></a>M&amp;M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06420">http://arxiv.org/abs/2308.06420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yen Nhi Truong Vu, Dan Guo, Ahmed Taha, Jason Su, Thomas Paul Matthews</li>
<li>for: 提高诊断率和避免假阳性结果</li>
<li>methods: 使用 sparse R-CNN，包括多视图交叉注意模块和多实例学习</li>
<li>results: 提高了检测和预测性能，并通过精细的ablation study证明每个组件的效果<details>
<summary>Abstract</summary>
Deep-learning-based object detection methods show promise for improving screening mammography, but high rates of false positives can hinder their effectiveness in clinical practice. To reduce false positives, we identify three challenges: (1) unlike natural images, a malignant mammogram typically contains only one malignant finding; (2) mammography exams contain two views of each breast, and both views ought to be considered to make a correct assessment; (3) most mammograms are negative and do not contain any findings. In this work, we tackle the three aforementioned challenges by: (1) leveraging Sparse R-CNN and showing that sparse detectors are more appropriate than dense detectors for mammography; (2) including a multi-view cross-attention module to synthesize information from different views; (3) incorporating multi-instance learning (MIL) to train with unannotated images and perform breast-level classification. The resulting model, M&M, is a Multi-view and Multi-instance learning system that can both localize malignant findings and provide breast-level predictions. We validate M&M's detection and classification performance using five mammography datasets. In addition, we demonstrate the effectiveness of each proposed component through comprehensive ablation studies.
</details>
<details>
<summary>摘要</summary>
深度学习基于对象检测方法在萤幕检查中显示出优秀表现，但高 false positive 率可能会阻碍其在临床实践中的效iveness。为了减少 false positive，我们标识了三个挑战：（1）癌症肺像素通常只包含一个癌症发现;（2）萤幕检查包括两个视图每一个乳腺，需要考虑两个视图来确定正确的评估;（3）大多数萤幕检查为正常图像，没有任何发现。在这种情况下，我们解决了这三个挑战，通过：（1）利用稀疏 R-CNN，并证明稀疏检测器更适合萤幕检查;（2）添加多视图交叉注意模块，以将不同视图的信息相互协同;（3）采用多例学习（MIL），以使用无注释图像进行训练，并在乳腺级别进行预测。得到的模型称为 M&M，它可以同时localize 癌症发现和进行乳腺级别预测。我们验证 M&M 的检测和预测性能使用五个萤幕检查 dataset。此外，我们还通过完整的减少研究，证明每一个提案的效果。
</details></li>
</ul>
<hr>
<h2 id="Improving-Pseudo-Labels-for-Open-Vocabulary-Object-Detection"><a href="#Improving-Pseudo-Labels-for-Open-Vocabulary-Object-Detection" class="headerlink" title="Improving Pseudo Labels for Open-Vocabulary Object Detection"></a>Improving Pseudo Labels for Open-Vocabulary Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06412">http://arxiv.org/abs/2308.06412</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiyu Zhao, Samuel Schulter, Long Zhao, Zhixing Zhang, Vijay Kumar B. G, Yumin Suh, Manmohan Chandraker, Dimitris N. Metaxas</li>
<li>for: 提高开放词汇物体检测（OVD）中使用预先训练的视觉语言模型（VLM）生成的假标签（PL）的性能。</li>
<li>methods: 提出在线自我训练和拆分并融合头（SAS-Det）方法，包括自我训练VLMs生成高质量PL，并利用拆分并融合头除去PL的地方噪声，同时 fusion complementary knowledge learned from precise ground truth和噪声PL。</li>
<li>results: 在COCO和LVISbenchmark上 achieved 37.4 AP$_{50}$和27.3 AP$_r$，胜过先前的状态艺术模型，并且 Pseudo labeling 速度比过去的方法快三倍。<details>
<summary>Abstract</summary>
Recent studies show promising performance in open-vocabulary object detection (OVD) using pseudo labels (PLs) from pretrained vision and language models (VLMs). However, PLs generated by VLMs are extremely noisy due to the gap between the pretraining objective of VLMs and OVD, which blocks further advances on PLs. In this paper, we aim to reduce the noise in PLs and propose a method called online Self-training And a Split-and-fusion head for OVD (SAS-Det). First, the self-training finetunes VLMs to generate high quality PLs while prevents forgetting the knowledge learned in the pretraining. Second, a split-and-fusion (SAF) head is designed to remove the noise in localization of PLs, which is usually ignored in existing methods. It also fuses complementary knowledge learned from both precise ground truth and noisy pseudo labels to boost the performance. Extensive experiments demonstrate SAS-Det is both efficient and effective. Our pseudo labeling is 3 times faster than prior methods. SAS-Det outperforms prior state-of-the-art models of the same scale by a clear margin and achieves 37.4 AP$_{50}$ and 27.3 AP$_r$ on novel categories of the COCO and LVIS benchmarks, respectively.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Detecting-and-Preventing-Hallucinations-in-Large-Vision-Language-Models"><a href="#Detecting-and-Preventing-Hallucinations-in-Large-Vision-Language-Models" class="headerlink" title="Detecting and Preventing Hallucinations in Large Vision Language Models"></a>Detecting and Preventing Hallucinations in Large Vision Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06394">http://arxiv.org/abs/2308.06394</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anisha Gunjal, Jihan Yin, Erhan Bas<br>for:The paper aims to address the issue of hallucinations in instruction-tuned large vision language models (LVLMs) for visual question answering (VQA).methods:The authors introduce a new dataset called M-HalDetect, which consists of 16,000 fine-grained annotations on VQA examples to train and benchmark models for hallucination detection and prevention. They also propose a novel optimization method called Fine-grained Direct Preference Optimization (FDPO) to reduce hallucinations in LVLMs.results:The authors evaluate the effectiveness of M-HalDetect and FDPO using human evaluation and find that they reduce hallucination rates in InstructBLIP by 41% and 55%, respectively. They also find that their reward model generalizes to other multi-modal models, reducing hallucinations in LLaVA and mPLUG-OWL by 15% and 57%, respectively, and has strong correlation with human evaluated accuracy scores.<details>
<summary>Abstract</summary>
Instruction tuned Large Vision Language Models (LVLMs) have significantly advanced in generalizing across a diverse set of multi-modal tasks, especially for Visual Question Answering (VQA). However, generating detailed responses that are visually grounded is still a challenging task for these models. We find that even the current state-of-the-art LVLMs (InstructBLIP) still contain a staggering 30 percent of the hallucinatory text in the form of non-existent objects, unfaithful descriptions, and inaccurate relationships. To address this, we introduce M-HalDetect, a (M)ultimodal (Hal)lucination (Detect)ion Dataset that can be used to train and benchmark models for hallucination detection and prevention. M-HalDetect consists of 16k fine-grained annotations on VQA examples, making it the first comprehensive multi-modal hallucination detection dataset for detailed image descriptions. Unlike previous work that only consider object hallucination, we additionally annotate both entity descriptions and relationships that are unfaithful. To demonstrate the potential of this dataset for hallucination prevention, we optimize InstructBLIP through our novel Fine-grained Direct Preference Optimization (FDPO). We also train fine-grained multi-modal reward models from InstructBLIP and evaluate their effectiveness with best-of-n rejection sampling. We perform human evaluation on both FDPO and rejection sampling, and find that they reduce hallucination rates in InstructBLIP by 41% and 55% respectively. We also find that our reward model generalizes to other multi-modal models, reducing hallucinations in LLaVA and mPLUG-OWL by 15% and 57% respectively, and has strong correlation with human evaluated accuracy scores.
</details>
<details>
<summary>摘要</summary>
干脆大视语言模型（LVLM）在多modal任务上 generalized 了，特别是对于视觉问答（VQA）。然而，生成视觉固有的回答仍然是current state-of-the-art LVLMs（InstructBLIP）中的挑战。我们发现，甚至当前最佳LVLMs中还有30%的hallucination text，包括不存在的物体、不准确的描述和关系。为了解决这个问题，我们提出了M-HalDetect数据集，可以用于训练和对比模型，以检测和避免hallucination。M-HalDetect包括16k精细的VQA示例注释，使其成为首个多modal hallucination detection数据集。不同于之前的工作，我们不仅考虑物体hallucination，还注释了不准确的实体描述和关系。为了证明M-HalDetect的可用性，我们对InstructBLIP进行了novel Fine-grained Direct Preference Optimization（FDPO）优化。我们还使用InstructBLIP中的精细多modal奖励模型进行训练，并使用best-of-n拒绝采样来评估其效果。我们对FDPO和拒绝采样进行了人工评估，发现它们可以降低InstructBLIP中的hallucination率41%和55%。此外，我们发现我们的奖励模型可以普适化到其他多modal模型，降低LLaVA和mPLUG-OWL中的hallucination率15%和57%，并与人类评估精度成对。
</details></li>
</ul>
<hr>
<h2 id="R2S100K-Road-Region-Segmentation-Dataset-For-Semi-Supervised-Autonomous-Driving-in-the-Wild"><a href="#R2S100K-Road-Region-Segmentation-Dataset-For-Semi-Supervised-Autonomous-Driving-in-the-Wild" class="headerlink" title="R2S100K: Road-Region Segmentation Dataset For Semi-Supervised Autonomous Driving in the Wild"></a>R2S100K: Road-Region Segmentation Dataset For Semi-Supervised Autonomous Driving in the Wild</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06393">http://arxiv.org/abs/2308.06393</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Atif Butt, Hassan Ali, Adnan Qayyum, Waqas Sultani, Ala Al-Fuqaha, Junaid Qadir</li>
<li>for: 这项研究的目的是提供一个大规模的、多样化的道路区域分割数据集，以便为自动驾驶技术的发展提供更好的支持。</li>
<li>methods: 这项研究使用了一种名为Efficient Data Sampling（EDS）的自我教学框架，通过利用无标注数据来提高学习效果，同时还使用了 semi-supervised learning 方法。</li>
<li>results: 实验结果表明，提出的方法可以显著改善 semantic segmentation 任务的泛化能力，同时也可以降低标注成本。<details>
<summary>Abstract</summary>
Semantic understanding of roadways is a key enabling factor for safe autonomous driving. However, existing autonomous driving datasets provide well-structured urban roads while ignoring unstructured roadways containing distress, potholes, water puddles, and various kinds of road patches i.e., earthen, gravel etc. To this end, we introduce Road Region Segmentation dataset (R2S100K) -- a large-scale dataset and benchmark for training and evaluation of road segmentation in aforementioned challenging unstructured roadways. R2S100K comprises 100K images extracted from a large and diverse set of video sequences covering more than 1000 KM of roadways. Out of these 100K privacy respecting images, 14,000 images have fine pixel-labeling of road regions, with 86,000 unlabeled images that can be leveraged through semi-supervised learning methods. Alongside, we present an Efficient Data Sampling (EDS) based self-training framework to improve learning by leveraging unlabeled data. Our experimental results demonstrate that the proposed method significantly improves learning methods in generalizability and reduces the labeling cost for semantic segmentation tasks. Our benchmark will be publicly available to facilitate future research at https://r2s100k.github.io/.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>路径理解是自驾投控车中关键的能力因素。然而，现有的自驾投控车数据集只提供了有效的城市路径，而忽略了不结构化的路径中的压力、沟壑、水泥等等。为此，我们介绍了路径区域分割数据集（R2S100K）——一个大规模的数据集和标准 для训练和评估路径分割在上述挑战性的路径上。R2S100K包含100K张图像，其中14,000张图像有细腻的像素标注路径区域，剩下86,000张图像可以通过半有结构学习方法进行利用。此外，我们提出了一种效率的数据采样（EDS）基于的自动训练框架，以提高学习的通用性和减少标注成本。我们的实验结果表明，提posed方法可以显著提高学习方法的通用性和减少标注成本。我们的标准将在https://r2s100k.github.io/上公开，以便未来的研究。
</details></li>
</ul>
<hr>
<h2 id="U-RED-Unsupervised-3D-Shape-Retrieval-and-Deformation-for-Partial-Point-Clouds"><a href="#U-RED-Unsupervised-3D-Shape-Retrieval-and-Deformation-for-Partial-Point-Clouds" class="headerlink" title="U-RED: Unsupervised 3D Shape Retrieval and Deformation for Partial Point Clouds"></a>U-RED: Unsupervised 3D Shape Retrieval and Deformation for Partial Point Clouds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06383">http://arxiv.org/abs/2308.06383</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhangcyg/u-red">https://github.com/zhangcyg/u-red</a></li>
<li>paper_authors: Yan Di, Chenyangguang Zhang, Ruida Zhang, Fabian Manhardt, Yongzhi Su, Jason Rambach, Didier Stricker, Xiangyang Ji, Federico Tombari</li>
<li>for: 本文提出了一种不supervised shape retrieval和扭形管道，用于从已有的CAD模型库中检索和扭形匹配目标对象。</li>
<li>methods: 该管道使用了一种新的点级差异指导度量来抗随机变量，并通过将所有可能的全形对象投影到单位球面上来处理一个部分观察的一对多关系。</li>
<li>results: 在PartNet、ComplementMe和Scan2CAD等 sintetic和实际数据集上，U-RED比前状态艺术方法提高47.3%、16.7%和31.6%。<details>
<summary>Abstract</summary>
In this paper, we propose U-RED, an Unsupervised shape REtrieval and Deformation pipeline that takes an arbitrary object observation as input, typically captured by RGB images or scans, and jointly retrieves and deforms the geometrically similar CAD models from a pre-established database to tightly match the target. Considering existing methods typically fail to handle noisy partial observations, U-RED is designed to address this issue from two aspects. First, since one partial shape may correspond to multiple potential full shapes, the retrieval method must allow such an ambiguous one-to-many relationship. Thereby U-RED learns to project all possible full shapes of a partial target onto the surface of a unit sphere. Then during inference, each sampling on the sphere will yield a feasible retrieval. Second, since real-world partial observations usually contain noticeable noise, a reliable learned metric that measures the similarity between shapes is necessary for stable retrieval. In U-RED, we design a novel point-wise residual-guided metric that allows noise-robust comparison. Extensive experiments on the synthetic datasets PartNet, ComplementMe and the real-world dataset Scan2CAD demonstrate that U-RED surpasses existing state-of-the-art approaches by 47.3%, 16.7% and 31.6% respectively under Chamfer Distance.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了无监督的形状检索和扭曲管道（U-RED），它可以将从RGB图像或扫描得到的任意物体观察作为输入，并将相似的CAD模型从预设的数据库中检索出来，以便与目标物体紧密匹配。现有的方法通常无法处理干扰性的部分观察，因此U-RED在两个方面进行了改进。首先，由于一个部分形状可能对应多个可能的全形状，因此检索方法必须允许这种杂乱的一对多关系。U-RED通过将所有可能的全形状 проек到单位球上来解决这个问题。然后，在推理时，每个样本在球上的抽象都将产生一个可能的检索。其次，由于实际的部分观察通常含有显著的干扰，因此需要一个可靠的学习的形状相似度量表，以确保稳定的检索。在U-RED中，我们设计了一种新的点级差异导向的形状相似度量表，允许比较干扰的形状。我们在PartNet、ComplementMe和Scan2CAD等 sintetic和实际数据集上进行了广泛的实验，结果显示，U-RED在Chamfer Distance下比现有状态的方法提高47.3%、16.7%和31.6%。
</details></li>
</ul>
<hr>
<h2 id="CATS-v2-Hybrid-encoders-for-robust-medical-segmentation"><a href="#CATS-v2-Hybrid-encoders-for-robust-medical-segmentation" class="headerlink" title="CATS v2: Hybrid encoders for robust medical segmentation"></a>CATS v2: Hybrid encoders for robust medical segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06377">http://arxiv.org/abs/2308.06377</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/haoli12345/cats">https://github.com/haoli12345/cats</a></li>
<li>paper_authors: Hao Li, Han Liu, Dewei Hu, Xing Yao, Jiacheng Wang, Ipek Oguz</li>
<li>for: 这个研究的目的是提出一个以CATS为基础的对� части�内部构成，以提高医疗影像分类�的精度和意义性。</li>
<li>methods: 这个研究使用了CATS v2模型，其中包括一个具有传播�的 Hybrid 构成，该构成包括一个 CNN 基础的 Encoder 路径和一个传播�的 Transformer 路径。</li>
<li>results: 在两个公共挑战赛 datasets 上进行评估，CATS v2 模型在分类 VS 和肾脏癌等项目上表现出较高的 Dice  scores，较以前的方法为高。<details>
<summary>Abstract</summary>
Convolutional Neural Networks (CNNs) have exhibited strong performance in medical image segmentation tasks by capturing high-level (local) information, such as edges and textures. However, due to the limited field of view of convolution kernel, it is hard for CNNs to fully represent global information. Recently, transformers have shown good performance for medical image segmentation due to their ability to better model long-range dependencies. Nevertheless, transformers struggle to capture high-level spatial features as effectively as CNNs. A good segmentation model should learn a better representation from local and global features to be both precise and semantically accurate. In our previous work, we proposed CATS, which is a U-shaped segmentation network augmented with transformer encoder. In this work, we further extend this model and propose CATS v2 with hybrid encoders. Specifically, hybrid encoders consist of a CNN-based encoder path paralleled to a transformer path with a shifted window, which better leverage both local and global information to produce robust 3D medical image segmentation. We fuse the information from the convolutional encoder and the transformer at the skip connections of different resolutions to form the final segmentation. The proposed method is evaluated on two public challenge datasets: Cross-Modality Domain Adaptation (CrossMoDA) and task 5 of Medical Segmentation Decathlon (MSD-5), to segment vestibular schwannoma (VS) and prostate, respectively. Compared with the state-of-the-art methods, our approach demonstrates superior performance in terms of higher Dice scores.
</details>
<details>
<summary>摘要</summary>
卷积神经网络（CNN）在医疗图像分割任务中表现出色，捕捉到高级（本地）信息，如边缘和 тексту层。然而，由于卷积核的视野有限，使得CNN难以完全表征全局信息。近些年来， transformer 在医疗图像分割中表现良好，主要是因为它们能够更好地模型长距离依赖关系。然而， transformer 在捕捉高级空间特征方面表现不如 CNN 好。为了建立一个好的分割模型，需要学习更好的 Representation 来兼顾本地和全局特征，以确保准确和Semantic 准确。在我们之前的工作中，我们提出了CATS，它是一个 U-shaped 分割网络，通过添加 transformer 编码器来提高性能。在这个工作中，我们进一步扩展了CATS 模型，并提出了CATS v2 模型，它使用了混合编码器。具体来说，混合编码器包括一个 CNN 基于编码器路径和一个 shifted window 的 transformer 路径，这两者可以更好地利用本地和全局信息，以生成Robust 3D 医疗图像分割。我们在不同的分辨率之间进行 skip 连接，将 convolutional 编码器和 transformer 的信息融合起来，以生成最终的分割。我们在 Cross-Modality Domain Adaptation（CrossMoDA）和 Medical Segmentation Decathlon 任务（MSD-5）上进行了评估，对 vestibular schwannoma 和 prostate 进行了分割。与当前的状态艺术方法相比，我们的方法在 Dice 分数方面表现出色。
</details></li>
</ul>
<hr>
<h2 id="Surrogate-Model-for-Geological-CO2-Storage-and-Its-Use-in-MCMC-based-History-Matching"><a href="#Surrogate-Model-for-Geological-CO2-Storage-and-Its-Use-in-MCMC-based-History-Matching" class="headerlink" title="Surrogate Model for Geological CO2 Storage and Its Use in MCMC-based History Matching"></a>Surrogate Model for Geological CO2 Storage and Its Use in MCMC-based History Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06341">http://arxiv.org/abs/2308.06341</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifu Han, Francois P. Hamon, Su Jiang, Louis J. Durlofsky</li>
<li>for: 这个研究targets an important application in geological carbon storage operations, specifically history matching of storage systems with high prior geological uncertainty.</li>
<li>methods: The authors extend a recently introduced recurrent R-U-Net surrogate model to treat geomodel realizations drawn from a wide range of geological scenarios, using flow simulation results and a Markov chain Monte Carlo history matching workflow.</li>
<li>results: The surrogate model provides accurate predictions for new realizations over the full range of geological scenarios, with median relative error of 1.3% in pressure and 4.5% in saturation. The incorporation of the surrogate model into the history matching workflow reduces geological uncertainty and leads to posterior 3D pressure and saturation fields that display much closer agreement with the true-model responses than prior predictions.<details>
<summary>Abstract</summary>
Deep-learning-based surrogate models show great promise for use in geological carbon storage operations. In this work we target an important application - the history matching of storage systems characterized by a high degree of (prior) geological uncertainty. Toward this goal, we extend the recently introduced recurrent R-U-Net surrogate model to treat geomodel realizations drawn from a wide range of geological scenarios. These scenarios are defined by a set of metaparameters, which include the mean and standard deviation of log-permeability, permeability anisotropy ratio, horizontal correlation length, etc. An infinite number of realizations can be generated for each set of metaparameters, so the range of prior uncertainty is large. The surrogate model is trained with flow simulation results, generated using the open-source simulator GEOS, for 2000 random realizations. The flow problems involve four wells, each injecting 1 Mt CO2/year, for 30 years. The trained surrogate model is shown to provide accurate predictions for new realizations over the full range of geological scenarios, with median relative error of 1.3% in pressure and 4.5% in saturation. The surrogate model is incorporated into a Markov chain Monte Carlo history matching workflow, where the goal is to generate history matched realizations and posterior estimates of the metaparameters. We show that, using observed data from monitoring wells in synthetic `true' models, geological uncertainty is reduced substantially. This leads to posterior 3D pressure and saturation fields that display much closer agreement with the true-model responses than do prior predictions.
</details>
<details>
<summary>摘要</summary>
深度学习基本的代理模型在地质碳存储操作中表现出了极大的承诺。在这项工作中，我们target了一个重要应用 - 地质风险很高的存储系统历史匹配。为达到这个目标，我们将Recurrent R-U-Net代理模型扩展到处理各种不同的地质场景。这些场景是通过一组元参数来定义的，其中包括含量风险的平均值和标准差、滤 filtering ratio、水平相关长度等。可以生成无数量的实例 для每个元参数，因此地质风险的范围是非常广泛。我们使用GEOS开源模拟器对2000个随机实例进行流体模拟，并将模型训练于这些实例。训练后，代理模型能够准确预测新的实例，并且在全面的地质场景下显示了 median相对误差为1.3%的压力和4.5%的浓度。这个代理模型被 incorporated into Markov chain Monte Carlo历史匹配工作流程中，以生成历史匹配实例和 posterior 的元参数估计。我们显示，使用 synthetic 'true' 模型中的观测数据，地质风险可以减少得非常多。这导致了 posterior 3D 压力和浓度场 displaying much closer agreement with the true-model responses than prior predictions。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Based-Open-Source-Toolkit-for-Eosinophil-Detection-in-Pediatric-Eosinophilic-Esophagitis"><a href="#Deep-Learning-Based-Open-Source-Toolkit-for-Eosinophil-Detection-in-Pediatric-Eosinophilic-Esophagitis" class="headerlink" title="Deep Learning-Based Open Source Toolkit for Eosinophil Detection in Pediatric Eosinophilic Esophagitis"></a>Deep Learning-Based Open Source Toolkit for Eosinophil Detection in Pediatric Eosinophilic Esophagitis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06333">http://arxiv.org/abs/2308.06333</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hrlblab/open-eoe">https://github.com/hrlblab/open-eoe</a></li>
<li>paper_authors: Juming Xiong, Yilin Liu, Ruining Deng, Regina N Tyree, Hernan Correa, Girish Hiremath, Yaohong Wang, Yuankai Huo<br>for:This paper aims to develop an open-source toolkit for automated detection of eosinophils in whole slide images for the diagnosis of eosinophilic esophagitis.methods:The toolkit uses deep learning-based object detection models and ensemble learning to improve the accuracy and reliability of eosinophil detection.results:The toolkit was tested on a set of 289 whole slide images and achieved an accuracy of 91% in detecting eosinophils at the widely accepted threshold of &gt;&#x3D; 15 per high power field for diagnosing eosinophilic esophagitis.<details>
<summary>Abstract</summary>
Eosinophilic Esophagitis (EoE) is a chronic, immune/antigen-mediated esophageal disease, characterized by symptoms related to esophageal dysfunction and histological evidence of eosinophil-dominant inflammation. Owing to the intricate microscopic representation of EoE in imaging, current methodologies which depend on manual identification are not only labor-intensive but also prone to inaccuracies. In this study, we develop an open-source toolkit, named Open-EoE, to perform end-to-end whole slide image (WSI) level eosinophil (Eos) detection using one line of command via Docker. Specifically, the toolkit supports three state-of-the-art deep learning-based object detection models. Furthermore, Open-EoE further optimizes the performance by implementing an ensemble learning strategy, and enhancing the precision and reliability of our results. The experimental results demonstrated that the Open-EoE toolkit can efficiently detect Eos on a testing set with 289 WSIs. At the widely accepted threshold of >= 15 Eos per high power field (HPF) for diagnosing EoE, the Open-EoE achieved an accuracy of 91%, showing decent consistency with pathologist evaluations. This suggests a promising avenue for integrating machine learning methodologies into the diagnostic process for EoE. The docker and source code has been made publicly available at https://github.com/hrlblab/Open-EoE.
</details>
<details>
<summary>摘要</summary>
《Eosinophilic Esophagitis (EoE)是一种慢性、免疫/抗原诱导的食道疾病，表现为食道功能障碍和 Histological 证据显示的吸收性黑色素细胞滥多性Inflammation。由于EoE的微scopic表现在成像中复杂，目前的方法ologies依靠 manual identification 不仅劳累也容易出错。本研究中，我们开发了一个开源工具kit，名为 Open-EoE，通过一行命令 via Docker 来实现整个扫描图像 (WSI) 层的吸收性黑色素细胞 (Eos) 检测。Specifically，工具kit 支持三种 state-of-the-art deep learning-based object detection 模型。此外，Open-EoE 还进一步优化了性能，通过实现 ensemble learning 策略，并提高了结果的精度和可靠性。实验结果表明，Open-EoE 工具kit 可以有效地检测 Eos 在289个 WSIs 上。在 widely accepted 的 >= 15 Eos per high power field (HPF) 的标准下，Open-EoE 达到了 91% 的准确率，与Pathologist 评估相当一致。这表明可以将机器学习方法 integrate 到 EoE 诊断过程中，并且 Open-EoE 的 Docker 和源代码已经在 https://github.com/hrlblab/Open-EoE 上公开 released。
</details></li>
</ul>
<hr>
<h2 id="Revolutionizing-Space-Health-Swin-FSR-Advancing-Super-Resolution-of-Fundus-Images-for-SANS-Visual-Assessment-Technology"><a href="#Revolutionizing-Space-Health-Swin-FSR-Advancing-Super-Resolution-of-Fundus-Images-for-SANS-Visual-Assessment-Technology" class="headerlink" title="Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology"></a>Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06332">http://arxiv.org/abs/2308.06332</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/FarihaHossain/SwinFSR">https://github.com/FarihaHossain/SwinFSR</a></li>
<li>paper_authors: Khondker Fariha Hossain, Sharif Amit Kamran, Joshua Ong, Andrew G. Lee, Alireza Tavakkoli<br>for: 这paper是为了提出一种基于SwinTransformer的眼内画像超分辨模型，用于解决在各种各样的眼内图像识别任务中的数据传输压缩问题。methods: 这paper使用了SwinTransformer搭配空间和深度精度注意力来实现眼内图像超分辨。results: 这paper在三个公共数据集上达到了Peak signal-to-noise-ratio（PSNR）47.89、49.00和45.32，并在NASA提供的一个专用数据集上达到了相当的比较结果。<details>
<summary>Abstract</summary>
The rapid accessibility of portable and affordable retinal imaging devices has made early differential diagnosis easier. For example, color funduscopy imaging is readily available in remote villages, which can help to identify diseases like age-related macular degeneration (AMD), glaucoma, or pathological myopia (PM). On the other hand, astronauts at the International Space Station utilize this camera for identifying spaceflight-associated neuro-ocular syndrome (SANS). However, due to the unavailability of experts in these locations, the data has to be transferred to an urban healthcare facility (AMD and glaucoma) or a terrestrial station (e.g, SANS) for more precise disease identification. Moreover, due to low bandwidth limits, the imaging data has to be compressed for transfer between these two places. Different super-resolution algorithms have been proposed throughout the years to address this. Furthermore, with the advent of deep learning, the field has advanced so much that x2 and x4 compressed images can be decompressed to their original form without losing spatial information. In this paper, we introduce a novel model called Swin-FSR that utilizes Swin Transformer with spatial and depth-wise attention for fundus image super-resolution. Our architecture achieves Peak signal-to-noise-ratio (PSNR) of 47.89, 49.00 and 45.32 on three public datasets, namely iChallenge-AMD, iChallenge-PM, and G1020. Additionally, we tested the model's effectiveness on a privately held dataset for SANS provided by NASA and achieved comparable results against previous architectures.
</details>
<details>
<summary>摘要</summary>
“快速访问可携带便宜的肉眼成像设备，使得早期差异诊断变得更加容易。例如，颜色基准成像技术可以在偏远的村庄中提供，以帮助诊断年龄相关macular degeneration（AMD）、高压病（glaucoma）或 PATHOLOGICAL MYOPIA（PM）等疾病。然而，由于这些地点缺乏专业人士，因此数据必须被传输到城市医疗机构（AMD和 glaucoma）或地面站（例如，SANS）进行更加精确的疾病诊断。此外，由于带宽限制，成像数据必须进行压缩传输。过去数年，一些超分辨算法已经提出来解决这个问题。此外，随着深度学习的发展，这一领域已经进步到了非常高的水平，可以使得压缩后的成像数据被 decompress 到原始形式，而不会产生空间信息损失。本文提出了一种名为 Swin-FSR 的新模型，该模型使用 Swin Transformer 与空间和深度宽度注意来进行肉眼成像超分辨。我们的架构实现了 Peak signal-to-noise-ratio（PSNR）的 47.89、49.00 和 45.32 在三个公共数据集上，namely iChallenge-AMD、iChallenge-PM 和 G1020。此外，我们对 NASA 提供的一个私人保留数据集进行测试，并实现了与前一代架构相当的效果。”
</details></li>
</ul>
<hr>
<h2 id="A-Hierarchical-Descriptor-Framework-for-On-the-Fly-Anatomical-Location-Matching-between-Longitudinal-Studies"><a href="#A-Hierarchical-Descriptor-Framework-for-On-the-Fly-Anatomical-Location-Matching-between-Longitudinal-Studies" class="headerlink" title="A Hierarchical Descriptor Framework for On-the-Fly Anatomical Location Matching between Longitudinal Studies"></a>A Hierarchical Descriptor Framework for On-the-Fly Anatomical Location Matching between Longitudinal Studies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07337">http://arxiv.org/abs/2308.07337</a></li>
<li>repo_url: None</li>
<li>paper_authors: Halid Ziya Yerebakan, Yoshihisa Shinagawa, Mahesh Ranganath, Simon Allen-Raffl, Gerardo Hermosillo Valadez</li>
<li>for: 医疗图像 longitudinal 比较中匹配 анатомиче位置</li>
<li>methods: 使用 hierarchical sparse sampling 计算查询点描述符，然后使用 hierarchical search 找到最相似的点在目标图像中</li>
<li>results: 实现了减少计算时间至毫秒级单个CPU上，可以帮助医生在实时比较相似的 анатомиче位置而无需额外建筑或存储变换场景Is there anything else I can help you with?<details>
<summary>Abstract</summary>
We propose a method to match anatomical locations between pairs of medical images in longitudinal comparisons. The matching is made possible by computing a descriptor of the query point in a source image based on a hierarchical sparse sampling of image intensities that encode the location information. Then, a hierarchical search operation finds the corresponding point with the most similar descriptor in the target image. This simple yet powerful strategy reduces the computational time of mapping points to a millisecond scale on a single CPU. Thus, radiologists can compare similar anatomical locations in near real-time without requiring extra architectural costs for precomputing or storing deformation fields from registrations. Our algorithm does not require prior training, resampling, segmentation, or affine transformation steps. We have tested our algorithm on the recently published Deep Lesion Tracking dataset annotations. We observed more accurate matching compared to Deep Lesion Tracker while being 24 times faster than the most precise algorithm reported therein. We also investigated the matching accuracy on CT and MR modalities and compared the proposed algorithm's accuracy against ground truth consolidated from multiple radiologists.
</details>
<details>
<summary>摘要</summary>
我们提出了一种方法，用于在医疗影像对比中匹配解剖位置。该方法基于源图像中计算查询点的特征器，该特征器是基于层次稀疏抽象的图像强度值，这些值编码了位置信息。然后，使用层次搜索操作找到目标图像中最相似的点。这种简单 yet 强大的策略可以在单个 CPU 上减少比较时间到毫秒级，因此让 radiologist 可以在实时比较相似的解剖位置，无需额外的建筑成本或存储扭变场的预计算或存储。我们的算法不需要先行训练、扩充、分割或非对映变换步骤。我们在最近发布的 Deep Lesion Tracking 数据集注释中进行了测试，并观察到比 Deep Lesion Tracker 更准确的匹配，同时比最精确的算法 report 在其中的 24 倍 faster。我们还 investigate 了该算法的匹配精度在 CT 和 MR Modalities 上，并与多名医生共同协调的ground truth进行比较。
</details></li>
</ul>
<hr>
<h2 id="FunnyBirds-A-Synthetic-Vision-Dataset-for-a-Part-Based-Analysis-of-Explainable-AI-Methods"><a href="#FunnyBirds-A-Synthetic-Vision-Dataset-for-a-Part-Based-Analysis-of-Explainable-AI-Methods" class="headerlink" title="FunnyBirds: A Synthetic Vision Dataset for a Part-Based Analysis of Explainable AI Methods"></a>FunnyBirds: A Synthetic Vision Dataset for a Part-Based Analysis of Explainable AI Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06248">http://arxiv.org/abs/2308.06248</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/visinf/funnybirds">https://github.com/visinf/funnybirds</a></li>
<li>paper_authors: Robin Hesse, Simone Schaub-Meyer, Stefan Roth</li>
<li>for: 这个论文的目的是解释人工智能（XAI）领域中复杂的深度神经网络模型的内部工作方式。</li>
<li>methods: 这篇论文使用了一种新的Synthetic vision dataset，叫做FunnyBirds，以及一系列自动评估协议来解决XAI中缺乏ground truth解释的挑战。</li>
<li>results: 通过使用FunnyBirds dataset和自动评估协议，这篇论文报告了24种不同的神经网络模型和XAI方法的结果，并证明了这些方法在一种完全自动和系统的方式下的优缺点。<details>
<summary>Abstract</summary>
The field of explainable artificial intelligence (XAI) aims to uncover the inner workings of complex deep neural models. While being crucial for safety-critical domains, XAI inherently lacks ground-truth explanations, making its automatic evaluation an unsolved problem. We address this challenge by proposing a novel synthetic vision dataset, named FunnyBirds, and accompanying automatic evaluation protocols. Our dataset allows performing semantically meaningful image interventions, e.g., removing individual object parts, which has three important implications. First, it enables analyzing explanations on a part level, which is closer to human comprehension than existing methods that evaluate on a pixel level. Second, by comparing the model output for inputs with removed parts, we can estimate ground-truth part importances that should be reflected in the explanations. Third, by mapping individual explanations into a common space of part importances, we can analyze a variety of different explanation types in a single common framework. Using our tools, we report results for 24 different combinations of neural models and XAI methods, demonstrating the strengths and weaknesses of the assessed methods in a fully automatic and systematic manner.
</details>
<details>
<summary>摘要</summary>
field of explainable artificial intelligence (XAI) 目的是暴露复杂深度神经网络模型的内部工作原理。而这种技术在安全关键领域非常重要，但XAI本身缺乏真实的解释，这使得自动评估成为一个未解决的问题。我们解决这个挑战 by proposing a novel synthetic vision dataset， named FunnyBirds，以及相应的自动评估协议。我们的数据集允许执行Semantically meaningful image interventions，例如 removing individual object parts，这有三个重要的后果。首先，它允许分析解释的部级划分，这更加接近人类的理解，而不是现有的方法，它们会评估像素级划分。其次，通过比较模型输出的各个部分输入，我们可以估算出各个部分的真实重要性，这些重要性应该反映在解释中。最后，我们可以将各种不同类型的解释映射到一个共同的部分重要性空间中，以便分析多种不同的解释类型在单一的框架中。使用我们的工具，我们报告了24种不同的神经网络模型和XAI方法的结果，这些结果 demonstrate了评估方法的优劣点在一个完全自动和系统的方式上。
</details></li>
</ul>
<hr>
<h2 id="Continual-Face-Forgery-Detection-via-Historical-Distribution-Preserving"><a href="#Continual-Face-Forgery-Detection-via-Historical-Distribution-Preserving" class="headerlink" title="Continual Face Forgery Detection via Historical Distribution Preserving"></a>Continual Face Forgery Detection via Historical Distribution Preserving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06217">http://arxiv.org/abs/2308.06217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ke Sun, Shen Chen, Taiping Yao, Xiaoshuai Sun, Shouhong Ding, Rongrong Ji</li>
<li>for: 防止面部伪造攻击的安全威胁</li>
<li>methods: 使用普遍攻击伪造模型、知识传递和历史分布保持等方法</li>
<li>results: 比前一代方法高效地检测新的伪造攻击，并维持了面部伪造 distribuition的稳定性<details>
<summary>Abstract</summary>
Face forgery techniques have advanced rapidly and pose serious security threats. Existing face forgery detection methods try to learn generalizable features, but they still fall short of practical application. Additionally, finetuning these methods on historical training data is resource-intensive in terms of time and storage. In this paper, we focus on a novel and challenging problem: Continual Face Forgery Detection (CFFD), which aims to efficiently learn from new forgery attacks without forgetting previous ones. Specifically, we propose a Historical Distribution Preserving (HDP) framework that reserves and preserves the distributions of historical faces. To achieve this, we use universal adversarial perturbation (UAP) to simulate historical forgery distribution, and knowledge distillation to maintain the distribution variation of real faces across different models. We also construct a new benchmark for CFFD with three evaluation protocols. Our extensive experiments on the benchmarks show that our method outperforms the state-of-the-art competitors.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Face forgery techniques have advanced rapidly and pose serious security threats. Existing face forgery detection methods try to learn generalizable features, but they still fall short of practical application. Additionally, finetuning these methods on historical training data is resource-intensive in terms of time and storage. In this paper, we focus on a novel and challenging problem: Continual Face Forgery Detection (CFFD), which aims to efficiently learn from new forgery attacks without forgetting previous ones. Specifically, we propose a Historical Distribution Preserving (HDP) framework that reserves and preserves the distributions of historical faces. To achieve this, we use universal adversarial perturbation (UAP) to simulate historical forgery distribution, and knowledge distillation to maintain the distribution variation of real faces across different models. We also construct a new benchmark for CFFD with three evaluation protocols. Our extensive experiments on the benchmarks show that our method outperforms the state-of-the-art competitors."中文简体版：现代面孔伪造技术得到了快速发展，对安全提供了严重的威胁。现有的面孔伪造检测方法尝试学习通用特征，但 ainda fall short of practical application。此外，在历史训练数据上进行finetuning这些方法是费时和占用存储空间的。在这篇论文中，我们关注一个新和挑战的问题： continual face forgery detection（CFFD），该问题的目标是高效地从新的伪造攻击中学习，而不是忘记之前的。我们提出了一个历史分布保持（HDP）框架，该框架保留和保持历史面孔的分布。为了实现这一目标，我们使用通用对抗扰动（UAP）来模拟历史伪造分布，并使用知识蒸馏来保持实际面孔的分布变化。我们还建立了一个新的CFFD数据集和三个评估协议。我们的广泛实验表明，我们的方法在CFFD中表现出了优于当前竞争者。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/12/cs.CV_2023_08_12/" data-id="clpxp03yq00i1fm882kku0bbe" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_08_12" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/12/cs.AI_2023_08_12/" class="article-date">
  <time datetime="2023-08-12T12:00:00.000Z" itemprop="datePublished">2023-08-12</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/12/cs.AI_2023_08_12/">cs.AI - 2023-08-12</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="VisIT-Bench-A-Benchmark-for-Vision-Language-Instruction-Following-Inspired-by-Real-World-Use"><a href="#VisIT-Bench-A-Benchmark-for-Vision-Language-Instruction-Following-Inspired-by-Real-World-Use" class="headerlink" title="VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use"></a>VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06595">http://arxiv.org/abs/2308.06595</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yonatan Bitton, Hritik Bansal, Jack Hessel, Rulin Shao, Wanrong Zhu, Anas Awadalla, Josh Gardner, Rohan Taori, Ludwig Schimdt</li>
<li>for: 评估视觉语言模型在真实世界中的 instrucion-following 能力（evaluate vision-language models’ ability to follow instructions in real-world scenarios）</li>
<li>methods: 使用 70 个 ‘instruction families’ 和 592 个测试查询（use 70 instruction families and 592 test queries），包括从基本认知到游戏和创意生成等多种任务（including tasks such as basic recognition, game playing, and creative generation）</li>
<li>results: 使用人工和自动评估方法，发现现有模型与参考模型之间的质量差距 relativelly large（using both human and automatic evaluation methods, the quality gap between existing models and reference models is relatively large），提供了一个动态参与的项目，让实验室和研究人员可以简单地在项目网站上提交自己的模型答案（providing a dynamic project that allows researchers and practitioners to simply submit their model’s responses on the project website）<details>
<summary>Abstract</summary>
We introduce VisIT-Bench (Visual InsTruction Benchmark), a benchmark for evaluation of instruction-following vision-language models for real-world use. Our starting point is curating 70 'instruction families' that we envision instruction tuned vision-language models should be able to address. Extending beyond evaluations like VQAv2 and COCO, tasks range from basic recognition to game playing and creative generation. Following curation, our dataset comprises 592 test queries, each with a human-authored instruction-conditioned caption. These descriptions surface instruction-specific factors, e.g., for an instruction asking about the accessibility of a storefront for wheelchair users, the instruction-conditioned caption describes ramps/potential obstacles. These descriptions enable 1) collecting human-verified reference outputs for each instance; and 2) automatic evaluation of candidate multimodal generations using a text-only LLM, aligning with human judgment. We quantify quality gaps between models and references using both human and automatic evaluations; e.g., the top-performing instruction-following model wins against the GPT-4 reference in just 27% of the comparison. VisIT-Bench is dynamic to participate, practitioners simply submit their model's response on the project website; Data, code and leaderboard is available at visit-bench.github.io.
</details>
<details>
<summary>摘要</summary>
我们介绍VisIT-Bench（视觉指令比赛），一个用于评估视觉语言模型的实际应用场景的 benchmark。我们开始于精心选择70个“指令家庭”，我们认为视觉语言模型应该能够解决这些指令。我们的数据集包括592个测试查询，每个查询都有一个人工生成的指令条件描述。这些描述包括指令特有的因素，例如一个指令要求关于轮椅用户是否可以进入商店的访问性，描述了斜坡/潜在障碍物。这些描述允许我们收集人工验证的参考输出 для每个实例，并使用文本 только LLM 自动评估候选的多Modal生成。我们使用人工和自动评估来衡量模型和参考之间的质量差距，例如，最高级别的指令遵循模型只在与 GPT-4 参考的比赛中赢得27%。VisIT-Bench 是开放的，参与者可以在项目网站上提交他们的模型的回答。数据、代码和排名信息可以在 visit-bench.github.io 上获得。
</details></li>
</ul>
<hr>
<h2 id="Value-Distributional-Model-Based-Reinforcement-Learning"><a href="#Value-Distributional-Model-Based-Reinforcement-Learning" class="headerlink" title="Value-Distributional Model-Based Reinforcement Learning"></a>Value-Distributional Model-Based Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06590">http://arxiv.org/abs/2308.06590</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/djdprogramming/adfa2">https://github.com/djdprogramming/adfa2</a></li>
<li>paper_authors: Carlos E. Luis, Alessandro G. Bottero, Julia Vinogradska, Felix Berkenkamp, Jan Peters</li>
<li>for: 这个论文目的是为了解决sequential decision-making任务中的uncertainty quantification问题。</li>
<li>methods: 这个论文使用了model-based Bayesian reinforcement learning的方法，其中的目标是学习Markov决策过程中参数不确定性induced的 posterior distribution over value functions。</li>
<li>results: 论文的实验表明，EQR算法可以在 continuous-control tasks 中比Established model-based和model-free算法表现出性能优势。<details>
<summary>Abstract</summary>
Quantifying uncertainty about a policy's long-term performance is important to solve sequential decision-making tasks. We study the problem from a model-based Bayesian reinforcement learning perspective, where the goal is to learn the posterior distribution over value functions induced by parameter (epistemic) uncertainty of the Markov decision process. Previous work restricts the analysis to a few moments of the distribution over values or imposes a particular distribution shape, e.g., Gaussians. Inspired by distributional reinforcement learning, we introduce a Bellman operator whose fixed-point is the value distribution function. Based on our theory, we propose Epistemic Quantile-Regression (EQR), a model-based algorithm that learns a value distribution function that can be used for policy optimization. Evaluation across several continuous-control tasks shows performance benefits with respect to established model-based and model-free algorithms.
</details>
<details>
<summary>摘要</summary>
<<SYS>>量化政策长期表现的不确定性是解决sequential decision-making任务的重要问题。我们从model-based Bayesian reinforcement learning的视角 изуча这个问题，目标是学习Markov决策过程中参数（эпистемиче）不确定性引起的 posterior distribution over value functions。先前的工作只考虑了这些分布的一些瞬间或假设了特定的分布形式，例如 Gaussian。 inspirited by distributional reinforcement learning, we introduce a Bellman operator whose fixed-point is the value distribution function。 Based on our theory, we propose Epistemic Quantile-Regression (EQR), a model-based algorithm that learns a value distribution function that can be used for policy optimization. 评估在多个连续控制任务上表现出与已有的model-based和model-free算法相比的性能优势。Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Approximate-Answering-of-Graph-Queries"><a href="#Approximate-Answering-of-Graph-Queries" class="headerlink" title="Approximate Answering of Graph Queries"></a>Approximate Answering of Graph Queries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06585">http://arxiv.org/abs/2308.06585</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Cochez, Dimitrios Alivanistos, Erik Arakelyan, Max Berrendorf, Daniel Daza, Mikhail Galkin, Pasquale Minervini, Mathias Niepert, Hongyu Ren</li>
<li>for: 本文旨在介绍几种方法，以帮助回答含有不完整信息的知识图（KG）中的查询。</li>
<li>methods: 本文提出了多种方法，包括基于预测、基于潜在相似性、基于证据等方法，以满足不同类型的查询需求。</li>
<li>results: 这些方法可以帮助解决各种查询问题，如答案推断、 Entity Disambiguation、 Relation extraction 等。但是，这些方法受到图数据不完整和不准确的限制。<details>
<summary>Abstract</summary>
Knowledge graphs (KGs) are inherently incomplete because of incomplete world knowledge and bias in what is the input to the KG. Additionally, world knowledge constantly expands and evolves, making existing facts deprecated or introducing new ones. However, we would still want to be able to answer queries as if the graph were complete. In this chapter, we will give an overview of several methods which have been proposed to answer queries in such a setting. We will first provide an overview of the different query types which can be supported by these methods and datasets typically used for evaluation, as well as an insight into their limitations. Then, we give an overview of the different approaches and describe them in terms of expressiveness, supported graph types, and inference capabilities.
</details>
<details>
<summary>摘要</summary>
知识图（KG）自然而然地是不完整的，因为世界知识的不完整和输入KG中的偏见。此外，世界知识不断扩展和发展，使现有的事实过时或引入新的事实。然而，我们仍然希望能够回答问题，作为如果图完整一样。在这章中，我们将给出不同类型的查询支持的方法的概述，以及通常用于评估的数据集，以及这些方法的局限性。然后，我们将对不同的方法进行描述，包括表达力、支持的图类型和推理能力。
</details></li>
</ul>
<hr>
<h2 id="4DRVO-Net-Deep-4D-Radar-Visual-Odometry-Using-Multi-Modal-and-Multi-Scale-Adaptive-Fusion"><a href="#4DRVO-Net-Deep-4D-Radar-Visual-Odometry-Using-Multi-Modal-and-Multi-Scale-Adaptive-Fusion" class="headerlink" title="4DRVO-Net: Deep 4D Radar-Visual Odometry Using Multi-Modal and Multi-Scale Adaptive Fusion"></a>4DRVO-Net: Deep 4D Radar-Visual Odometry Using Multi-Modal and Multi-Scale Adaptive Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06573">http://arxiv.org/abs/2308.06573</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guirong Zhuo, Shouyi Lu, Huanyu Zhou, Lianqing Zheng, Lu Xiong<br>for:* 4D radar–visual odometry (4DRVO) is an attractive solution for achieving accurate and robust pose estimation by integrating complementary information from 4D radar and cameras.methods:* 4DRVO-Net leverages a feature pyramid, pose warping, and cost volume (PWC) network architecture to progressively estimate and refine poses, with a multi-scale feature extraction network called Radar-PointNet++ that fully considers rich 4D radar point information.* An adaptive 4D radar–camera fusion module (A-RCFM) is designed to automatically select image features based on 4D radar point features, facilitating multi-scale cross-modal feature interaction and adaptive multi-modal feature fusion.results:* Our method outperforms all learning-based and geometry-based methods for most sequences in the VoD dataset, and has exhibited promising performance that closely approaches that of the 64-line LiDAR odometry results of A-LOAM without mapping optimization.<details>
<summary>Abstract</summary>
Four-dimensional (4D) radar--visual odometry (4DRVO) integrates complementary information from 4D radar and cameras, making it an attractive solution for achieving accurate and robust pose estimation. However, 4DRVO may exhibit significant tracking errors owing to three main factors: 1) sparsity of 4D radar point clouds; 2) inaccurate data association and insufficient feature interaction between the 4D radar and camera; and 3) disturbances caused by dynamic objects in the environment, affecting odometry estimation. In this paper, we present 4DRVO-Net, which is a method for 4D radar--visual odometry. This method leverages the feature pyramid, pose warping, and cost volume (PWC) network architecture to progressively estimate and refine poses. Specifically, we propose a multi-scale feature extraction network called Radar-PointNet++ that fully considers rich 4D radar point information, enabling fine-grained learning for sparse 4D radar point clouds. To effectively integrate the two modalities, we design an adaptive 4D radar--camera fusion module (A-RCFM) that automatically selects image features based on 4D radar point features, facilitating multi-scale cross-modal feature interaction and adaptive multi-modal feature fusion. In addition, we introduce a velocity-guided point-confidence estimation module to measure local motion patterns, reduce the influence of dynamic objects and outliers, and provide continuous updates during pose refinement. We demonstrate the excellent performance of our method and the effectiveness of each module design on both the VoD and in-house datasets. Our method outperforms all learning-based and geometry-based methods for most sequences in the VoD dataset. Furthermore, it has exhibited promising performance that closely approaches that of the 64-line LiDAR odometry results of A-LOAM without mapping optimization.
</details>
<details>
<summary>摘要</summary>
四维度（4D）雷达--视觉协调（4DRVO）结合了不同信息，使得它成为了精度和可靠性很高的pose estimation的有力解决方案。然而，4DRVO可能会出现严重的跟踪错误，这些错误主要来自于以下三个原因：1）4D雷达点云稀疏; 2）摄像头和雷达数据的不准确相关和不足的特征互动; 3）环境中的动态对象的干扰，影响 pose estimation。在这篇文章中，我们提出了4DRVO-Net，这是一种4D雷达--视觉协调方法。这种方法利用了特征层、pose扭曲和成本量网络架构，逐步估算和精化pose。我们提出了一种多尺度特征提取网络，叫做Radar-PointNet++,该网络可以全面考虑4D雷达点云的丰富信息，以便细化学习稀疏4D雷达点云。为了有效地结合两种模式，我们设计了自适应4D雷达--摄像头融合模块（A-RCFM），该模块可以根据4D雷达点云特征自动选择摄像头特征，实现了多尺度交互和自适应多模式特征融合。此外，我们引入了速度导向点信任度估计模块，可以测量本地运动趋势，减少动态对象和异常点的影响，并在pose精化过程中提供连续更新。我们在VoD和自有 dataset上展示了我们的方法的优秀性和每个模块设计的有效性。我们的方法在大多数序列上超过了所有学习基于和几何基于的方法，并且在64行LiDAR odometry结果的A-LOAM不需要地图优化的情况下，表现出了可观的表现。
</details></li>
</ul>
<hr>
<h2 id="ModelScope-Text-to-Video-Technical-Report"><a href="#ModelScope-Text-to-Video-Technical-Report" class="headerlink" title="ModelScope Text-to-Video Technical Report"></a>ModelScope Text-to-Video Technical Report</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06571">http://arxiv.org/abs/2308.06571</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, Shiwei Zhang</li>
<li>for: 这个论文旨在描述一种基于文本-图像合成模型（即Stable Diffusion）的文本-视频合成模型（ModelScopeT2V）。</li>
<li>methods: 该模型采用了空间-时间块来保证渠道生成顺序和运动过渡的一致性，并且可以在训练和推理阶段适应不同的帧数。模型包括三个组件（即VQGAN、文本编码器和杂噪UNet），总共含1.7亿个参数，其中0.5亿个参数专门用于时间能力。</li>
<li>results: 模型在三个评价指标上表现出优于当前状态艺术方法。代码和在线demo可以在\url{<a target="_blank" rel="noopener" href="https://modelscope.cn/models/damo/text-to-video-synthesis/summary%7D%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://modelscope.cn/models/damo/text-to-video-synthesis/summary}中找到。</a><details>
<summary>Abstract</summary>
This paper introduces ModelScopeT2V, a text-to-video synthesis model that evolves from a text-to-image synthesis model (i.e., Stable Diffusion). ModelScopeT2V incorporates spatio-temporal blocks to ensure consistent frame generation and smooth movement transitions. The model could adapt to varying frame numbers during training and inference, rendering it suitable for both image-text and video-text datasets. ModelScopeT2V brings together three components (i.e., VQGAN, a text encoder, and a denoising UNet), totally comprising 1.7 billion parameters, in which 0.5 billion parameters are dedicated to temporal capabilities. The model demonstrates superior performance over state-of-the-art methods across three evaluation metrics. The code and an online demo are available at \url{https://modelscope.cn/models/damo/text-to-video-synthesis/summary}.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了ModelScopeT2V，一种文本到视频合成模型，它从文本到图像合成模型（即稳定扩散）中演化出来。ModelScopeT2V包含空间-时间块来保证 Frame 生成的一致性和平滑的运动过渡。模型可以在训练和推理过程中适应不同的帧数，因此适用于图像-文本和视频-文本数据集。ModelScopeT2V由三个组件（即 VQGAN、文本编码器和杂净 UNet）组成，总共含有1.7亿参数，其中0.5亿参数专门用于时间能力。模型在三个评价指标上表现出色，超过了当前最佳方法。代码和在线示例可以在 \url{https://modelscope.cn/models/damo/text-to-video-synthesis/summary} 上获取。
</details></li>
</ul>
<hr>
<h2 id="MC-DRE-Multi-Aspect-Cross-Integration-for-Drug-Event-Entity-Extraction"><a href="#MC-DRE-Multi-Aspect-Cross-Integration-for-Drug-Event-Entity-Extraction" class="headerlink" title="MC-DRE: Multi-Aspect Cross Integration for Drug Event&#x2F;Entity Extraction"></a>MC-DRE: Multi-Aspect Cross Integration for Drug Event&#x2F;Entity Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06546">http://arxiv.org/abs/2308.06546</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Yang, Soyeon Caren Han, Siqu Long, Josiah Poon, Goran Nenadic</li>
<li>For: This paper proposes a new multi-aspect cross-integration framework for drug entity&#x2F;event detection in drug-related documents.* Methods: The proposed framework uses multi-aspect encoders to describe semantic, syntactic, and medical document contextual information, and conducts cross-integration of different contextual information in three ways: key-value cross, attention cross, and feedforward cross.* Results: The proposed model outperforms all state-of-the-art (SOTA) models on two widely used tasks, flat entity detection and discontinuous event extraction.<details>
<summary>Abstract</summary>
Extracting meaningful drug-related information chunks, such as adverse drug events (ADE), is crucial for preventing morbidity and saving many lives. Most ADEs are reported via an unstructured conversation with the medical context, so applying a general entity recognition approach is not sufficient enough. In this paper, we propose a new multi-aspect cross-integration framework for drug entity/event detection by capturing and aligning different context/language/knowledge properties from drug-related documents. We first construct multi-aspect encoders to describe semantic, syntactic, and medical document contextual information by conducting those slot tagging tasks, main drug entity/event detection, part-of-speech tagging, and general medical named entity recognition. Then, each encoder conducts cross-integration with other contextual information in three ways: the key-value cross, attention cross, and feedforward cross, so the multi-encoders are integrated in depth. Our model outperforms all SOTA on two widely used tasks, flat entity detection and discontinuous event extraction.
</details>
<details>
<summary>摘要</summary>
<<SYS>>提取有用的药物相关信息块，如负面影响（ADE），对避免负担和拯救生命非常重要。大多数ADE都是通过不结构化的医疗讨论报告的方式报告的，因此使用一般的实体识别方法不够。在这篇论文中，我们提议一种新的多方面融合框架，用于药物实体/事件检测，通过捕捉和对照不同语言/知识/文档上下文的信息来描述药物相关文档。我们首先构建多方面编码器，用于描述语义、语法和医疗文档上下文信息，包括插槽标注任务、主药物实体/事件检测、语法标注和普通医学实体识别。然后，每个编码器进行了三种跨integration：键值跨、注意力跨和Feedforward跨，以融合多个上下文信息。我们的模型在两个常用任务上都超过了所有SOTA的性能。
</details></li>
</ul>
<hr>
<h2 id="Digital-elevation-model-correction-in-urban-areas-using-extreme-gradient-boosting-land-cover-and-terrain-parameters"><a href="#Digital-elevation-model-correction-in-urban-areas-using-extreme-gradient-boosting-land-cover-and-terrain-parameters" class="headerlink" title="Digital elevation model correction in urban areas using extreme gradient boosting, land cover and terrain parameters"></a>Digital elevation model correction in urban areas using extreme gradient boosting, land cover and terrain parameters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06545">http://arxiv.org/abs/2308.06545</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chukwuma Okolie, Jon Mills, Adedayo Adeleke, Julian Smit</li>
<li>For: The paper aims to enhance the accuracy of medium-resolution digital elevation models (DEMs) in urban areas, specifically in Cape Town, South Africa, for hydrological and environmental modelling.* Methods: The authors use the extreme gradient boosting (XGBoost) ensemble algorithm to correct the DEMs, with eleven predictor variables including elevation, urban footprints, slope, aspect, surface roughness, and more.* Results: The corrected DEMs achieved significant accuracy gains, with a root mean square error (RMSE) improvement of 46-53% for Copernicus DEM and 72-73% for AW3D DEM, compared to other proposed methods. These results demonstrate the potential of gradient boosted trees for enhancing DEM quality and improving hydrological modelling in urban catchments.Here is the same information in Simplified Chinese text, as requested:* For: 这个论文的目的是提高城市区域中的数字高程模型（DEM）的准确性，以便于水文和环境模型。* Methods: 作者使用极限Gradient Boosting（XGBoost）ensemble算法来修正DEM，使用的predictor变量包括高程、城市脚印、坡度、方向、表面荒凉、地形位置指数、地形荒凉指数、地形表面 текстура等 eleven个变量。* Results: 修正后的DEM实现了显著的准确性提高，比如 Copernicus DEM的RMSE提高46-53%，AW3D DEM的RMSE提高72-73%，与其他提议的方法相比。这些结果表明极限Gradient Boosting树可以提高DEM的质量，并且为城市catchments中的水文模型提供改善。<details>
<summary>Abstract</summary>
The accuracy of digital elevation models (DEMs) in urban areas is influenced by numerous factors including land cover and terrain irregularities. Moreover, building artifacts in global DEMs cause artificial blocking of surface flow pathways. This compromises their quality and adequacy for hydrological and environmental modelling in urban landscapes where precise and accurate terrain information is needed. In this study, the extreme gradient boosting (XGBoost) ensemble algorithm is adopted for enhancing the accuracy of two medium-resolution 30m DEMs over Cape Town, South Africa: Copernicus GLO-30 and ALOS World 3D (AW3D). XGBoost is a scalable, portable and versatile gradient boosting library that can solve many environmental modelling problems. The training datasets are comprised of eleven predictor variables including elevation, urban footprints, slope, aspect, surface roughness, topographic position index, terrain ruggedness index, terrain surface texture, vector roughness measure, forest cover and bare ground cover. The target variable (elevation error) was calculated with respect to highly accurate airborne LiDAR. After training and testing, the model was applied for correcting the DEMs at two implementation sites. The correction achieved significant accuracy gains which are competitive with other proposed methods. The root mean square error (RMSE) of Copernicus DEM improved by 46 to 53% while the RMSE of AW3D DEM improved by 72 to 73%. These results showcase the potential of gradient boosted trees for enhancing the quality of DEMs, and for improved hydrological modelling in urban catchments.
</details>
<details>
<summary>摘要</summary>
地数模型（DEM）在城市地区的准确性受到多种因素的影响，包括地表覆盖物和地形 irregularities。此外，全球 DEM 中的建筑物略导致表面流道路径的人工堵塞，从而降低其质量和适用性 для水文环境模型在城市景观中，需要精准和准确的地形信息。在这种研究中，我们采用了极限拟合搅拌（XGBoost）ensemble算法来提高两个中等分辨率 30 m DEM 的准确性，即 Copernicus GLO-30 和 ALOS World 3D（AW3D）。XGBoost 是一种可扩展、可移植和多样的拟合搅拌库，可以解决许多环境模型问题。训练数据集包括 eleven 个预测变量，包括高程、城市脚印、坡度、方向、表面粗糙度、地形坡度指数、地形表面文化、向量粗糙度度量、森林覆盖率和裸地覆盖率。target variable （高程误差）与高精度飞行 LiDAR 进行计算。之后，模型被应用于修正 DEM 的两个实施场景。修正后，DEM 的Root Mean Square Error（RMSE）提高了46%到53%，AW3D DEM 的 RMSE 提高了72%到73%。这些结果显示了拟合搅拌树的潜在可能性，以及对城市流域水文模型的改进。
</details></li>
</ul>
<hr>
<h2 id="Dealing-with-Small-Datasets-for-Deep-Learning-in-Medical-Imaging-An-Evaluation-of-Self-Supervised-Pre-Training-on-CT-Scans-Comparing-Contrastive-and-Masked-Autoencoder-Methods-for-Convolutional-Models"><a href="#Dealing-with-Small-Datasets-for-Deep-Learning-in-Medical-Imaging-An-Evaluation-of-Self-Supervised-Pre-Training-on-CT-Scans-Comparing-Contrastive-and-Masked-Autoencoder-Methods-for-Convolutional-Models" class="headerlink" title="Dealing with Small Datasets for Deep Learning in Medical Imaging: An Evaluation of Self-Supervised Pre-Training on CT Scans Comparing Contrastive and Masked Autoencoder Methods for Convolutional Models"></a>Dealing with Small Datasets for Deep Learning in Medical Imaging: An Evaluation of Self-Supervised Pre-Training on CT Scans Comparing Contrastive and Masked Autoencoder Methods for Convolutional Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06534">http://arxiv.org/abs/2308.06534</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wolfda95/ssl-medicalimagining-cl-mae">https://github.com/wolfda95/ssl-medicalimagining-cl-mae</a></li>
<li>paper_authors: Daniel Wolf, Tristan Payer, Catharina Silvia Lisson, Christoph Gerhard Lisson, Meinrad Beer, Timo Ropinski, Michael Götz</li>
<li>for: 这篇论文旨在探讨deep learning在医疗影像领域中的应用，以减少诊断错误、轻量化医生工作负担，并加快诊断。</li>
<li>methods: 这篇论文使用了自动标注学习方法，包括对大量无标注影像进行自动标注。</li>
<li>results: 研究发现，使用SparK预训方法可以更好地适应小型标注数据，并且在诊断任务中表现更好。<details>
<summary>Abstract</summary>
Deep learning in medical imaging has the potential to minimize the risk of diagnostic errors, reduce radiologist workload, and accelerate diagnosis. Training such deep learning models requires large and accurate datasets, with annotations for all training samples. However, in the medical imaging domain, annotated datasets for specific tasks are often small due to the high complexity of annotations, limited access, or the rarity of diseases. To address this challenge, deep learning models can be pre-trained on large image datasets without annotations using methods from the field of self-supervised learning. After pre-training, small annotated datasets are sufficient to fine-tune the models for a specific task. The most popular self-supervised pre-training approaches in medical imaging are based on contrastive learning. However, recent studies in natural image processing indicate a strong potential for masked autoencoder approaches. Our work compares state-of-the-art contrastive learning methods with the recently introduced masked autoencoder approach "SparK" for convolutional neural networks (CNNs) on medical images. Therefore we pre-train on a large unannotated CT image dataset and fine-tune on several CT classification tasks. Due to the challenge of obtaining sufficient annotated training data in medical imaging, it is of particular interest to evaluate how the self-supervised pre-training methods perform when fine-tuning on small datasets. By experimenting with gradually reducing the training dataset size for fine-tuning, we find that the reduction has different effects depending on the type of pre-training chosen. The SparK pre-training method is more robust to the training dataset size than the contrastive methods. Based on our results, we propose the SparK pre-training for medical imaging tasks with only small annotated datasets.
</details>
<details>
<summary>摘要</summary>
深度学习在医疗影像领域可能减少诊断错误风险，减轻放射学家的工作负担，并加速诊断。深度学习模型的训练需要大量和准确的数据集，并将所有训练样本标注。然而，在医疗影像领域，特定任务的标注数据集经常很小，这可能由标注的复杂性、访问限制或疾病的罕见性引起。为解决这个挑战，可以使用自动标注学习的方法进行深度学习模型的预训练。在预训练后，只需要小量的标注数据集来精度地调整模型 для特定任务。医疗影像领域最受欢迎的自动标注预训练方法是对比学习。然而，最近的自然图像处理研究表明，遮盲 autoencoder 方法有很强的潜在性。我们的工作比较了当前状态的对比学习方法和新引入的遮盲 autoencoder 方法 "SparK" 在医疗影像中的 convolutional neural networks (CNNs) 上。因此，我们预训练在大量无注释 CT 图像数据集上，并在多个 CT 分类任务上进行精度调整。由于医疗影像领域获得足够的注释训练数据是困难的，因此特别关心自动标注预训练方法在小型注释数据集上的性能。通过逐渐减少 fine-tuning 数据集大小的实验，我们发现降低的效果与预训练方法的类型有很大的差异。SparK 预训练方法在训练数据集尺寸减少后表现更加稳定。根据我们的结果，我们建议使用 SparK 预训练方法进行医疗影像任务，只需要小量的注释训练数据。
</details></li>
</ul>
<hr>
<h2 id="Learning-Abstract-Visual-Reasoning-via-Task-Decomposition-A-Case-Study-in-Raven-Progressive-Matrices"><a href="#Learning-Abstract-Visual-Reasoning-via-Task-Decomposition-A-Case-Study-in-Raven-Progressive-Matrices" class="headerlink" title="Learning Abstract Visual Reasoning via Task Decomposition: A Case Study in Raven Progressive Matrices"></a>Learning Abstract Visual Reasoning via Task Decomposition: A Case Study in Raven Progressive Matrices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06528">http://arxiv.org/abs/2308.06528</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jakubkwiatkowski/abstract_compositional_transformer">https://github.com/jakubkwiatkowski/abstract_compositional_transformer</a></li>
<li>paper_authors: Jakub Kwiatkowski, Krzysztof Krawiec</li>
<li>for: The paper aims to improve the performance of solving Raven Progressive Matrices (RPM) tasks using deep learning.</li>
<li>methods: The proposed method uses a transformer-based architecture to predict the visual properties of individual objects and their arrangements, rather than directly choosing the answer. The model parses the visual input into tokens and is trained using self-supervised methods with various masking regimes.</li>
<li>results: The proposed method outperforms state-of-the-art methods and provides interesting insights and partial explanations about the inference. Additionally, the design of the method is immune to biases that exist in some RPM benchmarks.Here’s the simplified Chinese text for the three key points:</li>
<li>for: 这篇论文目的是使用深度学习方法改进解决Raven Progressive Matrices (RPM)任务。</li>
<li>methods: 提议的方法使用 transformer 架构，而不是直接选择答案，而是预测图像中对象的视觉属性和排列。模型将视觉输入解析成 токен，并使用自我超vised 训练方法，包括不同的掩蔽方式。</li>
<li>results: 提议的方法不仅超越了当前的方法，还提供了有趣的解释和偏好。此外，方法的设计也免备了一些 RPM 数据集中的偏见。<details>
<summary>Abstract</summary>
One of the challenges in learning to perform abstract reasoning is that problems are often posed as monolithic tasks, with no intermediate subgoals. In Raven Progressive Matrices (RPM), the task is to choose one of the available answers given a context, where both contexts and answers are composite images featuring multiple objects in various spatial arrangements. As this high-level goal is the only guidance available, learning is challenging and most contemporary solvers tend to be opaque. In this study, we propose a deep learning architecture based on the transformer blueprint which, rather than directly making the above choice, predicts the visual properties of individual objects and their arrangements. The multidimensional predictions obtained in this way are then directly juxtaposed to choose the answer. We consider a few ways in which the model parses the visual input into tokens and several regimes of masking parts of the input in self-supervised training. In experimental assessment, the models not only outperform state-of-the-art methods but also provide interesting insights and partial explanations about the inference. The design of the method also makes it immune to biases that are known to exist in some RPM benchmarks.
</details>
<details>
<summary>摘要</summary>
一个learning抽象逻辑的挑战是问题经常被提出为单一任务，没有中间目标。在Raven进步矩阵（RPM）中，任务是根据上下文选择一个可用的答案，上下文和答案都是复杂的图像组合，包括多个物体在不同的空间排列。由于这个高级目标是唯一的指导，学习是困难的，大多数当代解决方案都是透明的。在这项研究中，我们提议一种基于转换器蓝图的深度学习架构，而不是直接选择上述选择，而是预测图像中对象的视觉属性和排列。得到的多维预测可以直接相互对比，从而选择答案。我们考虑了一些将视觉输入分解成токен的方法，以及在自然supervised训练中隐藏部分输入的方法。在实验评估中，模型不仅超越了当前的方法，还提供了有趣的结论和部分解释，关于推理过程。此外，方法的设计还使其免受一些RPMbenchmark中已知的偏见。
</details></li>
</ul>
<hr>
<h2 id="SLoRA-Federated-Parameter-Efficient-Fine-Tuning-of-Language-Models"><a href="#SLoRA-Federated-Parameter-Efficient-Fine-Tuning-of-Language-Models" class="headerlink" title="SLoRA: Federated Parameter Efficient Fine-Tuning of Language Models"></a>SLoRA: Federated Parameter Efficient Fine-Tuning of Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06522">http://arxiv.org/abs/2308.06522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sara Babakniya, Ahmed Roushdy Elkordy, Yahya H. Ezzeldin, Qingfeng Liu, Kee-Bong Song, Mostafa El-Khamy, Salman Avestimehr</li>
<li>for: 这篇论文目的是探讨在 Federated Learning（FL）中使用已经预训练的 transformer 模型进行调整，以获得最佳的语言任务结果。</li>
<li>methods: 这篇论文使用的方法包括 parameter efficient fine-tuning（PEFT）和一个名为 SLoRA 的新方法，用于在高度多标的数据情况下bridge the performance gap between PEFT 和全部调整。</li>
<li>results: 实验结果显示，SLoRA 可以 дости持比 full fine-tuning 相似的性能，并在大约 $\sim 1%$ 的稀疏更新下实现大约 $90%$ 的训练时间减少。<details>
<summary>Abstract</summary>
Transfer learning via fine-tuning pre-trained transformer models has gained significant success in delivering state-of-the-art results across various NLP tasks. In the absence of centralized data, Federated Learning (FL) can benefit from distributed and private data of the FL edge clients for fine-tuning. However, due to the limited communication, computation, and storage capabilities of edge devices and the huge sizes of popular transformer models, efficient fine-tuning is crucial to make federated training feasible. This work explores the opportunities and challenges associated with applying parameter efficient fine-tuning (PEFT) methods in different FL settings for language tasks. Specifically, our investigation reveals that as the data across users becomes more diverse, the gap between fully fine-tuning the model and employing PEFT methods widens. To bridge this performance gap, we propose a method called SLoRA, which overcomes the key limitations of LoRA in high heterogeneous data scenarios through a novel data-driven initialization technique. Our experimental results demonstrate that SLoRA achieves performance comparable to full fine-tuning, with significant sparse updates with approximately $\sim 1\%$ density while reducing training time by up to $90\%$.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate the following text into Simplified Chinese: Transfer learning via fine-tuning pre-trained transformer models has gained significant success in delivering state-of-the-art results across various NLP tasks. In the absence of centralized data, Federated Learning (FL) can benefit from distributed and private data of the FL edge clients for fine-tuning. However, due to the limited communication, computation, and storage capabilities of edge devices and the huge sizes of popular transformer models, efficient fine-tuning is crucial to make federated training feasible. This work explores the opportunities and challenges associated with applying parameter efficient fine-tuning (PEFT) methods in different FL settings for language tasks. Specifically, our investigation reveals that as the data across users becomes more diverse, the gap between fully fine-tuning the model and employing PEFT methods widens. To bridge this performance gap, we propose a method called SLoRA, which overcomes the key limitations of LoRA in high heterogeneous data scenarios through a novel data-driven initialization technique. Our experimental results demonstrate that SLoRA achieves performance comparable to full fine-tuning, with significant sparse updates with approximately $\sim 1\%$ density while reducing training time by up to $90\%$.Transfer learning via fine-tuning pre-trained transformer models 在各种 NLP 任务中取得了很大的成功，但在没有中央数据的情况下，Federated Learning (FL) 可以利用分布式和私有的 FL 边缘客户端数据进行 fine-tuning。然而，由于边缘设备的限制性，包括通信、计算和存储能力，以及流行的 transformer 模型的巨大大小，fficient fine-tuning 是使 federated 训练可行的关键。这个工作探讨了在不同的 FL 设置下，用于语言任务的 PEFT 方法所面临的机会和挑战。我们的调查发现，随着用户数据的多样化，完全 fine-tuning 和 PEFT 方法之间的性能差距逐渐扩大。为了弥补这个性能差距，我们提议一种名为 SLoRA 的方法，通过一种新的数据驱动初始化技术，超越 LoRA 在高多样性数据场景中的关键局限性。我们的实验结果表明，SLoRA 可以与完全 fine-tuning 相比，在 $\sim 1\%$ 杂点上实现相似的性能，同时减少训练时间达到 $90\%$。
</details></li>
</ul>
<hr>
<h2 id="One-bit-Flip-is-All-You-Need-When-Bit-flip-Attack-Meets-Model-Training"><a href="#One-bit-Flip-is-All-You-Need-When-Bit-flip-Attack-Meets-Model-Training" class="headerlink" title="One-bit Flip is All You Need: When Bit-flip Attack Meets Model Training"></a>One-bit Flip is All You Need: When Bit-flip Attack Meets Model Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07934">http://arxiv.org/abs/2308.07934</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jianshuod/tba">https://github.com/jianshuod/tba</a></li>
<li>paper_authors: Jianshuo Dong, Han Qiu, Yiming Li, Tianwei Zhang, Yuanjie Li, Zeqi Lai, Chao Zhang, Shu-Tao Xia</li>
<li>For: This paper aims to propose a training-assisted bit flip attack on deep neural networks (DNNs) to compromise their security.* Methods: The attack exploits memory fault inject techniques such as row hammer and involves the adversary in the training stage to build a high-risk model. The attack can convert the high-risk model to a malicious one on the victim’s side by flipping only one critical bit on average in the deployment stage.* Results: The attack poses a significant threat even when defenses are employed, and the adversary can easily convert the high-risk model to a malicious one by flipping only one critical bit on average.Here is the information in Simplified Chinese text:</li>
<li>for: 这篇论文目的是提出一种基于训练的位置攻击，用于攻击深度神经网络（DNNs）的安全性。</li>
<li>methods: 该攻击利用了内存错误注入技术，如行撞击，并在训练阶段由敌方参与建立高风险模型。攻击者可以在部署阶段通过只flipping一个关键位来将高风险模型转换为恶意模型。</li>
<li>results: 该攻击可以快速地转换高风险模型为恶意模型，并且对防御措施仍然构成了一定的威胁。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) are widely deployed on real-world devices. Concerns regarding their security have gained great attention from researchers. Recently, a new weight modification attack called bit flip attack (BFA) was proposed, which exploits memory fault inject techniques such as row hammer to attack quantized models in the deployment stage. With only a few bit flips, the target model can be rendered useless as a random guesser or even be implanted with malicious functionalities. In this work, we seek to further reduce the number of bit flips. We propose a training-assisted bit flip attack, in which the adversary is involved in the training stage to build a high-risk model to release. This high-risk model, obtained coupled with a corresponding malicious model, behaves normally and can escape various detection methods. The results on benchmark datasets show that an adversary can easily convert this high-risk but normal model to a malicious one on victim's side by \textbf{flipping only one critical bit} on average in the deployment stage. Moreover, our attack still poses a significant threat even when defenses are employed. The codes for reproducing main experiments are available at \url{https://github.com/jianshuod/TBA}.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="HyperFormer-Enhancing-Entity-and-Relation-Interaction-for-Hyper-Relational-Knowledge-Graph-Completion"><a href="#HyperFormer-Enhancing-Entity-and-Relation-Interaction-for-Hyper-Relational-Knowledge-Graph-Completion" class="headerlink" title="HyperFormer: Enhancing Entity and Relation Interaction for Hyper-Relational Knowledge Graph Completion"></a>HyperFormer: Enhancing Entity and Relation Interaction for Hyper-Relational Knowledge Graph Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06512">http://arxiv.org/abs/2308.06512</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhiweihu1103/hkgc-hyperformer">https://github.com/zhiweihu1103/hkgc-hyperformer</a></li>
<li>paper_authors: Zhiwei Hu, Víctor Gutiérrez-Basulto, Zhiliang Xiang, Ru Li, Jeff Z. Pan</li>
<li>for: 这个论文主要目标是完善具有 attribute-value 赋值的高级知识图（HKG），以推理未知 triple 而考虑其赋值。</li>
<li>methods: 这个论文提出了 HyperFormer 模型，该模型利用了本地级别的序列信息，包括实体、关系和赋值的内容，以提高 triple 预测的精度。模型包括三个不同模块：实体邻居聚合模块、关系赋值聚合模块和卷积推理模块。</li>
<li>results: 经过广泛的实验 validate 了 HyperFormer 模型在三个知识图 datasets 上的效果，并且在不同的条件下进行了比较。模型在实验中表现出了明显的优势。代码和数据可以在 GitHub 上找到。<details>
<summary>Abstract</summary>
Hyper-relational knowledge graphs (HKGs) extend standard knowledge graphs by associating attribute-value qualifiers to triples, which effectively represent additional fine-grained information about its associated triple. Hyper-relational knowledge graph completion (HKGC) aims at inferring unknown triples while considering its qualifiers. Most existing approaches to HKGC exploit a global-level graph structure to encode hyper-relational knowledge into the graph convolution message passing process. However, the addition of multi-hop information might bring noise into the triple prediction process. To address this problem, we propose HyperFormer, a model that considers local-level sequential information, which encodes the content of the entities, relations and qualifiers of a triple. More precisely, HyperFormer is composed of three different modules: an entity neighbor aggregator module allowing to integrate the information of the neighbors of an entity to capture different perspectives of it; a relation qualifier aggregator module to integrate hyper-relational knowledge into the corresponding relation to refine the representation of relational content; a convolution-based bidirectional interaction module based on a convolutional operation, capturing pairwise bidirectional interactions of entity-relation, entity-qualifier, and relation-qualifier. realize the depth perception of the content related to the current statement. Furthermore, we introduce a Mixture-of-Experts strategy into the feed-forward layers of HyperFormer to strengthen its representation capabilities while reducing the amount of model parameters and computation. Extensive experiments on three well-known datasets with four different conditions demonstrate HyperFormer's effectiveness. Datasets and code are available at https://github.com/zhiweihu1103/HKGC-HyperFormer.
</details>
<details>
<summary>摘要</summary>
超过标准知识 graphs (HKGs) 将 attribute-value 资讯 associates 到 triplets, 实际表示了对应 triplets 的详细信息。 hyper-relational 知识图完成 (HKGC) 目标是预测未知 triplets, 考虑其资讯。现有大多数 HKGC 方法利用全局级图结构编码 hyper-relational 知识到图 convolution 消息传递过程中。然而，添加多个跳跃信息可能会带来 triple 预测过程中的噪声。为解决这个问题，我们提出了 HyperFormer，一种模型，考虑本地级别的顺序信息，对 entitites、关系和资讯的内容进行编码。更加准确地说，HyperFormer 由三个不同模块组成：一个 entity neighbor aggregator 模块，用于将 entity 的 neighborgraph 信息集成，以 Capture 不同的 perspective of it; 一个 relation qualifier aggregator 模块，用于将 hyper-relational 知识 integrate 到对应关系中，以 Refine 关系内容的表示; 一个基于 convolution 操作的 bidirectional interaction module，用于 Capture entity-relation、entity-qualifier 和 relation-qualifier 对的 pairwise bidirectional interactions, 实现对当前声明的深度认知。此外，我们在 HyperFormer 的 feed-forward 层中引入 Mixture-of-Experts 策略，以增强其表示能力，同时减少模型参数和计算量。extensive experiments 表明 HyperFormer 有效。数据集和代码可以在 <https://github.com/zhiweihu1103/HKGC-HyperFormer> 上获取。
</details></li>
</ul>
<hr>
<h2 id="Three-Ways-of-Using-Large-Language-Models-to-Evaluate-Chat"><a href="#Three-Ways-of-Using-Large-Language-Models-to-Evaluate-Chat" class="headerlink" title="Three Ways of Using Large Language Models to Evaluate Chat"></a>Three Ways of Using Large Language Models to Evaluate Chat</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06502">http://arxiv.org/abs/2308.06502</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/oplatek/chateval-llm">https://github.com/oplatek/chateval-llm</a></li>
<li>paper_authors: Ondřej Plátek, Vojtěch Hudeček, Patricia Schmidtová, Mateusz Lango, Ondřej Dušek</li>
<li>for: 这个论文描述了由team6提交的ChatEval竞赛中的系统，包括三种基于大语言模型（LLMs）预测对话机器人回复质量的方法。</li>
<li>methods: 论文描述了三种方法，包括使用动态少量示例从矢量存储中提取提示，以及对其他两种方法的分析和未来工作的需求。</li>
<li>results: 论文报告了基于这三种方法的改进，包括使用动态少量示例从矢量存储中提取提示的改进。同时，论文还报告了其他两种方法的性能分析和未来工作的需求。<details>
<summary>Abstract</summary>
This paper describes the systems submitted by team6 for ChatEval, the DSTC 11 Track 4 competition. We present three different approaches to predicting turn-level qualities of chatbot responses based on large language models (LLMs). We report improvement over the baseline using dynamic few-shot examples from a vector store for the prompts for ChatGPT. We also analyze the performance of the other two approaches and report needed improvements for future work. We developed the three systems over just two weeks, showing the potential of LLMs for this task. An ablation study conducted after the challenge deadline shows that the new Llama 2 models are closing the performance gap between ChatGPT and open-source LLMs. However, we find that the Llama 2 models do not benefit from few-shot examples in the same way as ChatGPT.
</details>
<details>
<summary>摘要</summary>
这篇论文描述了团队6在ChatEval DSTC 11 Track 4比赛中提交的三种不同方法来预测对话机器人响应质量。我们使用大型自然语言模型（LLM）来预测对话机器人响应的每个转折质量。我们发现使用动态少量示例从向量存储中提取的Prompt对ChatGPT的性能有所提升。我们还分析了其他两种方法的性能并报告了未来工作中所需的改进。我们在只有两周时间内开发了这三种系统，这表明LLMs在这个任务中的潜力。经过比赛结束后的抽象研究发现，新的Llama 2模型在关键性能方面追近ChatGPT和开源LLMs的性能。然而，我们发现Llama 2模型不如ChatGPT那样受益于少量示例。
</details></li>
</ul>
<hr>
<h2 id="Latent-Emission-Augmented-Perspective-Taking-LEAPT-for-Human-Robot-Interaction"><a href="#Latent-Emission-Augmented-Perspective-Taking-LEAPT-for-Human-Robot-Interaction" class="headerlink" title="Latent Emission-Augmented Perspective-Taking (LEAPT) for Human-Robot Interaction"></a>Latent Emission-Augmented Perspective-Taking (LEAPT) for Human-Robot Interaction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06498">http://arxiv.org/abs/2308.06498</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaiqi Chen, Jing Yu Lim, Kingsley Kuan, Harold Soh</li>
<li>for: 本文是为了帮助机器人进行视角理解，即理解人类的视角和信念。</li>
<li>methods: 本文使用了深度世界模型，允许机器人进行视觉和概念上的视角理解，即能够推断人类看到和信任的内容。</li>
<li>results: 实验表明，本方法在三个半可见人机交互任务中表现出色，与现有的基准值进行比较，显著超越了基准值。<details>
<summary>Abstract</summary>
Perspective-taking is the ability to perceive or understand a situation or concept from another individual's point of view, and is crucial in daily human interactions. Enabling robots to perform perspective-taking remains an unsolved problem; existing approaches that use deterministic or handcrafted methods are unable to accurately account for uncertainty in partially-observable settings. This work proposes to address this limitation via a deep world model that enables a robot to perform both perception and conceptual perspective taking, i.e., the robot is able to infer what a human sees and believes. The key innovation is a decomposed multi-modal latent state space model able to generate and augment fictitious observations/emissions. Optimizing the ELBO that arises from this probabilistic graphical model enables the learning of uncertainty in latent space, which facilitates uncertainty estimation from high-dimensional observations. We tasked our model to predict human observations and beliefs on three partially-observable HRI tasks. Experiments show that our method significantly outperforms existing baselines and is able to infer visual observations available to other agent and their internal beliefs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="EgoPoser-Robust-Real-Time-Ego-Body-Pose-Estimation-in-Large-Scenes"><a href="#EgoPoser-Robust-Real-Time-Ego-Body-Pose-Estimation-in-Large-Scenes" class="headerlink" title="EgoPoser: Robust Real-Time Ego-Body Pose Estimation in Large Scenes"></a>EgoPoser: Robust Real-Time Ego-Body Pose Estimation in Large Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06493">http://arxiv.org/abs/2308.06493</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxi Jiang, Paul Streli, Manuel Meier, Christian Holz</li>
<li>for: 这篇论文旨在解决headset上的 egopose估计问题，即只使用头和手部位的位姿来估计全身姿态。</li>
<li>methods: 该论文提出了一种新的输入表示方法和一种新的运动分解方法，以估计全身姿态独立于全局位置。此外，它还能够对不同用户的体型进行robust模型。</li>
<li>results: 实验表明，该论文在质量和量化上都有较好的表现，而且可以保持高速推断速度（大于600帧&#x2F;秒）。这篇论文为将来的工作提供了一个可靠的基线，即全身姿态估计不再需要外部捕捉，并可以在大景观环境中扩展。<details>
<summary>Abstract</summary>
Full-body ego-pose estimation from head and hand poses alone has become an active area of research to power articulate avatar representation on headset-based platforms. However, existing methods over-rely on the confines of the motion-capture spaces in which datasets were recorded, while simultaneously assuming continuous capture of joint motions and uniform body dimensions. In this paper, we propose EgoPoser, which overcomes these limitations by 1) rethinking the input representation for headset-based ego-pose estimation and introducing a novel motion decomposition method that predicts full-body pose independent of global positions, 2) robustly modeling body pose from intermittent hand position and orientation tracking only when inside a headset's field of view, and 3) generalizing across various body sizes for different users. Our experiments show that EgoPoser outperforms state-of-the-art methods both qualitatively and quantitatively, while maintaining a high inference speed of over 600 fps. EgoPoser establishes a robust baseline for future work, where full-body pose estimation needs no longer rely on outside-in capture and can scale to large-scene environments.
</details>
<details>
<summary>摘要</summary>
全身ego姿 estimation从头和手姿alone已成为研究的活跃领域，以提供头盔平台上的人物表现。然而，现有方法受到数据采集空间的限制，同时假设持续采集 JOINT 动作和一致体 dimensions。在这篇论文中，我们提出了 EgoPoser，它缓解了这些限制，通过：1. 重新定义头盔基于的输入表示，并 introduce 一种新的运动分解方法，可以独立地预测全身姿。2. 可靠地模型体姿从头盔视野内部的间歇手姿和方向追踪。3. 对不同用户的体型进行一致化。我们的实验表明，EgoPoser 超过了现有方法的质量和量化表现，同时保持了高速度推断速度超过 600 fps。EgoPoser 建立了一个可靠的基线，将全身姿推断带到大景景环境中。
</details></li>
</ul>
<hr>
<h2 id="Generating-Faithful-Text-From-a-Knowledge-Graph-with-Noisy-Reference-Text"><a href="#Generating-Faithful-Text-From-a-Knowledge-Graph-with-Noisy-Reference-Text" class="headerlink" title="Generating Faithful Text From a Knowledge Graph with Noisy Reference Text"></a>Generating Faithful Text From a Knowledge Graph with Noisy Reference Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06488">http://arxiv.org/abs/2308.06488</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tahsina Hashem, Weiqing Wang, Derry Tanti Wijaya, Mohammed Eunus Ali, Yuan-Fang Li</li>
<li>for: 这个论文的目的是提出一种基于知识图（KG）的自然语言生成模型，能够生成准确表示知识图信息的自然语言文本。</li>
<li>methods: 该模型使用了对抗学习和可控文本生成技术，以提高模型对 faithful 信息的识别和控制。</li>
<li>results: 论文的实验结果表明，该模型在 faithfulness 方面表现出色，超过了现有的状态艺文。<details>
<summary>Abstract</summary>
Knowledge Graph (KG)-to-Text generation aims at generating fluent natural-language text that accurately represents the information of a given knowledge graph. While significant progress has been made in this task by exploiting the power of pre-trained language models (PLMs) with appropriate graph structure-aware modules, existing models still fall short of generating faithful text, especially when the ground-truth natural-language text contains additional information that is not present in the graph. In this paper, we develop a KG-to-text generation model that can generate faithful natural-language text from a given graph, in the presence of noisy reference text. Our framework incorporates two core ideas: Firstly, we utilize contrastive learning to enhance the model's ability to differentiate between faithful and hallucinated information in the text, thereby encouraging the decoder to generate text that aligns with the input graph. Secondly, we empower the decoder to control the level of hallucination in the generated text by employing a controllable text generation technique. We evaluate our model's performance through the standard quantitative metrics as well as a ChatGPT-based quantitative and qualitative analysis. Our evaluation demonstrates the superior performance of our model over state-of-the-art KG-to-text models on faithfulness.
</details>
<details>
<summary>摘要</summary>
知识图（KG）-to-文本生成目标是生成流畅自然语言文本，准确表达给定知识图中的信息。虽然现有模型通过利用适当的前训练语言模型（PLMs）和合适的图结构意识模块，已经取得了显著的进步，但现有模型仍然无法生成准确的文本，特别是当参考文本中含有不在知识图中的信息时。在这篇论文中，我们开发了一种KG-to-文本生成模型，可以从给定图生成准确的自然语言文本，并在参考文本中含有噪音时提供 faithful 的文本生成。我们的框架包括两个核心想法：首先，我们利用对比学习增强模型的能力，在文本中划分 faithful 和幻想信息，从而让解码器生成与输入图相关的文本。其次，我们赋予解码器控制幻想度的能力，通过使用可控文本生成技术。我们通过标准的量化度量以及基于 ChatGPT 的量化和质量分析进行评估。我们的评估结果表明，我们的模型在准确性方面与当前状态的 KG-to-文本模型相比，表现出优异的性能。
</details></li>
</ul>
<hr>
<h2 id="Not-So-Robust-After-All-Evaluating-the-Robustness-of-Deep-Neural-Networks-to-Unseen-Adversarial-Attacks"><a href="#Not-So-Robust-After-All-Evaluating-the-Robustness-of-Deep-Neural-Networks-to-Unseen-Adversarial-Attacks" class="headerlink" title="Not So Robust After All: Evaluating the Robustness of Deep Neural Networks to Unseen Adversarial Attacks"></a>Not So Robust After All: Evaluating the Robustness of Deep Neural Networks to Unseen Adversarial Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06467">http://arxiv.org/abs/2308.06467</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roman Garaev, Bader Rasheed, Adil Khan<br>for: This study aims to challenge the efficacy and generalization of contemporary defense mechanisms against adversarial attacks.methods: The study explores the hypothesis proposed by Ilyas et. al, which posits that DNN image features can be either robust or non-robust, with adversarial attacks targeting the latter. The study employs canonical correlation analysis, visualizes the representations, and calculates the mean distance between these representations and various DNN decision boundaries.results: The study finds a significant difference between $L_2$ and $L_{\infty}$ norms, which could provide insights into the potential dangers posed by $L_{\infty}$ norm attacks, previously underestimated by the research community.<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) have gained prominence in various applications, such as classification, recognition, and prediction, prompting increased scrutiny of their properties. A fundamental attribute of traditional DNNs is their vulnerability to modifications in input data, which has resulted in the investigation of adversarial attacks. These attacks manipulate the data in order to mislead a DNN. This study aims to challenge the efficacy and generalization of contemporary defense mechanisms against adversarial attacks. Specifically, we explore the hypothesis proposed by Ilyas et. al, which posits that DNN image features can be either robust or non-robust, with adversarial attacks targeting the latter. This hypothesis suggests that training a DNN on a dataset consisting solely of robust features should produce a model resistant to adversarial attacks. However, our experiments demonstrate that this is not universally true. To gain further insights into our findings, we analyze the impact of adversarial attack norms on DNN representations, focusing on samples subjected to $L_2$ and $L_{\infty}$ norm attacks. Further, we employ canonical correlation analysis, visualize the representations, and calculate the mean distance between these representations and various DNN decision boundaries. Our results reveal a significant difference between $L_2$ and $L_{\infty}$ norms, which could provide insights into the potential dangers posed by $L_{\infty}$ norm attacks, previously underestimated by the research community.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Multi-Label-Knowledge-Distillation"><a href="#Multi-Label-Knowledge-Distillation" class="headerlink" title="Multi-Label Knowledge Distillation"></a>Multi-Label Knowledge Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06453">http://arxiv.org/abs/2308.06453</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/penghui-yang/l2d">https://github.com/penghui-yang/l2d</a></li>
<li>paper_authors: Penghui Yang, Ming-Kun Xie, Chen-Chen Zong, Lei Feng, Gang Niu, Masashi Sugiyama, Sheng-Jun Huang</li>
<li>for: 这篇论文主要针对多标签学习问题，旨在提出一种基于知识储存技术的多标签知识传递方法。</li>
<li>methods: 该方法首先将多标签学习问题分解成多个二分类问题，然后通过分别对每个二分类问题进行知识储存来增强学习的特征表示。同时，该方法还利用标签嵌入结构来提高特征表示的独特性。</li>
<li>results: 实验结果表明，提出的方法可以减少标签之间的知识冲突，并且在多个 benchmark 数据集上达到了较高的性能水平，比较于其他比较方法。<details>
<summary>Abstract</summary>
Existing knowledge distillation methods typically work by imparting the knowledge of output logits or intermediate feature maps from the teacher network to the student network, which is very successful in multi-class single-label learning. However, these methods can hardly be extended to the multi-label learning scenario, where each instance is associated with multiple semantic labels, because the prediction probabilities do not sum to one and feature maps of the whole example may ignore minor classes in such a scenario. In this paper, we propose a novel multi-label knowledge distillation method. On one hand, it exploits the informative semantic knowledge from the logits by dividing the multi-label learning problem into a set of binary classification problems; on the other hand, it enhances the distinctiveness of the learned feature representations by leveraging the structural information of label-wise embeddings. Experimental results on multiple benchmark datasets validate that the proposed method can avoid knowledge counteraction among labels, thus achieving superior performance against diverse comparing methods. Our code is available at: https://github.com/penghui-yang/L2D
</details>
<details>
<summary>摘要</summary>
现有的知识传授方法通常是将教师网络的输出几何或中间特征图形知识传授到学生网络，这很成功在多类单 Label 学习中。但这些方法几乎无法扩展到多Label学习情况下，因为预测概率不会加总到一，且特征图形全例可能忽略次要类别。在本文中，我们提出了一个新的多Label知识传授方法。一方面，它利用多Label学习问题中的 semantic 知识，将问题分成多个二分类问题；另一方面，它利用类别对称信息来强化学习的特征表现。实验结果显示，提案的方法可以避免知识抵触 Label 之间，因此在多个比较方法面上获得了更好的性能。我们的代码可以在：https://github.com/penghui-yang/L2D 中找到。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Equivariant-Mixup"><a href="#Semantic-Equivariant-Mixup" class="headerlink" title="Semantic Equivariant Mixup"></a>Semantic Equivariant Mixup</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06451">http://arxiv.org/abs/2308.06451</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zongbo Han, Tianchi Xie, Bingzhe Wu, Qinghua Hu, Changqing Zhang</li>
<li>for: 提高模型对分布Shift的 Robustness，通过在表示空间强制保持输入数据的结构不变。</li>
<li>methods: 基于semantic-equivariance assumption的generic mixup regularization，使得模型在混合样本中学习更多的semantic information。</li>
<li>results: 经过extensive empirical studies和qualitative analyzes，表明提出的方法可以提高模型的Robustness和Generalization能力。<details>
<summary>Abstract</summary>
Mixup is a well-established data augmentation technique, which can extend the training distribution and regularize the neural networks by creating ''mixed'' samples based on the label-equivariance assumption, i.e., a proportional mixup of the input data results in the corresponding labels being mixed in the same proportion. However, previous mixup variants may fail to exploit the label-independent information in mixed samples during training, which usually contains richer semantic information. To further release the power of mixup, we first improve the previous label-equivariance assumption by the semantic-equivariance assumption, which states that the proportional mixup of the input data should lead to the corresponding representation being mixed in the same proportion. Then a generic mixup regularization at the representation level is proposed, which can further regularize the model with the semantic information in mixed samples. At a high level, the proposed semantic equivariant mixup (sem) encourages the structure of the input data to be preserved in the representation space, i.e., the change of input will result in the obtained representation information changing in the same way. Different from previous mixup variants, which tend to over-focus on the label-related information, the proposed method aims to preserve richer semantic information in the input with semantic-equivariance assumption, thereby improving the robustness of the model against distribution shifts. We conduct extensive empirical studies and qualitative analyzes to demonstrate the effectiveness of our proposed method. The code of the manuscript is in the supplement.
</details>
<details>
<summary>摘要</summary>
混合是一种已有的数据增强技术，可以使得训练分布延伸并规范神经网络，通过创建基于标签相似性假设的混合样本。然而，先前的混合变体可能会忽略混合样本中的标签独立信息，这些信息通常含有更加丰富的 semantics。为了更好地发挥混合的力量，我们首先提高了先前的标签相似性假设，使其转化为 semantics相似性假设，即混合输入数据时，应该对应的表示也在同样的比例进行混合。然后，我们提出了一种通用的混合规范，可以在表示层进行规范，以更加规范模型。总的来说，我们的 semantic equivariant mixup（sem）方法要求输入数据的结构在表示空间保持不变，即输入变化后，获得的表示信息也会在同样的比例进行变化。与先前的混合变体不同，我们的方法更关注于保持输入中更加丰富的 semantics信息，从而提高模型对分布偏移的Robustness。我们进行了广泛的实验和质量分析，以证明我们的提议的效iveness。代码在附录中。
</details></li>
</ul>
<hr>
<h2 id="A-Sequential-Meta-Transfer-SMT-Learning-to-Combat-Complexities-of-Physics-Informed-Neural-Networks-Application-to-Composites-Autoclave-Processing"><a href="#A-Sequential-Meta-Transfer-SMT-Learning-to-Combat-Complexities-of-Physics-Informed-Neural-Networks-Application-to-Composites-Autoclave-Processing" class="headerlink" title="A Sequential Meta-Transfer (SMT) Learning to Combat Complexities of Physics-Informed Neural Networks: Application to Composites Autoclave Processing"></a>A Sequential Meta-Transfer (SMT) Learning to Combat Complexities of Physics-Informed Neural Networks: Application to Composites Autoclave Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06447">http://arxiv.org/abs/2308.06447</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/miladramzy/sequentialmetatransferpinns">https://github.com/miladramzy/sequentialmetatransferpinns</a></li>
<li>paper_authors: Milad Ramezankhani, Abbas S. Milani</li>
<li>for: 解决非线性偏微分方程（PDE）问题，提高物理学法的泛化能力。</li>
<li>methods: 使用sequential meta-transfer（SMT）学习框架，将时间域分解成小时段，每个时间段使用meta-学习器进行快速适应。</li>
<li>results: 在一个复杂系统中，通过使用SMT学习框架，可以明显提高PINNs的适应能力，同时减少计算成本，提高效率。<details>
<summary>Abstract</summary>
Physics-Informed Neural Networks (PINNs) have gained popularity in solving nonlinear partial differential equations (PDEs) via integrating physical laws into the training of neural networks, making them superior in many scientific and engineering applications. However, conventional PINNs still fall short in accurately approximating the solution of complex systems with strong nonlinearity, especially in long temporal domains. Besides, since PINNs are designed to approximate a specific realization of a given PDE system, they lack the necessary generalizability to efficiently adapt to new system configurations. This entails computationally expensive re-training from scratch for any new change in the system. To address these shortfalls, in this work a novel sequential meta-transfer (SMT) learning framework is proposed, offering a unified solution for both fast training and efficient adaptation of PINNs in highly nonlinear systems with long temporal domains. Specifically, the framework decomposes PDE's time domain into smaller time segments to create "easier" PDE problems for PINNs training. Then for each time interval, a meta-learner is assigned and trained to achieve an optimal initial state for rapid adaptation to a range of related tasks. Transfer learning principles are then leveraged across time intervals to further reduce the computational cost.Through a composites autoclave processing case study, it is shown that SMT is clearly able to enhance the adaptability of PINNs while significantly reducing computational cost, by a factor of 100.
</details>
<details>
<summary>摘要</summary>
物理学教导神经网络（PINNs）在解决非线性偏微分方程（PDEs）中得到了广泛应用，通过将物理法则 integrate到神经网络训练中，使其在科学和工程应用中优于传统方法。然而，传统的PINNs在处理复杂系统中仍然缺乏精度，特别是在长时间域内。此外，由于PINNs是为某种特定的PDE系统进行适应，因此缺乏可重用的扩展性，需要在新系统配置时重新从零开始训练，这会增加计算成本。为了解决这些缺陷，本文提出了一种新的时序顺序多模式学习（SMT）框架，用于快速训练和高效适应PINNs在非线性系统中。特别是，该框架将时间域 decomposes 为 smaller time segments，以创建"更容易"的PDE问题，以便PINNs的快速训练。然后，每个时间段中分配了一个meta-学习器，并在快速适应一系列相关任务的基础上进行了优化。然后，通过转移学习原理，在时间间隔内进行了进一步的计算成本减少。通过一个复杂材料自动炉处理案例研究，显示SMT可以明显提高PINNs的适应性，同时显著减少计算成本，比例为100。
</details></li>
</ul>
<hr>
<h2 id="Sensitivity-Aware-Mixed-Precision-Quantization-and-Width-Optimization-of-Deep-Neural-Networks-Through-Cluster-Based-Tree-Structured-Parzen-Estimation"><a href="#Sensitivity-Aware-Mixed-Precision-Quantization-and-Width-Optimization-of-Deep-Neural-Networks-Through-Cluster-Based-Tree-Structured-Parzen-Estimation" class="headerlink" title="Sensitivity-Aware Mixed-Precision Quantization and Width Optimization of Deep Neural Networks Through Cluster-Based Tree-Structured Parzen Estimation"></a>Sensitivity-Aware Mixed-Precision Quantization and Width Optimization of Deep Neural Networks Through Cluster-Based Tree-Structured Parzen Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06422">http://arxiv.org/abs/2308.06422</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seyedarmin Azizi, Mahdi Nazemi, Arash Fayyazi, Massoud Pedram</li>
<li>for: 这篇论文的目的是提出一种自动选择神经网络层的最佳位元数和层宽的搜寻方法，以提高深度学习模型的效率。</li>
<li>methods: 这篇论文使用的方法包括对神经网络层的位元数和层宽进行自动选择，并使用希瑟尔基于删除的搜寻范围缩小技术，以便快速寻找最佳设计。它还使用树结构的Parzen估计器来建立代表性模型，以便快速探索不同的架构可能性。</li>
<li>results: 这篇论文的结果显示，与现有的压缩策略相比，这种方法可以实现20%的模型大小减少，不会对准确性产生影响。另外，这种方法的搜寻时间仅需12倍于目前最佳搜寻策略，使得快速设计和实现深度学习解决方案成为可能。<details>
<summary>Abstract</summary>
As the complexity and computational demands of deep learning models rise, the need for effective optimization methods for neural network designs becomes paramount. This work introduces an innovative search mechanism for automatically selecting the best bit-width and layer-width for individual neural network layers. This leads to a marked enhancement in deep neural network efficiency. The search domain is strategically reduced by leveraging Hessian-based pruning, ensuring the removal of non-crucial parameters. Subsequently, we detail the development of surrogate models for favorable and unfavorable outcomes by employing a cluster-based tree-structured Parzen estimator. This strategy allows for a streamlined exploration of architectural possibilities and swift pinpointing of top-performing designs. Through rigorous testing on well-known datasets, our method proves its distinct advantage over existing methods. Compared to leading compression strategies, our approach records an impressive 20% decrease in model size without compromising accuracy. Additionally, our method boasts a 12x reduction in search time relative to the best search-focused strategies currently available. As a result, our proposed method represents a leap forward in neural network design optimization, paving the way for quick model design and implementation in settings with limited resources, thereby propelling the potential of scalable deep learning solutions.
</details>
<details>
<summary>摘要</summary>
“深度学习模型的复杂性和计算需求逐渐增加，因此有效地优化神经网络设计的搜索方法变得非常重要。这项工作提出了一种新的搜索机制，可以自动选择神经网络层的最佳位数和宽度。这会导致深度神经网络的效率得到明显提高。在搜索空间中，我们利用希腊拟合法（Hessian-based pruning）缩小搜索范围，以便快速消除不重要的参数。然后，我们采用分布式树结构的Parzen估计器来构建代表性模型，以便快速探索不同的建筑方案。这种策略可以快速寻找最佳设计，并且可以保证模型的准确性不受影响。我们对知名的数据集进行了严格的测试，并证明了我们的方法与现有方法相比，可以录入20%的模型大小减少，同时保持准确性不变。此外，我们的方法可以在搜索时间方面实现12倍的提升，相比于目前最佳的搜索焦点策略。因此，我们的提议方法代表了神经网络设计优化领域的一大突破，为具有限制资源的场景中快速实现神经网络设计，铺平深度学习解决方案的可能性。”
</details></li>
</ul>
<hr>
<h2 id="Pedestrian-Trajectory-Prediction-in-Pedestrian-Vehicle-Mixed-Environments-A-Systematic-Review"><a href="#Pedestrian-Trajectory-Prediction-in-Pedestrian-Vehicle-Mixed-Environments-A-Systematic-Review" class="headerlink" title="Pedestrian Trajectory Prediction in Pedestrian-Vehicle Mixed Environments: A Systematic Review"></a>Pedestrian Trajectory Prediction in Pedestrian-Vehicle Mixed Environments: A Systematic Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06419">http://arxiv.org/abs/2308.06419</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahsa Golchoubian, Moojan Ghafurian, Kerstin Dautenhahn, Nasser Lashgarian Azad</li>
<li>for: The paper is written for the development of practical pedestrian trajectory prediction algorithms for autonomous vehicles (AVs) in unstructured environments.</li>
<li>methods: The paper systematically reviews different methods proposed in the literature for modelling pedestrian trajectory prediction in the presence of vehicles, and investigates specific considerations for pedestrian-vehicle interaction.</li>
<li>results: The paper provides an overview of datasets containing trajectory data of both pedestrians and vehicles used by the reviewed papers, and discusses research gaps and directions for future work, such as the need for more effective definition of interacting agents in deep learning methods and the need for more datasets of mixed traffic in unstructured environments.Here are the three points in Simplified Chinese text:</li>
<li>for: 本文是为了开发可行的步行者轨迹预测算法，用于自动驾驶车辆（AV）在无结构环境中。</li>
<li>methods: 本文系统地查询了Literature中的不同方法，用于模拟步行者轨迹预测在车辆存在下。</li>
<li>results: 本文提供了各种数据集，包括步行者和车辆的轨迹数据，并讨论了未来研究的潜在空间，如深度学习方法中的交互代理定义和无结构环境中混合交通数据的收集。<details>
<summary>Abstract</summary>
Planning an autonomous vehicle's (AV) path in a space shared with pedestrians requires reasoning about pedestrians' future trajectories. A practical pedestrian trajectory prediction algorithm for the use of AVs needs to consider the effect of the vehicle's interactions with the pedestrians on pedestrians' future motion behaviours. In this regard, this paper systematically reviews different methods proposed in the literature for modelling pedestrian trajectory prediction in presence of vehicles that can be applied for unstructured environments. This paper also investigates specific considerations for pedestrian-vehicle interaction (compared with pedestrian-pedestrian interaction) and reviews how different variables such as prediction uncertainties and behavioural differences are accounted for in the previously proposed prediction models. PRISMA guidelines were followed. Articles that did not consider vehicle and pedestrian interactions or actual trajectories, and articles that only focused on road crossing were excluded. A total of 1260 unique peer-reviewed articles from ACM Digital Library, IEEE Xplore, and Scopus databases were identified in the search. 64 articles were included in the final review as they met the inclusion and exclusion criteria. An overview of datasets containing trajectory data of both pedestrians and vehicles used by the reviewed papers has been provided. Research gaps and directions for future work, such as having more effective definition of interacting agents in deep learning methods and the need for gathering more datasets of mixed traffic in unstructured environments are discussed.
</details>
<details>
<summary>摘要</summary>
планирование пути автономного транспортного средства (АВ) в пространстве, разделенном с пешеходами, требует расчета будущих траекторий пешеходов. практический алгоритм предсказания траекторий пешеходов для использования АВ должен учитывать влияние взаимодействия автомобиля с пешеходами на будущие движения людей. в этом смысле, этот папяр систематически обзорывает разные методы, предложенные в литературе для моделирования предсказания траекторий пешеходов в присутствии автомобилей, которые могут быть применены в неструктурированных средах. папяр также рассматривает специфические условия для взаимодействия пешеходов и автомобилей (в сравнении с взаимодействием пешеходов-пешеходов) и обзоры, как различные переменные, такие как неопределенности предсказаний и различия в поведении, учитываются в предыдущих предсказательных моделях. following PRISMA guidelines, articles that did not consider vehicle and pedestrian interactions or actual trajectories, and articles that only focused on road crossing were excluded. a total of 1260 unique peer-reviewed articles from ACM Digital Library, IEEE Xplore, and Scopus databases were identified in the search. 64 articles were included in the final review as they met the inclusion and exclusion criteria. an overview of datasets containing trajectory data of both pedestrians and vehicles used by the reviewed papers has been provided. research gaps and directions for future work, such as having more effective definition of interacting agents in deep learning methods and the need for gathering more datasets of mixed traffic in unstructured environments, are discussed.
</details></li>
</ul>
<hr>
<h2 id="Dialogue-Possibilities-between-a-Human-Supervisor-and-UAM-Air-Traffic-Management-Route-Alteration"><a href="#Dialogue-Possibilities-between-a-Human-Supervisor-and-UAM-Air-Traffic-Management-Route-Alteration" class="headerlink" title="Dialogue Possibilities between a Human Supervisor and UAM Air Traffic Management: Route Alteration"></a>Dialogue Possibilities between a Human Supervisor and UAM Air Traffic Management: Route Alteration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06411">http://arxiv.org/abs/2308.06411</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeongseok Kim, Kangjin Kim</li>
<li>for: 本研究旨在提出一种基于知识表示和逻辑的城市航空交通管理（UATM）拓扑管理方法，以便快速Identify safe和高效的 Routes in a carefully sampled environment.</li>
<li>methods: 本方法使用Answer Set Programming（ASP）实现，其中包括非 monotonic reasoning和两个阶段对话，考虑安全和可能的影响因素。</li>
<li>results: 经过多个查询从两个 simulations scenarios， validate了提出的方法的可靠性和有效性。I hope this helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
This paper introduces a novel approach to detour management in Urban Air Traffic Management (UATM) using knowledge representation and reasoning. It aims to understand the complexities and requirements of UAM detours, enabling a method that quickly identifies safe and efficient routes in a carefully sampled environment. This method implemented in Answer Set Programming uses non-monotonic reasoning and a two-phase conversation between a human manager and the UATM system, considering factors like safety and potential impacts. The robustness and efficacy of the proposed method were validated through several queries from two simulation scenarios, contributing to the symbiosis of human knowledge and advanced AI techniques. The paper provides an introduction, citing relevant studies, problem formulation, solution, discussions, and concluding comments.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种新的偏航管理方法（Detour Management），用于城市空中交通管理（UATM），利用知识表示和推理。它旨在理解城市垂直飞行偏航的复杂性和需求，以便快速地确定安全和高效的路径，并在精心采样的环境中进行。这种方法使用了非 monotonic 推理和两个阶段的人工管理和UATM系统之间的对话，考虑了安全和可能的影响因素。该方法的可靠性和有效性通过多个查询来 validate，来自两个 simulate enario。这篇论文提供了引言、相关研究、问题表述、解决方案、讨论和结论。
</details></li>
</ul>
<hr>
<h2 id="A-Brain-Computer-Interface-Augmented-Reality-Framework-with-Auto-Adaptive-SSVEP-Recognition"><a href="#A-Brain-Computer-Interface-Augmented-Reality-Framework-with-Auto-Adaptive-SSVEP-Recognition" class="headerlink" title="A Brain-Computer Interface Augmented Reality Framework with Auto-Adaptive SSVEP Recognition"></a>A Brain-Computer Interface Augmented Reality Framework with Auto-Adaptive SSVEP Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06401">http://arxiv.org/abs/2308.06401</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yasmine Mustafa, Mohamed Elmahallawy, Tie Luo, Seif Eldawlatly</li>
<li>for: 该研究旨在开发一种可以满足不同个体的脑电信号特点的简单适应集合分类系统，以便在脑机接口（BCI）和增强现实（AR）技术的应用中提高抗骚抗振性能。</li>
<li>methods: 该研究使用了稳态视觉谱波（SSVEP）信号 Pattern，并提出了一种简单的BCI-AR框架，以支持广泛的SSVEP-based BCI-AR应用程序的开发。</li>
<li>results: 测试结果显示，我们的ensemble分类方法在SSVEP-based BCI-AR应用程序中表现出了Robust性，并且与之前的研究相比，我们的方法在包括头部运动的情况下仍然能够达到80%的正确率（在PC上）和77%的正确率（使用HoloLens AR头盔）。此外，我们的视觉刺激时间为5秒，相对较短。<details>
<summary>Abstract</summary>
Brain-Computer Interface (BCI) initially gained attention for developing applications that aid physically impaired individuals. Recently, the idea of integrating BCI with Augmented Reality (AR) emerged, which uses BCI not only to enhance the quality of life for individuals with disabilities but also to develop mainstream applications for healthy users. One commonly used BCI signal pattern is the Steady-state Visually-evoked Potential (SSVEP), which captures the brain's response to flickering visual stimuli. SSVEP-based BCI-AR applications enable users to express their needs/wants by simply looking at corresponding command options. However, individuals are different in brain signals and thus require per-subject SSVEP recognition. Moreover, muscle movements and eye blinks interfere with brain signals, and thus subjects are required to remain still during BCI experiments, which limits AR engagement. In this paper, we (1) propose a simple adaptive ensemble classification system that handles the inter-subject variability, (2) present a simple BCI-AR framework that supports the development of a wide range of SSVEP-based BCI-AR applications, and (3) evaluate the performance of our ensemble algorithm in an SSVEP-based BCI-AR application with head rotations which has demonstrated robustness to the movement interference. Our testing on multiple subjects achieved a mean accuracy of 80\% on a PC and 77\% using the HoloLens AR headset, both of which surpass previous studies that incorporate individual classifiers and head movements. In addition, our visual stimulation time is 5 seconds which is relatively short. The statistically significant results show that our ensemble classification approach outperforms individual classifiers in SSVEP-based BCIs.
</details>
<details>
<summary>摘要</summary>
Initially, Brain-Computer Interface (BCI) 引起关注的应用是为Physically impaired individuals 提高生活质量。然而， BCIs 的潜在应用不仅限于这些人群，还可以为健康用户开发主流应用程序。 BCIs 使用 Steady-state Visually-evoked Potential (SSVEP) 信号模式， capture 脑的响应，并使用 BCIs 来表达需求或愿望。然而，每个人的脑信号不同，因此需要每个人SSVEP 认知。此外，肌肉运动和眼睛跳动会干扰脑信号，因此需要用户在BCI实验中保持静止，限制了AR的应用。在这篇论文中，我们提出了一种简单的适应集成分类系统，可以处理每个人的差异。我们还提出了一种支持广泛SSVEP 基于 BCIs 应用程序的简单AR框架。我们的结果表明，我们的集成分类方法在SSVEP 基于 BCIs 的AR应用程序中，可以快速响应用户的需求或愿望，并且在多个测试人群中表现出 statistically significant 的表现。我们的测试结果显示，我们的集成分类方法在PC 和 HoloLens AR 头盔中都可以达到80%和77%的准确率，这 beiden超过了以个体分类器和头部运动混合的前一 Studies。此外，我们的视觉刺激时间为5秒，相对较短。总之，我们的研究表明，集成分类方法在SSVEP 基于 BCIs 的AR应用程序中表现出了优于个体分类器的表现。这 suggets that our ensemble classification approach can be a promising solution for developing mainstream BCI-AR applications.
</details></li>
</ul>
<hr>
<h2 id="ZYN-Zero-Shot-Reward-Models-with-Yes-No-Questions"><a href="#ZYN-Zero-Shot-Reward-Models-with-Yes-No-Questions" class="headerlink" title="ZYN: Zero-Shot Reward Models with Yes-No Questions"></a>ZYN: Zero-Shot Reward Models with Yes-No Questions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06385">http://arxiv.org/abs/2308.06385</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vicgalle/zero-shot-reward-models">https://github.com/vicgalle/zero-shot-reward-models</a></li>
<li>paper_authors: Victor Gallego</li>
<li>for: 本文提出了一种解决方案，用于指导语言模型生成文本，以便与人类操作员的偏好相align。</li>
<li>methods: 该方法使用另一个语言模型作为批评者和奖励模型，通过一个Yes-No问题的提问来表达用户偏好，无需进一步的标注数据。</li>
<li>results: 在不同的文本生成领域中，包括毒瘤化、修正电影评论的情感、控制模型对某个话题的看法，以及个性化文本生成器的推荐等方面，实验证明了提议的ZYN框架的可能性。<details>
<summary>Abstract</summary>
In this work, we address the problem of directing the text generations of a LLM towards a desired behavior, aligning the generated text with the preferences of the human operator. We propose using another language model as a critic, reward model in a zero-shot way thanks to the prompt of a Yes-No question that represents the user preferences, without requiring further labeled data. This zero-shot reward model provides the learning signal to further fine-tune the base LLM using reinforcement learning, as in RLAIF; yet our approach is also compatible in other contexts such as quality-diversity search. Extensive evidence of the capabilities of the proposed ZYN framework is provided through experiments in different domains related to text generation, including detoxification; optimizing sentiment of movie reviews, or any other attribute; steering the opinion about a particular topic the model may have; and personalizing prompt generators for text-to-image tasks. Code to be released at \url{https://github.com/vicgalle/zero-shot-reward-models/}.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们解决了直接将语言生成模型（LLM）引导到所需行为的问题，将生成的文本与人类运行员的偏好相align。我们提议使用另一个语言模型作为批评者、奖励模型，通过Zero-shot manner，只需通过问题提示（Yes-No问题）表示用户偏好，无需更多的标注数据。这种Zero-shot奖励模型为基础LLM进行了进一步微调，使用束缚学习，类似RLAIF;而我们的方法也可以在其他上下文中使用，如质量多样性搜索。我们通过不同领域的文本生成实验提供了广泛的证据，包括毒瘤化、修改电影评论的情感、控制模型对某个话题的看法，以及个性化提示生成器 для文本到图像任务。代码将在 \url{https://github.com/vicgalle/zero-shot-reward-models/} 上发布。
</details></li>
</ul>
<hr>
<h2 id="DCNFIS-Deep-Convolutional-Neuro-Fuzzy-Inference-System"><a href="#DCNFIS-Deep-Convolutional-Neuro-Fuzzy-Inference-System" class="headerlink" title="DCNFIS: Deep Convolutional Neuro-Fuzzy Inference System"></a>DCNFIS: Deep Convolutional Neuro-Fuzzy Inference System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06378">http://arxiv.org/abs/2308.06378</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mojtaba Yeganejou, Kimia Honari, Ryan Kluzinski, Scott Dick, Michael Lipsett, James Miller</li>
<li>for: 该研究旨在提出一种新的深度学习模型，以提高模型的透明度而不增加准确性的损失。</li>
<li>methods: 该研究使用了深度 convolutional neuro-fuzzy inference system (DCNFIS)，即将深度学习模型和逻辑学习模型相结合，以提高模型的透明度。</li>
<li>results: 研究发现，DCNFIS可以与现有的三种 convolutional neural networks 相比，在四个公共数据集上表现相当准确。此外，DCNFIS还可以超过当前的深度逻辑系统的性能。此外，通过解释来源于逻辑规则的质量分析，该研究还发现了一些有用的特性。<details>
<summary>Abstract</summary>
A key challenge in eXplainable Artificial Intelligence is the well-known tradeoff between the transparency of an algorithm (i.e., how easily a human can directly understand the algorithm, as opposed to receiving a post-hoc explanation), and its accuracy. We report on the design of a new deep network that achieves improved transparency without sacrificing accuracy. We design a deep convolutional neuro-fuzzy inference system (DCNFIS) by hybridizing fuzzy logic and deep learning models and show that DCNFIS performs as accurately as three existing convolutional neural networks on four well-known datasets. We furthermore that DCNFIS outperforms state-of-the-art deep fuzzy systems. We then exploit the transparency of fuzzy logic by deriving explanations, in the form of saliency maps, from the fuzzy rules encoded in DCNFIS. We investigate the properties of these explanations in greater depth using the Fashion-MNIST dataset.
</details>
<details>
<summary>摘要</summary>
一个主要挑战在可解释人工智能是论文质量和直观性之间的交换。我们报告了一种新的深度网络的设计，该网络可以提高直观性而无需牺牲准确性。我们将深度 convolutional neuro-fuzzy inference system (DCNFIS) 设计为混合深度学习和规则逻辑模型，并证明 DCNFIS 在四个常见数据集上表现和三种现有的 convolutional neural networks 相同。此外，我们还证明 DCNFIS 在深度逻辑系统中表现更出色。然后，我们利用规则逻辑的透明性，从 DCNFIS 中提取出解释，以幻灯片的形式表示。我们在 Fashion-MNIST 数据集中进一步调查了这些解释的性质。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-and-Knowledge-Graphs-Opportunities-and-Challenges"><a href="#Large-Language-Models-and-Knowledge-Graphs-Opportunities-and-Challenges" class="headerlink" title="Large Language Models and Knowledge Graphs: Opportunities and Challenges"></a>Large Language Models and Knowledge Graphs: Opportunities and Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06374">http://arxiv.org/abs/2308.06374</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Jeff Z. Pan, Simon Razniewski, Jan-Christoph Kalo, Sneha Singhania, Jiaoyan Chen, Stefan Dietze, Hajira Jabeen, Janna Omeliyanenko, Wen Zhang, Matteo Lissandrini, Russa Biswas, Gerard de Melo, Angela Bonifati, Edlira Vakaj, Mauro Dragoni, Damien Graux</li>
<li>for: 本研究论文探讨了大语言模型（LLM）在知识表示方面的发展，以及这些模型对知识图和 parametric knowledge 的影响。</li>
<li>methods: 本文使用了许多现有的知识表示方法，如知识图和Parametric knowledge，以及一些新的研究方法。</li>
<li>results: 本文总结了一些关于 LLMs 和知识图的共识和观点，并提出了一些可能的研究方向和挑战。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have taken Knowledge Representation -- and the world -- by storm. This inflection point marks a shift from explicit knowledge representation to a renewed focus on the hybrid representation of both explicit knowledge and parametric knowledge. In this position paper, we will discuss some of the common debate points within the community on LLMs (parametric knowledge) and Knowledge Graphs (explicit knowledge) and speculate on opportunities and visions that the renewed focus brings, as well as related research topics and challenges.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Wireless-Federated-k-Means-Clustering-with-Non-coherent-Over-the-Air-Computation"><a href="#Wireless-Federated-k-Means-Clustering-with-Non-coherent-Over-the-Air-Computation" class="headerlink" title="Wireless Federated $k$-Means Clustering with Non-coherent Over-the-Air Computation"></a>Wireless Federated $k$-Means Clustering with Non-coherent Over-the-Air Computation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06371">http://arxiv.org/abs/2308.06371</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alphan Sahin</li>
<li>for: 降低无线网络上实现 Federated k-means 算法时的每次通信延迟</li>
<li>methods: 使用 Over-the-air computation（OAC）方案，通过编码器利用数字征在均匀数系统中的表示，通过无线多访问通道的信号积加性性质消除精确时钟和频率同步需求</li>
<li>results: 对客户位置 clustering 场景进行 demonstration，比较标准 k-means  clustering 和提议方法的性能，结果显示提议方法与标准 k-means 性能相似，同时降低了通信延迟<details>
<summary>Abstract</summary>
In this study, we propose using an over-the-air computation (OAC) scheme for the federated k-means clustering algorithm to reduce the per-round communication latency when it is implemented over a wireless network. The OAC scheme relies on an encoder exploiting the representation of a number in a balanced number system and computes the sum of the updates for the federated k-means via signal superposition property of wireless multiple-access channels non-coherently to eliminate the need for precise phase and time synchronization. Also, a reinitialization method for ineffectively used centroids is proposed to improve the performance of the proposed method for heterogeneous data distribution. For a customer-location clustering scenario, we demonstrate the performance of the proposed algorithm and compare it with the standard k-means clustering. Our results show that the proposed approach performs similarly to the standard k-means while reducing communication latency.
</details>
<details>
<summary>摘要</summary>
在这种研究中，我们提议使用无线电 computation（OAC）方案来降低在无线网络上实现 federated k-means 算法时的每轮通信延迟。 OAC 方案利用一个编码器利用数字 representation 在平衡数系统中的特性，通过无线多接入通道的信号重叠性性质来消除精确的时钟和相位同步需求。此外，我们还提出了一种重新初始化不合适使用的中心点方法，以提高提案方法在不同数据分布情况下的性能。为一个客户位置 clustering 场景，我们展示了提案的算法性能和标准 k-means 集群算法的比较，我们的结果表明，提案的方法与标准 k-means 集群算法性能相似，同时降低了通信延迟。
</details></li>
</ul>
<hr>
<h2 id="Topic-Level-Bayesian-Surprise-and-Serendipity-for-Recommender-Systems"><a href="#Topic-Level-Bayesian-Surprise-and-Serendipity-for-Recommender-Systems" class="headerlink" title="Topic-Level Bayesian Surprise and Serendipity for Recommender Systems"></a>Topic-Level Bayesian Surprise and Serendipity for Recommender Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06368">http://arxiv.org/abs/2308.06368</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ton-moy/surprise-and-serendipity">https://github.com/ton-moy/surprise-and-serendipity</a></li>
<li>paper_authors: Tonmoy Hasan, Razvan Bunescu</li>
<li>for: 提高推荐系统的多样性，使用高度可能性的推荐项，让用户体验到新、未看过的类别。</li>
<li>methods: 使用 bayesian 惊喜来衡量item的意外性，并结合协同推荐算法来找到相似用户。</li>
<li>results: 实验结果表明，使用 bayesian 惊喜与距离基于的优化方法相比，对于时间和主题层次的意外性的评估更加准确，并且在推荐高度可能性的项目方面获得更好的性能。<details>
<summary>Abstract</summary>
A recommender system that optimizes its recommendations solely to fit a user's history of ratings for consumed items can create a filter bubble, wherein the user does not get to experience items from novel, unseen categories. One approach to mitigate this undesired behavior is to recommend items with high potential for serendipity, namely surprising items that are likely to be highly rated. In this paper, we propose a content-based formulation of serendipity that is rooted in Bayesian surprise and use it to measure the serendipity of items after they are consumed and rated by the user. When coupled with a collaborative-filtering component that identifies similar users, this enables recommending items with high potential for serendipity. To facilitate the evaluation of topic-level models for surprise and serendipity, we introduce a dataset of book reading histories extracted from Goodreads, containing over 26 thousand users and close to 1.3 million books, where we manually annotate 449 books read by 4 users in terms of their time-dependent, topic-level surprise. Experimental evaluations show that models that use Bayesian surprise correlate much better with the manual annotations of topic-level surprise than distance-based heuristics, and also obtain better serendipitous item recommendation performance.
</details>
<details>
<summary>摘要</summary>
一个推荐系统仅将推荐项目调整为用户的预先消耗项目历史，可能会创建一个范例弹性泡箱，让用户无法体验到未看过的类别。为了解决这个问题，可以推荐有高可能性的意外项目，即吸引用户高度评价的项目。在这篇论文中，我们提出了基于bayesian surprise的内容基于的serendipity表现，并使用它来衡量项目被用户过后评价后的surprise程度。当与相似用户的协同组件一起使用时，这将允许推荐高可能性的意外项目。为了评估主题层模型的惊喜和意外性表现，我们引入了Goodreads上的阅读历史数据集，包括26,000名用户和1,300,000本书，其中我们 manually annotate 449本被4名用户阅读的书籍，以时间依赖的主题层惊喜作为标准。实验评估显示，使用bayesian surprise的模型与距离基于的规律来的模型相比，具有更高的惊喜和意外性表现，并且在serendipity项目推荐上也有更好的表现。
</details></li>
</ul>
<hr>
<h2 id="Causally-Linking-Health-Application-Data-and-Personal-Information-Management-Tools"><a href="#Causally-Linking-Health-Application-Data-and-Personal-Information-Management-Tools" class="headerlink" title="Causally Linking Health Application Data and Personal Information Management Tools"></a>Causally Linking Health Application Data and Personal Information Management Tools</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08556">http://arxiv.org/abs/2308.08556</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saturnino Luz, Masood Masoodian</li>
<li>for: 本研究旨在开发一种整合多种数据源、分析和可见化工具，以帮助用户更好地理解健康变量之间的 causal 连接。</li>
<li>methods: 本研究使用了数据挖掘、时间序列分析和可见化技术，并将这些技术与各种健康应用程序集成。</li>
<li>results: 研究人员通过提供用户可见化时间序列数据，使用者可以更好地理解健康变量之间的关系，从而帮助用户更好地管理健康。<details>
<summary>Abstract</summary>
The proliferation of consumer health devices such as smart watches, sleep monitors, smart scales, etc, in many countries, has not only led to growing interest in health monitoring, but also to the development of a countless number of ``smart'' applications to support the exploration of such data by members of the general public, sometimes with integration into professional health services. While a variety of health data streams has been made available by such devices to users, these streams are often presented as separate time-series visualizations, in which the potential relationships between health variables are not explicitly made visible. Furthermore, despite the fact that other aspects of life, such as work and social connectivity, have become increasingly digitised, health and well-being applications make little use of the potentially useful contextual information provided by widely used personal information management tools, such as shared calendar and email systems. This paper presents a framework for the integration of these diverse data sources, analytic and visualization tools, with inference methods and graphical user interfaces to help users by highlighting causal connections among such time-series.
</details>
<details>
<summary>摘要</summary>
“随着各国消费者医疗设备的普及，如智能手表、睡眠监测仪、智能秤 scales 等，人们对健康监测的兴趣不 только增加，而且促使了大量的``智能''应用程序的开发，以支持公众成员对健康数据的探索，并有时与专业医疗服务集成。而这些医疗设备提供的健康数据流量，经常以分开的时间序列视图方式显示出来，无法直观地显示健康变量之间的可能关系。此外，尽管其他方面的生活，如工作和社交连接，已经 Digitized，健康和福祉应用却几乎不使用广泛使用的个人信息管理工具，如共享日历和邮件系统，具有可营利的上下文信息。本文提出了将这些多种数据源、分析和视图工具、推理方法和图形用户界面集成起来，以帮助用户更好地探索健康数据的关系。”
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-to-Identify-Social-Determinants-of-Health-in-Electronic-Health-Records"><a href="#Large-Language-Models-to-Identify-Social-Determinants-of-Health-in-Electronic-Health-Records" class="headerlink" title="Large Language Models to Identify Social Determinants of Health in Electronic Health Records"></a>Large Language Models to Identify Social Determinants of Health in Electronic Health Records</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06354">http://arxiv.org/abs/2308.06354</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aim-harvard/sdoh">https://github.com/aim-harvard/sdoh</a></li>
<li>paper_authors: Marco Guevara, Shan Chen, Spencer Thomas, Tafadzwa L. Chaunzwa, Idalid Franco, Benjamin Kann, Shalini Moningi, Jack Qian, Madeleine Goldstein, Susan Harper, Hugo JWL Aerts, Guergana K. Savova, Raymond H. Mak, Danielle S. Bitterman</li>
<li>For: The paper aims to extract social determinants of health (SDoH) from electronic health records (EHRs) to improve patient outcomes.* Methods: The study uses large language models to extract SDoH from free text in EHRs, and experiments with synthetic data generation to improve the extraction of scarce SDoH data.* Results: The best-performing models were fine-tuned Flan-T5 XL and Flan-T5 XXL, which outperformed zero- and few-shot performance of ChatGPT-family models and showed less algorithmic bias. The models identified 93.8% of patients with adverse SDoH, while ICD-10 codes captured only 2.0%.Here’s the information in Simplified Chinese text:* 为：本研究用大语言模型提取电子医疗记录中社会determinants of health（SDoH），以提高患者结果。* 方法：研究使用自由文本中的SDoH，并对缺乏SDoH数据进行生成数据的尝试。* 结果：最佳表现的模型是精细调整后的Flan-T5 XL和Flan-T5 XXL，它们在比较shot setting下表现得更好，并且表现出较少的算法偏见。模型可以准确地提取93.8%的患者有不良SDoH，而ICD-10代码只能捕捉2.0%。<details>
<summary>Abstract</summary>
Social determinants of health (SDoH) have an important impact on patient outcomes but are incompletely collected from the electronic health records (EHR). This study researched the ability of large language models to extract SDoH from free text in EHRs, where they are most commonly documented, and explored the role of synthetic clinical text for improving the extraction of these scarcely documented, yet extremely valuable, clinical data. 800 patient notes were annotated for SDoH categories, and several transformer-based models were evaluated. The study also experimented with synthetic data generation and assessed for algorithmic bias. Our best-performing models were fine-tuned Flan-T5 XL (macro-F1 0.71) for any SDoH, and Flan-T5 XXL (macro-F1 0.70). The benefit of augmenting fine-tuning with synthetic data varied across model architecture and size, with smaller Flan-T5 models (base and large) showing the greatest improvements in performance (delta F1 +0.12 to +0.23). Model performance was similar on the in-hospital system dataset but worse on the MIMIC-III dataset. Our best-performing fine-tuned models outperformed zero- and few-shot performance of ChatGPT-family models for both tasks. These fine-tuned models were less likely than ChatGPT to change their prediction when race/ethnicity and gender descriptors were added to the text, suggesting less algorithmic bias (p<0.05). At the patient-level, our models identified 93.8% of patients with adverse SDoH, while ICD-10 codes captured 2.0%. Our method can effectively extracted SDoH information from clinic notes, performing better compare to GPT zero- and few-shot settings. These models could enhance real-world evidence on SDoH and aid in identifying patients needing social support.
</details>
<details>
<summary>摘要</summary>
社会 determinants of health (SDoH) 有重要的影响 på patient outcomes，但是它们从电子健康记录 (EHR) 中 incomplete 收集。这项研究检查了大型自然语言模型能否从自由文本中提取 SDoH，其中最常见的位置是 EHR 中。研究还检查了使用生成的Synthetic clinical text 来提高提取这些罕见 yet extremely valuable 的临床数据的能力。研究采用了800份病人笔记，并评估了多种 transformer-based 模型。研究还进行了生成数据的评估和算法偏见的检查。我们的最佳表现模型是 Fine-tuned Flan-T5 XL (macro-F1 0.71) 和 Fine-tuned Flan-T5 XXL (macro-F1 0.70)。使用生成数据进行 augmentation 的效果因模型结构和大小而异，小型 Flan-T5 模型（基本和大型）在性能提升中表现最佳（delta F1 +0.12到 +0.23）。模型在医院内系统数据集上的表现相似，但在 MIMIC-III 数据集上表现更差。我们的最佳精度调整模型在 zero-和 few-shot 任务上表现更好，并且比 ChatGPT 家族模型更少改变其预测结果，这表明它们更少受到算法偏见（p<0.05）。在 patient 级别上，我们的模型可以识别93.8%的患者拥有不利的 SDoH，而 ICD-10 代码只能识别2.0%。我们的方法可以有效地从临床笔记中提取 SDoH 信息，并在 GPT zero-和 few-shot 设置下表现更好。这些模型可以增强实际证据，并帮助 indentify 需要社会支持的患者。
</details></li>
</ul>
<hr>
<h2 id="Combining-feature-aggregation-and-geometric-similarity-for-re-identification-of-patterned-animals"><a href="#Combining-feature-aggregation-and-geometric-similarity-for-re-identification-of-patterned-animals" class="headerlink" title="Combining feature aggregation and geometric similarity for re-identification of patterned animals"></a>Combining feature aggregation and geometric similarity for re-identification of patterned animals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06335">http://arxiv.org/abs/2308.06335</a></li>
<li>repo_url: None</li>
<li>paper_authors: Veikka Immonen, Ekaterina Nepovinnykh, Tuomas Eerola, Charles V. Stewart, Heikki Kälviäinen</li>
<li>For: The paper is written for studying animal populations by using image-based re-identification of individual animals.* Methods: The paper combines two types of pattern similarity metrics: pattern appearance similarity and geometric pattern similarity.* Results: The proposed combination of pattern similarity metrics achieves promising re-identification accuracies for Saimaa ringed seals and whale sharks.Here’s the text in Simplified Chinese:</li>
<li>for: 研究动物种群，通过图像基于个体重新识别。</li>
<li>methods:  combining两种 patrern similarity metrics： patrern appearance similarity和几何 patrern similarity。</li>
<li>results: 提议的combinaison achieve promising的重新识别精度 дляSaimaa环形海豹和鲸鱼。<details>
<summary>Abstract</summary>
Image-based re-identification of animal individuals allows gathering of information such as migration patterns of the animals over time. This, together with large image volumes collected using camera traps and crowdsourcing, opens novel possibilities to study animal populations. For many species, the re-identification can be done by analyzing the permanent fur, feather, or skin patterns that are unique to each individual. In this paper, we address the re-identification by combining two types of pattern similarity metrics: 1) pattern appearance similarity obtained by pattern feature aggregation and 2) geometric pattern similarity obtained by analyzing the geometric consistency of pattern similarities. The proposed combination allows to efficiently utilize both the local and global pattern features, providing a general re-identification approach that can be applied to a wide variety of different pattern types. In the experimental part of the work, we demonstrate that the method achieves promising re-identification accuracies for Saimaa ringed seals and whale sharks.
</details>
<details>
<summary>摘要</summary>
图像基于个体重新识别动物，可以获取动物迁徙趋势的信息，并且通过摄像头和人员参与投票，收集大量图像。这些图像可以用于研究动物种群。许多物种的重新识别可以通过分析永久性毛发、羽毛或皮肤特征来完成，这些特征是每个个体唯一的。在这篇论文中，我们提出了结合两种模式相似度度量的方法：1）图像出现相似度度量，通过图像特征聚合获得，2）几何模式相似度度量，通过分析模式相似度的几何一致性来获得。该方法可以有效利用本地和全局模式特征，提供一种通用的重新识别方法，可以应用于多种不同的模式类型。在实验部分，我们示例了对Saimaa环形鳐和鲸鱼等动物的重新识别准确率。
</details></li>
</ul>
<hr>
<h2 id="Foundation-Model-is-Efficient-Multimodal-Multitask-Model-Selector"><a href="#Foundation-Model-is-Efficient-Multimodal-Multitask-Model-Selector" class="headerlink" title="Foundation Model is Efficient Multimodal Multitask Model Selector"></a>Foundation Model is Efficient Multimodal Multitask Model Selector</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06262">http://arxiv.org/abs/2308.06262</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/opengvlab/multitask-model-selector">https://github.com/opengvlab/multitask-model-selector</a></li>
<li>paper_authors: Fanqing Meng, Wenqi Shao, Zhanglin Peng, Chonghe Jiang, Kaipeng Zhang, Yu Qiao, Ping Luo</li>
<li>For: 本文研究了一个未得到充分研究的问题：给一个集合 pré-trained neural networks，预测它们在每个多 modal 任务上的性能，而不需要 fine-tuning 它们。* Methods: 本文提出了一种高效的多任务模型选择器（EMMS），使用大规模基础模型将多个下游任务的多种标签格式转化为一个统一的噪声标签嵌入。EMMS 可以通过一种简单的负权重回归来估计模型的传输性能，可以高效地解决一个 Alternating Minimization 算法。* Results: 广泛的实验表明，EMMS 是一种快速、有效和通用的模型选择器，可以高效地评估 pré-trained 模型的传输性能。例如，相比之前的 state-of-the-art 方法 LogME 增强我们的标签嵌入，EMMS 在图像识别、引用、描述、视觉问答和文本问答等五个下游任务上实现了9.0%、26.3%、20.1%、54.8% 和12.2% 的性能提升，同时带来5.13x、6.29x、3.59x、6.19x 和5.66x 的速度提升。代码可以在 <a target="_blank" rel="noopener" href="https://github.com/OpenGVLab/Multitask-Model-Selector">https://github.com/OpenGVLab/Multitask-Model-Selector</a> 上获取。<details>
<summary>Abstract</summary>
This paper investigates an under-explored but important problem: given a collection of pre-trained neural networks, predicting their performance on each multi-modal task without fine-tuning them, such as image recognition, referring, captioning, visual question answering, and text question answering. A brute-force approach is to finetune all models on all target datasets, bringing high computational costs. Although recent-advanced approaches employed lightweight metrics to measure models' transferability,they often depend heavily on the prior knowledge of a single task, making them inapplicable in a multi-modal multi-task scenario. To tackle this issue, we propose an efficient multi-task model selector (EMMS), which employs large-scale foundation models to transform diverse label formats such as categories, texts, and bounding boxes of different downstream tasks into a unified noisy label embedding. EMMS can estimate a model's transferability through a simple weighted linear regression, which can be efficiently solved by an alternating minimization algorithm with a convergence guarantee. Extensive experiments on 5 downstream tasks with 24 datasets show that EMMS is fast, effective, and generic enough to assess the transferability of pre-trained models, making it the first model selection method in the multi-task scenario. For instance, compared with the state-of-the-art method LogME enhanced by our label embeddings, EMMS achieves 9.0\%, 26.3\%, 20.1\%, 54.8\%, 12.2\% performance gain on image recognition, referring, captioning, visual question answering, and text question answering, while bringing 5.13x, 6.29x, 3.59x, 6.19x, and 5.66x speedup in wall-clock time, respectively. The code is available at https://github.com/OpenGVLab/Multitask-Model-Selector.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Enhancing-Network-Management-Using-Code-Generated-by-Large-Language-Models"><a href="#Enhancing-Network-Management-Using-Code-Generated-by-Large-Language-Models" class="headerlink" title="Enhancing Network Management Using Code Generated by Large Language Models"></a>Enhancing Network Management Using Code Generated by Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06261">http://arxiv.org/abs/2308.06261</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020">https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020</a></li>
<li>paper_authors: Sathiya Kumaran Mani, Yajie Zhou, Kevin Hsieh, Santiago Segarra, Ranveer Chandra, Srikanth Kandula</li>
<li>for: This paper aims to provide a novel approach for natural-language-based network management, leveraging large language models (LLMs) to generate task-specific code from natural language queries.</li>
<li>methods: The proposed approach utilizes LLMs to generate code, addressing the challenges of explainability, scalability, and privacy by allowing network operators to inspect the generated code and eliminating the need to share network data with LLMs.</li>
<li>results: The prototype system designed and evaluated in the paper demonstrates high accuracy, cost-effectiveness, and potential for further enhancements using complementary program synthesis techniques.<details>
<summary>Abstract</summary>
Analyzing network topologies and communication graphs plays a crucial role in contemporary network management. However, the absence of a cohesive approach leads to a challenging learning curve, heightened errors, and inefficiencies. In this paper, we introduce a novel approach to facilitate a natural-language-based network management experience, utilizing large language models (LLMs) to generate task-specific code from natural language queries. This method tackles the challenges of explainability, scalability, and privacy by allowing network operators to inspect the generated code, eliminating the need to share network data with LLMs, and concentrating on application-specific requests combined with general program synthesis techniques. We design and evaluate a prototype system using benchmark applications, showcasing high accuracy, cost-effectiveness, and the potential for further enhancements using complementary program synthesis techniques.
</details>
<details>
<summary>摘要</summary>
现代网络管理中分析网络拓扑和通信图是关键。然而，由于缺乏一致的方法，会导致学习曲线困难、错误高伸和不效率。在这篇论文中，我们介绍一种新的方法，使得网络管理人员可以通过自然语言查询来获得任务特定的代码。这种方法解决了解释性、可扩展性和隐私问题，因为网络数据不需要与大语言模型（LLMs）分享，而是专注于应用特定的请求，并结合通用程序生成技术。我们设计并评估了一个原型系统，使用标准套件应用程序进行评估，显示高精度、成本效果和可能性。
</details></li>
</ul>
<hr>
<h2 id="ChatGPT-based-Investment-Portfolio-Selection"><a href="#ChatGPT-based-Investment-Portfolio-Selection" class="headerlink" title="ChatGPT-based Investment Portfolio Selection"></a>ChatGPT-based Investment Portfolio Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06260">http://arxiv.org/abs/2308.06260</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oleksandr Romanko, Akhilesh Narayan, Roy H. Kwon</li>
<li>for: 投资组合选择（portfolio selection）</li>
<li>methods: 使用生成AI模型（ChatGPT）获取S&amp;P500市场指数中可能有潜力的股票，并对这些股票进行优化配置</li>
<li>results: 结果表明，使用ChatGPT进行股票选择可以带来更好的回报，但是在分配股票重量方面可能不如量化优化模型。但是将AI生成的股票选择与量化优化模型相结合，可以获得更好的投资效果，建议将来投资决策中采用协同approach。<details>
<summary>Abstract</summary>
In this paper, we explore potential uses of generative AI models, such as ChatGPT, for investment portfolio selection. Trusting investment advice from Generative Pre-Trained Transformer (GPT) models is a challenge due to model "hallucinations", necessitating careful verification and validation of the output. Therefore, we take an alternative approach. We use ChatGPT to obtain a universe of stocks from S&P500 market index that are potentially attractive for investing. Subsequently, we compared various portfolio optimization strategies that utilized this AI-generated trading universe, evaluating those against quantitative portfolio optimization models as well as comparing to some of the popular investment funds. Our findings indicate that ChatGPT is effective in stock selection but may not perform as well in assigning optimal weights to stocks within the portfolio. But when stocks selection by ChatGPT is combined with established portfolio optimization models, we achieve even better results. By blending strengths of AI-generated stock selection with advanced quantitative optimization techniques, we observed the potential for more robust and favorable investment outcomes, suggesting a hybrid approach for more effective and reliable investment decision-making in the future.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们探讨了使用生成AI模型，如ChatGPT，来选择投资 portefolio的可能性。因为GPT模型的“幻觉”问题，使得对模型输出的信任具有挑战性，因此我们采取了一种不同的方法。我们使用ChatGPT来获取S&P500市场指数中可能有吸引力的股票，然后比较了不同的投资组合优化策略，包括使用这些AI生成的交易宇宙，与量化投资优化模型进行比较，以及与一些流行的投资基金进行比较。我们的发现表明，ChatGPT在股票选择方面是有效的，但可能不如在分配股票 weights 方面表现好。但当ChatGPT生成的股票选择与已有的量化优化模型相结合时，我们可以获得更好的投资结果。通过融合AI生成的股票选择和已有的量化优化技术，我们发现了一种更加有效和可靠的投资决策方法，建议将这种方法应用于未来的投资决策中。
</details></li>
</ul>
<hr>
<h2 id="Automated-Sizing-and-Training-of-Efficient-Deep-Autoencoders-using-Second-Order-Algorithms"><a href="#Automated-Sizing-and-Training-of-Efficient-Deep-Autoencoders-using-Second-Order-Algorithms" class="headerlink" title="Automated Sizing and Training of Efficient Deep Autoencoders using Second Order Algorithms"></a>Automated Sizing and Training of Efficient Deep Autoencoders using Second Order Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06221">http://arxiv.org/abs/2308.06221</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kanishka Tyagi, Chinmay Rane, Michael Manry</li>
<li>For: 本研究旨在提出一种多步训练方法，用于设计通用线性分类器。* Methods: 首先，通过回归获得初始多类线性分类器。然后，通过减少无用输入的方式，降低验证错误。同时，通过类似于霍-卡什洛夫规则的方法，提高 DESIRED 输出。接着，输出推定器被扩展为一个通用的线性分类器中的多层感知器。* Results: 通过组合剪枝和增长策略，提高输入单元的推定器，并将输出单元扩展为一个通用的线性分类器中的多层感知器。最后，通过改进每个深度学习块，提高整体深度学习模型的性能。<details>
<summary>Abstract</summary>
We propose a multi-step training method for designing generalized linear classifiers. First, an initial multi-class linear classifier is found through regression. Then validation error is minimized by pruning of unnecessary inputs. Simultaneously, desired outputs are improved via a method similar to the Ho-Kashyap rule. Next, the output discriminants are scaled to be net functions of sigmoidal output units in a generalized linear classifier. We then develop a family of batch training algorithm for the multi layer perceptron that optimizes its hidden layer size and number of training epochs. Next, we combine pruning with a growing approach. Later, the input units are scaled to be the net function of the sigmoidal output units that are then feed into as input to the MLP. We then propose resulting improvements in each of the deep learning blocks thereby improving the overall performance of the deep architecture. We discuss the principles and formulation regarding learning algorithms for deep autoencoders. We investigate several problems in deep autoencoders networks including training issues, the theoretical, mathematical and experimental justification that the networks are linear, optimizing the number of hidden units in each layer and determining the depth of the deep learning model. A direct implication of the current work is the ability to construct fast deep learning models using desktop level computational resources. This, in our opinion, promotes our design philosophy of building small but powerful algorithms. Performance gains are demonstrated at each step. Using widely available datasets, the final network's ten fold testing error is shown to be less than that of several other linear, generalized linear classifiers, multi layer perceptron and deep learners reported in the literature.
</details>
<details>
<summary>摘要</summary>
我们提出了一种多步训练方法用于设计通用线性分类器。首先，通过回归获得初始多类线性分类器。然后，通过减少不必要的输入，降低验证错误。同时，通过类似于霍-卡什纳规则的方法，提高期望的输出。接着，输出推定器被映射到通用线性分类器中的sigmoid输出单元。然后，我们开发了一家批处理训练算法，用于最优化多层感知器的隐藏层大小和训练轮次数。接着，我们结合剪除和增长方法。最后，输入单元被映射到sigmoid输出单元的网络中，并且这些输入单元被用作多层感知器的输入。我们then propose  several improvements in each deep learning block, leading to improved overall performance of the deep architecture. We discuss the principles and formulation of learning algorithms for deep autoencoders, and investigate several problems in deep autoencoder networks, including training issues, theoretical, mathematical, and experimental justification that the networks are linear, optimizing the number of hidden units in each layer, and determining the depth of the deep learning model. A direct implication of our work is the ability to construct fast deep learning models using desktop-level computational resources, which promotes our design philosophy of building small but powerful algorithms. Performance gains are demonstrated at each step. Using widely available datasets, the final network's ten-fold testing error is shown to be less than that of several other linear, generalized linear classifiers, multi-layer perceptron, and deep learners reported in the literature.
</details></li>
</ul>
<hr>
<h2 id="Safety-in-Traffic-Management-Systems-A-Comprehensive-Survey"><a href="#Safety-in-Traffic-Management-Systems-A-Comprehensive-Survey" class="headerlink" title="Safety in Traffic Management Systems: A Comprehensive Survey"></a>Safety in Traffic Management Systems: A Comprehensive Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06204">http://arxiv.org/abs/2308.06204</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenlu Du, Ankan Dash, Jing Li, Hua Wei, Guiling Wang</li>
<li>for: 这篇论文旨在提供对交通管理系统安全性的全面回顾，包括交通管理系统中出现的各种安全问题、当前研究的状况以及提高交通管理系统安全性的技术和方法。</li>
<li>methods: 论文使用了文献综述的方法，概括了交通管理系统中的安全问题，并分析了当前研究的状况和提议。</li>
<li>results: 论文总结了当前研究的结果和限制，并提出了未来研究的方向。<details>
<summary>Abstract</summary>
Traffic management systems play a vital role in ensuring safe and efficient transportation on roads. However, the use of advanced technologies in traffic management systems has introduced new safety challenges. Therefore, it is important to ensure the safety of these systems to prevent accidents and minimize their impact on road users. In this survey, we provide a comprehensive review of the literature on safety in traffic management systems. Specifically, we discuss the different safety issues that arise in traffic management systems, the current state of research on safety in these systems, and the techniques and methods proposed to ensure the safety of these systems. We also identify the limitations of the existing research and suggest future research directions.
</details>
<details>
<summary>摘要</summary>
交通管理系统在公路上的交通运输中发挥了关键作用，但是使用先进技术的交通管理系统引入了新的安全挑战。因此，确保交通管理系统的安全性是非常重要的，以避免事故和减少它们对公路用户的影响。在这份调查中，我们提供了交通管理系统安全的全面评论。 Specifically，我们讨论了交通管理系统中不同的安全问题，当前的研究进展、以及为确保交通管理系统安全的技术和方法。我们还识别了现有研究的限制，并建议未来的研究方向。Note: Please note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/12/cs.AI_2023_08_12/" data-id="clpxp03u20026fm88g48se63u" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_08_12" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/12/cs.CL_2023_08_12/" class="article-date">
  <time datetime="2023-08-12T11:00:00.000Z" itemprop="datePublished">2023-08-12</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/12/cs.CL_2023_08_12/">cs.CL - 2023-08-12</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="MT4CrossOIE-Multi-stage-Tuning-for-Cross-lingual-Open-Information-Extraction"><a href="#MT4CrossOIE-Multi-stage-Tuning-for-Cross-lingual-Open-Information-Extraction" class="headerlink" title="MT4CrossOIE: Multi-stage Tuning for Cross-lingual Open Information Extraction"></a>MT4CrossOIE: Multi-stage Tuning for Cross-lingual Open Information Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06552">http://arxiv.org/abs/2308.06552</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/CSJianYang/Multilingual-Multimodal-NLP/tree/main/MT4CrossOIE">https://github.com/CSJianYang/Multilingual-Multimodal-NLP/tree/main/MT4CrossOIE</a></li>
<li>paper_authors: Zixiang Wang, Linzheng Chai, Jian Yang, Jiaqi Bai, Yuwei Yin, Jiaheng Liu, Hongcheng Guo, Tongliang Li, Liqun Yang, Hebboul Zine el-abidine, Zhoujun Li</li>
<li>for: 提高多语言开放信息提取（Cross-Lingual Open Information Extraction，简称CrossIE）的效果，使得模型能够在不同语言的文本上提取结构化信息。</li>
<li>methods: 提出了一种多阶段调整框架MT4CrossIE，通过将语言特定的知识注入到共享模型中来提高crossIE的性能。</li>
<li>results: 实验结果表明，通过组合模型基于和数据基于的转移技术，MT4CrossIE可以在多种benchmark上提高crossIE的性能，并且多个语言特定模块的组合对crossIE的性能有积极的影响。<details>
<summary>Abstract</summary>
Cross-lingual open information extraction aims to extract structured information from raw text across multiple languages. Previous work uses a shared cross-lingual pre-trained model to handle the different languages but underuses the potential of the language-specific representation. In this paper, we propose an effective multi-stage tuning framework called MT4CrossIE, designed for enhancing cross-lingual open information extraction by injecting language-specific knowledge into the shared model. Specifically, the cross-lingual pre-trained model is first tuned in a shared semantic space (e.g., embedding matrix) in the fixed encoder and then other components are optimized in the second stage. After enough training, we freeze the pre-trained model and tune the multiple extra low-rank language-specific modules using mixture-of-LoRAs for model-based cross-lingual transfer. In addition, we leverage two-stage prompting to encourage the large language model (LLM) to annotate the multi-lingual raw data for data-based cross-lingual transfer. The model is trained with multi-lingual objectives on our proposed dataset OpenIE4++ by combing the model-based and data-based transfer techniques. Experimental results on various benchmarks emphasize the importance of aggregating multiple plug-in-and-play language-specific modules and demonstrate the effectiveness of MT4CrossIE in cross-lingual OIE\footnote{\url{https://github.com/CSJianYang/Multilingual-Multimodal-NLP}.
</details>
<details>
<summary>摘要</summary>
cross-lingual开放信息提取目标在多种语言之间提取结构化信息。先前的工作使用共享的cross-lingual预训练模型处理不同语言，但是未能充分利用语言特定表示的潜在优势。在这篇论文中，我们提出了一种高效的多阶段调整框架MT4CrossIE，用于提高cross-lingual开放信息提取。具体来说，在共享的semantic space（例如embedding matrix）中首先对cross-lingual预训练模型进行共享调整，然后其他组件在第二阶段进行优化。经过充分训练后，我们冻结预训练模型，并使用mixture-of-LoRAs进行模型基于cross-lingual传递。此外，我们利用两阶段提示来鼓励大语言模型（LLM）对多语言原始数据进行标注，以实现数据基于cross-lingual传递。我们在OpenIE4++数据集上训练了多语言目标，并结合模型基于和数据基于传递技术。实验结果在多个benchmark上表明，汇集多个插件和Play语言特定模块的重要性，并证明MT4CrossIE在cross-lingual OIE中的效果。
</details></li>
</ul>
<hr>
<h2 id="Alternative-Pseudo-Labeling-for-Semi-Supervised-Automatic-Speech-Recognition"><a href="#Alternative-Pseudo-Labeling-for-Semi-Supervised-Automatic-Speech-Recognition" class="headerlink" title="Alternative Pseudo-Labeling for Semi-Supervised Automatic Speech Recognition"></a>Alternative Pseudo-Labeling for Semi-Supervised Automatic Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06547">http://arxiv.org/abs/2308.06547</a></li>
<li>repo_url: None</li>
<li>paper_authors: Han Zhu, Dongji Gao, Gaofeng Cheng, Daniel Povey, Pengyuan Zhang, Yonghong Yan<br>for: 提高自动语音识别器的性能在半监督学习中，当标注数据稀缺时methods: 提议一种新的替代 pseudo-labeling 框架，包括一个通用的 CTC 损失函数、验证错误 Pseudo-label 的方法和自动调整 thresholdresults: 在实验中，该框架可以在半监督学习中提高自动语音识别器的性能，并且可以自动调整 threshold，避免手动调整 threshold 的痛苦<details>
<summary>Abstract</summary>
When labeled data is insufficient, semi-supervised learning with the pseudo-labeling technique can significantly improve the performance of automatic speech recognition. However, pseudo-labels are often noisy, containing numerous incorrect tokens. Taking noisy labels as ground-truth in the loss function results in suboptimal performance. Previous works attempted to mitigate this issue by either filtering out the nosiest pseudo-labels or improving the overall quality of pseudo-labels. While these methods are effective to some extent, it is unrealistic to entirely eliminate incorrect tokens in pseudo-labels. In this work, we propose a novel framework named alternative pseudo-labeling to tackle the issue of noisy pseudo-labels from the perspective of the training objective. The framework comprises several components. Firstly, a generalized CTC loss function is introduced to handle noisy pseudo-labels by accepting alternative tokens in the positions of incorrect tokens. Applying this loss function in pseudo-labeling requires detecting incorrect tokens in the predicted pseudo-labels. In this work, we adopt a confidence-based error detection method that identifies the incorrect tokens by comparing their confidence scores with a given threshold, thus necessitating the confidence score to be discriminative. Hence, the second proposed technique is the contrastive CTC loss function that widens the confidence gap between the correctly and incorrectly predicted tokens, thereby improving the error detection ability. Additionally, obtaining satisfactory performance with confidence-based error detection typically requires extensive threshold tuning. Instead, we propose an automatic thresholding method that uses labeled data as a proxy for determining the threshold, thus saving the pain of manual tuning.
</details>
<details>
<summary>摘要</summary>
当标注数据不足时，半超vised学习使用pseudo-labeling技术可以显著提高自动语音识别的性能。然而，pseudo-labels经常含有许多错误的标签。使用含有错误标签的labels作为损失函数中的参考数据会导致优化性能的问题。先前的工作已经尝试过抑制这个问题，可以是通过筛选pseudo-labels中的最噪音标签，或者提高pseudo-labels的质量。尽管这些方法有一定的效果，但是完全消除pseudo-labels中的错误标签是不现实的。在这种情况下，我们提出了一种新的框架，即代理pseudo-labeling。该框架包括以下几个组成部分：1. 一种通用的CTC损失函数，可以处理含有错误标签的pseudo-labels。该损失函数可以接受pseudo-labels中的错误标签，并且可以在预测过程中检测错误标签。2. 一种信息储存基于错误检测方法，可以在预测过程中检测pseudo-labels中的错误标签。该方法通过比较预测的信息储存与给定的阈值进行比较，以确定错误标签。3. 一种自动调整阈值的方法，可以使用标注数据作为代理，以便不需要手动调整阈值。这种新的框架可以减少因为使用含有错误标签的labels而导致的优化性能问题，并且可以在不完全消除pseudo-labels中的错误标签的情况下提高自动语音识别的性能。
</details></li>
</ul>
<hr>
<h2 id="With-a-Little-Help-from-the-Authors-Reproducing-Human-Evaluation-of-an-MT-Error-Detector"><a href="#With-a-Little-Help-from-the-Authors-Reproducing-Human-Evaluation-of-an-MT-Error-Detector" class="headerlink" title="With a Little Help from the Authors: Reproducing Human Evaluation of an MT Error Detector"></a>With a Little Help from the Authors: Reproducing Human Evaluation of an MT Error Detector</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06527">http://arxiv.org/abs/2308.06527</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ondřej Plátek, Mateusz Lango, Ondřej Dušek</li>
<li>for: 本研究重新实现了瓦姆瓦斯和塞恩兹（2022）的人工评估实验，该实验evaluated一个自动检测机器翻译输出中的过度和不足翻译（翻译包含更多或更少信息 than the original）的自动系统。</li>
<li>methods: 我们使用了作者提供的文档和代码，但在重新实现实验setup时，我们发现了一些问题，并提供了改进可重现性的建议。</li>
<li>results: 我们的复制结果大致与原始研究的结论相符，但在某些情况下，我们 observeda statistically significant differences，表明了人工标注的高变异性。<details>
<summary>Abstract</summary>
This work presents our efforts to reproduce the results of the human evaluation experiment presented in the paper of Vamvas and Sennrich (2022), which evaluated an automatic system detecting over- and undertranslations (translations containing more or less information than the original) in machine translation (MT) outputs. Despite the high quality of the documentation and code provided by the authors, we discuss some problems we found in reproducing the exact experimental setup and offer recommendations for improving reproducibility. Our replicated results generally confirm the conclusions of the original study, but in some cases, statistically significant differences were observed, suggesting a high variability of human annotation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="AutoConv-Automatically-Generating-Information-seeking-Conversations-with-Large-Language-Models"><a href="#AutoConv-Automatically-Generating-Information-seeking-Conversations-with-Large-Language-Models" class="headerlink" title="AutoConv: Automatically Generating Information-seeking Conversations with Large Language Models"></a>AutoConv: Automatically Generating Information-seeking Conversations with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06507">http://arxiv.org/abs/2308.06507</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siheng Li, Cheng Yang, Yichun Yin, Xinyu Zhu, Zesen Cheng, Lifeng Shang, Xin Jiang, Qun Liu, Yujiu Yang</li>
<li>for: 提高信息寻求对话生成的训练数据稀缺性问题</li>
<li>methods: 利用大语言模型几何学习和生成能力，将对话生成问题定义为语言模型预测问题，并在几个人对话的基础上训练语言模型来捕捉信息寻求过程的特征，生成高质量的synthetic对话</li>
<li>results: 对两个常用的数据集进行实验，证明AutoConv具有显著的提升和减少人工标注的依赖性<details>
<summary>Abstract</summary>
Information-seeking conversation, which aims to help users gather information through conversation, has achieved great progress in recent years. However, the research is still stymied by the scarcity of training data. To alleviate this problem, we propose AutoConv for synthetic conversation generation, which takes advantage of the few-shot learning ability and generation capacity of large language models (LLM). Specifically, we formulate the conversation generation problem as a language modeling task, then finetune an LLM with a few human conversations to capture the characteristics of the information-seeking process and use it for generating synthetic conversations with high quality. Experimental results on two frequently-used datasets verify that AutoConv has substantial improvements over strong baselines and alleviates the dependence on human annotation. In addition, we also provide several analysis studies to promote future research.
</details>
<details>
<summary>摘要</summary>
信息寻求对话，目前已经取得了很大的进步，但研究还面临着数据缺乏的问题。为解决这个问题，我们提出了AutoConv，它利用大语言模型（LLM）的几shot学习能力和生成能力来生成高质量的人工对话。具体来说，我们将对话生成问题定义为语言模型化问题，然后使用一些人类对话来训练LLM，以capture信息寻求过程中的特点。实验结果表明，AutoConv在两个常用的数据集上具有显著的提升和减少人类注释的依赖性。此外，我们还提供了一些分析研究，以便未来的研究。
</details></li>
</ul>
<hr>
<h2 id="NewsDialogues-Towards-Proactive-News-Grounded-Conversation"><a href="#NewsDialogues-Towards-Proactive-News-Grounded-Conversation" class="headerlink" title="NewsDialogues: Towards Proactive News Grounded Conversation"></a>NewsDialogues: Towards Proactive News Grounded Conversation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06501">http://arxiv.org/abs/2308.06501</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sihengli99/newsdialogues">https://github.com/sihengli99/newsdialogues</a></li>
<li>paper_authors: Siheng Li, Yichun Yin, Cheng Yang, Wangjie Jiang, Yiwei Li, Zesen Cheng, Lifeng Shang, Xin Jiang, Qun Liu, Yujiu Yang</li>
<li>for: 本研究旨在提出一种新任务——积极新闻附加对话，以便对话系统可以主动领导对话，基于新闻中的一些关键话题。</li>
<li>methods: 本研究使用了一种名为Predict-Generate-Rank的方法，包括一个生成器用于预测和生成基于新闻的知识，以及一个排名器用于对多个回答进行排名，以避免曝光偏见。</li>
<li>results: 经过广泛的实验，研究发现了一些关键发现和挑战，以及提出了未来研究的一些方向。<details>
<summary>Abstract</summary>
Hot news is one of the most popular topics in daily conversations. However, news grounded conversation has long been stymied by the lack of well-designed task definition and scarce data. In this paper, we propose a novel task, Proactive News Grounded Conversation, in which a dialogue system can proactively lead the conversation based on some key topics of the news. In addition, both information-seeking and chit-chat scenarios are included realistically, where the user may ask a series of questions about the news details or express their opinions and be eager to chat. To further develop this novel task, we collect a human-to-human Chinese dialogue dataset \ts{NewsDialogues}, which includes 1K conversations with a total of 14.6K utterances and detailed annotations for target topics and knowledge spans. Furthermore, we propose a method named Predict-Generate-Rank, consisting of a generator for grounded knowledge prediction and response generation, and a ranker for the ranking of multiple responses to alleviate the exposure bias. We conduct comprehensive experiments to demonstrate the effectiveness of the proposed method and further present several key findings and challenges to prompt future research.
</details>
<details>
<summary>摘要</summary>
热门新闻是日常对话中最受欢迎的话题之一，然而新闻基于对话的探讨长期受到缺乏有效定义任务和珍贵数据的限制。在这篇论文中，我们提出了一个新任务：主动新闻基于对话（Proactive News Grounded Conversation），在其中对话系统可以主动引导对话，基于新闻中的一些关键话题。此外，我们还包括了信息寻求和聊天场景，用户可能会提问新闻细节的多个问题或表达自己的意见并且很有兴趣聊天。为了进一步开发这个新任务，我们收集了一个人类对话 dataset 《NewsDialogues》，该 dataset 包含了1000个对话，总共14600个语音和详细的注释目标话题和知识范围。此外，我们还提出了一种方法，即预测生成排名（Predict-Generate-Rank），该方法包括一个生成基于新闻的预测和回答生成器，以及一个排名器用于多个答案的排名，以降低曝光偏见。我们进行了广泛的实验，以示提出的方法的效iveness，并提出了一些关键发现和未来研究的挑战。
</details></li>
</ul>
<hr>
<h2 id="GPT-4-Is-Too-Smart-To-Be-Safe-Stealthy-Chat-with-LLMs-via-Cipher"><a href="#GPT-4-Is-Too-Smart-To-Be-Safe-Stealthy-Chat-with-LLMs-via-Cipher" class="headerlink" title="GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher"></a>GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06463">http://arxiv.org/abs/2308.06463</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/robustnlp/cipherchat">https://github.com/robustnlp/cipherchat</a></li>
<li>paper_authors: Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Pinjia He, Shuming Shi, Zhaopeng Tu</li>
<li>for: 本研究旨在探讨大语言模型（LLMs）的安全定制是否可以扩展到非自然语言（cipher）领域。</li>
<li>methods: 我们提出了一种名为 CipherChat 的新框架，允许人类通过密码提示和几个几shot 加密示例与 LLMs 进行交流。我们使用 CipherChat 评估当今最先进的 LLMs，包括 ChatGPT 和 GPT-4，在不同的人类密码下的11个安全领域中的表现。</li>
<li>results: 实验结果表明，某些密码可以在某些安全领域中绕过 GPT-4 的安全定制，这说明了在非自然语言领域中的安全定制的必要性。此外，我们发现 LLMs 似乎有一种’’秘密密码’’，并提出了一种名为 SelfCipher 的新方法，可以通过角色扮演和几个示例来触发这种能力。SelfCipher  surprisingly 在大多数情况下超过了现有的人类密码。我们将代码和数据发布在 GitHub 上（<a target="_blank" rel="noopener" href="https://github.com/RobustNLP/CipherChat">https://github.com/RobustNLP/CipherChat</a>）。<details>
<summary>Abstract</summary>
Safety lies at the core of the development of Large Language Models (LLMs). There is ample work on aligning LLMs with human ethics and preferences, including data filtering in pretraining, supervised fine-tuning, reinforcement learning from human feedback, and red teaming, etc. In this study, we discover that chat in cipher can bypass the safety alignment techniques of LLMs, which are mainly conducted in natural languages. We propose a novel framework CipherChat to systematically examine the generalizability of safety alignment to non-natural languages -- ciphers. CipherChat enables humans to chat with LLMs through cipher prompts topped with system role descriptions and few-shot enciphered demonstrations. We use CipherChat to assess state-of-the-art LLMs, including ChatGPT and GPT-4 for different representative human ciphers across 11 safety domains in both English and Chinese. Experimental results show that certain ciphers succeed almost 100% of the time to bypass the safety alignment of GPT-4 in several safety domains, demonstrating the necessity of developing safety alignment for non-natural languages. Notably, we identify that LLMs seem to have a ''secret cipher'', and propose a novel SelfCipher that uses only role play and several demonstrations in natural language to evoke this capability. SelfCipher surprisingly outperforms existing human ciphers in almost all cases. Our code and data will be released at https://github.com/RobustNLP/CipherChat.
</details>
<details>
<summary>摘要</summary>
安全是大语言模型（LLM）的核心发展之一。有很多工作在将 LLM 与人类伦理和偏好相匹配，包括预处理数据筛选、监督练习、人类反馈强化学习和红团等等。在这项研究中，我们发现可以通过密码来绕过 LLM 的安全对齐技术，这些技术主要是在自然语言上进行的。我们提出了一个新的框架 CipherChat，用于系统地检验非自然语言（密码）上 LLM 的安全对齐可行性。CipherChat 允许人们通过密码提示和系统角色描述以及几个加密示例来与 LLM 进行交流。我们使用 CipherChat 测试了当前的状态体 LLM，包括 ChatGPT 和 GPT-4，在不同的人类密码上进行了11个安全领域的测试。实验结果显示，某些密码可以在多个安全领域中绕过 GPT-4 的安全对齐，这说明了非自然语言的安全对齐的必要性。另外，我们发现 LLM 似乎有一个“秘密密码”，我们提出了一种新的 SelfCipher，只需要通过角色扮演和几个示例来诱发这种能力。SelfCipher  surprisingly 在大多数情况下超过了现有的人类密码。我们的代码和数据将在 GitHub 上发布。
</details></li>
</ul>
<hr>
<h2 id="Text-to-Video-a-Two-stage-Framework-for-Zero-shot-Identity-agnostic-Talking-head-Generation"><a href="#Text-to-Video-a-Two-stage-Framework-for-Zero-shot-Identity-agnostic-Talking-head-Generation" class="headerlink" title="Text-to-Video: a Two-stage Framework for Zero-shot Identity-agnostic Talking-head Generation"></a>Text-to-Video: a Two-stage Framework for Zero-shot Identity-agnostic Talking-head Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06457">http://arxiv.org/abs/2308.06457</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhichaowang970201/text-to-video">https://github.com/zhichaowang970201/text-to-video</a></li>
<li>paper_authors: Zhichao Wang, Mengyu Dai, Keld Lundgaard</li>
<li>for: 本研究旨在提供一种基于文本的视频创建方法，具体来说是一种人脸无关的视频审核方法，以便在不同的语言和语速下生成可观看的视频。</li>
<li>methods: 本研究提出了一种两stage的方法，包括文本至语音转化和语音驱动的人脸讲话生成。在第一阶段，我们利用预训练的零shot模型实现文本至语音转化。在第二阶段，我们使用语音驱动的人脸讲话生成方法，以生成有趣的视频。</li>
<li>results: 本研究通过对不同的文本和语音样本进行比较分析，找到了最佳的文本至语音转化和语音驱动的人脸讲话生成方法。此外，我们还提供了一些Audio和视频示例，可以在以下链接中找到：<a target="_blank" rel="noopener" href="https://github.com/ZhichaoWang970201/Text-to-Video/tree/main%E3%80%82">https://github.com/ZhichaoWang970201/Text-to-Video/tree/main。</a><details>
<summary>Abstract</summary>
The advent of ChatGPT has introduced innovative methods for information gathering and analysis. However, the information provided by ChatGPT is limited to text, and the visualization of this information remains constrained. Previous research has explored zero-shot text-to-video (TTV) approaches to transform text into videos. However, these methods lacked control over the identity of the generated audio, i.e., not identity-agnostic, hindering their effectiveness. To address this limitation, we propose a novel two-stage framework for person-agnostic video cloning, specifically focusing on TTV generation. In the first stage, we leverage pretrained zero-shot models to achieve text-to-speech (TTS) conversion. In the second stage, an audio-driven talking head generation method is employed to produce compelling videos privided the audio generated in the first stage. This paper presents a comparative analysis of different TTS and audio-driven talking head generation methods, identifying the most promising approach for future research and development. Some audio and videos samples can be found in the following link: https://github.com/ZhichaoWang970201/Text-to-Video/tree/main.
</details>
<details>
<summary>摘要</summary>
随着ChatGPT的出现，新的信息收集和分析方法得到了推动。然而，ChatGPT提供的信息仅限于文本，视觉化这些信息仍然受限。先前的研究曾经 explore zero-shot文本到视频（TTV）方法，将文本转换成视频。然而，这些方法缺乏控制音频个体的能力，妨碍其效iveness。为了解决这个限制，我们提议一种新的两阶段框架，专门针对人具无关的视频副本。在第一阶段，我们利用预训练的零shot模型实现文本到语音（TTS）转换。在第二阶段，我们使用音频驱动的人物头部生成方法生成有吸引力的视频，只要提供在第一阶段生成的音频。本文对不同的TTS和音频驱动人物头部生成方法进行比较分析，并确定未来研究和发展的最佳方法。有关音频和视频样例，请参考以下链接：https://github.com/ZhichaoWang970201/Text-to-Video/tree/main。
</details></li>
</ul>
<hr>
<h2 id="Demonstration-based-learning-for-few-shot-biomedical-named-entity-recognition-under-machine-reading-comprehension"><a href="#Demonstration-based-learning-for-few-shot-biomedical-named-entity-recognition-under-machine-reading-comprehension" class="headerlink" title="Demonstration-based learning for few-shot biomedical named entity recognition under machine reading comprehension"></a>Demonstration-based learning for few-shot biomedical named entity recognition under machine reading comprehension</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06454">http://arxiv.org/abs/2308.06454</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leilei Su, Jian Chen, Yifan Peng, Cong Sun</li>
<li>for: 提高几少 BioNER 模型的识别能力</li>
<li>methods: 利用示例学习方法，将 BioNER 转化为机器阅读理解问题</li>
<li>results: 在6个数据集上，与基eline方法比较，提高了1.1%和1.0%的平均 F1 分数，并且可以与大量注释数据的完全监督学习方法竞争<details>
<summary>Abstract</summary>
Although deep learning techniques have shown significant achievements, they frequently depend on extensive amounts of hand-labeled data and tend to perform inadequately in few-shot scenarios. The objective of this study is to devise a strategy that can improve the model's capability to recognize biomedical entities in scenarios of few-shot learning. By redefining biomedical named entity recognition (BioNER) as a machine reading comprehension (MRC) problem, we propose a demonstration-based learning method to address few-shot BioNER, which involves constructing appropriate task demonstrations. In assessing our proposed method, we compared the proposed method with existing advanced methods using six benchmark datasets, including BC4CHEMD, BC5CDR-Chemical, BC5CDR-Disease, NCBI-Disease, BC2GM, and JNLPBA. We examined the models' efficacy by reporting F1 scores from both the 25-shot and 50-shot learning experiments. In 25-shot learning, we observed 1.1% improvements in the average F1 scores compared to the baseline method, reaching 61.7%, 84.1%, 69.1%, 70.1%, 50.6%, and 59.9% on six datasets, respectively. In 50-shot learning, we further improved the average F1 scores by 1.0% compared to the baseline method, reaching 73.1%, 86.8%, 76.1%, 75.6%, 61.7%, and 65.4%, respectively. We reported that in the realm of few-shot learning BioNER, MRC-based language models are much more proficient in recognizing biomedical entities compared to the sequence labeling approach. Furthermore, our MRC-language models can compete successfully with fully-supervised learning methodologies that rely heavily on the availability of abundant annotated data. These results highlight possible pathways for future advancements in few-shot BioNER methodologies.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:尽管深度学习技术已经达到了显著的成就，但它们往往需要大量的手动标注数据，并在几个shot场景下表现不佳。本研究的目标是提出一种策略，以提高模型在几个shot学习中识别生物医学实体的能力。我们将生物医学命名实体识别（BioNER）定义为机器阅读理解（MRC）问题，并提出了一种示例学习方法来解决几个shot BioNER。我们使用了六个标准测试集来评估我们的提议方法，包括BC4CHEMD、BC5CDR-Chemical、BC5CDR-疾病、NCBI-疾病、BC2GM和JNLPBA。我们根据25个shot和50个shot的学习实验来评估模型的效果，并发现在25个shot学习中，我们的方法与基eline方法相比，平均F1分数提高了1.1%，达到了61.7%、84.1%、69.1%、70.1%、50.6%和59.9%的水平。在50个shot学习中，我们进一步提高了平均F1分数，达到了73.1%、86.8%、76.1%、75.6%、61.7%和65.4%的水平。我们发现，在几个shot BioNER中，基于MRC语言模型的方法比sequence标注方法更有才能地识别生物医学实体。此外，我们的MRC语言模型可以与充分监督学习方法竞争，这些方法依赖于大量的注释数据的可用性。这些结果透视了未来几个shot BioNER方法的可能发展道路。
</details></li>
</ul>
<hr>
<h2 id="Simple-Model-Also-Works-A-Novel-Emotion-Recognition-Network-in-Textual-Conversation-Based-on-Curriculum-Learning-Strategy"><a href="#Simple-Model-Also-Works-A-Novel-Emotion-Recognition-Network-in-Textual-Conversation-Based-on-Curriculum-Learning-Strategy" class="headerlink" title="Simple Model Also Works: A Novel Emotion Recognition Network in Textual Conversation Based on Curriculum Learning Strategy"></a>Simple Model Also Works: A Novel Emotion Recognition Network in Textual Conversation Based on Curriculum Learning Strategy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06450">http://arxiv.org/abs/2308.06450</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiang Li, Xiaoping Wang, Yingjian Liu, Qing Zhou, Zhigang Zeng</li>
<li>for: 本研究主要针对对话中的情感识别 зада项 (Emotion Recognition in Conversation, ERC) 进行研究，以提高情感识别的效率和精度。</li>
<li>methods: 本研究提出了一个基于学习范例 (Curriculum Learning, CL) 的情感识别网络 (Emotion Recognition Network, ERNetCL)，并使用时间编码 (Temporal Encoder, TE) 和空间编码 (Spatial Encoder, SE) 来融合先前的方法，以优化情感识别的效率和精度。</li>
<li>results: 实验结果显示，本研究的提案方法可以优化情感识别的效率和精度，并与其他基于标准方法的方法相比，具有明显的性能优化。<details>
<summary>Abstract</summary>
Emotion Recognition in Conversation (ERC) has emerged as a research hotspot in domains such as conversational robots and question-answer systems. How to efficiently and adequately retrieve contextual emotional cues has been one of the key challenges in the ERC task. Existing efforts do not fully model the context and employ complex network structures, resulting in excessive computational resource overhead without substantial performance improvement. In this paper, we propose a novel Emotion Recognition Network based on Curriculum Learning strategy (ERNetCL). The proposed ERNetCL primarily consists of Temporal Encoder (TE), Spatial Encoder (SE), and Curriculum Learning (CL) loss. We utilize TE and SE to combine the strengths of previous methods in a simplistic manner to efficiently capture temporal and spatial contextual information in the conversation. To simulate the way humans learn curriculum from easy to hard, we apply the idea of CL to the ERC task to progressively optimize the network parameters of ERNetCL. At the beginning of training, we assign lower learning weights to difficult samples. As the epoch increases, the learning weights for these samples are gradually raised. Extensive experiments on four datasets exhibit that our proposed method is effective and dramatically beats other baseline models.
</details>
<details>
<summary>摘要</summary>
《对话中情感识别（ERC）》在领域如会话机器人和问答系统中已经成为研究热点。 efficiently和准确地检索上下文情感cue是ERC任务中的关键挑战。现有尝试没有完全考虑上下文，使用复杂的网络结构，导致计算资源占用过高而无法提供明显的性能提升。本文提出了一种基于学习纲程（CL）的情感识别网络（ERNetCL）。我们利用TE和SE组合previous方法的优点，以简单的方式高效地捕捉对话中的时间和空间上下文信息。通过模仿人类学习纲程的思想，我们在ERC任务中应用CL来逐渐优化ERNetCL的网络参数。在训练的开始时，我们将难度较高的样本分配低学习权重。随着epoch增加，这些样本的学习权重逐渐升高。我们在四个数据集进行了广泛的实验，结果显示，我们的提议方法效果明显，可以很好地超越基准模型。
</details></li>
</ul>
<hr>
<h2 id="Performance-Prediction-for-Multi-hop-Questions"><a href="#Performance-Prediction-for-Multi-hop-Questions" class="headerlink" title="Performance Prediction for Multi-hop Questions"></a>Performance Prediction for Multi-hop Questions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06431">http://arxiv.org/abs/2308.06431</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammadreza Samadi, Davood Rafiei</li>
<li>for: 预测开放领域多步问答（QA）问题的评估难度。</li>
<li>methods: 提出了一种新的预测方法multHP，用于预测开放领域多步问答问题的表现。</li>
<li>results: 对largest multi-hop QA数据集进行了广泛的评估，并显示了提档的表现，比传统单步QPP模型更好。 Additionally, the approach can be effectively used to optimize the parameters of QA systems, such as the number of documents to be retrieved, resulting in improved overall retrieval performance.<details>
<summary>Abstract</summary>
We study the problem of Query Performance Prediction (QPP) for open-domain multi-hop Question Answering (QA), where the task is to estimate the difficulty of evaluating a multi-hop question over a corpus. Despite the extensive research on predicting the performance of ad-hoc and QA retrieval models, there has been a lack of study on the estimation of the difficulty of multi-hop questions. The problem is challenging due to the multi-step nature of the retrieval process, potential dependency of the steps and the reasoning involved. To tackle this challenge, we propose multHP, a novel pre-retrieval method for predicting the performance of open-domain multi-hop questions. Our extensive evaluation on the largest multi-hop QA dataset using several modern QA systems shows that the proposed model is a strong predictor of the performance, outperforming traditional single-hop QPP models. Additionally, we demonstrate that our approach can be effectively used to optimize the parameters of QA systems, such as the number of documents to be retrieved, resulting in improved overall retrieval performance.
</details>
<details>
<summary>摘要</summary>
我们研究了开放领域多步问答（QA）中的问题评估性能预测（QPP）问题，即估计评估一个多步问题的难度。尽管有很多关于预测广泛问答和搜索模型性能的研究，但是没有研究了多步问题的预测。这个问题具有多步搜索过程的多样性和步骤之间的依赖关系，以及需要进行推理。为解决这个挑战，我们提出了 multHP，一种新的预测开放领域多步问题性能的方法。我们对最大的多步问答数据集进行了广泛的评估，结果表明，我们提出的模型是一个强大的性能预测器，超过了传统单步 QPP 模型。此外，我们还证明了我们的方法可以有效地用于优化 QA 系统的参数，例如检索文档的数量，从而提高总体检索性能。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Planning-with-a-LLM"><a href="#Dynamic-Planning-with-a-LLM" class="headerlink" title="Dynamic Planning with a LLM"></a>Dynamic Planning with a LLM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06391">http://arxiv.org/abs/2308.06391</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/itl-ed/llm-dp">https://github.com/itl-ed/llm-dp</a></li>
<li>paper_authors: Gautier Dagan, Frank Keller, Alex Lascarides</li>
<li>for: 解决 embodied agent 应用问题，尤其是复杂的计划需要多步骤的情况。</li>
<li>methods: 融合 neural network 和 symbolic planner，使用 LLM 和 traditional planner 共同解决 embodied task。</li>
<li>results: LLM-DP 比 naive LLM ReAct baseline 更快和更高效地解决 Alfworld 问题。<details>
<summary>Abstract</summary>
While Large Language Models (LLMs) can solve many NLP tasks in zero-shot settings, applications involving embodied agents remain problematic. In particular, complex plans that require multi-step reasoning become difficult and too costly as the context window grows. Planning requires understanding the likely effects of one's actions and identifying whether the current environment satisfies the goal state. While symbolic planners find optimal solutions quickly, they require a complete and accurate representation of the planning problem, severely limiting their use in practical scenarios. In contrast, modern LLMs cope with noisy observations and high levels of uncertainty when reasoning about a task. Our work presents LLM Dynamic Planner (LLM-DP): a neuro-symbolic framework where an LLM works hand-in-hand with a traditional planner to solve an embodied task. Given action-descriptions, LLM-DP solves Alfworld faster and more efficiently than a naive LLM ReAct baseline.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）可以解决许多自然语言处理任务在零模式下，但是包含身体代理的应用仍然是问题。特别是复杂的计划需要多步骤的理解和识别环境是否满足目标状态。而符号计划器可以快速找到优化解决方案，但它们需要完整和准确地表示计划问题，因此在实际场景中几乎无法使用。相比之下，现代LLM在面临噪音观察和高度不确定性时仍然能够有效地理解任务。我们的工作提出了LLM动态规划器（LLM-DP）：一种神经符号框架，在LLM与传统计划器之间协作解决身体任务。给出动作描述，LLM-DP比预期的LLM ReAct基线更快和高效地解决了Alfworld任务。
</details></li>
</ul>
<hr>
<h2 id="Bilingual-Streaming-ASR-with-Grapheme-units-and-Auxiliary-Monolingual-Loss"><a href="#Bilingual-Streaming-ASR-with-Grapheme-units-and-Auxiliary-Monolingual-Loss" class="headerlink" title="Bilingual Streaming ASR with Grapheme units and Auxiliary Monolingual Loss"></a>Bilingual Streaming ASR with Grapheme units and Auxiliary Monolingual Loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06327">http://arxiv.org/abs/2308.06327</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Soleymanpour, Mahmoud Al Ismail, Fahimeh Bahmaninezhad, Kshitiz Kumar, Jian Wu</li>
<li>for: 这个研究旨在提供一个英文作为次要区域的混合自动语音识别（ASR）设置中的双语解决方案。</li>
<li>methods: 我们的主要开发包括： (a) 发音词库使用文字单位而不是语音单位， (b) 完全双语对焦模型和随后的双语流式变数模型， (c) 平行Encoder结构 WITH 语言识别（LID）损失， (d) 平行Encoder WITH 辅助损失 для单语言预测。</li>
<li>results: 我们的工作在大规模训练和测试任务中显示出强大的英文混合能力。特别是双语IT模型在一个混合IT任务中从46.5%降至13.8%，同时也与单语IT模型（9.5%）在IT测试中仅差0.6%。<details>
<summary>Abstract</summary>
We introduce a bilingual solution to support English as secondary locale for most primary locales in hybrid automatic speech recognition (ASR) settings. Our key developments constitute: (a) pronunciation lexicon with grapheme units instead of phone units, (b) a fully bilingual alignment model and subsequently bilingual streaming transformer model, (c) a parallel encoder structure with language identification (LID) loss, (d) parallel encoder with an auxiliary loss for monolingual projections. We conclude that in comparison to LID loss, our proposed auxiliary loss is superior in specializing the parallel encoders to respective monolingual locales, and that contributes to stronger bilingual learning. We evaluate our work on large-scale training and test tasks for bilingual Spanish (ES) and bilingual Italian (IT) applications. Our bilingual models demonstrate strong English code-mixing capability. In particular, the bilingual IT model improves the word error rate (WER) for a code-mix IT task from 46.5% to 13.8%, while also achieving a close parity (9.6%) with the monolingual IT model (9.5%) over IT tests.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种双语解决方案，以英语为次要地区的hybrid自动语音识别（ASR）设置中支持英语。我们的关键发展包括：（a）使用字母单位而不是语音单位的发音词典，（b）完全双语对应模型和随后的双语流Transformer模型，（c）并行编码结构和语言标识（LID）损失，（d）并行编码器和辅助损失 для单语投影。我们认为，相比LID损失，我们提posed的辅助损失可以更好地特化并行编码器到各自的单语本地，从而为双语学习带来更强的特点。我们对大规模训练和测试任务进行了两种双语西班牙（ES）和双语意大利（IT）应用。我们的双语模型在英语混合代码 Task中显示出了强大的英语混合能力。特别是，双语IT模型将IT任务中的代码混合WER从46.5%降低至13.8%，同时也与单语IT模型（9.5%）在IT测试任务上达到了近似的水平（9.6%）。
</details></li>
</ul>
<hr>
<h2 id="Self-Alignment-with-Instruction-Backtranslation"><a href="#Self-Alignment-with-Instruction-Backtranslation" class="headerlink" title="Self-Alignment with Instruction Backtranslation"></a>Self-Alignment with Instruction Backtranslation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06259">http://arxiv.org/abs/2308.06259</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Spico197/Humback">https://github.com/Spico197/Humback</a></li>
<li>paper_authors: Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason Weston, Mike Lewis</li>
<li>for: 这个论文是为了提高语言模型的质量而写的（improve the quality of language models）。</li>
<li>methods: 这个论文使用自动生成的指令Prompt来自我增强语言模型（instruction backtranslation）。</li>
<li>results: 这个论文的方法可以高效地自我增强语言模型，并且在Alpaca领导者榜单上表现更好于所有不使用热静质量数据的LLaMa模型（outperforms all other LLaMa-based models on the Alpaca leaderboard not relying on distillation data）。<details>
<summary>Abstract</summary>
We present a scalable method to build a high quality instruction following language model by automatically labelling human-written text with corresponding instructions. Our approach, named instruction backtranslation, starts with a language model finetuned on a small amount of seed data, and a given web corpus. The seed model is used to construct training examples by generating instruction prompts for web documents (self-augmentation), and then selecting high quality examples from among these candidates (self-curation). This data is then used to finetune a stronger model. Finetuning LLaMa on two iterations of our approach yields a model that outperforms all other LLaMa-based models on the Alpaca leaderboard not relying on distillation data, demonstrating highly effective self-alignment.
</details>
<details>
<summary>摘要</summary>
我们提出了一种可扩展的方法，用于建立高质量的指令遵循语言模型，通过自动将人工写好的文本标记为相应的指令。我们的方法名为指令反翻译。我们的方法开始于一个基于小量种子数据的语言模型，并使用给定的网络资料。这个种子模型用于生成指令提示文本（自我扩充），然后选择高质量的示例从中间（自我审核）。这些数据然后用于训练更强的模型。两轮我们的方法训练后，我们的模型在不使用热静质料的情况下在Alpaca排行榜上表现最佳， demonstarting highly effective self-alignment。
</details></li>
</ul>
<hr>
<h2 id="KETM-A-Knowledge-Enhanced-Text-Matching-method"><a href="#KETM-A-Knowledge-Enhanced-Text-Matching-method" class="headerlink" title="KETM:A Knowledge-Enhanced Text Matching method"></a>KETM:A Knowledge-Enhanced Text Matching method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06235">http://arxiv.org/abs/2308.06235</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/1094701018/ketm">https://github.com/1094701018/ketm</a></li>
<li>paper_authors: Kexin Jiang, Yahui Zhao, Guozhe Jin, Zhenguo Zhang, Rongyi Cui</li>
<li>for: 这个论文是为了提高文本匹配 task 的性能，通过增强模型理解和逻辑能力。</li>
<li>methods: 该模型使用 Wiktionary  retrieve 文本单词定义作为外部知识，并通过多angle pooling 提取文本和知识的特征向量。然后，通过权重门限机制将文本和知识进行权重 fusión，以提高模型的理解和逻辑能力。</li>
<li>results: 在四个 datasets 上进行了实验验证，结果显示，该模型在所有四个 datasets 上都表现良好，并且与不添加外部知识的基本模型相比，该模型的性能有所提高，这证明了该模型的有效性。<details>
<summary>Abstract</summary>
Text matching is the task of matching two texts and determining the relationship between them, which has extensive applications in natural language processing tasks such as reading comprehension, and Question-Answering systems. The mainstream approach is to compute text representations or to interact with the text through attention mechanism, which is effective in text matching tasks. However, the performance of these models is insufficient for texts that require commonsense knowledge-based reasoning. To this end, in this paper, We introduce a new model for text matching called the Knowledge Enhanced Text Matching model (KETM), to enrich contextual representations with real-world common-sense knowledge from external knowledge sources to enhance our model understanding and reasoning. First, we use Wiktionary to retrieve the text word definitions as our external knowledge. Secondly, we feed text and knowledge to the text matching module to extract their feature vectors. The text matching module is used as an interaction module by integrating the encoder layer, the co-attention layer, and the aggregation layer. Specifically, the interaction process is iterated several times to obtain in-depth interaction information and extract the feature vectors of text and knowledge by multi-angle pooling. Then, we fuse text and knowledge using a gating mechanism to learn the ratio of text and knowledge fusion by a neural network that prevents noise generated by knowledge. After that, experimental validation on four datasets are carried out, and the experimental results show that our proposed model performs well on all four datasets, and the performance of our method is improved compared to the base model without adding external knowledge, which validates the effectiveness of our proposed method. The code is available at https://github.com/1094701018/KETM
</details>
<details>
<summary>摘要</summary>
文本匹配是自然语言处理任务中的一项重要任务，它有广泛的应用于阅读理解和问答系统等领域。主流方法是计算文本表示或通过注意力机制与文本进行交互，这些模型在文本匹配任务中表现良好。然而，这些模型在需要通过常识知ledge来解释的文本匹配任务时表现不够。为此，我们在这篇论文中提出了一种新的文本匹配模型，即知识增强文本匹配模型（KETM），以增强我们的模型理解和解释能力。首先，我们使用Wiktionary来提取文本单词定义作为我们的外部知识。然后，我们将文本和知识传递给文本匹配模块，以提取它们的特征向量。文本匹配模块被用作交互模块，并组合了编码层、相关层和聚合层。具体来说，交互过程会多次迭代，以获取深入的交互信息，并使用多角度聚合来提取文本和知识的特征向量。然后，我们使用阻块机制来融合文本和知识，以学习文本和知识的权重比例。最后，我们对四个数据集进行了实验验证，结果表明，我们提出的方法在所有四个数据集上表现出色，并且与基本模型相比，我们的方法性能有所提高，这 validate了我们的方法的有效性。代码可以在 GitHub 上找到。
</details></li>
</ul>
<hr>
<h2 id="A-Large-Language-Model-Enhanced-Conversational-Recommender-System"><a href="#A-Large-Language-Model-Enhanced-Conversational-Recommender-System" class="headerlink" title="A Large Language Model Enhanced Conversational Recommender System"></a>A Large Language Model Enhanced Conversational Recommender System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06212">http://arxiv.org/abs/2308.06212</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Feng, Shuchang Liu, Zhenghai Xue, Qingpeng Cai, Lantao Hu, Peng Jiang, Kun Gai, Fei Sun</li>
<li>for: 提高会话式推荐系统的效果（improve the effectiveness of conversational recommender systems）</li>
<li>methods: 利用大语言模型（Large Language Models, LLM）的理解和生成能力，并与专家模型（expert models）合作，以解决不同的子任务（sub-tasks）。</li>
<li>results: 实验结果表明，使用RLPF进行精心调整LLM，可以提高会话式推荐系统的性能（performance）。<details>
<summary>Abstract</summary>
Conversational recommender systems (CRSs) aim to recommend high-quality items to users through a dialogue interface. It usually contains multiple sub-tasks, such as user preference elicitation, recommendation, explanation, and item information search. To develop effective CRSs, there are some challenges: 1) how to properly manage sub-tasks; 2) how to effectively solve different sub-tasks; and 3) how to correctly generate responses that interact with users. Recently, Large Language Models (LLMs) have exhibited an unprecedented ability to reason and generate, presenting a new opportunity to develop more powerful CRSs. In this work, we propose a new LLM-based CRS, referred to as LLMCRS, to address the above challenges. For sub-task management, we leverage the reasoning ability of LLM to effectively manage sub-task. For sub-task solving, we collaborate LLM with expert models of different sub-tasks to achieve the enhanced performance. For response generation, we utilize the generation ability of LLM as a language interface to better interact with users. Specifically, LLMCRS divides the workflow into four stages: sub-task detection, model matching, sub-task execution, and response generation. LLMCRS also designs schema-based instruction, demonstration-based instruction, dynamic sub-task and model matching, and summary-based generation to instruct LLM to generate desired results in the workflow. Finally, to adapt LLM to conversational recommendations, we also propose to fine-tune LLM with reinforcement learning from CRSs performance feedback, referred to as RLPF. Experimental results on benchmark datasets show that LLMCRS with RLPF outperforms the existing methods.
</details>
<details>
<summary>摘要</summary>
文本： conversational recommender systems (CRSs) 目的是为用户提供高质量的ITEM通过对话界面。它通常包含多个子任务，例如用户偏好描述、推荐、解释和ITEM信息搜索。为开发有效的CRS，存在一些挑战：1）如何正确地管理子任务；2）如何有效地解决不同的子任务；和3）如何正确地生成与用户交互的响应。现在，大型自然语言模型（LLM）在理解和生成方面表现出了无 precedent的能力，这提供了一个新的机会，以开发更有力的CRS。在这项工作中，我们提出了一种基于LLM的CRS，称之为LLMCRS，以解决以下挑战。LLMCRS分为四个阶段：子任务检测、模型匹配、子任务执行和响应生成。LLMCRS还实现了 schema-based instruction、demonstration-based instruction、动态子任务和模型匹配以及摘要生成，以使LLM生成愿望的结果。最后，为适应对话推荐，我们还提出了使用强化学习来调整LLM的性能反馈，称之为RLPF。实验结果表明，LLMCRS与RLPF比现有方法高效。Here's the translation:文本： conversational recommender systems (CRSs) 目的是为用户提供高质量的ITEM通过对话界面。它通常包含多个子任务，例如用户偏好描述、推荐、解释和ITEM信息搜索。为开发有效的CRS，存在一些挑战：1）如何正确地管理子任务；2）如何有效地解决不同的子任务；和3）如何正确地生成与用户交互的响应。现在，大型自然语言模型（LLM）在理解和生成方面表现出了无 precedent的能力，这提供了一个新的机会，以开发更有力的CRS。在这项工作中，我们提出了一种基于LLM的CRS，称之为LLMCRS，以解决以下挑战。LLMCRS分为四个阶段：子任务检测、模型匹配、子任务执行和响应生成。LLMCRS还实现了 schema-based instruction、demonstration-based instruction、动态子任务和模型匹配以及摘要生成，以使LLM生成愿望的结果。最后，为适应对话推荐，我们还提出了使用强化学习来调整LLM的性能反馈，称之为RLPF。实验结果表明，LLMCRS与RLPF比现有方法高效。
</details></li>
</ul>
<hr>
<h2 id="Thinking-Like-an-Expert-Multimodal-Hypergraph-of-Thought-HoT-Reasoning-to-boost-Foundation-Modals"><a href="#Thinking-Like-an-Expert-Multimodal-Hypergraph-of-Thought-HoT-Reasoning-to-boost-Foundation-Modals" class="headerlink" title="Thinking Like an Expert:Multimodal Hypergraph-of-Thought (HoT) Reasoning to boost Foundation Modals"></a>Thinking Like an Expert:Multimodal Hypergraph-of-Thought (HoT) Reasoning to boost Foundation Modals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06207">http://arxiv.org/abs/2308.06207</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fanglong Yao, Changyuan Tian, Jintao Liu, Zequn Zhang, Qing Liu, Li Jin, Shuchao Li, Xiaoyu Li, Xian Sun<br>for: This paper aims to enhance the reasoning ability of foundation models by proposing a multimodal Hypergraph-of-Thought (HoT) reasoning paradigm, which can handle high-order multi-hop reasoning and multimodal comparative judgement.methods: The proposed HoT reasoning paradigm utilizes a textual hypergraph-of-thought and a visual hypergraph-of-thought, along with Cross-modal Co-Attention Graph Learning for multimodal comparative verification.results: Experimentations on the ScienceQA benchmark show that the proposed HoT-based T5 outperforms CoT-based GPT3.5 and chatGPT, and is on par with CoT-based GPT4 with a lower model size.<details>
<summary>Abstract</summary>
Reasoning ability is one of the most crucial capabilities of a foundation model, signifying its capacity to address complex reasoning tasks. Chain-of-Thought (CoT) technique is widely regarded as one of the effective methods for enhancing the reasoning ability of foundation models and has garnered significant attention. However, the reasoning process of CoT is linear, step-by-step, similar to personal logical reasoning, suitable for solving general and slightly complicated problems. On the contrary, the thinking pattern of an expert owns two prominent characteristics that cannot be handled appropriately in CoT, i.e., high-order multi-hop reasoning and multimodal comparative judgement. Therefore, the core motivation of this paper is transcending CoT to construct a reasoning paradigm that can think like an expert. The hyperedge of a hypergraph could connect various vertices, making it naturally suitable for modelling high-order relationships. Inspired by this, this paper innovatively proposes a multimodal Hypergraph-of-Thought (HoT) reasoning paradigm, which enables the foundation models to possess the expert-level ability of high-order multi-hop reasoning and multimodal comparative judgement. Specifically, a textual hypergraph-of-thought is constructed utilizing triple as the primary thought to model higher-order relationships, and a hyperedge-of-thought is generated through multi-hop walking paths to achieve multi-hop inference. Furthermore, we devise a visual hypergraph-of-thought to interact with the textual hypergraph-of-thought via Cross-modal Co-Attention Graph Learning for multimodal comparative verification. Experimentations on the ScienceQA benchmark demonstrate the proposed HoT-based T5 outperforms CoT-based GPT3.5 and chatGPT, which is on par with CoT-based GPT4 with a lower model size.
</details>
<details>
<summary>摘要</summary>
基本模型的逻辑能力是其最重要的能力之一，表明其能够解决复杂的逻辑任务。链条思维（CoT）技术是提高基本模型的逻辑能力的有效方法，吸引了广泛的关注。然而，CoT的逻辑过程是线性的，步骤式的，类似于个人的逻辑思维，适用于解决一般和些许复杂的问题。然而，专家的思维模式具有两个突出的特征，不能由CoT处理得好，即高阶多跳 reasoning和多模态比较判断。因此，本文的核心动机是超越CoT，建立一种能够思考如专家的逻辑模型。基于hypergraph的Hypergraph-of-Thought（HoT）逻辑模型是这种思考的核心。在这种模型中，hyperedge可以连接多个顶点，使其自然地适用于高阶关系的模elling。从这个意义上，本文创新地提出了一种多模态HoT逻辑模型，使基本模型具有专家水平的高阶多跳 reasoning和多模态比较判断能力。具体来说，通过 triple作为主要思想来模型高阶关系，并通过多跳步行路径生成多跳推理。此外，我们还提出了跨模态的Co-Attention图学习来实现多模态比较验证。实验结果表明，基于HoT的T5超过了CoT-based GPT3.5和chatGPT，与CoT-based GPT4的性能相似，但模型规模较小。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/12/cs.CL_2023_08_12/" data-id="clpxp03wf00a8fm8852ah8rnw" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_12" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/12/cs.LG_2023_08_12/" class="article-date">
  <time datetime="2023-08-12T10:00:00.000Z" itemprop="datePublished">2023-08-12</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/12/cs.LG_2023_08_12/">cs.LG - 2023-08-12</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="CoverNav-Cover-Following-Navigation-Planning-in-Unstructured-Outdoor-Environment-with-Deep-Reinforcement-Learning"><a href="#CoverNav-Cover-Following-Navigation-Planning-in-Unstructured-Outdoor-Environment-with-Deep-Reinforcement-Learning" class="headerlink" title="CoverNav: Cover Following Navigation Planning in Unstructured Outdoor Environment with Deep Reinforcement Learning"></a>CoverNav: Cover Following Navigation Planning in Unstructured Outdoor Environment with Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06594">http://arxiv.org/abs/2308.06594</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jumman Hossain, Abu-Zaher Faridee, Nirmalya Roy, Anjan Basak, Derrik E. Asher</li>
<li>for: 这 paper 的目的是提出一种基于 Deep Reinforcement Learning (DRL) 算法，用于在 offroad 环境中避免被外部观察者发现，并在有观察者存在的情况下安全地前往预定的目的地。</li>
<li>methods: 该算法使用了一个本地成本地图，以帮助选择最佳的遮盾和低成本路径，并使用了3D 点云数据、机器人的位置和指定目标信息来计算本地成本地图。</li>
<li>results: CoverNav 在 Unity  simulate 环境中被评估，并显示了在 terrain 中保持动态可行性的能力，并在不同的高度enario 中实现了最大的目标距离和成功率。<details>
<summary>Abstract</summary>
Autonomous navigation in offroad environments has been extensively studied in the robotics field. However, navigation in covert situations where an autonomous vehicle needs to remain hidden from outside observers remains an underexplored area. In this paper, we propose a novel Deep Reinforcement Learning (DRL) based algorithm, called CoverNav, for identifying covert and navigable trajectories with minimal cost in offroad terrains and jungle environments in the presence of observers. CoverNav focuses on unmanned ground vehicles seeking shelters and taking covers while safely navigating to a predefined destination. Our proposed DRL method computes a local cost map that helps distinguish which path will grant the maximal covertness while maintaining a low cost trajectory using an elevation map generated from 3D point cloud data, the robot's pose, and directed goal information. CoverNav helps robot agents to learn the low elevation terrain using a reward function while penalizing it proportionately when it experiences high elevation. If an observer is spotted, CoverNav enables the robot to select natural obstacles (e.g., rocks, houses, disabled vehicles, trees, etc.) and use them as shelters to hide behind. We evaluate CoverNav using the Unity simulation environment and show that it guarantees dynamically feasible velocities in the terrain when fed with an elevation map generated by another DRL based navigation algorithm. Additionally, we evaluate CoverNav's effectiveness in achieving a maximum goal distance of 12 meters and its success rate in different elevation scenarios with and without cover objects. We observe competitive performance comparable to state of the art (SOTA) methods without compromising accuracy.
</details>
<details>
<summary>摘要</summary>
自主导航在非道路环境中已经得到了RoboticsField的广泛研究。然而，在保持外部观察者隐私的情况下，自主导航仍然是一个未得到充分探索的领域。在这篇论文中，我们提出了一种基于深度优化学习（DRL）算法，称为CoverNav，用于在非道路地形和热带环境中寻找最佳隐蔽和可行的路径，并在外部观察者存在的情况下保持最佳的隐蔽性。CoverNav的设计目标是让无人地面车辆在保持安全的情况下寻找遮盾和避险的方式，并最终达到预定的目标地点。我们提出的DRL方法计算了当地的成本图，以帮助分辨出最佳隐蔽路径，同时保持低成本 trajectory使用3D点云数据、机器人的姿态和指定目标信息。CoverNav帮助机器人代理人学习低高度地形，通过一个奖励函数，同时对高高度增加惩罚。如果检测到观察者，CoverNav允许机器人选择自然障碍物（如岩石、房屋、瘫痪车辆、树木等），并使用它们作为遮盾隐藏。我们使用Unity simulate环境进行评估，并证明CoverNav可以在地形中保持动态可行速度。此外，我们还评估了CoverNav在不同高度场景中的效果和成功率，并发现其与状态之最好的方法（SOTA）的性能相似，无需损失精度。
</details></li>
</ul>
<hr>
<h2 id="Value-Distributional-Model-Based-Reinforcement-Learning"><a href="#Value-Distributional-Model-Based-Reinforcement-Learning" class="headerlink" title="Value-Distributional Model-Based Reinforcement Learning"></a>Value-Distributional Model-Based Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06590">http://arxiv.org/abs/2308.06590</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/djdprogramming/adfa2">https://github.com/djdprogramming/adfa2</a></li>
<li>paper_authors: Carlos E. Luis, Alessandro G. Bottero, Julia Vinogradska, Felix Berkenkamp, Jan Peters</li>
<li>for: 本研究旨在解决Sequential Decision-Making任务中的不确定性问题，通过基于搜索学习的模型based Bayesian reinforcement learning的角度来评估政策的长期表现。</li>
<li>methods: 本研究使用了分布式权值函数的思想，通过引入Bellman операktor的固定点来学习值函数的 posterior distribution。</li>
<li>results: 对多个连续控制任务的评估表明，EQR算法可以比以前的模型基于和模型自由算法表现更好，具有性能优势。<details>
<summary>Abstract</summary>
Quantifying uncertainty about a policy's long-term performance is important to solve sequential decision-making tasks. We study the problem from a model-based Bayesian reinforcement learning perspective, where the goal is to learn the posterior distribution over value functions induced by parameter (epistemic) uncertainty of the Markov decision process. Previous work restricts the analysis to a few moments of the distribution over values or imposes a particular distribution shape, e.g., Gaussians. Inspired by distributional reinforcement learning, we introduce a Bellman operator whose fixed-point is the value distribution function. Based on our theory, we propose Epistemic Quantile-Regression (EQR), a model-based algorithm that learns a value distribution function that can be used for policy optimization. Evaluation across several continuous-control tasks shows performance benefits with respect to established model-based and model-free algorithms.
</details>
<details>
<summary>摘要</summary>
Important 是量化一个政策的长期表现uncertainty，解决sequential decision-making tasks。我们从model-based Bayesian reinforcement learning的角度研究这个问题，目标是学习Markov decision process中参数（epistemic）uncertainty引起的值函数 posterior distribution。前一任 restricts the analysis to a few moments of the distribution over values or imposes a particular distribution shape, e.g., Gaussians。drawing inspiration from distributional reinforcement learning，我们引入一个Bellman operator，其fixed-point是值分布函数。根据我们的理论，我们提出Epistemic Quantile-Regression（EQR），一种model-based算法，可以学习一个可以用于政策优化的值分布函数。在许多连续控制任务上，我们的算法表现出了与已知model-based和model-free算法相比的性能优势。
</details></li>
</ul>
<hr>
<h2 id="Approximate-Answering-of-Graph-Queries"><a href="#Approximate-Answering-of-Graph-Queries" class="headerlink" title="Approximate Answering of Graph Queries"></a>Approximate Answering of Graph Queries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06585">http://arxiv.org/abs/2308.06585</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Cochez, Dimitrios Alivanistos, Erik Arakelyan, Max Berrendorf, Daniel Daza, Mikhail Galkin, Pasquale Minervini, Mathias Niepert, Hongyu Ren</li>
<li>for:  Answering queries in an incomplete knowledge graph (KG) setting.</li>
<li>methods: Several methods have been proposed to answer queries in an incomplete KG setting, including approaches based on semantic search, knowledge graph completion, and embedding-based methods.</li>
<li>results: These methods have been shown to be effective in answering queries in an incomplete KG setting, but they have limitations in terms of expressiveness, supported graph types, and inference capabilities.Here is the same information in Simplified Chinese text:</li>
<li>for:  Answering queries in an incomplete知识图(KG) setting.</li>
<li>methods:  Several methods have been proposed to answer queries in an incomplete KG setting, including基于semantic search的方法、knowledge graph completion的方法和embedding-based methods.</li>
<li>results:  These methods have been shown to be effective in answering queries in an incomplete KG setting, but they have limitations in terms of expressiveness、supported graph types和inference capabilities.<details>
<summary>Abstract</summary>
Knowledge graphs (KGs) are inherently incomplete because of incomplete world knowledge and bias in what is the input to the KG. Additionally, world knowledge constantly expands and evolves, making existing facts deprecated or introducing new ones. However, we would still want to be able to answer queries as if the graph were complete. In this chapter, we will give an overview of several methods which have been proposed to answer queries in such a setting. We will first provide an overview of the different query types which can be supported by these methods and datasets typically used for evaluation, as well as an insight into their limitations. Then, we give an overview of the different approaches and describe them in terms of expressiveness, supported graph types, and inference capabilities.
</details>
<details>
<summary>摘要</summary>
知识图（KG）自然而然 incomplete，因为世界知识不完整和输入KG中的偏见。然而，我们仍然想能够回答尚未完善的查询。在这章中，我们将给出几种提出来的方法，以及它们在支持不同类型的查询和评估 datasets 的限制。然后，我们将对这些方法进行概述，包括它们在表达力、支持的图类型和推理能力方面的特点。Here's the breakdown of the text into Simplified Chinese characters:知识图 (KG) 自然而然 incomplete 因为世界知识不完整和输入 KG 中的偏见。然而，我们仍然想能够回答尚未完善的查询。在这章中，我们将给出几种提出来的方法，以及它们在支持不同类型的查询和评估 datasets 的限制。然后，我们将对这些方法进行概述，包括它们在表达力、支持的图类型和推理能力方面的特点。
</details></li>
</ul>
<hr>
<h2 id="A-new-solution-and-concrete-implementation-steps-for-Artificial-General-Intelligence"><a href="#A-new-solution-and-concrete-implementation-steps-for-Artificial-General-Intelligence" class="headerlink" title="A new solution and concrete implementation steps for Artificial General Intelligence"></a>A new solution and concrete implementation steps for Artificial General Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09721">http://arxiv.org/abs/2308.09721</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongcong Chen, Ting Zeng, Jun Zhang</li>
<li>for: 这个论文目标是解决现有技术的缺陷，以实现更广泛的人工智能应用。</li>
<li>methods: 该论文使用现有技术和解决现有技术的缺陷，以实现更广泛的人工智能应用。</li>
<li>results: 该论文提出了解决现有技术的缺陷，以实现更广泛的人工智能应用的方法。<details>
<summary>Abstract</summary>
At present, the mainstream artificial intelligence generally adopts the technical path of "attention mechanism + deep learning" + "reinforcement learning". It has made great progress in the field of AIGC (Artificial Intelligence Generated Content), setting off the technical wave of big models[ 2][13 ]. But in areas that need to interact with the actual environment, such as elderly care, home nanny, agricultural production, and vehicle driving, trial and error are expensive and a reinforcement learning process that requires much trial and error is difficult to achieve. Therefore, in order to achieve Artificial General Intelligence(AGI) that can be applied to any field, we need to use both existing technologies and solve the defects of existing technologies, so as to further develop the technological wave of artificial intelligence. In this paper, we analyze the limitations of the technical route of large models, and by addressing these limitations, we propose solutions, thus solving the inherent defects of large models. In this paper, we will reveal how to achieve true AGI step by step.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:现在，主流人工智能通常采用“注意机制+深度学习”+“强化学习”技术路线。这种方法在AIGC（人工智能生成内容）领域已经取得了 significativadeeps[ 2][13 ]， triggering a technological wave of big models. However, in areas that require interaction with the actual environment, such as elderly care, home nanny, agricultural production, and vehicle driving, trial and error are costly and a reinforcement learning process that requires much trial and error is difficult to achieve. Therefore, to achieve Artificial General Intelligence (AGI) that can be applied to any field, we need to leverage both existing technologies and address the limitations of existing technologies, in order to further develop the technological wave of artificial intelligence. In this paper, we analyze the limitations of the technical route of large models, and by addressing these limitations, we propose solutions, thus solving the inherent defects of large models. Through this paper, we will reveal how to achieve true AGI step by step.
</details></li>
</ul>
<hr>
<h2 id="EquiDiff-A-Conditional-Equivariant-Diffusion-Model-For-Trajectory-Prediction"><a href="#EquiDiff-A-Conditional-Equivariant-Diffusion-Model-For-Trajectory-Prediction" class="headerlink" title="EquiDiff: A Conditional Equivariant Diffusion Model For Trajectory Prediction"></a>EquiDiff: A Conditional Equivariant Diffusion Model For Trajectory Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06564">http://arxiv.org/abs/2308.06564</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kehua Chen, Xianda Chen, Zihan Yu, Meixin Zhu, Hai Yang</li>
<li>for: 预测自动驾驶车辆的未来路径，以确保安全和有效的运行。</li>
<li>methods: 使用深度学习的各种方法，包括权重排序网络和强化学习，以及基于 conditional diffusion model 的 EquiDiff 模型，通过 integrate 历史信息和随机 Gaussian 噪声来预测未来路径。</li>
<li>results: EquiDiff 模型在 NGSIM 数据集上的实验结果表明，在短期预测方面表现出色，但在长期预测方面有些较高的错误率。此外，我们还进行了一项ablation 研究，以Investigate 各组件对预测精度的贡献。同时，我们还提供了 diffusion 模型生成过程的视觉化，以提供预测结果的不确定性的视觉化。<details>
<summary>Abstract</summary>
Accurate trajectory prediction is crucial for the safe and efficient operation of autonomous vehicles. The growing popularity of deep learning has led to the development of numerous methods for trajectory prediction. While deterministic deep learning models have been widely used, deep generative models have gained popularity as they learn data distributions from training data and account for trajectory uncertainties. In this study, we propose EquiDiff, a deep generative model for predicting future vehicle trajectories. EquiDiff is based on the conditional diffusion model, which generates future trajectories by incorporating historical information and random Gaussian noise. The backbone model of EquiDiff is an SO(2)-equivariant transformer that fully utilizes the geometric properties of location coordinates. In addition, we employ Recurrent Neural Networks and Graph Attention Networks to extract social interactions from historical trajectories. To evaluate the performance of EquiDiff, we conduct extensive experiments on the NGSIM dataset. Our results demonstrate that EquiDiff outperforms other baseline models in short-term prediction, but has slightly higher errors for long-term prediction. Furthermore, we conduct an ablation study to investigate the contribution of each component of EquiDiff to the prediction accuracy. Additionally, we present a visualization of the generation process of our diffusion model, providing insights into the uncertainty of the prediction.
</details>
<details>
<summary>摘要</summary>
准确的轨迹预测是自动驾驶车辆运行的关键。随着深度学习的普及，许多方法已经被开发出来用于轨迹预测。而使用权值函数的决定性深度学习模型在轨迹预测中广泛应用。在本研究中，我们提出了EquiDiff，一种基于条件扩散模型的深度生成模型，用于预测未来车辆的轨迹。EquiDiff通过将历史信息和随机 Gaussian 噪声纳入条件扩散模型，生成未来车辆的轨迹。我们的核心模型是一个SO(2)-共轭变换器，它完全利用了坐标点的几何属性。此外，我们还使用循环神经网络和图注意网络来提取历史轨迹中的社会互动。为了评估EquiDiff的性能，我们在NGSIM数据集上进行了广泛的实验。我们的结果显示，EquiDiff在短期预测方面胜过其他基准模型，但在长期预测方面有些微的错误。此外，我们还进行了减少分析，以了解各组件对预测精度的贡献。此外，我们还提供了生成过程中扩散模型的视觉化，为预测不确定性提供了更多的视角。
</details></li>
</ul>
<hr>
<h2 id="Human-Behavior-based-Personalized-Meal-Recommendation-and-Menu-Planning-Social-System"><a href="#Human-Behavior-based-Personalized-Meal-Recommendation-and-Menu-Planning-Social-System" class="headerlink" title="Human Behavior-based Personalized Meal Recommendation and Menu Planning Social System"></a>Human Behavior-based Personalized Meal Recommendation and Menu Planning Social System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06549">http://arxiv.org/abs/2308.06549</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tanvir Islam, Anika Rahman Joyita, Md. Golam Rabiul Alam, Mohammad Mehedi Hassan, Md. Rafiul Hassan, Raffaele Gravina</li>
<li>for: 这个研究旨在提供一种基于情感计算的餐单推荐和菜单规划方法，以满足用户不同情感的需求。</li>
<li>methods: 该研究使用了问卷调查和偏好认知来确定用户的餐食偏好，并使用电encephalography信号来检测用户对不同食物的情感。在这个研究中，我们使用了14栽 wireless Emotive Epoc+来测量用户对不同食物的情感。</li>
<li>results: 实验结果表明，该提议的情感计算、餐单推荐和菜单规划算法在多种评价参数上表现良好。<details>
<summary>Abstract</summary>
The traditional dietary recommendation systems are basically nutrition or health-aware where the human feelings on food are ignored. Human affects vary when it comes to food cravings, and not all foods are appealing in all moods. A questionnaire-based and preference-aware meal recommendation system can be a solution. However, automated recognition of social affects on different foods and planning the menu considering nutritional demand and social-affect has some significant benefits of the questionnaire-based and preference-aware meal recommendations. A patient with severe illness, a person in a coma, or patients with locked-in syndrome and amyotrophic lateral sclerosis (ALS) cannot express their meal preferences. Therefore, the proposed framework includes a social-affective computing module to recognize the affects of different meals where the person's affect is detected using electroencephalography signals. EEG allows to capture the brain signals and analyze them to anticipate affective toward a food. In this study, we have used a 14-channel wireless Emotive Epoc+ to measure affectivity for different food items. A hierarchical ensemble method is applied to predict affectivity upon multiple feature extraction methods and TOPSIS (Technique for Order of Preference by Similarity to Ideal Solution) is used to generate a food list based on the predicted affectivity. In addition to the meal recommendation, an automated menu planning approach is also proposed considering a person's energy intake requirement, affectivity, and nutritional values of the different menus. The bin-packing algorithm is used for the personalized menu planning of breakfast, lunch, dinner, and snacks. The experimental findings reveal that the suggested affective computing, meal recommendation, and menu planning algorithms perform well across a variety of assessment parameters.
</details>
<details>
<summary>摘要</summary>
传统的饮食建议系统基本上是nutrition或健康意识的，忽略了人类情感对食物的影响。人们对食物的欲望和喜好可以在不同的情感状态下发生变化，问卷和喜好意识的饭菜推荐系统可能是一个解决方案。然而，自动地认知社会情感对不同食物的影响，并根据营养需求和社会情感规划菜单，有一些显著的优点。例如，患有严重疾病、昏迷状态或locked-in syndrome和amyotrophic lateral sclerosis（ALS）患者无法表达他们的饭菜偏好。因此，我们的框架包括一个社交情感计算模块，用于识别不同饭菜中的情感。我们使用14栏 wireless Emotive Epoc+来测量不同饭品的情感响应。我们采用了层次ensemble方法来预测情感，并使用TOPSIS（理想解决方案的技术）来生成基于预测情感的饭品列表。此外，我们还提出了一种自动菜单规划方法，考虑人类能量摄入需求、情感和不同菜单的营养价值。使用bin-packing算法进行个性化菜单规划的早餐、午餐、晚餐和小吃。实验结果表明，我们提出的情感计算、饭菜推荐和菜单规划算法在多种评价参数上表现良好。
</details></li>
</ul>
<hr>
<h2 id="Digital-elevation-model-correction-in-urban-areas-using-extreme-gradient-boosting-land-cover-and-terrain-parameters"><a href="#Digital-elevation-model-correction-in-urban-areas-using-extreme-gradient-boosting-land-cover-and-terrain-parameters" class="headerlink" title="Digital elevation model correction in urban areas using extreme gradient boosting, land cover and terrain parameters"></a>Digital elevation model correction in urban areas using extreme gradient boosting, land cover and terrain parameters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06545">http://arxiv.org/abs/2308.06545</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chukwuma Okolie, Jon Mills, Adedayo Adeleke, Julian Smit</li>
<li>for: This paper aims to enhance the accuracy of medium-resolution digital elevation models (DEMs) in urban areas using the extreme gradient boosting (XGBoost) ensemble algorithm.</li>
<li>methods: The XGBoost algorithm was applied to two medium-resolution DEMs over Cape Town, South Africa, using eleven predictor variables, including elevation, urban footprints, and terrain features.</li>
<li>results: The correction achieved significant accuracy gains, with the root mean square error (RMSE) of the DEMs improving by 46-53% and 72-73%, respectively, compared to other proposed methods. These results demonstrate the potential of gradient boosted trees for enhancing the quality of DEMs and improving hydrological modelling in urban catchments.Here is the same information in Simplified Chinese text:</li>
<li>for: 本研究目的是使用极限梯度加权树（XGBoost）ensemble算法提高城市区域中数字高程模型（DEMs）的精度。</li>
<li>methods: XGBoost算法应用于两个中Resolution DEMs上cape Town,南非，使用eleven predictor variable，包括高程、城市脚本、地形特征等。</li>
<li>results: 修正得到了显著的准确性提高，高程误差根mean square error（RMSE）提高46-53%和72-73%，相比其他提议的方法。这些结果表明梯度加权树可以提高DEMs的质量和城市流域 hydrological modelling中的准确性。<details>
<summary>Abstract</summary>
The accuracy of digital elevation models (DEMs) in urban areas is influenced by numerous factors including land cover and terrain irregularities. Moreover, building artifacts in global DEMs cause artificial blocking of surface flow pathways. This compromises their quality and adequacy for hydrological and environmental modelling in urban landscapes where precise and accurate terrain information is needed. In this study, the extreme gradient boosting (XGBoost) ensemble algorithm is adopted for enhancing the accuracy of two medium-resolution 30m DEMs over Cape Town, South Africa: Copernicus GLO-30 and ALOS World 3D (AW3D). XGBoost is a scalable, portable and versatile gradient boosting library that can solve many environmental modelling problems. The training datasets are comprised of eleven predictor variables including elevation, urban footprints, slope, aspect, surface roughness, topographic position index, terrain ruggedness index, terrain surface texture, vector roughness measure, forest cover and bare ground cover. The target variable (elevation error) was calculated with respect to highly accurate airborne LiDAR. After training and testing, the model was applied for correcting the DEMs at two implementation sites. The correction achieved significant accuracy gains which are competitive with other proposed methods. The root mean square error (RMSE) of Copernicus DEM improved by 46 to 53% while the RMSE of AW3D DEM improved by 72 to 73%. These results showcase the potential of gradient boosted trees for enhancing the quality of DEMs, and for improved hydrological modelling in urban catchments.
</details>
<details>
<summary>摘要</summary>
“城市地区数字高程模型（DEM）的准确性受多种因素影响，包括地面覆盖和地形差异。此外，全球DEM中的建筑物artefact会导致 superficiale流动的人工堵塞，从而下降其质量和适用性于城市地区的水文和环境模型。本研究采用extrem Gradient Boosting（XGBoost）ensemble算法来提高两个中等分辨率30米DEM的准确性，即Copernicus GLO-30和ALOS World 3D（AW3D）。XGBoost是一种可扩展、可移植和多样的梯度提升库，可以解决许多环境模型问题。训练数据集包括11个预测变量，包括高程、城市覆盖面积、坡度、方向、表面粗糙度、地形位置指数、地形抗 roughness指数、地形表面文字、向量粗糙度度量、森林覆盖率和无 veg 覆盖率。目标变量（高程错误）与高精度飞行 LiDAR 进行计算。经过训练和测试，模型在两个实施地点应用于修正DEM。修正后的DEM准确性提高了46%到53%，而AW3D DEM的准确性提高了72%到73%。这些结果表明梯度增进树可以提高DEM的质量，并且为城市水文模型提供了改进的可能性。”
</details></li>
</ul>
<hr>
<h2 id="Dealing-with-Small-Datasets-for-Deep-Learning-in-Medical-Imaging-An-Evaluation-of-Self-Supervised-Pre-Training-on-CT-Scans-Comparing-Contrastive-and-Masked-Autoencoder-Methods-for-Convolutional-Models"><a href="#Dealing-with-Small-Datasets-for-Deep-Learning-in-Medical-Imaging-An-Evaluation-of-Self-Supervised-Pre-Training-on-CT-Scans-Comparing-Contrastive-and-Masked-Autoencoder-Methods-for-Convolutional-Models" class="headerlink" title="Dealing with Small Datasets for Deep Learning in Medical Imaging: An Evaluation of Self-Supervised Pre-Training on CT Scans Comparing Contrastive and Masked Autoencoder Methods for Convolutional Models"></a>Dealing with Small Datasets for Deep Learning in Medical Imaging: An Evaluation of Self-Supervised Pre-Training on CT Scans Comparing Contrastive and Masked Autoencoder Methods for Convolutional Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06534">http://arxiv.org/abs/2308.06534</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wolfda95/ssl-medicalimagining-cl-mae">https://github.com/wolfda95/ssl-medicalimagining-cl-mae</a></li>
<li>paper_authors: Daniel Wolf, Tristan Payer, Catharina Silvia Lisson, Christoph Gerhard Lisson, Meinrad Beer, Timo Ropinski, Michael Götz</li>
<li>for: 这篇研究的目的是为了探讨在医疗影像领域中使用深度学习模型，以减少医生负担，提高诊断速度，并最小化诊断错误的风险。</li>
<li>methods: 这篇研究使用了自然语言处理领域的自我超vised学习方法，将深度学习模型训练在大量无标注的医疗影像 dataset 上，然后使用小量标注 dataset 进行精度训练。</li>
<li>results: 研究发现，使用 SparK 自我超vised学习方法可以更好地适应小量标注 dataset，并且在不同的训练 dataset 大小下表现出不同的优势。因此，这篇研究建议在医疗影像领域使用 SparK 自我超vised学习方法，以提高深度学习模型的精度和效率。<details>
<summary>Abstract</summary>
Deep learning in medical imaging has the potential to minimize the risk of diagnostic errors, reduce radiologist workload, and accelerate diagnosis. Training such deep learning models requires large and accurate datasets, with annotations for all training samples. However, in the medical imaging domain, annotated datasets for specific tasks are often small due to the high complexity of annotations, limited access, or the rarity of diseases. To address this challenge, deep learning models can be pre-trained on large image datasets without annotations using methods from the field of self-supervised learning. After pre-training, small annotated datasets are sufficient to fine-tune the models for a specific task. The most popular self-supervised pre-training approaches in medical imaging are based on contrastive learning. However, recent studies in natural image processing indicate a strong potential for masked autoencoder approaches. Our work compares state-of-the-art contrastive learning methods with the recently introduced masked autoencoder approach "SparK" for convolutional neural networks (CNNs) on medical images. Therefore we pre-train on a large unannotated CT image dataset and fine-tune on several CT classification tasks. Due to the challenge of obtaining sufficient annotated training data in medical imaging, it is of particular interest to evaluate how the self-supervised pre-training methods perform when fine-tuning on small datasets. By experimenting with gradually reducing the training dataset size for fine-tuning, we find that the reduction has different effects depending on the type of pre-training chosen. The SparK pre-training method is more robust to the training dataset size than the contrastive methods. Based on our results, we propose the SparK pre-training for medical imaging tasks with only small annotated datasets.
</details>
<details>
<summary>摘要</summary>
深度学习在医疗影像领域可能减少诊断错误的风险，减轻放射学家的工作负担，并加速诊断。深度学习模型的训练需要大量和准确的数据集，并将所有训练样本注解。然而，在医疗影像领域，特定任务的注解数据集经常受到高复杂性的限制，限制了获得可用的数据。为解决这个挑战，深度学习模型可以通过不注解的图像集进行自我超vised学习。在这种情况下，小型注解数据集可以进行精度的微调。我们的工作 comparing state-of-the-art contrastive learning方法和最近引入的"SparK"隐藏自动编码器方法（MAE）在医疗影像领域的 convolutional neural networks（CNNs）中进行比较。因此，我们在大量无注解CT图像集上进行预训练，然后在多个CT分类任务上进行微调。由于在医疗影像领域获得足够的注解训练数据是困难的，因此我们特别关注在小型注解数据集上进行微调时的性能。我们通过逐渐减少微调数据集的大小来评估不同类型的预训练方法的性能。我们发现，使用SparK预训练方法可以更好地抗衡训练数据集的大小。根据我们的结果，我们提议在医疗影像任务中使用SparK预训练方法，即使只有小型注解数据集。
</details></li>
</ul>
<hr>
<h2 id="Learning-Abstract-Visual-Reasoning-via-Task-Decomposition-A-Case-Study-in-Raven-Progressive-Matrices"><a href="#Learning-Abstract-Visual-Reasoning-via-Task-Decomposition-A-Case-Study-in-Raven-Progressive-Matrices" class="headerlink" title="Learning Abstract Visual Reasoning via Task Decomposition: A Case Study in Raven Progressive Matrices"></a>Learning Abstract Visual Reasoning via Task Decomposition: A Case Study in Raven Progressive Matrices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06528">http://arxiv.org/abs/2308.06528</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jakubkwiatkowski/abstract_compositional_transformer">https://github.com/jakubkwiatkowski/abstract_compositional_transformer</a></li>
<li>paper_authors: Jakub Kwiatkowski, Krzysztof Krawiec</li>
<li>for: 本研究旨在提高 Abstract Reasoning 的能力，通过预测图像中对象的视觉属性和排列来解决 Raven Progressive Matrices (RPM) 问题。</li>
<li>methods: 本研究使用 transformer 架构，通过预测图像中对象的视觉属性和排列来解决 RPM 问题。研究还考虑了不同的图像分割方法和自动Masking 技术。</li>
<li>results: 实验结果表明，该方法不仅超越了当前最佳方法，还提供了有趣的思路和部分解释，帮助理解 RPM 问题的决策过程。此外，该方法还具有免除一些已知 RPM 标准准样的偏见的优点。<details>
<summary>Abstract</summary>
One of the challenges in learning to perform abstract reasoning is that problems are often posed as monolithic tasks, with no intermediate subgoals. In Raven Progressive Matrices (RPM), the task is to choose one of the available answers given a context, where both contexts and answers are composite images featuring multiple objects in various spatial arrangements. As this high-level goal is the only guidance available, learning is challenging and most contemporary solvers tend to be opaque. In this study, we propose a deep learning architecture based on the transformer blueprint which, rather than directly making the above choice, predicts the visual properties of individual objects and their arrangements. The multidimensional predictions obtained in this way are then directly juxtaposed to choose the answer. We consider a few ways in which the model parses the visual input into tokens and several regimes of masking parts of the input in self-supervised training. In experimental assessment, the models not only outperform state-of-the-art methods but also provide interesting insights and partial explanations about the inference. The design of the method also makes it immune to biases that are known to exist in some RPM benchmarks.
</details>
<details>
<summary>摘要</summary>
一个学习抽象逻辑的挑战是问题frequently pose as monolithic tasks，without intermediate subgoals. In Raven Progressive Matrices (RPM), the task is to choose one of the available answers given a context, where both contexts and answers are composite images featuring multiple objects in various spatial arrangements. As this high-level goal is the only guidance available, learning is challenging and most contemporary solvers tend to be opaque. In this study, we propose a deep learning architecture based on the transformer blueprint, which rather than directly making the above choice, predicts the visual properties of individual objects and their arrangements. The multidimensional predictions obtained in this way are then directly juxtaposed to choose the answer. We consider a few ways in which the model parses the visual input into tokens and several regimes of masking parts of the input in self-supervised training. In experimental assessment, the models not only outperform state-of-the-art methods but also provide interesting insights and partial explanations about the inference. The design of the method also makes it immune to biases that are known to exist in some RPM benchmarks.Here is the translation in Traditional Chinese:一个学习抽象逻辑的挑战是问题frequently pose as monolithic tasks，without intermediate subgoals. In Raven Progressive Matrices (RPM), the task is to choose one of the available answers given a context, where both contexts and answers are composite images featuring multiple objects in various spatial arrangements. As this high-level goal is the only guidance available, learning is challenging and most contemporary solvers tend to be opaque. In this study, we propose a deep learning architecture based on the transformer blueprint, which rather than directly making the above choice, predicts the visual properties of individual objects and their arrangements. The multidimensional predictions obtained in this way are then directly juxtaposed to choose the answer. We consider a few ways in which the model parses the visual input into tokens and several regimes of masking parts of the input in self-supervised training. In experimental assessment, the models not only outperform state-of-the-art methods but also provide interesting insights and partial explanations about the inference. The design of the method also makes it immune to biases that are known to exist in some RPM benchmarks.
</details></li>
</ul>
<hr>
<h2 id="SLoRA-Federated-Parameter-Efficient-Fine-Tuning-of-Language-Models"><a href="#SLoRA-Federated-Parameter-Efficient-Fine-Tuning-of-Language-Models" class="headerlink" title="SLoRA: Federated Parameter Efficient Fine-Tuning of Language Models"></a>SLoRA: Federated Parameter Efficient Fine-Tuning of Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06522">http://arxiv.org/abs/2308.06522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sara Babakniya, Ahmed Roushdy Elkordy, Yahya H. Ezzeldin, Qingfeng Liu, Kee-Bong Song, Mostafa El-Khamy, Salman Avestimehr</li>
<li>for: 这个研究旨在探讨在分布式语言任务中应用精简 parameter fine-tuning（PEFT）方法，以提高 Federated Learning（FL） 的可行性和效率。</li>
<li>methods: 本研究使用了 parameter efficient fine-tuning（PEFT）方法，并提出了一个名为 SLoRA 的新方法，具有跨用户数据的可靠性和高效性。</li>
<li>results: 实验结果显示，SLoRA 可以与全量 fine-tuning 相比，实现高度可 sparse 的更新，并在高 hetrogenous 数据场景下提高了表现。特别是，SLoRA 可以实现 $\sim 1%$ 的紧密更新，并降低了训练时间，高达 $90%$。<details>
<summary>Abstract</summary>
Transfer learning via fine-tuning pre-trained transformer models has gained significant success in delivering state-of-the-art results across various NLP tasks. In the absence of centralized data, Federated Learning (FL) can benefit from distributed and private data of the FL edge clients for fine-tuning. However, due to the limited communication, computation, and storage capabilities of edge devices and the huge sizes of popular transformer models, efficient fine-tuning is crucial to make federated training feasible. This work explores the opportunities and challenges associated with applying parameter efficient fine-tuning (PEFT) methods in different FL settings for language tasks. Specifically, our investigation reveals that as the data across users becomes more diverse, the gap between fully fine-tuning the model and employing PEFT methods widens. To bridge this performance gap, we propose a method called SLoRA, which overcomes the key limitations of LoRA in high heterogeneous data scenarios through a novel data-driven initialization technique. Our experimental results demonstrate that SLoRA achieves performance comparable to full fine-tuning, with significant sparse updates with approximately $\sim 1\%$ density while reducing training time by up to $90\%$.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本到简化中文。</SYS>>基于预训练变换器模型的迁移学习已经在不同的自然语言处理任务中带来了显著的成果，包括语音识别、文本分类、翻译等。在中央数据缺乏的情况下，联邦学习（FL）可以利用分布式和私有的edge客户端数据进行微调。然而，由于edge设备的通信、计算和存储能力的限制，以及流行的变换器模型的大型，高效的微调是必要的以使联邦训练成为可能。这项工作探讨了在不同的FL设置下应用Parameter Efficient Fine-tuning（PEFT）方法的机会和挑战。具体来说，我们的调查发现，当用户数据变得更加多样化时，完全微调和PEFT方法之间的性能差距加大。为 bridging这个性能差距，我们提议了一种名为SLoRA的方法，通过一种新的数据驱动初始化技术，超越LoRA在高多样性数据场景中的关键限制。我们的实验结果表明，SLoRA可以与全部微调达到相同的性能水平，并在大约1%的稀疏更新下降低训练时间约90%。
</details></li>
</ul>
<hr>
<h2 id="One-bit-Flip-is-All-You-Need-When-Bit-flip-Attack-Meets-Model-Training"><a href="#One-bit-Flip-is-All-You-Need-When-Bit-flip-Attack-Meets-Model-Training" class="headerlink" title="One-bit Flip is All You Need: When Bit-flip Attack Meets Model Training"></a>One-bit Flip is All You Need: When Bit-flip Attack Meets Model Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07934">http://arxiv.org/abs/2308.07934</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jianshuod/tba">https://github.com/jianshuod/tba</a></li>
<li>paper_authors: Jianshuo Dong, Han Qiu, Yiming Li, Tianwei Zhang, Yuanjie Li, Zeqi Lai, Chao Zhang, Shu-Tao Xia</li>
<li>for: 防御深度神经网络（DNNs）在实际设备上的安全性问题。</li>
<li>methods: 利用记忆FAULT INJECT技术实现行ammer attack，通过修改模型的权重来攻击量化模型在部署阶段。</li>
<li>results: 通过修改一个关键位的bit，可以轻松地将正常模型转化为恶意模型，并且这种攻击还可以绕过一些检测方法。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) are widely deployed on real-world devices. Concerns regarding their security have gained great attention from researchers. Recently, a new weight modification attack called bit flip attack (BFA) was proposed, which exploits memory fault inject techniques such as row hammer to attack quantized models in the deployment stage. With only a few bit flips, the target model can be rendered useless as a random guesser or even be implanted with malicious functionalities. In this work, we seek to further reduce the number of bit flips. We propose a training-assisted bit flip attack, in which the adversary is involved in the training stage to build a high-risk model to release. This high-risk model, obtained coupled with a corresponding malicious model, behaves normally and can escape various detection methods. The results on benchmark datasets show that an adversary can easily convert this high-risk but normal model to a malicious one on victim's side by \textbf{flipping only one critical bit} on average in the deployment stage. Moreover, our attack still poses a significant threat even when defenses are employed. The codes for reproducing main experiments are available at \url{https://github.com/jianshuod/TBA}.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）在实际设备上广泛应用。关于其安全性的问题吸引了研究者的广泛关注。最近，一种新的权值修改攻击方法called bit flip attack（BFA）被提出，它利用内存错误注入技术such as row hammer攻击部署阶段的量化模型。只需几个比特软件，目标模型就可以变成随机猜测器或甚至被恶意模型植入。在这种工作中，我们尝试降低比特软件的数量。我们提出了帮助者参与训练阶段的训练帮助攻击，以建立一个高风险模型，并将其发布。这个高风险模型，结合相应的恶意模型，在发布阶段 behave normally，并可以逃脱多种检测方法。我们的攻击仍然对于防御措施产生威胁。实验结果表明，一个攻击者可以在部署阶段通过flipping only one critical bit的方式，将高风险模型转换为恶意模型，而且这种攻击仍然有效even when defenses are employed。代码可以在 <https://github.com/jianshuod/TBA> 中进行重现主要实验。
</details></li>
</ul>
<hr>
<h2 id="Performance-Analysis-for-Resource-Constrained-Decentralized-Federated-Learning-Over-Wireless-Networks"><a href="#Performance-Analysis-for-Resource-Constrained-Decentralized-Federated-Learning-Over-Wireless-Networks" class="headerlink" title="Performance Analysis for Resource Constrained Decentralized Federated Learning Over Wireless Networks"></a>Performance Analysis for Resource Constrained Decentralized Federated Learning Over Wireless Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06496">http://arxiv.org/abs/2308.06496</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhigang Yan, Dong Li</li>
<li>for: 这个研究旨在分析资源受限的分布式机器学习（DFL）系统中的通信效率优化。</li>
<li>methods: 这个研究使用了不同的通信方案（数位和类比）来分析内部通信效率。</li>
<li>results: 研究发现，这些通信方案可以提供内部模型的训练，并且可以在不同的通信条件下进行优化。<details>
<summary>Abstract</summary>
Federated learning (FL) can lead to significant communication overhead and reliance on a central server. To address these challenges, decentralized federated learning (DFL) has been proposed as a more resilient framework. DFL involves parameter exchange between devices through a wireless network. This study analyzes the performance of resource-constrained DFL using different communication schemes (digital and analog) over wireless networks to optimize communication efficiency. Specifically, we provide convergence bounds for both digital and analog transmission approaches, enabling analysis of the model performance trained on DFL. Furthermore, for digital transmission, we investigate and analyze resource allocation between computation and communication and convergence rates, obtaining its communication complexity and the minimum probability of correction communication required for convergence guarantee. For analog transmission, we discuss the impact of channel fading and noise on the model performance and the maximum errors accumulation with convergence guarantee over fading channels. Finally, we conduct numerical simulations to evaluate the performance and convergence rate of convolutional neural networks (CNNs) and Vision Transformer (ViT) trained in the DFL framework on fashion-MNIST and CIFAR-10 datasets. Our simulation results validate our analysis and discussion, revealing how to improve performance by optimizing system parameters under different communication conditions.
</details>
<details>
<summary>摘要</summary>
联合学习（FL）可能会带来重要的通信负担和依赖中央服务器。为了解决这些挑战，分散式联合学习（DFL）已经被提议作为更可靠的框架。DFL通过装置间的参数交换来进行学习。本研究分析了受限制的DFL在无线网络上的表现，使用不同的通信方案（数位和模拟），以便最佳化通信效率。具体来说，我们提供了两种通信方法的整合界限，以及对数位传输的资源分配和通信复杂度的分析。另外，我们还考虑了频道折射和噪音对模型性能的影响，并分析了在折射通道上获得最大错误的组合。最后，我们将进行数据实验，以评估在DFL框架中训练过滤神经网络和探索神经网络的性能和融合率。我们的实验结果验证了我们的分析和讨论，并显示了如何通过优化系统参数来提高性能。
</details></li>
</ul>
<hr>
<h2 id="Flexible-Keyword-Spotting-based-on-Homogeneous-Audio-Text-Embedding"><a href="#Flexible-Keyword-Spotting-based-on-Homogeneous-Audio-Text-Embedding" class="headerlink" title="Flexible Keyword Spotting based on Homogeneous Audio-Text Embedding"></a>Flexible Keyword Spotting based on Homogeneous Audio-Text Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06472">http://arxiv.org/abs/2308.06472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kumari Nishu, Minsik Cho, Paul Dixon, Devang Naik</li>
<li>for: 这篇论文主要关注于efficiently detecting arbitrary keywords in audio-text modalities, using an audio-compliant text encoder to reduce the mismatch between text and audio embeddings.</li>
<li>methods: 本文提出了一个新的架构，使用一个具有同步表示的文本编码器，将文本转换为phonemes使用grapheme-to-phoneme（G2P）模型，然后将phonemes转换为嵌入使用代表性的phoneme вектор，从低质量的话语资料集中提取。此外，本文还使用可替代的关键生成技术来开发一个Audio-Text嵌入验证器。</li>
<li>results: 实验结果显示，本文的方法在Libriphrase hard dataset上比前一个state-of-the-art的结果高出84.21%到92.7%，且下降了23.36%到14.4%的Equal-Error-Rate（EER）值。<details>
<summary>Abstract</summary>
Spotting user-defined/flexible keywords represented in text frequently uses an expensive text encoder for joint analysis with an audio encoder in an embedding space, which can suffer from heterogeneous modality representation (i.e., large mismatch) and increased complexity. In this work, we propose a novel architecture to efficiently detect arbitrary keywords based on an audio-compliant text encoder which inherently has homogeneous representation with audio embedding, and it is also much smaller than a compatible text encoder. Our text encoder converts the text to phonemes using a grapheme-to-phoneme (G2P) model, and then to an embedding using representative phoneme vectors, extracted from the paired audio encoder on rich speech datasets. We further augment our method with confusable keyword generation to develop an audio-text embedding verifier with strong discriminative power. Experimental results show that our scheme outperforms the state-of-the-art results on Libriphrase hard dataset, increasing Area Under the ROC Curve (AUC) metric from 84.21% to 92.7% and reducing Equal-Error-Rate (EER) metric from 23.36% to 14.4%.
</details>
<details>
<summary>摘要</summary>
通常情况下，用户定义/灵活关键词在文本中的检测通常需要使用昂贵的文本编码器进行共同分析，并与音频编码器在嵌入空间进行结合分析，这可能会导致不同类型的表达（大匹配度）和复杂性增加。在这种工作中，我们提出了一种新的架构，可以有效地检测任意关键词，基于兼容音频编码器的文本编码器，该编码器具有兼容音频嵌入的同型表示，并且比兼容文本编码器更小。我们的文本编码器将文本转换为音频的phoneme使用图eme-to-phoneme（G2P）模型，然后将其转换为嵌入使用表示音频嵌入的phoneme вектор。我们还将我们的方法与可能的关键词生成进行增强，以开发一个具有强大抑制力的音频-文本嵌入验证器。实验结果表明，我们的方案在Libriphrase困难数据集上的成绩高于当前最佳结果，从84.21%提高到92.7%，并将相同错误率（EER）从23.36%降低到14.4%。
</details></li>
</ul>
<hr>
<h2 id="Volterra-Accentuated-Non-Linear-Dynamical-Admittance-VANYA-to-model-Deforestation-An-Exemplification-from-the-Amazon-Rainforest"><a href="#Volterra-Accentuated-Non-Linear-Dynamical-Admittance-VANYA-to-model-Deforestation-An-Exemplification-from-the-Amazon-Rainforest" class="headerlink" title="Volterra Accentuated Non-Linear Dynamical Admittance (VANYA) to model Deforestation: An Exemplification from the Amazon Rainforest"></a>Volterra Accentuated Non-Linear Dynamical Admittance (VANYA) to model Deforestation: An Exemplification from the Amazon Rainforest</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06471">http://arxiv.org/abs/2308.06471</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karthik R., Ramamoorthy A.</li>
<li>for: 本研究旨在预测雨林覆盖率，通过 integrate 猎物驱动力学和决策支持系统。</li>
<li>methods: 本研究使用 VANYA 模型，包括猎物驱动力学和决策支持系统，并对 Amazon 雨林数据进行预测。</li>
<li>results: 研究发现 VANYA 模型在预测雨林覆盖率方面表现出色，比 Long Short-Term Memory、N-BEATS 和 RCN 等其他预测器更为精准。<details>
<summary>Abstract</summary>
Intelligent automation supports us against cyclones, droughts, and seismic events with recent technology advancements. Algorithmic learning has advanced fields like neuroscience, genetics, and human-computer interaction. Time-series data boosts progress. Challenges persist in adopting these approaches in traditional fields. Neural networks face comprehension and bias issues. AI's expansion across scientific areas is due to adaptable descriptors and combinatorial argumentation. This article focuses on modeling Forest loss using the VANYA Model, incorporating Prey Predator Dynamics. VANYA predicts forest cover, demonstrated on Amazon Rainforest data against other forecasters like Long Short-Term Memory, N-BEATS, RCN.
</details>
<details>
<summary>摘要</summary>
智能自动化支持我们面对风暴、旱情和地震事件，因为最近技术的发展。算法学习已经提高了神经科学、遗传学和人机交互等领域。时间序列数据提高了进步。但在传统领域中采纳这些方法仍存在挑战。神经网络受理解和偏见问题困扰。AI的扩展到科学领域归功于可变描述和组合论证。本文将关注用VANYA模型预测森林损失，包括猎 Predator Dynamics。VANYA预测森林覆盖率，通过对亚马逊雨林数据进行比较，与其他预测器如Long Short-Term Memory、N-BEATS、RCN。
</details></li>
</ul>
<hr>
<h2 id="Tiny-and-Efficient-Model-for-the-Edge-Detection-Generalization"><a href="#Tiny-and-Efficient-Model-for-the-Edge-Detection-Generalization" class="headerlink" title="Tiny and Efficient Model for the Edge Detection Generalization"></a>Tiny and Efficient Model for the Edge Detection Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06468">http://arxiv.org/abs/2308.06468</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xavysp/teed">https://github.com/xavysp/teed</a></li>
<li>paper_authors: Xavier Soria, Yachuan Li, Mohammad Rouhani, Angel D. Sappa</li>
<li>For: 提高边检测精度，降低模型复杂度* Methods: 提出了一种轻量级卷积神经网络TEED，只有58K参数，比State-of-the-art模型少得多。* Results: 模型训练时间快（less than 30 minutes），每 epoch快（less than 5 minutes），预测边映射清晰度高。新提出的测试集可以评估边检测模型的通用性。<details>
<summary>Abstract</summary>
Most high-level computer vision tasks rely on low-level image operations as their initial processes. Operations such as edge detection, image enhancement, and super-resolution, provide the foundations for higher level image analysis. In this work we address the edge detection considering three main objectives: simplicity, efficiency, and generalization since current state-of-the-art (SOTA) edge detection models are increased in complexity for better accuracy. To achieve this, we present Tiny and Efficient Edge Detector (TEED), a light convolutional neural network with only $58K$ parameters, less than $0.2$% of the state-of-the-art models. Training on the BIPED dataset takes $less than 30 minutes$, with each epoch requiring $less than 5 minutes$. Our proposed model is easy to train and it quickly converges within very first few epochs, while the predicted edge-maps are crisp and of high quality. Additionally, we propose a new dataset to test the generalization of edge detection, which comprises samples from popular images used in edge detection and image segmentation. The source code is available in https://github.com/xavysp/TEED.
</details>
<details>
<summary>摘要</summary>
大多数高级计算机视觉任务都依赖于低级图像操作作为其初始过程。操作如边检测、图像提高和超分解，为更高级的图像分析提供基础。在这项工作中，我们考虑边检测三个主要目标：简单、高效和普适，因为当前状态艺术（SOTA）边检测模型在精度方面增加了复杂度。为了实现这一点，我们提出了简单和高效的边检测器（TEED），这是一个具有只有58000个参数的轻量级卷积神经网络。在BIPE dataset上训练时间仅占少于30分钟，每个epoch仅需5分钟左右。我们提出的模型轻松训练，快速 converge在第一些epoch中，而预测的边图具有高质量。此外，我们还提出了一个新的测试普适性边检测的数据集，该数据集包括流行的图像used在边检测和图像分割中。源代码可以在https://github.com/xavysp/TEED上获取。
</details></li>
</ul>
<hr>
<h2 id="Not-So-Robust-After-All-Evaluating-the-Robustness-of-Deep-Neural-Networks-to-Unseen-Adversarial-Attacks"><a href="#Not-So-Robust-After-All-Evaluating-the-Robustness-of-Deep-Neural-Networks-to-Unseen-Adversarial-Attacks" class="headerlink" title="Not So Robust After All: Evaluating the Robustness of Deep Neural Networks to Unseen Adversarial Attacks"></a>Not So Robust After All: Evaluating the Robustness of Deep Neural Networks to Unseen Adversarial Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06467">http://arxiv.org/abs/2308.06467</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roman Garaev, Bader Rasheed, Adil Khan</li>
<li>for: 这种研究旨在挑战当今防御机制对假数据攻击的有效性和通用性。</li>
<li>methods: 该研究使用了 adversarial attacks 来挑战当今的 DNN 模型。</li>
<li>results: 研究发现，train DNN 模型使用只有 robust 特征集时，并不能保证模型免受假数据攻击。此外，研究还发现 $L_2$ 和 $L_{\infty}$  нор的攻击对 DNN 表示的影响不同，这可能会对研究者提供有用的启示。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) have gained prominence in various applications, such as classification, recognition, and prediction, prompting increased scrutiny of their properties. A fundamental attribute of traditional DNNs is their vulnerability to modifications in input data, which has resulted in the investigation of adversarial attacks. These attacks manipulate the data in order to mislead a DNN. This study aims to challenge the efficacy and generalization of contemporary defense mechanisms against adversarial attacks. Specifically, we explore the hypothesis proposed by Ilyas et. al, which posits that DNN image features can be either robust or non-robust, with adversarial attacks targeting the latter. This hypothesis suggests that training a DNN on a dataset consisting solely of robust features should produce a model resistant to adversarial attacks. However, our experiments demonstrate that this is not universally true. To gain further insights into our findings, we analyze the impact of adversarial attack norms on DNN representations, focusing on samples subjected to $L_2$ and $L_{\infty}$ norm attacks. Further, we employ canonical correlation analysis, visualize the representations, and calculate the mean distance between these representations and various DNN decision boundaries. Our results reveal a significant difference between $L_2$ and $L_{\infty}$ norms, which could provide insights into the potential dangers posed by $L_{\infty}$ norm attacks, previously underestimated by the research community.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-One-dimensional-HEVC-video-steganalysis-method-using-the-Optimality-of-Predicted-Motion-Vectors"><a href="#A-One-dimensional-HEVC-video-steganalysis-method-using-the-Optimality-of-Predicted-Motion-Vectors" class="headerlink" title="A One-dimensional HEVC video steganalysis method using the Optimality of Predicted Motion Vectors"></a>A One-dimensional HEVC video steganalysis method using the Optimality of Predicted Motion Vectors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06464">http://arxiv.org/abs/2308.06464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jun Li, Minqing Zhang, Ke Niu, Yingnan Zhang, Xiaoyuan Yang</li>
<li>for: 增强HEVC标准视频隐藏通信的检测性能</li>
<li>methods: 基于优化的预测动作向量(MVP)的特征提取</li>
<li>results: 对两个通用数据集的三种常见隐藏通信方法进行检测，与四种现有的检测方法进行比较，实验结果表明提议的优化率of MVP在所有覆盖视频中为100%，而在所有隐藏视频中为 less than 100%，因此可以准确地分辨覆盖视频和隐藏视频，并在实际应用中具有无模型训练和低计算复杂度。<details>
<summary>Abstract</summary>
Among steganalysis techniques, detection against motion vector (MV) domain-based video steganography in High Efficiency Video Coding (HEVC) standard remains a hot and challenging issue. For the purpose of improving the detection performance, this paper proposes a steganalysis feature based on the optimality of predicted MVs with a dimension of one. Firstly, we point out that the motion vector prediction (MVP) of the prediction unit (PU) encoded using the Advanced Motion Vector Prediction (AMVP) technique satisfies the local optimality in the cover video. Secondly, we analyze that in HEVC video, message embedding either using MVP index or motion vector differences (MVD) may destroy the above optimality of MVP. And then, we define the optimal rate of MVP in HEVC video as a steganalysis feature. Finally, we conduct steganalysis detection experiments on two general datasets for three popular steganography methods and compare the performance with four state-of-the-art steganalysis methods. The experimental results show that the proposed optimal rate of MVP for all cover videos is 100\%, while the optimal rate of MVP for all stego videos is less than 100\%. Therefore, the proposed steganography scheme can accurately distinguish between cover videos and stego videos, and it is efficiently applied to practical scenarios with no model training and low computational complexity.
</details>
<details>
<summary>摘要</summary>
在隐藏分析技术中，对高效视频编码标准（HEVC）中的动态vector域基于视频隐藏技术进行检测仍然是一个热点和挑战。为了提高检测性能，这篇论文提出了基于预测动态vector（MVP）的隐藏特征。首先，我们指出HEVC视频中的预测单元（PU）使用高级动态vector预测（AMVP）技术编码时，预测动态vector的优化性在覆盖视频中是本地优化的。其次，我们分析HEVC视频中的信息嵌入（使用MVP索引或动态vector差（MVD））可能会破坏上述优化性。然后，我们定义HEVC视频中MVP的优化率作为隐藏特征。最后，我们对两个通用数据集上三种流行的隐藏技术进行检测试验，并与四种现状顶尖隐藏检测方法进行比较。实验结果表明，我们提出的优化率对所有覆盖视频是100%，而对所有隐藏视频是少于100%。因此，我们的隐藏方案可以准确地 отлича出覆盖视频和隐藏视频，并在实际应用中具有无模型训练和低计算复杂度。
</details></li>
</ul>
<hr>
<h2 id="Multi-Label-Knowledge-Distillation"><a href="#Multi-Label-Knowledge-Distillation" class="headerlink" title="Multi-Label Knowledge Distillation"></a>Multi-Label Knowledge Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06453">http://arxiv.org/abs/2308.06453</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/penghui-yang/l2d">https://github.com/penghui-yang/l2d</a></li>
<li>paper_authors: Penghui Yang, Ming-Kun Xie, Chen-Chen Zong, Lei Feng, Gang Niu, Masashi Sugiyama, Sheng-Jun Huang</li>
<li>for: 本研究是为了解决多类 label 学习中的知识填充问题，因为传统的知识填充方法难以在多类 label 学习中应用。</li>
<li>methods: 本研究提出了一种新的多类 label 知识填充方法，它利用了类别 embeddings 的结构信息，并将多类 label 学习问题分解成多个 binary 分类问题，以提高知识填充效果。</li>
<li>results: 实验结果表明，提出的方法可以避免类标签之间的知识冲突，并在多个 benchmark 数据集上达到了Superior性能，比较方法的性能。<details>
<summary>Abstract</summary>
Existing knowledge distillation methods typically work by imparting the knowledge of output logits or intermediate feature maps from the teacher network to the student network, which is very successful in multi-class single-label learning. However, these methods can hardly be extended to the multi-label learning scenario, where each instance is associated with multiple semantic labels, because the prediction probabilities do not sum to one and feature maps of the whole example may ignore minor classes in such a scenario. In this paper, we propose a novel multi-label knowledge distillation method. On one hand, it exploits the informative semantic knowledge from the logits by dividing the multi-label learning problem into a set of binary classification problems; on the other hand, it enhances the distinctiveness of the learned feature representations by leveraging the structural information of label-wise embeddings. Experimental results on multiple benchmark datasets validate that the proposed method can avoid knowledge counteraction among labels, thus achieving superior performance against diverse comparing methods. Our code is available at: https://github.com/penghui-yang/L2D
</details>
<details>
<summary>摘要</summary>
traditional knowledge distillation methods typically work by imparting the knowledge of output logits or intermediate feature maps from the teacher network to the student network, which is very successful in multi-class single-label learning. However, these methods can hardly be extended to the multi-label learning scenario, where each instance is associated with multiple semantic labels, because the prediction probabilities do not sum to one and feature maps of the whole example may ignore minor classes in such a scenario. In this paper, we propose a novel multi-label knowledge distillation method. On one hand, it exploits the informative semantic knowledge from the logits by dividing the multi-label learning problem into a set of binary classification problems; on the other hand, it enhances the distinctiveness of the learned feature representations by leveraging the structural information of label-wise embeddings. Experimental results on multiple benchmark datasets validate that the proposed method can avoid knowledge counteraction among labels, thus achieving superior performance against diverse comparing methods. Our code is available at: https://github.com/penghui-yang/L2D.Here's the translation in Traditional Chinese:传统的知识传递方法通常是将教师网络的输出条件或中间特征图形传递到学生网络中，在多类单 Label 学习中非常成功。然而，这些方法很难扩展到多 Label 学习情况下，因为每个例子都 associates 多个Semantic 标签，预测概率不等于一，特征图形可能将次要类别忽略。在这篇论文中，我们提出了一个新的多 Label 知识传递方法。一方面，它利用了条件的Semantic 知识，将多 Label 学习问题分成多个binary 分类问题；另一方面，它增强了学习的特征表现的明确性，通过利用标签对应的结构信息。实验结果显示，提案的方法可以避免标签之间的知识对抗，因此在多种比较方法面表现出色。我们的代码可以在：https://github.com/penghui-yang/L2D 中找到。
</details></li>
</ul>
<hr>
<h2 id="Latent-Random-Steps-as-Relaxations-of-Max-Cut-Min-Cut-and-More"><a href="#Latent-Random-Steps-as-Relaxations-of-Max-Cut-Min-Cut-and-More" class="headerlink" title="Latent Random Steps as Relaxations of Max-Cut, Min-Cut, and More"></a>Latent Random Steps as Relaxations of Max-Cut, Min-Cut, and More</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06448">http://arxiv.org/abs/2308.06448</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sudhanshu Chanpuriya, Cameron Musco</li>
<li>for: 本研究旨在提出一种基于非正式矩阵因子化的probabilistic模型，用于融合群集和简化图structure。</li>
<li>methods: 该模型基于Random Walk进程的分解，并通过简单的梯度下降优化。</li>
<li>results: 该算法可以relax hard clustering问题，并将其转化为一个 tractable 的问题。 furthermore, 该模型在synthetic graph和一些不监控学习任务中表现良好，如orthographic和phonological数据的bipartite和tripartite clustering。<details>
<summary>Abstract</summary>
Algorithms for node clustering typically focus on finding homophilous structure in graphs. That is, they find sets of similar nodes with many edges within, rather than across, the clusters. However, graphs often also exhibit heterophilous structure, as exemplified by (nearly) bipartite and tripartite graphs, where most edges occur across the clusters. Grappling with such structure is typically left to the task of graph simplification. We present a probabilistic model based on non-negative matrix factorization which unifies clustering and simplification, and provides a framework for modeling arbitrary graph structure. Our model is based on factorizing the process of taking a random walk on the graph. It permits an unconstrained parametrization, allowing for optimization via simple gradient descent. By relaxing the hard clustering to a soft clustering, our algorithm relaxes potentially hard clustering problems to a tractable ones. We illustrate our algorithm's capabilities on a synthetic graph, as well as simple unsupervised learning tasks involving bipartite and tripartite clustering of orthographic and phonological data.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:Algorithms for node clustering通常是查找图граhp的同质结构，即找到多数边连接的节点集，而不是跨集的边。然而，图 oftentimes also exhibits heterophilous structure, such as (nearly) bipartite and tripartite graphs, where most edges occur across the clusters. Previously, dealing with such structure was left to the task of graph simplification. We present a probabilistic model based on non-negative matrix factorization, which unifies clustering and simplification, and provides a framework for modeling arbitrary graph structure. Our model is based on factorizing the process of taking a random walk on the graph. It permits an unconstrained parametrization, allowing for optimization via simple gradient descent. By relaxing the hard clustering to a soft clustering, our algorithm relaxes potentially hard clustering problems to a tractable ones. We illustrate our algorithm's capabilities on a synthetic graph, as well as simple unsupervised learning tasks involving bipartite and tripartite clustering of orthographic and phonological data.
</details></li>
</ul>
<hr>
<h2 id="A-Sequential-Meta-Transfer-SMT-Learning-to-Combat-Complexities-of-Physics-Informed-Neural-Networks-Application-to-Composites-Autoclave-Processing"><a href="#A-Sequential-Meta-Transfer-SMT-Learning-to-Combat-Complexities-of-Physics-Informed-Neural-Networks-Application-to-Composites-Autoclave-Processing" class="headerlink" title="A Sequential Meta-Transfer (SMT) Learning to Combat Complexities of Physics-Informed Neural Networks: Application to Composites Autoclave Processing"></a>A Sequential Meta-Transfer (SMT) Learning to Combat Complexities of Physics-Informed Neural Networks: Application to Composites Autoclave Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06447">http://arxiv.org/abs/2308.06447</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/miladramzy/sequentialmetatransferpinns">https://github.com/miladramzy/sequentialmetatransferpinns</a></li>
<li>paper_authors: Milad Ramezankhani, Abbas S. Milani</li>
<li>for: 解决非线性偏微分方程（PDE）的快速解决方法，提高科学和工程应用中的精度和效率。</li>
<li>methods: 基于物理法则的神经网络（PINNs），通过将物理法则integrated到神经网络的训练中，使其在解决非线性系统方程方面表现出优异。</li>
<li>results: 在一个复杂的材料制造过程例子中，提出了一种新的Sequential Meta-Transfer（SMT）学习框架，可以快速地适应非线性系统中的变化，并大幅降低计算成本。<details>
<summary>Abstract</summary>
Physics-Informed Neural Networks (PINNs) have gained popularity in solving nonlinear partial differential equations (PDEs) via integrating physical laws into the training of neural networks, making them superior in many scientific and engineering applications. However, conventional PINNs still fall short in accurately approximating the solution of complex systems with strong nonlinearity, especially in long temporal domains. Besides, since PINNs are designed to approximate a specific realization of a given PDE system, they lack the necessary generalizability to efficiently adapt to new system configurations. This entails computationally expensive re-training from scratch for any new change in the system. To address these shortfalls, in this work a novel sequential meta-transfer (SMT) learning framework is proposed, offering a unified solution for both fast training and efficient adaptation of PINNs in highly nonlinear systems with long temporal domains. Specifically, the framework decomposes PDE's time domain into smaller time segments to create "easier" PDE problems for PINNs training. Then for each time interval, a meta-learner is assigned and trained to achieve an optimal initial state for rapid adaptation to a range of related tasks. Transfer learning principles are then leveraged across time intervals to further reduce the computational cost.Through a composites autoclave processing case study, it is shown that SMT is clearly able to enhance the adaptability of PINNs while significantly reducing computational cost, by a factor of 100.
</details>
<details>
<summary>摘要</summary>
为了解决这些缺陷，这个研究提出了一个 novel sequential meta-transfer (SMT) 学习框架，它可以实现快速训练和高效适应 PINNs 在高非线性系统中。 Specifically, the framework decomposes PDE's time domain into smaller time segments to create "easier" PDE problems for PINNs training. Then for each time interval, a meta-learner is assigned and trained to achieve an optimal initial state for rapid adaptation to a range of related tasks. Transfer learning principles are then leveraged across time intervals to further reduce the computational cost.通过一个 composites autoclave processing 案例研究，显示了 SMT 能够增强 PINNs 的适应能力，同时大幅降低计算成本，比例为 100。
</details></li>
</ul>
<hr>
<h2 id="Neural-Latent-Aligner-Cross-trial-Alignment-for-Learning-Representations-of-Complex-Naturalistic-Neural-Data"><a href="#Neural-Latent-Aligner-Cross-trial-Alignment-for-Learning-Representations-of-Complex-Naturalistic-Neural-Data" class="headerlink" title="Neural Latent Aligner: Cross-trial Alignment for Learning Representations of Complex, Naturalistic Neural Data"></a>Neural Latent Aligner: Cross-trial Alignment for Learning Representations of Complex, Naturalistic Neural Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06443">http://arxiv.org/abs/2308.06443</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheol Jun Cho, Edward F. Chang, Gopala K. Anumanchipalli</li>
<li>for: 本研究的目的是理解人类行为的神经实现，以便更好地理解神经科学中的复杂行为。</li>
<li>methods: 该研究提出了一种新的无监督学习框架——神经幽Alignment（NLA），用于找到有用的神经表示，并使用了一种完全可导的时间折叠模型（TWM）来解决 trial的时间不同问题。</li>
<li>results: 当应用于自然的说话ECoG数据时，该模型可以学习更好的表示来编码行为，特别是在低维度空间中。TWM被实验证明，并且当Visualized的折叠 manifold上可以看到共享的神经轨迹 across trials。<details>
<summary>Abstract</summary>
Understanding the neural implementation of complex human behaviors is one of the major goals in neuroscience. To this end, it is crucial to find a true representation of the neural data, which is challenging due to the high complexity of behaviors and the low signal-to-ratio (SNR) of the signals. Here, we propose a novel unsupervised learning framework, Neural Latent Aligner (NLA), to find well-constrained, behaviorally relevant neural representations of complex behaviors. The key idea is to align representations across repeated trials to learn cross-trial consistent information. Furthermore, we propose a novel, fully differentiable time warping model (TWM) to resolve the temporal misalignment of trials. When applied to intracranial electrocorticography (ECoG) of natural speaking, our model learns better representations for decoding behaviors than the baseline models, especially in lower dimensional space. The TWM is empirically validated by measuring behavioral coherence between aligned trials. The proposed framework learns more cross-trial consistent representations than the baselines, and when visualized, the manifold reveals shared neural trajectories across trials.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:理解人类复杂行为的神经实现是生物科学的一个主要目标。为 достичь这个目标，寻找神经数据真正的表示是非常重要，但是由于行为的高复杂性和神经信号噪声比（SNR）的低，这是一项挑战。我们提出了一种新的无监督学习框架，神经缺失匹配（NLA），以获取行为相关的神经表示。我们的关键想法是在重复试验中对表示进行对齐，以学习跨试验的一致信息。此外，我们还提出了一种完全可微分的时间折叠模型（TWM），以解决试验时间的不一致问题。当应用于自然说话的内部电rocorticography（ECoG）数据时，我们的模型可以学习更好的表示，特别是在低维度空间中。TWM被验证了通过测量试验之间的行为一致性。我们的框架可以更好地学习跨试验一致的表示，并且当Visualized时，折叠 manifold  revelas shared neural trajectories across trials。
</details></li>
</ul>
<hr>
<h2 id="A-Domain-adaptive-Physics-informed-Neural-Network-for-Inverse-Problems-of-Maxwell’s-Equations-in-Heterogeneous-Media"><a href="#A-Domain-adaptive-Physics-informed-Neural-Network-for-Inverse-Problems-of-Maxwell’s-Equations-in-Heterogeneous-Media" class="headerlink" title="A Domain-adaptive Physics-informed Neural Network for Inverse Problems of Maxwell’s Equations in Heterogeneous Media"></a>A Domain-adaptive Physics-informed Neural Network for Inverse Problems of Maxwell’s Equations in Heterogeneous Media</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06436">http://arxiv.org/abs/2308.06436</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiyuan Piao, Hong Gu, Aina Wang, Pan Qin</li>
<li>for: 解决Maxwell方程组在不同媒质中的逆问题</li>
<li>methods: 使用physics-informed神经网络（PINN）和领域适应训练策略</li>
<li>results: 提出了一种领域适应PINN（da-PINN），并在两个案例研究中证明了其效果<details>
<summary>Abstract</summary>
Maxwell's equations are a collection of coupled partial differential equations (PDEs) that, together with the Lorentz force law, constitute the basis of classical electromagnetism and electric circuits. Effectively solving Maxwell's equations is crucial in various fields, like electromagnetic scattering and antenna design optimization. Physics-informed neural networks (PINNs) have shown powerful ability in solving PDEs. However, PINNs still struggle to solve Maxwell's equations in heterogeneous media. To this end, we propose a domain-adaptive PINN (da-PINN) to solve inverse problems of Maxwell's equations in heterogeneous media. First, we propose a location parameter of media interface to decompose the whole domain into several sub-domains. Furthermore, the electromagnetic interface conditions are incorporated into a loss function to improve the prediction performance near the interface. Then, we propose a domain-adaptive training strategy for da-PINN. Finally, the effectiveness of da-PINN is verified with two case studies.
</details>
<details>
<summary>摘要</summary>
马克斯威尔方程是一系列相互关联的偏微分方程（PDEs），与 Lorentz 力法则共同构成了经典电磁学和电路。有效解决马克斯威尔方程是在各种领域中重要，如电磁散射和天线设计优化。 физи学 Informed Neural Networks（PINNs）已经显示出解决 PDEs 的强大能力。然而，PINNs 仍然在不同媒体中解决马克斯威尔方程困难。为此，我们提出了域 adaptive PINN（da-PINN）来解决马克斯威尔方程的反向问题在不同媒体中。首先，我们提出了媒体界面位置参数来分解整个域 into 多个子域。然后，我们将电磁界面条件纳入损失函数以提高预测性能 near 界面。最后，我们提出了域 adaptive 训练策略 для da-PINN。 Finally， da-PINN 的有效性被两个案例研究所验证。
</details></li>
</ul>
<hr>
<h2 id="Learn-Single-horizon-Disease-Evolution-for-Predictive-Generation-of-Post-therapeutic-Neovascular-Age-related-Macular-Degeneration"><a href="#Learn-Single-horizon-Disease-Evolution-for-Predictive-Generation-of-Post-therapeutic-Neovascular-Age-related-Macular-Degeneration" class="headerlink" title="Learn Single-horizon Disease Evolution for Predictive Generation of Post-therapeutic Neovascular Age-related Macular Degeneration"></a>Learn Single-horizon Disease Evolution for Predictive Generation of Post-therapeutic Neovascular Age-related Macular Degeneration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06432">http://arxiv.org/abs/2308.06432</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhan Zhang, Kun Huang, Mingchao Li, Songtao Yuan, Qiang Chen</li>
<li>for: 预测 age-related macular degeneration (nAMD) 疾病发展，生成post-therapeutic SD-OCT图像</li>
<li>methods: 提posed a single-horizon disease evolution network (SHENet)，包括Feature Encoder、Graph Evolution Module和Feature Decoder，通过 adversarial training 确保疾病演化学习的有效性</li>
<li>results: 比较其他生成方法，SHENet 的生成 SD-OCT 图像具有最高的图像质量，同时保持着 структура和内容的准确预测，并且在质量和效果上具有更好的视觉效果<details>
<summary>Abstract</summary>
Most of the existing disease prediction methods in the field of medical image processing fall into two classes, namely image-to-category predictions and image-to-parameter predictions. Few works have focused on image-to-image predictions. Different from multi-horizon predictions in other fields, ophthalmologists prefer to show more confidence in single-horizon predictions due to the low tolerance of predictive risk. We propose a single-horizon disease evolution network (SHENet) to predictively generate post-therapeutic SD-OCT images by inputting pre-therapeutic SD-OCT images with neovascular age-related macular degeneration (nAMD). In SHENet, a feature encoder converts the input SD-OCT images to deep features, then a graph evolution module predicts the process of disease evolution in high-dimensional latent space and outputs the predicted deep features, and lastly, feature decoder recovers the predicted deep features to SD-OCT images. We further propose an evolution reinforcement module to ensure the effectiveness of disease evolution learning and obtain realistic SD-OCT images by adversarial training. SHENet is validated on 383 SD-OCT cubes of 22 nAMD patients based on three well-designed schemes based on the quantitative and qualitative evaluations. Compared with other generative methods, the generative SD-OCT images of SHENet have the highest image quality. Besides, SHENet achieves the best structure protection and content prediction. Qualitative evaluations also demonstrate that SHENet has a better visual effect than other methods. SHENet can generate post-therapeutic SD-OCT images with both high prediction performance and good image quality, which has great potential to help ophthalmologists forecast the therapeutic effect of nAMD.
</details>
<details>
<summary>摘要</summary>
大多数现有的疾病预测方法在医学影像处理领域都归类为图像到类别预测和图像到参数预测，少数工作强调图像到图像预测。与其他多个时间预测不同，眼科医生偏好单个时间预测，因为预测风险的容忍度较低。我们提出了单个时间疾病演化网络（SHENet），用于预测治疗后SD-OCT图像。SHENet使用FeatureEncoder将输入SD-OCT图像转换为深度特征，然后使用图像演化模块预测疾病演化过程在高维潜在空间中，并输出预测的深度特征。最后，FeatureDecoder重建预测的深度特征为SD-OCT图像。我们还提出了演化增强模块，以确保疾病演化学习的有效性并获得真实的SD-OCT图像。SHENet在383个SD-OCT立方体上进行了三种基于量化和质量评价的验证。与其他生成方法相比，SHENet生成的SD-OCT图像的图像质量最高。此外，SHENet也达到了最佳结构保护和内容预测。质量评价还表明，SHENet的视觉效果更好。SHENet可以生成治疗后SD-OCT图像，具有高预测性和好图像质量，这对眼科医生预测nAMD的效果具有很大潜力。
</details></li>
</ul>
<hr>
<h2 id="Genetic-heterogeneity-analysis-using-genetic-algorithm-and-network-science"><a href="#Genetic-heterogeneity-analysis-using-genetic-algorithm-and-network-science" class="headerlink" title="Genetic heterogeneity analysis using genetic algorithm and network science"></a>Genetic heterogeneity analysis using genetic algorithm and network science</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06429">http://arxiv.org/abs/2308.06429</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhendong Sha, Yuanzhu Chen, Ting Hu</li>
<li>for: 这个论文目的是通过基因组宽度关联研究(GWAS)发现疾病感染的遗传变量。</li>
<li>methods: 这篇论文使用了一种新的特征选择机制，即特征合选网络(FCS-Net)，以EXTRACT多样化的基因变量。FCS-Net使用了一种遗传算理算法(GA)和一种非线性机器学习算法来检测特征互作。</li>
<li>results: 实验表明，FCS-Net可以有效地检测特征互作，并且可以在一个案例-控制患肠癌GWAS数据集中提取出新的合成特征。这些合成特征可以用来解释患肠癌的遗传多样性。<details>
<summary>Abstract</summary>
Through genome-wide association studies (GWAS), disease susceptible genetic variables can be identified by comparing the genetic data of individuals with and without a specific disease. However, the discovery of these associations poses a significant challenge due to genetic heterogeneity and feature interactions. Genetic variables intertwined with these effects often exhibit lower effect-size, and thus can be difficult to be detected using machine learning feature selection methods. To address these challenges, this paper introduces a novel feature selection mechanism for GWAS, named Feature Co-selection Network (FCSNet). FCS-Net is designed to extract heterogeneous subsets of genetic variables from a network constructed from multiple independent feature selection runs based on a genetic algorithm (GA), an evolutionary learning algorithm. We employ a non-linear machine learning algorithm to detect feature interaction. We introduce the Community Risk Score (CRS), a synthetic feature designed to quantify the collective disease association of each variable subset. Our experiment showcases the effectiveness of the utilized GA-based feature selection method in identifying feature interactions through synthetic data analysis. Furthermore, we apply our novel approach to a case-control colorectal cancer GWAS dataset. The resulting synthetic features are then used to explain the genetic heterogeneity in an additional case-only GWAS dataset.
</details>
<details>
<summary>摘要</summary>
通过全基因组协作研究（GWAS），可以通过比较患病者和无病者的遗传数据来确定疾病易感的遗传变量。然而，发现这些相互作用的挑战是由于遗传多样性和特征互作所致。遗传变量与这些效果相互作用的情况下经常表现出较低的效果大小，因此可能difficult to be detected using machine learning feature selection methods。为解决这些挑战，本文提出了一种新的特征选择机制，称为特征相互选择网络（FCSNet）。FCS-Net是基于多个独立的特征选择运行的一个网络结构，使用一种遗传算法（GA）进行进化学习算法。我们使用一种非线性机器学习算法来探测特征相互作用。我们还引入了一个名为社区风险分数（CRS）的合成特征，用于评估每个变量子集的疾病相关度。我们的实验表明，使用我们的新采用的GA基于特征选择方法可以快速和有效地检测特征相互作用。此外，我们还应用了我们的新方法于一个患肠癌GWAS数据集。得到的合成特征然后用于解释一个额外的case-only GWAS数据集中的遗传多样性。
</details></li>
</ul>
<hr>
<h2 id="Multiclass-Learnability-Does-Not-Imply-Sample-Compression"><a href="#Multiclass-Learnability-Does-Not-Imply-Sample-Compression" class="headerlink" title="Multiclass Learnability Does Not Imply Sample Compression"></a>Multiclass Learnability Does Not Imply Sample Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06424">http://arxiv.org/abs/2308.06424</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chirag Pabbaraju</li>
<li>for: 该论文讨论了一种叫做”样本压缩”的问题，即对于每个由一个假设来标注的样本，是否可以只保留一小部分样本，以便从整个样本上获取标签。</li>
<li>methods: 论文使用了一种名叫”VC dimension”的概念，它是用于描述一个假设类型的复杂度的一种指标。论文还使用了一种名叫”DS dimension”的概念，它是用于描述一个多类假设类型的复杂度的一种指标。</li>
<li>results: 论文的结果表明，对于每个有限多个假设类型，都存在一个可以压缩样本的方法，其中的压缩率只取决于假设类型的VC dimension。但是，对于多类假设类型，不存在一个可以压缩样本的方法，其中的压缩率只取决于假设类型的DS dimension。<details>
<summary>Abstract</summary>
A hypothesis class admits a sample compression scheme, if for every sample labeled by a hypothesis from the class, it is possible to retain only a small subsample, using which the labels on the entire sample can be inferred. The size of the compression scheme is an upper bound on the size of the subsample produced. Every learnable binary hypothesis class (which must necessarily have finite VC dimension) admits a sample compression scheme of size only a finite function of its VC dimension, independent of the sample size. For multiclass hypothesis classes, the analog of VC dimension is the DS dimension. We show that the analogous statement pertaining to sample compression is not true for multiclass hypothesis classes: every learnable multiclass hypothesis class, which must necessarily have finite DS dimension, does not admit a sample compression scheme of size only a finite function of its DS dimension.
</details>
<details>
<summary>摘要</summary>
一个假设集合承认样本压缩方案，如果对每个由假设集合中的一个假设标注的样本，只需保留一小样本，使得整个样本上的标注可以被推断出来。压缩方案的大小是样本上的子样本的上限。每个可学习的二分类假设集合（必然具有有限VC维度）承认一个样本压缩方案，它的大小只是VC维度的一个有限函数，不виси于样本的大小。对多类假设集合，相应的VC维度是DS维度。我们显示，对多类假设集合，其相应的压缩方案不存在，即每个可学习的多类假设集合，它必然具有有限DS维度，但不存在一个只是DS维度的有限函数的压缩方案。
</details></li>
</ul>
<hr>
<h2 id="Sensitivity-Aware-Mixed-Precision-Quantization-and-Width-Optimization-of-Deep-Neural-Networks-Through-Cluster-Based-Tree-Structured-Parzen-Estimation"><a href="#Sensitivity-Aware-Mixed-Precision-Quantization-and-Width-Optimization-of-Deep-Neural-Networks-Through-Cluster-Based-Tree-Structured-Parzen-Estimation" class="headerlink" title="Sensitivity-Aware Mixed-Precision Quantization and Width Optimization of Deep Neural Networks Through Cluster-Based Tree-Structured Parzen Estimation"></a>Sensitivity-Aware Mixed-Precision Quantization and Width Optimization of Deep Neural Networks Through Cluster-Based Tree-Structured Parzen Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06422">http://arxiv.org/abs/2308.06422</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seyedarmin Azizi, Mahdi Nazemi, Arash Fayyazi, Massoud Pedram</li>
<li>for: 提高深度学习模型的效率，自动选择最佳bit Width和层Width。</li>
<li>methods: 使用Hessian-based pruning和cluster-based tree-structured Parzen estimator来缩小搜索空间，并开发surrogate模型。</li>
<li>results: 在知名数据集上进行严格测试，与现有方法相比，提供20%的模型大小减少和12倍的搜索时间减少，代表了深度学习模型设计优化领域的一大突破。<details>
<summary>Abstract</summary>
As the complexity and computational demands of deep learning models rise, the need for effective optimization methods for neural network designs becomes paramount. This work introduces an innovative search mechanism for automatically selecting the best bit-width and layer-width for individual neural network layers. This leads to a marked enhancement in deep neural network efficiency. The search domain is strategically reduced by leveraging Hessian-based pruning, ensuring the removal of non-crucial parameters. Subsequently, we detail the development of surrogate models for favorable and unfavorable outcomes by employing a cluster-based tree-structured Parzen estimator. This strategy allows for a streamlined exploration of architectural possibilities and swift pinpointing of top-performing designs. Through rigorous testing on well-known datasets, our method proves its distinct advantage over existing methods. Compared to leading compression strategies, our approach records an impressive 20% decrease in model size without compromising accuracy. Additionally, our method boasts a 12x reduction in search time relative to the best search-focused strategies currently available. As a result, our proposed method represents a leap forward in neural network design optimization, paving the way for quick model design and implementation in settings with limited resources, thereby propelling the potential of scalable deep learning solutions.
</details>
<details>
<summary>摘要</summary>
“深度学习模型的复杂性和计算需求逐渐增长，因此选择最佳 neural network 层的位数和层宽成为了一项非常重要的优化方法。本工作提出了一种新的搜索机制，可以自动选择最佳位数和层宽，从而提高深度神经网络的效率。搜索空间通过利用希尔比ан-基于的剔除来减少，确保移除不必要的参数。然后，我们详细介绍了使用分布式树结构的 Parzen 估计器来开发备受欢迎和不欢迎的结果的代理模型。这种策略可以快速探索不同的建筑方案，并快速定位最佳的设计。我们对知名的数据集进行了严格的测试，并证明了我们的方法与现有方法相比，能够减少模型大小20%，而不会影响准确性。此外，我们的方法可以在搜索时间上减少12倍，相比最佳的搜索焦点策略。因此，我们的提议方法代表了深度神经网络设计优化领域的一大突破，为有限资源的设置中快速实现模型设计和实现，从而推动了可拓展的深度学习解决方案。”
</details></li>
</ul>
<hr>
<h2 id="Pedestrian-Trajectory-Prediction-in-Pedestrian-Vehicle-Mixed-Environments-A-Systematic-Review"><a href="#Pedestrian-Trajectory-Prediction-in-Pedestrian-Vehicle-Mixed-Environments-A-Systematic-Review" class="headerlink" title="Pedestrian Trajectory Prediction in Pedestrian-Vehicle Mixed Environments: A Systematic Review"></a>Pedestrian Trajectory Prediction in Pedestrian-Vehicle Mixed Environments: A Systematic Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06419">http://arxiv.org/abs/2308.06419</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahsa Golchoubian, Moojan Ghafurian, Kerstin Dautenhahn, Nasser Lashgarian Azad</li>
<li>for: 本研究は自动驾驶车辆（AV）在共享空间中的轨迹规划问题的解决方案。</li>
<li>methods: 本文系统atically review了 Literature中关于模拟行人轨迹预测的不同方法，这些方法可以应用于不结构化环境中。</li>
<li>results: 本文对pedestrian-vehicle交互（与人与人交互）进行了专门考虑，并review了不同变量（如预测uncertainty和行为差异）在已提出的预测模型中如何考虑。<details>
<summary>Abstract</summary>
Planning an autonomous vehicle's (AV) path in a space shared with pedestrians requires reasoning about pedestrians' future trajectories. A practical pedestrian trajectory prediction algorithm for the use of AVs needs to consider the effect of the vehicle's interactions with the pedestrians on pedestrians' future motion behaviours. In this regard, this paper systematically reviews different methods proposed in the literature for modelling pedestrian trajectory prediction in presence of vehicles that can be applied for unstructured environments. This paper also investigates specific considerations for pedestrian-vehicle interaction (compared with pedestrian-pedestrian interaction) and reviews how different variables such as prediction uncertainties and behavioural differences are accounted for in the previously proposed prediction models. PRISMA guidelines were followed. Articles that did not consider vehicle and pedestrian interactions or actual trajectories, and articles that only focused on road crossing were excluded. A total of 1260 unique peer-reviewed articles from ACM Digital Library, IEEE Xplore, and Scopus databases were identified in the search. 64 articles were included in the final review as they met the inclusion and exclusion criteria. An overview of datasets containing trajectory data of both pedestrians and vehicles used by the reviewed papers has been provided. Research gaps and directions for future work, such as having more effective definition of interacting agents in deep learning methods and the need for gathering more datasets of mixed traffic in unstructured environments are discussed.
</details>
<details>
<summary>摘要</summary>
планирование пути автономного транспортного средства (АВ) в пространстве, где находятся пешеходы, требует рассмотрения прогнозируемых траекторий пешеходов. практический алгоритм предсказания траекторий пешеходов для использования АВ должен учитывать влияние взаимодействия автомобиля с пешеходами на будущие движения людей. в этом отношении, этот документ систематически обзорывает различные методы, предложенные в литературе для моделирования предсказания траекторий пешеходов в присутствии автомобилей, которые могут быть применены в неструктурированных средах. документ также исследует конкретные аспекты взаимодействия пешехода-автомобиль (в сравнении с взаимодействием пешехода-пешеход) и обзоры, как различные переменные, такие как неопределенности предсказания и различия в поведении, учитываются в предыдущих моделях предсказания. Following PRISMA guidelines, articles that did not consider the interactions between vehicles and pedestrians or actual trajectories, and articles that only focused on road crossing were excluded. A total of 1260 unique peer-reviewed articles from ACM Digital Library, IEEE Xplore, and Scopus databases were identified in the search. 64 articles were included in the final review as they met the inclusion and exclusion criteria. An overview of datasets containing trajectory data of both pedestrians and vehicles used by the reviewed papers has been provided. Research gaps and directions for future work, such as the need for more effective definitions of interacting agents in deep learning methods and the need for gathering more datasets of mixed traffic in unstructured environments, are discussed.
</details></li>
</ul>
<hr>
<h2 id="Learning-Bayesian-Networks-with-Heterogeneous-Agronomic-Data-Sets-via-Mixed-Effect-Models-and-Hierarchical-Clustering"><a href="#Learning-Bayesian-Networks-with-Heterogeneous-Agronomic-Data-Sets-via-Mixed-Effect-Models-and-Hierarchical-Clustering" class="headerlink" title="Learning Bayesian Networks with Heterogeneous Agronomic Data Sets via Mixed-Effect Models and Hierarchical Clustering"></a>Learning Bayesian Networks with Heterogeneous Agronomic Data Sets via Mixed-Effect Models and Hierarchical Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06399">http://arxiv.org/abs/2308.06399</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lorenzo Vallegi, Marco Scutari, Federico Mattia Stefanini</li>
<li>For: This paper is written for researchers and practitioners who work with complex data sets in various fields, particularly agronomic studies. The paper aims to provide a novel approach for modeling causal relationships using Bayesian networks (BNs) and to demonstrate its effectiveness in handling hierarchical data.* Methods: The paper introduces a new approach that integrates random effects into BN learning, which is rooted in linear mixed-effects models. The approach uses directed acyclic graphs to illustrate the connections between variables and can handle complex networks of causal relationships.* Results: The paper reports that employing this approach can enhance structural learning, leading to the discovery of new connections and improved model specification. The approach also results in a reduction in prediction errors from 28% to 17%. The results suggest that the approach is effective in handling complex data sets and can improve the accuracy of predictions.<details>
<summary>Abstract</summary>
Research involving diverse but related data sets, where associations between covariates and outcomes may vary, is prevalent in various fields including agronomic studies. In these scenarios, hierarchical models, also known as multilevel models, are frequently employed to assimilate information from different data sets while accommodating their distinct characteristics. However, their structure extend beyond simple heterogeneity, as variables often form complex networks of causal relationships.   Bayesian networks (BNs) provide a powerful framework for modelling such relationships using directed acyclic graphs to illustrate the connections between variables. This study introduces a novel approach that integrates random effects into BN learning. Rooted in linear mixed-effects models, this approach is particularly well-suited for handling hierarchical data. Results from a real-world agronomic trial suggest that employing this approach enhances structural learning, leading to the discovery of new connections and the improvement of improved model specification. Furthermore, we observe a reduction in prediction errors from 28\% to 17\%. By extending the applicability of BNs to complex data set structures, this approach contributes to the effective utilisation of BNs for hierarchical agronomic data. This, in turn, enhances their value as decision-support tools in the field.
</details>
<details>
<summary>摘要</summary>
研究涉及多个相关数据集，其中变量之间可能存在复杂的关系，在各个领域，如农学研究中很普遍。在这些情况下，层次模型，也称为多级模型， часто被使用来整合不同数据集的信息，同时适应它们的特点。然而，这些模型的结构超出了简单的不同性，因为变量经常形成复杂的 causal 关系网络。 bayesian networks（BN）提供一种强大的模型化这些关系的框架，使用指向无环图来示出变量之间的连接。本研究提出了一种新的方法，即在 bayesian networks 学习中添加随机效应。基于线性混合效应模型，这种方法特别适合处理层次数据。实际的农学试验结果表明，通过使用这种方法，可以提高结构学习的效果，发现新的连接，并改善模型规定。此外，我们发现预测错误率从28%降低到17%。通过扩展 bayesian networks 的应用范围，这种方法为层次农学数据的有效利用做出了贡献，从而提高了 bayesian networks 作为决策支持工具的价值。
</details></li>
</ul>
<hr>
<h2 id="Detecting-and-Preventing-Hallucinations-in-Large-Vision-Language-Models"><a href="#Detecting-and-Preventing-Hallucinations-in-Large-Vision-Language-Models" class="headerlink" title="Detecting and Preventing Hallucinations in Large Vision Language Models"></a>Detecting and Preventing Hallucinations in Large Vision Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06394">http://arxiv.org/abs/2308.06394</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anisha Gunjal, Jihan Yin, Erhan Bas</li>
<li>for: 本研究的目的是提高大观语言模型（LVLM）在多modal任务中的泛化能力，特别是对Visual Question Answering（VQA）任务的泛化。</li>
<li>methods: 我们使用了InstructBLIP模型，并通过我们的novel Fine-grained Direct Preference Optimization（FDPO）和 fine-grained多Modal reward模型来优化这个模型，以避免hallucination。</li>
<li>results: 我们的实验结果表明，使用FDPO和rejection sampling可以将InstructBLIP模型中的hallucination率降低41%和55%，并且我们的 reward模型可以在其他多Modal模型上提高泛化能力，降低LLaVA和mPLUG-OWL模型中的hallucination率15%和57%。<details>
<summary>Abstract</summary>
Instruction tuned Large Vision Language Models (LVLMs) have significantly advanced in generalizing across a diverse set of multi-modal tasks, especially for Visual Question Answering (VQA). However, generating detailed responses that are visually grounded is still a challenging task for these models. We find that even the current state-of-the-art LVLMs (InstructBLIP) still contain a staggering 30 percent of the hallucinatory text in the form of non-existent objects, unfaithful descriptions, and inaccurate relationships. To address this, we introduce M-HalDetect, a (M)ultimodal (Hal)lucination (Detect)ion Dataset that can be used to train and benchmark models for hallucination detection and prevention. M-HalDetect consists of 16k fine-grained annotations on VQA examples, making it the first comprehensive multi-modal hallucination detection dataset for detailed image descriptions. Unlike previous work that only consider object hallucination, we additionally annotate both entity descriptions and relationships that are unfaithful. To demonstrate the potential of this dataset for hallucination prevention, we optimize InstructBLIP through our novel Fine-grained Direct Preference Optimization (FDPO). We also train fine-grained multi-modal reward models from InstructBLIP and evaluate their effectiveness with best-of-n rejection sampling. We perform human evaluation on both FDPO and rejection sampling, and find that they reduce hallucination rates in InstructBLIP by 41% and 55% respectively. We also find that our reward model generalizes to other multi-modal models, reducing hallucinations in LLaVA and mPLUG-OWL by 15% and 57% respectively, and has strong correlation with human evaluated accuracy scores.
</details>
<details>
<summary>摘要</summary>
压缩 Large Vision Language Models (LVLMs) 在多modal任务上的总体化进步了很多，特别是视觉问答 (VQA)。然而，生成具体的视觉基于的回答仍然是这些模型的挑战。我们发现，even the current state-of-the-art LVLMs (InstructBLIP) 仍然包含30%的幻觉文本，包括不存在的对象、不准确的描述和关系。为解决这个问题，我们介绍了 M-HalDetect，一个多modal的幻觉检测数据集，可以用于训练和对模型的幻觉检测和预防。M-HalDetect 包含16k 细化的 VQA 示例注释，使其成为首个多modal幻觉检测数据集。不同于之前的工作仅考虑对象幻觉，我们还注释了不准确的实体描述和关系。为证明这个数据集的潜力，我们使用我们的新的精细直接偏好优化 (FDPO) 方法优化 InstructBLIP。我们还使用基于 InstructBLIP 的精细多modal奖励模型，并通过best-of-n 拒绝采样评估其效果。我们进行了人工评估，发现 FDPO 和拒绝采样都能减少 InstructBLIP 中幻觉率 by 41% 和 55%  соответственно。此外，我们发现我们的奖励模型可以泛化到其他多modal模型，减少 LLaVA 和 mPLUG-OWL 中的幻觉率 by 15% 和 57%  соответственно，并与人类评估准确率之间存在强相关性。
</details></li>
</ul>
<hr>
<h2 id="Phoneme-Hallucinator-One-shot-Voice-Conversion-via-Set-Expansion"><a href="#Phoneme-Hallucinator-One-shot-Voice-Conversion-via-Set-Expansion" class="headerlink" title="Phoneme Hallucinator: One-shot Voice Conversion via Set Expansion"></a>Phoneme Hallucinator: One-shot Voice Conversion via Set Expansion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06382">http://arxiv.org/abs/2308.06382</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/PhonemeHallucinator/Phoneme_Hallucinator">https://github.com/PhonemeHallucinator/Phoneme_Hallucinator</a></li>
<li>paper_authors: Siyuan Shan, Yang Li, Amartya Banerjee, Junier B. Oliva</li>
<li>for: 本研究旨在解决现有VC方法中的一个矛盾，即保持语言内容的同时实现高度的 speaker similarity。</li>
<li>methods: 本研究提出了一种新的VC模型，即“phoneme hallucinator”，该模型可以基于短时间内的目标说话者声音（例如3秒）生成多样化和高质量的目标说话者音频。</li>
<li>results: 对比 existed VC方法，本研究的“phoneme hallucinator”模型在语言内容和说话者相似性两个方面都达到了更高的性能。<details>
<summary>Abstract</summary>
Voice conversion (VC) aims at altering a person's voice to make it sound similar to the voice of another person while preserving linguistic content. Existing methods suffer from a dilemma between content intelligibility and speaker similarity; i.e., methods with higher intelligibility usually have a lower speaker similarity, while methods with higher speaker similarity usually require plenty of target speaker voice data to achieve high intelligibility. In this work, we propose a novel method \textit{Phoneme Hallucinator} that achieves the best of both worlds. Phoneme Hallucinator is a one-shot VC model; it adopts a novel model to hallucinate diversified and high-fidelity target speaker phonemes based just on a short target speaker voice (e.g. 3 seconds). The hallucinated phonemes are then exploited to perform neighbor-based voice conversion. Our model is a text-free, any-to-any VC model that requires no text annotations and supports conversion to any unseen speaker. Objective and subjective evaluations show that \textit{Phoneme Hallucinator} outperforms existing VC methods for both intelligibility and speaker similarity.
</details>
<details>
<summary>摘要</summary>
声音转换（VC）目标是使一个人的声音变得更像另一个人的声音，保持语言内容不变。现有方法受到内容理解和发音相似之间的矛盾，即方法更高的理解能力通常需要大量目标 speaker 的声音数据来实现高度的发音相似。在这项工作中，我们提出了一种新方法——《phoneme hallucinator》。这是一个一架VC模型，它采用了一种新的模型来幻化目标 speaker 的多样化和高品质的发音，只需要短时间的目标 speaker 声音（例如3秒）。幻化的发音然后被利用于邻居基于的声音转换。我们的模型是文本无需、任何到任何的VC模型，不需要文本注释，并且支持转换到任何未看过的发音。对象和主观评估表明，《phoneme hallucinator》在理解和发音相似性方面都高于现有的VC方法。
</details></li>
</ul>
<hr>
<h2 id="DCNFIS-Deep-Convolutional-Neuro-Fuzzy-Inference-System"><a href="#DCNFIS-Deep-Convolutional-Neuro-Fuzzy-Inference-System" class="headerlink" title="DCNFIS: Deep Convolutional Neuro-Fuzzy Inference System"></a>DCNFIS: Deep Convolutional Neuro-Fuzzy Inference System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06378">http://arxiv.org/abs/2308.06378</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mojtaba Yeganejou, Kimia Honari, Ryan Kluzinski, Scott Dick, Michael Lipsett, James Miller</li>
<li>for: 提高 искусственный интеллект的可解释性，即让人可以直观地理解算法的工作方式，而不是仅仅得到后果的解释。</li>
<li>methods: 通过混合深度学习和符谱逻辑模型，设计了一种深度 convolutional neuro-fuzzy inference system (DCNFIS)，以提高算法的可解释性而不损失准确性。</li>
<li>results: DCNFIS在四个常用的数据集上表现与三种现有的 convolutional neural networks 相当，并且在深度符谱系统中表现出色，可以提供明确的解释。<details>
<summary>Abstract</summary>
A key challenge in eXplainable Artificial Intelligence is the well-known tradeoff between the transparency of an algorithm (i.e., how easily a human can directly understand the algorithm, as opposed to receiving a post-hoc explanation), and its accuracy. We report on the design of a new deep network that achieves improved transparency without sacrificing accuracy. We design a deep convolutional neuro-fuzzy inference system (DCNFIS) by hybridizing fuzzy logic and deep learning models and show that DCNFIS performs as accurately as three existing convolutional neural networks on four well-known datasets. We furthermore that DCNFIS outperforms state-of-the-art deep fuzzy systems. We then exploit the transparency of fuzzy logic by deriving explanations, in the form of saliency maps, from the fuzzy rules encoded in DCNFIS. We investigate the properties of these explanations in greater depth using the Fashion-MNIST dataset.
</details>
<details>
<summary>摘要</summary>
一个主要挑战在可解释人工智能中是论知识（即人类可以直接理解算法，而不是接受后勤解释）和准确性之间的贸易。我们报告了一种新的深度网络的设计，该网络实现了改善的透明度，不 sacrifice准确性。我们设计了一种深度 convolutional neuro-fuzzy inference system (DCNFIS)，通过混合深度学习模型和多valued 逻辑模型，并证明 DCNFIS 与三种现有的 convolutional neural networks 在四个知名的数据集上表现相同。此外，我们发现 DCNFIS 在深度逻辑系统中的透明度，可以从 fuzzy 规则中 derivation 出解释，例如 saliency maps。我们在 Fashion-MNIST 数据集上进行了更深入的调查，并证明这些解释具有某些性质。
</details></li>
</ul>
<hr>
<h2 id="UAMM-UBET-Automated-Market-Maker"><a href="#UAMM-UBET-Automated-Market-Maker" class="headerlink" title="UAMM: UBET Automated Market Maker"></a>UAMM: UBET Automated Market Maker</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06375">http://arxiv.org/abs/2308.06375</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Jiwoong Im, Alexander Kondratskiy, Vincent Harvey, Hsuan-Wei Fu</li>
<li>for: 这篇论文是关于抽象市场制定机制（AMM），用于中央化交易所（DEX）的价格机制。</li>
<li>methods: 该论文提出了一种新的价格计算方法，称为UBET AMM（UAMM），该方法考虑了外部市场价格和流动性池的不稳定损失。</li>
<li>results: 作者们示出了该方法可以消除外部市场价格是有效的情况下的买卖假象。<details>
<summary>Abstract</summary>
Automated market makers (AMMs) are pricing mechanisms utilized by decentralized exchanges (DEX). Traditional AMM approaches are constrained by pricing solely based on their own liquidity pool, without consideration of external markets or risk management for liquidity providers. In this paper, we propose a new approach known as UBET AMM (UAMM), which calculates prices by considering external market prices and the impermanent loss of the liquidity pool. Despite relying on external market prices, our method maintains the desired properties of a constant product curve when computing slippages. The key element of UAMM is determining the appropriate slippage amount based on the desired target balance, which encourages the liquidity pool to minimize impermanent loss. We demonstrate that our approach eliminates arbitrage opportunities when external market prices are efficient.
</details>
<details>
<summary>摘要</summary>
自动化市场制造者（AMM）是分布式交易所（DEX）中的价格调节机制。传统AMM方法仅基于自己的流动性池来价格调节，无视外部市场或流动性提供者的风险管理。在这篇论文中，我们提出了一新的方法，known as UBET AMM（UAMM），它根据外部市场价格和流动性池的不稳定损失来计算价格。尽管依赖外部市场价格，我们的方法仍然保持欲要的常量产品曲线价格调节。UBAMM的关键元素是根据目标库存量来决定适当的滑动量，这样将流动性池最小化不稳定损失。我们显示，我们的方法可以在有效的外部市场价格下消除投资机会。
</details></li>
</ul>
<hr>
<h2 id="Topic-Level-Bayesian-Surprise-and-Serendipity-for-Recommender-Systems"><a href="#Topic-Level-Bayesian-Surprise-and-Serendipity-for-Recommender-Systems" class="headerlink" title="Topic-Level Bayesian Surprise and Serendipity for Recommender Systems"></a>Topic-Level Bayesian Surprise and Serendipity for Recommender Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06368">http://arxiv.org/abs/2308.06368</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ton-moy/surprise-and-serendipity">https://github.com/ton-moy/surprise-and-serendipity</a></li>
<li>paper_authors: Tonmoy Hasan, Razvan Bunescu</li>
<li>for: 本研究旨在提高个性化推荐系统的效果，使用高度可能性的Item推荐，以减少用户接触到的Filter Bubble问题。</li>
<li>methods: 该研究使用了Bayesian surprise来量化Item的意外性，并结合协同推荐算法，以找到用户可能很喜欢的高度可能性Item。</li>
<li>results: 实验结果表明，使用Bayesian surprise来量化Item的意外性，与距离基于的估计法相比，具有更高的相关性和更好的服务器端推荐性能。<details>
<summary>Abstract</summary>
A recommender system that optimizes its recommendations solely to fit a user's history of ratings for consumed items can create a filter bubble, wherein the user does not get to experience items from novel, unseen categories. One approach to mitigate this undesired behavior is to recommend items with high potential for serendipity, namely surprising items that are likely to be highly rated. In this paper, we propose a content-based formulation of serendipity that is rooted in Bayesian surprise and use it to measure the serendipity of items after they are consumed and rated by the user. When coupled with a collaborative-filtering component that identifies similar users, this enables recommending items with high potential for serendipity. To facilitate the evaluation of topic-level models for surprise and serendipity, we introduce a dataset of book reading histories extracted from Goodreads, containing over 26 thousand users and close to 1.3 million books, where we manually annotate 449 books read by 4 users in terms of their time-dependent, topic-level surprise. Experimental evaluations show that models that use Bayesian surprise correlate much better with the manual annotations of topic-level surprise than distance-based heuristics, and also obtain better serendipitous item recommendation performance.
</details>
<details>
<summary>摘要</summary>
一个受推荐系统可以专注于让用户过去的评分历史中的项目进行最佳化，则可能导致一个范本弹簧（filter bubble），使用户没有机会体验到 novel、未看过的类别中的项目。为了解决这个问题，我们可以推荐项目具有高度的惊喜性，即可能具有高度评价的项目。在这篇文章中，我们提出了基于 bayesian 惊喜的内容基式的调和方法，并使用这个方法来衡量项目被用户过去评分后的惊喜性。当与相似用户的协同推荐部分结合时，这个方法可以提供高度惊喜性的项目推荐。为了促进题目级模型的惊喜和创新性的评估，我们将goodreads中的阅读历史数据集提取出超过26000名用户和约130000本书的数据，并 manually annotate 449本被4名用户阅读的书籍，以时间依赖的题目级惊喜作为标准。实验结果显示，使用 bayesian 惊喜的模型与距离基于的评估方法相比，具有更好的惊喜性和创新性的项目推荐性能。
</details></li>
</ul>
<hr>
<h2 id="Learning-Distributions-via-Monte-Carlo-Marginalization"><a href="#Learning-Distributions-via-Monte-Carlo-Marginalization" class="headerlink" title="Learning Distributions via Monte-Carlo Marginalization"></a>Learning Distributions via Monte-Carlo Marginalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06352">http://arxiv.org/abs/2308.06352</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenqiu Zhao, Guanfang Dong, Anup Basu</li>
<li>For: 学习不可求解分布的方法* Methods: 使用参数化分布模型（如混合 Gaussian Mixture Model）来近似不可求解分布，并使用 Monte-Carlo Marginalization 和 Kernel Density Estimation 解决计算复杂性和优化问题* Results: 提出了一种可 differentiable 的分布学习方法，并在标准数据集和 sintetic data 上进行了实验，证明了该方法的效果。此外，该方法还可以在 VAE 中代替变量推理，并且可以生成更好的图像。<details>
<summary>Abstract</summary>
We propose a novel method to learn intractable distributions from their samples. The main idea is to use a parametric distribution model, such as a Gaussian Mixture Model (GMM), to approximate intractable distributions by minimizing the KL-divergence. Based on this idea, there are two challenges that need to be addressed. First, the computational complexity of KL-divergence is unacceptable when the dimensions of distributions increases. The Monte-Carlo Marginalization (MCMarg) is proposed to address this issue. The second challenge is the differentiability of the optimization process, since the target distribution is intractable. We handle this problem by using Kernel Density Estimation (KDE). The proposed approach is a powerful tool to learn complex distributions and the entire process is differentiable. Thus, it can be a better substitute of the variational inference in variational auto-encoders (VAE). One strong evidence of the benefit of our method is that the distributions learned by the proposed approach can generate better images even based on a pre-trained VAE's decoder. Based on this point, we devise a distribution learning auto-encoder which is better than VAE under the same network architecture. Experiments on standard dataset and synthetic data demonstrate the efficiency of the proposed approach.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法来学习不可解 Distributions 的样本。主要思想是使用参数化分布模型，如 Gaussian Mixture Model (GMM)，来approximate不可解 Distributions 的 KL- divergence 的最小值。在这个想法的基础之上，需要解决两个挑战。首先，随着分布的维度增加，KL- divergence 的计算复杂性变得不可接受。我们提出了 Monte-Carlo Marginalization (MCMarg) 来解决这个问题。其次，目标分布是不可导的，因此需要使用 Kernel Density Estimation (KDE) 来处理这个问题。我们的方法可以学习复杂的分布，整个过程是导数的，因此可以作为 VAE 的更好的替代方案。我们的方法可以在标准数据集和 synthetic data 上进行实验，并且得到了良好的效果。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and widely used in other countries as well. The translation is based on the standard grammar and vocabulary of Simplified Chinese, and may differ slightly from the Traditional Chinese used in Taiwan and other countries.
</details></li>
</ul>
<hr>
<h2 id="Mirror-Diffusion-Models"><a href="#Mirror-Diffusion-Models" class="headerlink" title="Mirror Diffusion Models"></a>Mirror Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06342">http://arxiv.org/abs/2308.06342</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cran/DIMORA">https://github.com/cran/DIMORA</a></li>
<li>paper_authors: Jaesung Tae</li>
<li>for: 这份报告旨在提出一种对组 categorical 资料进行生成的方法，并且提供一个理论框架来适应受限的领域。</li>
<li>methods: 这种方法基于镜像朗凯文法，并且将其应用到简单的扩散过程中。这些方法可以自然地扩展到受欢迎的领域，如图像和文本生成。</li>
<li>results: 这种方法可以在简单的扩散过程中生成高质量的组 categorical 资料，并且可以实现受限的领域中的生成。<details>
<summary>Abstract</summary>
Diffusion models have successfully been applied to generative tasks in various continuous domains. However, applying diffusion to discrete categorical data remains a non-trivial task. Moreover, generation in continuous domains often requires clipping in practice, which motivates the need for a theoretical framework for adapting diffusion to constrained domains. Inspired by the mirror Langevin algorithm for the constrained sampling problem, in this theoretical report we propose Mirror Diffusion Models (MDMs). We demonstrate MDMs in the context of simplex diffusion and propose natural extensions to popular domains such as image and text generation.
</details>
<details>
<summary>摘要</summary>
Diffusion models have successfully been applied to generative tasks in various continuous domains. However, applying diffusion to discrete categorical data remains a non-trivial task. Moreover, generation in continuous domains often requires clipping in practice, which motivates the need for a theoretical framework for adapting diffusion to constrained domains. Inspired by the mirror Langevin algorithm for the constrained sampling problem, in this theoretical report we propose Mirror Diffusion Models (MDMs). We demonstrate MDMs in the context of simplex diffusion and propose natural extensions to popular domains such as image and text generation.Here's the translation in Traditional Chinese:Diffusion models have successfully been applied to generative tasks in various continuous domains. However, applying diffusion to discrete categorical data remains a non-trivial task. Moreover, generation in continuous domains often requires clipping in practice, which motivates the need for a theoretical framework for adapting diffusion to constrained domains. Inspired by the mirror Langevin algorithm for the constrained sampling problem, in this theoretical report we propose Mirror Diffusion Models (MDMs). We demonstrate MDMs in the context of simplex diffusion and propose natural extensions to popular domains such as image and text generation.
</details></li>
</ul>
<hr>
<h2 id="Size-Lowerbounds-for-Deep-Operator-Networks"><a href="#Size-Lowerbounds-for-Deep-Operator-Networks" class="headerlink" title="Size Lowerbounds for Deep Operator Networks"></a>Size Lowerbounds for Deep Operator Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06338">http://arxiv.org/abs/2308.06338</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anirbit Mukherjee, Amartya Roy</li>
<li>for: 本研究目的是Establishing a first-of-its-kind data-dependent lower bound on the size of DeepONets required to reduce empirical error on noisy data.</li>
<li>methods: 本研究使用Deep Operator Networks (DeepONets) paradigm to solve regression in infinite dimensions and families of PDEs in one shot.</li>
<li>results: 研究发现，为了在$n$个数据点上获得低训练错误，Common output dimension of branch and trunk net必须在$\Omega \left ( \sqrt{n} \right )$scaling。这种情况在解决势动扩散反应PDE中进行实验，并证明在固定模型大小下，通过增加common output dimension来逐渐下降训练错误，数据训练集可能需要平方倍增。<details>
<summary>Abstract</summary>
Deep Operator Networks are an increasingly popular paradigm for solving regression in infinite dimensions and hence solve families of PDEs in one shot. In this work, we aim to establish a first-of-its-kind data-dependent lowerbound on the size of DeepONets required for them to be able to reduce empirical error on noisy data. In particular, we show that for low training errors to be obtained on $n$ data points it is necessary that the common output dimension of the branch and the trunk net be scaling as $\Omega \left ( {\sqrt{n} \right )$. This inspires our experiments with DeepONets solving the advection-diffusion-reaction PDE, where we demonstrate the possibility that at a fixed model size, to leverage increase in this common output dimension and get monotonic lowering of training error, the size of the training data might necessarily need to scale quadratically with it.
</details>
<details>
<summary>摘要</summary>
深度网络（DeepONet）是一种在无穷维度中进行回归的增 Popular 模式，可以解决 families of PDEs 的问题一并。在这项工作中，我们想要建立一个数据висимы的下界，以确定 DeepONets 的大小需要在噪音数据上减少 empirical error。 Specifically，我们显示了在 $n$ 个数据点上获得低训练错误需要branch和trunk网的公共输出维度Scale as $\Omega \left ( \sqrt{n} \right )$.这种情况在我们解决了diffusion-advection-reaction PDE 的实验中得到了证明，我们发现在固定模型大小下，可以通过增加 common output dimension来降低训练错误，但是training data的大小可能需要 quadratic 增长。
</details></li>
</ul>
<hr>
<h2 id="Foundation-Model-is-Efficient-Multimodal-Multitask-Model-Selector"><a href="#Foundation-Model-is-Efficient-Multimodal-Multitask-Model-Selector" class="headerlink" title="Foundation Model is Efficient Multimodal Multitask Model Selector"></a>Foundation Model is Efficient Multimodal Multitask Model Selector</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06262">http://arxiv.org/abs/2308.06262</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/opengvlab/multitask-model-selector">https://github.com/opengvlab/multitask-model-selector</a></li>
<li>paper_authors: Fanqing Meng, Wenqi Shao, Zhanglin Peng, Chonghe Jiang, Kaipeng Zhang, Yu Qiao, Ping Luo</li>
<li>for: 这个论文研究了一个未经探索的重要问题：给一个集合的预训练神经网络，预测它们在每个多Modal任务上的性能，不需要细致调整。</li>
<li>methods: 这个论文提出了一种高效的多任务模型选择器（EMMS），使用大规模基础模型将多种下游任务的标签格式转换成一个统一的噪声标签嵌入。EMMS可以通过一种简单的权重线性回归来估算模型的传输性，该算法可以通过一种交互式最小化算法来高效解决。</li>
<li>results: 对5个下游任务和24个数据集进行了广泛的实验，显示了EMMS的高效性、有效性和通用性。比如，相比采用LogME进行加强的状态的方法，EMMS在图像识别、引用、描述、视觉问答和文本问答等5个任务上 achieve了9.0%、26.3%、20.1%、54.8%和12.2%的性能提升，同时带来5.13倍、6.29倍、3.59倍、6.19倍和5.66倍的计划时间提升。代码可以在<a target="_blank" rel="noopener" href="https://github.com/OpenGVLab/Multitask-Model-Selector%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/OpenGVLab/Multitask-Model-Selector上获取。</a><details>
<summary>Abstract</summary>
This paper investigates an under-explored but important problem: given a collection of pre-trained neural networks, predicting their performance on each multi-modal task without fine-tuning them, such as image recognition, referring, captioning, visual question answering, and text question answering. A brute-force approach is to finetune all models on all target datasets, bringing high computational costs. Although recent-advanced approaches employed lightweight metrics to measure models' transferability,they often depend heavily on the prior knowledge of a single task, making them inapplicable in a multi-modal multi-task scenario. To tackle this issue, we propose an efficient multi-task model selector (EMMS), which employs large-scale foundation models to transform diverse label formats such as categories, texts, and bounding boxes of different downstream tasks into a unified noisy label embedding. EMMS can estimate a model's transferability through a simple weighted linear regression, which can be efficiently solved by an alternating minimization algorithm with a convergence guarantee. Extensive experiments on 5 downstream tasks with 24 datasets show that EMMS is fast, effective, and generic enough to assess the transferability of pre-trained models, making it the first model selection method in the multi-task scenario. For instance, compared with the state-of-the-art method LogME enhanced by our label embeddings, EMMS achieves 9.0\%, 26.3\%, 20.1\%, 54.8\%, 12.2\% performance gain on image recognition, referring, captioning, visual question answering, and text question answering, while bringing 5.13x, 6.29x, 3.59x, 6.19x, and 5.66x speedup in wall-clock time, respectively. The code is available at https://github.com/OpenGVLab/Multitask-Model-Selector.
</details>
<details>
<summary>摘要</summary>
Currently, a brute-force approach is to fine-tune all models on all target datasets, which is computationally expensive. Recent advanced approaches use lightweight metrics to measure models' transferability, but these metrics often rely on prior knowledge of a single task and are not applicable in a multi-modal multi-task scenario.To address this issue, we propose an Efficient Multi-task Model Selector (EMMS), which uses large-scale foundation models to transform diverse label formats into a unified noisy label embedding. EMMS estimates a model's transferability through a simple weighted linear regression, which can be efficiently solved by an alternating minimization algorithm with a convergence guarantee.Our extensive experiments on 5 downstream tasks with 24 datasets show that EMMS is fast, effective, and generic enough to assess the transferability of pre-trained models. Compared with the state-of-the-art method LogME enhanced by our label embeddings, EMMS achieves a 9.0%, 26.3%, 20.1%, 54.8%, and 12.2% performance gain on image recognition, referring, captioning, visual question answering, and text question answering, respectively. Additionally, EMMS brings 5.13x, 6.29x, 3.59x, 6.19x, and 5.66x speedup in wall-clock time, respectively.The code for EMMS is available at https://github.com/OpenGVLab/Multitask-Model-Selector.
</details></li>
</ul>
<hr>
<h2 id="Predicting-Resilience-with-Neural-Networks"><a href="#Predicting-Resilience-with-Neural-Networks" class="headerlink" title="Predicting Resilience with Neural Networks"></a>Predicting Resilience with Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06309">http://arxiv.org/abs/2308.06309</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karen da Mata, Priscila Silva, Lance Fiondella</li>
<li>for: This paper aims to propose and evaluate alternative neural network (NN) approaches to model and predict system performance, including negative and positive factors driving resilience, in order to quantify the impact of disruptive events and restorative activities.</li>
<li>methods: The paper proposes three alternative NN approaches, including Artificial Neural Networks, Recurrent Neural Networks, and Long-Short Term Memory (LSTM), to model and predict system performance.</li>
<li>results: The results show that NN models outperformed a classical statistical model on all goodness-of-fit measures, with LSTMs achieving an over 60% higher adjusted R squared and decreased predictive error by 34-fold compared to the traditional method. These results suggest that NN models are both feasible and accurate for predicting resilience and may find practical use in many important domains.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目的是提出和评估基于神经网络（NN）的方法，以量化系统性能，包括负面和正面因素对系统抗异常性的影响。</li>
<li>methods: 论文提出了三种代表性的NN方法，包括人工神经网络、循环神经网络和长短期记忆（LSTM），以量化系统性能。</li>
<li>results: 结果表明，NN模型在所有准确度度量上都高于传统统计模型，具体来说，LSTM模型在所有准确度度量上高于60%，并将预测错误量减少34倍。这些结果表明，NN模型可以准确地预测系统抗异常性，并在许多重要领域发现实际应用。<details>
<summary>Abstract</summary>
Resilience engineering studies the ability of a system to survive and recover from disruptive events, which finds applications in several domains. Most studies emphasize resilience metrics to quantify system performance, whereas recent studies propose statistical modeling approaches to project system recovery time after degradation. Moreover, past studies are either performed on data after recovering or limited to idealized trends. Therefore, this paper proposes three alternative neural network (NN) approaches including (i) Artificial Neural Networks, (ii) Recurrent Neural Networks, and (iii) Long-Short Term Memory (LSTM) to model and predict system performance, including negative and positive factors driving resilience to quantify the impact of disruptive events and restorative activities. Goodness-of-fit measures are computed to evaluate the models and compared with a classical statistical model, including mean squared error and adjusted R squared. Our results indicate that NN models outperformed the traditional model on all goodness-of-fit measures. More specifically, LSTMs achieved an over 60\% higher adjusted R squared, and decreased predictive error by 34-fold compared to the traditional method. These results suggest that NN models to predict resilience are both feasible and accurate and may find practical use in many important domains.
</details>
<details>
<summary>摘要</summary>
“恢复工程”（Resilience engineering）研究系统对瘫痪事件的抗衡能力和恢复时间，这些应用在多个领域。大多数研究强调系统表现的量化指标（resilience metrics），而现在的研究则提出了使用统计模型估算系统恢复时间。然而，过去的研究都是基于已经恢复的数据或仅对理想化趋势进行研究。因此，本文提出了三种人工神经网络（Artificial Neural Networks）方法，包括（i）人工神经网络（Artificial Neural Networks）、（ii）循环神经网络（Recurrent Neural Networks）和（iii）长期记忆运算（Long-Short Term Memory，LSTM），用于模拟和预测系统表现，包括负和正因素影响系统抗衡能力。我们 Compute 好igkeit-of-fit 度量来评估这些模型，并与传统的统计模型进行比较，包括平均方差和修正系数。我们的结果显示，NN 模型在所有好igkeit-of-fit 度量上表现更好，特别是 LSTM 模型的调整 R 平方error 高于 60%，并降低预测误差34倍。这些结果表示 NN 模型可以实现系统抗衡的预测，并且具有高准确性。这些模型可能在许多重要领域中找到实际应用。
</details></li>
</ul>
<hr>
<h2 id="FunnyBirds-A-Synthetic-Vision-Dataset-for-a-Part-Based-Analysis-of-Explainable-AI-Methods"><a href="#FunnyBirds-A-Synthetic-Vision-Dataset-for-a-Part-Based-Analysis-of-Explainable-AI-Methods" class="headerlink" title="FunnyBirds: A Synthetic Vision Dataset for a Part-Based Analysis of Explainable AI Methods"></a>FunnyBirds: A Synthetic Vision Dataset for a Part-Based Analysis of Explainable AI Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06248">http://arxiv.org/abs/2308.06248</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/visinf/funnybirds">https://github.com/visinf/funnybirds</a></li>
<li>paper_authors: Robin Hesse, Simone Schaub-Meyer, Stefan Roth</li>
<li>for:  This paper aims to address the challenge of evaluating the quality of explainable artificial intelligence (XAI) methods, which is an important problem in safety-critical domains where XAI is used.</li>
<li>methods:  The paper proposes a novel synthetic vision dataset called FunnyBirds, as well as accompanying automatic evaluation protocols. The dataset allows for semantically meaningful image interventions, such as removing individual object parts, which enables the analysis of explanations on a part level and the estimation of ground-truth part importances.</li>
<li>results:  The paper reports results for 24 different combinations of neural models and XAI methods, demonstrating the strengths and weaknesses of the assessed methods in a fully automatic and systematic manner. The results show that the proposed evaluation protocols can provide valuable insights into the quality of XAI methods and can help to identify areas for improvement.<details>
<summary>Abstract</summary>
The field of explainable artificial intelligence (XAI) aims to uncover the inner workings of complex deep neural models. While being crucial for safety-critical domains, XAI inherently lacks ground-truth explanations, making its automatic evaluation an unsolved problem. We address this challenge by proposing a novel synthetic vision dataset, named FunnyBirds, and accompanying automatic evaluation protocols. Our dataset allows performing semantically meaningful image interventions, e.g., removing individual object parts, which has three important implications. First, it enables analyzing explanations on a part level, which is closer to human comprehension than existing methods that evaluate on a pixel level. Second, by comparing the model output for inputs with removed parts, we can estimate ground-truth part importances that should be reflected in the explanations. Third, by mapping individual explanations into a common space of part importances, we can analyze a variety of different explanation types in a single common framework. Using our tools, we report results for 24 different combinations of neural models and XAI methods, demonstrating the strengths and weaknesses of the assessed methods in a fully automatic and systematic manner.
</details>
<details>
<summary>摘要</summary>
领域的可解释人工智能（XAI）目的是探索复杂的深度神经网络模型的内部工作机制。而这是安全关键领域的关键，但XAI自然lacks ground-truth explanations，这使得自动评估成为一个未解决的问题。我们解决这个挑战 by proposing a novel synthetic vision dataset，名为FunnyBirds，以及一系列自动评估协议。我们的dataset允许进行semantically meaningful的图像交互，例如去除个体物体部分，这有三个重要的含义。首先，它允许分析解释的部级别，这更加接近人类理解的水平 than existing methods that evaluate on a pixel level。第二，通过比较模型输出对各个部分去除后的输出，我们可以估算ground-truth part importances，这些importances应该反映在解释中。第三，将各种解释映射到一个共同的部分重要性空间中，我们可以分析多种不同的解释类型在一个共同框架中。使用我们的工具，我们对24种不同的神经网络模型和XAI方法进行了报告，并demonstrated它们在自动和系统atic的方式下的优劣点。
</details></li>
</ul>
<hr>
<h2 id="Private-Distribution-Learning-with-Public-Data-The-View-from-Sample-Compression"><a href="#Private-Distribution-Learning-with-Public-Data-The-View-from-Sample-Compression" class="headerlink" title="Private Distribution Learning with Public Data: The View from Sample Compression"></a>Private Distribution Learning with Public Data: The View from Sample Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06239">http://arxiv.org/abs/2308.06239</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shai Ben-David, Alex Bie, Clément L. Canonne, Gautam Kamath, Vikrant Singhal</li>
<li>for: 本研究考虑了一种名为公共-私有学习的问题，在这种设置下，学习者只能访问公共和私有样本，并且需要根据这些样本来估算一个未知分布$p$的值，同时保证隐私性。</li>
<li>methods: 本研究使用了纯量化隐私学习来保证隐私性，并使用了一种叫做列学习的方法来连接公共-私有学习的问题。</li>
<li>results: 本研究得出了一些新的结论，包括对于任意$k$-mixture的加aussian over $\mathbb R^d$的样本复杂度上界，以及对于agnostic和分布shift抗性的学习者的结论。此外，研究还发现了一个关于公共-私有学习的closure性性质，即对于Gaussian over $\mathbb R^d$，至少需要$d$个公共样本来保证私有学习的隐私性。<details>
<summary>Abstract</summary>
We study the problem of private distribution learning with access to public data. In this setup, which we refer to as public-private learning, the learner is given public and private samples drawn from an unknown distribution $p$ belonging to a class $\mathcal Q$, with the goal of outputting an estimate of $p$ while adhering to privacy constraints (here, pure differential privacy) only with respect to the private samples.   We show that the public-private learnability of a class $\mathcal Q$ is connected to the existence of a sample compression scheme for $\mathcal Q$, as well as to an intermediate notion we refer to as list learning. Leveraging this connection: (1) approximately recovers previous results on Gaussians over $\mathbb R^d$; and (2) leads to new ones, including sample complexity upper bounds for arbitrary $k$-mixtures of Gaussians over $\mathbb R^d$, results for agnostic and distribution-shift resistant learners, as well as closure properties for public-private learnability under taking mixtures and products of distributions. Finally, via the connection to list learning, we show that for Gaussians in $\mathbb R^d$, at least $d$ public samples are necessary for private learnability, which is close to the known upper bound of $d+1$ public samples.
</details>
<details>
<summary>摘要</summary>
我们研究公共分布学习问题，即在公共数据和私人数据之间的学习问题。在这种设置下，我们称之为公共私人学习。学习者被公共和私人样本所提供，这些样本来自未知分布$p$，属于一个类$\mathcal Q$。学习者的目标是输出一个估计$p$，同时遵守隐私限制（在这种情况下是纯度ifferential privacy）只针对私人样本。我们证明了公共私人学习的可行性与存在一个样本压缩算法，以及一个中间概念——列表学习有关系。通过这种关系，我们可以：1. 约束previous结果中的高斯分布在$\mathbb R^d$上的恢复;2. 导出新的结果，包括$k$-mixture高斯分布在$\mathbb R^d$上的样本复杂性上的Upper bound，以及agnostic和分布shift抗性的学习者的结果。此外，通过与列表学习的连接，我们还证明了对于高斯分布在$\mathbb R^d$上，至少需要$d$个公共样本以便私人学习可行，这与已知的最高bound($d+1$个公共样本)很接近。
</details></li>
</ul>
<hr>
<h2 id="MaxFloodCast-Ensemble-Machine-Learning-Model-for-Predicting-Peak-Inundation-Depth-And-Decoding-Influencing-Features"><a href="#MaxFloodCast-Ensemble-Machine-Learning-Model-for-Predicting-Peak-Inundation-Depth-And-Decoding-Influencing-Features" class="headerlink" title="MaxFloodCast: Ensemble Machine Learning Model for Predicting Peak Inundation Depth And Decoding Influencing Features"></a>MaxFloodCast: Ensemble Machine Learning Model for Predicting Peak Inundation Depth And Decoding Influencing Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06228">http://arxiv.org/abs/2308.06228</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng-Chun Lee, Lipai Huang, Federico Antolini, Matthew Garcia, Andrew Juanb, Samuel D. Brody, Ali Mostafavi</li>
<li>for:  This study aims to provide efficient and interpretable flood inundation depth predictions using a machine learning model, MaxFloodCast, which can support near-time floodplain management and emergency operations.</li>
<li>methods:  The study uses physics-based hydrodynamic simulations to train the MaxFloodCast model, which achieves reliable flood inundation depth predictions with an average R-squared of 0.949 and a Root Mean Square Error of 0.61 ft on unseen data.</li>
<li>results:  The study validates the MaxFloodCast model against Hurricane Harvey and Storm Imelda, demonstrating its potential in supporting flood risk management and emergency operations. The model provides critical information for decision-makers to prioritize areas with critical facilities and to examine how rainfall in other watersheds influences flood exposure in one area.<details>
<summary>Abstract</summary>
Timely, accurate, and reliable information is essential for decision-makers, emergency managers, and infrastructure operators during flood events. This study demonstrates a proposed machine learning model, MaxFloodCast, trained on physics-based hydrodynamic simulations in Harris County, offers efficient and interpretable flood inundation depth predictions. Achieving an average R-squared of 0.949 and a Root Mean Square Error of 0.61 ft on unseen data, it proves reliable in forecasting peak flood inundation depths. Validated against Hurricane Harvey and Storm Imelda, MaxFloodCast shows the potential in supporting near-time floodplain management and emergency operations. The model's interpretability aids decision-makers in offering critical information to inform flood mitigation strategies, to prioritize areas with critical facilities and to examine how rainfall in other watersheds influences flood exposure in one area. The MaxFloodCast model enables accurate and interpretable inundation depth predictions while significantly reducing computational time, thereby supporting emergency response efforts and flood risk management more effectively.
</details>
<details>
<summary>摘要</summary>
时间、准确、可靠的信息是决策者、紧急管理者和基础设施运营员 durante 洪水事件的重要资讯。本研究展示了一个提议的机器学习模型MaxFloodCast，基于物理基础的水动力 simulations在哈里斯县训练，可提供优化和可解释的洪涛深度预测。在未见数据上，它实现了0.949的平均R-squared和0.61 ft的根幂平均误差。这证明了MaxFloodCast在预测洪涛峰值深度方面的可靠性。验证了飓风哈维和飓风Imelda，MaxFloodCast表明它具有支持即时洪平原管理和紧急作业的潜力。模型的解释性帮助决策者提供重要信息，以帮助实施洪水缓解策略，优先级有 kritical 设施区域，并考虑在其他水系中降雨如何影响洪涛暴露在一个区域。MaxFloodCast 模型可提供高精度和解释性的洪涛深度预测，同时大幅降低计算时间，因此更有效地支持紧急回应努力和洪水风险管理。
</details></li>
</ul>
<hr>
<h2 id="Automated-Sizing-and-Training-of-Efficient-Deep-Autoencoders-using-Second-Order-Algorithms"><a href="#Automated-Sizing-and-Training-of-Efficient-Deep-Autoencoders-using-Second-Order-Algorithms" class="headerlink" title="Automated Sizing and Training of Efficient Deep Autoencoders using Second Order Algorithms"></a>Automated Sizing and Training of Efficient Deep Autoencoders using Second Order Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06221">http://arxiv.org/abs/2308.06221</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kanishka Tyagi, Chinmay Rane, Michael Manry</li>
<li>for: 这个论文是为了设计一种通用的线性分类器而写的。</li>
<li>methods: 论文使用了一种多步训练方法，包括初始化多类线性分类器、验证错误最小化、提高输出和批量训练算法。</li>
<li>results: 论文通过多步训练和杜邦法听到一个高效的深度学习模型，并且在多个公共数据集上实现了性能提升。<details>
<summary>Abstract</summary>
We propose a multi-step training method for designing generalized linear classifiers. First, an initial multi-class linear classifier is found through regression. Then validation error is minimized by pruning of unnecessary inputs. Simultaneously, desired outputs are improved via a method similar to the Ho-Kashyap rule. Next, the output discriminants are scaled to be net functions of sigmoidal output units in a generalized linear classifier. We then develop a family of batch training algorithm for the multi layer perceptron that optimizes its hidden layer size and number of training epochs. Next, we combine pruning with a growing approach. Later, the input units are scaled to be the net function of the sigmoidal output units that are then feed into as input to the MLP. We then propose resulting improvements in each of the deep learning blocks thereby improving the overall performance of the deep architecture. We discuss the principles and formulation regarding learning algorithms for deep autoencoders. We investigate several problems in deep autoencoders networks including training issues, the theoretical, mathematical and experimental justification that the networks are linear, optimizing the number of hidden units in each layer and determining the depth of the deep learning model. A direct implication of the current work is the ability to construct fast deep learning models using desktop level computational resources. This, in our opinion, promotes our design philosophy of building small but powerful algorithms. Performance gains are demonstrated at each step. Using widely available datasets, the final network's ten fold testing error is shown to be less than that of several other linear, generalized linear classifiers, multi layer perceptron and deep learners reported in the literature.
</details>
<details>
<summary>摘要</summary>
我们提出一种多步训练方法用于设计通用线性分类器。首先，通过回归获得初始多类线性分类器。然后，通过剪枝消除不必要的输入，提高验证错误。同时，通过类似于何-卡希普规则提高 желаем的输出。接着，输出推定器被映射到通用线性分类器中的sigmoid输出单元。我们然后开发了一家批处理训练算法，用于最优化多层感知器的隐藏层大小和训练轮数。接着，我们结合剪枝与增长方法。然后，输入单元被映射到sigmoid输出单元中的net函数。我们最后提出了改进每个深度学习块的结果，从而提高整体深度学习模型的性能。我们讨论了深度学习算法的学习原理和形式，并对深度学习网络中的许多问题进行研究，包括训练问题、理论、数学和实验的正确性。我们的研究表明，通过使用桌面级计算资源，可以快速构建深度学习模型，这与我们的设计哲学相符。我们的实验表明，使用常用的数据集，最终网络的十倍测试错误小于其他线性、通用线性分类器、多层感知器和深度学习者在文献中报道的错误。
</details></li>
</ul>
<hr>
<h2 id="Change-Point-Detection-With-Conceptors"><a href="#Change-Point-Detection-With-Conceptors" class="headerlink" title="Change Point Detection With Conceptors"></a>Change Point Detection With Conceptors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06213">http://arxiv.org/abs/2308.06213</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/noahgade/changepointdetectionwithconceptors">https://github.com/noahgade/changepointdetectionwithconceptors</a></li>
<li>paper_authors: Noah D. Gade, Jordan Rodu</li>
<li>for: 这篇论文主要是为了解决时间序列中数据生成过程发生变化的问题。</li>
<li>methods: 该方法使用一个概念矩阵来学习时间序列的特征动力学，然后通过一个随机回归神经网络来抽象数据，最后通过一个多variate的距离量来确定变化点。</li>
<li>results: 该方法可以提供可靠的变化点估计，并且可以通过Bootstrap方法来生成资料的类型1错误控制。在一些模拟数据和实际数据上测试了该方法，并评估了其性能。<details>
<summary>Abstract</summary>
Offline change point detection seeks to identify points in a time series where the data generating process changes. This problem is well studied for univariate i.i.d. data, but becomes challenging with increasing dimension and temporal dependence. For the at most one change point problem, we propose the use of a conceptor matrix to learn the characteristic dynamics of a specified training window in a time series. The associated random recurrent neural network acts as a featurizer of the data, and change points are identified from a univariate quantification of the distance between the featurization and the space spanned by a representative conceptor matrix. This model agnostic method can suggest potential locations of interest that warrant further study. We prove that, under mild assumptions, the method provides a consistent estimate of the true change point, and quantile estimates for statistics are produced via a moving block bootstrap of the original data. The method is tested on simulations from several classes of processes, and we evaluate performance with clustering metrics, graphical methods, and observed Type 1 error control. We apply our method to publicly available neural data from rats experiencing bouts of non-REM sleep prior to exploration of a radial maze.
</details>
<details>
<summary>摘要</summary>
停机变点检测目标是找到时间序列中数据生成过程中的变化点。这个问题在独立同分布数据上得到了广泛的研究，但是随着维度和时间相关性的增加，这个问题就变得更加挑战。为了解决这个问题，我们提出了一种基于特征动态矩阵的方法。这个矩阵可以学习指定的训练窗口中的特征动态，然后通过一个随机循环神经网络来抽象数据。通过评估这个抽象和特征矩阵所生成的空间之间的距离，我们可以确定变点。这种模型无关的方法可以提供有利于进一步研究的可能性。我们证明，在某些假设下，这种方法可以提供一个一致的变点估计，并且可以通过移动块bootstrap来生成量统计。我们在一些类型的过程的 simulations 上测试了这种方法，并评估了它们的性能使用 clustering 度量、图形方法和观察到的类型一错控制。最后，我们应用了这种方法在公共可用的 neural 数据上，该数据来自于在非 REM 睡眠前的猫鼠在 радиаль 迷宫中的探索。
</details></li>
</ul>
<hr>
<h2 id="Safety-in-Traffic-Management-Systems-A-Comprehensive-Survey"><a href="#Safety-in-Traffic-Management-Systems-A-Comprehensive-Survey" class="headerlink" title="Safety in Traffic Management Systems: A Comprehensive Survey"></a>Safety in Traffic Management Systems: A Comprehensive Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06204">http://arxiv.org/abs/2308.06204</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenlu Du, Ankan Dash, Jing Li, Hua Wei, Guiling Wang</li>
<li>for: 这项研究的目的是为了对交通管理系统中的安全问题进行全面的文献综述，以便更好地了解这些系统的安全问题，并提出解决方案。</li>
<li>methods: 本文使用了文献综述的方法来检查交通管理系统中的安全问题，并分析了现有的研究成果。</li>
<li>results: 本文发现了交通管理系统中的安全问题，包括系统设计缺陷、车辆通信问题、人工意识问题等，并提出了一些解决方案，如使用隐藏马尔文网络、增加人工意识等。同时，本文也指出了现有研究的限制，如缺乏实验数据和难以模拟实际情况等。<details>
<summary>Abstract</summary>
Traffic management systems play a vital role in ensuring safe and efficient transportation on roads. However, the use of advanced technologies in traffic management systems has introduced new safety challenges. Therefore, it is important to ensure the safety of these systems to prevent accidents and minimize their impact on road users. In this survey, we provide a comprehensive review of the literature on safety in traffic management systems. Specifically, we discuss the different safety issues that arise in traffic management systems, the current state of research on safety in these systems, and the techniques and methods proposed to ensure the safety of these systems. We also identify the limitations of the existing research and suggest future research directions.
</details>
<details>
<summary>摘要</summary>
交通管理系统在路面交通安全和效率的问题上扮演着重要的角色。然而，进步的科技应用在交通管理系统中带来了新的安全挑战。因此，确保交通管理系统的安全性是不可或缺的。在这份调查中，我们提供了交通管理系统安全的全面评论。具体来说，我们讨论了交通管理系统中不同的安全问题，现有的研究状况，以及确保这些系统安全的技术和方法。我们还识别出现有的研究限制，并建议未来研究方向。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/12/cs.LG_2023_08_12/" data-id="clpxp041f00q2fm8850iqc7h0" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_12" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/12/eess.IV_2023_08_12/" class="article-date">
  <time datetime="2023-08-12T09:00:00.000Z" itemprop="datePublished">2023-08-12</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/12/eess.IV_2023_08_12/">eess.IV - 2023-08-12</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Semantic-Communications-with-Explicit-Semantic-Base-for-Image-Transmission"><a href="#Semantic-Communications-with-Explicit-Semantic-Base-for-Image-Transmission" class="headerlink" title="Semantic Communications with Explicit Semantic Base for Image Transmission"></a>Semantic Communications with Explicit Semantic Base for Image Transmission</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06599">http://arxiv.org/abs/2308.06599</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuan Zheng, Fengyu Wang, Wenjun Xu, Miao Pan, Ping Zhang</li>
<li>for: 提高下一代通信系统的可靠性和效率，通过增强信息的意义表示和同步。</li>
<li>methods: 提出一个基于显式 semantics 基础 (Seb) 的Semantic 图像传输框架，通过自适应的 Seb 生成和应用来实现图像的表示和传输。</li>
<li>results: 对比州的实验结果显示，提出的框架可以与现有的方法相比，在不同的信号噪含率 (SNR) 下提高 peak signal-to-noise ratio (PSNR) 的表现，提高图像的重建精度。<details>
<summary>Abstract</summary>
Semantic communications, aiming at ensuring the successful delivery of the meaning of information, are expected to be one of the potential techniques for the next generation communications. However, the knowledge forming and synchronizing mechanism that enables semantic communication systems to extract and interpret the semantics of information according to the communication intents is still immature. In this paper, we propose a semantic image transmission framework with explicit semantic base (Seb), where Sebs are generated and employed as the knowledge shared between the transmitter and the receiver with flexible granularity. To represent images with Sebs, a novel Seb-based reference image generator is proposed to generate Sebs and then decompose the transmitted images. To further encode/decode the residual information for precise image reconstruction, a Seb-based image encoder/decoder is proposed. The key components of the proposed framework are optimized jointly by end-to-end (E2E) training, where the loss function is dedicated designed to tackle the problem of nondifferentiable operation in Seb-based reference image generator by introducing a gradient approximation mechanism. Extensive experiments show that the proposed framework outperforms state-of-art works by 0.5 - 1.5 dB in peak signal-to-noise ratio (PSNR) w.r.t. different signal-to-noise ratio (SNR).
</details>
<details>
<summary>摘要</summary>
semantic 通信技术，预计将成为下一代通信技术的一个 potential 方向，但现有的知识形成和同步机制仍然是幼稚的。本文提出了一种具有显式semantic base（Seb）的semantic 图像传输框架，其中Sebs是在传输和接收方之间共享的知识，并可以在不同的粒度水平进行flexible 分配。为了将图像表示为Sebs，本文提出了一种新的Seb-based reference image generator，该generator可以生成Sebs并将传输的图像进行解码。另外，为了进一步编码/解码剩余信息以实现精确的图像重建，本文提出了一种基于Seb的图像编码器/解码器。关键组件的优化由综合（E2E）训练进行，loss函数通过引入梯度近似机制来解决Seb-based reference image generator中的非导数操作问题。实验表明，提议的框架可以与当前最佳性能相比提高0.5-1.5 dB的峰值信号噪声比（PSNR）。
</details></li>
</ul>
<hr>
<h2 id="On-Versatile-Video-Coding-at-UHD-with-Machine-Learning-Based-Super-Resolution"><a href="#On-Versatile-Video-Coding-at-UHD-with-Machine-Learning-Based-Super-Resolution" class="headerlink" title="On Versatile Video Coding at UHD with Machine-Learning-Based Super-Resolution"></a>On Versatile Video Coding at UHD with Machine-Learning-Based Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06570">http://arxiv.org/abs/2308.06570</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kristian Fischer, Christian Herglotz, André Kaup</li>
<li>for: 提高4K数据编码质量</li>
<li>methods: 使用Machine Learning基于单图超解算法和下一代VVC编码器</li>
<li>results: 可以获得12%~18%的Bjontegaard delta rate增加，并减少 compression artifacts 和loss of detailsHere’s a breakdown of each point:</li>
<li>for: 这篇论文是为了提高4K数据的编码质量而写的。</li>
<li>methods: 这篇论文使用了Machine Learning基于单图超解算法和下一代VVC编码器。</li>
<li>results: 根据论文的测试结果，可以获得12%~18%的Bjontegaard delta rate增加，同时减少 compression artifacts 和loss of details。<details>
<summary>Abstract</summary>
Coding 4K data has become of vital interest in recent years, since the amount of 4K data is significantly increasing. We propose a coding chain with spatial down- and upscaling that combines the next-generation VVC codec with machine learning based single image super-resolution algorithms for 4K. The investigated coding chain, which spatially downscales the 4K data before coding, shows superior quality than the conventional VVC reference software for low bitrate scenarios. Throughout several tests, we find that up to 12 % and 18 % Bjontegaard delta rate gains can be achieved on average when coding 4K sequences with VVC and QP values above 34 and 42, respectively. Additionally, the investigated scenario with up- and downscaling helps to reduce the loss of details and compression artifacts, as it is shown in a visual example.
</details>
<details>
<summary>摘要</summary>
coding 4K 数据已成为近年来的焦点，因为4K 数据量在不断增加。我们提议一种具有空间下采样和上采样的编码链，该链 combining 下一代 VVC 编码器和基于机器学习的单张图像超解算法，用于4K。我们的调查显示，将4K 数据先下采样后编码，可以在低比特率场景下实现较高的质量。在多个测试中，我们发现，使用 VVC 和 QP 值高于 34 和 42 时，可以实现平均 12% 到 18% Bjontegaard Delta 率增加。此外，我们发现，将数据上下采样可以降低数据的丢失细节和压缩artefacts，如图像示例所示。Note: "Bjontegaard delta rate" refers to the difference between the peak signal-to-noise ratio (PSNR) of the original and reconstructed signals, which is a common measure of the quality of video compression. A lower Bjontegaard delta rate indicates better compression quality.
</details></li>
</ul>
<hr>
<h2 id="Three-dimensional-echo-shifted-EPI-with-simultaneous-blip-up-and-blip-down-acquisitions-for-correcting-geometric-distortion"><a href="#Three-dimensional-echo-shifted-EPI-with-simultaneous-blip-up-and-blip-down-acquisitions-for-correcting-geometric-distortion" class="headerlink" title="Three-dimensional echo-shifted EPI with simultaneous blip-up and blip-down acquisitions for correcting geometric distortion"></a>Three-dimensional echo-shifted EPI with simultaneous blip-up and blip-down acquisitions for correcting geometric distortion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06557">http://arxiv.org/abs/2308.06557</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaibao Sun, Zhifeng Chen, Guangyu Dan, Qingfei Luo, Lirong Yan, Feng Liu, Xiaohong Joe Zhou<br>For:* 这个研究旨在超越BUDA doublescan时间和降低功能MRI应用中的挑战，通过开发一种三维echo-shifted EPI BUDA（esEPI-BUDA）技术，以获得单shot中获得两个blip-up和blip-down数据集。Methods:* 这种三维esEPI-BUDA序列使用了一种echo-shifting策略，生成了两个EPI读取车的读取信号。这两个读取信号的k-空间轨迹相互交叠，并且使用了opposite phase-encoding gradient方向。这两个k-空间数据集分别使用3D SENSE算法重建，并生成了时间解决B0-场图像。Results:* 在一个phantom和一个人类大脑图像研究中，这种3D esEPI-BUDA技术可以有效地纠正几何扭曲。在人类大脑图像中，可以看到视觉激活区和其BOLD响应，与普通3D echo-planar图像相似。<details>
<summary>Abstract</summary>
Purpose: Echo-planar imaging (EPI) with blip-up/down acquisition (BUDA) can provide high-quality images with minimal distortions by using two readout trains with opposing phase-encoding gradients. Because of the need for two separate acquisitions, BUDA doubles the scan time and degrades the temporal resolution when compared to single-shot EPI, presenting a major challenge for many applications, particularly functional MRI (fMRI). This study aims at overcoming this challenge by developing an echo-shifted EPI BUDA (esEPI-BUDA) technique to acquire both blip-up and blip-down datasets in a single shot. Methods: A three-dimensional (3D) esEPI-BUDA pulse sequence was designed by using an echo-shifting strategy to produce two EPI readout trains. These readout trains produced a pair of k-space datasets whose k-space trajectories were interleaved with opposite phase-encoding gradient directions. The two k-space datasets were separately reconstructed using a 3D SENSE algorithm, from which time-resolved B0-field maps were derived using TOPUP in FSL and then input into a forward model of joint parallel imaging reconstruction to correct for geometric distortion. In addition, Hankel structured low-rank constraint was incorporated into the reconstruction framework to improve image quality by mitigating the phase errors between the two interleaved k-space datasets. Results: The 3D esEPI-BUDA technique was demonstrated in a phantom and an fMRI study on healthy human subjects. Geometric distortions were effectively corrected in both phantom and human brain images. In the fMRI study, the visual activation volumes and their BOLD responses were comparable to those from conventional 3D echo-planar images. Conclusion: The improved imaging efficiency and dynamic distortion correction capability afforded by 3D esEPI-BUDA are expected to benefit many EPI applications.
</details>
<details>
<summary>摘要</summary>
目的：使用电平扫描（EPI）和折叠/下降获取（BUDA）技术可以提供高质量图像，并且减少了图像扭曲。然而，由于需要两个分离的获取，BUDA将扫描时间 Doubles和功能磁共振成像（fMRI）等应用中的时间分辨率下降为主要挑战。本研究旨在解决这个挑战，通过开发一种三维电平扫描BUDA（esEPI-BUDA）技术，以获取一个单击数据集。方法：使用电平扫描扩展策略，生成两个EPI读取列。这两个读取列在干扰方向上具有相反的频率编码梯度。这两个k空间数据集分别使用3D SENSE算法重建，并使用FSL中的TOPUP算法生成时间相关的B0场图。然后，将这些图像输入到一种前向模型，以正确地修正几何错误。此外，在重建框架中添加了具有Hankel结构的低级别约束，以提高图像质量，减少相位错误。结果：在荚体和人类大脑图像中，使用3D esEPI-BUDA技术可以有效地纠正几何错误。在fMRI研究中，观察到的视觉激活体和其BOLD响应与普通3D电平扫描图像相同。结论：3D esEPI-BUDA技术的改进的扫描效率和动态几何纠正能力，预期会对许多EPI应用产生积极的影响。
</details></li>
</ul>
<hr>
<h2 id="The-Color-Clifford-Hardy-Signal-Application-to-Color-Edge-Detection-and-Optical-Flow"><a href="#The-Color-Clifford-Hardy-Signal-Application-to-Color-Edge-Detection-and-Optical-Flow" class="headerlink" title="The Color Clifford Hardy Signal: Application to Color Edge Detection and Optical Flow"></a>The Color Clifford Hardy Signal: Application to Color Edge Detection and Optical Flow</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06485">http://arxiv.org/abs/2308.06485</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoxiao Hu, Kit Ian Kou, Cuiming Zou, Dong Cheng</li>
<li>for: This paper introduces a new approach to processing color images using the color Clifford Hardy signal, which is a high-dimensional analytic function.</li>
<li>methods: The paper proposes five methods for edge detection in color images based on the local feature representation of the color Clifford Hardy signal. These methods utilize the multi-scale structure of the signal to resist noise and improve edge detection accuracy.</li>
<li>results: The proposed methods are evaluated using image quality assessment criteria and are shown to be superior to traditional edge detection methods in terms of robustness to noise and accuracy. Additionally, an example application of color optical flow detection using the proposed approach is provided.<details>
<summary>Abstract</summary>
This paper introduces the idea of the color Clifford Hardy signal, which can be used to process color images. As a complex analytic function's high-dimensional analogue, the color Clifford Hardy signal inherits many desirable qualities of analyticity. A crucial tool for getting the color and structural data is the local feature representation of a color image in the color Clifford Hardy signal. By looking at the extended Cauchy-Riemann equations in the high-dimensional space, it is possible to see the connection between the different parts of the color Clifford Hardy signal. Based on the distinctive and important local amplitude and local phase generated by the color Clifford Hardy signal, we propose five methods to identify the edges of color images with relation to a certain color. To prove the superiority of the offered methodologies, numerous comparative studies employing image quality assessment criteria are used. Specifically by using the multi-scale structure of the color Clifford Hardy signal, the proposed approaches are resistant to a variety of noises. In addition, a color optical flow detection method with anti-noise ability is provided as an example of application.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Out-of-distribution-multi-view-auto-encoders-for-prostate-cancer-lesion-detection"><a href="#Out-of-distribution-multi-view-auto-encoders-for-prostate-cancer-lesion-detection" class="headerlink" title="Out-of-distribution multi-view auto-encoders for prostate cancer lesion detection"></a>Out-of-distribution multi-view auto-encoders for prostate cancer lesion detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06481">http://arxiv.org/abs/2308.06481</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alvaro Fernandez-Quilez, Linas Vidziunas, Ørjan Kløvfjell Thoresen, Ketil Oppedal, Svein Reidar Kjosavik, Trygve Eftestøl</li>
<li>for: 这个研究旨在提出一种基于不监督学习的医学领域深度学习方法，以减少罕见的标注数据。</li>
<li>methods: 我们提出了一种多流水平方法，可以处理不同的T2w方向，以提高肝癌液体检测的性能。</li>
<li>results: 我们在公开可用的数据集上评估了我们的方法，与单向方法相比，我们的方法实现了更好的检测结果（AUC&#x3D;82.3%）。<details>
<summary>Abstract</summary>
Traditional deep learning (DL) approaches based on supervised learning paradigms require large amounts of annotated data that are rarely available in the medical domain. Unsupervised Out-of-distribution (OOD) detection is an alternative that requires less annotated data. Further, OOD applications exploit the class skewness commonly present in medical data. Magnetic resonance imaging (MRI) has proven to be useful for prostate cancer (PCa) diagnosis and management, but current DL approaches rely on T2w axial MRI, which suffers from low out-of-plane resolution. We propose a multi-stream approach to accommodate different T2w directions to improve the performance of PCa lesion detection in an OOD approach. We evaluate our approach on a publicly available data-set, obtaining better detection results in terms of AUC when compared to a single direction approach (73.1 vs 82.3). Our results show the potential of OOD approaches for PCa lesion detection based on MRI.
</details>
<details>
<summary>摘要</summary>
传统的深度学习（DL）方法基于指导学习思想需要庞大量的标注数据，而医疗领域中这些数据很少。无监管 OUT-OF-DISTRIBUTION（OOD）检测是一种alternative，它需要 fewer annotated data。另外，OOD应用可以利用医学数据中的类倾斜。核磁共振成像（MRI）已经被证明是肠癌（PCa）诊断和管理的有用工具，但现有的DL方法仅仅利用T2w极向MRI，这种MRIuffer from low out-of-plane resolution。我们提议一种多流处理方法，以便同时处理不同的T2w方向，以提高PCa涂抹检测的性能。我们在公共可用数据集上评估了我们的方法，并与单向方法（73.1）进行比较，得到了更好的检测结果（AUC），即82.3。我们的结果表明，基于MRI的PCa涂抹检测可以通过OOD方法实现更高的性能。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-multi-view-data-without-annotations-for-prostate-MRI-segmentation-A-contrastive-approach"><a href="#Leveraging-multi-view-data-without-annotations-for-prostate-MRI-segmentation-A-contrastive-approach" class="headerlink" title="Leveraging multi-view data without annotations for prostate MRI segmentation: A contrastive approach"></a>Leveraging multi-view data without annotations for prostate MRI segmentation: A contrastive approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06477">http://arxiv.org/abs/2308.06477</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tim Nikolass Lindeijer, Tord Martin Ytredal, Trygve Eftestøl, Tobias Nordström, Fredrik Jäderling, Martin Eklund, Alvaro Fernandez-Quilez</li>
<li>for: 这个研究的目的是提高自动脑下对肾脏的分类和量化描述，以支持肾脏癌病的诊断。</li>
<li>methods: 这个研究使用了一种对照方法，即在训练时使用多个检测方向的数据，并且不需要手动标注。</li>
<li>results: 研究结果显示，使用对照方法可以提高肾脏分类的精确度，并且在不同的检测方向下实现了较好的一致性。<details>
<summary>Abstract</summary>
An accurate prostate delineation and volume characterization can support the clinical assessment of prostate cancer. A large amount of automatic prostate segmentation tools consider exclusively the axial MRI direction in spite of the availability as per acquisition protocols of multi-view data. Further, when multi-view data is exploited, manual annotations and availability at test time for all the views is commonly assumed. In this work, we explore a contrastive approach at training time to leverage multi-view data without annotations and provide flexibility at deployment time in the event of missing views. We propose a triplet encoder and single decoder network based on U-Net, tU-Net (triplet U-Net). Our proposed architecture is able to exploit non-annotated sagittal and coronal views via contrastive learning to improve the segmentation from a volumetric perspective. For that purpose, we introduce the concept of inter-view similarity in the latent space. To guide the training, we combine a dice score loss calculated with respect to the axial view and its manual annotations together with a multi-view contrastive loss. tU-Net shows statistical improvement in dice score coefficient (DSC) with respect to only axial view (91.25+-0.52% compared to 86.40+-1.50%,P<.001). Sensitivity analysis reveals the volumetric positive impact of the contrastive loss when paired with tU-Net (2.85+-1.34% compared to 3.81+-1.88%,P<.001). Further, our approach shows good external volumetric generalization in an in-house dataset when tested with multi-view data (2.76+-1.89% compared to 3.92+-3.31%,P=.002), showing the feasibility of exploiting non-annotated multi-view data through contrastive learning whilst providing flexibility at deployment in the event of missing views.
</details>
<details>
<summary>摘要</summary>
<<sys.translation.skip>>精准的肾脏定位和体积特征化可以支持肾脏癌诊断。大量自动肾脏分割工具忽略了多视图数据的可用性，即使据获取协议可以获得多视图数据。此外，当使用多视图数据时，手动标注和测试时 disponibility 通常被假设。在这种情况下，我们提出了一种对比方法，使得在训练时可以利用多视图数据无需标注，并在部署时提供灵活性。我们提出了一种基于 U-Net 的 triplet 编码器和单个解码器网络，我们称之为 tU-Net（ triplet U-Net）。我们的提议的架构可以通过对不同视图的对比学习利用非标注的 sagittal 和横截视图来提高分割。为此，我们引入了视图间相似性的概念在幂空间。为了导航训练，我们将 dice 分数损失与AXIAL 视图和其手动标注相加，并与多视图对比损失相结合。 tU-Net 显示在 DSC 系数上有统计学上的提升（91.25+-0.52% 相比 86.40+-1.50%,P<.001）。敏感分析表明，对于 tU-Net 来说，对比损失的负面影响是可观的（2.85+-1.34% 相比 3.81+-1.88%,P<.001）。此外，我们的方法在我们的内部数据集中表现了良好的外部Volumetric 普适性（2.76+-1.89% 相比 3.92+-3.31%,P=.002），这表明了可以通过对比学习利用非标注多视图数据，并在部署时提供灵活性。
</details></li>
</ul>
<hr>
<h2 id="CATS-v2-Hybrid-encoders-for-robust-medical-segmentation"><a href="#CATS-v2-Hybrid-encoders-for-robust-medical-segmentation" class="headerlink" title="CATS v2: Hybrid encoders for robust medical segmentation"></a>CATS v2: Hybrid encoders for robust medical segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06377">http://arxiv.org/abs/2308.06377</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/haoli12345/cats">https://github.com/haoli12345/cats</a></li>
<li>paper_authors: Hao Li, Han Liu, Dewei Hu, Xing Yao, Jiacheng Wang, Ipek Oguz<br>for:This paper proposes a new method for 3D medical image segmentation, specifically for vestibular schwannoma (VS) and prostate segmentation.methods:The proposed method uses a hybrid encoder consisting of a CNN-based encoder path and a transformer path with a shifted window to leverage both local and global information.results:The proposed method demonstrates superior performance in terms of higher Dice scores compared to state-of-the-art methods on two public challenge datasets (CrossMoDA and MSD-5) for VS and prostate segmentation.<details>
<summary>Abstract</summary>
Convolutional Neural Networks (CNNs) have exhibited strong performance in medical image segmentation tasks by capturing high-level (local) information, such as edges and textures. However, due to the limited field of view of convolution kernel, it is hard for CNNs to fully represent global information. Recently, transformers have shown good performance for medical image segmentation due to their ability to better model long-range dependencies. Nevertheless, transformers struggle to capture high-level spatial features as effectively as CNNs. A good segmentation model should learn a better representation from local and global features to be both precise and semantically accurate. In our previous work, we proposed CATS, which is a U-shaped segmentation network augmented with transformer encoder. In this work, we further extend this model and propose CATS v2 with hybrid encoders. Specifically, hybrid encoders consist of a CNN-based encoder path paralleled to a transformer path with a shifted window, which better leverage both local and global information to produce robust 3D medical image segmentation. We fuse the information from the convolutional encoder and the transformer at the skip connections of different resolutions to form the final segmentation. The proposed method is evaluated on two public challenge datasets: Cross-Modality Domain Adaptation (CrossMoDA) and task 5 of Medical Segmentation Decathlon (MSD-5), to segment vestibular schwannoma (VS) and prostate, respectively. Compared with the state-of-the-art methods, our approach demonstrates superior performance in terms of higher Dice scores.
</details>
<details>
<summary>摘要</summary>
卷积神经网络（CNN）在医学图像分割任务中表现出色，通过捕捉高级（本地）信息，如边缘和文本ure。然而，由于卷积核心的视野有限，使得CNN难以完全表征全局信息。而在最近的几年，转移器在医学图像分割中表现良好，这是因为它们可以更好地模型长距离依赖关系。然而，转移器在捕捉高级空间特征方面表现不如CNN一样好。一个好的分割模型应该学习更好的表示本地和全局特征，以便具有高精度和Semantic Accuracy。在我们之前的工作中，我们提出了CATS，它是一个U型分割网络，其中包括转移器编码器。在这个工作中，我们进一步扩展了这个模型，并提出了CATS v2，其中包括混合编码器。特别是，混合编码器包括一个基于CNN的编码器路径和一个偏移窗口的转移器路径，这些路径都可以更好地利用本地和全局信息，以生成Robust 3D医学图像分割。我们在不同分辨率的 skip 连接中 fusion 了 convolutional 编码器和转移器的信息，以生成最终的分割。我们的方法在 Cross-Modality Domain Adaptation（CrossMoDA）和 Medical Segmentation Decathlon（MSD-5）两个公共挑战数据集上进行评估，用于分割 vestibular schwannoma（VS）和肾脏，分别。与当前状态艺术方法相比，我们的方法在 dice 分数上表现出优于其他方法。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Based-Open-Source-Toolkit-for-Eosinophil-Detection-in-Pediatric-Eosinophilic-Esophagitis"><a href="#Deep-Learning-Based-Open-Source-Toolkit-for-Eosinophil-Detection-in-Pediatric-Eosinophilic-Esophagitis" class="headerlink" title="Deep Learning-Based Open Source Toolkit for Eosinophil Detection in Pediatric Eosinophilic Esophagitis"></a>Deep Learning-Based Open Source Toolkit for Eosinophil Detection in Pediatric Eosinophilic Esophagitis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06333">http://arxiv.org/abs/2308.06333</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hrlblab/open-eoe">https://github.com/hrlblab/open-eoe</a></li>
<li>paper_authors: Juming Xiong, Yilin Liu, Ruining Deng, Regina N Tyree, Hernan Correa, Girish Hiremath, Yaohong Wang, Yuankai Huo<br>for: 这个研究是为了开发一个开源的工具集（Open-EoE），用于检测食道检查图像（Whole Slide Image，WSI）中的嗜酸细胞（Eos）。methods: 该工具集使用三种state-of-the-art深度学习基于对象检测模型，并实现了一种 ensemble learning 策略以提高性能。results: 实验结果表明，Open-EoE 工具集可以有效地检测食道检查图像中的嗜酸细胞，并达到了91%的准确率，与专业病理学家的评估相一致。<details>
<summary>Abstract</summary>
Eosinophilic Esophagitis (EoE) is a chronic, immune/antigen-mediated esophageal disease, characterized by symptoms related to esophageal dysfunction and histological evidence of eosinophil-dominant inflammation. Owing to the intricate microscopic representation of EoE in imaging, current methodologies which depend on manual identification are not only labor-intensive but also prone to inaccuracies. In this study, we develop an open-source toolkit, named Open-EoE, to perform end-to-end whole slide image (WSI) level eosinophil (Eos) detection using one line of command via Docker. Specifically, the toolkit supports three state-of-the-art deep learning-based object detection models. Furthermore, Open-EoE further optimizes the performance by implementing an ensemble learning strategy, and enhancing the precision and reliability of our results. The experimental results demonstrated that the Open-EoE toolkit can efficiently detect Eos on a testing set with 289 WSIs. At the widely accepted threshold of >= 15 Eos per high power field (HPF) for diagnosing EoE, the Open-EoE achieved an accuracy of 91%, showing decent consistency with pathologist evaluations. This suggests a promising avenue for integrating machine learning methodologies into the diagnostic process for EoE. The docker and source code has been made publicly available at https://github.com/hrlblab/Open-EoE.
</details>
<details>
<summary>摘要</summary>
《营养细胞性食管炎（EoE）》是一种慢性、免疫/抗原识别的食管疾病，表现为食管功能障碍和 histological 证明中充满嗜铁细胞的inflammation。由于诊断EoE的微scopic representation在成像中复杂，现有的方法ologies都是人工识别，不仅劳动密集，还容易出错。在这项研究中，我们开发了一个开源工具包，名为Open-EoE，通过一行命令via Docker进行整个报告图像（WSI）层级的嗜铁细胞（Eos）检测。具体来说，工具支持三种当前顶尖的深度学习基于对象检测模型。此外，Open-EoE还进一步优化了性能，通过实现 ensemble learning 策略，提高了结果的精度和可靠性。实验结果表明，Open-EoE 工具包可以有效地检测289张WSIs中的嗜铁细胞。在 widely accepted 的 >= 15 Eos per high power field（HPF）的标准reshold上，Open-EoE 达到了91%的准确率，与Pathologist 评估相当一致。这表明了机器学习方法的可能性在EoE 诊断过程中的应用。docker 和源代码已经公开发布在https://github.com/hrlblab/Open-EoE。
</details></li>
</ul>
<hr>
<h2 id="Revolutionizing-Space-Health-Swin-FSR-Advancing-Super-Resolution-of-Fundus-Images-for-SANS-Visual-Assessment-Technology"><a href="#Revolutionizing-Space-Health-Swin-FSR-Advancing-Super-Resolution-of-Fundus-Images-for-SANS-Visual-Assessment-Technology" class="headerlink" title="Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology"></a>Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06332">http://arxiv.org/abs/2308.06332</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/FarihaHossain/SwinFSR">https://github.com/FarihaHossain/SwinFSR</a></li>
<li>paper_authors: Khondker Fariha Hossain, Sharif Amit Kamran, Joshua Ong, Andrew G. Lee, Alireza Tavakkoli<br>for:This paper is written for the purpose of developing a novel model for fundus image super-resolution, specifically using Swin Transformer with spatial and depth-wise attention.methods:The paper utilizes a novel model called Swin-FSR, which combines Swin Transformer with spatial and depth-wise attention for fundus image super-resolution.results:The paper achieves Peak signal-to-noise-ratio (PSNR) of 47.89, 49.00 and 45.32 on three public datasets, namely iChallenge-AMD, iChallenge-PM, and G1020. Additionally, the model showed comparable results on a privately held dataset for Spaceflight-associated Neuro-ocular Syndrome (SANS) provided by NASA.<details>
<summary>Abstract</summary>
The rapid accessibility of portable and affordable retinal imaging devices has made early differential diagnosis easier. For example, color funduscopy imaging is readily available in remote villages, which can help to identify diseases like age-related macular degeneration (AMD), glaucoma, or pathological myopia (PM). On the other hand, astronauts at the International Space Station utilize this camera for identifying spaceflight-associated neuro-ocular syndrome (SANS). However, due to the unavailability of experts in these locations, the data has to be transferred to an urban healthcare facility (AMD and glaucoma) or a terrestrial station (e.g, SANS) for more precise disease identification. Moreover, due to low bandwidth limits, the imaging data has to be compressed for transfer between these two places. Different super-resolution algorithms have been proposed throughout the years to address this. Furthermore, with the advent of deep learning, the field has advanced so much that x2 and x4 compressed images can be decompressed to their original form without losing spatial information. In this paper, we introduce a novel model called Swin-FSR that utilizes Swin Transformer with spatial and depth-wise attention for fundus image super-resolution. Our architecture achieves Peak signal-to-noise-ratio (PSNR) of 47.89, 49.00 and 45.32 on three public datasets, namely iChallenge-AMD, iChallenge-PM, and G1020. Additionally, we tested the model's effectiveness on a privately held dataset for SANS provided by NASA and achieved comparable results against previous architectures.
</details>
<details>
<summary>摘要</summary>
随着可携式和Affordable的Retinal imaging设备的快速访问，早期差异诊断变得更加容易。例如，颜色基准摄影是在偏远村庄中ready available，可以 помо助于诊断年龄相关的macular degeneration（AMD）、 glaucoma 或pathological myopia（PM）等疾病。然而，由于这些地点缺乏专家，因此数据必须被传输到城市医疗机构（AMD和glaucoma）或者地球站（例如，SANS）进行更加精确的疾病诊断。此外，由于带宽限制，摄影数据必须进行压缩传输。过去的多年来，不同的超分辨率算法已经被提出来解决这个问题。此外，随着深度学习的出现，这一领域已经进步到了非常高的水平，可以使x2和x4压缩的图像得到原始形态的还原，无需失去空间信息。在本文中，我们提出了一种新的模型called Swin-FSR，该模型利用SwinTransformer和空间和深度精度注意力来进行fundus图像超分辨率。我们的架构实现了Peak signal-to-noise-ratio（PSNR）的47.89、49.00和45.32在三个公共数据集上，即iChallenge-AMD、iChallenge-PM和G1020。此外，我们对NASA提供的一个私人保持的SANS数据集进行测试，并与之前的建筑物实现了相似的结果。
</details></li>
</ul>
<hr>
<h2 id="A-Hierarchical-Descriptor-Framework-for-On-the-Fly-Anatomical-Location-Matching-between-Longitudinal-Studies"><a href="#A-Hierarchical-Descriptor-Framework-for-On-the-Fly-Anatomical-Location-Matching-between-Longitudinal-Studies" class="headerlink" title="A Hierarchical Descriptor Framework for On-the-Fly Anatomical Location Matching between Longitudinal Studies"></a>A Hierarchical Descriptor Framework for On-the-Fly Anatomical Location Matching between Longitudinal Studies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07337">http://arxiv.org/abs/2308.07337</a></li>
<li>repo_url: None</li>
<li>paper_authors: Halid Ziya Yerebakan, Yoshihisa Shinagawa, Mahesh Ranganath, Simon Allen-Raffl, Gerardo Hermosillo Valadez</li>
<li>for: 医疗影像比较 longitudinal 比较中的相似性匹配</li>
<li>methods: 基于 hierarchical sparse sampling 的描述子计算和 hierarchical 搜索</li>
<li>results: 减少计算时间至毫秒级别，无需依赖于预训练、重新映射或多模态转换，并且在 Deep Lesion Tracking 数据集上达到更高的匹配精度，比最精确的算法 faster 24 倍。<details>
<summary>Abstract</summary>
We propose a method to match anatomical locations between pairs of medical images in longitudinal comparisons. The matching is made possible by computing a descriptor of the query point in a source image based on a hierarchical sparse sampling of image intensities that encode the location information. Then, a hierarchical search operation finds the corresponding point with the most similar descriptor in the target image. This simple yet powerful strategy reduces the computational time of mapping points to a millisecond scale on a single CPU. Thus, radiologists can compare similar anatomical locations in near real-time without requiring extra architectural costs for precomputing or storing deformation fields from registrations. Our algorithm does not require prior training, resampling, segmentation, or affine transformation steps. We have tested our algorithm on the recently published Deep Lesion Tracking dataset annotations. We observed more accurate matching compared to Deep Lesion Tracker while being 24 times faster than the most precise algorithm reported therein. We also investigated the matching accuracy on CT and MR modalities and compared the proposed algorithm's accuracy against ground truth consolidated from multiple radiologists.
</details>
<details>
<summary>摘要</summary>
我们提出了一种方法，用于在医疗影像序列中匹配 анатомиче位置。该方法基于源图像中查询点的Descriptor，该Descriptor通过 hierarchical sparse sampling 图像强度编码位置信息来计算。然后，使用 hierarchical 搜索操作找到目标图像中最相似的点。这种简单 yet powerful 策略可以在单个 CPU 上减少计算时间到毫秒级，因此 radiologist 可以在实时比较相似的 анатомиче位置，不需要额外的建筑成本或存储投影场景的预处理或存储步骤。我们的算法不需要先行训练、重新采样、分割或 affine 变换步骤。我们在 Deep Lesion Tracking 数据集注释中进行了测试，并观察到比 Deep Lesion Tracker 更高的匹配精度，同时比最精确的算法reported therein 24 倍快。我们还对 CT 和 MR 模式进行了匹配精度的研究，并与多名医生共同合理的ground truth 进行了比较。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/12/eess.IV_2023_08_12/" data-id="clpxp048l0193fm88fh871aee" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/11/cs.SD_2023_08_11/" class="article-date">
  <time datetime="2023-08-11T15:00:00.000Z" itemprop="datePublished">2023-08-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/11/cs.SD_2023_08_11/">cs.SD - 2023-08-11</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Improving-Joint-Speech-Text-Representations-Without-Alignment"><a href="#Improving-Joint-Speech-Text-Representations-Without-Alignment" class="headerlink" title="Improving Joint Speech-Text Representations Without Alignment"></a>Improving Joint Speech-Text Representations Without Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06125">http://arxiv.org/abs/2308.06125</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cal Peyser, Zhong Meng, Ke Hu, Rohit Prabhavalkar, Andrew Rosenberg, Tara N. Sainath, Michael Picheny, Kyunghyun Cho</li>
<li>for: 这个论文旨在探讨文本描述生成中的字段表示空间，以及如何在这个空间中同时表示文本和语音。</li>
<li>methods: 这个论文使用了联合语音文本编码器，通过将语音和文本域合并到一起，以大参数模型的能力为基础。这些方法显示了搭配性，但需要特殊地处理语音和文本序列长度的差异。</li>
<li>results: 这个论文提供了证据表明联合语音文本编码器可以自然地实现多媒体表示的一致性，并且可以通过抛弃序列长度来避免对预测的影响。这种损失可以提高下游WR的性能，包括大参数单语言和多语言系统。<details>
<summary>Abstract</summary>
The last year has seen astonishing progress in text-prompted image generation premised on the idea of a cross-modal representation space in which the text and image domains are represented jointly. In ASR, this idea has found application as joint speech-text encoders that can scale to the capacities of very large parameter models by being trained on both unpaired speech and text. While these methods show promise, they have required special treatment of the sequence-length mismatch inherent in speech and text, either by up-sampling heuristics or an explicit alignment model. In this work, we offer evidence that joint speech-text encoders naturally achieve consistent representations across modalities by disregarding sequence length, and argue that consistency losses could forgive length differences and simply assume the best alignment. We show that such a loss improves downstream WER in both a large-parameter monolingual and multilingual system.
</details>
<details>
<summary>摘要</summary>
最近一年内，文本引导图像生成技术呈现出惊人的进步，基于跨Modal Representation空间的想法，在文本和图像领域 jointly 表示。在ASR中，这个想法得到应用，实现了合并语音和文本的encoder，可以通过训练大参数模型来扩大 capacities。虽然这些方法显示了承诺，但它们需要特殊地处理语音和文本的序列长度差异，可能通过上折表示或者显式对齐模型。在这项工作中，我们提供了证据，表明联合语音和文本encoder可以自然地实现多modalities的一致表示，并且提议使用一致损失来补偿序列长度差异。我们展示了这种损失可以提高下游WER，包括大参数训练的单语言和多语言系统。
</details></li>
</ul>
<hr>
<h2 id="Lip2Vec-Efficient-and-Robust-Visual-Speech-Recognition-via-Latent-to-Latent-Visual-to-Audio-Representation-Mapping"><a href="#Lip2Vec-Efficient-and-Robust-Visual-Speech-Recognition-via-Latent-to-Latent-Visual-to-Audio-Representation-Mapping" class="headerlink" title="Lip2Vec: Efficient and Robust Visual Speech Recognition via Latent-to-Latent Visual to Audio Representation Mapping"></a>Lip2Vec: Efficient and Robust Visual Speech Recognition via Latent-to-Latent Visual to Audio Representation Mapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06112">http://arxiv.org/abs/2308.06112</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yasser Abdelaziz Dahou Djilali, Sanath Narayan, Haithem Boussaid, Ebtessam Almazrouei, Merouane Debbah</li>
<li>for: 文章目的是提出一种简单的视觉语音识别（VSR）方法，以便在不需要大量标注数据的情况下进行训练和测试。</li>
<li>methods: 方法基于学习一个先验模型，将视觉语音编码器中的征表示映射到对应的音频对的征表示中，以实现有效的文本解码。</li>
<li>results: 对于LRS3数据集，提出的方法可以与完全监督学习方法相比，达到26个WRR（识别错误率）。与现有的SoTA方法不同，该方法在VoxCeleb测试集上保持了合理的性能。<details>
<summary>Abstract</summary>
Visual Speech Recognition (VSR) differs from the common perception tasks as it requires deeper reasoning over the video sequence, even by human experts. Despite the recent advances in VSR, current approaches rely on labeled data to fully train or finetune their models predicting the target speech. This hinders their ability to generalize well beyond the training set and leads to performance degeneration under out-of-distribution challenging scenarios. Unlike previous works that involve auxiliary losses or complex training procedures and architectures, we propose a simple approach, named Lip2Vec that is based on learning a prior model. Given a robust visual speech encoder, this network maps the encoded latent representations of the lip sequence to their corresponding latents from the audio pair, which are sufficiently invariant for effective text decoding. The generated audio representation is then decoded to text using an off-the-shelf Audio Speech Recognition (ASR) model. The proposed model compares favorably with fully-supervised learning methods on the LRS3 dataset achieving 26 WER. Unlike SoTA approaches, our model keeps a reasonable performance on the VoxCeleb test set. We believe that reprogramming the VSR as an ASR task narrows the performance gap between the two and paves the way for more flexible formulations of lip reading.
</details>
<details>
<summary>摘要</summary>
“视觉语音识别（VSR）与常见的识别任务不同，它需要对视频序列进行更深刻的推理，甚至到了人类专家的水平。Despite recent advances in VSR, current approaches still rely on labeled data to fully train or fine-tune their models to predict the target speech, which hinders their ability to generalize well beyond the training set and leads to performance degradation under out-of-distribution challenging scenarios. Unlike previous works that involve auxiliary losses or complex training procedures and architectures, we propose a simple approach, named Lip2Vec, which is based on learning a prior model. Given a robust visual speech encoder, this network maps the encoded latent representations of the lip sequence to their corresponding latents from the audio pair, which are sufficient for effective text decoding. The generated audio representation is then decoded to text using an off-the-shelf Audio Speech Recognition (ASR) model. The proposed model compares favorably with fully-supervised learning methods on the LRS3 dataset, achieving 26 WER. Unlike SoTA approaches, our model maintains a reasonable performance on the VoxCeleb test set. We believe that reprogramming the VSR as an ASR task narrows the performance gap between the two and paves the way for more flexible formulations of lip reading.”Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="An-Autoethnographic-Exploration-of-XAI-in-Algorithmic-Composition"><a href="#An-Autoethnographic-Exploration-of-XAI-in-Algorithmic-Composition" class="headerlink" title="An Autoethnographic Exploration of XAI in Algorithmic Composition"></a>An Autoethnographic Exploration of XAI in Algorithmic Composition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06089">http://arxiv.org/abs/2308.06089</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ashley Noel-Hirst, Nick Bryan-Kinns</li>
<li>for: 本研究旨在探讨如何使用可解释的人工智能（XAI）生成模型来帮助音乐创作。</li>
<li>methods: 本研究使用MeasureVAE生成模型，并对其中的可解释层次进行训练，以便在爱尔兰传统音乐上进行音乐创作。</li>
<li>results: 研究发现，在音乐创作过程中，探索性的音乐工作流程更加强调音乐训练集中的音乐特征，而不是生成模型本身的特征。这种方法还表明XAI模型可以被合理地应用于更复杂和多元的音乐创作工作流程中。<details>
<summary>Abstract</summary>
Machine Learning models are capable of generating complex music across a range of genres from folk to classical music. However, current generative music AI models are typically difficult to understand and control in meaningful ways. Whilst research has started to explore how explainable AI (XAI) generative models might be created for music, no generative XAI models have been studied in music making practice. This paper introduces an autoethnographic study of the use of the MeasureVAE generative music XAI model with interpretable latent dimensions trained on Irish folk music. Findings suggest that the exploratory nature of the music-making workflow foregrounds musical features of the training dataset rather than features of the generative model itself. The appropriation of an XAI model within an iterative workflow highlights the potential of XAI models to form part of a richer and more complex workflow than they were initially designed for.
</details>
<details>
<summary>摘要</summary>
文本翻译成简化中文：机器学习模型可以生成多种音乐类型，从民族音乐到古典音乐。然而，当前的生成音乐AI模型通常具有困难理解和控制的问题。研究已经开始探讨如何创建可解释的AI生成模型（XAI），但没有任何生成XAI模型在音乐创作实践中被研究。这篇论文介绍了一个自传式研究，使用了可解释的维度进行训练的MeasureVAE生成音乐XAI模型，并对 И尔兰民族音乐进行了应用。发现结果表明，音乐创作工作流程的探索性强调了训练数据集中的音乐特征，而不是生成模型自身的特征。在Iterative workflow中应用XAI模型，表明XAI模型可以成为更加复杂和多元的工作流程的一部分。
</details></li>
</ul>
<hr>
<h2 id="Audio-is-all-in-one-speech-driven-gesture-synthetics-using-WavLM-pre-trained-model"><a href="#Audio-is-all-in-one-speech-driven-gesture-synthetics-using-WavLM-pre-trained-model" class="headerlink" title="Audio is all in one: speech-driven gesture synthetics using WavLM pre-trained model"></a>Audio is all in one: speech-driven gesture synthetics using WavLM pre-trained model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05995">http://arxiv.org/abs/2308.05995</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fan Zhang, Naye Ji, Fuxing Gao, Siyuan Zhao, Zhaohan Wang, Shunman Li</li>
<li>for: 这 paper 的目的是为数字人类创造领域中的合作语言姿势生成。</li>
<li>methods: 这 paper 使用的方法是基于 transformer 架构的 speech-conditional 扩散模型，使用 WavLM 预训练模型提取低级和高级声音信息，并通过 adaptive layer norm 架构学习声音信息和合作姿势之间的关系。</li>
<li>results: 经过对 Trinity、ZEGGS 和 BEAT 数据集的评估，这 paper 的模型能够生成自然的合作姿势，并且可以控制姿势的风格和性格。<details>
<summary>Abstract</summary>
The generation of co-speech gestures for digital humans is an emerging area in the field of virtual human creation. Prior research has made progress by using acoustic and semantic information as input and adopting classify method to identify the person's ID and emotion for driving co-speech gesture generation. However, this endeavour still faces significant challenges. These challenges go beyond the intricate interplay between co-speech gestures, speech acoustic, and semantics; they also encompass the complexities associated with personality, emotion, and other obscure but important factors. This paper introduces "diffmotion-v2," a speech-conditional diffusion-based and non-autoregressive transformer-based generative model with WavLM pre-trained model. It can produce individual and stylized full-body co-speech gestures only using raw speech audio, eliminating the need for complex multimodal processing and manually annotated. Firstly, considering that speech audio not only contains acoustic and semantic features but also conveys personality traits, emotions, and more subtle information related to accompanying gestures, we pioneer the adaptation of WavLM, a large-scale pre-trained model, to extract low-level and high-level audio information. Secondly, we introduce an adaptive layer norm architecture in the transformer-based layer to learn the relationship between speech information and accompanying gestures. Extensive subjective evaluation experiments are conducted on the Trinity, ZEGGS, and BEAT datasets to confirm the WavLM and the model's ability to synthesize natural co-speech gestures with various styles.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>虚拟人类创造领域中的同声动作生成是一个emerging领域。先前的研究已经做出了进步，使用音声和语义信息作为输入，采用分类方法确定人的ID和情绪，以驱动同声动作生成。然而，这种努力仍然面临着重大挑战。这些挑战不仅包括同声动作、音声和语义之间的细微相互作用，还包括人格、情绪和其他重要而不可预测的因素。本文介绍“diffmotion-v2”模型，这是一种基于 transformer 架构的 speech-conditional 扩散型生成模型，使用 WavLM 预训练模型。它可以根据原始的 Raw Speech 音频生成个性化和风格化的全身同声动作，不需要详细的多媒体处理和手动标注。首先，我们认为音频不仅包含音声和语义信息，还包含人格特征和情绪信息，因此我们采用 WavLM 预训练模型来提取低级和高级音频信息。其次，我们引入 transformer 架构中的 adaptive 层 нор方法，以学习音频信息和同声动作之间的关系。我们在 Trinity、ZEGGS 和 BEAT  dataset 上进行了大量主观评估实验，以证明 WavLM 和模型的能力生成自然的同声动作。
</details></li>
</ul>
<hr>
<h2 id="Advancing-the-study-of-Large-Scale-Learning-in-Overlapped-Speech-Detection"><a href="#Advancing-the-study-of-Large-Scale-Learning-in-Overlapped-Speech-Detection" class="headerlink" title="Advancing the study of Large-Scale Learning in Overlapped Speech Detection"></a>Advancing the study of Large-Scale Learning in Overlapped Speech Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05987">http://arxiv.org/abs/2308.05987</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaohui Yin, Jingguang Tian, Xinhui Hu, Xinkang Xu</li>
<li>for: 多个party会话中的干扰语音检测（OSD）是speech应用中的一个重要部分，但大多数现有的OSD模型都是基于特定的数据集进行训练和评估，这限制了这些模型的应用场景。</li>
<li>methods: 本研究提出了大规模学习（LSL）在OSD中的应用，并设计了522小时不同语言和风格的标注音频作为大规模数据集。并通过对不同深度神经网络基于OSD模型的比较性试验来评估LSL在OSD任务中的效果和OSD模型的性能。</li>
<li>results: 研究结果表明，LSL可以显著提高OSD模型的性能和Robustness，并且CF-OSD模型基于LSL在16K单频OSD任务中取得了最佳性能，其F1分数为80.8%和52.0% separately在Alimeeting测试集和DIHARD II评估集上。<details>
<summary>Abstract</summary>
Overlapped Speech Detection (OSD) is an important part of speech applications involving analysis of multi-party conversations. However, Most of the existing OSD models are trained and evaluated on specific dataset, which limits the application scenarios of these models. In order to solve this problem, we conduct a study of large-scale learning (LSL) in OSD and propose a more general 16K single-channel OSD model. In our study, 522 hours of labeled audio in different languages and styles are collected and used as the large-scale dataset. Rigorous comparative experiments are designed and used to evaluate the effectiveness of LSL in OSD task and the performance of OSD models based on different deep neural networks. The results show that LSL can significantly improve the performance and robustness of OSD models, and the OSD model based on Conformer (CF-OSD) with LSL is currently the best 16K single-channel OSD model. Moreover, the CF-OSD with LSL establishes a state-of-the-art performance with a F1-score of 80.8% and 52.0% on the Alimeeting test set and DIHARD II evaluation set, respectively.
</details>
<details>
<summary>摘要</summary>
《 overlap speech detection (OSD) 是一种重要的语音应用程序中的分析多方会话的一部分。然而，现有的大多数 OSD 模型都是基于特定数据集进行训练和评估，这限制了这些模型的应用场景。为解决这个问题，我们进行了大规模学习 (LSL) 在 OSD 中的研究，并提出了一种更加通用的 16K 单通道 OSD 模型。在我们的研究中，我们收集了 522 小时不同语言和风格的标注音频，并使用这些大规模数据集进行训练和测试。我们设计了严格的比较实验，用于评估 LSL 在 OSD 任务中的效果和不同深度神经网络上的 OSD 模型性能。结果显示，LSL 可以明显提高 OSD 模型的性能和 Robustness，并且 CF-OSD  WITH LSL 目前是最佳的 16K 单通道 OSD 模型。此外，CF-OSD  WITH LSL 在 Alimeeting 测试集和 DIHARD II 评估集上的 F1 分数分别达到了 80.8% 和 52.0%。
</details></li>
</ul>
<hr>
<h2 id="AudioLDM-2-Learning-Holistic-Audio-Generation-with-Self-supervised-Pretraining"><a href="#AudioLDM-2-Learning-Holistic-Audio-Generation-with-Self-supervised-Pretraining" class="headerlink" title="AudioLDM 2: Learning Holistic Audio Generation with Self-supervised Pretraining"></a>AudioLDM 2: Learning Holistic Audio Generation with Self-supervised Pretraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05734">http://arxiv.org/abs/2308.05734</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/haoheliu/AudioLDM2">https://github.com/haoheliu/AudioLDM2</a></li>
<li>paper_authors: Haohe Liu, Qiao Tian, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Yuping Wang, Wenwu Wang, Yuxuan Wang, Mark D. Plumbley</li>
<li>for: 这个论文的目的是提出一个框架，用于同时生成不同类型的声音（如speech、音乐和声效），并且使用同一种学习方法来满足不同类型的目标和偏见。</li>
<li>methods: 该框架使用了一种通用的声音表示（LOA），将任何类型的声音都可以翻译为LOA，并使用一个GPT-2模型将不同类型的modalities翻译为LOA。在生成过程中，使用一个Latent Diffusion模型， conditioned on LOA，进行自我监督的声音生成学习。</li>
<li>results: 实验结果表明，该框架可以在主要的benchmark上达到新的州OF-THE-ART或与之前的方法竞争的性能。 Code和demo可以在<a target="_blank" rel="noopener" href="https://audioldm.github.io/audioldm2%E4%B8%AD%E8%8E%B7%E5%8F%96%E3%80%82">https://audioldm.github.io/audioldm2中获取。</a><details>
<summary>Abstract</summary>
Although audio generation shares commonalities across different types of audio, such as speech, music, and sound effects, designing models for each type requires careful consideration of specific objectives and biases that can significantly differ from those of other types. To bring us closer to a unified perspective of audio generation, this paper proposes a framework that utilizes the same learning method for speech, music, and sound effect generation. Our framework introduces a general representation of audio, called language of audio (LOA). Any audio can be translated into LOA based on AudioMAE, a self-supervised pre-trained representation learning model. In the generation process, we translate any modalities into LOA by using a GPT-2 model, and we perform self-supervised audio generation learning with a latent diffusion model conditioned on LOA. The proposed framework naturally brings advantages such as in-context learning abilities and reusable self-supervised pretrained AudioMAE and latent diffusion models. Experiments on the major benchmarks of text-to-audio, text-to-music, and text-to-speech demonstrate new state-of-the-art or competitive performance to previous approaches. Our demo and code are available at https://audioldm.github.io/audioldm2.
</details>
<details>
<summary>摘要</summary>
尽管听音生成存在不同类型的听音之间共同之处，例如语音、音乐和声音效果，但设计模型时需要仔细考虑每种类型的特定目标和偏见，这些偏见与其他类型的偏见可能有所不同。为了带我们更近到听音生成的统一视角，这篇论文提出了一个框架，该框架利用同一种学习方法来生成不同类型的听音。我们的框架引入了一个通用的听音表示（LOA），任何听音都可以根据AudioMAE自动学习的预训练表示学习模型翻译为LOA。在生成过程中，我们使用GPT-2模型将任何模式翻译为LOA，然后使用干扰扩散模型在LOA上进行自主学习。我们的提议框架自然带来了一些优点，例如在上下文学习能力和可重用的自动学习AudioMAE和干扰扩散模型。我们的实验在文本到听音、文本到音乐和文本到语音的主要benchmark上达到了新的州OF-THE-ART或与前一代方法竞争的性能。我们的demo和代码可以在https://audioldm.github.io/audioldm2中找到。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/11/cs.SD_2023_08_11/" data-id="clpxp044900xxfm888pt9d3fr" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_08_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/11/cs.CV_2023_08_11/" class="article-date">
  <time datetime="2023-08-11T13:00:00.000Z" itemprop="datePublished">2023-08-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/11/cs.CV_2023_08_11/">cs.CV - 2023-08-11</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="DIG-In-Evaluating-Disparities-in-Image-Generations-with-Indicators-for-Geographic-Diversity"><a href="#DIG-In-Evaluating-Disparities-in-Image-Generations-with-Indicators-for-Geographic-Diversity" class="headerlink" title="DIG In: Evaluating Disparities in Image Generations with Indicators for Geographic Diversity"></a>DIG In: Evaluating Disparities in Image Generations with Indicators for Geographic Diversity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06198">http://arxiv.org/abs/2308.06198</a></li>
<li>repo_url: None</li>
<li>paper_authors: Melissa Hall, Candace Ross, Adina Williams, Nicolas Carion, Michal Drozdzal, Adriana Romero Soriano</li>
<li>for: 这个研究旨在评估文本到图像生成系统中的可信度、多样性和提示一致性，以及这些系统在不同地区的使用情况。</li>
<li>methods: 研究人员提出了三个指标来评估文本到图像生成系统的可信度、多样性和提示一致性，包括对世界各地的图像生成、地域差异和提示信息的影响。</li>
<li>results: 研究结果显示，当提示为非洲和西亚地区时，模型的生成图像的可信度和多样性较低，而提示包含地理信息会导致生成图像的一致性和多样性减退。此外，研究还发现了一些物体的地域差异较大。这些结果表明，随着图像生成质量的提高，图像的真实 Representation 在世界各地受到了影响。<details>
<summary>Abstract</summary>
The unprecedented photorealistic results achieved by recent text-to-image generative systems and their increasing use as plug-and-play content creation solutions make it crucial to understand their potential biases. In this work, we introduce three indicators to evaluate the realism, diversity and prompt-generation consistency of text-to-image generative systems when prompted to generate objects from across the world. Our indicators complement qualitative analysis of the broader impact of such systems by enabling automatic and efficient benchmarking of geographic disparities, an important step towards building responsible visual content creation systems. We use our proposed indicators to analyze potential geographic biases in state-of-the-art visual content creation systems and find that: (1) models have less realism and diversity of generations when prompting for Africa and West Asia than Europe, (2) prompting with geographic information comes at a cost to prompt-consistency and diversity of generated images, and (3) models exhibit more region-level disparities for some objects than others. Perhaps most interestingly, our indicators suggest that progress in image generation quality has come at the cost of real-world geographic representation. Our comprehensive evaluation constitutes a crucial step towards ensuring a positive experience of visual content creation for everyone.
</details>
<details>
<summary>摘要</summary>
“Recent text-to-image生成系统实现了无 precedent 的实景实现，并在实际应用中普遍用作内置的内容创建解决方案。因此，理解这些系统的可能偏见成为核心。在这个工作中，我们提出三个指标来评估文本生成系统当前对世界各地的物品生成的实现度、多样性和描述稳定性。我们的指标可以辅助自动和高效地评估地域差异，从而构建责任感的视觉内容创建系统。我们使用我们提出的指标进行分析，发现：（1）当提交 Africah 和 West Asia 时，模型的实现度和多样性较低；（2）对地理信息进行提交会导致实现稳定性和多样性的损失；（3）模型对一些物品的地域差异较大。最有趣的是，我们的指标显示，实际图像质量的进步对于地域呈现来说是否增。我们的全面评估是建立责任感的视觉内容创建系统的重要一步。”
</details></li>
</ul>
<hr>
<h2 id="Towards-Packaging-Unit-Detection-for-Automated-Palletizing-Tasks"><a href="#Towards-Packaging-Unit-Detection-for-Automated-Palletizing-Tasks" class="headerlink" title="Towards Packaging Unit Detection for Automated Palletizing Tasks"></a>Towards Packaging Unit Detection for Automated Palletizing Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06306">http://arxiv.org/abs/2308.06306</a></li>
<li>repo_url: None</li>
<li>paper_authors: Markus Völk, Kilian Kleeberger, Werner Kraus, Richard Bormann</li>
<li>for: 这篇论文是用于解决各种自动化装卸任务中的包装单位检测问题。</li>
<li>methods: 本论文提出了一种基于人工生成数据的方法，可以对真实世界的包装单位进行检测，并且不需要进一步训练或设置。这种方法可以处理稀疏且低质量的感应数据，可以利用专家知识，并且具有广泛的产品和应用场景的应用能力。</li>
<li>results: 本论文的实验结果显示，提出的方法能够实现高度的准确性和稳定性，并且在实际应用中具有很好的表现。此外，论文还详细介绍了一个实验室示范器和一个商业解决方案，它们都是根据本论文的方法进行开发的。<details>
<summary>Abstract</summary>
For various automated palletizing tasks, the detection of packaging units is a crucial step preceding the actual handling of the packaging units by an industrial robot. We propose an approach to this challenging problem that is fully trained on synthetically generated data and can be robustly applied to arbitrary real world packaging units without further training or setup effort. The proposed approach is able to handle sparse and low quality sensor data, can exploit prior knowledge if available and generalizes well to a wide range of products and application scenarios. To demonstrate the practical use of our approach, we conduct an extensive evaluation on real-world data with a wide range of different retail products. Further, we integrated our approach in a lab demonstrator and a commercial solution will be marketed through an industrial partner.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Discovering-Local-Binary-Pattern-Equation-for-Foreground-Object-Removal-in-Videos"><a href="#Discovering-Local-Binary-Pattern-Equation-for-Foreground-Object-Removal-in-Videos" class="headerlink" title="Discovering Local Binary Pattern Equation for Foreground Object Removal in Videos"></a>Discovering Local Binary Pattern Equation for Foreground Object Removal in Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06305">http://arxiv.org/abs/2308.06305</a></li>
<li>repo_url: None</li>
<li>paper_authors: Caroline Pacheco do Espirito Silva, Andrews Cordolino Sobral, Antoine Vacavant, Thierry Bouwmans, Felippe De Souza</li>
<li>for: 自动发现具有背景和前景的场景中移动部分的Local Binary Pattern（LBP）处理方法</li>
<li>methods: 使用 симвоlic regression 自动发现 LBP 公式</li>
<li>results: 实验结果显示，提案的方法可以对实际的城市景象进行高品质的分类，并与先前的州前测试数据表现出色。<details>
<summary>Abstract</summary>
Designing a novel Local Binary Pattern (LBP) process usually relies heavily on human experts' knowledge and experience in the area. Even experts are often left with tedious episodes of trial and error until they identify an optimal LBP for a particular dataset. To address this problem, we present a novel symbolic regression able to automatically discover LBP formulas to remove the moving parts of a scene by segmenting it into a background and a foreground. Experimental results conducted on real videos of outdoor urban scenes under various conditions show that the LBPs discovered by the proposed approach significantly outperform the previous state-of-the-art LBP descriptors both qualitatively and quantitatively. Our source code and data will be available online.
</details>
<details>
<summary>摘要</summary>
通常情况下，设计本地二进制 patrern（LBP）处理程序需要几乎依赖人类专家的知识和经验。甚至专家们也经常会遭遇辗转的尝试和错误，直到他们找到一个特定数据集的优化LBP。为解决这个问题，我们提出了一种新的符号回归算法，能够自动找到去掉场景中移动部分的LBP方程。我们的实验结果表明，对于实际的户外都市场景，我们的方法可以明显超越前一个状态的艺术LBP描述符， both qualitatively and quantitatively。我们的源代码和数据将在线上公开。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-the-Localization-in-Weakly-Supervised-Object-Localization"><a href="#Rethinking-the-Localization-in-Weakly-Supervised-Object-Localization" class="headerlink" title="Rethinking the Localization in Weakly Supervised Object Localization"></a>Rethinking the Localization in Weakly Supervised Object Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06161">http://arxiv.org/abs/2308.06161</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tzzcl/PSOL">https://github.com/tzzcl/PSOL</a></li>
<li>paper_authors: Rui Xu, Yong Luo, Han Hu, Bo Du, Jialie Shen, Yonggang Wen</li>
<li>for: 本文针对强度指导物体Localization（WSOL）进行研究，WSOL是计算机视觉中最受欢迎且最具挑战性的任务之一，这任务的目的是在仅有图像水平指导下，将图像中的物体Localization。</li>
<li>methods: 本文提出了两个方法来解决WSOL中的问题，第一个方法是替换单一类别检测（SCR）为多类别检测（BCD），以便在图像中多次检测多个物体。第二个方法是设计一个权重Entropy（WE）损失函数，用于降低随机矩形的负面影响。</li>
<li>results: 实验结果显示，我们的方法可以对Popular CUB-200-2011和ImageNet-1K datasets进行广泛的测试，并获得了效果的结果。<details>
<summary>Abstract</summary>
Weakly supervised object localization (WSOL) is one of the most popular and challenging tasks in computer vision. This task is to localize the objects in the images given only the image-level supervision. Recently, dividing WSOL into two parts (class-agnostic object localization and object classification) has become the state-of-the-art pipeline for this task. However, existing solutions under this pipeline usually suffer from the following drawbacks: 1) they are not flexible since they can only localize one object for each image due to the adopted single-class regression (SCR) for localization; 2) the generated pseudo bounding boxes may be noisy, but the negative impact of such noise is not well addressed. To remedy these drawbacks, we first propose to replace SCR with a binary-class detector (BCD) for localizing multiple objects, where the detector is trained by discriminating the foreground and background. Then we design a weighted entropy (WE) loss using the unlabeled data to reduce the negative impact of noisy bounding boxes. Extensive experiments on the popular CUB-200-2011 and ImageNet-1K datasets demonstrate the effectiveness of our method.
</details>
<details>
<summary>摘要</summary>
弱级指导对象定位（WSOL）是计算机视觉中最受欢迎且最有挑战的任务之一。这个任务的目标是根据图像水平的监督来地将对象在图像中Localize。最新的解决方案通常受到以下几个缺点的影响：1）它们不够灵活，因为它们只能在每个图像中Localize一个对象，因为采用的是单个类 regression（SCR）来实现Localization；2）生成的假 bounding box 可能具有噪音，但这种噪音的负面影响并未得到足够的解决。为了缓解这些缺点，我们首先提议将 SCR 替换为二分类探测器（BCD），以在多个对象中Localize。然后，我们设计了一种权重Entropy（WE）损失函数，使用无标注数据来减少噪音 bounding box 的负面影响。我们在popular CUB-200-2011 和 ImageNet-1K 数据集上进行了广泛的实验，并证明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="DatasetDM-Synthesizing-Data-with-Perception-Annotations-Using-Diffusion-Models"><a href="#DatasetDM-Synthesizing-Data-with-Perception-Annotations-Using-Diffusion-Models" class="headerlink" title="DatasetDM: Synthesizing Data with Perception Annotations Using Diffusion Models"></a>DatasetDM: Synthesizing Data with Perception Annotations Using Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06160">http://arxiv.org/abs/2308.06160</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/showlab/datasetdm">https://github.com/showlab/datasetdm</a></li>
<li>paper_authors: Weijia Wu, Yuzhong Zhao, Hao Chen, Yuchao Gu, Rui Zhao, Yefei He, Hong Zhou, Mike Zheng Shou, Chunhua Shen</li>
<li>for: 本文提出了一种生成各种感知数据的数据生成模型，以便用于训练多种下游任务的感知模型。</li>
<li>methods: 该模型基于预训练的扩散模型，并通过文本引导图像生成来生成各种感知数据，包括分割mask和深度。</li>
<li>results: 该方法可以生成具有高质量感知标注的无限多个synthetic图像，并且可以在不同的领域进行鲁棒的适应和零shot掌握。此外，该方法还具有高度的可重复性和可扩展性，可以有效地应用于多种任务和新的任务组合。<details>
<summary>Abstract</summary>
Current deep networks are very data-hungry and benefit from training on largescale datasets, which are often time-consuming to collect and annotate. By contrast, synthetic data can be generated infinitely using generative models such as DALL-E and diffusion models, with minimal effort and cost. In this paper, we present DatasetDM, a generic dataset generation model that can produce diverse synthetic images and the corresponding high-quality perception annotations (e.g., segmentation masks, and depth). Our method builds upon the pre-trained diffusion model and extends text-guided image synthesis to perception data generation. We show that the rich latent code of the diffusion model can be effectively decoded as accurate perception annotations using a decoder module. Training the decoder only needs less than 1% (around 100 images) manually labeled images, enabling the generation of an infinitely large annotated dataset. Then these synthetic data can be used for training various perception models for downstream tasks. To showcase the power of the proposed approach, we generate datasets with rich dense pixel-wise labels for a wide range of downstream tasks, including semantic segmentation, instance segmentation, and depth estimation. Notably, it achieves 1) state-of-the-art results on semantic segmentation and instance segmentation; 2) significantly more robust on domain generalization than using the real data alone; and state-of-the-art results in zero-shot segmentation setting; and 3) flexibility for efficient application and novel task composition (e.g., image editing). The project website and code can be found at https://weijiawu.github.io/DatasetDM_page/ and https://github.com/showlab/DatasetDM, respectively
</details>
<details>
<summary>摘要</summary>
现代深度网络很需要大量数据进行训练，而收集和标注大量数据可能是时间consuming和成本高的。然而，使用生成模型如DALL-E和扩散模型可以生成无穷量的 sintetic 数据，只需要微不足百的人工标注图像。在这篇论文中，我们提出了 DatasetDM，一种通用的数据生成模型，可以生成多样化的 sintetic 图像和相应的高质量感知注释（例如分割mask和深度）。我们的方法基于预训练的扩散模型，并扩展了文本引导的图像生成到感知数据生成。我们发现，扩散模型的含义密集代码可以高效地解码为准确的感知注释，只需要训练decoder模块 fewer than 1% (around 100 images) 的人工标注图像。这些生成的synthetic数据可以用于训练多种感知模型，以满足下游任务。我们在各种下游任务中实现了1) 状态的最佳结果，包括semantic segmentation和instance segmentation; 2) 在预测领域内显著更加稳定，比使用真实数据alone; 和3) 可以有效地应用和创新任务（例如图像编辑）。相关项目网站和代码可以在https://weijiawu.github.io/DatasetDM_page/ 和https://github.com/showlab/DatasetDM 中找到。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Large-scale-AUV-based-Visual-Seafloor-Mapping"><a href="#Efficient-Large-scale-AUV-based-Visual-Seafloor-Mapping" class="headerlink" title="Efficient Large-scale AUV-based Visual Seafloor Mapping"></a>Efficient Large-scale AUV-based Visual Seafloor Mapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06147">http://arxiv.org/abs/2308.06147</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengkun She, Yifan Song, David Nakath, Kevin Köser</li>
<li>for: 这项研究旨在提高自动化潜水车（AUV）在深海中进行三维重建的精度和效率。</li>
<li>methods: 该系统利用最新的深海拍摄技术和视觉地图生成技术，以帮助自动化潜水车在深海中重建海底三维模型。</li>
<li>results: 该系统在多次实验船次中展示了其可靠性和实用性，并且比增量重建更快，而且性能与增量重建相当。<details>
<summary>Abstract</summary>
Driven by the increasing number of marine data science applications, there is a growing interest in surveying and exploring the vast, uncharted terrain of the deep sea with robotic platforms. Despite impressive results achieved by many on-land visual mapping algorithms in the past decades, transferring these methods from land to the deep sea remains a challenge due to harsh environmental conditions. Typically, deep-sea exploration involves the use of autonomous underwater vehicles (AUVs) equipped with high-resolution cameras and artificial illumination systems. However, images obtained in this manner often suffer from heterogeneous illumination and quality degradation due to attenuation and scattering, on top of refraction of light rays. All of this together often lets on-land SLAM approaches fail underwater or makes Structure-from-Motion approaches drift or omit difficult images, resulting in gaps, jumps or weakly registered areas. In this work, we present a system that incorporates recent developments in underwater imaging and visual mapping to facilitate automated robotic 3D reconstruction of hectares of seafloor. Our approach is efficient in that it detects and reconsiders difficult, weakly registered areas, to avoid omitting images and to make better use of limited dive time; on the other hand it is computationally efficient; leveraging a hybrid approach combining benefits from SLAM and Structure-from-Motion that runs much faster than incremental reconstructions while achieving at least on-par performance. The proposed system has been extensively tested and evaluated during several research cruises, demonstrating its robustness and practicality in real-world conditions.
</details>
<details>
<summary>摘要</summary>
随着 marine data science 应用的增加，有越来越多的人对深海的未探索领域进行探索和调查，使用 робоット平台。虽然过去数十年来的陆地视图算法已经取得了很好的成果，但将这些方法从陆地传播到深海仍然是一个挑战，因为深海环境条件非常恶劣。通常，深海探索使用自动化潜水车（AUV）配备高分辨率摄像头和人工照明系统。但是，在这种情况下获得的图像经常受到不均匀照明和质量强化的影响，加之光线干扰和折射，导致在陆地SLAM方法下不能在水下工作或者Structure-from-Motion方法会在水下偏移或者忽略Difficult Images，导致图像中的缺失、跳跃和弱联系区域。在这项工作中，我们提出了一种系统，利用最新的深海摄像头和视图地图技术来自动化机器人3D重建深海底的百 hectare。我们的方法能够检测和重新考虑Difficult，弱联系区域，以避免忽略图像和更好地利用有限的潜水时间；同时，它具有计算效率，利用一种hybrid方法，结合SLAM和Structure-from-Motion两种方法，运行得更快，而且性能至少与逐步重建相当。我们的系统在多次研究考察中得到了广泛的测试和评估，并在实际条件下显示了其可靠性和实用性。
</details></li>
</ul>
<hr>
<h2 id="CompTLL-UNet-Compressed-Domain-Text-Line-Localization-in-Challenging-Handwritten-Documents-using-Deep-Feature-Learning-from-JPEG-Coefficients"><a href="#CompTLL-UNet-Compressed-Domain-Text-Line-Localization-in-Challenging-Handwritten-Documents-using-Deep-Feature-Learning-from-JPEG-Coefficients" class="headerlink" title="CompTLL-UNet: Compressed Domain Text-Line Localization in Challenging Handwritten Documents using Deep Feature Learning from JPEG Coefficients"></a>CompTLL-UNet: Compressed Domain Text-Line Localization in Challenging Handwritten Documents using Deep Feature Learning from JPEG Coefficients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06142">http://arxiv.org/abs/2308.06142</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bulla Rajesh, Sk Mahafuz Zaman, Mohammed Javed, P. Nagabhushan</li>
<li>for: 本研究旨在提出一种直接从JPEG压缩系数中进行文本线程地理Localization的方法，以解决手写文本图像中文本线程的自动地理化问题。</li>
<li>methods: 本研究使用了一种Modified U-Net architecture，称为Compressed Text-Line Localization Network (CompTLL-UNet)，以直接从JPEG压缩系数中学习深度特征，并将其应用于文本线程地理Localization问题。</li>
<li>results: 研究人员通过对ICDAR2017（cBAD）和ICDAR2019（cBAD）标准测试集进行训练和测试，并得到了在JPEG压缩domain中的state-of-the-art表现，同时减少了存储和计算成本。<details>
<summary>Abstract</summary>
Automatic localization of text-lines in handwritten documents is still an open and challenging research problem. Various writing issues such as uneven spacing between the lines, oscillating and touching text, and the presence of skew become much more challenging when the case of complex handwritten document images are considered for segmentation directly in their respective compressed representation. This is because, the conventional way of processing compressed documents is through decompression, but here in this paper, we propose an idea that employs deep feature learning directly from the JPEG compressed coefficients without full decompression to accomplish text-line localization in the JPEG compressed domain. A modified U-Net architecture known as Compressed Text-Line Localization Network (CompTLL-UNet) is designed to accomplish it. The model is trained and tested with JPEG compressed version of benchmark datasets including ICDAR2017 (cBAD) and ICDAR2019 (cBAD), reporting the state-of-the-art performance with reduced storage and computational costs in the JPEG compressed domain.
</details>
<details>
<summary>摘要</summary>
自动化手写文档中文行的本地化仍然是一个开放的研究问题。不同的手写问题，如行间距不均匀、文本振荡和触擦，以及扭曲的存在，在考虑复杂手写文档图像时变得非常困难。这是因为，传统的处理压缩文档的方法是通过解压缩，但在这篇论文中，我们提出了一个想法，即直接从JPEG压缩系数中使用深度特征学习来完成文行本地化。我们设计了一种修改后的U-Net架构，称为Compressed Text-Line Localization Network (CompTLL-UNet)，以实现其目的。我们在JPEG压缩版本的标准评价数据集ICDAR2017（cBAD）和ICDAR2019（cBAD）进行训练和测试，并实现了当今最佳性能，同时减少存储和计算成本在JPEG压缩Domain中。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-Quantification-for-Image-based-Traffic-Prediction-across-Cities"><a href="#Uncertainty-Quantification-for-Image-based-Traffic-Prediction-across-Cities" class="headerlink" title="Uncertainty Quantification for Image-based Traffic Prediction across Cities"></a>Uncertainty Quantification for Image-based Traffic Prediction across Cities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06129">http://arxiv.org/abs/2308.06129</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alextimans/traffic4cast-uncertainty">https://github.com/alextimans/traffic4cast-uncertainty</a></li>
<li>paper_authors: Alexander Timans, Nina Wiedemann, Nishant Kumar, Ye Hong, Martin Raubal<br>for:  This paper aims to investigate the application of uncertainty quantification (UQ) methods for traffic prediction, and to evaluate the effectiveness of UQ methods in providing meaningful uncertainty estimates for city-wide traffic dynamics.methods:  The paper compares two epistemic and two aleatoric UQ methods on both temporal and spatio-temporal transfer tasks, and demonstrates how uncertainty estimates can be employed for unsupervised outlier detection on changes in city traffic dynamics.results:  The paper finds that meaningful uncertainty estimates can be recovered using UQ methods, and that these estimates can be used to capture both temporal and spatial effects on traffic behavior in a representative case study for the city of Moscow.<details>
<summary>Abstract</summary>
Despite the strong predictive performance of deep learning models for traffic prediction, their widespread deployment in real-world intelligent transportation systems has been restrained by a lack of interpretability. Uncertainty quantification (UQ) methods provide an approach to induce probabilistic reasoning, improve decision-making and enhance model deployment potential. To gain a comprehensive picture of the usefulness of existing UQ methods for traffic prediction and the relation between obtained uncertainties and city-wide traffic dynamics, we investigate their application to a large-scale image-based traffic dataset spanning multiple cities and time periods. We compare two epistemic and two aleatoric UQ methods on both temporal and spatio-temporal transfer tasks, and find that meaningful uncertainty estimates can be recovered. We further demonstrate how uncertainty estimates can be employed for unsupervised outlier detection on changes in city traffic dynamics. We find that our approach can capture both temporal and spatial effects on traffic behaviour in a representative case study for the city of Moscow. Our work presents a further step towards boosting uncertainty awareness in traffic prediction tasks, and aims to highlight the value contribution of UQ methods to a better understanding of city traffic dynamics.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Taming-the-Power-of-Diffusion-Models-for-High-Quality-Virtual-Try-On-with-Appearance-Flow"><a href="#Taming-the-Power-of-Diffusion-Models-for-High-Quality-Virtual-Try-On-with-Appearance-Flow" class="headerlink" title="Taming the Power of Diffusion Models for High-Quality Virtual Try-On with Appearance Flow"></a>Taming the Power of Diffusion Models for High-Quality Virtual Try-On with Appearance Flow</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06101">http://arxiv.org/abs/2308.06101</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bcmi/DCI-VTON-Virtual-Try-On">https://github.com/bcmi/DCI-VTON-Virtual-Try-On</a></li>
<li>paper_authors: Junhong Gou, Siyu Sun, Jianfu Zhang, Jianlou Si, Chen Qian, Liqing Zhang</li>
<li>for: 用于实现虚拟试穿，即将衣服从一个图像传输到另一个图像中，保持衣服和人体细节的图像合成任务。</li>
<li>methods: 使用噪声扩散模型（Diffusion Model）来生成高质量图像，并使用准备模块（Warping Module）来引导噪声扩散模型的生成。</li>
<li>results: 通过将衣服经过准备模块进行初始处理，以保持衣服的本地细节，然后将渗透模型的输出与人体图像相加，实现高质量和真实的虚拟试穿结果。<details>
<summary>Abstract</summary>
Virtual try-on is a critical image synthesis task that aims to transfer clothes from one image to another while preserving the details of both humans and clothes. While many existing methods rely on Generative Adversarial Networks (GANs) to achieve this, flaws can still occur, particularly at high resolutions. Recently, the diffusion model has emerged as a promising alternative for generating high-quality images in various applications. However, simply using clothes as a condition for guiding the diffusion model to inpaint is insufficient to maintain the details of the clothes. To overcome this challenge, we propose an exemplar-based inpainting approach that leverages a warping module to guide the diffusion model's generation effectively. The warping module performs initial processing on the clothes, which helps to preserve the local details of the clothes. We then combine the warped clothes with clothes-agnostic person image and add noise as the input of diffusion model. Additionally, the warped clothes is used as local conditions for each denoising process to ensure that the resulting output retains as much detail as possible. Our approach, namely Diffusion-based Conditional Inpainting for Virtual Try-ON (DCI-VTON), effectively utilizes the power of the diffusion model, and the incorporation of the warping module helps to produce high-quality and realistic virtual try-on results. Experimental results on VITON-HD demonstrate the effectiveness and superiority of our method.
</details>
<details>
<summary>摘要</summary>
“虚拟试穿”是一个重要的图像合成任务，目的是将衣服从一个图像转移到另一个，同时保留人体和衣服的细节。许多现有方法利用生成对抗网络（GANs）实现此目的，但缺陷仍然存在，特别是在高分辨率下。随后，扩散模型在许多应用中作为生成高品质图像的可能性而崛起。然而，仅将衣服作为导向扩散模型填充的条件，无法维持衣服的细节。为解决这个挑战，我们提出了一个示范基本填充方法，利用扭曲模组将衣服进行初始处理，帮助保留衣服的地方细节。我们然后结合扭曲衣服和不同于衣服的人像图混合，加入杂音作为扩散模型的输入。此外，扭曲衣服也用作每个降噪过程的地方条件，以确保结果保留最多细节。我们统称这种方法为扩散基于对应填充（DCI-VTON）。我们的方法充分利用扩散模型的力量，并通过扭曲模组的帮助，实现高品质和实际的虚拟试穿结果。实验结果显示，我们的方法在VITON-HD上得到了优异的效果。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-based-Visual-Counterfactual-Explanations-–-Towards-Systematic-Quantitative-Evaluation"><a href="#Diffusion-based-Visual-Counterfactual-Explanations-–-Towards-Systematic-Quantitative-Evaluation" class="headerlink" title="Diffusion-based Visual Counterfactual Explanations – Towards Systematic Quantitative Evaluation"></a>Diffusion-based Visual Counterfactual Explanations – Towards Systematic Quantitative Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06100">http://arxiv.org/abs/2308.06100</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cairo-thws/dbvce_eval">https://github.com/cairo-thws/dbvce_eval</a></li>
<li>paper_authors: Philipp Vaeth, Alexander M. Fruehwald, Benjamin Paassen, Magda Gregorova</li>
<li>for: 本研究旨在系统地评估最新的视觉对照解释（VCE）方法，并提出一组最小的度量来评估这些方法。</li>
<li>methods: 本研究使用了涉及潜在的设计选择的扩散基于生成模型来生成高维像素图像，并对这些方法进行系统评估。</li>
<li>results: 研究发现了多种方向，以便未来驱动VCE方法的进步和改进。此外，研究还提供了一个有价值的指南，以便其他研究人员在评估对照解释时保持一致和透明度。<details>
<summary>Abstract</summary>
Latest methods for visual counterfactual explanations (VCE) harness the power of deep generative models to synthesize new examples of high-dimensional images of impressive quality. However, it is currently difficult to compare the performance of these VCE methods as the evaluation procedures largely vary and often boil down to visual inspection of individual examples and small scale user studies. In this work, we propose a framework for systematic, quantitative evaluation of the VCE methods and a minimal set of metrics to be used. We use this framework to explore the effects of certain crucial design choices in the latest diffusion-based generative models for VCEs of natural image classification (ImageNet). We conduct a battery of ablation-like experiments, generating thousands of VCEs for a suite of classifiers of various complexity, accuracy and robustness. Our findings suggest multiple directions for future advancements and improvements of VCE methods. By sharing our methodology and our approach to tackle the computational challenges of such a study on a limited hardware setup (including the complete code base), we offer a valuable guidance for researchers in the field fostering consistency and transparency in the assessment of counterfactual explanations.
</details>
<details>
<summary>摘要</summary>
最新的视觉对照解释（VCE）方法充分利用深度生成模型来生成高维度图像，但目前难以比较这些VCE方法的表现，因为评估方法多样化并经常降至视觉检查个例和小规模用户研究。在这项工作中，我们提出了一个系统化、量化评估VCE方法的框架，以及一组最小化的度量。我们使用这个框架来探索latest扩散基本生成模型中的某些关键设计选择对自然图像分类（ImageNet）VCE的影响。我们进行了一系列减少类似实验，生成了数以千计的VCE，对一组不同的分类器进行评估。我们的发现表明了未来VCE方法的多个方向的进攻，并提供了一个有价值的指南，以帮助研究人员在评估对照解释时保持一致和透明度。
</details></li>
</ul>
<hr>
<h2 id="Automated-Construction-of-Time-Space-Diagrams-for-Traffic-Analysis-Using-Street-View-Video-Sequence"><a href="#Automated-Construction-of-Time-Space-Diagrams-for-Traffic-Analysis-Using-Street-View-Video-Sequence" class="headerlink" title="Automated Construction of Time-Space Diagrams for Traffic Analysis Using Street-View Video Sequence"></a>Automated Construction of Time-Space Diagrams for Traffic Analysis Using Street-View Video Sequence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06098">http://arxiv.org/abs/2308.06098</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tanay Rastogi, Mårten Björkman</li>
<li>for: 这个论文旨在利用在运动车辆上安装的摄像头捕捉的街景视频序列来构建时空图表，以分析交通流量和优化交通基础设施和交通管理策略。</li>
<li>methods: 该研究使用了最新的YOLOv5、StrongSORT和光学测距技术来从视频数据中推断车辆轨迹，并生成时空图表。</li>
<li>results: 评估结果表明，该方法可以从视频数据中提取车辆轨迹，但是存在一些误差，可以通过提高检测器、跟踪器和距离计算组件的性能来缓解这些误差。<details>
<summary>Abstract</summary>
Time-space diagrams are essential tools for analyzing traffic patterns and optimizing transportation infrastructure and traffic management strategies. Traditional data collection methods for these diagrams have limitations in terms of temporal and spatial coverage. Recent advancements in camera technology have overcome these limitations and provided extensive urban data. In this study, we propose an innovative approach to constructing time-space diagrams by utilizing street-view video sequences captured by cameras mounted on moving vehicles. Using the state-of-the-art YOLOv5, StrongSORT, and photogrammetry techniques for distance calculation, we can infer vehicle trajectories from the video data and generate time-space diagrams. To evaluate the effectiveness of our proposed method, we utilized datasets from the KITTI computer vision benchmark suite. The evaluation results demonstrate that our approach can generate trajectories from video data, although there are some errors that can be mitigated by improving the performance of the detector, tracker, and distance calculation components. In conclusion, the utilization of street-view video sequences captured by cameras mounted on moving vehicles, combined with state-of-the-art computer vision techniques, has immense potential for constructing comprehensive time-space diagrams. These diagrams offer valuable insights into traffic patterns and contribute to the design of transportation infrastructure and traffic management strategies.
</details>
<details>
<summary>摘要</summary>
时空图是交通Pattern分析和交通基础设施和交通管理策略优化的重要工具。传统数据采集方法有限制性，尤其是在时间和空间方面。现代摄像头技术的发展已经突破了这些限制，提供了广泛的都市数据。本研究提出了一种创新的时空图构建方法，利用行车中的街景视频序列，通过安装在移动 Vehicle 上的摄像头捕捉的视频数据来进行推断。使用最新的 YOLOv5、StrongSORT 和光学计算技术，我们可以从视频数据中推断车辆轨迹，并生成时空图。为了评估我们的提议方法的效果，我们利用了 KITTI 计算机视觉标准库中的数据集。评估结果表明，我们的方法可以从视频数据中提取车辆轨迹，但是存在一些错误，这些错误可以通过提高探测器、跟踪器和距离计算组件的性能来减少。因此，通过利用行车中的街景视频序列，并利用现代计算机视觉技术，我们的方法具有广泛的应用前景，可以为交通基础设施和交通管理策略的设计提供有价值的信息。
</details></li>
</ul>
<hr>
<h2 id="RIGID-Recurrent-GAN-Inversion-and-Editing-of-Real-Face-Videos"><a href="#RIGID-Recurrent-GAN-Inversion-and-Editing-of-Real-Face-Videos" class="headerlink" title="RIGID: Recurrent GAN Inversion and Editing of Real Face Videos"></a>RIGID: Recurrent GAN Inversion and Editing of Real Face Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06097">http://arxiv.org/abs/2308.06097</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yangyang Xu, Shengfeng He, Kwan-Yee K. Wong, Ping Luo</li>
<li>for: 本研究旨在应用GAN的强大可编辑性到实际图像上，但现有方法通常是对视频帧进行分解式逆合，导致时间上的不一致问题。</li>
<li>methods: 我们提出了一种统一的循环框架，名为RIGID，以同时强制满足时间相关的GAN逆合和人脸编辑。我们的方法从三个方面模型了时间关系。</li>
<li>results: 我们的方法可以Qualitatively and quantitatively超过现有方法，并且可以应用于不同的编辑任务。<details>
<summary>Abstract</summary>
GAN inversion is indispensable for applying the powerful editability of GAN to real images. However, existing methods invert video frames individually often leading to undesired inconsistent results over time. In this paper, we propose a unified recurrent framework, named \textbf{R}ecurrent v\textbf{I}deo \textbf{G}AN \textbf{I}nversion and e\textbf{D}iting (RIGID), to explicitly and simultaneously enforce temporally coherent GAN inversion and facial editing of real videos. Our approach models the temporal relations between current and previous frames from three aspects. To enable a faithful real video reconstruction, we first maximize the inversion fidelity and consistency by learning a temporal compensated latent code. Second, we observe incoherent noises lie in the high-frequency domain that can be disentangled from the latent space. Third, to remove the inconsistency after attribute manipulation, we propose an \textit{in-between frame composition constraint} such that the arbitrary frame must be a direct composite of its neighboring frames. Our unified framework learns the inherent coherence between input frames in an end-to-end manner, and therefore it is agnostic to a specific attribute and can be applied to arbitrary editing of the same video without re-training. Extensive experiments demonstrate that RIGID outperforms state-of-the-art methods qualitatively and quantitatively in both inversion and editing tasks. The deliverables can be found in \url{https://cnnlstm.github.io/RIGID}
</details>
<details>
<summary>摘要</summary>
GAN逆转是应用GAN的强大编辑能力的必要条件。然而，现有方法通常对视频帧进行逆转，经常导致时间上的不一致结果。在这篇论文中，我们提出了一个统一的回归框架，名为Recurrent Video GAN Inversion and Editing（RIGID），以确立和同时执行时间相关的GAN逆转和人脸编辑。我们的方法从三个方面模型视频帧之间的时间关系：1. 通过学习时间补偿的秘密码来最大化逆转准确率和一致性。2. 发现高频域中的不一致噪声可以从秘密码空间分离。3. 在执行特性修改后，我们提出了一个“间隔帧组合约束”，使得任意帧必须是其邻近帧的直接组合。我们的统一框架在端到端方式学习视频帧之间的自然一致性，因此它是不同特性的编辑无需重新训练。广泛的实验表明，RIGID在逆转和编辑任务中都能够胜过现有方法，详细的结果可以在<https://cnnlstm.github.io/RIGID>中找到。
</details></li>
</ul>
<hr>
<h2 id="Experts-Weights-Averaging-A-New-General-Training-Scheme-for-Vision-Transformers"><a href="#Experts-Weights-Averaging-A-New-General-Training-Scheme-for-Vision-Transformers" class="headerlink" title="Experts Weights Averaging: A New General Training Scheme for Vision Transformers"></a>Experts Weights Averaging: A New General Training Scheme for Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06093">http://arxiv.org/abs/2308.06093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongqi Huang, Peng Ye, Xiaoshui Huang, Sheng Li, Tao Chen, Wanli Ouyang</li>
<li>for: 这篇论文是为了提出一种新的普适训练策略 для Transformer 视觉模型（ViT），以提高性能而不增加推理成本。</li>
<li>methods: 这种训练策略利用 Mixture-of-Experts（MoE）机制，在训练和推理阶段分别使用不同的 FFN 和 MoE，以实现性能提高和成本降低。</li>
<li>results:  experiments 表明，这种训练策略可以在多个 2D 和 3D 视觉任务、不同的 ViT 架构和数据集上实现性能提高，并且可以应用于 fine-tuning ViTs 中进一步提高性能。<details>
<summary>Abstract</summary>
Structural re-parameterization is a general training scheme for Convolutional Neural Networks (CNNs), which achieves performance improvement without increasing inference cost. As Vision Transformers (ViTs) are gradually surpassing CNNs in various visual tasks, one may question: if a training scheme specifically for ViTs exists that can also achieve performance improvement without increasing inference cost? Recently, Mixture-of-Experts (MoE) has attracted increasing attention, as it can efficiently scale up the capacity of Transformers at a fixed cost through sparsely activated experts. Considering that MoE can also be viewed as a multi-branch structure, can we utilize MoE to implement a ViT training scheme similar to structural re-parameterization? In this paper, we affirmatively answer these questions, with a new general training strategy for ViTs. Specifically, we decouple the training and inference phases of ViTs. During training, we replace some Feed-Forward Networks (FFNs) of the ViT with specially designed, more efficient MoEs that assign tokens to experts by random uniform partition, and perform Experts Weights Averaging (EWA) on these MoEs at the end of each iteration. After training, we convert each MoE into an FFN by averaging the experts, transforming the model back into original ViT for inference. We further provide a theoretical analysis to show why and how it works. Comprehensive experiments across various 2D and 3D visual tasks, ViT architectures, and datasets validate the effectiveness and generalizability of the proposed training scheme. Besides, our training scheme can also be applied to improve performance when fine-tuning ViTs. Lastly, but equally important, the proposed EWA technique can significantly improve the effectiveness of naive MoE in various 2D visual small datasets and 3D visual tasks.
</details>
<details>
<summary>摘要</summary>
通用的training scheme for Convolutional Neural Networks (CNNs) 是structural re-parameterization, which can improve performance without increasing inference cost. As Vision Transformers (ViTs) are gradually surpassing CNNs in various visual tasks, one may wonder if there is a training scheme specifically for ViTs that can also achieve performance improvement without increasing inference cost. Recently, Mixture-of-Experts (MoE) has attracted increasing attention, as it can efficiently scale up the capacity of Transformers at a fixed cost through sparsely activated experts. Considering that MoE can also be viewed as a multi-branch structure, can we utilize MoE to implement a ViT training scheme similar to structural re-parameterization? In this paper, we answer these questions affirmatively, with a new general training strategy for ViTs. Specifically, we decouple the training and inference phases of ViTs. During training, we replace some Feed-Forward Networks (FFNs) of the ViT with specially designed, more efficient MoEs that assign tokens to experts by random uniform partition, and perform Experts Weights Averaging (EWA) on these MoEs at the end of each iteration. After training, we convert each MoE into an FFN by averaging the experts, transforming the model back into original ViT for inference. We further provide a theoretical analysis to show why and how it works. Comprehensive experiments across various 2D and 3D visual tasks, ViT architectures, and datasets validate the effectiveness and generalizability of the proposed training scheme. Besides, our training scheme can also be applied to improve performance when fine-tuning ViTs. Lastly, but equally important, the proposed EWA technique can significantly improve the effectiveness of naive MoE in various 2D visual small datasets and 3D visual tasks.
</details></li>
</ul>
<hr>
<h2 id="Versatile-Face-Animator-Driving-Arbitrary-3D-Facial-Avatar-in-RGBD-Space"><a href="#Versatile-Face-Animator-Driving-Arbitrary-3D-Facial-Avatar-in-RGBD-Space" class="headerlink" title="Versatile Face Animator: Driving Arbitrary 3D Facial Avatar in RGBD Space"></a>Versatile Face Animator: Driving Arbitrary 3D Facial Avatar in RGBD Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06076">http://arxiv.org/abs/2308.06076</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoyu Wang, Haozhe Wu, Junliang Xing, Jia Jia</li>
<li>for: 这paper的目的是提出一种新的自动化面部动画框架，以便在电影制作和游戏行业中创造真实的3D面部动画，并且能够减少成本和增加效率。</li>
<li>methods: 这paper使用的方法包括面部动画捕捉和动作重定向，通过端到端的方式实现无需blendshape或rigs的face animation生成。具体来说， authors propose了一种RGBD动画模块，通过层次动作字典学习面部动作从 Raw RGBD视频中，并将RGBD图像细化到3D面部模型中，以便无论面部模型的topology、文字、blendshape或rigs是什么，都可以生成高质量的3D面部动画。</li>
<li>results: 实验表明， authors的提出的框架可以生成出吸引人的3D面部动画效果， highlighting its potential as a promising solution for the cost-effective and efficient production of facial animation in the metaverse.<details>
<summary>Abstract</summary>
Creating realistic 3D facial animation is crucial for various applications in the movie production and gaming industry, especially with the burgeoning demand in the metaverse. However, prevalent methods such as blendshape-based approaches and facial rigging techniques are time-consuming, labor-intensive, and lack standardized configurations, making facial animation production challenging and costly. In this paper, we propose a novel self-supervised framework, Versatile Face Animator, which combines facial motion capture with motion retargeting in an end-to-end manner, eliminating the need for blendshapes or rigs. Our method has the following two main characteristics: 1) we propose an RGBD animation module to learn facial motion from raw RGBD videos by hierarchical motion dictionaries and animate RGBD images rendered from 3D facial mesh coarse-to-fine, enabling facial animation on arbitrary 3D characters regardless of their topology, textures, blendshapes, and rigs; and 2) we introduce a mesh retarget module to utilize RGBD animation to create 3D facial animation by manipulating facial mesh with controller transformations, which are estimated from dense optical flow fields and blended together with geodesic-distance-based weights. Comprehensive experiments demonstrate the effectiveness of our proposed framework in generating impressive 3D facial animation results, highlighting its potential as a promising solution for the cost-effective and efficient production of facial animation in the metaverse.
</details>
<details>
<summary>摘要</summary>
创造真实的3D人脸动画对于电影制作和游戏业界的各种应用非常重要，特别是在metaverse领域的发展。然而，现有的方法，如blendshape-basedapproaches和facial rigging技术，是时间消耗大、劳动密集，而且没有标准化的配置，使得人脸动画生产变得困难和昂贵。在这篇论文中，我们提出了一种新的自主学习框架，名为Versatile Face Animator，它将facial motion capture和动作重定向结合在一起，从而消除了需要blendshapes或rigs的需求。我们的方法有以下两个主要特点：1. 我们提出了一种RGBD动画模块，通过层次动作词典来学习人脸动作从raw RGBD视频中，并将RGBD图像渲染为3D人脸模型，从而实现人脸动画的生成。这种方法允许人脸动画生产在任何3D人脸模型上，无论其topology、texture、blendshapes或rigs等属性。2. 我们引入了一种网格重定向模块，通过使用 dense optical flow Fields和geodesic-distance-based weights来估算控制器变换，并将其与RGBD动画结合以生成3D人脸动画。我们的实验表明，我们提出的框架可以生成出吸引人的3D人脸动画结果， highlighting its potential as a promising solution for the cost-effective and efficient production of facial animation in the metaverse.
</details></li>
</ul>
<hr>
<h2 id="Out-of-Distribution-Detection-for-Monocular-Depth-Estimation"><a href="#Out-of-Distribution-Detection-for-Monocular-Depth-Estimation" class="headerlink" title="Out-of-Distribution Detection for Monocular Depth Estimation"></a>Out-of-Distribution Detection for Monocular Depth Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06072">http://arxiv.org/abs/2308.06072</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julia Hornauer, Adrian Holzbock, Vasileios Belagiannis</li>
<li>for: 本研究旨在提高单目深度估计中的不确定性估计方法，主要是针对图像噪声引入的数据不确定性。</li>
<li>methods: 我们使用 anomaly detection 技术来检测 encoder-decoder 深度估计模型中的数据不符合分布（out-of-distribution，OOD）图像。我们使用固定深度encoder提取特征，然后使用只有在 Distribution 数据进行图像重建训练。因此，OOD 图像会导致高重建错误，我们可以用这个错误来 отличи出在 Distribution 和 OOD 图像之间。</li>
<li>results: 我们在标准的 NYU Depth V2 和 KITTI 测试集上进行了实验，并发现我们的后期方法可以在不修改已经训练过的 encoder-decoder 深度估计模型上达到 astonishingly 好的性能，并且超过了现有的不确定性估计方法。<details>
<summary>Abstract</summary>
In monocular depth estimation, uncertainty estimation approaches mainly target the data uncertainty introduced by image noise. In contrast to prior work, we address the uncertainty due to lack of knowledge, which is relevant for the detection of data not represented by the training distribution, the so-called out-of-distribution (OOD) data. Motivated by anomaly detection, we propose to detect OOD images from an encoder-decoder depth estimation model based on the reconstruction error. Given the features extracted with the fixed depth encoder, we train an image decoder for image reconstruction using only in-distribution data. Consequently, OOD images result in a high reconstruction error, which we use to distinguish between in- and out-of-distribution samples. We built our experiments on the standard NYU Depth V2 and KITTI benchmarks as in-distribution data. Our post hoc method performs astonishingly well on different models and outperforms existing uncertainty estimation approaches without modifying the trained encoder-decoder depth estimation model.
</details>
<details>
<summary>摘要</summary>
单目深度估算中的不确定性估计方法主要针对图像噪声引入的数据不确定性。与先前的工作不同，我们处理lack of knowledge引起的不确定性，这是对于检测没有被训练分布表示的数据（即out-of-distribution，OOD数据）的探测。受到异常检测的 inspirited by，我们提议通过重建错误来探测OOD图像。我们使用固定深度encoder提取特征，然后使用只有在distribution数据进行图像重建训练。因此，OOD图像会导致高重建错误，我们可以使用这个错误来分辨在-和out-of-distribution样本。我们的实验基于标准的NYU Depth V2和KITTIbenchmark中的in-distribution数据。我们的后期方法在不修改已经训练过的encoder-decoder深度估算模型上表现出色，并且超过了现有的不确定性估计方法。
</details></li>
</ul>
<hr>
<h2 id="Head-Rotation-in-Denoising-Diffusion-Models"><a href="#Head-Rotation-in-Denoising-Diffusion-Models" class="headerlink" title="Head Rotation in Denoising Diffusion Models"></a>Head Rotation in Denoising Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06057">http://arxiv.org/abs/2308.06057</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/asperti/head-rotation">https://github.com/asperti/head-rotation</a></li>
<li>paper_authors: Andrea Asperti, Gabriele Colasuonno, Antonio Guerra</li>
<li>for: 本研究旨在探讨 Denoising Diffusion Models (DDM) 在深度生成模型领域的应用，特别是在批处理和修改图像中的特征特点。</li>
<li>methods: 本研究使用了一种新的嵌入技术，即 Denoising Diffusion Implicit Models (DDIM)，以实现高维latent space中的 semantics 探索和特征修改。</li>
<li>results: 研究表明，通过对数据集样本进行linear regression，可以获得不同拜角度($\pm 30^o$) 的批处理结果，保留图像的特征特点。此外，研究还发现了一种基于照明方向的图像分类法，将 CelebA 图像分为三个主要组：左、中、右。<details>
<summary>Abstract</summary>
Denoising Diffusion Models (DDM) are emerging as the cutting-edge technology in the realm of deep generative modeling, challenging the dominance of Generative Adversarial Networks. However, effectively exploring the latent space's semantics and identifying compelling trajectories for manipulating and editing important attributes of the generated samples remains challenging, primarily due to the high-dimensional nature of the latent space. In this study, we specifically concentrate on face rotation, which is known to be one of the most intricate editing operations. By leveraging a recent embedding technique for Denoising Diffusion Implicit Models (DDIM), we achieve, in many cases, noteworthy manipulations encompassing a wide rotation angle of $\pm 30^o$, preserving the distinct characteristics of the individual. Our methodology exploits the computation of trajectories approximating clouds of latent representations of dataset samples with different yaw rotations through linear regression. Specific trajectories are obtained by restricting the analysis to subsets of data sharing significant attributes with the source image. One of these attributes is the light provenance: a byproduct of our research is a labeling of CelebA, categorizing images into three major groups based on the illumination direction: left, center, and right.
</details>
<details>
<summary>摘要</summary>
德提高扩散模型（DDM）正在深度生成模型领域中崛起，挑战对抗生成对抗网络的主导地位。然而，深入探索生成空间的 semantics 和找到编辑重要特征的吸引人之路仍然是一大挑战，主要因为生成空间的维度很高。在这个研究中，我们专注于脸 rotate，被认为是深度生成模型中最复杂的编辑操作。我们通过利用最近的 Denoising Diffusion Implicit Models（DDIM）嵌入技术，在许多情况下实现了30度的旋转角范围内的出色操作，并保持个体特征的独特性。我们的方法利用计算 trajectories  approximating clouds of latent representations of dataset samples with different yaw rotations through linear regression。specific trajectories 是通过对数据集中 sharing  significative attributes with the source image 进行限制分析获得的。其中一个特征是光源来源：我们的研究中的一个成果是对 CelebA 图像进行了分类，将图像分为三个主要组 Based on the illumination direction：left, center, and right。
</details></li>
</ul>
<hr>
<h2 id="Computer-Aided-Cytology-Diagnosis-in-Animals-CNN-Based-Image-Quality-Assessment-for-Accurate-Disease-Classification"><a href="#Computer-Aided-Cytology-Diagnosis-in-Animals-CNN-Based-Image-Quality-Assessment-for-Accurate-Disease-Classification" class="headerlink" title="Computer-Aided Cytology Diagnosis in Animals: CNN-Based Image Quality Assessment for Accurate Disease Classification"></a>Computer-Aided Cytology Diagnosis in Animals: CNN-Based Image Quality Assessment for Accurate Disease Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06055">http://arxiv.org/abs/2308.06055</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan Krupiński, Maciej Wielgosz, Szymon Mazurek, Krystian Strzałka, Paweł Russek, Jakub Caputa, Daria Łukasik, Jakub Grzeszczyk, Michał Karwatowski, Rafał Fraczek, Ernest Jamro, Marcin Pietroń, Sebastian Koryciak, Agnieszka Dąbrowska-Boruch, Kazimierz Wiatr</li>
<li>for: 这个论文是为了开发一个基于计算机的病理诊断系统，用于动物病理诊断。</li>
<li>methods: 论文使用了卷积神经网络（CNN）进行图像质量评估（IQA）。</li>
<li>results: 研究发现，使用CNN进行IQA可以提高病理诊断的准确率。<details>
<summary>Abstract</summary>
This paper presents a computer-aided cytology diagnosis system designed for animals, focusing on image quality assessment (IQA) using Convolutional Neural Networks (CNNs). The system's building blocks are tailored to seamlessly integrate IQA, ensuring reliable performance in disease classification. We extensively investigate the CNN's ability to handle various image variations and scenarios, analyzing the impact on detecting low-quality input data. Additionally, the network's capacity to differentiate valid cellular samples from those with artifacts is evaluated. Our study employs a ResNet18 network architecture and explores the effects of input sizes and cropping strategies on model performance. The research sheds light on the significance of CNN-based IQA in computer-aided cytology diagnosis for animals, enhancing the accuracy of disease classification.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "computer-aided cytology diagnosis system" is translated as "计算机助成细胞诊断系统" (computer-aided cytology diagnosis system)* "Convolutional Neural Networks" is translated as "卷积神经网络" (Convolutional Neural Networks)* "image quality assessment" is translated as "图像质量评估" (image quality assessment)* "IQA" is translated as "IQA" (IQA)* "disease classification" is translated as "疾病分类" (disease classification)* "ResNet18" is translated as "ResNet18" (ResNet18)* "input sizes" is translated as "输入大小" (input sizes)* "cropping strategies" is translated as "剪辑策略" (cropping strategies)
</details></li>
</ul>
<hr>
<h2 id="Hardware-Accelerators-in-Autonomous-Driving"><a href="#Hardware-Accelerators-in-Autonomous-Driving" class="headerlink" title="Hardware Accelerators in Autonomous Driving"></a>Hardware Accelerators in Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06054">http://arxiv.org/abs/2308.06054</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ken Power, Shailendra Deva, Ting Wang, Julius Li, Ciarán Eising</li>
<li>for: 本研究旨在概述机器学习加速器在自动驾驶汽车中的应用，以提高机器视觉任务的性能和可靠性。</li>
<li>methods: 本文使用了多种机器学习模型和特殊目标处理器来加速机器视觉任务。</li>
<li>results: 本研究提出了一些建议和未来研究方向，以推动机器学习加速器在自动驾驶领域的发展。<details>
<summary>Abstract</summary>
Computing platforms in autonomous vehicles record large amounts of data from many sensors, process the data through machine learning models, and make decisions to ensure the vehicle's safe operation. Fast, accurate, and reliable decision-making is critical. Traditional computer processors lack the power and flexibility needed for the perception and machine vision demands of advanced autonomous driving tasks. Hardware accelerators are special-purpose coprocessors that help autonomous vehicles meet performance requirements for higher levels of autonomy. This paper provides an overview of ML accelerators with examples of their use for machine vision in autonomous vehicles. We offer recommendations for researchers and practitioners and highlight a trajectory for ongoing and future research in this emerging field.
</details>
<details>
<summary>摘要</summary>
计算平台在自动驾驶车辆记录大量感知器数据，通过机器学习模型进行处理，以确保车辆安全运行。快速、准确、可靠的决策是关键。传统计算机处理器缺乏高级自动驾驶任务的感知和机器视觉需求的能力。硬件加速器是特殊用途的辅助处理器，帮助自动驾驶车辆实现更高的自主驱动水平。本文提供机器学习加速器的概述，以及机器视觉在自动驾驶车辆中的应用例子。我们对研究人员和实践者提出建议，并高亮了该领域的未来研究轨迹。
</details></li>
</ul>
<hr>
<h2 id="Towards-Instance-adaptive-Inference-for-Federated-Learning"><a href="#Towards-Instance-adaptive-Inference-for-Federated-Learning" class="headerlink" title="Towards Instance-adaptive Inference for Federated Learning"></a>Towards Instance-adaptive Inference for Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06051">http://arxiv.org/abs/2308.06051</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chunmeifeng/fedins">https://github.com/chunmeifeng/fedins</a></li>
<li>paper_authors: Chun-Mei Feng, Kai Yu, Nian Liu, Xinxing Xu, Salman Khan, Wangmeng Zuo</li>
<li>for: 这种论文是为了解决 federated learning (FL) 中的客户端数据不均衡问题，特别是在复杂的实际数据上出现的内部客户端数据不均衡。</li>
<li>methods: 该论文提出了一种新的 FL 算法，即 FedIns，用于处理内部客户端数据不均衡。该算法通过实例特定的折衡策略来实现实例特定的折衡，从而降低了客户端数据不均衡的影响。</li>
<li>results: 实验结果表明， compare to 状态之前的方法，FedIns 可以达到6.64%的提升，并且与少于15%的通信成本相对。<details>
<summary>Abstract</summary>
Federated learning (FL) is a distributed learning paradigm that enables multiple clients to learn a powerful global model by aggregating local training. However, the performance of the global model is often hampered by non-i.i.d. distribution among the clients, requiring extensive efforts to mitigate inter-client data heterogeneity. Going beyond inter-client data heterogeneity, we note that intra-client heterogeneity can also be observed on complex real-world data and seriously deteriorate FL performance. In this paper, we present a novel FL algorithm, i.e., FedIns, to handle intra-client data heterogeneity by enabling instance-adaptive inference in the FL framework. Instead of huge instance-adaptive models, we resort to a parameter-efficient fine-tuning method, i.e., scale and shift deep features (SSF), upon a pre-trained model. Specifically, we first train an SSF pool for each client, and aggregate these SSF pools on the server side, thus still maintaining a low communication cost. To enable instance-adaptive inference, for a given instance, we dynamically find the best-matched SSF subsets from the pool and aggregate them to generate an adaptive SSF specified for the instance, thereby reducing the intra-client as well as the inter-client heterogeneity. Extensive experiments show that our FedIns outperforms state-of-the-art FL algorithms, e.g., a 6.64\% improvement against the top-performing method with less than 15\% communication cost on Tiny-ImageNet. Our code and models will be publicly released.
</details>
<details>
<summary>摘要</summary>
联合学习（FL）是一种分布式学习 paradigma，允许多个客户端学习一个强大的全球模型，通过聚合本地训练来提高性能。然而，全球模型的性能 oftentimes 受到客户端数据不均衡的影响，需要大量努力来mitigate inter-client data heterogeneity。此外，我们注意到了复杂的实际数据中的内部客户端数据不均衡，也会严重影响FL性能。在这篇论文中，我们提出了一种新的FL算法，即FedIns，用于处理内部客户端数据不均衡。在FL框架中，我们使用 instance-adaptive inference 来适应实例特点，而不需要巨大的实例特化模型。具体来说，我们首先在每个客户端上训练一个scale and shift deep features（SSF）池，然后在服务器端将这些SSF池进行聚合，以保持低通信成本。为了实现实例特化的推理，对于一个实例，我们会在实例特点上动态找到最佳的SSF子集，并将这些子集聚合以生成一个适应该实例的adaptive SSF，从而降低内部客户端以及客户端之间的数据不均衡。我们的 FedIns 在 Tiny-ImageNet 上比顶尖方法提高6.64%，并且通信成本低于15%。我们将代码和模型公开发布。
</details></li>
</ul>
<hr>
<h2 id="Diverse-Data-Augmentation-with-Diffusions-for-Effective-Test-time-Prompt-Tuning"><a href="#Diverse-Data-Augmentation-with-Diffusions-for-Effective-Test-time-Prompt-Tuning" class="headerlink" title="Diverse Data Augmentation with Diffusions for Effective Test-time Prompt Tuning"></a>Diverse Data Augmentation with Diffusions for Effective Test-time Prompt Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06038">http://arxiv.org/abs/2308.06038</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chunmeifeng/DiffTPT">https://github.com/chunmeifeng/DiffTPT</a></li>
<li>paper_authors: Chun-Mei Feng, Kai Yu, Yong Liu, Salman Khan, Wangmeng Zuo</li>
<li>for: 这个研究paper的目的是提出一种名为DiffTPT的新的测试时间Prompt Tuning方法，用于在不知道的新领域上适应下测试样本。</li>
<li>methods: 这个方法使用了预训条件模型，并利用预训 diffusion 模型生成多元和有用的新数据，以扩大模型在不知道的新领域上的适应能力。</li>
<li>results: 在测试数据集上，DiffTPT 方法可以提高零件精度的正确率，比起现有的state-of-the-art TPT 方法，提高了5.13%。<details>
<summary>Abstract</summary>
Benefiting from prompt tuning, recent years have witnessed the promising performance of pre-trained vision-language models, e.g., CLIP, on versatile downstream tasks. In this paper, we focus on a particular setting of learning adaptive prompts on the fly for each test sample from an unseen new domain, which is known as test-time prompt tuning (TPT). Existing TPT methods typically rely on data augmentation and confidence selection. However, conventional data augmentation techniques, e.g., random resized crops, suffers from the lack of data diversity, while entropy-based confidence selection alone is not sufficient to guarantee prediction fidelity. To address these issues, we propose a novel TPT method, named DiffTPT, which leverages pre-trained diffusion models to generate diverse and informative new data. Specifically, we incorporate augmented data by both conventional method and pre-trained stable diffusion to exploit their respective merits, improving the models ability to adapt to unknown new test data. Moreover, to ensure the prediction fidelity of generated data, we introduce a cosine similarity-based filtration technique to select the generated data with higher similarity to the single test sample. Our experiments on test datasets with distribution shifts and unseen categories demonstrate that DiffTPT improves the zero-shot accuracy by an average of 5.13\% compared to the state-of-the-art TPT method. Our code and models will be publicly released.
</details>
<details>
<summary>摘要</summary>
在最近几年，快速调参的推动下，预训练的视觉语言模型，如CLIP，在多种下游任务上表现出了扎实的潜力。在这篇论文中，我们关注一种特殊的学习适应 prompt 的方法，即在测试阶段调参（TPT）。现有的 TPT 方法通常基于数据增强和信任选择。然而，传统的数据增强技术，例如随机resize crop，缺乏数据多样性，而基于Entropy的信任选择独立不能保证预测准确性。为解决这些问题，我们提出了一种新的 TPT 方法，名为DiffTPT，它利用预训练的扩散模型生成多样和有用的新数据。具体来说，我们结合了传统的数据增强方法和预训练的稳定扩散来利用它们的优势，提高模型对未知新测试数据的适应能力。此外，为保证生成数据的预测准确性，我们引入了cosine similarity基于的筛选技术，选择生成数据与单个测试样本更高的相似性。我们的实验表明，DiffTPT 在分布偏移和未知类型的测试集上提高了零aser准精度，相比前州之最的 TPT 方法。我们的代码和模型将公开发布。
</details></li>
</ul>
<hr>
<h2 id="Masked-Attention-Diffusion-Guidance-for-Spatially-Controlling-Text-to-Image-Generation"><a href="#Masked-Attention-Diffusion-Guidance-for-Spatially-Controlling-Text-to-Image-Generation" class="headerlink" title="Masked-Attention Diffusion Guidance for Spatially Controlling Text-to-Image Generation"></a>Masked-Attention Diffusion Guidance for Spatially Controlling Text-to-Image Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06027">http://arxiv.org/abs/2308.06027</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuki Endo</li>
<li>for: 这个论文的目的是提出一种不需要额外训练 diffusion models 的文本到图像生成方法，以实现更好的空间控制。</li>
<li>methods: 该方法基于 cross-attention maps 的 pozitional 关系，通过 directly swapping cross-attention maps 与常数 maps 计算自 semantic regions 来控制注意力。 此外，提出了 masked-attention guidance，通过 manipulate 噪音图来控制注意力，使图像更 faithful 于 semantic masks。</li>
<li>results: 实验表明，该方法可以比基eline 更加准确地控制空间，并且可以生成更 faithful 于 semantic masks 的图像。<details>
<summary>Abstract</summary>
Text-to-image synthesis has achieved high-quality results with recent advances in diffusion models. However, text input alone has high spatial ambiguity and limited user controllability. Most existing methods allow spatial control through additional visual guidance (e.g, sketches and semantic masks) but require additional training with annotated images. In this paper, we propose a method for spatially controlling text-to-image generation without further training of diffusion models. Our method is based on the insight that the cross-attention maps reflect the positional relationship between words and pixels. Our aim is to control the attention maps according to given semantic masks and text prompts. To this end, we first explore a simple approach of directly swapping the cross-attention maps with constant maps computed from the semantic regions. Moreover, we propose masked-attention guidance, which can generate images more faithful to semantic masks than the first approach. Masked-attention guidance indirectly controls attention to each word and pixel according to the semantic regions by manipulating noise images fed to diffusion models. Experiments show that our method enables more accurate spatial control than baselines qualitatively and quantitatively.
</details>
<details>
<summary>摘要</summary>
为达到这个目标，我们首先探索了直接将抽象映射换成固定的映射，从Semantic Regions中计算的常数映射。此外，我们还提出了受隐藏图像干扰的掩码引导，可以在Semantic Mask中更加准确地控制注意力。这种方法通过在扩散模型中隐藏图像的干扰来控制每个字和像素的注意力，从而更加准确地实现Semantic Mask中的图像生成。实验表明，我们的方法可以较为精确地控制文本到图像的空间位置，相比基线方法。
</details></li>
</ul>
<hr>
<h2 id="Spatial-information-Guided-Adaptive-Context-aware-Network-for-Efficient-RGB-D-Semantic-Segmentation"><a href="#Spatial-information-Guided-Adaptive-Context-aware-Network-for-Efficient-RGB-D-Semantic-Segmentation" class="headerlink" title="Spatial-information Guided Adaptive Context-aware Network for Efficient RGB-D Semantic Segmentation"></a>Spatial-information Guided Adaptive Context-aware Network for Efficient RGB-D Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06024">http://arxiv.org/abs/2308.06024</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mvme-hbut/sgacnet">https://github.com/mvme-hbut/sgacnet</a></li>
<li>paper_authors: Yang Zhang, Chenyun Xiong, Junjie Liu, Xuhui Ye, Guodong Sun</li>
<li>for: 这篇论文主要针对mobile robots中的RGB-D语义分割问题，即在环境中分割对象和场景，并对其进行分类。</li>
<li>methods: 该论文提出了一种高效的encoder-decoder网络，该网络通过杂合模式和空间杂合注意力模块来有效地捕捉多级RGB-D特征。此外，论文还提出了一种全球导航本地相互关系模块，以获取足够高级上下文信息。</li>
<li>results: 实验结果表明，该方法在NYUv2、SUN RGB-D和Cityscapes数据集上比州前方法更好地平衡了分割精度、计算时间和参数数量。<details>
<summary>Abstract</summary>
Efficient RGB-D semantic segmentation has received considerable attention in mobile robots, which plays a vital role in analyzing and recognizing environmental information. According to previous studies, depth information can provide corresponding geometric relationships for objects and scenes, but actual depth data usually exist as noise. To avoid unfavorable effects on segmentation accuracy and computation, it is necessary to design an efficient framework to leverage cross-modal correlations and complementary cues. In this paper, we propose an efficient lightweight encoder-decoder network that reduces the computational parameters and guarantees the robustness of the algorithm. Working with channel and spatial fusion attention modules, our network effectively captures multi-level RGB-D features. A globally guided local affinity context module is proposed to obtain sufficient high-level context information. The decoder utilizes a lightweight residual unit that combines short- and long-distance information with a few redundant computations. Experimental results on NYUv2, SUN RGB-D, and Cityscapes datasets show that our method achieves a better trade-off among segmentation accuracy, inference time, and parameters than the state-of-the-art methods. The source code will be at https://github.com/MVME-HBUT/SGACNet
</details>
<details>
<summary>摘要</summary>
高效的RGB-Dsemantic segmentation在移动机器人中得到了广泛的关注，它对环境信息的分析和识别具有重要作用。根据前一些研究，深度信息可以提供对物体和场景的相对性 geometric 关系，但实际的深度数据通常存在噪声。为了避免不利的影响于分 segmentation 精度和计算，需要设计一个有效的框架，利用交叉模态的相关性和补做信息。在这篇论文中，我们提出了一种高效的轻量级 encoder-decoder 网络，它可以减少计算参数并保证算法的稳定性。与核心和空间拼接注意力模块结合，我们的网络可以有效地捕捉多级 RGB-D 特征。我们还提出了一种全局导引的地方相互关系上下文模块，以获得足够的高级上下文信息。解码器使用一种轻量级径相关单元，将短距离和长距离信息相结合，并且只有一些冗余计算。实验结果表明，我们的方法在 NYUv2、SUN RGB-D 和 Cityscapes 数据集上实现了更好的精度至灵活性至参数的平衡，比现有方法更好。代码将在 GitHub 上发布，链接在 <https://github.com/MVME-HBUT/SGACNet> 。
</details></li>
</ul>
<hr>
<h2 id="Scale-Preserving-Automatic-Concept-Extraction-SPACE"><a href="#Scale-Preserving-Automatic-Concept-Extraction-SPACE" class="headerlink" title="Scale-Preserving Automatic Concept Extraction (SPACE)"></a>Scale-Preserving Automatic Concept Extraction (SPACE)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06022">http://arxiv.org/abs/2308.06022</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/data-science-in-mechanical-engineering/space">https://github.com/data-science-in-mechanical-engineering/space</a></li>
<li>paper_authors: Andrés Felipe Posada-Moreno, Lukas Kreisköther, Tassilo Glander, Sebastian Trimpe</li>
<li>for: 提高工业四点零中 convolutional neural network（CNN）的可靠性和透明度，以及减少经济损失和人类生命风险。</li>
<li>methods: 基于图像 slice 的方法，避免缺失缩放问题，提供全面的概念描述。</li>
<li>results: 在工业质量控制领域的三个图像分类 dataset 上，SPACE 方法比其他方法表现更好，提供了可操作的概念描述，帮助理解 CNN 的决策机制。<details>
<summary>Abstract</summary>
Convolutional Neural Networks (CNN) have become a common choice for industrial quality control, as well as other critical applications in the Industry 4.0. When these CNNs behave in ways unexpected to human users or developers, severe consequences can arise, such as economic losses or an increased risk to human life. Concept extraction techniques can be applied to increase the reliability and transparency of CNNs through generating global explanations for trained neural network models. The decisive features of image datasets in quality control often depend on the feature's scale; for example, the size of a hole or an edge. However, existing concept extraction methods do not correctly represent scale, which leads to problems interpreting these models as we show herein. To address this issue, we introduce the Scale-Preserving Automatic Concept Extraction (SPACE) algorithm, as a state-of-the-art alternative concept extraction technique for CNNs, focused on industrial applications. SPACE is specifically designed to overcome the aforementioned problems by avoiding scale changes throughout the concept extraction process. SPACE proposes an approach based on square slices of input images, which are selected and then tiled before being clustered into concepts. Our method provides explanations of the models' decision-making process in the form of human-understandable concepts. We evaluate SPACE on three image classification datasets in the context of industrial quality control. Through experimental results, we illustrate how SPACE outperforms other methods and provides actionable insights on the decision mechanisms of CNNs. Finally, code for the implementation of SPACE is provided.
</details>
<details>
<summary>摘要</summary>
convolutional neural networks (CNN) 已成为工业品质控制的常用选择，以及工业4.0其他关键应用程序。当这些CNN表现出人类用户或开发者所未预料的行为时，可能会导致经济损失或增加人类生命风险。概念提取技术可以应用于增加CNN的可靠性和透明度，通过生成全局解释以便训练的神经网络模型。图像 Dataset 中的特征特征frequently dependent on the scale of the feature; 例如，图像中的孔或边缘的大小。然而，现有的概念提取方法不正确地表示尺度，这会导致解释问题。为解决这个问题，我们介绍了Scale-Preserving Automatic Concept Extraction (SPACE)算法，作为工业应用场景中的现代替代方法。SPACE专门设计用于解决以上问题，避免在概念提取过程中改变尺度。SPACE提出基于输入图像的方方块，选择并粘贴后，再分类为概念。我们的方法可以提供神经网络模型决策过程的人类可理解的解释。我们在三个图像分类Dataset中进行了实验，并证明了SPACE在工业品质控制领域的表现优于其他方法，并提供了可行的解释。最后，我们提供了实现SPACE的代码。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Generalization-of-Universal-Adversarial-Perturbation-through-Gradient-Aggregation"><a href="#Enhancing-Generalization-of-Universal-Adversarial-Perturbation-through-Gradient-Aggregation" class="headerlink" title="Enhancing Generalization of Universal Adversarial Perturbation through Gradient Aggregation"></a>Enhancing Generalization of Universal Adversarial Perturbation through Gradient Aggregation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06015">http://arxiv.org/abs/2308.06015</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liuxuannan/stochastic-gradient-aggregation">https://github.com/liuxuannan/stochastic-gradient-aggregation</a></li>
<li>paper_authors: Xuannan Liu, Yaoyao Zhong, Yuhang Zhang, Lixiong Qin, Weihong Deng</li>
<li>for: 提高 universal adversarial perturbation (UAP) 的泛化能力，解决 UAP 生成方法中的梯度消失和局部最优化问题。</li>
<li>methods: 提出 Stochastic Gradient Aggregation (SGA) 方法，通过多个小批量训练和内部多次归一化来稳定梯度和减少量化误差，从而提高 UAP 的泛化能力。</li>
<li>results: EXTENSIVE experiments on the standard ImageNet dataset demonstrate that our method significantly enhances the generalization ability of UAP and outperforms other state-of-the-art methods。<details>
<summary>Abstract</summary>
Deep neural networks are vulnerable to universal adversarial perturbation (UAP), an instance-agnostic perturbation capable of fooling the target model for most samples. Compared to instance-specific adversarial examples, UAP is more challenging as it needs to generalize across various samples and models. In this paper, we examine the serious dilemma of UAP generation methods from a generalization perspective -- the gradient vanishing problem using small-batch stochastic gradient optimization and the local optima problem using large-batch optimization. To address these problems, we propose a simple and effective method called Stochastic Gradient Aggregation (SGA), which alleviates the gradient vanishing and escapes from poor local optima at the same time. Specifically, SGA employs the small-batch training to perform multiple iterations of inner pre-search. Then, all the inner gradients are aggregated as a one-step gradient estimation to enhance the gradient stability and reduce quantization errors. Extensive experiments on the standard ImageNet dataset demonstrate that our method significantly enhances the generalization ability of UAP and outperforms other state-of-the-art methods. The code is available at https://github.com/liuxuannan/Stochastic-Gradient-Aggregation.
</details>
<details>
<summary>摘要</summary>
TRANSLATION:深度神经网络受到通用对抗扰动（UAP）的威胁，UAP是一种能够骗取目标模型的大多数样本的实例无关扰动。与实例特定对抗示例相比，UAP更加挑战性，因为它需要在不同的样本和模型之间进行泛化。在这篇文章中，我们对UAP生成方法的泛化问题进行了深入的检查——梯度消失问题和大批量优化问题。为了解决这些问题，我们提出了一种简单有效的方法，即随机梯度聚合（SGA）。SGA使用小批量训练来进行多次内部预搜，然后将所有内部梯度聚合为一个步骤的梯度估计，以提高梯度稳定性和减少量化误差。广泛的实验表明，我们的方法可以很好地提高UAP的泛化能力，并超过了当前状态的方法。代码可以在https://github.com/liuxuannan/Stochastic-Gradient-Aggregation中获取。
</details></li>
</ul>
<hr>
<h2 id="ViGT-Proposal-free-Video-Grounding-with-Learnable-Token-in-Transformer"><a href="#ViGT-Proposal-free-Video-Grounding-with-Learnable-Token-in-Transformer" class="headerlink" title="ViGT: Proposal-free Video Grounding with Learnable Token in Transformer"></a>ViGT: Proposal-free Video Grounding with Learnable Token in Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06009">http://arxiv.org/abs/2308.06009</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kun Li, Dan Guo, Meng Wang</li>
<li>for: 本研究targets the video grounding (VG) task, aiming to locate the queried action or event in an untrimmed video based on rich linguistic descriptions.</li>
<li>methods: 我们提出了一种novel boundary regression paradigm, which performs regression token learning in a transformer. Specifically, we present a simple but effective proposal-free framework, namely Video Grounding Transformer (ViGT), which predicts the temporal boundary using a learnable regression token rather than multi-modal or cross-modal features.</li>
<li>results: 在三个公共数据集（ANet Captions、TACoS和YouCookII）上，我们的提案ViGT表现出色，并进行了广泛的ablation study和质量分析以验证我们的解释性。<details>
<summary>Abstract</summary>
The video grounding (VG) task aims to locate the queried action or event in an untrimmed video based on rich linguistic descriptions. Existing proposal-free methods are trapped in complex interaction between video and query, overemphasizing cross-modal feature fusion and feature correlation for VG. In this paper, we propose a novel boundary regression paradigm that performs regression token learning in a transformer. Particularly, we present a simple but effective proposal-free framework, namely Video Grounding Transformer (ViGT), which predicts the temporal boundary using a learnable regression token rather than multi-modal or cross-modal features. In ViGT, the benefits of a learnable token are manifested as follows. (1) The token is unrelated to the video or the query and avoids data bias toward the original video and query. (2) The token simultaneously performs global context aggregation from video and query features. First, we employed a sharing feature encoder to project both video and query into a joint feature space before performing cross-modal co-attention (i.e., video-to-query attention and query-to-video attention) to highlight discriminative features in each modality. Furthermore, we concatenated a learnable regression token [REG] with the video and query features as the input of a vision-language transformer. Finally, we utilized the token [REG] to predict the target moment and visual features to constrain the foreground and background probabilities at each timestamp. The proposed ViGT performed well on three public datasets: ANet Captions, TACoS and YouCookII. Extensive ablation studies and qualitative analysis further validated the interpretability of ViGT.
</details>
<details>
<summary>摘要</summary>
视频定位（VG）任务的目标是在没有提案的情况下，基于丰富的语言描述，在不归档的视频中找到查询的动作或事件。现有的方法受到视频和查询之间的复杂交互的限制，过度强调cross-modal特征融合和特征相关性，导致VG任务的实现困难。在本文中，我们提出了一种新的boundary regression paradigm，即使用一个学习的回归токеン（REG）来实现视频定位。特别是，我们提出了一种简单 yet effective的无提案框架，即视频定位变换器（ViGT），该框架通过学习回归token来预测目标时刻，而不是通过多Modal或cross-modal特征。在ViGT中， learnable token的好处如下：1. token不与视频或查询有关，因此避免了数据偏好向原始视频和查询。2. token同时进行全局上下文聚合从视频和查询特征中。首先，我们使用了一个共享特征编码器将视频和查询映射到一个共同特征空间，然后进行cross-modal协同注意力（即视频到查询注意力和查询到视频注意力）以便强调每个模式中的特征。此外，我们将视频和查询特征 concatenated with a learnable回归token [REG]作为视频-语言变换器的输入。最后，我们使用了这个token [REG]来预测目标时刻，并使用视频和查询特征来约束背景和前景概率在每个时间戳。我们的ViGT在ANet Captions、TACoS和YouCookII等三个公共数据集上表现出色，并进行了广泛的归一化研究和质量分析，以证明ViGT的可读性。
</details></li>
</ul>
<hr>
<h2 id="Image-based-Geolocalization-by-Ground-to-2-5D-Map-Matching"><a href="#Image-based-Geolocalization-by-Ground-to-2-5D-Map-Matching" class="headerlink" title="Image-based Geolocalization by Ground-to-2.5D Map Matching"></a>Image-based Geolocalization by Ground-to-2.5D Map Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05993">http://arxiv.org/abs/2308.05993</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengjie Zhou, Liu Liu, Yiran Zhong</li>
<li>for: 图像基于地理位置问题的解决方案，即将平面图像与地图照片匹配。</li>
<li>methods: 我们提出了一种新的方法，通过在2.5D空间中使用高度信息来提高跨视图匹配。我们首先将2D地图与平面图像对齐使用极限变换，然后利用全球混合来混合多模态特征从2D和2.5D地图来增强定位编码的 distintiveness。</li>
<li>results: 我们的方法在两种常见的定位方法，单图定位和路径定位中实现了显著高于前一代2D地图基于方法的定位精度和更快的收敛速度。<details>
<summary>Abstract</summary>
We study the image-based geolocalization problem that aims to locate ground-view query images on cartographic maps. Previous methods often utilize cross-view localization techniques to match ground-view query images with 2D maps. However, the performance of these methods is frequently unsatisfactory due to the significant cross-view appearance differences. In this paper, we extend cross-view matching to 2.5D spaces, where the heights of the structures - such as trees, buildings, and other objects - can provide additional information to guide the cross-view matching. We present a new approach to learning representative embeddings from multi-model data. Specifically, we first align 2D maps to ground-view panoramic images with polar transform to reduce the gap between panoramic images and maps. Then we leverage global fusion to fuse the multi-modal features from 2D and 2.5D maps to increase the distinctiveness of location embeddings. We construct the first large-scale ground-to-2.5D map geolocalization dataset to validate our method and facilitate the research. We test our learned embeddings on two popular localization approaches, i.e., single-image based localization, and route based localization. Extensive experiments demonstrate that our proposed method achieves significantly higher localization accuracy and faster convergence than previous 2D map-based approaches.
</details>
<details>
<summary>摘要</summary>
我们研究图像基于地理位置 Localization 问题，即将地面查询图像与地图映射到一起。过去的方法 frequently utilize cross-view localization技术来匹配地面查询图像与2D地图。然而，这些方法的性能frequently不满足，因为跨视图的外观差异很大。在这篇论文中，我们将cross-view matching扩展到2.5D空间，其中结构高度，如树、建筑和其他物体的高度，可以提供更多的信息来导航跨视图匹配。我们提出了一种新的方法来学习表示 embedding从多模型数据中。特别是，我们首先将2D地图与地面全景图像使用极体 transform进行对齐，以降低全景图像与地图之间的差异。然后，我们利用全球混合来混合2D和2.5D地图的多模态特征，以提高定位编码的特征性。我们构建了首个大规模的地面到2.5D地图地理位置定位数据集，以验证我们的方法和促进研究。我们测试了我们学习的编码，并与单个图像基于定位和路径基于定位进行比较。广泛的实验表明，我们的提议方法在前2D地图基于方法的定位精度和更快的收敛速度上具有显著优势。
</details></li>
</ul>
<hr>
<h2 id="Cyclic-Bootstrap-Labeling-for-Weakly-Supervised-Object-Detection"><a href="#Cyclic-Bootstrap-Labeling-for-Weakly-Supervised-Object-Detection" class="headerlink" title="Cyclic-Bootstrap Labeling for Weakly Supervised Object Detection"></a>Cyclic-Bootstrap Labeling for Weakly Supervised Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05991">http://arxiv.org/abs/2308.05991</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yinyf0804/wsod-cbl">https://github.com/yinyf0804/wsod-cbl</a></li>
<li>paper_authors: Yufei Yin, Jiajun Deng, Wengang Zhou, Li Li, Houqiang Li</li>
<li>for: 提高weakly supervised object detection的精度，增强多个实例检测网络（MIDN）的 Pseudo-labeling 质量。</li>
<li>methods: 我们提出了一种新的weakly supervised object detection框架，即Cylic-Bootstrap Labeling（CBL），它利用了一个可靠的教师网络来优化MIDN的 Pseudo-labeling。特别是，我们使用了一种权重加权移动平均策略，以便利用不同的修正模块的输出。此外，我们还提出了一种新的类别特定的排名精灵抽象算法，用于使MIDN受益于教师网络的输出。</li>
<li>results: 我们在PASCAL VOC 2007 &amp; 2012和COCO datasets上进行了广泛的实验，并证明了我们的CBL框架在weakly supervised object detection中表现出色。<details>
<summary>Abstract</summary>
Recent progress in weakly supervised object detection is featured by a combination of multiple instance detection networks (MIDN) and ordinal online refinement. However, with only image-level annotation, MIDN inevitably assigns high scores to some unexpected region proposals when generating pseudo labels. These inaccurate high-scoring region proposals will mislead the training of subsequent refinement modules and thus hamper the detection performance. In this work, we explore how to ameliorate the quality of pseudo-labeling in MIDN. Formally, we devise Cyclic-Bootstrap Labeling (CBL), a novel weakly supervised object detection pipeline, which optimizes MIDN with rank information from a reliable teacher network. Specifically, we obtain this teacher network by introducing a weighted exponential moving average strategy to take advantage of various refinement modules. A novel class-specific ranking distillation algorithm is proposed to leverage the output of weighted ensembled teacher network for distilling MIDN with rank information. As a result, MIDN is guided to assign higher scores to accurate proposals among their neighboring ones, thus benefiting the subsequent pseudo labeling. Extensive experiments on the prevalent PASCAL VOC 2007 \& 2012 and COCO datasets demonstrate the superior performance of our CBL framework. Code will be available at https://github.com/Yinyf0804/WSOD-CBL/.
</details>
<details>
<summary>摘要</summary>
最近的弱监督对象检测进步主要表现为多个实例检测网络（MIDN）和顺序在线级化。然而，只有图像级别的标注，MIDN会不可避免地将一些意外的区域提案分配高分数。这些不准确的高分数区域提案会 Mislead 后续的级化模块的训练，从而降低检测性能。在这种情况下，我们explore如何改善MIDN中 pseudo-标签的质量。我们提出了一种新的弱监督对象检测框架，即循环 bootstrap labeling（CBL），该框架利用多个可靠的教师网络来优化MIDN。具体来说，我们通过引入权重加权移动平均策略来获得这些教师网络。此外，我们还提出了一种类别特定的排名精炼算法，以利用多个 ensembled 教师网络的输出来精炼MIDN。这使得MIDN可以更好地将准确的提案与其邻近的提案分配高分数。这种方法的实验结果表明，我们的CBL框架在PASCAL VOC 2007 和 COCO datasets上显示出了superior的性能。代码将在https://github.com/Yinyf0804/WSOD-CBL/上公布。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Classification-of-Blood-Cell-Images-Using-Convolutional-Neural-Network"><a href="#Automatic-Classification-of-Blood-Cell-Images-Using-Convolutional-Neural-Network" class="headerlink" title="Automatic Classification of Blood Cell Images Using Convolutional Neural Network"></a>Automatic Classification of Blood Cell Images Using Convolutional Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06300">http://arxiv.org/abs/2308.06300</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rabia Asghar, Sanjay Kumar, Paul Hynds, Abeera Mahfooz<br>for: 这项研究的目的是自动分类血液细胞。methods: 这项研究使用了转移学习的 convolutional neural network (CNN) 模型，包括 VGG16、VGG19、ResNet-50、ResNet-101、ResNet-152、InceptionV3、MobileNetV2 和 DenseNet-20，并应用于 PBC 数据集的正常 DIB。results: 实验结果表明，提议的 CNN 模型在 PBC 数据集上达到了 99.91% 的准确率，与之前在文献中报道的结果相比，我们的提议的 convolutional neural network 模型在血液细胞分类方面表现竞争力强。<details>
<summary>Abstract</summary>
Human blood primarily comprises plasma, red blood cells, white blood cells, and platelets. It plays a vital role in transporting nutrients to different organs, where it stores essential health-related data about the human body. Blood cells are utilized to defend the body against diverse infections, including fungi, viruses, and bacteria. Hence, blood analysis can help physicians assess an individual's physiological condition. Blood cells have been sub-classified into eight groups: Neutrophils, eosinophils, basophils, lymphocytes, monocytes, immature granulocytes (promyelocytes, myelocytes, and metamyelocytes), erythroblasts, and platelets or thrombocytes on the basis of their nucleus, shape, and cytoplasm. Traditionally, pathologists and hematologists in laboratories have examined these blood cells using a microscope before manually classifying them. The manual approach is slower and more prone to human error. Therefore, it is essential to automate this process. In our paper, transfer learning with CNN pre-trained models. VGG16, VGG19, ResNet-50, ResNet-101, ResNet-152, InceptionV3, MobileNetV2, and DenseNet-20 applied to the PBC dataset's normal DIB. The overall accuracy achieved with these models lies between 91.375 and 94.72%. Hence, inspired by these pre-trained architectures, a model has been proposed to automatically classify the ten types of blood cells with increased accuracy. A novel CNN-based framework has been presented to improve accuracy. The proposed CNN model has been tested on the PBC dataset normal DIB. The outcomes of the experiments demonstrate that our CNN-based framework designed for blood cell classification attains an accuracy of 99.91% on the PBC dataset. Our proposed convolutional neural network model performs competitively when compared to earlier results reported in the literature.
</details>
<details>
<summary>摘要</summary>
人体血液主要由血浆、红细胞、白细胞和板块组成。它扮演着重要的Transportation和存储功能，帮助医生评估人体 physiological 状况。血液细胞可以用来防御身体各种感染，包括真菌、病毒和 бактери。因此，血液分析可以帮助医生评估个体的 physiological 状况。血液细胞可以分为八种类型：neutrophils、eosinophils、basophils、lymphocytes、monocytes、immature granulocytes（promyelocytes、myelocytes和metamyelocytes）、erythroblasts和板块或 trombocytes ，根据其核心、形态和 cytoplasm。在医学实验室中，传统上，病理学家和血液学家使用 Mikroskop  manually 分类这些血液细胞。这种手动方法 slower 和更容易出现人类错误。因此，需要自动化这个过程。在我们的论文中，我们使用了转移学习的 CNN 预训练模型。VGG16、VGG19、ResNet-50、ResNet-101、ResNet-152、InceptionV3、MobileNetV2和DenseNet-20 应用于 PBC 数据集的 normal DIB。模型的总准确率在这些模型之间处于 91.375% 和 94.72% 之间。因此，我们提出了一种基于 CNN 的框架，以提高准确率。我们提出的 CNN 模型在 PBC 数据集 normal DIB 上进行测试，实验结果表明，我们的 CNN 基于框架在 PBC 数据集上达到了 99.91% 的准确率。我们的提出的 convolutional neural network 模型与之前在文献中报道的结果相比，表现竞争力强。
</details></li>
</ul>
<hr>
<h2 id="MS3D-Ensemble-of-Experts-for-Multi-Source-Unsupervised-Domain-Adaption-in-3D-Object-Detection"><a href="#MS3D-Ensemble-of-Experts-for-Multi-Source-Unsupervised-Domain-Adaption-in-3D-Object-Detection" class="headerlink" title="MS3D++: Ensemble of Experts for Multi-Source Unsupervised Domain Adaption in 3D Object Detection"></a>MS3D++: Ensemble of Experts for Multi-Source Unsupervised Domain Adaption in 3D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05988">http://arxiv.org/abs/2308.05988</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/darrenjkt/ms3d">https://github.com/darrenjkt/ms3d</a></li>
<li>paper_authors: Darren Tsai, Julie Stephany Berrio, Mao Shan, Eduardo Nebot, Stewart Worrall</li>
<li>for:  addressed the issue of 3D detectors’ poor performance in unfamiliar domains due to domain gap</li>
<li>methods:  introduced MS3D++, a self-training framework for multi-source unsupervised domain adaptation in 3D object detection, which generates high-quality pseudo-labels and fuses predictions of an ensemble of multi-frame pre-trained detectors</li>
<li>results:  achieved state-of-the-art performance, comparable to training with human-annotated labels in Bird’s Eye View (BEV) evaluation for both low and high density lidar, on Waymo, nuScenes and Lyft datasets.<details>
<summary>Abstract</summary>
Deploying 3D detectors in unfamiliar domains has been demonstrated to result in a drastic drop of up to 70-90% in detection rate due to variations in lidar, geographical region, or weather conditions from their original training dataset. This domain gap leads to missing detections for densely observed objects, misaligned confidence scores, and increased high-confidence false positives, rendering the detector highly unreliable. To address this, we introduce MS3D++, a self-training framework for multi-source unsupervised domain adaptation in 3D object detection. MS3D++ provides a straightforward approach to domain adaptation by generating high-quality pseudo-labels, enabling the adaptation of 3D detectors to a diverse range of lidar types, regardless of their density. Our approach effectively fuses predictions of an ensemble of multi-frame pre-trained detectors from different source domains to improve domain generalization. We subsequently refine the predictions temporally to ensure temporal consistency in box localization and object classification. Furthermore, we present an in-depth study into the performance and idiosyncrasies of various 3D detector components in a cross-domain context, providing valuable insights for improved cross-domain detector ensembling. Experimental results on Waymo, nuScenes and Lyft demonstrate that detectors trained with MS3D++ pseudo-labels achieve state-of-the-art performance, comparable to training with human-annotated labels in Bird's Eye View (BEV) evaluation for both low and high density lidar.
</details>
<details>
<summary>摘要</summary>
部署3D探测器在不熟悉的领域中有示出大幅下降，达到70-90%的探测率下降，这是由于探测器的原始训练数据集和领域不同而导致的领域差距。这种领域差距会导致密集观测的对象丢失、信度不对的置信度和高置信度的假阳性，使探测器变得不可靠。为解决这问题，我们提出了MS3D++自动化框架，用于多源无监督领域适应3D对象探测。MS3D++提供了一种简单的适应方法，通过生成高质量的 Pseudo-标签来适应不同类型的雷达，无论其密度如何。我们的方法可以将多帧预训练探测器的预测结果集成，以提高领域通用性。然后，我们将预测结果进行时间推理，以确保盒子的时间一致性和物体的分类。此外，我们还进行了跨领域3D探测器组件的性能研究，提供了有价值的探测器组合研究指导。实验结果表明，使用MS3D++ Pseudo-标签训练的探测器在Waymo、nuScenes和Lyft上达到了状态之arte的性能，与人工标注的BEV评价相当，包括低密度和高密度雷达。
</details></li>
</ul>
<hr>
<h2 id="Zero-shot-Text-driven-Physically-Interpretable-Face-Editing"><a href="#Zero-shot-Text-driven-Physically-Interpretable-Face-Editing" class="headerlink" title="Zero-shot Text-driven Physically Interpretable Face Editing"></a>Zero-shot Text-driven Physically Interpretable Face Editing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05976">http://arxiv.org/abs/2308.05976</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yapeng Meng, Songru Yang, Xu Hu, Rui Zhao, Lincheng Li, Zhenwei Shi, Zhengxia Zou</li>
<li>for: 这篇论文提出了一种基于自由文本提示的新型面部编辑方法，以便在不同的文本提示下进行面部编辑。</li>
<li>methods: 这篇论文使用了一种新的vector流场模型来实现面部编辑，这种模型可以通过控制图像像素的坐标和颜色的偏移来实现面部编辑。</li>
<li>results:  compared with现有的文本驱动的面部编辑方法，这种方法可以生成高同一性和图像质量的面部编辑结果，并且可以在实时视频面部编辑中使用。<details>
<summary>Abstract</summary>
This paper proposes a novel and physically interpretable method for face editing based on arbitrary text prompts. Different from previous GAN-inversion-based face editing methods that manipulate the latent space of GANs, or diffusion-based methods that model image manipulation as a reverse diffusion process, we regard the face editing process as imposing vector flow fields on face images, representing the offset of spatial coordinates and color for each image pixel. Under the above-proposed paradigm, we represent the vector flow field in two ways: 1) explicitly represent the flow vectors with rasterized tensors, and 2) implicitly parameterize the flow vectors as continuous, smooth, and resolution-agnostic neural fields, by leveraging the recent advances of implicit neural representations. The flow vectors are iteratively optimized under the guidance of the pre-trained Contrastive Language-Image Pretraining~(CLIP) model by maximizing the correlation between the edited image and the text prompt. We also propose a learning-based one-shot face editing framework, which is fast and adaptable to any text prompt input. Our method can also be flexibly extended to real-time video face editing. Compared with state-of-the-art text-driven face editing methods, our method can generate physically interpretable face editing results with high identity consistency and image quality. Our code will be made publicly available.
</details>
<details>
<summary>摘要</summary>
In our proposed paradigm, we represent the vector flow field in two ways:1. Explicitly represent the flow vectors with rasterized tensors.2. Implicitly parameterize the flow vectors as continuous, smooth, and resolution-agnostic neural fields by leveraging recent advances in implicit neural representations.The flow vectors are iteratively optimized under the guidance of the pre-trained Contrastive Language-Image Pretraining (CLIP) model by maximizing the correlation between the edited image and the text prompt. We also propose a learning-based one-shot face editing framework, which is fast and adaptable to any text prompt input. Our method can also be flexibly extended to real-time video face editing.Compared with state-of-the-art text-driven face editing methods, our method can generate physically interpretable face editing results with high identity consistency and image quality. Our code will be made publicly available.
</details></li>
</ul>
<hr>
<h2 id="Focused-Specific-Objects-NeRF"><a href="#Focused-Specific-Objects-NeRF" class="headerlink" title="Focused Specific Objects NeRF"></a>Focused Specific Objects NeRF</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05970">http://arxiv.org/abs/2308.05970</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuesong Li, Feng Pan, Helong Yan, Xiuli Xin, Xiaoxue Feng</li>
<li>for: 提高NeRF模型的快速训练和高质量渲染，适用于复杂场景。</li>
<li>methods: 利用场景 semantic priors 提高快速训练，使网络只关注特定目标而不受背景影响。 可以提高训练速度7.78倍，并且更快地渲染小至中型目标。 此外，这种改进适用于所有NeRF模型。</li>
<li>results: 通过弱监督和粗粒抽象采样，进一步加速训练并保持渲染质量。 此外，提出了新的场景编辑技术，可以实现特定Semantic targets的独特显示或掩蔽渲染。 解决不supervised区域错误推理问题，我们还设计了一个自动化 loops，结合形态运算和聚类。<details>
<summary>Abstract</summary>
Most NeRF-based models are designed for learning the entire scene, and complex scenes can lead to longer learning times and poorer rendering effects. This paper utilizes scene semantic priors to make improvements in fast training, allowing the network to focus on the specific targets and not be affected by complex backgrounds. The training speed can be increased by 7.78 times with better rendering effect, and small to medium sized targets can be rendered faster. In addition, this improvement applies to all NeRF-based models. Considering the inherent multi-view consistency and smoothness of NeRF, this paper also studies weak supervision by sparsely sampling negative ray samples. With this method, training can be further accelerated and rendering quality can be maintained. Finally, this paper extends pixel semantic and color rendering formulas and proposes a new scene editing technique that can achieve unique displays of the specific semantic targets or masking them in rendering. To address the problem of unsupervised regions incorrect inferences in the scene, we also designed a self-supervised loop that combines morphological operations and clustering.
</details>
<details>
<summary>摘要</summary>
大多数NeRF基于模型是为整个场景学习，复杂的场景可能会导致更长的学习时间和更差的渲染效果。这篇论文利用场景 semantic 预测来进行改进快速训练，使网络只关注特定的目标而不受背景的影响。通过这种方法，训练速度可以提高7.78倍，并且小到中等大小的目标可以更快地渲染。此外，这种改进还适用于所有NeRF基于模型。由于NeRF的内置多视图一致性和平滑性，这篇论文还研究了稀疏采样负方向样本的弱监督学习。通过这种方法，训练可以进一步加速，并且渲染质量可以保持。最后，这篇论文扩展像素semantic和颜色渲染公式，并提出了一种新的场景编辑技术，可以实现特定semantictarget的唯一显示或隐藏其在渲染中。为了解决场景中无监督区域的错误推断问题，我们还设计了一种自动化环节，结合形态运算和分群。
</details></li>
</ul>
<hr>
<h2 id="YOLOrtho-–-A-Unified-Framework-for-Teeth-Enumeration-and-Dental-Disease-Detection"><a href="#YOLOrtho-–-A-Unified-Framework-for-Teeth-Enumeration-and-Dental-Disease-Detection" class="headerlink" title="YOLOrtho – A Unified Framework for Teeth Enumeration and Dental Disease Detection"></a>YOLOrtho – A Unified Framework for Teeth Enumeration and Dental Disease Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05967">http://arxiv.org/abs/2308.05967</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shenxiao Mei, Chenglong Ma, Feihong Shen, Huikai Wu</li>
<li>For: 本研究的目的是开发一种结合了牙齿数量和牙疾病识别的综合框架，以提高牙医的诊断效率和准确率。* Methods: 我们采用了一种基于CoordConv的模型结构，并在模型中插入了一个更多的 upsampling layer，以更好地利用数据并同时学习牙齿检测和疾病识别。* Results: 我们的模型在实验中得到了较大的扩散模型的更好的效果，并且可以准确地识别牙齿和牙疾病。<details>
<summary>Abstract</summary>
Detecting dental diseases through panoramic X-rays images is a standard procedure for dentists. Normally, a dentist need to identify diseases and find the infected teeth. While numerous machine learning models adopting this two-step procedure have been developed, there has not been an end-to-end model that can identify teeth and their associated diseases at the same time. To fill the gap, we develop YOLOrtho, a unified framework for teeth enumeration and dental disease detection. We develop our model on Dentex Challenge 2023 data, which consists of three distinct types of annotated data. The first part is labeled with quadrant, and the second part is labeled with quadrant and enumeration and the third part is labeled with quadrant, enumeration and disease. To further improve detection, we make use of Tufts Dental public dataset. To fully utilize the data and learn both teeth detection and disease identification simultaneously, we formulate diseases as attributes attached to their corresponding teeth. Due to the nature of position relation in teeth enumeration, We replace convolution layer with CoordConv in our model to provide more position information for the model. We also adjust the model architecture and insert one more upsampling layer in FPN in favor of large object detection. Finally, we propose a post-process strategy for teeth layout that corrects teeth enumeration based on linear sum assignment. Results from experiments show that our model exceeds large Diffusion-based model.
</details>
<details>
<summary>摘要</summary>
检测牙科疾病通过扫描影像是 dentist 的标准程序。通常， dentist 需要识别疾病并找到感染的牙齿。虽然许多机器学习模型采用了这两步过程，但没有一个端到端模型可以同时识别牙齿和其相关疾病。为填补这一空白，我们开发了 YOLOrtho，一个综合框架 для牙齿编号和牙科疾病检测。我们在 Dentex Challenge 2023 数据集上验证我们的模型，该数据集包括三种不同的注释数据。第一部分是标注了 quadrant，第二部分是标注了 quadrant 和编号，第三部分是标注了 quadrant、编号和疾病。为了进一步提高检测精度，我们使用 Tufts Dental 公共数据集。为了充分利用数据并同时学习牙齿检测和疾病识别，我们将疾病视为牙齿上的特征，并将疾病分别附加到它们所对应的牙齿上。由于牙齿编号的位置关系，我们将卷积层替换为 CoordConv，以提供更多的位置信息 для模型。我们还调整模型结构，并在 FPN 中添加一个更多的膨敛层，以便更好地检测大对象。最后，我们提出了一种采用线性归一化的牙齿布局修正策略，以修正牙齿编号。实验结果显示，我们的模型超越了大 diffusion-based 模型。
</details></li>
</ul>
<hr>
<h2 id="Compositional-Learning-in-Transformer-Based-Human-Object-Interaction-Detection"><a href="#Compositional-Learning-in-Transformer-Based-Human-Object-Interaction-Detection" class="headerlink" title="Compositional Learning in Transformer-Based Human-Object Interaction Detection"></a>Compositional Learning in Transformer-Based Human-Object Interaction Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05961">http://arxiv.org/abs/2308.05961</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zikun Zhuang, Ruihao Qian, Chi Xie, Shuang Liang</li>
<li>for: 本研究旨在解决人机对象交互（HOI）检测中长尾分布的问题，通过启发式学习和组合学习来提高HOI检测的性能。</li>
<li>methods: 我们提出了一种基于 transformer 框架的组合 HOI 学习方法，利用人物对象对的表示和交互表示在不同 HOI 实例中进行重新组合，以获得更加 ricther 的上下文信息和更好的知识泛化。</li>
<li>results: 我们的简单 yet effective 方法在 experiments 中达到了领先的性能水平，特别是在 rare HOI 类中表现出色。<details>
<summary>Abstract</summary>
Human-object interaction (HOI) detection is an important part of understanding human activities and visual scenes. The long-tailed distribution of labeled instances is a primary challenge in HOI detection, promoting research in few-shot and zero-shot learning. Inspired by the combinatorial nature of HOI triplets, some existing approaches adopt the idea of compositional learning, in which object and action features are learned individually and re-composed as new training samples. However, these methods follow the CNN-based two-stage paradigm with limited feature extraction ability, and often rely on auxiliary information for better performance. Without introducing any additional information, we creatively propose a transformer-based framework for compositional HOI learning. Human-object pair representations and interaction representations are re-composed across different HOI instances, which involves richer contextual information and promotes the generalization of knowledge. Experiments show our simple but effective method achieves state-of-the-art performance, especially on rare HOI classes.
</details>
<details>
<summary>摘要</summary>
人机物交互（HOI）检测是理解人类活动和视觉场景的重要组成部分。长板分布的标注实例是HOI检测的主要挑战，促进了几个shot和零shot学习的研究。以HOI三元组的 combinatorial 性为 inspiration，一些现有的方法采用了组合学习的想法，在Object和动作特征上学习并重新组合为新的训练样本。然而，这些方法通常采用了基于CNN的两Stage paradigm，具有有限的特征提取能力，并经常利用辅助信息来提高性能。而无需任何额外信息，我们创新地提出了基于 transformer 框架的 HOI 学习方法。人机对象对的表示和交互表示在不同的 HOI 实例中被重新组合，这里包含了更加丰富的上下文信息，提高了知识的通用性。实验表明，我们简单 yet effective 的方法可以达到领先的性能，特别是在罕见 HOI 类上。
</details></li>
</ul>
<hr>
<h2 id="Classification-of-White-Blood-Cells-Using-Machine-and-Deep-Learning-Models-A-Systematic-Review"><a href="#Classification-of-White-Blood-Cells-Using-Machine-and-Deep-Learning-Models-A-Systematic-Review" class="headerlink" title="Classification of White Blood Cells Using Machine and Deep Learning Models: A Systematic Review"></a>Classification of White Blood Cells Using Machine and Deep Learning Models: A Systematic Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06296">http://arxiv.org/abs/2308.06296</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rabia Asghar, Sanjay Kumar, Paul Hynds, Arslan Shaukat</li>
<li>for: 这种研究的目的是帮助改善医疗图像分析领域中白细胞类型分类的精度。</li>
<li>methods: 这种研究使用了现代的机器学习（ML）和深度学习（DL）技术，包括血液图像、MRI、X射线等医疗图像领域的数据。</li>
<li>results: 研究发现，随着近年来ML和DL技术的不断发展和应用，白细胞类型分类的精度得到了显著改善，但还存在一些挑战，如数据集的可用性和医疗人员的培训等。<details>
<summary>Abstract</summary>
Machine learning (ML) and deep learning (DL) models have been employed to significantly improve analyses of medical imagery, with these approaches used to enhance the accuracy of prediction and classification. Model predictions and classifications assist diagnoses of various cancers and tumors. This review presents an in-depth analysis of modern techniques applied within the domain of medical image analysis for white blood cell classification. The methodologies that use blood smear images, magnetic resonance imaging (MRI), X-rays, and similar medical imaging domains are identified and discussed, with a detailed analysis of ML/DL techniques applied to the classification of white blood cells (WBCs) representing the primary focus of the review. The data utilized in this research has been extracted from a collection of 136 primary papers that were published between the years 2006 and 2023. The most widely used techniques and best-performing white blood cell classification methods are identified. While the use of ML and DL for white blood cell classification has concurrently increased and improved in recent year, significant challenges remain - 1) Availability of appropriate datasets remain the primary challenge, and may be resolved using data augmentation techniques. 2) Medical training of researchers is recommended to improve current understanding of white blood cell structure and subsequent selection of appropriate classification models. 3) Advanced DL networks including Generative Adversarial Networks, R-CNN, Fast R-CNN, and faster R-CNN will likely be increasingly employed to supplement or replace current techniques.
</details>
<details>
<summary>摘要</summary>
医学影像分析（ML）和深度学习（DL）模型已经广泛应用于医学影像分析中，以提高预测和分类的准确性。这些模型的预测和分类帮助诊断各种恶性肿瘤。本文归纳了现代医学影像分析领域中的现代技术，并详细分析了用于白血球类别的ML/DL技术。研究使用的数据来自于2006年至2023年发表的136篇原始论文。最常用的技术和最佳白血球类别方法被标出。虽然在最近几年，用于白血球类别的ML和DL技术的使用和改进得到了普遍应用，但是还存在一些挑战，包括：1）获得适当数据集是最主要的挑战，可以通过数据增强技术解决。2）医学研究人员的培训可以提高现代白血球结构的理解，并选择适当的类别模型。3）将来，高级的DL网络，如生成对抗网络、R-CNN、快速R-CNN和更快的R-CNN将可能被广泛应用，以补充或取代当前的技术。
</details></li>
</ul>
<hr>
<h2 id="Learned-Point-Cloud-Compression-for-Classification"><a href="#Learned-Point-Cloud-Compression-for-Classification" class="headerlink" title="Learned Point Cloud Compression for Classification"></a>Learned Point Cloud Compression for Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05959">http://arxiv.org/abs/2308.05959</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/multimedialabsfu/learned-point-cloud-compression-for-classification">https://github.com/multimedialabsfu/learned-point-cloud-compression-for-classification</a></li>
<li>paper_authors: Mateen Ulhaq, Ivan V. Bajić</li>
<li>for: 这个论文是为了提出一种特殊的点云编码器，用于在服务器端进行深度学习机器视觉任务的点云数据传输和处理。</li>
<li>methods: 该编码器基于PointNet，并且实现了一个高度特殊的点云编码方法，以实现更好的环境-质量负担比。</li>
<li>results: 相比于非特殊编码器，该编码器在ModelNet40数据集上可以实现94%的BD-比特率减少，同时保持高度的准确率。此外，对于低资源的终端设备，我们还提出了两种轻量级的编码器配置，可以实现相似的BD-比特率减少，同时具有较低的顶部1准确率下降和较低的编码器端kMACs&#x2F;点。<details>
<summary>Abstract</summary>
Deep learning is increasingly being used to perform machine vision tasks such as classification, object detection, and segmentation on 3D point cloud data. However, deep learning inference is computationally expensive. The limited computational capabilities of end devices thus necessitate a codec for transmitting point cloud data over the network for server-side processing. Such a codec must be lightweight and capable of achieving high compression ratios without sacrificing accuracy. Motivated by this, we present a novel point cloud codec that is highly specialized for the machine task of classification. Our codec, based on PointNet, achieves a significantly better rate-accuracy trade-off in comparison to alternative methods. In particular, it achieves a 94% reduction in BD-bitrate over non-specialized codecs on the ModelNet40 dataset. For low-resource end devices, we also propose two lightweight configurations of our encoder that achieve similar BD-bitrate reductions of 93% and 92% with 3% and 5% drops in top-1 accuracy, while consuming only 0.470 and 0.048 encoder-side kMACs/point, respectively. Our codec demonstrates the potential of specialized codecs for machine analysis of point clouds, and provides a basis for extension to more complex tasks and datasets in the future.
</details>
<details>
<summary>摘要</summary>
深度学习在进行三维点云数据的机器视觉任务中变得越来越普遍，如分类、物体检测和分割。然而，深度学习推理是计算昂贵的。因此，为了将点云数据传输到服务器进行处理，需要一个轻量级的编码器。这个编码器必须具有高度压缩比和低计算成本，而不是牺牲准确性。驱动了这一点，我们提出了一种特化于机器分类任务的点云编码器。我们的编码器基于PointNet，与其他方法相比，实现了显著更好的速率准确性质量比。具体来说，在ModelNet40数据集上，我们的编码器可以达到94%的BD-比特率减少，而且只需0.470和0.048 encoder-side kMACs/点。为低资源的端device，我们还提出了两种轻量级的编码器配置，可以实现相同的BD-比特率减少，分别为93%和92%，而且只需3%和5%的顶部一个精度下降，并且只需0.470和0.048 encoder-side kMACs/点。我们的编码器表明特化编码器在机器分析点云数据方面的潜力，并提供了未来扩展到更复杂的任务和数据集的基础。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-Aware-Cross-Modal-Transfer-Network-for-Sketch-Based-3D-Shape-Retrieval"><a href="#Uncertainty-Aware-Cross-Modal-Transfer-Network-for-Sketch-Based-3D-Shape-Retrieval" class="headerlink" title="Uncertainty-Aware Cross-Modal Transfer Network for Sketch-Based 3D Shape Retrieval"></a>Uncertainty-Aware Cross-Modal Transfer Network for Sketch-Based 3D Shape Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05948">http://arxiv.org/abs/2308.05948</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiyang Cai, Jiaming Lu, Jiewen Wang, Shuang Liang</li>
<li>for: 本研究旨在解决粗糙和噪声存在的手绘图像数据中的低质量样本问题，提高三维形状检索的精度。</li>
<li>methods: 本研究提出了一种不确定性意识的跨Modal传输网络（UACTN），它将手绘图像和三维形状的表示学分解为两个独立任务：手绘图像的分类学习和三维形状的特征传输。首先，我们提出了一种端到端的分类方法，同时学习手绘图像的特征和不确定性，以便通过不同水平的不确定性来防止噪声绘图的抖抖。然后，三维形状的特征被映射到预学习的手绘图像空间中，进行特征对齐。</li>
<li>results: 对两个标准 benchmark进行了广泛的实验和剖析研究，证明了我们提出的方法在比前STATE-OF-THE-ART方法更高的精度和稳定性。<details>
<summary>Abstract</summary>
In recent years, sketch-based 3D shape retrieval has attracted growing attention. While many previous studies have focused on cross-modal matching between hand-drawn sketches and 3D shapes, the critical issue of how to handle low-quality and noisy samples in sketch data has been largely neglected. This paper presents an uncertainty-aware cross-modal transfer network (UACTN) that addresses this issue. UACTN decouples the representation learning of sketches and 3D shapes into two separate tasks: classification-based sketch uncertainty learning and 3D shape feature transfer. We first introduce an end-to-end classification-based approach that simultaneously learns sketch features and uncertainty, allowing uncertainty to prevent overfitting noisy sketches by assigning different levels of importance to clean and noisy sketches. Then, 3D shape features are mapped into the pre-learned sketch embedding space for feature alignment. Extensive experiments and ablation studies on two benchmarks demonstrate the superiority of our proposed method compared to state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
在最近几年，基于绘制的3D形状检索已经吸引了越来越多的关注。许多先前的研究都集中在手绘素描和3D形状之间的跨模态匹配上，但是对于绘制数据中噪声和低质量样本的处理问题一直受到了很大的忽略。这篇论文提出了一种不确定性意识的跨模态传输网络（UACTN），解决这个问题。UACTN将绘制素描和3D形状的表示学分解为两个独立的任务：基于绘制素描的分类学习和3D形状特征传输。我们首先介绍了一种端到端的分类基本方法，同时学习绘制素描和不确定性，使不确定性防止噪声绘制素描中的过拟合。然后，3D形状特征被映射到预先学习的绘制元素空间中进行特征对齐。广泛的实验和减少研究在两个标准 bench 上表明了我们提出的方法的优越性。
</details></li>
</ul>
<hr>
<h2 id="Generalizing-Event-Based-Motion-Deblurring-in-Real-World-Scenarios"><a href="#Generalizing-Event-Based-Motion-Deblurring-in-Real-World-Scenarios" class="headerlink" title="Generalizing Event-Based Motion Deblurring in Real-World Scenarios"></a>Generalizing Event-Based Motion Deblurring in Real-World Scenarios</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05932">http://arxiv.org/abs/2308.05932</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xiangz-0/gem">https://github.com/xiangz-0/gem</a></li>
<li>paper_authors: Xiang Zhang, Lei Yu, Wen Yang, Jianzhuang Liu, Gui-Song Xia</li>
<li>for: 这篇论文旨在普遍化事件基 Motion deblurring 性能，并适应实际场景中的不同空间和时间尺度的运动模糊。</li>
<li>methods: 该论文提出了一种缩放意识网络，可以自适应输入空间Scale和学习不同的时间尺度运动模糊。并提出了一种两个阶段自我超视训练方案，以适应实际数据分布。</li>
<li>results: 该论文的方法可以高效地还原隐藏的图像的亮度和结构，并通过自适应学习来普遍化运动模糊处理的性能，以适应实际场景中的不同空间和时间尺度的运动模糊。<details>
<summary>Abstract</summary>
Event-based motion deblurring has shown promising results by exploiting low-latency events. However, current approaches are limited in their practical usage, as they assume the same spatial resolution of inputs and specific blurriness distributions. This work addresses these limitations and aims to generalize the performance of event-based deblurring in real-world scenarios. We propose a scale-aware network that allows flexible input spatial scales and enables learning from different temporal scales of motion blur. A two-stage self-supervised learning scheme is then developed to fit real-world data distribution. By utilizing the relativity of blurriness, our approach efficiently ensures the restored brightness and structure of latent images and further generalizes deblurring performance to handle varying spatial and temporal scales of motion blur in a self-distillation manner. Our method is extensively evaluated, demonstrating remarkable performance, and we also introduce a real-world dataset consisting of multi-scale blurry frames and events to facilitate research in event-based deblurring.
</details>
<details>
<summary>摘要</summary>
Event-based motion deblurring 已经显示出了有前途的结果，通过利用低延迟事件。然而，现有的方法受到一些限制，因为它们假设输入的空间分辨率和特定的模糊分布相同。这项工作解决了这些限制，并寻求将事件基于的模糊除掉应用于实际场景中。我们提议一种可灵活输入空间比例的缩放网络，以及一种两阶段自我超vised学习方案，以适应实际数据分布。通过利用模糊程度的相对性，我们的方法能够有效地保持原始的明亮和结构，并在自我滤清过程中广泛应用。我们的方法在评价中表现出色，并且我们还介绍了一个包含多个滤镜速度和事件的实际数据集，以便进行事件基于的模糊除掉研究。
</details></li>
</ul>
<hr>
<h2 id="CaPhy-Capturing-Physical-Properties-for-Animatable-Human-Avatars"><a href="#CaPhy-Capturing-Physical-Properties-for-Animatable-Human-Avatars" class="headerlink" title="CaPhy: Capturing Physical Properties for Animatable Human Avatars"></a>CaPhy: Capturing Physical Properties for Animatable Human Avatars</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05925">http://arxiv.org/abs/2308.05925</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaoqi Su, Liangxiao Hu, Siyou Lin, Hongwen Zhang, Shengping Zhang, Justus Thies, Yebin Liu</li>
<li>for:  reconstruction of animatable human avatars with realistic dynamic properties for clothing</li>
<li>methods: combination of unsupervised training with physics-based losses and 3D-supervised training using scanned data, optimization of physical parameters using gradient constraints</li>
<li>results: ability to generalize to novel poses with realistic dynamic cloth deformations, superior quantitative and qualitative results compared with previous methodsHere’s the text in Simplified Chinese:</li>
<li>for: 创建有动态衣物特性的可动人形模型</li>
<li>methods: 组合不监督学习和物理损失，以及3D监督学习使用扫描数据，并对物理参数进行梯度约束优化</li>
<li>results: 能够扩展到新的姿势，并实现真实的动态布料扭曲和皱纹效果，与之前的方法相比有较高的量化和质量效果<details>
<summary>Abstract</summary>
We present CaPhy, a novel method for reconstructing animatable human avatars with realistic dynamic properties for clothing. Specifically, we aim for capturing the geometric and physical properties of the clothing from real observations. This allows us to apply novel poses to the human avatar with physically correct deformations and wrinkles of the clothing. To this end, we combine unsupervised training with physics-based losses and 3D-supervised training using scanned data to reconstruct a dynamic model of clothing that is physically realistic and conforms to the human scans. We also optimize the physical parameters of the underlying physical model from the scans by introducing gradient constraints of the physics-based losses. In contrast to previous work on 3D avatar reconstruction, our method is able to generalize to novel poses with realistic dynamic cloth deformations. Experiments on several subjects demonstrate that our method can estimate the physical properties of the garments, resulting in superior quantitative and qualitative results compared with previous methods.
</details>
<details>
<summary>摘要</summary>
我们提出了CaPhy方法，一种新的人工智能方法，用于重建具有真实动态性质的人工人体模型。我们的目标是从实际观察中捕捉人体服装的几何和物理性质。这使得我们可以将人工人体模型应用到新的姿势上，并且通过物理正确的扭曲和皱纹来模拟人体的动态行为。为此，我们结合无监督训练、物理学损失和3D监督训练使用扫描数据来重建一个物理realistic的服装动态模型。此外，我们还通过引入物理损失的梯度约束来优化物理参数。与前一代3D人体重建方法不同，我们的方法能够泛化到新的姿势上，并且能够模拟真实的动态皮肤扭曲。我们在多个主题上进行了实验，并证明了我们的方法可以估计人体服装的物理参数，从而实现了较好的量化和质量上的效果。
</details></li>
</ul>
<hr>
<h2 id="BATINet-Background-Aware-Text-to-Image-Synthesis-and-Manipulation-Network"><a href="#BATINet-Background-Aware-Text-to-Image-Synthesis-and-Manipulation-Network" class="headerlink" title="BATINet: Background-Aware Text to Image Synthesis and Manipulation Network"></a>BATINet: Background-Aware Text to Image Synthesis and Manipulation Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05921">http://arxiv.org/abs/2308.05921</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryugo Morita, Zhiqiang Zhang, Jinjia Zhou</li>
<li>for: 本研究旨在生成文本描述的背景图像中的前景内容，以匹配输入背景图像的样式。</li>
<li>methods: 该研究提出了一种 Background-Aware Text to Image synthesis and manipulation Network (BATINet)，包括两个关键组件：位置探测网络 (PDN) 和融合网络 (HN)。PDN 检测文本相关对象在背景图像中最有可能的位置，而 HN 使得生成的内容与背景样式信息融合。</li>
<li>results: 该研究通过对 CUB 数据集进行质量和量化评估，表明了该模型比其他状态 искусственный方法更高效。此外，该模型还可以应用于文本指导图像修改任务，解决最大化对象形状的修改任务。<details>
<summary>Abstract</summary>
Background-Induced Text2Image (BIT2I) aims to generate foreground content according to the text on the given background image. Most studies focus on generating high-quality foreground content, although they ignore the relationship between the two contents. In this study, we analyzed a novel Background-Aware Text2Image (BAT2I) task in which the generated content matches the input background. We proposed a Background-Aware Text to Image synthesis and manipulation Network (BATINet), which contains two key components: Position Detect Network (PDN) and Harmonize Network (HN). The PDN detects the most plausible position of the text-relevant object in the background image. The HN harmonizes the generated content referring to background style information. Finally, we reconstructed the generation network, which consists of the multi-GAN and attention module to match more user preferences. Moreover, we can apply BATINet to text-guided image manipulation. It solves the most challenging task of manipulating the shape of an object. We demonstrated through qualitative and quantitative evaluations on the CUB dataset that the proposed model outperforms other state-of-the-art methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Semantics2Hands-Transferring-Hand-Motion-Semantics-between-Avatars"><a href="#Semantics2Hands-Transferring-Hand-Motion-Semantics-between-Avatars" class="headerlink" title="Semantics2Hands: Transferring Hand Motion Semantics between Avatars"></a>Semantics2Hands: Transferring Hand Motion Semantics between Avatars</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05920">http://arxiv.org/abs/2308.05920</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abcyzj/Semantics2Hands">https://github.com/abcyzj/Semantics2Hands</a></li>
<li>paper_authors: Zijie Ye, Jia Jia, Junliang Xing</li>
<li>for: 这篇论文主要目标是在不同的人工手模型之间传递手势semantics。</li>
<li>methods: 作者提出了一种基于解剖学semantic matrix（ASM）的方法，通过定量地编码手势semantics，实现精准地重定向手势。然后，他们使用一种基于解剖学semantics重建网络（ASRN）来从源ASM到目标手关节弯曲的映射。</li>
<li>results: 作者通过在同频和交叉频段的手势重定向任务中评估了他们的方法，并证明了其在质量和量化上与当前状态OF THE ARTS具有显著优势。<details>
<summary>Abstract</summary>
Human hands, the primary means of non-verbal communication, convey intricate semantics in various scenarios. Due to the high sensitivity of individuals to hand motions, even minor errors in hand motions can significantly impact the user experience. Real applications often involve multiple avatars with varying hand shapes, highlighting the importance of maintaining the intricate semantics of hand motions across the avatars. Therefore, this paper aims to transfer the hand motion semantics between diverse avatars based on their respective hand models. To address this problem, we introduce a novel anatomy-based semantic matrix (ASM) that encodes the semantics of hand motions. The ASM quantifies the positions of the palm and other joints relative to the local frame of the corresponding joint, enabling precise retargeting of hand motions. Subsequently, we obtain a mapping function from the source ASM to the target hand joint rotations by employing an anatomy-based semantics reconstruction network (ASRN). We train the ASRN using a semi-supervised learning strategy on the Mixamo and InterHand2.6M datasets. We evaluate our method in intra-domain and cross-domain hand motion retargeting tasks. The qualitative and quantitative results demonstrate the significant superiority of our ASRN over the state-of-the-arts.
</details>
<details>
<summary>摘要</summary>
人类手部，非语言交流的主要手段，在多种场景中传递细腻的 semantics。由于人类对手势的敏感性强，even slight errors in hand motions can significantly impact the user experience。实际应用中常有多个化身，各自的手形不同，因此维护手势 semantics的细腻性在多个化身之间是非常重要的。因此，这篇论文旨在将多个化身中的手势 semantics 转移到彼此的手 JOINTS 模型上。为解决这个问题，我们提出了一种新的 анатомиче基于的semantic matrix (ASM)，该矩阵编码了手势 semantics。ASM 量化了手部和其他关节的位置 relative to the local frame of the corresponding joint，使得精准地重定向手势。然后，我们获得了一个从源 ASM 到目标手关节旋转的映射函数，通过使用一种基于 анатомиче基的semantics reconstruction network (ASRN)来实现。我们使用一种半监督学习策略在 Mixamo 和 InterHand2.6M 数据集上训练 ASRN。我们对 intra-domain 和 cross-domain 手势重定向任务进行评估。结果表明，我们的 ASRN 在 Qualitative 和量化方面具有显著的优势，胜过当前状态艺的。
</details></li>
</ul>
<hr>
<h2 id="Collaborative-Tracking-Learning-for-Frame-Rate-Insensitive-Multi-Object-Tracking"><a href="#Collaborative-Tracking-Learning-for-Frame-Rate-Insensitive-Multi-Object-Tracking" class="headerlink" title="Collaborative Tracking Learning for Frame-Rate-Insensitive Multi-Object Tracking"></a>Collaborative Tracking Learning for Frame-Rate-Insensitive Multi-Object Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05911">http://arxiv.org/abs/2308.05911</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiheng Liu, Junta Wu, Yi Fu</li>
<li>for: 提高edge设备的计算、存储和功耗成本，实现高效的多目标跟踪（MOT）</li>
<li>methods: 提出了一种搜索式跟踪学习（ColTrack）方法，通过多个历史查询来共同跟踪目标，并在每两个temporal块解码器之间插入信息级别融合模块，以更好地融合时间特征。此外，还提出了跟踪对象一致损失函数，以引导历史查询之间的交互。</li>
<li>results: 在高帧率视频中，ColTrack比 state-of-the-art 方法在大规模 datasets Dancetrack 和 BDD100K 上表现出更高的性能，并超过了现有的端到端方法在 MOT17 上的表现。此外，ColTrack 在低帧率视频中也具有明显的优势，可以降低帧率要求，同时保持更高的性能，从而实现更快的处理速度。<details>
<summary>Abstract</summary>
Multi-object tracking (MOT) at low frame rates can reduce computational, storage and power overhead to better meet the constraints of edge devices. Many existing MOT methods suffer from significant performance degradation in low-frame-rate videos due to significant location and appearance changes between adjacent frames. To this end, we propose to explore collaborative tracking learning (ColTrack) for frame-rate-insensitive MOT in a query-based end-to-end manner. Multiple historical queries of the same target jointly track it with richer temporal descriptions. Meanwhile, we insert an information refinement module between every two temporal blocking decoders to better fuse temporal clues and refine features. Moreover, a tracking object consistency loss is proposed to guide the interaction between historical queries. Extensive experimental results demonstrate that in high-frame-rate videos, ColTrack obtains higher performance than state-of-the-art methods on large-scale datasets Dancetrack and BDD100K, and outperforms the existing end-to-end methods on MOT17. More importantly, ColTrack has a significant advantage over state-of-the-art methods in low-frame-rate videos, which allows it to obtain faster processing speeds by reducing frame-rate requirements while maintaining higher performance. Code will be released at https://github.com/yolomax/ColTrack
</details>
<details>
<summary>摘要</summary>
多bject tracking（MOT）在低帧率下可以降低计算、存储和功能开销，以更好地满足边缘设备的限制。许多现有的MOT方法在低帧率视频中表现不佳，因为 между邻帧的位置和外观改变很大。为此，我们提出了协同跟踪学习（ColTrack），用于极低帧率下的框架缺失感知MOT。多个历史查询在同一个目标上共同跟踪，使用更 ricoh 的时间描述。此外，我们在每两个时间块解码器之间插入信息精细模块，以更好地融合时间断言和细化特征。此外，我们还提出了跟踪对象一致损失，以引导历史查询之间的互动。实验结果表明，在高帧率视频中，ColTrack可以与现有的方法匹配或超越其性能，并在MOT17上跟踪对象的框架缺失下表现更好。此外，ColTrack在低帧率视频中表现更优于现有的方法，可以降低帧率要求，同时保持高性能。代码将在https://github.com/yolomax/ColTrack上发布。
</details></li>
</ul>
<hr>
<h2 id="Semantic-embedded-Similarity-Prototype-for-Scene-Recognition"><a href="#Semantic-embedded-Similarity-Prototype-for-Scene-Recognition" class="headerlink" title="Semantic-embedded Similarity Prototype for Scene Recognition"></a>Semantic-embedded Similarity Prototype for Scene Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05896">http://arxiv.org/abs/2308.05896</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chuanxin Song, Hanbo Wu, Xin Ma</li>
<li>for: 提高Scene recognition的准确性，不增加网络参数</li>
<li>methods: 使用统计策略描述Scene中的semantic知识，并利用这些知识来构建一个相似性原型，以支持网络训练</li>
<li>results: 对多个benchmark进行了全面评估，并确认了该相似性原型可以提高现有网络的性能，无需增加计算负担<details>
<summary>Abstract</summary>
Due to the high inter-class similarity caused by the complex composition within scenes and the co-existing objects across scenes, various studies have explored object semantic knowledge within scenes to improve scene recognition. However, a resulting issue arises as semantic segmentation or object detection techniques demand heavy computational power, thereby burdening the network considerably. This limitation often renders object-assisted approaches incompatible with edge devices. In contrast, this paper proposes a semantic-based similarity prototype that assists the scene recognition network to achieve higher accuracy without increasing network parameters. It is simple and can be plug-and-played into existing pipelines. More specifically, a statistical strategy is introduced to depict semantic knowledge in scenes as class-level semantic representations. These representations are utilized to explore inter-class correlations, ultimately constructing a similarity prototype. Furthermore, we propose two ways to use the similarity prototype to support network training from the perspective of gradient label softening and batch-level contrastive loss, respectively. Comprehensive evaluations on multiple benchmarks show that our similarity prototype enhances the performance of existing networks without adding any computational burden. Code and the statistical similarity prototype will be available soon.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)由于场景中物体的复杂composición和场景之间的对象协同存在高度的类间相似性，许多研究已经探索了场景内物体的semantic知识，以提高场景认知。然而，这些方法通常需要大量的计算能力，对网络造成沉重的负担。这限制了对象辅助方法在边缘设备上的应用。相比之下，这篇论文提出了一种基于semantic的相似性原型，可以帮助场景认知网络提高准确性，而不需增加网络参数。它简单，可以与现有管道一起使用。更具体地，我们引入了一种统计策略，将场景中的semantic知识映射到类级semantic表示上。这些表示可以探索场景中类之间的相似性，最终构建一个相似性原型。此外，我们还提出了在网络训练的视角下使用相似性原型的两种方法： gradient label softening 和 batch-level contrastive loss。多个 benchmark 的全面评估表明，我们的相似性原型可以帮助现有网络提高性能，而无需增加计算负担。代码和统计相似性原型将很快地提供。
</details></li>
</ul>
<hr>
<h2 id="Aphid-Cluster-Recognition-and-Detection-in-the-Wild-Using-Deep-Learning-Models"><a href="#Aphid-Cluster-Recognition-and-Detection-in-the-Wild-Using-Deep-Learning-Models" class="headerlink" title="Aphid Cluster Recognition and Detection in the Wild Using Deep Learning Models"></a>Aphid Cluster Recognition and Detection in the Wild Using Deep Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05881">http://arxiv.org/abs/2308.05881</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianxiao Zhang, Kaidong Li, Xiangyu Chen, Cuncong Zhong, Bo Luo, Ivan Grijalva, Brian McCornack, Daniel Flippo, Ajay Sharda, Guanghui Wang</li>
<li>for: 用于检测螟蛾群落，提高化学防治效率和环境可持续性。</li>
<li>methods: 使用深度学习模型检测螟蛾群落，并对图像进行裁剪处理，生成了151,380个标注图像 patch。</li>
<li>results: 对四种 state-of-the-art 对象检测模型（VFNet、GFLV2、PAA 和 ATSS）进行实验，并证明所有模型在螟蛾数据集上具有稳定的相似性表现，并且通过合并邻近螟蛾群落和移除小 cluster 提高了性能约17%。<details>
<summary>Abstract</summary>
Aphid infestation poses a significant threat to crop production, rural communities, and global food security. While chemical pest control is crucial for maximizing yields, applying chemicals across entire fields is both environmentally unsustainable and costly. Hence, precise localization and management of aphids are essential for targeted pesticide application. The paper primarily focuses on using deep learning models for detecting aphid clusters. We propose a novel approach for estimating infection levels by detecting aphid clusters. To facilitate this research, we have captured a large-scale dataset from sorghum fields, manually selected 5,447 images containing aphids, and annotated each individual aphid cluster within these images. To facilitate the use of machine learning models, we further process the images by cropping them into patches, resulting in a labeled dataset comprising 151,380 image patches. Then, we implemented and compared the performance of four state-of-the-art object detection models (VFNet, GFLV2, PAA, and ATSS) on the aphid dataset. Extensive experimental results show that all models yield stable similar performance in terms of average precision and recall. We then propose to merge close neighboring clusters and remove tiny clusters caused by cropping, and the performance is further boosted by around 17%. The study demonstrates the feasibility of automatically detecting and managing insects using machine learning models. The labeled dataset will be made openly available to the research community.
</details>
<details>
<summary>摘要</summary>
螟蛂滋生对农村社区和全球食品安全构成了重要威胁。虽然化学防治 insect 是提高产量的重要手段，但是在整个田野上应用化学药品是环境不可持续和昂贵的。因此，精准地Localization和管理螟蛂是非常重要的。本文主要关注使用深度学习模型来探测螟蛂群。我们提出了一种新的方法，利用深度学习模型来估算感染水平。为了进行这项研究，我们在甘蔗田中采集了大规模数据集，手动选择了5447张图像，并在每张图像中标注了每个螟蛂群。为了使机器学习模型更易使用，我们进一步处理了图像，将其分割成小块，得到了151380个标注的图像块。然后，我们实现了和比较了四种当前最佳对象检测模型（VFNet、GFLV2、PAA和ATSS）在螟蛂数据集上的性能。结果显示，所有模型在精度和报告方面具有稳定的相似性。我们然后提议将邻近的螟蛂群合并并 removes 小 clusters，性能得到了约17%的提升。研究表明，使用机器学习模型自动检测和管理昆虫是可能的。标注数据集将被开放提供给研究社区。
</details></li>
</ul>
<hr>
<h2 id="Vision-Backbone-Enhancement-via-Multi-Stage-Cross-Scale-Attention"><a href="#Vision-Backbone-Enhancement-via-Multi-Stage-Cross-Scale-Attention" class="headerlink" title="Vision Backbone Enhancement via Multi-Stage Cross-Scale Attention"></a>Vision Backbone Enhancement via Multi-Stage Cross-Scale Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05872">http://arxiv.org/abs/2308.05872</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liang Shang, Yanli Liu, Zhengyang Lou, Shuxue Quan, Nagesh Adluru, Bochen Guan, William A. Sethares</li>
<li>for: 提高视觉任务中 CNN 和 ViT 的性能，增加多stage和跨Scale的交互。</li>
<li>methods: 提出了一个简单的添加注意力模块，使得不同阶段和尺度的特征图可以进行多stage和跨Scale的交互。</li>
<li>results: 实验表明，在多个下游任务中，MSCSA 可以提供明显的性能提升，具有相对较少的额外计算量和运行时间。<details>
<summary>Abstract</summary>
Convolutional neural networks (CNNs) and vision transformers (ViTs) have achieved remarkable success in various vision tasks. However, many architectures do not consider interactions between feature maps from different stages and scales, which may limit their performance. In this work, we propose a simple add-on attention module to overcome these limitations via multi-stage and cross-scale interactions. Specifically, the proposed Multi-Stage Cross-Scale Attention (MSCSA) module takes feature maps from different stages to enable multi-stage interactions and achieves cross-scale interactions by computing self-attention at different scales based on the multi-stage feature maps. Our experiments on several downstream tasks show that MSCSA provides a significant performance boost with modest additional FLOPs and runtime.
</details>
<details>
<summary>摘要</summary>
卷积神经网络（CNN）和视transformer（ViT）在视觉任务中取得了非常出色的成绩。然而，许多architecture不考虑不同阶段和缩放的feature map之间的交互，这可能会限制其性能。在这个工作中，我们提出了一种简单的加载注意力模块，以便超越这些限制。具体来说，我们的多阶段跨度注意力（MSCSA）模块使用不同阶段的feature map来实现多阶段交互，并在不同缩放级别上计算基于多阶段feature map的自注意力。我们的实验表明，MSCSA可以提供显著的性能提升，而且增加的计算量和运行时间幅度都很小。
</details></li>
</ul>
<hr>
<h2 id="The-Multi-modality-Cell-Segmentation-Challenge-Towards-Universal-Solutions"><a href="#The-Multi-modality-Cell-Segmentation-Challenge-Towards-Universal-Solutions" class="headerlink" title="The Multi-modality Cell Segmentation Challenge: Towards Universal Solutions"></a>The Multi-modality Cell Segmentation Challenge: Towards Universal Solutions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05864">http://arxiv.org/abs/2308.05864</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jun Ma, Ronald Xie, Shamini Ayyadhury, Cheng Ge, Anubha Gupta, Ritu Gupta, Song Gu, Yao Zhang, Gihun Lee, Joonkee Kim, Wei Lou, Haofeng Li, Eric Upschulte, Timo Dickscheid, José Guilherme de Almeida, Yixin Wang, Lin Han, Xin Yang, Marco Labagnara, Sahand Jamal Rahi, Carly Kempster, Alice Pollitt, Leon Espinosa, Tâm Mignot, Jan Moritz Middeke, Jan-Niklas Eckardt, Wangkai Li, Zhaoyang Li, Xiaochen Cai, Bizhe Bai, Noah F. Greenwald, David Van Valen, Erin Weisbart, Beth A. Cimini, Zhuoshi Li, Chao Zuo, Oscar Brück, Gary D. Bader, Bo Wang</li>
<li>for: 本研究旨在提供一个多Modalities单元 segmentation的benchmark，以便用于生物学实验中的单元分析。</li>
<li>methods: 本研究使用Transformer基于的深度学习算法，可以在不同的显微镜像平台和组织类型中自动调整参数。</li>
<li>results: 研究发现，这种新的算法可以在多个Modalities的单元分析中提供更高的准确率和多样性。<details>
<summary>Abstract</summary>
Cell segmentation is a critical step for quantitative single-cell analysis in microscopy images. Existing cell segmentation methods are often tailored to specific modalities or require manual interventions to specify hyperparameters in different experimental settings. Here, we present a multi-modality cell segmentation benchmark, comprising over 1500 labeled images derived from more than 50 diverse biological experiments. The top participants developed a Transformer-based deep-learning algorithm that not only exceeds existing methods, but can also be applied to diverse microscopy images across imaging platforms and tissue types without manual parameter adjustments. This benchmark and the improved algorithm offer promising avenues for more accurate and versatile cell analysis in microscopy imaging.
</details>
<details>
<summary>摘要</summary>
细胞分 segmentation是单细胞分析中的关键步骤，现有的细胞分 segmentation方法 oftentimes 适应特定的Modalities or require manual interventions to specify hyperparameters in different experimental settings. 在这里，我们提出了一个多Modalities 细胞分 segmentation benchmark，包括超过1500个标注图像，来自更多的50种多样化的生物实验。顶尖参与者开发了一种基于Transformer的深度学习算法，不仅超越了现有的方法，还可以适用于多种微scopic imaging平台和组织类型无需手动参数调整。这个benchmark和改进的算法提供了更加准确和多样化的细胞分析方法。
</details></li>
</ul>
<hr>
<h2 id="SegDA-Maximum-Separable-Segment-Mask-with-Pseudo-Labels-for-Domain-Adaptive-Semantic-Segmentation"><a href="#SegDA-Maximum-Separable-Segment-Mask-with-Pseudo-Labels-for-Domain-Adaptive-Semantic-Segmentation" class="headerlink" title="SegDA: Maximum Separable Segment Mask with Pseudo Labels for Domain Adaptive Semantic Segmentation"></a>SegDA: Maximum Separable Segment Mask with Pseudo Labels for Domain Adaptive Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05851">http://arxiv.org/abs/2308.05851</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anant Khandelwal</li>
<li>for: 提高Unsupervised Domain Adaptation（UDA）方法的转移性能，解决目标领域 Label 缺乏问题。</li>
<li>methods: 提出SegDA模组，增强UDA方法的转移性能，通过学习最大分类separable segment表现。</li>
<li>results: 在四个UDA benchmark上实现了+2.2 mIoU、+2.0 mIoU、+5.9 mIoU、+2.6 mIoU 的提升，即GTA -&gt; Cityscapes、Synthia -&gt; Cityscapes、Cityscapes -&gt; DarkZurich、Cityscapes -&gt; ACDC。<details>
<summary>Abstract</summary>
Unsupervised Domain Adaptation (UDA) aims to solve the problem of label scarcity of the target domain by transferring the knowledge from the label rich source domain. Usually, the source domain consists of synthetic images for which the annotation is easily obtained using the well known computer graphics techniques. However, obtaining annotation for real world images (target domain) require lot of manual annotation effort and is very time consuming because it requires per pixel annotation. To address this problem we propose SegDA module to enhance transfer performance of UDA methods by learning the maximum separable segment representation. This resolves the problem of identifying visually similar classes like pedestrian/rider, sidewalk/road etc. We leveraged Equiangular Tight Frame (ETF) classifier inspired from Neural Collapse for maximal separation between segment classes. This causes the source domain pixel representation to collapse to a single vector forming a simplex vertices which are aligned to the maximal separable ETF classifier. We use this phenomenon to propose the novel architecture for domain adaptation of segment representation for target domain. Additionally, we proposed to estimate the noise in labelling the target domain images and update the decoder for noise correction which encourages the discovery of pixels for classes not identified in pseudo labels. We have used four UDA benchmarks simulating synthetic-to-real, daytime-to-nighttime, clear-to-adverse weather scenarios. Our proposed approach outperforms +2.2 mIoU on GTA -> Cityscapes, +2.0 mIoU on Synthia -> Cityscapes, +5.9 mIoU on Cityscapes -> DarkZurich, +2.6 mIoU on Cityscapes -> ACDC.
</details>
<details>
<summary>摘要</summary>
Unsupervised Domain Adaptation (UDA) 目标是解决目标领域中标签稀缺的问题，通过从标签充沛的源领域传递知识。通常，源领域包含合成图像，可以使用Well known计算机图形技术获得注解。然而，获得真实世界图像（目标领域）的注解需要大量的手动注解effort和时间consuming，因为它需要每个像素注解。为解决这个问题，我们提议SegDA模块，用于增强UDA方法的传递性能。这解决了类型相似的问题，如行人/骑行者、斜道/路等。我们利用Equiangular Tight Frame（ETF）分类器，启发自Neural Collapse，以实现最大分离。这导致源领域像素表示 collapse到一个简单x��ensional vertices，这些 vertices 与最大分离ETF分类器相互平行。我们使用这种现象，提出一种新的领域适应措施。此外，我们还提出了估计目标领域图像注解的噪声，并更新解码器进行噪声纠正，这会鼓励发现类不存在 pseudo labels 中的像素。我们在四个 UDA  benchmark 中使用了 simulating  synthetic-to-real、daytime-to-nighttime、clear-to-adverse weather 等enario。我们的提议方法在 GTA -> Cityscapes、Synthia -> Cityscapes、Cityscapes -> DarkZurich 和 Cityscapes -> ACDC 等四个 benchmark 上都有出色的表现，相对于基eline +2.2 mIoU、+2.0 mIoU、+5.9 mIoU 和 +2.6 mIoU。
</details></li>
</ul>
<hr>
<h2 id="Recognizing-Handwritten-Mathematical-Expressions-of-Vertical-Addition-and-Subtraction"><a href="#Recognizing-Handwritten-Mathematical-Expressions-of-Vertical-Addition-and-Subtraction" class="headerlink" title="Recognizing Handwritten Mathematical Expressions of Vertical Addition and Subtraction"></a>Recognizing Handwritten Mathematical Expressions of Vertical Addition and Subtraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05820">http://arxiv.org/abs/2308.05820</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/danielgol/hme-vas">https://github.com/danielgol/hme-vas</a></li>
<li>paper_authors: Daniel Rosa, Filipe R. Cordeiro, Ruan Carvalho, Everton Souza, Sergio Chevtchenko, Luiz Rodrigues, Marcelo Marinho, Thales Vieira, Valmir Macario</li>
<li>for: 这个论文的目的是提出一种新的手写数学表达识别方法，能够识别括论的添加和减法表达。</li>
<li>methods: 这个论文使用了一些现有的物体检测算法，包括YOLO v7、YOLO v8、YOLO-NAS、NanoDet和FCOS，以及一种新的识别方法，可以将 bounding box 映射到 LaTeX  markup 序列中。</li>
<li>results: 这个论文的结果表明，该方法能够高效地识别手写数学表达，并且可以在不同的环境中进行扩展。<details>
<summary>Abstract</summary>
Handwritten Mathematical Expression Recognition (HMER) is a challenging task with many educational applications. Recent methods for HMER have been developed for complex mathematical expressions in standard horizontal format. However, solutions for elementary mathematical expression, such as vertical addition and subtraction, have not been explored in the literature. This work proposes a new handwritten elementary mathematical expression dataset composed of addition and subtraction expressions in a vertical format. We also extended the MNIST dataset to generate artificial images with this structure. Furthermore, we proposed a solution for offline HMER, able to recognize vertical addition and subtraction expressions. Our analysis evaluated the object detection algorithms YOLO v7, YOLO v8, YOLO-NAS, NanoDet and FCOS for identifying the mathematical symbols. We also proposed a transcription method to map the bounding boxes from the object detection stage to a mathematical expression in the LATEX markup sequence. Results show that our approach is efficient, achieving a high expression recognition rate. The code and dataset are available at https://github.com/Danielgol/HME-VAS
</details>
<details>
<summary>摘要</summary>
手写数学表达识别（HMER）是一项具有许多教育应用的挑战性任务。现今的HMER方法主要关注于标准水平格式中的复杂数学表达。然而，对于基本数学表达，如垂直加减表达，在 литературе中没有得到了探讨。本工作提出了一个新的手写基本数学表达数据集，包括垂直加减表达的形式。此外，我们还扩展了MNIST数据集，生成了具有这种结构的人工图像。此外，我们还提出了一种OFFLINE HMER方法，能够识别垂直加减表达。我们的分析评估了YOLO v7、YOLO v8、YOLO-NAS、NanoDet和FCOS算法来识别数学符号。此外，我们还提出了一种映射矩阵方法，将物体检测阶段中的 bounding box 映射到LATEX markup语句中的数学表达。结果表明，我们的方法高效，达到了高表达识别率。代码和数据集可以在https://github.com/Danielgol/HME-VAS上获取。
</details></li>
</ul>
<hr>
<h2 id="Absorption-Based-Passive-Range-Imaging-from-Hyperspectral-Thermal-Measurements"><a href="#Absorption-Based-Passive-Range-Imaging-from-Hyperspectral-Thermal-Measurements" class="headerlink" title="Absorption-Based, Passive Range Imaging from Hyperspectral Thermal Measurements"></a>Absorption-Based, Passive Range Imaging from Hyperspectral Thermal Measurements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05818">http://arxiv.org/abs/2308.05818</a></li>
<li>repo_url: None</li>
<li>paper_authors: Unay Dorken Gallastegi, Hoover Rueda-Chacon, Martin J. Stevens, Vivek K Goyal</li>
<li>for:  This paper is written for the purpose of developing a novel passive range imaging method based on atmospheric absorption of ambient thermal radiance, which can be used to recover range features from remote objects in natural scenes without the need for active illumination.</li>
<li>methods:  The paper uses a computational approach that separates the effects of remote object material composition, temperature, and range on the spectrum of thermal radiance, and introduces a novel method that exploits atmospheric absorption to mitigate noise in low-contrast scenarios. The method jointly estimates range and intrinsic object properties by exploiting a variety of absorption lines spread over the infrared spectrum.</li>
<li>results:  The paper reports that the proposed method can recover range features from 15m to 150m in long-wave infrared (8–13 $\mu$m) hyperspectral image data acquired from natural scenes with no active illumination. The results show good qualitative match to unaligned lidar data.<details>
<summary>Abstract</summary>
Passive hyperspectral long-wave infrared measurements are remarkably informative about the surroundings, such as remote object material composition, temperature, and range; and air temperature and gas concentrations. Remote object material and temperature determine the spectrum of thermal radiance, and range, air temperature, and gas concentrations determine how this spectrum is modified by propagation to the sensor. We computationally separate these phenomena, introducing a novel passive range imaging method based on atmospheric absorption of ambient thermal radiance. Previously demonstrated passive absorption-based ranging methods assume hot and highly emitting objects. However, the temperature variation in natural scenes is usually low, making range imaging challenging. Our method benefits from explicit consideration of air emission and parametric modeling of atmospheric absorption. To mitigate noise in low-contrast scenarios, we jointly estimate range and intrinsic object properties by exploiting a variety of absorption lines spread over the infrared spectrum. Along with Monte Carlo simulations that demonstrate the importance of regularization, temperature differentials, and availability of many spectral bands, we apply this method to long-wave infrared (8--13 $\mu$m) hyperspectral image data acquired from natural scenes with no active illumination. Range features from 15m to 150m are recovered, with good qualitative match to unaligned lidar data.
</details>
<details>
<summary>摘要</summary>
活动式 hyperspectral 长波无源热红外测量具有极高的准确性，能够提供远程物体材质组成、温度和距离等信息，以及空气温度和气体成分。远程物体材质和温度会影响热辐射谱的спектrum，而距离、空气温度和气体成分会影响这种谱spectrum的修饰和传播到探测器。我们通过计算分离这些现象，提出了一种新的无源范围成像方法，基于大气吸收的热辐射 ambient。先前的无源吸收基于范围方法假设了热和高度发射的对象，但是自然场景中的温度变化通常很低，使范围成像变得困难。我们的方法具有详细考虑大气发射和参数化大气吸收的优势，以适应实际场景。为了降低在低对比度场景下的噪声，我们同时估算范围和对象内部特性，通过利用多种吸收线扩散在红外谱域中。此外，我们还通过 Monte Carlo 仿真示出了正则化、温度差和许多 spectral band 的重要性，并应用这种方法到长波无源热红外（8-13 $\mu$m） hyperspectral 图像数据，从自然场景中无活动照明获得范围信息。在15米至150米的范围内，可以恢复出较好的质量匹配，与不对齐的探测器数据相符。
</details></li>
</ul>
<hr>
<h2 id="Spintronics-for-image-recognition-performance-benchmarking-via-ultrafast-data-driven-simulations"><a href="#Spintronics-for-image-recognition-performance-benchmarking-via-ultrafast-data-driven-simulations" class="headerlink" title="Spintronics for image recognition : performance benchmarking via ultrafast data-driven simulations"></a>Spintronics for image recognition : performance benchmarking via ultrafast data-driven simulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05810">http://arxiv.org/abs/2308.05810</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anatole Moureaux, Chloé Chopin, Laurent Jacques, Flavio Abreu Araujo</li>
<li>for: 用于图像分类</li>
<li>methods: 使用硬件基于echo-state网络（ESN），利用磁通量普通磁铁结构（STVO）来实现图像分类</li>
<li>results: 使用DD-TEA模拟STVO动态，实现了图像分类 task的高精度性，并在MNIST、EMNIST-letters和Fashion MNIST等 dataset上达到了比较高的性能水平，但在EMNIST-letters和Fashion MNIST上的性能较低，这可能是因为系统架构的简单性和任务的复杂性的问题。<details>
<summary>Abstract</summary>
We present a demonstration of image classification using a hardware-based echo-state network (ESN) that relies on spintronic nanostructures known as vortex-based spin-torque oscillators (STVOs). Our network is realized using a single STVO multiplexed in time. To circumvent the challenges associated with repeated experimental manipulation of such a nanostructured system, we employ an ultrafast data-driven simulation framework called the data-driven Thiele equation approach (DD-TEA) to simulate the STVO dynamics. We use this approach to efficiently develop, optimize and test an STVO-based ESN for image classification using the MNIST dataset. We showcase the versatility of our solution by successfully applying it to solve classification challenges with the EMNIST-letters and Fashion MNIST datasets. Through our simulations, we determine that within a large ESN the results obtained using the STVO dynamics as an activation function are comparable to the ones obtained with other conventional nonlinear activation functions like the reLU and the sigmoid. While achieving state-of-the-art accuracy levels on the MNIST dataset, our model's performance on EMNIST-letters and Fashion MNIST is lower due to the relative simplicity of the system architecture and the increased complexity of the tasks. We expect that the DD-TEA framework will enable the exploration of more specialized neural architectures, ultimately leading to improved classification accuracy. This approach also holds promise for investigating and developing dedicated learning rules to further enhance classification performance.
</details>
<details>
<summary>摘要</summary>
我们提出了一种使用硬件基于的echo-state网络（ESN）进行图像分类，该网络基于旋转-基于磁通量（STVO）的磁镜结构。我们的网络使用了单个STVO，并在时间多态化中进行多批处理。为了缓解重复实验室中STVO系统的nanostructured系统的挑战，我们使用了一种高速数据驱动的模拟框架called the data-driven Thiele equation approach（DD-TEA）来模拟STVO动态。我们使用这种方法来有效地开发、优化和测试一个基于STVO的ESN，并在MNIST数据集上进行图像分类。我们成功地应用了这种解决方案，并在EMNIST-letters和Fashion MNIST数据集上解决了分类挑战。我们通过模拟结果发现，在大型ESN中，使用STVO动态作为激活函数的结果与其他常见非线性激活函数如reLU和sigmoid相比，具有相似的性能水平。而在MNIST数据集上，我们的模型实现了state-of-the-art的准确率水平，但在EMNIST-letters和Fashion MNIST数据集上，模型的性能较低，这主要归结于系统架构的简单性和任务的复杂性。我们预计，DD-TEA框架将能够探索更特化的神经网络架构，从而实现更高的分类精度。此外，这种方法还可以用于研究和开发特定的学习规则，以进一步提高分类性能。
</details></li>
</ul>
<hr>
<h2 id="Iterative-Reweighted-Least-Squares-Networks-With-Convergence-Guarantees-for-Solving-Inverse-Imaging-Problems"><a href="#Iterative-Reweighted-Least-Squares-Networks-With-Convergence-Guarantees-for-Solving-Inverse-Imaging-Problems" class="headerlink" title="Iterative Reweighted Least Squares Networks With Convergence Guarantees for Solving Inverse Imaging Problems"></a>Iterative Reweighted Least Squares Networks With Convergence Guarantees for Solving Inverse Imaging Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05745">http://arxiv.org/abs/2308.05745</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iaroslav Koshelev, Stamatios Lefkimmiatis</li>
<li>for: 这篇论文关注于图像重建任务中的分析基于图像正则化，旨在推广 sparse 和&#x2F;或低维解决方案。</li>
<li>methods: 作者提出了一种新的优化策略，基于可评估函数 Parametrize 图像正则化。这种策略是基于 Iteratively Reweighted Least Squares (IRLS) 方法，通常用于 synthesis-based $\ell_p$ 和 $\mathcal{S}_p$ norm 以及 analysis-based $\ell_1$ 和核函数正则化。</li>
<li>results: 作者证明了其优化算法在稳定点下线性收敛，并提供了一个上界 для收敛速率。此外，作者还提出了一种学习参数的方法，通过将学习过程视为一个随机双层优化问题来实现。通过证明了其优化算法的收敛性，这种学习方法可以成功完成。作者还对其 learned IRLS 变体进行了评估，并与其他现有的学习重建方法进行了比较。<details>
<summary>Abstract</summary>
In this work we present a novel optimization strategy for image reconstruction tasks under analysis-based image regularization, which promotes sparse and/or low-rank solutions in some learned transform domain. We parameterize such regularizers using potential functions that correspond to weighted extensions of the $\ell_p^p$-vector and $\mathcal{S}_p^p$ Schatten-matrix quasi-norms with $0 < p \le 1$. Our proposed minimization strategy extends the Iteratively Reweighted Least Squares (IRLS) method, typically used for synthesis-based $\ell_p$ and $\mathcal{S}_p$ norm and analysis-based $\ell_1$ and nuclear norm regularization. We prove that under mild conditions our minimization algorithm converges linearly to a stationary point, and we provide an upper bound for its convergence rate. Further, to select the parameters of the regularizers that deliver the best results for the problem at hand, we propose to learn them from training data by formulating the supervised learning process as a stochastic bilevel optimization problem. We show that thanks to the convergence guarantees of our proposed minimization strategy, such optimization can be successfully performed with a memory-efficient implicit back-propagation scheme. We implement our learned IRLS variants as recurrent networks and assess their performance on the challenging image reconstruction tasks of non-blind deblurring, super-resolution and demosaicking. The comparisons against other existing learned reconstruction approaches demonstrate that our overall method is very competitive and in many cases outperforms existing unrolled networks, whose number of parameters is orders of magnitude higher than in our case.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们提出了一种新的优化策略，用于图像重建任务中的分析基于图像正则化。我们使用可调函数来parameterize这些正则izer，这些函数对应于weighted扩展的 $\ell_p^p$ 评估函数和 $\mathcal{S}_p^p$ 施密特矩阵评估函数的 $0 < p \le 1$。我们的提出的最小化策略是基于Iteratively Reweighted Least Squares（IRLS）方法，通常用于 synthesis-based $\ell_p$ 和 $\mathcal{S}_p$ 评估和分析基于 $\ell_1$ 和核评估的正则化。我们证明了，在某些条件下，我们的最小化算法会线性收敛到稳定点，并提供了最小化速度的上限。此外，为了选择正则izer的参数，以实现问题中最佳的结果，我们提议通过训练数据来学习这些参数，并将其表示为随机双层优化问题。我们证明了， благодаря我们提出的最小化策略的收敛保证，这种优化可以成功地进行，使用内存高效的隐式循环反射学习。我们实现了我们学习IRLS变体的recurrent neural network，并对非盲杂化、超分辨和排版重建任务进行评估。与其他已知的学习重建方法相比，我们的总方法在许多情况下表现非常竞争力强，而且在许多情况下even outperform existing unrolled networks， whose number of parameters is orders of magnitude higher than ours。
</details></li>
</ul>
<hr>
<h2 id="PlankAssembly-Robust-3D-Reconstruction-from-Three-Orthographic-Views-with-Learnt-Shape-Programs"><a href="#PlankAssembly-Robust-3D-Reconstruction-from-Three-Orthographic-Views-with-Learnt-Shape-Programs" class="headerlink" title="PlankAssembly: Robust 3D Reconstruction from Three Orthographic Views with Learnt Shape Programs"></a>PlankAssembly: Robust 3D Reconstruction from Three Orthographic Views with Learnt Shape Programs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05744">http://arxiv.org/abs/2308.05744</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/manycore-research/PlankAssembly">https://github.com/manycore-research/PlankAssembly</a></li>
<li>paper_authors: Wentao Hu, Jia Zheng, Zixin Zhang, Xiaojun Yuan, Jian Yin, Zihan Zhou</li>
<li>for: 自动将2D线描图 transformed into3D CAD模型</li>
<li>methods: 使用Transformer序列生成模型和形状程序</li>
<li>results: 与现有方法相比，our方法在干扰或 incomplete输入时表现出色<details>
<summary>Abstract</summary>
In this paper, we develop a new method to automatically convert 2D line drawings from three orthographic views into 3D CAD models. Existing methods for this problem reconstruct 3D models by back-projecting the 2D observations into 3D space while maintaining explicit correspondence between the input and output. Such methods are sensitive to errors and noises in the input, thus often fail in practice where the input drawings created by human designers are imperfect. To overcome this difficulty, we leverage the attention mechanism in a Transformer-based sequence generation model to learn flexible mappings between the input and output. Further, we design shape programs which are suitable for generating the objects of interest to boost the reconstruction accuracy and facilitate CAD modeling applications. Experiments on a new benchmark dataset show that our method significantly outperforms existing ones when the inputs are noisy or incomplete.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的方法，用于自动将二维线 drawing从三个orthographic视图转换为三维 CAD 模型。现有的方法在这个问题上 reconstruction 3D 模型，而不是维护explicit的对应关系 между输入和输出。这些方法在输入中存在错误和噪声时容易失败。为了解决这个困难，我们利用了注意力机制在Transformer-based sequence generation模型中学习flexible的对应关系。此外，我们还设计了适合生成目标对象的形状程序，以提高重建精度和促进 CAD 模型应用。在一个新的 bencmark 数据集上进行了实验，我们的方法在输入噪声或不完整时表现出了显著的优异性。
</details></li>
</ul>
<hr>
<h2 id="Zero-Grads-Ever-Given-Learning-Local-Surrogate-Losses-for-Non-Differentiable-Graphics"><a href="#Zero-Grads-Ever-Given-Learning-Local-Surrogate-Losses-for-Non-Differentiable-Graphics" class="headerlink" title="Zero Grads Ever Given: Learning Local Surrogate Losses for Non-Differentiable Graphics"></a>Zero Grads Ever Given: Learning Local Surrogate Losses for Non-Differentiable Graphics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05739">http://arxiv.org/abs/2308.05739</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Fischer, Tobias Ritschel</li>
<li>for: 缺乏定义或ZeroGradients问题的图形优化</li>
<li>methods: 自动学习神经网络对象函数的替换、在线自监督学习、跟踪式采样</li>
<li>results: 可以优化非对称非导数的图形问题，比如视野渲染、批处理模型和物理驱动动画优化，并且可以扩展到更高维度问题。<details>
<summary>Abstract</summary>
Gradient-based optimization is now ubiquitous across graphics, but unfortunately can not be applied to problems with undefined or zero gradients. To circumvent this issue, the loss function can be manually replaced by a "surrogate" that has similar minima but is differentiable. Our proposed framework, ZeroGrads, automates this process by learning a neural approximation of the objective function, the surrogate, which in turn can be used to differentiate through arbitrary black-box graphics pipelines. We train the surrogate on an actively smoothed version of the objective and encourage locality, focusing the surrogate's capacity on what matters at the current training episode. The fitting is performed online, alongside the parameter optimization, and self-supervised, without pre-computed data or pre-trained models. As sampling the objective is expensive (it requires a full rendering or simulator run), we devise an efficient sampling scheme that allows for tractable run-times and competitive performance at little overhead. We demonstrate optimizing diverse non-convex, non-differentiable black-box problems in graphics, such as visibility in rendering, discrete parameter spaces in procedural modelling or optimal control in physics-driven animation. In contrast to more traditional algorithms, our approach scales well to higher dimensions, which we demonstrate on problems with up to 35k interlinked variables.
</details>
<details>
<summary>摘要</summary>
gradient-based优化现在Graphics中 ubique,但是它无法应用于具有未定义或Zero Gradient的问题。为了缺页这个问题，损失函数可以手动被替换为一个"代理"，该函数有相似的 минимумы，但是可微的。我们的提议的框架ZeroGrads自动实现了这个过程，它学习一个神经网络的目标函数，代理函数，该函数可以在arbitrary黑盒图形处理管道中进行导数。我们在训练过程中使用一种活动缓和的损失函数，并且强调地方性，使得代理函数的容量集中在当前训练集中。我们在线上进行训练，并且是自我supervised，不需要预计算数据或预训练模型。由于抽样损失函数是Expensive（需要全部渲染或simulatorrun），我们开发了一种高效的抽样方案，使得运行时间可以被控制，并且性能与其他方法相匹配。我们在Graphics中优化了多种非对称、非导数的黑盒问题，如渲染中的可见性、procedural模型中的分割参数空间和物理驱动的动画中的优化问题。与传统算法相比，我们的方法可以扩展到更高的维度，我们在35k个相互连接的变量上进行了示例。
</details></li>
</ul>
<hr>
<h2 id="Follow-Anything-Open-set-detection-tracking-and-following-in-real-time"><a href="#Follow-Anything-Open-set-detection-tracking-and-following-in-real-time" class="headerlink" title="Follow Anything: Open-set detection, tracking, and following in real-time"></a>Follow Anything: Open-set detection, tracking, and following in real-time</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05737">http://arxiv.org/abs/2308.05737</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alaamaalouf/followanything">https://github.com/alaamaalouf/followanything</a></li>
<li>paper_authors: Alaa Maalouf, Ninad Jadhav, Krishna Murthy Jatavallabhula, Makram Chahine, Daniel M. Vogt, Robert J. Wood, Antonio Torralba, Daniela Rus</li>
<li>for: 本研究旨在开发一个能够实时检测、跟踪和跟踪任何对象的 роботизирован系统，以满足各种工业自动化、物流和仓储、医疗和安全等领域中的需求。</li>
<li>methods: 本研究使用的方法是一种开放 vocabulary 和多Modal 模型，可以在实时检测和跟踪中应用于未知类型的对象。它利用大规模预训练模型（基础模型）提供的丰富视觉描述符，对输入图像序列进行检测和分割，并跟踪图像帧中的检测和分割结果。</li>
<li>results: 本研究在一个真实世界的 роботизирован系统（微型飞行器）上进行了实验，并证明了 FAn 可以在实时控制循环中无顾 occlusion 和对象重新出现的情况下准确地跟踪对象。此外，FAn 可以在一个笔记型的 GPU 上运行，实现每秒 6-20 帧的 Throughput。为了促进快速采用、部署和扩展，我们将所有代码打包在 GitHub 上的项目页面（<a target="_blank" rel="noopener" href="https://github.com/alaamaalouf/FollowAnything%EF%BC%89%EF%BC%8C%E5%B9%B6%E9%99%84%E4%B8%8A%E4%BA%86%E4%B8%80%E4%B8%AA5%E5%88%86%E9%92%9F%E7%9A%84">https://github.com/alaamaalouf/FollowAnything），并附上了一个5分钟的</a> Explainer 视频（<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=6Mgt3EPytrw%EF%BC%89%E3%80%82">https://www.youtube.com/watch?v=6Mgt3EPytrw）。</a><details>
<summary>Abstract</summary>
Tracking and following objects of interest is critical to several robotics use cases, ranging from industrial automation to logistics and warehousing, to healthcare and security. In this paper, we present a robotic system to detect, track, and follow any object in real-time. Our approach, dubbed ``follow anything'' (FAn), is an open-vocabulary and multimodal model -- it is not restricted to concepts seen at training time and can be applied to novel classes at inference time using text, images, or click queries. Leveraging rich visual descriptors from large-scale pre-trained models (foundation models), FAn can detect and segment objects by matching multimodal queries (text, images, clicks) against an input image sequence. These detected and segmented objects are tracked across image frames, all while accounting for occlusion and object re-emergence. We demonstrate FAn on a real-world robotic system (a micro aerial vehicle) and report its ability to seamlessly follow the objects of interest in a real-time control loop. FAn can be deployed on a laptop with a lightweight (6-8 GB) graphics card, achieving a throughput of 6-20 frames per second. To enable rapid adoption, deployment, and extensibility, we open-source all our code on our project webpage at https://github.com/alaamaalouf/FollowAnything . We also encourage the reader the watch our 5-minutes explainer video in this https://www.youtube.com/watch?v=6Mgt3EPytrw .
</details>
<details>
<summary>摘要</summary>
tracking和跟踪对象是 robotics 应用场景中的关键，从制造自动化到物流和仓储，到医疗和安全。在这篇论文中，我们提出了一种用于实时检测、跟踪和追踪任何对象的机器人系统。我们的方法，命名为“随时跟踪”（FAn），不受训练时的概念限制，可以在推理时应用于新的类型。通过使用大规模预训练模型（基础模型）提供的丰富视觉描述符，FAn 可以在输入图像序列中检测和分割对象，并在图像帧中跟踪这些检测到的对象，同时考虑 occlusion 和对象重新出现。我们在一种真实的机器人系统（微型飞行器）上实现了 FAn，并发现它可以在实时控制循环中不间断地跟踪对象。FAn 可以在具有6-8 GB 的轻量级图形处理器（GPU）上运行， achieving a throughput of 6-20 frames per second。为了促进快速采用、部署和扩展，我们在项目网站上公开了所有代码（https://github.com/alaamaalouf/FollowAnything）。我们还邀请读者查看我们的5分钟解释视频（https://www.youtube.com/watch?v=6Mgt3EPytrw）。
</details></li>
</ul>
<hr>
<h2 id="MapTRv2-An-End-to-End-Framework-for-Online-Vectorized-HD-Map-Construction"><a href="#MapTRv2-An-End-to-End-Framework-for-Online-Vectorized-HD-Map-Construction" class="headerlink" title="MapTRv2: An End-to-End Framework for Online Vectorized HD Map Construction"></a>MapTRv2: An End-to-End Framework for Online Vectorized HD Map Construction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05736">http://arxiv.org/abs/2308.05736</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hustvl/maptr">https://github.com/hustvl/maptr</a></li>
<li>paper_authors: Bencheng Liao, Shaoyu Chen, Yunchi Zhang, Bo Jiang, Qian Zhang, Wenyu Liu, Chang Huang, Xinggang Wang</li>
<li>for: 本研究旨在提供一种在线vector化高清地图构建框架，以便实现自动驾驶系统中的规划。</li>
<li>methods: 本方法提议了一种统一的 permutation-equivalent 模型方法，即将地图元素表示为一组等效排序的点集，以准确描述地图元素的形状和稳定学习过程。 我们还设计了层次查询嵌入 schema和 hierarchical bipartite matching 来灵活地编码结构化地图信息。</li>
<li>results: 我们的方法可以快速地在实时推理速度下 convergence，并在 nuScenes 和 Argoverse2  datasets 上达到了状态对照性的表现。丰富的质量结果显示了在复杂和多样化的驾驶场景中的稳定和可靠的地图构建质量。<details>
<summary>Abstract</summary>
High-definition (HD) map provides abundant and precise static environmental information of the driving scene, serving as a fundamental and indispensable component for planning in autonomous driving system. In this paper, we present \textbf{Map} \textbf{TR}ansformer, an end-to-end framework for online vectorized HD map construction. We propose a unified permutation-equivalent modeling approach, \ie, modeling map element as a point set with a group of equivalent permutations, which accurately describes the shape of map element and stabilizes the learning process. We design a hierarchical query embedding scheme to flexibly encode structured map information and perform hierarchical bipartite matching for map element learning. To speed up convergence, we further introduce auxiliary one-to-many matching and dense supervision. The proposed method well copes with various map elements with arbitrary shapes. It runs at real-time inference speed and achieves state-of-the-art performance on both nuScenes and Argoverse2 datasets. Abundant qualitative results show stable and robust map construction quality in complex and various driving scenes. Code and more demos are available at \url{https://github.com/hustvl/MapTR} for facilitating further studies and applications.
</details>
<details>
<summary>摘要</summary>
高清晰地图（HD地图）提供了驾驶场景中精确和丰富的静态环境信息， serves as 驾驶自动化系统的基本和不可或缺的组件。在这篇论文中，我们提出了 \textbf{Map} \textbf{TR}ansformer，一种端到端框架，用于在线vectorized HD地图建构。我们提出了一种统一 permutation-equivalent 模型方法，即将地图元素模型为一个点集，并使用一组等效排序来准确描述地图元素的形状，从而稳定学习过程。我们设计了层次查询嵌入方案，以便flexibly编码结构化地图信息，并在多个纬度上进行层次对应。为了加速收敛，我们还引入了辅助一对多对应和密集监督。提出的方法可以快速和稳定地构建多种形状的地图元素，并在真实时间执行速度下达到了 nuScenes 和 Argoverse2 数据集的国际级表现。丰富的质量图像显示了在复杂和多样的驾驶场景中的稳定和可靠的地图建构质量。代码和更多示例可以在 \url{https://github.com/hustvl/MapTR} 上找到，以便进一步的研究和应用。
</details></li>
</ul>
<hr>
<h2 id="FrozenRecon-Pose-free-3D-Scene-Reconstruction-with-Frozen-Depth-Models"><a href="#FrozenRecon-Pose-free-3D-Scene-Reconstruction-with-Frozen-Depth-Models" class="headerlink" title="FrozenRecon: Pose-free 3D Scene Reconstruction with Frozen Depth Models"></a>FrozenRecon: Pose-free 3D Scene Reconstruction with Frozen Depth Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05733">http://arxiv.org/abs/2308.05733</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aim-uofa/FrozenRecon">https://github.com/aim-uofa/FrozenRecon</a></li>
<li>paper_authors: Guangkai Xu, Wei Yin, Hao Chen, Chunhua Shen, Kai Cheng, Feng Zhao<br>for:The paper is written for the task of 3D scene reconstruction, specifically addressing the challenge of robustly obtaining camera poses and achieving dense scene reconstruction in diverse real-world scenarios.methods:The paper proposes a novel test-time optimization approach that leverages pre-trained affine-invariant depth models, such as LeReS, to ensure inter-frame consistency and achieve robust scene reconstruction. The approach involves freezing the depth predictions, rectifying them with a geometric consistency alignment module, and employing the resulting scale-consistent depth maps to obtain camera poses and reconstruct the scene.results:The paper achieves state-of-the-art cross-dataset reconstruction on five zero-shot testing datasets, demonstrating the effectiveness of the proposed approach in handling diverse real-world scenarios and improving the robustness of 3D scene reconstruction.<details>
<summary>Abstract</summary>
3D scene reconstruction is a long-standing vision task. Existing approaches can be categorized into geometry-based and learning-based methods. The former leverages multi-view geometry but can face catastrophic failures due to the reliance on accurate pixel correspondence across views. The latter was proffered to mitigate these issues by learning 2D or 3D representation directly. However, without a large-scale video or 3D training data, it can hardly generalize to diverse real-world scenarios due to the presence of tens of millions or even billions of optimization parameters in the deep network. Recently, robust monocular depth estimation models trained with large-scale datasets have been proven to possess weak 3D geometry prior, but they are insufficient for reconstruction due to the unknown camera parameters, the affine-invariant property, and inter-frame inconsistency. Here, we propose a novel test-time optimization approach that can transfer the robustness of affine-invariant depth models such as LeReS to challenging diverse scenes while ensuring inter-frame consistency, with only dozens of parameters to optimize per video frame. Specifically, our approach involves freezing the pre-trained affine-invariant depth model's depth predictions, rectifying them by optimizing the unknown scale-shift values with a geometric consistency alignment module, and employing the resulting scale-consistent depth maps to robustly obtain camera poses and achieve dense scene reconstruction, even in low-texture regions. Experiments show that our method achieves state-of-the-art cross-dataset reconstruction on five zero-shot testing datasets.
</details>
<details>
<summary>摘要</summary>
3D场景重建是一个长期的视觉任务。现有的方法可以分为基于几何学的方法和学习基于方法。前者利用多视图几何学，但可能会遇到几何学灾难，因为需要精准的像素对应关系 across views。后者提出了使用 directly 学习 2D 或 3D 表示，但在没有大规模视频或 3D 训练数据时，它很难总结到多样化的实际场景中。最近，一些稳定的单视图深度估计模型，通过大规模数据集训练，已经证明了具有弱型 3D 几何特征，但它们无法承受不知道摄像头参数，Affine-invariant 性和inter-frame不一致性。我们提出了一种新的测试时优化方法，可以在多样化场景中传递稳定的 Affine-invariant 深度模型 LeReS 的稳定性，同时保证 inter-frame 一致性，只需要每帧Optimize 几十个参数。我们的方法包括冻结预训练 Affine-invariant 深度模型的深度预测，对其进行 rectify 操作，使用 resulting scale-consistent depth maps 来Robustly 获取摄像头姿态和实现密集场景重建，即使在低文本区域。实验表明，我们的方法在五个零学习测试数据集上 achieve state-of-the-art cross-dataset reconstruction。
</details></li>
</ul>
<hr>
<h2 id="Deformable-Mixer-Transformer-with-Gating-for-Multi-Task-Learning-of-Dense-Prediction"><a href="#Deformable-Mixer-Transformer-with-Gating-for-Multi-Task-Learning-of-Dense-Prediction" class="headerlink" title="Deformable Mixer Transformer with Gating for Multi-Task Learning of Dense Prediction"></a>Deformable Mixer Transformer with Gating for Multi-Task Learning of Dense Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05721">http://arxiv.org/abs/2308.05721</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yangyangxu0/demtg">https://github.com/yangyangxu0/demtg</a></li>
<li>paper_authors: Yangyang Xu, Yibo Yang, Bernard Ghanemm, Lefei Zhang, Du Bo, Dacheng Tao</li>
<li>for: 这个研究旨在开发一个新的多任务学习（Multi-task learning，MTL）模型，可以结合具有弹性和对话的对称卷积（Deformable CNN）和问题基于对称（Query-based Transformer）的优点，以提高MTL的效能。</li>
<li>methods: 这个模型使用了一个简单且有效的encoder-decoder架构，组合了对称和注意力机制，具有弹性和全面的特征，可以对多个任务进行精确的预测。</li>
<li>results: 实验结果显示，提案的DeMTG模型比现有的Transformer-based和CNN-based竞争模型在多个度量上表现更好，并且需要更少的GFLOPs。 codes和模型可以在<a target="_blank" rel="noopener" href="https://github.com/yangyangxu0/DeMTG%E4%B8%8A%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/yangyangxu0/DeMTG上下载。</a><details>
<summary>Abstract</summary>
CNNs and Transformers have their own advantages and both have been widely used for dense prediction in multi-task learning (MTL). Most of the current studies on MTL solely rely on CNN or Transformer. In this work, we present a novel MTL model by combining both merits of deformable CNN and query-based Transformer with shared gating for multi-task learning of dense prediction. This combination may offer a simple and efficient solution owing to its powerful and flexible task-specific learning and advantages of lower cost, less complexity and smaller parameters than the traditional MTL methods. We introduce deformable mixer Transformer with gating (DeMTG), a simple and effective encoder-decoder architecture up-to-date that incorporates the convolution and attention mechanism in a unified network for MTL. It is exquisitely designed to use advantages of each block, and provide deformable and comprehensive features for all tasks from local and global perspective. First, the deformable mixer encoder contains two types of operators: the channel-aware mixing operator leveraged to allow communication among different channels, and the spatial-aware deformable operator with deformable convolution applied to efficiently sample more informative spatial locations. Second, the task-aware gating transformer decoder is used to perform the task-specific predictions, in which task interaction block integrated with self-attention is applied to capture task interaction features, and the task query block integrated with gating attention is leveraged to select corresponding task-specific features. Further, the experiment results demonstrate that the proposed DeMTG uses fewer GFLOPs and significantly outperforms current Transformer-based and CNN-based competitive models on a variety of metrics on three dense prediction datasets. Our code and models are available at https://github.com/yangyangxu0/DeMTG.
</details>
<details>
<summary>摘要</summary>
CNN 和 Transformer 都有自己的优点，两者都广泛用于密集预测多任务学习（MTL）。现有大多数MTL研究都仅仅靠坐标 CNN 或 Transformer。在这种工作中，我们提出了一种新的MTL模型，该模型将具有扭曲 CNN 和查询基于 Transformer 的共享锁定，以便实现多任务密集预测。这种结合可能会提供一种简单、高效的解决方案，因为它可以充分利用每个任务的特点，并且具有更低的成本、更低的复杂性和更小的参数。我们称之为“扭曲混合变换器”（DeMTG），它是一种简单而有效的编码器-解码器架构，它将 convolution 和注意力机制 integrate 在一个网络中，以便实现 MTL。它的设计非常灵活，可以根据每个任务的需求进行定制。首先，扭曲混合encoder包含两种操作符：通道意识混合操作符，可以允许不同通道之间的交流，以及空间意识扭曲操作符，通过高效地采样更多的空间位置来提高预测性能。其次，任务意识解码器使用任务交互块和自注意力来捕捉任务交互特征，并使用任务查询块和锁定注意力来选择相应的任务特定特征。此外，我们的实验结果表明，提案的 DeMTG 使用更少的 GFLOPs，并在多个维度上显著超越现有的 Transformer 基于和 CNN 基于竞争模型。我们的代码和模型可以在 GitHub 上获取。
</details></li>
</ul>
<hr>
<h2 id="Temporally-Adaptive-Models-for-Efficient-Video-Understanding"><a href="#Temporally-Adaptive-Models-for-Efficient-Video-Understanding" class="headerlink" title="Temporally-Adaptive Models for Efficient Video Understanding"></a>Temporally-Adaptive Models for Efficient Video Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05787">http://arxiv.org/abs/2308.05787</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alibaba-mmai-research/TAdaConv">https://github.com/alibaba-mmai-research/TAdaConv</a></li>
<li>paper_authors: Ziyuan Huang, Shiwei Zhang, Liang Pan, Zhiwu Qing, Yingya Zhang, Ziwei Liu, Marcelo H. Ang Jr</li>
<li>for: 这个研究旨在提高视频理解模型中的时间模型化能力，以提高视频理解的精度和效率。</li>
<li>methods: 该研究提出了Temporally-Adaptive Convolutions（TAdaConv），它在视频中的每帧中进行自适应权重调整，以便更好地模型视频中的时间动态。TAdaConv使得空间核函数获得了时间模型化能力，从而提高了模型的表现。</li>
<li>results: 根据实验结果，TAdaConvNeXtV2和TAdaFormer在不同的视频理解benchmark中与现有的卷积和Transformer-based模型竞争性地表现，并且在一些任务上具有更高的精度和效率。<details>
<summary>Abstract</summary>
Spatial convolutions are extensively used in numerous deep video models. It fundamentally assumes spatio-temporal invariance, i.e., using shared weights for every location in different frames. This work presents Temporally-Adaptive Convolutions (TAdaConv) for video understanding, which shows that adaptive weight calibration along the temporal dimension is an efficient way to facilitate modeling complex temporal dynamics in videos. Specifically, TAdaConv empowers spatial convolutions with temporal modeling abilities by calibrating the convolution weights for each frame according to its local and global temporal context. Compared to existing operations for temporal modeling, TAdaConv is more efficient as it operates over the convolution kernels instead of the features, whose dimension is an order of magnitude smaller than the spatial resolutions. Further, kernel calibration brings an increased model capacity. Based on this readily plug-in operation TAdaConv as well as its extension, i.e., TAdaConvV2, we construct TAdaBlocks to empower ConvNeXt and Vision Transformer to have strong temporal modeling capabilities. Empirical results show TAdaConvNeXtV2 and TAdaFormer perform competitively against state-of-the-art convolutional and Transformer-based models in various video understanding benchmarks. Our codes and models are released at: https://github.com/alibaba-mmai-research/TAdaConv.
</details>
<details>
<summary>摘要</summary>
“空间卷积广泛应用于深度视频模型中。它假设空间时间不变性，即使用共享权重 для每帧不同帧。本工作介绍了Temporally-Adaptive Convolutions（TAdaConv） для视频理解，它表明了在时间维度上进行权重调整是一种高效的方式来模型视频复杂的时间动态。具体来说，TAdaConv使得空间卷积具有时间模型能力，通过对每帧的卷积权重进行本地和全局时间上下文的调整。与现有的时间模型操作相比，TAdaConv更高效，因为它操作在卷积核心上而不是特征上，特征维度与空间分辨率相比只有一个数量级。此外，权重调整提高了模型的容量。基于这种可插入操作，我们构建了TAdaBlocks来激发ConvNeXt和Vision Transformer具有强大的时间模型能力。实验结果显示TAdaConvNeXtV2和TAdaFormer与状态态的 convolutional 和 Transformer-based 模型在不同的视频理解 benchmark 中竞争性。我们的代码和模型在：https://github.com/alibaba-mmai-research/TAdaConv。”
</details></li>
</ul>
<hr>
<h2 id="Spatial-Pathomics-Toolkit-for-Quantitative-Analysis-of-Podocyte-Nuclei-with-Histology-and-Spatial-Transcriptomics-Data-in-Renal-Pathology"><a href="#Spatial-Pathomics-Toolkit-for-Quantitative-Analysis-of-Podocyte-Nuclei-with-Histology-and-Spatial-Transcriptomics-Data-in-Renal-Pathology" class="headerlink" title="Spatial Pathomics Toolkit for Quantitative Analysis of Podocyte Nuclei with Histology and Spatial Transcriptomics Data in Renal Pathology"></a>Spatial Pathomics Toolkit for Quantitative Analysis of Podocyte Nuclei with Histology and Spatial Transcriptomics Data in Renal Pathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06288">http://arxiv.org/abs/2308.06288</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hrlblab/spatial_pathomics">https://github.com/hrlblab/spatial_pathomics</a></li>
<li>paper_authors: Jiayuan Chen, Yu Wang, Ruining Deng, Quan Liu, Can Cui, Tianyuan Yao, Yilin Liu, Jianyong Zhong, Agnes B. Fogo, Haichun Yang, Shilin Zhao, Yuankai Huo</li>
<li>for: 这篇论文的目的是提出一种新的工具包，用于全面评估肾脏病变中的podocyte细胞特征。</li>
<li>methods: 这个工具包包括三个主要组成部分：1）实例对象分割，以准确地识别podocyte核lei；2）pathomics特征生成，从识别的核lei中提取了一系列量化特征；3）Robust统计分析，为探索质量特征之间的空间关系提供了一个全面的探索。</li>
<li>results: 该工具包成功提取和分析了podocyte核lei的形态和文化特征，并通过统计分析发现了一系列podocyte形omic特征。此外，工具还能够揭示肾脏病变中podocyte分布的空间信息，为肾脏病变的研究提供了新的视角。<details>
<summary>Abstract</summary>
Podocytes, specialized epithelial cells that envelop the glomerular capillaries, play a pivotal role in maintaining renal health. The current description and quantification of features on pathology slides are limited, prompting the need for innovative solutions to comprehensively assess diverse phenotypic attributes within Whole Slide Images (WSIs). In particular, understanding the morphological characteristics of podocytes, terminally differentiated glomerular epithelial cells, is crucial for studying glomerular injury. This paper introduces the Spatial Pathomics Toolkit (SPT) and applies it to podocyte pathomics. The SPT consists of three main components: (1) instance object segmentation, enabling precise identification of podocyte nuclei; (2) pathomics feature generation, extracting a comprehensive array of quantitative features from the identified nuclei; and (3) robust statistical analyses, facilitating a comprehensive exploration of spatial relationships between morphological and spatial transcriptomics features.The SPT successfully extracted and analyzed morphological and textural features from podocyte nuclei, revealing a multitude of podocyte morphomic features through statistical analysis. Additionally, we demonstrated the SPT's ability to unravel spatial information inherent to podocyte distribution, shedding light on spatial patterns associated with glomerular injury. By disseminating the SPT, our goal is to provide the research community with a powerful and user-friendly resource that advances cellular spatial pathomics in renal pathology. The implementation and its complete source code of the toolkit are made openly accessible at https://github.com/hrlblab/spatial_pathomics.
</details>
<details>
<summary>摘要</summary>
PODOSITE 是特殊的血浆上皮细胞，它们环绕血浆 капиllaries，对肾健康具有重要作用。现有的描述和量化特征是有限的，因此需要创新的解决方案来全面评估多样化的phenotypic特征在整个扫描图像（WSIs）中。特别是理解PODOSITE的形态特征非常重要，以study glomerular injury。本文介绍了Spatial Pathomics Toolkit（SPT）并应用它于PODOSITE pathomics。SPT包括三个主要组成部分：（1）实体对象分割，可以准确地识别PODOSITE核lei；（2）pathomics特征生成，从识别的核lei中提取了一系列数量特征；以及（3）Robust统计分析，使得可以全面探索扫描图像中PODOSITE的形态特征和空间特征之间的关系。SPT成功地提取和分析PODOSITE核lei的形态和文化特征，揭示了许多PODOSITE形态特征，并通过统计分析，探索了PODOSITE分布的空间特征，为glomerular injury提供了新的视角。我们的目标是通过普及SPT，为研究社区提供一个强大和易用的资源，以提高细胞空间Pathomics在肾病理学中的发展。SPT的实现和完整的源代码可以在https://github.com/hrlblab/spatial_pathomics上免费获取。
</details></li>
</ul>
<hr>
<h2 id="Shadow-Datasets-New-challenging-datasets-for-Causal-Representation-Learning"><a href="#Shadow-Datasets-New-challenging-datasets-for-Causal-Representation-Learning" class="headerlink" title="Shadow Datasets, New challenging datasets for Causal Representation Learning"></a>Shadow Datasets, New challenging datasets for Causal Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05707">http://arxiv.org/abs/2308.05707</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Jiagengzhu/Shadow-dataset-for-crl">https://github.com/Jiagengzhu/Shadow-dataset-for-crl</a></li>
<li>paper_authors: Jiageng Zhu, Hanchen Xie, Jianhua Wu, Jiazhi Li, Mahyar Khayatkhoei, Mohamed E. Hussein, Wael AbdAlmageed</li>
<li>For: 本研究旨在探讨语义因素之间的 causal 关系，并提出了一种基于弱监督学习的 causal representation learning（CRL）方法。* Methods: 该方法使用了一种基于 Generative Adversarial Networks（GANs）的弱监督学习方法，并在四个现有的数据集（Pendulum、Flow、CelebA（BEARD）和CelebA（SMILE））上进行了评估。* Results: 研究人员发现，使用该方法可以在具有更多多样化生成因素的更复杂的 causal 图上找到更好的 causal 关系。此外，他们还修改了现有的real数据集（CelebA（BEARD）和CelebA（SMILE））的原始 causal 图，以更好地适应数据集的分布。<details>
<summary>Abstract</summary>
Discovering causal relations among semantic factors is an emergent topic in representation learning. Most causal representation learning (CRL) methods are fully supervised, which is impractical due to costly labeling. To resolve this restriction, weakly supervised CRL methods were introduced. To evaluate CRL performance, four existing datasets, Pendulum, Flow, CelebA(BEARD) and CelebA(SMILE), are utilized. However, existing CRL datasets are limited to simple graphs with few generative factors. Thus we propose two new datasets with a larger number of diverse generative factors and more sophisticated causal graphs. In addition, current real datasets, CelebA(BEARD) and CelebA(SMILE), the originally proposed causal graphs are not aligned with the dataset distributions. Thus, we propose modifications to them.
</details>
<details>
<summary>摘要</summary>
发现 semantic factor 之间的 causal 关系是 representation learning 中一个出现的话题。大多数 causal representation learning（CRL）方法是强制执行的，这是因为标注成本高昂。为解决这些限制，弱类标注 CRL 方法被引入。为评估 CRL 性能，四个现有的数据集， Pendulum、Flow、CelebA（BEARD）和 CelebA（SMILE），被使用。然而，现有的 CRL 数据集受限于简单的图像和少量生成因素。因此，我们提议两个新的数据集，它们具有更多的多样化的生成因素和更复杂的 causal 图。此外，原始的 real datasets，CelebA（BEARD）和 CelebA（SMILE），其提posed的 causal 图与数据集分布不匹配。因此，我们提议修改它们。
</details></li>
</ul>
<hr>
<h2 id="Masked-Diffusion-as-Self-supervised-Representation-Learner"><a href="#Masked-Diffusion-as-Self-supervised-Representation-Learner" class="headerlink" title="Masked Diffusion as Self-supervised Representation Learner"></a>Masked Diffusion as Self-supervised Representation Learner</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05695">http://arxiv.org/abs/2308.05695</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zixuan Pan, Jianxu Chen, Yiyu Shi</li>
<li>for: 这个论文是用于探讨Diffusion模型在生成和表示学习中的关系，以及如何使用Masking机制来提高Diffusion模型的表示学习能力。</li>
<li>methods: 该论文使用了Diffusion模型，并将传统的加法 Gaussian 噪声替换为Masking机制来进行自我超级vised学习。</li>
<li>results: 该论文在医疗和自然图像 semantic segmentation 任务中达到了优秀的表现，特别是在少shot场景下。<details>
<summary>Abstract</summary>
Denoising diffusion probabilistic models have recently demonstrated state-of-the-art generative performance and been used as strong pixel-level representation learners. This paper decomposes the interrelation between the generative capability and representation learning ability inherent in diffusion models. We present masked diffusion model (MDM), a scalable self-supervised representation learner that substitutes the conventional additive Gaussian noise of traditional diffusion with a masking mechanism. Our proposed approach convincingly surpasses prior benchmarks, demonstrating remarkable advancements in both medical and natural image semantic segmentation tasks, particularly within the context of few-shot scenario.
</details>
<details>
<summary>摘要</summary>
diffusion 模型在最近几年已经展示了状态算法的极优生成性能，并且用作强大的像素级表示学习器。这篇论文分析了diffusion模型中生成能力和表示学习能力之间的关系。我们提出了受掩码机制取代传统的加法 Gaussian 噪声的masked diffusion model（MDM）。我们的提议方法在医学和自然图像Semantic segmentation任务中表现出色，特别是在几个shot场景下。
</details></li>
</ul>
<hr>
<h2 id="Leverage-Weakly-Annotation-to-Pixel-wise-Annotation-via-Zero-shot-Segment-Anything-Model-for-Molecular-empowered-Learning"><a href="#Leverage-Weakly-Annotation-to-Pixel-wise-Annotation-via-Zero-shot-Segment-Anything-Model-for-Molecular-empowered-Learning" class="headerlink" title="Leverage Weakly Annotation to Pixel-wise Annotation via Zero-shot Segment Anything Model for Molecular-empowered Learning"></a>Leverage Weakly Annotation to Pixel-wise Annotation via Zero-shot Segment Anything Model for Molecular-empowered Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05785">http://arxiv.org/abs/2308.05785</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xueyuan Li, Ruining Deng, Yucheng Tang, Shunxing Bao, Haichun Yang, Yuankai Huo</li>
<li>for: 这个研究旨在发展一种可以实现批量标注的人工智能方法，以便将数位实验室数组据数据集（Giga-pixel Whole Slide Imaging，WSI）中的多个细胞型态识别为精确为可能。</li>
<li>methods: 这个研究使用了一种名为“对应强化学习”的方法，即使用强化学习来将类别为细胞型态的图像转换为对应的细胞类别。此外，还使用了一种名为“强制标注”的方法，即将图像中的细胞类别标注为精确的细胞类别。</li>
<li>results: 研究结果显示，使用“对应强化学习”和“强制标注”的方法可以实现对细胞型态的精确识别，并且可以降低非专家标注者的努力，只需要对图像进行简单的标注。此外，这种方法并不会对数据集的质量产生影响。<details>
<summary>Abstract</summary>
Precise identification of multiple cell classes in high-resolution Giga-pixel whole slide imaging (WSI) is critical for various clinical scenarios. Building an AI model for this purpose typically requires pixel-level annotations, which are often unscalable and must be done by skilled domain experts (e.g., pathologists). However, these annotations can be prone to errors, especially when distinguishing between intricate cell types (e.g., podocytes and mesangial cells) using only visual inspection. Interestingly, a recent study showed that lay annotators, when using extra immunofluorescence (IF) images for reference (referred to as molecular-empowered learning), can sometimes outperform domain experts in labeling. Despite this, the resource-intensive task of manual delineation remains a necessity during the annotation process. In this paper, we explore the potential of bypassing pixel-level delineation by employing the recent segment anything model (SAM) on weak box annotation in a zero-shot learning approach. Specifically, we harness SAM's ability to produce pixel-level annotations from box annotations and utilize these SAM-generated labels to train a segmentation model. Our findings show that the proposed SAM-assisted molecular-empowered learning (SAM-L) can diminish the labeling efforts for lay annotators by only requiring weak box annotations. This is achieved without compromising annotation accuracy or the performance of the deep learning-based segmentation. This research represents a significant advancement in democratizing the annotation process for training pathological image segmentation, relying solely on non-expert annotators.
</details>
<details>
<summary>摘要</summary>
高精度整个扫描图像（WSI）中多个细胞类型的精准识别是许多临床应用场景中的关键。建立AI模型用于这种目的通常需要像素级别的标注，但这些标注通常是不可扩展的并且需要具备专业知识（如病理学家）进行完成。然而，这些标注可能存在错误，特别是在辨别复杂细胞类型（如 podocytes 和 mesangial cells）时使用 только视觉检查。让人感兴奋的是，一项最近的研究表示，使用附加的免疫抗体（IF）图像作参考（称为分子驱动学习），非专业标注人员可以在标注时与专业人员相比表现更出色。尽管这样，手动分割任务仍然是标注过程中的必需任务。在这篇论文中，我们探讨使用最近的分类任何事物模型（SAM）在零容量学习方法中绕过像素级别分割的可能性。我们利用SAM的能力生成像素级别标注从箱子标注，并使用这些SAM生成的标注来训练分割模型。我们的发现表明，我们的SAM-助け学习（SAM-L）可以减少非专业标注人员的标注努力，只需要弱型箱子标注。这是在不妨害标注精度或深度学习基于图像分割的性能下进行的。这项研究表明了在训练病理图像分割模型时，不需要专业人员进行标注，可以仅仅通过非专业标注人员完成。
</details></li>
</ul>
<hr>
<h2 id="High-performance-Data-Management-for-Whole-Slide-Image-Analysis-in-Digital-Pathology"><a href="#High-performance-Data-Management-for-Whole-Slide-Image-Analysis-in-Digital-Pathology" class="headerlink" title="High-performance Data Management for Whole Slide Image Analysis in Digital Pathology"></a>High-performance Data Management for Whole Slide Image Analysis in Digital Pathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05784">http://arxiv.org/abs/2308.05784</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hrlblab/adios">https://github.com/hrlblab/adios</a></li>
<li>paper_authors: Haoju Leng, Ruining Deng, Shunxing Bao, Dazheng Fang, Bryan A. Millis, Yucheng Tang, Haichun Yang, Xiao Wang, Yifan Peng, Lipeng Wan, Yuankai Huo</li>
<li>For: The paper is written to address the computational bottleneck in input-output (I&#x2F;O) system when deploying image analysis algorithms on whole-slide images (WSIs).* Methods: The paper proposes the use of Adaptable IO System version 2 (ADIOS2) to streamline data management across WSIs and reduce data retrieval times.* Results: The paper shows that ADIOS2 achieves a two-fold speed-up compared to the brute-force approach in a CPU-based image analysis scenario, and its performance is on par with the cutting-edge GPU I&#x2F;O acceleration framework, NVIDIA Magnum IO GPU Direct Storage (GDS), in a GPU-based deep learning framework scenario.Here are the three points in Simplified Chinese text:* For: 本文是为了解决扫描图像分析过程中的计算瓶颈，尤其是在扫描图像（WSIs）上部署图像分析算法时。* Methods: 本文提出使用Adaptable IO System version 2（ADIOS2）来协调数据管理过程，以便更好地处理扫描图像中的数据访问。* Results: 本文显示，使用ADIOS2可以将计算瓶颈降低到一半，并且在使用深度学习框架时与NVIDIA Magnum IO GPU Direct Storage（GDS）的性能相当。<details>
<summary>Abstract</summary>
When dealing with giga-pixel digital pathology in whole-slide imaging, a notable proportion of data records holds relevance during each analysis operation. For instance, when deploying an image analysis algorithm on whole-slide images (WSI), the computational bottleneck often lies in the input-output (I/O) system. This is particularly notable as patch-level processing introduces a considerable I/O load onto the computer system. However, this data management process could be further paralleled, given the typical independence of patch-level image processes across different patches. This paper details our endeavors in tackling this data access challenge by implementing the Adaptable IO System version 2 (ADIOS2). Our focus has been constructing and releasing a digital pathology-centric pipeline using ADIOS2, which facilitates streamlined data management across WSIs. Additionally, we've developed strategies aimed at curtailing data retrieval times. The performance evaluation encompasses two key scenarios: (1) a pure CPU-based image analysis scenario ("CPU scenario"), and (2) a GPU-based deep learning framework scenario ("GPU scenario"). Our findings reveal noteworthy outcomes. Under the CPU scenario, ADIOS2 showcases an impressive two-fold speed-up compared to the brute-force approach. In the GPU scenario, its performance stands on par with the cutting-edge GPU I/O acceleration framework, NVIDIA Magnum IO GPU Direct Storage (GDS). From what we know, this appears to be among the initial instances, if any, of utilizing ADIOS2 within the field of digital pathology. The source code has been made publicly available at https://github.com/hrlblab/adios.
</details>
<details>
<summary>摘要</summary>
当处理 gigapixel 数字 PATHOLOGY 整个扫描图像时，一定比例的数据记录在每次分析操作中保持有效。例如，在应用图像分析算法于整个扫描图像（WSI）时，计算机系统的瓶颈通常出现在输入输出（I/O）系统中。特别是在使用 patch-level 处理时， introduce 一个较大的 I/O 负担到计算机系统中。但是，这个数据管理过程可以进一步并行化，因为不同的 patch 之间的图像处理通常具有一定的独立性。这篇论文详细介绍了我们在解决这个数据访问挑战方面的努力，我们实现了基于 ADIOS2 的数字 PATHOLOGY 中心管道，以便在 WSI 之间进行流畅的数据管理。此外，我们还开发了降低数据检索时间的策略。我们的性能评估包括两个关键场景：（1） CPU 场景（CPU 场景），和（2） GPU 场景（GPU 场景）。我们的发现表明了一些有趣的结果。在 CPU 场景下，ADIOS2 与简洁方法相比，显示出了很好的两倍速度提升。在 GPU 场景下，ADIOS2 的性能与当前最先进的 GPU I/O 加速框架，NVIDIA Magnum IO GPU Direct Storage（GDS）相当。我们知道，这可能是数字 PATHOLOGY 领域中首次使用 ADIOS2 的情况之一，如果不是唯一的。我们在 <https://github.com/hrlblab/adios> 上公开了源代码。
</details></li>
</ul>
<hr>
<h2 id="Multi-scale-Multi-site-Renal-Microvascular-Structures-Segmentation-for-Whole-Slide-Imaging-in-Renal-Pathology"><a href="#Multi-scale-Multi-site-Renal-Microvascular-Structures-Segmentation-for-Whole-Slide-Imaging-in-Renal-Pathology" class="headerlink" title="Multi-scale Multi-site Renal Microvascular Structures Segmentation for Whole Slide Imaging in Renal Pathology"></a>Multi-scale Multi-site Renal Microvascular Structures Segmentation for Whole Slide Imaging in Renal Pathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05782">http://arxiv.org/abs/2308.05782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Franklin Hu, Ruining Deng, Shunxing Bao, Haichun Yang, Yuankai Huo</li>
<li>for:  This paper is written for renal pathologists who need a computational tool for the quantitative analysis of renal microvascular structures.</li>
<li>methods:  The paper uses a novel single dynamic network method called Omni-Seg, which capitalizes on multi-site, multi-scale training data and utilizes partially labeled images to segment microvascular structures.</li>
<li>results:  The experimental results indicate that Omni-Seg outperforms other methods in terms of both the Dice Similarity Coefficient (DSC) and Intersection over Union (IoU).<details>
<summary>Abstract</summary>
Segmentation of microvascular structures, such as arterioles, venules, and capillaries, from human kidney whole slide images (WSI) has become a focal point in renal pathology. Current manual segmentation techniques are time-consuming and not feasible for large-scale digital pathology images. While deep learning-based methods offer a solution for automatic segmentation, most suffer from a limitation: they are designed for and restricted to training on single-site, single-scale data. In this paper, we present Omni-Seg, a novel single dynamic network method that capitalizes on multi-site, multi-scale training data. Unique to our approach, we utilize partially labeled images, where only one tissue type is labeled per training image, to segment microvascular structures. We train a singular deep network using images from two datasets, HuBMAP and NEPTUNE, across different magnifications (40x, 20x, 10x, and 5x). Experimental results indicate that Omni-Seg outperforms in terms of both the Dice Similarity Coefficient (DSC) and Intersection over Union (IoU). Our proposed method provides renal pathologists with a powerful computational tool for the quantitative analysis of renal microvascular structures.
</details>
<details>
<summary>摘要</summary>
Segmentation of microvascular structures, such as arterioles, venules, and capillaries, from human kidney whole slide images (WSI) has become a focal point in renal pathology. Current manual segmentation techniques are time-consuming and not feasible for large-scale digital pathology images. While deep learning-based methods offer a solution for automatic segmentation, most suffer from a limitation: they are designed for and restricted to training on single-site, single-scale data. In this paper, we present Omni-Seg, a novel single dynamic network method that capitalizes on multi-site, multi-scale training data. Unique to our approach, we utilize partially labeled images, where only one tissue type is labeled per training image, to segment microvascular structures. We train a singular deep network using images from two datasets, HuBMAP and NEPTUNE, across different magnifications (40x, 20x, 10x, and 5x). Experimental results indicate that Omni-Seg outperforms in terms of both the Dice Similarity Coefficient (DSC) and Intersection over Union (IoU). Our proposed method provides renal pathologists with a powerful computational tool for the quantitative analysis of renal microvascular structures.Here's the translation in Traditional Chinese: segmentation of microvascular structures, such as arterioles, venules, and capillaries, from human kidney whole slide images (WSI) has become a focal point in renal pathology. Current manual segmentation techniques are time-consuming and not feasible for large-scale digital pathology images. While deep learning-based methods offer a solution for automatic segmentation, most suffer from a limitation: they are designed for and restricted to training on single-site, single-scale data. In this paper, we present Omni-Seg, a novel single dynamic network method that capitalizes on multi-site, multi-scale training data. Unique to our approach, we utilize partially labeled images, where only one tissue type is labeled per training image, to segment microvascular structures. We train a singular deep network using images from two datasets, HuBMAP and NEPTUNE, across different magnifications (40x, 20x, 10x, and 5x). Experimental results indicate that Omni-Seg outperforms in terms of both the Dice Similarity Coefficient (DSC) and Intersection over Union (IoU). Our proposed method provides renal pathologists with a powerful computational tool for the quantitative analysis of renal microvascular structures.
</details></li>
</ul>
<hr>
<h2 id="2D3D-MATR-2D-3D-Matching-Transformer-for-Detection-free-Registration-between-Images-and-Point-Clouds"><a href="#2D3D-MATR-2D-3D-Matching-Transformer-for-Detection-free-Registration-between-Images-and-Point-Clouds" class="headerlink" title="2D3D-MATR: 2D-3D Matching Transformer for Detection-free Registration between Images and Point Clouds"></a>2D3D-MATR: 2D-3D Matching Transformer for Detection-free Registration between Images and Point Clouds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05667">http://arxiv.org/abs/2308.05667</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minhao Li, Zheng Qin, Zhirui Gao, Renjiao Yi, Chenyang Zhu, Yulan Guo, Kai Xu</li>
<li>for: 准确和稳定的图像和点云对接问题（cross-modality registration）</li>
<li>methods: 提出了一种检测器-自由的方法（detection-free method），即2D3D-MATR，它首先在下采样后的图像和点云之间计算粗略匹配，然后将其扩展到形成密集匹配。该方法采用了一个巨量感知机制，将图像和点云之间的全局上下文约束和跨Modalities的相关性约束都带入学习。</li>
<li>results: 在两个公共测试集上进行了广泛的实验，证明了2D3D-MATR比前一个状态的P2-Net提高了约20个百分点的匹配率和10个百分点的对接回归率。<details>
<summary>Abstract</summary>
The commonly adopted detect-then-match approach to registration finds difficulties in the cross-modality cases due to the incompatible keypoint detection and inconsistent feature description. We propose, 2D3D-MATR, a detection-free method for accurate and robust registration between images and point clouds. Our method adopts a coarse-to-fine pipeline where it first computes coarse correspondences between downsampled patches of the input image and the point cloud and then extends them to form dense correspondences between pixels and points within the patch region. The coarse-level patch matching is based on transformer which jointly learns global contextual constraints with self-attention and cross-modality correlations with cross-attention. To resolve the scale ambiguity in patch matching, we construct a multi-scale pyramid for each image patch and learn to find for each point patch the best matching image patch at a proper resolution level. Extensive experiments on two public benchmarks demonstrate that 2D3D-MATR outperforms the previous state-of-the-art P2-Net by around $20$ percentage points on inlier ratio and over $10$ points on registration recall. Our code and models are available at https://github.com/minhaolee/2D3DMATR.
</details>
<details>
<summary>摘要</summary>
通常采用检测然后匹配的方法在跨Modalities的情况下遇到困难，这是因为针对不同模式的关键点检测和特征描述不兼容。我们提议了一种没有检测的方法，即2D3D-MATR，用于准确和可靠地将图像和点云进行registrations。我们的方法采用一个分支扩充的管道，首先计算下采样的图像和点云之间的粗略匹配，然后将其扩展到形成点云和图像之间的密集匹配。粗略水平的patch匹配基于转换器，它同时学习全局上下文约束和自我注意力以及跨Modalities的相关性。为解决粗略水平的匹配抖动问题，我们构建了每个图像区域的多尺度 pyramid，并学习找到每个点云区域的最佳匹配图像区域的合适的分辨率水平。我们的实验表明，2D3D-MATR比前一个状态的P2-Net提高了约20个百分点的匹配率和10个百分点的注册回归率。我们的代码和模型可以在https://github.com/minhaolee/2D3DMATR中获取。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/11/cs.CV_2023_08_11/" data-id="clpxp03yr00i5fm88853f73p6" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_08_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/11/cs.AI_2023_08_11/" class="article-date">
  <time datetime="2023-08-11T12:00:00.000Z" itemprop="datePublished">2023-08-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/11/cs.AI_2023_08_11/">cs.AI - 2023-08-11</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Towards-a-Causal-Probabilistic-Framework-for-Prediction-Action-Selection-Explanations-for-Robot-Block-Stacking-Tasks"><a href="#Towards-a-Causal-Probabilistic-Framework-for-Prediction-Action-Selection-Explanations-for-Robot-Block-Stacking-Tasks" class="headerlink" title="Towards a Causal Probabilistic Framework for Prediction, Action-Selection &amp; Explanations for Robot Block-Stacking Tasks"></a>Towards a Causal Probabilistic Framework for Prediction, Action-Selection &amp; Explanations for Robot Block-Stacking Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06203">http://arxiv.org/abs/2308.06203</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ricardo Cannizzaro, Jonathan Routley, Lars Kunze</li>
<li>for: 这个论文的目的是提供一种 causal probabilistic 框架，用于嵌入物理模拟能力到STRUCTURAL causal model 中，以便 robot 可以在堆叠任务中进行现状识别、下一步操作选择和后果解释。</li>
<li>methods: 该论文使用 causal inference 和 Bayesian networks 来编码形式化的 causal 关系知识，并将其与 probabilistic 表示方法结合使用。它还使用 physics simulation 来模拟堆叠任务的当前状态，并提出了一种基于 counterfactual explanation 的 post-hoc 解释方法。</li>
<li>results: 论文提出了一种 novel causal probabilistic 框架，并在 simulated 和实际的 robot 堆叠任务中提供了 exemplar 的 next-best action 选择结果。它还将在未来进行实验证明和扩展。<details>
<summary>Abstract</summary>
Uncertainties in the real world mean that is impossible for system designers to anticipate and explicitly design for all scenarios that a robot might encounter. Thus, robots designed like this are fragile and fail outside of highly-controlled environments. Causal models provide a principled framework to encode formal knowledge of the causal relationships that govern the robot's interaction with its environment, in addition to probabilistic representations of noise and uncertainty typically encountered by real-world robots. Combined with causal inference, these models permit an autonomous agent to understand, reason about, and explain its environment. In this work, we focus on the problem of a robot block-stacking task due to the fundamental perception and manipulation capabilities it demonstrates, required by many applications including warehouse logistics and domestic human support robotics. We propose a novel causal probabilistic framework to embed a physics simulation capability into a structural causal model to permit robots to perceive and assess the current state of a block-stacking task, reason about the next-best action from placement candidates, and generate post-hoc counterfactual explanations. We provide exemplar next-best action selection results and outline planned experimentation in simulated and real-world robot block-stacking tasks.
</details>
<details>
<summary>摘要</summary>
real-world uncertainty 意味着 robot 设计人员无法预测和专门设计 robot 可能遇到的所有场景。因此，以前的 robot 设计是脆弱的，只能在高度控制的环境中工作。 causal 模型提供了一个原则的框架，用于编码 robot 与环境的 causal 关系，以及常见的real-world Robot 遇到的抽象表示 uncertainty 和不确定性。 combined with causal inference, these models permit an autonomous agent to understand, reason about, and explain its environment.在这项工作中，我们关注了一个 robot 块堆叠任务，因为它需要 robot 拥有的基本感知和操作能力，这些能力是许多应用程序，包括仓库自动化和家庭支持 robotics 所需的。我们提出了一种新的 causal 概率框架，用于嵌入物理模拟能力到结构 causal 模型中，使 robot 能够识别和评估块堆叠任务的当前状态，从选择候选地点中选择下一个行动，并生成post-hoc counterfactual explanations。我们提供了示例的下一个行动选择结果，并详细介绍计划的实验在模拟和实际 robot 块堆叠任务中。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Predicate-Visual-Context-in-Detecting-of-Human-Object-Interactions"><a href="#Exploring-Predicate-Visual-Context-in-Detecting-of-Human-Object-Interactions" class="headerlink" title="Exploring Predicate Visual Context in Detecting of Human-Object Interactions"></a>Exploring Predicate Visual Context in Detecting of Human-Object Interactions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06202">http://arxiv.org/abs/2308.06202</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fredzzhang/pvic">https://github.com/fredzzhang/pvic</a></li>
<li>paper_authors: Frederic Z. Zhang, Yuhui Yuan, Dylan Campbell, Zhuoyao Zhong, Stephen Gould</li>
<li>for: 这 paper 的目的是研究人–物交互 (HOI) 问题，尤其是使用 two-stage transformer-based HOI detectors。</li>
<li>methods: 这 paper 使用 visualisations 和仔细的实验来研究如何重新引入图像特征，并通过改进查询设计、广泛探索键和值、以及盒子对位嵌入为空间指导来提高 predicate visual context (PViC)。</li>
<li>results: 这 paper 在 HICO-DET 和 V-COCO 测试集上表现出色，比前一代方法高效，同时保持低训练成本。<details>
<summary>Abstract</summary>
Recently, the DETR framework has emerged as the dominant approach for human--object interaction (HOI) research. In particular, two-stage transformer-based HOI detectors are amongst the most performant and training-efficient approaches. However, these often condition HOI classification on object features that lack fine-grained contextual information, eschewing pose and orientation information in favour of visual cues about object identity and box extremities. This naturally hinders the recognition of complex or ambiguous interactions. In this work, we study these issues through visualisations and carefully designed experiments. Accordingly, we investigate how best to re-introduce image features via cross-attention. With an improved query design, extensive exploration of keys and values, and box pair positional embeddings as spatial guidance, our model with enhanced predicate visual context (PViC) outperforms state-of-the-art methods on the HICO-DET and V-COCO benchmarks, while maintaining low training cost.
</details>
<details>
<summary>摘要</summary>
Here's the Simplified Chinese translation:最近，DETR框架在人物交互（HOI）研究中成为主流方法。特别是两stage转换器基于HOI检测器在性能和训练效率方面表现非常出色。然而，这些模型通常会基于缺乏细化上下文信息的对象特征进行分类，忽略对象的姿势和方向信息，而依赖视觉特征来判断对象的标识和边框端点。这会导致复杂或抽象的交互不能正确识别。在这个工作中，我们通过视觉化和仔细设计的实验来研究这些问题。我们研究如何通过交叉注意来重新引入图像特征，使用改进的查询设计、广泛探索的键和值、以及为空间导航而添加的盒对位嵌入。我们的模型具有加强 predicate visual context（PViC），在HICO-DET和V-COCO测试benchmark上显示出优于当前最佳方法，而且保持训练成本低。
</details></li>
</ul>
<hr>
<h2 id="Complex-Facial-Expression-Recognition-Using-Deep-Knowledge-Distillation-of-Basic-Features"><a href="#Complex-Facial-Expression-Recognition-Using-Deep-Knowledge-Distillation-of-Basic-Features" class="headerlink" title="Complex Facial Expression Recognition Using Deep Knowledge Distillation of Basic Features"></a>Complex Facial Expression Recognition Using Deep Knowledge Distillation of Basic Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06197">http://arxiv.org/abs/2308.06197</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/angusmaiden/complex-fer">https://github.com/angusmaiden/complex-fer</a></li>
<li>paper_authors: Angus Maiden, Bahareh Nakisa</li>
<li>for: 这个论文的目的是提出一种基于人类认知和学习的新型连续学习方法，以便准确地识别新的复杂表情类别使用少量示例。</li>
<li>methods: 该方法基于人类认知和学习，包括知识储存、知识总结和预测排序记忆等技术。它还使用 GradCAM 可视化来解释基本和复杂表情之间的关系。</li>
<li>results: 该方法可以准确地识别新的复杂表情类别，并且在连续学习中保持知识的稳定性。在新类别上达到了74.28%的总准确率，比非连续学习方法提高13.95%。此外，该方法还应用了几个示例学习来达到了100%的准确率。<details>
<summary>Abstract</summary>
Complex emotion recognition is a cognitive task that has so far eluded the same excellent performance of other tasks that are at or above the level of human cognition. Emotion recognition through facial expressions is particularly difficult due to the complexity of emotions expressed by the human face. For a machine to approach the same level of performance in this domain as a human, it may need to synthesise knowledge and understand new concepts in real-time as humans do. Humans are able to learn new concepts using only few examples, by distilling the important information from memories and discarding the rest. Similarly, continual learning methods learn new classes whilst retaining the knowledge of known classes, whilst few-shot learning methods are able to learn new classes using very few training examples. We propose a novel continual learning method inspired by human cognition and learning that can accurately recognise new compound expression classes using few training samples, by building on and retaining its knowledge of basic expression classes. Using GradCAM visualisations, we demonstrate the relationship between basic and compound facial expressions, which our method leverages through knowledge distillation and a novel Predictive Sorting Memory Replay. Our method achieves the current state-of-the-art in continual learning for complex facial expression recognition with 74.28% Overall Accuracy on new classes. We also demonstrate that using continual learning for complex facial expression recognition achieves far better performance than non-continual learning methods, improving on state-of-the-art non-continual learning methods by 13.95%. To the best of our knowledge, our work is also the first to apply few-shot learning to complex facial expression recognition, achieving the state-of-the-art with 100% accuracy using a single training sample for each expression class.
</details>
<details>
<summary>摘要</summary>
人工智能recognition of complex emotions是一个艰难的认知任务，尤其是通过脸部表达来认出情感。由于人脸上表达的情感复杂，因此机器需要同人类一样快速学习和捕捉新的概念。人类通过少量示例学习新的概念，并将重要信息提取出来，而不是把所有信息都记忆下来。我们提出了一种基于人类认知和学习的新型不断学习方法，可以准确地识别新的复杂表情类别，使用很少的训练样本。我们使用GradCAM视觉化来表示基本和复杂表情之间的关系，并通过知识储存和一种新的预测排序记忆来利用这种关系。我们的方法实现了当前领域的最佳性能，新类别的总准确率为74.28%。我们还证明了不断学习对复杂表情认知的表现远胜非不断学习方法，提高了状态之前的最佳非不断学习方法的13.95%。而且，我们是第一次将几 shot learning应用于复杂表情认知，实现了100%的准确率，只需用每个表情类别的一个训练样本。
</details></li>
</ul>
<hr>
<h2 id="Software-Doping-Analysis-for-Human-Oversight"><a href="#Software-Doping-Analysis-for-Human-Oversight" class="headerlink" title="Software Doping Analysis for Human Oversight"></a>Software Doping Analysis for Human Oversight</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06186">http://arxiv.org/abs/2308.06186</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sebastian Biewer, Kevin Baum, Sarah Sterz, Holger Hermanns, Sven Hetmank, Markus Langer, Anne Lauber-Rönsberg, Franz Lehr</li>
<li>for: 本文提出了一个框架，用于减少软件对社会 pose 的风险。这包括软件毒品和不公正的决策系统中的不公正和歧视。</li>
<li>methods: 本文结合了软件毒品分析的正式基础和概率证明技术，提出了一种黑盒分析技术，用于发现软件中的不良效果。这种技术应用于柴油车的排放清洁系统以及高风险决策系统中的人类评审。</li>
<li>results: 本文的方法可以帮助人类监督者更好地做出更负责任的决策，以促进更有效的人类监督。此外，文章还提供了一个法律、哲学和心理学等多方面的视角，探讨软件 pose 对社会的 potential 问题。<details>
<summary>Abstract</summary>
This article introduces a framework that is meant to assist in mitigating societal risks that software can pose. Concretely, this encompasses facets of software doping as well as unfairness and discrimination in high-risk decision-making systems. The term software doping refers to software that contains surreptitiously added functionality that is against the interest of the user. A prominent example of software doping are the tampered emission cleaning systems that were found in millions of cars around the world when the diesel emissions scandal surfaced. The first part of this article combines the formal foundations of software doping analysis with established probabilistic falsification techniques to arrive at a black-box analysis technique for identifying undesired effects of software. We apply this technique to emission cleaning systems in diesel cars but also to high-risk systems that evaluate humans in a possibly unfair or discriminating way. We demonstrate how our approach can assist humans-in-the-loop to make better informed and more responsible decisions. This is to promote effective human oversight, which will be a central requirement enforced by the European Union's upcoming AI Act. We complement our technical contribution with a juridically, philosophically, and psychologically informed perspective on the potential problems caused by such systems.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Physical-Adversarial-Attacks-For-Camera-based-Smart-Systems-Current-Trends-Categorization-Applications-Research-Challenges-and-Future-Outlook"><a href="#Physical-Adversarial-Attacks-For-Camera-based-Smart-Systems-Current-Trends-Categorization-Applications-Research-Challenges-and-Future-Outlook" class="headerlink" title="Physical Adversarial Attacks For Camera-based Smart Systems: Current Trends, Categorization, Applications, Research Challenges, and Future Outlook"></a>Physical Adversarial Attacks For Camera-based Smart Systems: Current Trends, Categorization, Applications, Research Challenges, and Future Outlook</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06173">http://arxiv.org/abs/2308.06173</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amira Guesmi, Muhammad Abdullah Hanif, Bassem Ouni, Muhammed Shafique</li>
<li>for: 本文提供了物理敌意攻击的全面评估，强调特征和应用场景，以便为研究人员、实践者和政策制定者提供一份有价值的资源，以掌握物理敌意攻击的全面特征和挑战，并为建立可靠和安全的深度学习系统而做出贡献。</li>
<li>methods: 本文涵盖了物理敌意攻击的多种方法，包括分类、检测、人脸识别、语义分割和深度估计等应用场景，并对这些攻击方法进行了效果、隐蔽性和稳定性的评估。</li>
<li>results: 本文对物理敌意整体攻击的效果进行了评估，并发现这些攻击方法在不同的应用场景中的表现强度不同，同时也发现了这些攻击方法的潜在漏洞和可能的应对策略。<details>
<summary>Abstract</summary>
In this paper, we present a comprehensive survey of the current trends focusing specifically on physical adversarial attacks. We aim to provide a thorough understanding of the concept of physical adversarial attacks, analyzing their key characteristics and distinguishing features. Furthermore, we explore the specific requirements and challenges associated with executing attacks in the physical world. Our article delves into various physical adversarial attack methods, categorized according to their target tasks in different applications, including classification, detection, face recognition, semantic segmentation and depth estimation. We assess the performance of these attack methods in terms of their effectiveness, stealthiness, and robustness. We examine how each technique strives to ensure the successful manipulation of DNNs while mitigating the risk of detection and withstanding real-world distortions. Lastly, we discuss the current challenges and outline potential future research directions in the field of physical adversarial attacks. We highlight the need for enhanced defense mechanisms, the exploration of novel attack strategies, the evaluation of attacks in different application domains, and the establishment of standardized benchmarks and evaluation criteria for physical adversarial attacks. Through this comprehensive survey, we aim to provide a valuable resource for researchers, practitioners, and policymakers to gain a holistic understanding of physical adversarial attacks in computer vision and facilitate the development of robust and secure DNN-based systems.
</details>
<details>
<summary>摘要</summary>
在本文中，我们提供了物理抗击攻击的全面报告，强调特定领域的当前趋势。我们的目标是为抗击攻击提供全面的理解，分析其关键特征和区别。此外，我们还探讨了在物理世界中执行攻击的具体要求和挑战。我们的文章探讨了不同应用领域中的物理抗击攻击方法，分为不同的目标任务，包括分类、检测、脸Recognition、 semantic segmentation 和深度估计。我们评估了这些攻击方法的效果、隐蔽性和可靠性。我们检查了每种技术如何在抗击DNN的同时避免检测和实际世界的扭曲。最后，我们讨论了现有的挑战和未来研究方向，包括增强防御机制、探索新的攻击策略、在不同应用领域中评估攻击、并建立DNN领域的标准化测试基准和评价标准。通过这篇全面报告，我们希望为研究人员、实践者和政策制定者提供一份有价值的资源，以便更好地理解物理抗击攻击，并促进robust和安全的DNN基于系统的发展。
</details></li>
</ul>
<hr>
<h2 id="Phased-Deep-Spatio-temporal-Learning-for-Highway-Traffic-Volume-Prediction"><a href="#Phased-Deep-Spatio-temporal-Learning-for-Highway-Traffic-Volume-Prediction" class="headerlink" title="Phased Deep Spatio-temporal Learning for Highway Traffic Volume Prediction"></a>Phased Deep Spatio-temporal Learning for Highway Traffic Volume Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06155">http://arxiv.org/abs/2308.06155</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weilong Ding, Tianpu Zhang, Zhe Wang</li>
<li>for: 预测城市高速公路交通量，提高城市现代生活质量。</li>
<li>methods: 使用深度空间时间学习方法，包括数据准备阶段、空间时间学习阶段和决策阶段，其中数据准备阶段通过精心normal化数据来抑制长尾分布，空间时间学习阶段使用FCN和LSTM组合模型考虑时空天气和历法特征，决策阶段通过对来自不同数据源的交通量进行权重平均来预测下一天城市高速公路交通量。</li>
<li>results: 使用实际数据进行了广泛的实验，结果显示该方法在预测城市高速公路交通量方面具有显著的改进，其中MPAE和R-squre度量达到5.269和0.997 respectively。<details>
<summary>Abstract</summary>
Inter-city highway transportation is significant for citizens' modern urban life and generates heterogeneous sensory data with spatio-temporal characteristics. As a routine analysis in transportation domain, daily traffic volume estimation faces challenges for highway toll stations including lacking of exploration of correlative spatio-temporal features from a long-term perspective and effective means to deal with data imbalance which always deteriorates the predictive performance. In this paper, a deep spatio-temporal learning method is proposed to predict daily traffic volume in three phases. In feature pre-processing phase, data is normalized elaborately according to latent long-tail distribution. In spatio-temporal learning phase, a hybrid model is employed combining fully convolution network (FCN) and long short-term memory (LSTM), which considers time, space, meteorology, and calendar from heterogeneous data. In decision phase, traffic volumes on a coming day at network-wide toll stations would be achieved effectively, which is especially calibrated for vital few highway stations. Using real-world data from one Chinese provincial highway, extensive experiments show our method has distinct improvement for predictive accuracy than various traditional models, reaching 5.269 and 0.997 in MPAE and R-squre metrics, respectively.
</details>
<details>
<summary>摘要</summary>
Modern urban жизнь的城市间交通运输非常重要，生成了多样化的感知数据，具有空间时间特征。为了解决高速公路收费站的日常交通量预测问题，包括缺乏长期纵探感知特征的探索和数据不均衡问题，这些问题都会导致预测性能下降。本文提出了深度空间时间学习方法，用于预测高速公路交通量。在特征预处理阶段，数据进行了精心Normalization，根据异常长尾分布。在空间时间学习阶段，我们使用了混合FCN和LSTM模型，考虑了时间、空间、气象和日历等多种多样数据。在决策阶段，通过对整个高速公路网络的收费站进行有效预测，实现了对重要的一些高速公路站点的准确预测。使用了一个中国省级高速公路的实际数据，我们进行了广泛的实验，并证明了我们的方法在预测精度方面与传统模型有显著提升，分别达到5.269和0.997的MPAE和R-squre指标。
</details></li>
</ul>
<hr>
<h2 id="Application-of-Artificial-Neural-Networks-for-Investigation-of-Pressure-Filtration-Performance-a-Zinc-Leaching-Filter-Cake-Moisture-Modeling"><a href="#Application-of-Artificial-Neural-Networks-for-Investigation-of-Pressure-Filtration-Performance-a-Zinc-Leaching-Filter-Cake-Moisture-Modeling" class="headerlink" title="Application of Artificial Neural Networks for Investigation of Pressure Filtration Performance, a Zinc Leaching Filter Cake Moisture Modeling"></a>Application of Artificial Neural Networks for Investigation of Pressure Filtration Performance, a Zinc Leaching Filter Cake Moisture Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06138">http://arxiv.org/abs/2308.06138</a></li>
<li>repo_url: None</li>
<li>paper_authors: Masoume Kazemi, Davood Moradkhani, Alireza A. Alipour</li>
<li>for: 这个研究的目的是开发一个人工神经网络模型，用于预测氧气压 фильtration过程中铅生产中的蛋白湿度。</li>
<li>methods: 这个研究使用了人工神经网络模型，并在288个试验中测试了两种不同的织物（S1和S2）。</li>
<li>results: 研究发现，使用人工神经网络模型可以高度准确地预测氧气压 фильtration过程中铅生产中的蛋白湿度，其R2值为0.88和0.83，MSE值为6.243x10-07和1.086x10-06，MAE值为0.00056和0.00088。<details>
<summary>Abstract</summary>
Machine Learning (ML) is a powerful tool for material science applications. Artificial Neural Network (ANN) is a machine learning technique that can provide high prediction accuracy. This study aimed to develop an ANN model to predict the cake moisture of the pressure filtration process of zinc production. The cake moisture was influenced by seven parameters: temperature (35 and 65 Celsius), solid concentration (0.2 and 0.38 g/L), pH (2, 3.5, and 5), air-blow time (2, 10, and 15 min), cake thickness (14, 20, 26, and 34 mm), pressure, and filtration time. The study conducted 288 tests using two types of fabrics: polypropylene (S1) and polyester (S2). The ANN model was evaluated by the Coefficient of determination (R2), the Mean Square Error (MSE), and the Mean Absolute Error (MAE) metrics for both datasets. The results showed R2 values of 0.88 and 0.83, MSE values of 6.243x10-07 and 1.086x10-06, and MAE values of 0.00056 and 0.00088 for S1 and S2, respectively. These results indicated that the ANN model could predict the cake moisture of pressure filtration in the zinc leaching process with high accuracy.
</details>
<details>
<summary>摘要</summary>
machine learning (ml) 是一种强大的工具 для材料科学应用。人工神经网络 (ann) 是一种机器学习技术，可以提供高精度预测。本研究目的是开发一个 ann 模型，以预测压 filtered 过程中锻 production 中糕点湿度。糕点湿度受到七个参数的影响：温度 (35 和 65 摄氏度), 固体浓度 (0.2 和 0.38 g/L), pH (2, 3.5, 和 5), 空气喷压时间 (2, 10, 和 15 分), 糕点厚度 (14, 20, 26, 和 34 mm), 压力, 和过滤时间。研究进行了 288 个测试，使用了两种不同的 fabrics： polypropylene (S1) 和 polyester (S2)。 ann 模型被评估使用 coefficient of determination (R2), mean square error (MSE), 和 mean absolute error (MAE) 度量器，对于两个数据集。结果表明，R2 值为 0.88 和 0.83，MSE 值为 6.243x10-07 和 1.086x10-06，MAE 值为 0.00056 和 0.00088，分别对 S1 和 S2 数据集。这些结果表明，ann 模型可以准确预测压 filtered 过程中锻 production 中糕点湿度。
</details></li>
</ul>
<hr>
<h2 id="A-Game-Theoretic-Framework-for-Joint-Forecasting-and-Planning"><a href="#A-Game-Theoretic-Framework-for-Joint-Forecasting-and-Planning" class="headerlink" title="A Game-Theoretic Framework for Joint Forecasting and Planning"></a>A Game-Theoretic Framework for Joint Forecasting and Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06137">http://arxiv.org/abs/2308.06137</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/portal-cornell/game-theoretic-forecasting-planning">https://github.com/portal-cornell/game-theoretic-forecasting-planning</a></li>
<li>paper_authors: Kushal Kedia, Prithwish Dan, Sanjiban Choudhury</li>
<li>for: 本研究旨在提供一种基于游戏理论的规划和预测方法，以提高机器人在人类存在下的安全规划。</li>
<li>methods: 本研究使用了一种新的游戏理论基础的规划和预测方法，其中包括一种新的评价函数，用于评估规划的性能。</li>
<li>results: 研究表明，使用该方法可以生成更安全的规划，并且在人类行为的长尾事件中表现更好。 codes 可以在 <a target="_blank" rel="noopener" href="https://github.com/portal-cornell/Game-Theoretic-Forecasting-Planning">https://github.com/portal-cornell/Game-Theoretic-Forecasting-Planning</a> 上下载。<details>
<summary>Abstract</summary>
Planning safe robot motions in the presence of humans requires reliable forecasts of future human motion. However, simply predicting the most likely motion from prior interactions does not guarantee safety. Such forecasts fail to model the long tail of possible events, which are rarely observed in limited datasets. On the other hand, planning for worst-case motions leads to overtly conservative behavior and a ``frozen robot''. Instead, we aim to learn forecasts that predict counterfactuals that humans guard against. We propose a novel game-theoretic framework for joint planning and forecasting with the payoff being the performance of the planner against the demonstrator, and present practical algorithms to train models in an end-to-end fashion. We demonstrate that our proposed algorithm results in safer plans in a crowd navigation simulator and real-world datasets of pedestrian motion. We release our code at https://github.com/portal-cornell/Game-Theoretic-Forecasting-Planning.
</details>
<details>
<summary>摘要</summary>
计划安全机器人运动在人类存在下需要可靠的未来人类运动预测。然而，只预测最有可能性的运动不能保证安全。这些预测不会考虑可能性较低的事件，这些事件在有限的数据集中rarely observed。相反，我们目标是学习预测 humans guard against counterfactuals。我们提出了一种新的游戏理论基础，其中的奖励是与示例者的表现相对评价，并提供了实用的算法来训练模型。我们在人群导航模拟器和实际人行动数据上示出了我们的提议算法可以提供更安全的计划。我们的代码可以在 <https://github.com/portal-cornell/Game-Theoretic-Forecasting-Planning> 上下载。
</details></li>
</ul>
<hr>
<h2 id="Improving-Joint-Speech-Text-Representations-Without-Alignment"><a href="#Improving-Joint-Speech-Text-Representations-Without-Alignment" class="headerlink" title="Improving Joint Speech-Text Representations Without Alignment"></a>Improving Joint Speech-Text Representations Without Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06125">http://arxiv.org/abs/2308.06125</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cal Peyser, Zhong Meng, Ke Hu, Rohit Prabhavalkar, Andrew Rosenberg, Tara N. Sainath, Michael Picheny, Kyunghyun Cho</li>
<li>for: 这个论文旨在提出一种基于Modal Space的文本提示图像生成方法，以提高文本检索和识别的效果。</li>
<li>methods: 该方法使用了共同表示空间，将文本和图像领域共同表示，并通过适应loss来忽略序列长度差异。</li>
<li>results: 该方法在大参数训练的声音-文本编码器中显示出了改善的下游WER性能， tanto在单语言系统中 alsowithin multilingual system。<details>
<summary>Abstract</summary>
The last year has seen astonishing progress in text-prompted image generation premised on the idea of a cross-modal representation space in which the text and image domains are represented jointly. In ASR, this idea has found application as joint speech-text encoders that can scale to the capacities of very large parameter models by being trained on both unpaired speech and text. While these methods show promise, they have required special treatment of the sequence-length mismatch inherent in speech and text, either by up-sampling heuristics or an explicit alignment model. In this work, we offer evidence that joint speech-text encoders naturally achieve consistent representations across modalities by disregarding sequence length, and argue that consistency losses could forgive length differences and simply assume the best alignment. We show that such a loss improves downstream WER in both a large-parameter monolingual and multilingual system.
</details>
<details>
<summary>摘要</summary>
最近一年内，文本提示图像生成技术呈现了惊人的进步，基于跨Modal Representation Space（MR）的想法。在ASR中，这种想法得到应用，通过同时训练语音和文本频谱的共同Encoder来实现大型参数模型的扩展。虽然这些方法显示搭配性，但它们需要特殊地处理语音和文本序列长度之间的差异，通常通过填充规则或显式对齐模型。在这项工作中，我们提供证据，表明 JOINT 语音文本 Encoder 自然地实现了不同Modalities中的一致性，而不需要特殊地处理序列长度。我们还证明，在大型单语言和多语言系统中，这种损失函数可以提高下游 WER。
</details></li>
</ul>
<hr>
<h2 id="Learning-Deductive-Reasoning-from-Synthetic-Corpus-based-on-Formal-Logic"><a href="#Learning-Deductive-Reasoning-from-Synthetic-Corpus-based-on-Formal-Logic" class="headerlink" title="Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic"></a>Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07336">http://arxiv.org/abs/2308.07336</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hitachi-nlp/fld">https://github.com/hitachi-nlp/fld</a></li>
<li>paper_authors: Terufumi Morishita, Gaku Morio, Atsuki Yamaguchi, Yasuhiro Sogawa</li>
<li>for: 本研究旨在使语言模型（LM）掌握逻辑推理能力。</li>
<li>methods: 我们采用了基于形式逻辑理论的有效集合推理规则，可以从多步骤中 derivation 任何其他推理规则。</li>
<li>results: 我们的实验表明，使用我们提议的$\textbf{FLD}$ Corpora进行训练，可以使LM acquire更加通用的逻辑推理能力，并且我们identified了推理推理能力中哪些方面可以通过推理 corpora增强LM，以及哪些方面无法增强。<details>
<summary>Abstract</summary>
We study a synthetic corpus-based approach for language models (LMs) to acquire logical deductive reasoning ability. The previous studies generated deduction examples using specific sets of deduction rules. However, these rules were limited or otherwise arbitrary. This can limit the generalizability of acquired deductive reasoning ability. We rethink this and adopt a well-grounded set of deduction rules based on formal logic theory, which can derive any other deduction rules when combined in a multistep way. We empirically verify that LMs trained on the proposed corpora, which we name $\textbf{FLD}$ ($\textbf{F}$ormal $\textbf{L}$ogic $\textbf{D}$eduction), acquire more generalizable deductive reasoning ability. Furthermore, we identify the aspects of deductive reasoning ability on which deduction corpora can enhance LMs and those on which they cannot. Finally, on the basis of these results, we discuss the future directions for applying deduction corpora or other approaches for each aspect. We release the code, data, and models.
</details>
<details>
<summary>摘要</summary>
我们研究了基于合成 corpora 的方法，以使语言模型（LM）掌握逻辑推理能力。过去的研究通常使用特定的推理规则生成推理示例，但这些规则受限或是Random。这会限制学习得到的逻辑推理能力的通用性。我们重新思考这一点，采用基于正式逻辑理论的固定的推理规则，这些规则可以在多步骤的情况下组合生成任何其他的推理规则。我们实验证明，使用我们命名的 $\textbf{FLD}$（$\textbf{F}$ormal $\textbf{L}$ogic $\textbf{D}$eduction）训练集，LM 可以获得更加通用的逻辑推理能力。此外，我们还识别了使用推理 corpora 可以增强LM的逻辑推理能力中的哪些方面，以及哪些方面无法增强。最后，我们根据这些结果，讨论未来如何应用推理 corpora 或其他方法来解决每个方面的问题。我们发布了代码、数据和模型。
</details></li>
</ul>
<hr>
<h2 id="Improving-Zero-Shot-Text-Matching-for-Financial-Auditing-with-Large-Language-Models"><a href="#Improving-Zero-Shot-Text-Matching-for-Financial-Auditing-with-Large-Language-Models" class="headerlink" title="Improving Zero-Shot Text Matching for Financial Auditing with Large Language Models"></a>Improving Zero-Shot Text Matching for Financial Auditing with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06111">http://arxiv.org/abs/2308.06111</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lars Hillebrand, Armin Berger, Tobias Deußer, Tim Dilmaghani, Mohamed Khaled, Bernd Kliem, Rüdiger Loitz, Maren Pielka, David Leonhard, Christian Bauckhage, Rafet Sifa</li>
<li>for: 帮助审核金融文档，提高审核效率和精度。</li>
<li>methods: 使用AI技术，提供相关文本段的推荐，以满足严格会计标准的法律要求。</li>
<li>results: 比较 existing 方法有 significat 性能提升，可以帮助审核人员更快速地完成审核任务。<details>
<summary>Abstract</summary>
Auditing financial documents is a very tedious and time-consuming process. As of today, it can already be simplified by employing AI-based solutions to recommend relevant text passages from a report for each legal requirement of rigorous accounting standards. However, these methods need to be fine-tuned regularly, and they require abundant annotated data, which is often lacking in industrial environments. Hence, we present ZeroShotALI, a novel recommender system that leverages a state-of-the-art large language model (LLM) in conjunction with a domain-specifically optimized transformer-based text-matching solution. We find that a two-step approach of first retrieving a number of best matching document sections per legal requirement with a custom BERT-based model and second filtering these selections using an LLM yields significant performance improvements over existing approaches.
</details>
<details>
<summary>摘要</summary>
审核财务文档是一项非常繁琐和时间consuming的过程。到目前为止，可以使用基于人工智能的解决方案来提供相关的文本段落，以满足严格的会计标准的法律要求。然而，这些方法需要定期细化，并且需要充足的注释数据，而这些数据在工业环境中经常缺乏。因此，我们提出了ZeroShotALI，一种新的推荐系统，利用现代大语言模型（LLM），并结合适应域特定的 transformer 基于文本匹配解决方案。我们发现，首先使用自定义 BERT 基本模型 retrieve 最佳匹配的文档段落，然后使用 LLM 进行筛选，可以实现显著性能提升。
</details></li>
</ul>
<hr>
<h2 id="Neural-Conversation-Models-and-How-to-Rein-Them-in-A-Survey-of-Failures-and-Fixes"><a href="#Neural-Conversation-Models-and-How-to-Rein-Them-in-A-Survey-of-Failures-and-Fixes" class="headerlink" title="Neural Conversation Models and How to Rein Them in: A Survey of Failures and Fixes"></a>Neural Conversation Models and How to Rein Them in: A Survey of Failures and Fixes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06095">http://arxiv.org/abs/2308.06095</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fabian Galetzka, Anne Beyer, David Schlangen</li>
<li>for: 这个论文旨在探讨基于强大语言模型的开放领域对话系统，以及如何使用语言模型实现合适的对话贡献。</li>
<li>methods: 该论文根据Grice的协作对话maxims进行了语言模型的解释，并系统化了相关研究的文献，包括使用数据、训练方法和解码方法来控制语言模型的性能。</li>
<li>results: 该论文提出了一些有前途的方法，并建议了未来研究的新方向，以提高语言模型的对话贡献质量。<details>
<summary>Abstract</summary>
Recent conditional language models are able to continue any kind of text source in an often seemingly fluent way. This fact encouraged research in the area of open-domain conversational systems that are based on powerful language models and aim to imitate an interlocutor by generating appropriate contributions to a written dialogue. From a linguistic perspective, however, the complexity of contributing to a conversation is high. In this survey, we interpret Grice's maxims of cooperative conversation from the perspective of this specific research area and systematize the literature under the aspect of what makes a contribution appropriate: A neural conversation model has to be fluent, informative, consistent, coherent, and follow social norms. In order to ensure these qualities, recent approaches try to tame the underlying language models at various intervention points, such as data, training regime or decoding. Sorted by these categories and intervention points, we discuss promising attempts and suggest novel ways for future research.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Reinforcement-Logic-Rule-Learning-for-Temporal-Point-Processes"><a href="#Reinforcement-Logic-Rule-Learning-for-Temporal-Point-Processes" class="headerlink" title="Reinforcement Logic Rule Learning for Temporal Point Processes"></a>Reinforcement Logic Rule Learning for Temporal Point Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06094">http://arxiv.org/abs/2308.06094</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Yang, Lu Wang, Kun Gao, Shuang Li</li>
<li>for: 这种方法用于扩展 temporal logic 规则集，以解释时间事件的发生。</li>
<li>methods: 该算法使用 temporal point process 模型和学习框架，逐渐优化规则内容和权重，直到 Observational event sequence 的可能性最大化。</li>
<li>results: 该方法在 both synthetic and real healthcare datasets 上获得了promising results。<details>
<summary>Abstract</summary>
We propose a framework that can incrementally expand the explanatory temporal logic rule set to explain the occurrence of temporal events. Leveraging the temporal point process modeling and learning framework, the rule content and weights will be gradually optimized until the likelihood of the observational event sequences is optimal. The proposed algorithm alternates between a master problem, where the current rule set weights are updated, and a subproblem, where a new rule is searched and included to best increase the likelihood. The formulated master problem is convex and relatively easy to solve using continuous optimization, whereas the subproblem requires searching the huge combinatorial rule predicate and relationship space. To tackle this challenge, we propose a neural search policy to learn to generate the new rule content as a sequence of actions. The policy parameters will be trained end-to-end using the reinforcement learning framework, where the reward signals can be efficiently queried by evaluating the subproblem objective. The trained policy can be used to generate new rules in a controllable way. We evaluate our methods on both synthetic and real healthcare datasets, obtaining promising results.
</details>
<details>
<summary>摘要</summary>
我们提出了一个框架，可以逐步扩展 temporal logic 规则集来解释时间事件的发生。利用 temporal point process 模型和学习框架，规则内容和权重将被渐进优化，直到观察事件序列的可能性最大化。我们的算法会 alternate between主问题和辅助问题。主问题中，当前规则集权重将被更新；辅助问题中，一个新的规则将被搜索并添加到最大化观察事件序列的可能性。主问题是 convex 的continuous optimization 问题，可以使用continuous optimization 方法解决；辅助问题则需要搜索庞大的 combinatorial rule predicate 和关系空间。为解决这个挑战，我们提出了一种 neural search 策略，可以学习生成新规则内容为一个序列的动作。策略参数将通过 reinforcement learning 框架进行end-to-end 训练，其中 reward signal 可以效率地被查询通过辅助问题的目标函数。训练好的策略可以控制性地生成新规则。我们对 synthetic 和实际医疗数据进行了evaluation， obtained promising results.
</details></li>
</ul>
<hr>
<h2 id="Toward-a-Better-Understanding-of-Loss-Functions-for-Collaborative-Filtering"><a href="#Toward-a-Better-Understanding-of-Loss-Functions-for-Collaborative-Filtering" class="headerlink" title="Toward a Better Understanding of Loss Functions for Collaborative Filtering"></a>Toward a Better Understanding of Loss Functions for Collaborative Filtering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06091">http://arxiv.org/abs/2308.06091</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/psm1206/mawu">https://github.com/psm1206/mawu</a></li>
<li>paper_authors: Seongmin Park, Mincheol Yoon, Jae-woong Lee, Hogun Park, Jongwuk Lee</li>
<li>for: 这篇论文探讨了现有的 collaborative filtering（CF）模型学习过程中的三个组成部分，即交互编码器、损失函数和负样本。</li>
<li>methods: 该论文分析了现有的损失函数之间的关系，并提出了一种新的损失函数：margin-aware alignment和weighted uniformity（MAWU），该损失函数能够考虑到数据集的特点，提高CF模型的性能。</li>
<li>results: 实验结果表明，当 equiped with MAWU，MF和LightGCN的性能与现有的CF模型相当或更高，特别是在许多实际应用中。<details>
<summary>Abstract</summary>
Collaborative filtering (CF) is a pivotal technique in modern recommender systems. The learning process of CF models typically consists of three components: interaction encoder, loss function, and negative sampling. Although many existing studies have proposed various CF models to design sophisticated interaction encoders, recent work shows that simply reformulating the loss functions can achieve significant performance gains. This paper delves into analyzing the relationship among existing loss functions. Our mathematical analysis reveals that the previous loss functions can be interpreted as alignment and uniformity functions: (i) the alignment matches user and item representations, and (ii) the uniformity disperses user and item distributions. Inspired by this analysis, we propose a novel loss function that improves the design of alignment and uniformity considering the unique patterns of datasets called Margin-aware Alignment and Weighted Uniformity (MAWU). The key novelty of MAWU is two-fold: (i) margin-aware alignment (MA) mitigates user/item-specific popularity biases, and (ii) weighted uniformity (WU) adjusts the significance between user and item uniformities to reflect the inherent characteristics of datasets. Extensive experimental results show that MF and LightGCN equipped with MAWU are comparable or superior to state-of-the-art CF models with various loss functions on three public datasets.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="An-Autoethnographic-Exploration-of-XAI-in-Algorithmic-Composition"><a href="#An-Autoethnographic-Exploration-of-XAI-in-Algorithmic-Composition" class="headerlink" title="An Autoethnographic Exploration of XAI in Algorithmic Composition"></a>An Autoethnographic Exploration of XAI in Algorithmic Composition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06089">http://arxiv.org/abs/2308.06089</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ashley Noel-Hirst, Nick Bryan-Kinns</li>
<li>for: 本研究旨在探讨如何使用可解释的人工智能（XAI）生成模型来创作传统爱尔兰民谣音乐。</li>
<li>methods: 该研究使用MeasureVAE生成模型，该模型具有可解释的秘密分量，并在爱尔兰民谣音乐上进行训练。</li>
<li>results: 研究发现，在音乐创作过程中，音乐创作者倾向于利用模型中的特征，而不是模型本身。这种方法可能扩展XAI模型的应用范围，并且可能为音乐创作者提供有用的创作工具。<details>
<summary>Abstract</summary>
Machine Learning models are capable of generating complex music across a range of genres from folk to classical music. However, current generative music AI models are typically difficult to understand and control in meaningful ways. Whilst research has started to explore how explainable AI (XAI) generative models might be created for music, no generative XAI models have been studied in music making practice. This paper introduces an autoethnographic study of the use of the MeasureVAE generative music XAI model with interpretable latent dimensions trained on Irish folk music. Findings suggest that the exploratory nature of the music-making workflow foregrounds musical features of the training dataset rather than features of the generative model itself. The appropriation of an XAI model within an iterative workflow highlights the potential of XAI models to form part of a richer and more complex workflow than they were initially designed for.
</details>
<details>
<summary>摘要</summary>
机器学习模型可以生成复杂的音乐，从folk到古典音乐。然而，当前的生成音乐AI模型通常具有难以理解和控制的问题。研究已经开始探索如何创建可解释的AI生成模型（XAI） для音乐，但没有任何生成XAI模型在音乐创作实践中被研究。这篇论文介绍了一个自传式的研究，使用MeasureVAE生成音乐XAI模型，具有可解释的潜在维度，在爱尔兰传统音乐上进行训练。发现结果表明，音乐创作过程的探索性强调了训练数据集中的音乐特征，而不是生成模型自身的特征。将XAI模型 incorporated into an iterative workflow highlights the potential of XAI models to form part of a richer and more complex workflow than they were initially designed for。
</details></li>
</ul>
<hr>
<h2 id="Assessing-Student-Errors-in-Experimentation-Using-Artificial-Intelligence-and-Large-Language-Models-A-Comparative-Study-with-Human-Raters"><a href="#Assessing-Student-Errors-in-Experimentation-Using-Artificial-Intelligence-and-Large-Language-Models-A-Comparative-Study-with-Human-Raters" class="headerlink" title="Assessing Student Errors in Experimentation Using Artificial Intelligence and Large Language Models: A Comparative Study with Human Raters"></a>Assessing Student Errors in Experimentation Using Artificial Intelligence and Large Language Models: A Comparative Study with Human Raters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06088">http://arxiv.org/abs/2308.06088</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arne Bewersdorff, Kathrin Seßler, Armin Baur, Enkelejda Kasneci, Claudia Nerdel</li>
<li>for: 本研究旨在提供一种基于大语言模型（LLM）的自动错误检测方法，以便为学生实验卷提供产生性、个性化的反馈。</li>
<li>methods: 研究使用了GPT-3.5和GPT-4系列的人工智能系统，并对人类评审者进行比较。</li>
<li>results: 研究发现，LLM系统可以准确地检测一些基本学生错误，如专注于依赖变量而不是预期观察（准确率为0.90）、修改进行中的实验（准确率为1）和VALIDATE_TRIALS（准确率为0.82）等。然而，检测更复杂的错误，如是否进行了有效的控制试验（准确率为0.60），具有更大的挑战。<details>
<summary>Abstract</summary>
Identifying logical errors in complex, incomplete or even contradictory and overall heterogeneous data like students' experimentation protocols is challenging. Recognizing the limitations of current evaluation methods, we investigate the potential of Large Language Models (LLMs) for automatically identifying student errors and streamlining teacher assessments. Our aim is to provide a foundation for productive, personalized feedback. Using a dataset of 65 student protocols, an Artificial Intelligence (AI) system based on the GPT-3.5 and GPT-4 series was developed and tested against human raters. Our results indicate varying levels of accuracy in error detection between the AI system and human raters. The AI system can accurately identify many fundamental student errors, for instance, the AI system identifies when a student is focusing the hypothesis not on the dependent variable but solely on an expected observation (acc. = 0.90), when a student modifies the trials in an ongoing investigation (acc. = 1), and whether a student is conducting valid test trials (acc. = 0.82) reliably. The identification of other, usually more complex errors, like whether a student conducts a valid control trial (acc. = .60), poses a greater challenge. This research explores not only the utility of AI in educational settings, but also contributes to the understanding of the capabilities of LLMs in error detection in inquiry-based learning like experimentation.
</details>
<details>
<summary>摘要</summary>
identifying logical errors in complex, incomplete or even contradictory data like students' experimentation protocols is challenging. recognizing the limitations of current evaluation methods, we investigate the potential of Large Language Models (LLMs) for automatically identifying student errors and streamlining teacher assessments. our aim is to provide a foundation for productive, personalized feedback. using a dataset of 65 student protocols, an Artificial Intelligence (AI) system based on the GPT-3.5 and GPT-4 series was developed and tested against human raters. our results indicate varying levels of accuracy in error detection between the AI system and human raters. the AI system can accurately identify many fundamental student errors, for instance, the AI system identifies when a student is focusing the hypothesis not on the dependent variable but solely on an expected observation (acc. = 0.90), when a student modifies the trials in an ongoing investigation (acc. = 1), and whether a student is conducting valid test trials (acc. = 0.82) reliably. the identification of other, usually more complex errors, like whether a student conducts a valid control trial (acc. = .60), poses a greater challenge. this research explores not only the utility of AI in educational settings, but also contributes to the understanding of the capabilities of LLMs in error detection in inquiry-based learning like experimentation.
</details></li>
</ul>
<hr>
<h2 id="Audio-Visual-Spatial-Integration-and-Recursive-Attention-for-Robust-Sound-Source-Localization"><a href="#Audio-Visual-Spatial-Integration-and-Recursive-Attention-for-Robust-Sound-Source-Localization" class="headerlink" title="Audio-Visual Spatial Integration and Recursive Attention for Robust Sound Source Localization"></a>Audio-Visual Spatial Integration and Recursive Attention for Robust Sound Source Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06087">http://arxiv.org/abs/2308.06087</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/visualaikhu/sira-ssl">https://github.com/visualaikhu/sira-ssl</a></li>
<li>paper_authors: Sung Jin Um, Dongjin Kim, Jung Uk Kim<br>for: 本研究旨在开发一种可以听视模式结合探测声音源的方法，以便机器可以在视觉场景中检测声音源的位置。methods: 该方法利用了两个模式的空间cue，即声音和视觉模式，并通过人类行为的模仿，对声音源进行探测。此外，我们还提出了一种循环注意网络，以模仿人类的循环注意行为。results: 我们的方法在Flickr SoundNet和VGG-Sound Source数据集上进行了广泛的实验，并得到了较好的result。与传统方法相比，我们的方法能够更好地探测声音源的位置。代码可以在GitHub上找到：<a target="_blank" rel="noopener" href="https://github.com/VisualAIKHU/SIRA-SSL%E3%80%82">https://github.com/VisualAIKHU/SIRA-SSL。</a><details>
<summary>Abstract</summary>
The objective of the sound source localization task is to enable machines to detect the location of sound-making objects within a visual scene. While the audio modality provides spatial cues to locate the sound source, existing approaches only use audio as an auxiliary role to compare spatial regions of the visual modality. Humans, on the other hand, utilize both audio and visual modalities as spatial cues to locate sound sources. In this paper, we propose an audio-visual spatial integration network that integrates spatial cues from both modalities to mimic human behavior when detecting sound-making objects. Additionally, we introduce a recursive attention network to mimic human behavior of iterative focusing on objects, resulting in more accurate attention regions. To effectively encode spatial information from both modalities, we propose audio-visual pair matching loss and spatial region alignment loss. By utilizing the spatial cues of audio-visual modalities and recursively focusing objects, our method can perform more robust sound source localization. Comprehensive experimental results on the Flickr SoundNet and VGG-Sound Source datasets demonstrate the superiority of our proposed method over existing approaches. Our code is available at: https://github.com/VisualAIKHU/SIRA-SSL
</details>
<details>
<summary>摘要</summary>
<sup>1</sup> 音源localization任务的目标是让机器在视觉场景中检测声音来源的位置。而现有的方法只是使用音频作为视觉模式的比较依据，而人类则是同时使用音频和视觉模式来定位声音来源。在这篇论文中，我们提出了一种audio-visual空间集成网络，该网络可以 integrates空间信息来自两个模式，以模仿人类行为。此外，我们还引入了一种循环注意网络，以模仿人类的反复关注对象，从而获得更准确的注意区域。为了有效地编码音频-视觉模式中的空间信息，我们提出了音频-视觉对匹配损失和空间区域对齐损失。通过利用音频-视觉模式中的空间信息和循环注意对象，我们的方法可以实现更加稳定的声音来源定位。我们的实验结果表明，我们的方法在Flickr SoundNet和VGG-Sound Source datasets上比现有方法更高效。我们的代码可以在：https://github.com/VisualAIKHU/SIRA-SSL中找到。
</details></li>
</ul>
<hr>
<h2 id="Cost-effective-On-device-Continual-Learning-over-Memory-Hierarchy-with-Miro"><a href="#Cost-effective-On-device-Continual-Learning-over-Memory-Hierarchy-with-Miro" class="headerlink" title="Cost-effective On-device Continual Learning over Memory Hierarchy with Miro"></a>Cost-effective On-device Continual Learning over Memory Hierarchy with Miro</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06053">http://arxiv.org/abs/2308.06053</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyue Ma, Suyeon Jeong, Minjia Zhang, Di Wang, Jonghyun Choi, Myeongjae Jeon</li>
<li>for: This paper focuses on training neural network models incrementally on edge devices using continual learning (CL), with the goal of achieving cost-effectiveness while maintaining high model accuracy.</li>
<li>methods: The paper explores the design space of hierarchical memory replay-based CL and presents a novel system runtime called Miro that dynamically configures the CL system based on resource states for the best cost-effectiveness. Miro also performs online profiling on parameters with clear accuracy-energy trade-offs and adapts to optimal values with low overhead.</li>
<li>results: The paper shows that Miro significantly outperforms baseline systems in terms of cost-effectiveness, achieving higher accuracy while using less energy on edge devices.<details>
<summary>Abstract</summary>
Continual learning (CL) trains NN models incrementally from a continuous stream of tasks. To remember previously learned knowledge, prior studies store old samples over a memory hierarchy and replay them when new tasks arrive. Edge devices that adopt CL to preserve data privacy are typically energy-sensitive and thus require high model accuracy while not compromising energy efficiency, i.e., cost-effectiveness. Our work is the first to explore the design space of hierarchical memory replay-based CL to gain insights into achieving cost-effectiveness on edge devices. We present Miro, a novel system runtime that carefully integrates our insights into the CL framework by enabling it to dynamically configure the CL system based on resource states for the best cost-effectiveness. To reach this goal, Miro also performs online profiling on parameters with clear accuracy-energy trade-offs and adapts to optimal values with low overhead. Extensive evaluations show that Miro significantly outperforms baseline systems we build for comparison, consistently achieving higher cost-effectiveness.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Learning-to-Guide-Human-Experts-via-Personalized-Large-Language-Models"><a href="#Learning-to-Guide-Human-Experts-via-Personalized-Large-Language-Models" class="headerlink" title="Learning to Guide Human Experts via Personalized Large Language Models"></a>Learning to Guide Human Experts via Personalized Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06039">http://arxiv.org/abs/2308.06039</a></li>
<li>repo_url: None</li>
<li>paper_authors: Debodeep Banerjee, Stefano Teso, Andrea Passerini</li>
<li>for: 学习延迟（learning to defer），一个预测器可以识别风险决策并将其延迟给人类专家。</li>
<li>methods: 我们提出了学习导航（LTG）框架，而不是提供准备好的决策，机器将提供有用的指导来导引决策，人类完全负责来到决策。</li>
<li>results: 我们介绍了SLOG实现，可以通过一小量的人类监督将普通的大型自然语言模型转换成一个能够生成文本指导的模块，并对医疗诊断任务进行初步但是有前途的实验。<details>
<summary>Abstract</summary>
In learning to defer, a predictor identifies risky decisions and defers them to a human expert. One key issue with this setup is that the expert may end up over-relying on the machine's decisions, due to anchoring bias. At the same time, whenever the machine chooses the deferral option the expert has to take decisions entirely unassisted. As a remedy, we propose learning to guide (LTG), an alternative framework in which -- rather than suggesting ready-made decisions -- the machine provides guidance useful to guide decision-making, and the human is entirely responsible for coming up with a decision. We also introduce SLOG, an LTG implementation that leverages (a small amount of) human supervision to convert a generic large language model into a module capable of generating textual guidance, and present preliminary but promising results on a medical diagnosis task.
</details>
<details>
<summary>摘要</summary>
在学习延迟中，一个预测器可以识别风险决策并将其延迟给人类专家。然而，一个问题是专家可能会因为锚定偏见过度依赖机器的决策。当机器选择延迟选项时，专家必须完全不帮助地做出决策。为了解决这个问题，我们提出了学习导航（LTG）框架，在这个框架中，机器不会提供准备好的决策，而是提供有用的指导，以帮助人类做出决策。我们还介绍了SLOG，一种LTG实现方式，通过（一定的）人类监督来将一个通用的大型自然语言模型转换成一个能够生成文本指导的模块，并发表了初步但有前途的医疗诊断任务结果。
</details></li>
</ul>
<hr>
<h2 id="Deep-Context-Interest-Network-for-Click-Through-Rate-Prediction"><a href="#Deep-Context-Interest-Network-for-Click-Through-Rate-Prediction" class="headerlink" title="Deep Context Interest Network for Click-Through Rate Prediction"></a>Deep Context Interest Network for Click-Through Rate Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06037">http://arxiv.org/abs/2308.06037</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuyang Hou, Zhe Wang, Qi Liu, Tan Qu, Jia Cheng, Jun Lei</li>
<li>for: 预测用户点击行为（Click-Through Rate，CTR），提高在线广告等行业中的表现。</li>
<li>methods: 提出了一种名为深度上下文兴趣网络（Deep Context Interest Network，DCIN）的新模型，将用户点击行为和其显示上下文集成到一起，以学习用户的上下文感兴趣。DCIN包括三个关键模块：1）位置意识上下文聚合模块（PCAM），通过注意力机制对显示项进行聚合; 2）反馈上下文融合模块（FCFM），通过非线性特征交互来融合点击和显示上下文表示; 3）兴趣匹配模块（IMM），通过匹配点击和显示上下文中的兴趣来活化用户的兴趣。</li>
<li>results: 在线上和离线上的评估中，DCIN方法显示出了明显的提高，特别是在大规模的产业环境中部署后，使用DCIN方法可以提高1.5%的CTR和1.5%的RPM。<details>
<summary>Abstract</summary>
Click-Through Rate (CTR) prediction, estimating the probability of a user clicking on an item, is essential in industrial applications, such as online advertising. Many works focus on user behavior modeling to improve CTR prediction performance. However, most of those methods only model users' positive interests from users' click items while ignoring the context information, which is the display items around the clicks, resulting in inferior performance. In this paper, we highlight the importance of context information on user behavior modeling and propose a novel model named Deep Context Interest Network (DCIN), which integrally models the click and its display context to learn users' context-aware interests. DCIN consists of three key modules: 1) Position-aware Context Aggregation Module (PCAM), which performs aggregation of display items with an attention mechanism; 2) Feedback-Context Fusion Module (FCFM), which fuses the representation of clicks and display contexts through non-linear feature interaction; 3) Interest Matching Module (IMM), which activates interests related with the target item. Moreover, we provide our hands-on solution to implement our DCIN model on large-scale industrial systems. The significant improvements in both offline and online evaluations demonstrate the superiority of our proposed DCIN method. Notably, DCIN has been deployed on our online advertising system serving the main traffic, which brings 1.5% CTR and 1.5% RPM lift.
</details>
<details>
<summary>摘要</summary>
Click-through rate (CTR) 预测，judging the probability of a user clicking on an item，是工业应用中的关键问题，如在线广告。许多研究都是用户行为模型来提高 CTR 预测性能。然而，大多数这些方法只是模型用户的正面兴趣，即用户点击的项目，而忽略了上下文信息，即点击项目的周围的显示项目，这会导致性能下降。在这篇论文中，我们强调了用户上下文信息的重要性，并提出了一种新的模型，即深度上下文兴趣网络（DCIN）。DCIN 包括三个关键模块：1）位置意识上下文聚合模块（PCAM），通过注意机制来聚合显示项目; 2）反馈上下文融合模块（FCFM），通过非线性特征交互来融合点击和显示上下文的表示; 3）兴趣匹配模块（IMM），通过匹配用户的兴趣和目标项目来活化用户的兴趣。此外，我们还提供了在大规模工业系统上实现 DCIN 模型的实践方法。在线和离线评估中，DCIN 方法显示出了明显的优势，特别是在主要流量上部署 DCIN 方法后，Click-through rate 提高 1.5%，和 Revenue Per Mille (RPM) 提高 1.5%。
</details></li>
</ul>
<hr>
<h2 id="Evidence-of-Human-Like-Visual-Linguistic-Integration-in-Multimodal-Large-Language-Models-During-Predictive-Language-Processing"><a href="#Evidence-of-Human-Like-Visual-Linguistic-Integration-in-Multimodal-Large-Language-Models-During-Predictive-Language-Processing" class="headerlink" title="Evidence of Human-Like Visual-Linguistic Integration in Multimodal Large Language Models During Predictive Language Processing"></a>Evidence of Human-Like Visual-Linguistic Integration in Multimodal Large Language Models During Predictive Language Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06035">http://arxiv.org/abs/2308.06035</a></li>
<li>repo_url: None</li>
<li>paper_authors: Viktor Kewenig, Christopher Edwards, Quitterie Lacome DEstalenx, Akilles Rechardt, Jeremy I Skipper, Gabriella Vigliocco<br>for:这篇论文探讨了大语言模型（LLM）的语言处理能力是否可以模拟人类的认知过程。methods:研究使用了多Modal language model（mLLM），它将视觉和语言嵌入空间与 transformer 型注意机制结合起来进行下一个词预测。results:研究发现，人类的预测性与 CLIP 分数相似，但不是对一个单modal LLM 进行比较。进一步的分析发现，当 CLIP 的视觉注意重量被干扰时，人类和 CLIP 的预测性都消失。此外，当同一个输入被传递给一个多 modal 模型而不具有注意时，人类和 CLIP 的预测性也消失。这些结果表明，在 mLLM 和人类之间的预测语言处理过程存在相似的多模态信息集成和注意机制。<details>
<summary>Abstract</summary>
The advanced language processing abilities of large language models (LLMs) have stimulated debate over their capacity to replicate human-like cognitive processes. One differentiating factor between language processing in LLMs and humans is that language input is often grounded in more than one perceptual modality, whereas most LLMs process solely text-based information. Multimodal grounding allows humans to integrate - e.g. visual context with linguistic information and thereby place constraints on the space of upcoming words, reducing cognitive load and improving perception and comprehension. Recent multimodal LLMs (mLLMs) combine visual and linguistic embedding spaces with a transformer type attention mechanism for next-word prediction. To what extent does predictive language processing based on multimodal input align in mLLMs and humans? To answer this question, 200 human participants watched short audio-visual clips and estimated the predictability of an upcoming verb or noun. The same clips were processed by the mLLM CLIP, with predictability scores based on a comparison of image and text feature vectors. Eye-tracking was used to estimate what visual features participants attended to, and CLIP's visual attention weights were recorded. We find that human estimates of predictability align significantly with CLIP scores, but not for a unimodal LLM of comparable parameter size. Further, alignment vanished when CLIP's visual attention weights were perturbed, and when the same input was fed to a multimodal model without attention. Analysing attention patterns, we find a significant spatial overlap between CLIP's visual attention weights and human eye-tracking data. Results suggest that comparable processes of integrating multimodal information, guided by attention to relevant visual features, supports predictive language processing in mLLMs and humans.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的高级语言处理能力已经引起了对其是否可以模拟人类认知过程的辩论。与人类语言处理不同的一点是，LLM通常只处理文本类型的语言输入，而人类则可以将多种感知模式结合起来，以帮助理解和理解语言。多模态基础设施（multimodal grounding）允许人类将视觉上下文与语言信息结合起来，从而限制下一个词的可能性，降低认知负担并提高理解和理解能力。最近的多模态语言模型（mLLM）将视觉和语言嵌入空间与转换器类型注意机制结合起来，以进行下一个词预测。我们问道，mLLM和人类的预测语言处理是否具有相似的特征？为了回答这个问题，我们请求200名参与者观看短 audio-visual clip，并估算下一个词或名称的预测可能性。同时，我们使用CLIP模型处理同一个clip，并根据图像和文本特征向量进行比较。使用眼动追踪来估算参与者关注的视觉特征，并记录CLIP的视觉注意力量。我们发现，人类的预测与CLIP得分之间存在显著的相似性，但不同于一个参数大小相当的单模态 LLM。此外，当CLIP的视觉注意力量被干扰时，或者当同一个输入被传递给没有注意力的多模态模型时，对应的Alignment消失。我们分析了注意力模式，发现CLIP的视觉注意力量和人类眼动追踪数据之间存在显著的空间重叠。结果表明，mLLM和人类在多模态信息的集成和注意力引导下实现了相似的预测语言处理过程。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-in-Cryptocurrency-Securities-Cases-Can-ChatGPT-Replace-Lawyers"><a href="#Large-Language-Models-in-Cryptocurrency-Securities-Cases-Can-ChatGPT-Replace-Lawyers" class="headerlink" title="Large Language Models in Cryptocurrency Securities Cases: Can ChatGPT Replace Lawyers?"></a>Large Language Models in Cryptocurrency Securities Cases: Can ChatGPT Replace Lawyers?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06032">http://arxiv.org/abs/2308.06032</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arianna Trozze, Toby Davies, Bennett Kleinberg</li>
<li>for: The paper aims to study the effectiveness of large language models (LLMs) in conducting legal tasks, specifically in the context of securities cases involving cryptocurrencies.</li>
<li>methods: The paper uses GPT-3.5, a large language model, to evaluate its legal reasoning and drafting capabilities in real-life cases. The authors compare the performance of GPT-3.5 with human lawyers in terms of determining potential violations and drafting legal complaints.</li>
<li>results: The paper finds that GPT-3.5’s legal reasoning skills are weak and misses additional correct violations, but it performs better in legal drafting. The authors also find that jurors’ decisions are not statistically significantly associated with the author of the document upon which they based their decisions. Overall, the paper suggests that LLMs cannot satisfactorily conduct legal reasoning tasks but could provide access to justice for more individuals by reducing the cost of legal services.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) could enhance access to the legal system. However, empirical research on their effectiveness in conducting legal tasks is scant. We study securities cases involving cryptocurrencies as one of numerous contexts where AI could support the legal process, studying LLMs' legal reasoning and drafting capabilities. We examine whether a) an LLM can accurately determine which laws are potentially being violated from a fact pattern, and b) whether there is a difference in juror decision-making based on complaints written by a lawyer compared to an LLM. We feed fact patterns from real-life cases to GPT-3.5 and evaluate its ability to determine correct potential violations from the scenario and exclude spurious violations. Second, we had mock jurors assess complaints written by the LLM and lawyers. GPT-3.5's legal reasoning skills proved weak, though we expect improvement in future models, particularly given the violations it suggested tended to be correct (it merely missed additional, correct violations). GPT-3.5 performed better at legal drafting, and jurors' decisions were not statistically significantly associated with the author of the document upon which they based their decisions. Because LLMs cannot satisfactorily conduct legal reasoning tasks, they would be unable to replace lawyers at this stage. However, their drafting skills (though, perhaps, still inferior to lawyers), could provide access to justice for more individuals by reducing the cost of legal services. Our research is the first to systematically study LLMs' legal drafting and reasoning capabilities in litigation, as well as in securities law and cryptocurrency-related misconduct.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs）可能会提高法律系统的存取。然而，实践研究这些模型在执行法律任务上的效果仍然缺乏。我们在投资业中使用 криптовалю牌为背景，研究 AI 可以支持法律程序，研究 LLMs 的法律推理和撰写能力。我们评估了 GPT-3.5 是否能够准确地决定从事例中可能违反的法律，以及法律执行人和 AI 撰写的文件是否会影响陪审员的决策。我们给 GPT-3.5 输入了实际案例的事例，并评估了它的法律推理和撰写能力。我们发现 GPT-3.5 的法律推理能力较弱，但是它还是能够准确地决定案例中的可能违反。在撰写方面，GPT-3.5 表现较好，并且陪审员的决策与文件的作者无关。由于 LLMs 目前无法实现法律推理任务，因此它们无法取代律师。但是，它们的撰写能力可能可以帮助更多人访问法律系统，因为它们可以降低法律服务的成本。我们的研究是首次系统地研究 LLMs 在诉讼中的法律推理和撰写能力，以及在投资业和 криптовалю牌相关的违法行为中。
</details></li>
</ul>
<hr>
<h2 id="AI-Assisted-Investigation-of-On-Chain-Parameters-Risky-Cryptocurrencies-and-Price-Factors"><a href="#AI-Assisted-Investigation-of-On-Chain-Parameters-Risky-Cryptocurrencies-and-Price-Factors" class="headerlink" title="AI-Assisted Investigation of On-Chain Parameters: Risky Cryptocurrencies and Price Factors"></a>AI-Assisted Investigation of On-Chain Parameters: Risky Cryptocurrencies and Price Factors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08554">http://arxiv.org/abs/2308.08554</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdulrezzak Zekiye, Semih Utku, Fadi Amroush, Oznur Ozkasap</li>
<li>for: 本研究旨在帮助投资者做出 Informed 投资决策，对 cryptocurrency 价格的影响因素进行分析，并identify 风险的 cryptocurrency。</li>
<li>methods: 本研究使用历史数据和人工智能算法对 on-chain 参数进行分析，以确定 cryptocurrency 价格的影响因素和风险度。</li>
<li>results: 分析发现，大约一third (39%) of cryptocurrencies 从市场中消失，只有一小部分 (10%) 存活了超过 1000 天。我们发现 cryptocurrency 价格和最大供应量以及总供应量之间存在显著负相关性，同时24小时交易量和价格也存在某种弱相关性。此外，我们使用 clustering 和分类来为投资者提供更全面的 cryptocurrency 认知，并使用多种分类器来预测 cryptocurrency 是否为风险。最终，我们使用 K-Nearest Neighbor 得到了最佳的 f1-score 76%。<details>
<summary>Abstract</summary>
Cryptocurrencies have become a popular and widely researched topic of interest in recent years for investors and scholars. In order to make informed investment decisions, it is essential to comprehend the factors that impact cryptocurrency prices and to identify risky cryptocurrencies. This paper focuses on analyzing historical data and using artificial intelligence algorithms on on-chain parameters to identify the factors affecting a cryptocurrency's price and to find risky cryptocurrencies. We conducted an analysis of historical cryptocurrencies' on-chain data and measured the correlation between the price and other parameters. In addition, we used clustering and classification in order to get a better understanding of a cryptocurrency and classify it as risky or not. The analysis revealed that a significant proportion of cryptocurrencies (39%) disappeared from the market, while only a small fraction (10%) survived for more than 1000 days. Our analysis revealed a significant negative correlation between cryptocurrency price and maximum and total supply, as well as a weak positive correlation between price and 24-hour trading volume. Moreover, we clustered cryptocurrencies into five distinct groups using their on-chain parameters, which provides investors with a more comprehensive understanding of a cryptocurrency when compared to those clustered with it. Finally, by implementing multiple classifiers to predict whether a cryptocurrency is risky or not, we obtained the best f1-score of 76% using K-Nearest Neighbor.
</details>
<details>
<summary>摘要</summary>
digital currencies 在过去几年中变得非常受投资者和学者关注。为了做出了解的投资决策，需要了解影响数字货币价格的因素，并识别风险货币。这篇论文探讨了历史数字货币的链上数据，并使用人工智能算法对其他参数进行分析。我们对历史数字货币的链上数据进行分析，并测量价格与其他参数之间的相关性。此外，我们还使用聚类和分类来更好地理解数字货币，并将其分为五个不同的组。最后，我们通过多种分类器预测数字货币是否为风险，并获得了最佳的 f1 分数（76%）使用 K-最近邻居。
</details></li>
</ul>
<hr>
<h2 id="Controlling-Character-Motions-without-Observable-Driving-Source"><a href="#Controlling-Character-Motions-without-Observable-Driving-Source" class="headerlink" title="Controlling Character Motions without Observable Driving Source"></a>Controlling Character Motions without Observable Driving Source</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06025">http://arxiv.org/abs/2308.06025</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiyuan Li, Bin Dai, Ziyi Zhou, Qi Yao, Baoyuan Wang</li>
<li>for: 生成无驱动源的多样化、自然的人体动作序列</li>
<li>methods: 提议一种系统性框架，结合VQ-VAE和征token级控制策略，并使用优化的奖励函数进行训练</li>
<li>results: 通过全面评估，我们的提议的框架可以有效地解决无驱动源生成中的OOD问题、缺乏多样性和不愿意 periodic 问题，并与其他强基eline比较高效。<details>
<summary>Abstract</summary>
How to generate diverse, life-like, and unlimited long head/body sequences without any driving source? We argue that this under-investigated research problem is non-trivial at all, and has unique technical challenges behind it. Without semantic constraints from the driving sources, using the standard autoregressive model to generate infinitely long sequences would easily result in 1) out-of-distribution (OOD) issue due to the accumulated error, 2) insufficient diversity to produce natural and life-like motion sequences and 3) undesired periodic patterns along the time. To tackle the above challenges, we propose a systematic framework that marries the benefits of VQ-VAE and a novel token-level control policy trained with reinforcement learning using carefully designed reward functions. A high-level prior model can be easily injected on top to generate unlimited long and diverse sequences. Although we focus on no driving sources now, our framework can be generalized for controlled synthesis with explicit driving sources. Through comprehensive evaluations, we conclude that our proposed framework can address all the above-mentioned challenges and outperform other strong baselines very significantly.
</details>
<details>
<summary>摘要</summary>
如何生成无驱动源的多样化、生命般自然的头部或身体序列？我们认为这是一个尚未受到充分研究的问题，具有独特的技术挑战。在没有 semantic 约束的情况下，使用标准的 autoregressive 模型来生成无限长的序列将导致1)  OUT-OF-DISTRIBUTION（OOD）问题 Due to the accumulated error，2) 不够的多样性以生成自然和生命般的动作序列，和3) 不愿意的 periodic 模式在时间上。为了解决以上挑战，我们提出了一个系统性的框架，该框架将 VQ-VAE 的优点和一种新的 токен级控制策略，通过使用 Carefully 设计的 reward 函数进行训练。高级别的 prior 模型可以轻松地在上面嵌入，以生成无限长和多样化的序列。虽然我们现在没有驱动源，但我们的框架可以通过 Controlled 的方式扩展到有显式驱动源的情况。通过全面的评估，我们结论是我们提出的框架可以解决所有以上挑战，并与其他强大的基准模型进行比较，显著超出其性能。
</details></li>
</ul>
<hr>
<h2 id="Optimizing-transformer-based-machine-translation-model-for-single-GPU-training-a-hyperparameter-ablation-study"><a href="#Optimizing-transformer-based-machine-translation-model-for-single-GPU-training-a-hyperparameter-ablation-study" class="headerlink" title="Optimizing transformer-based machine translation model for single GPU training: a hyperparameter ablation study"></a>Optimizing transformer-based machine translation model for single GPU training: a hyperparameter ablation study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06017">http://arxiv.org/abs/2308.06017</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luv Verma, Ketaki N. Kolhatkar</li>
<li>for:  explore the relationship between model complexity and performance in machine translation tasks</li>
<li>methods:  systematic investigation using ablation and a single NVIDIA A100 GPU</li>
<li>results:  unexpected insight that smaller models can be more effective, and the importance of precise hyperparameter tuning over mere scaling<details>
<summary>Abstract</summary>
In machine translation tasks, the relationship between model complexity and performance is often presumed to be linear, driving an increase in the number of parameters and consequent demands for computational resources like multiple GPUs. To explore this assumption, this study systematically investigates the effects of hyperparameters through ablation on a sequence-to-sequence machine translation pipeline, utilizing a single NVIDIA A100 GPU. Contrary to expectations, our experiments reveal that combinations with the most parameters were not necessarily the most effective. This unexpected insight prompted a careful reduction in parameter sizes, uncovering "sweet spots" that enable training sophisticated models on a single GPU without compromising translation quality. The findings demonstrate an intricate relationship between hyperparameter selection, model size, and computational resource needs. The insights from this study contribute to the ongoing efforts to make machine translation more accessible and cost-effective, emphasizing the importance of precise hyperparameter tuning over mere scaling.
</details>
<details>
<summary>摘要</summary>
在机器翻译任务中，模型复杂度和性能之间的关系 oft presumed 是线性的，驱动参数数量的增加和计算资源的需求，如多个GPU。为了探讨这个假设，这项研究系统atically investigate了机器翻译管道中的效果，使用单个NVIDIA A100 GPU。与预期不同，我们的实验发现，最多参数的组合并不总是最有效的。这 Unexpected insight prompted 我们进行精细的参数减少，探索“甜点”，使得在单个GPU上进行训练复杂的模型不会丧失翻译质量。这些发现表明了参数选择、模型大小和计算资源需求之间的复杂关系。这些发现对于使机器翻译更加可 accessible和cost-effective 有益，强调精确的参数调整的重要性，而不是仅仅是扩大。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-for-Telecom-Forthcoming-Impact-on-the-Industry"><a href="#Large-Language-Models-for-Telecom-Forthcoming-Impact-on-the-Industry" class="headerlink" title="Large Language Models for Telecom: Forthcoming Impact on the Industry"></a>Large Language Models for Telecom: Forthcoming Impact on the Industry</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06013">http://arxiv.org/abs/2308.06013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Maatouk, Nicola Piovesan, Fadhel Ayed, Antonio De Domenico, Merouane Debbah</li>
<li>For: The paper explores the potential impact of Large Language Models (LLMs) on the telecom industry, and provides insights into their current capabilities and limitations.* Methods: The paper examines the use cases that can be readily implemented in the telecom industry, streamlining numerous tasks that currently hinder operational efficiency and demand significant manpower and engineering expertise.* Results: The paper uncovers essential research directions that deal with the distinctive challenges of utilizing LLMs within the telecom domain, addressing these challenges to fully harness the potential of LLMs and unlock their capabilities within the telecom domain.Here are the three information points in Simplified Chinese text:* For: 论文探讨了大语言模型（LLMs）对电信业界的影响，并提供了其当前能力和限制的深入分析。* Methods: 论文检查了可以在电信业界快速实施的用例，使numerous tasksof operational efficiency and engineering expertise demanding tasks become more efficient and streamlined.* Results: 论文浮现出了在电信领域使用 LLMs 的特殊挑战，并提出了解决这些挑战的重要研究方向，以全面发挥 LLMS 的潜在力量和电信领域中的可能性。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have emerged as a transformative force, revolutionizing numerous fields well beyond the conventional domain of Natural Language Processing (NLP) and garnering unprecedented attention. As LLM technology continues to progress, the telecom industry is facing the prospect of its potential impact on its landscape. To elucidate these implications, we delve into the inner workings of LLMs, providing insights into their current capabilities and limitations. We also examine the use cases that can be readily implemented in the telecom industry, streamlining numerous tasks that currently hinder operational efficiency and demand significant manpower and engineering expertise. Furthermore, we uncover essential research directions that deal with the distinctive challenges of utilizing the LLMs within the telecom domain. Addressing these challenges represents a significant stride towards fully harnessing the potential of LLMs and unlocking their capabilities to the fullest extent within the telecom domain.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Large Language Models" (LLMs) is translated as "大型语言模型" (dàxìng yǔyán módel) in Simplified Chinese.* "Natural Language Processing" (NLP) is translated as "自然语言处理" (zìrán yǔyán xῡxí) in Simplified Chinese.* "telecom industry" is translated as "电信行业" (diànxiāng xíngyè) in Simplified Chinese.* "inner workings" is translated as "内部机制" (nèibù jīzhì) in Simplified Chinese.* "capabilities and limitations" is translated as "能力和局限性" (nénglì yǔ jiǔxiàn xìng) in Simplified Chinese.* "use cases" is translated as "应用场景" (yìngyòu scènes) in Simplified Chinese.* "immediately" is translated as "立即" (lìjí) in Simplified Chinese.* "significant manpower and engineering expertise" is translated as "巨大的人力和工程培训" (kùde dì zhōngyàng yǔgōng zhìxíng) in Simplified Chinese.* "essential research directions" is translated as "重要的研究方向" (zhòngyào de yánjiù fāngdìng) in Simplified Chinese.* "distinctive challenges" is translated as "特殊的挑战" (tèshū de tiǎozhàn) in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Deep-Task-specific-Bottom-Representation-Network-for-Multi-Task-Recommendation"><a href="#Deep-Task-specific-Bottom-Representation-Network-for-Multi-Task-Recommendation" class="headerlink" title="Deep Task-specific Bottom Representation Network for Multi-Task Recommendation"></a>Deep Task-specific Bottom Representation Network for Multi-Task Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05996">http://arxiv.org/abs/2308.05996</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qi Liu, Zhilong Zhou, Gangwei Jiang, Tiezheng Ge, Defu Lian</li>
<li>for: 提高 recommendation system 的性能，解决多任务学习中任务之间的负向传递问题。</li>
<li>methods: 提出了 Deep Task-specific Bottom Representation Network (DTRN)，通过在底层表示模型学习阶段为每个任务分别学习专门的表示，解决任务之间的负向传递问题。</li>
<li>results: 通过实验证明，DTRN 可以提高 recommendation system 的性能，并且可以与现有的多任务学习方法结合使用。<details>
<summary>Abstract</summary>
Neural-based multi-task learning (MTL) has gained significant improvement, and it has been successfully applied to recommendation system (RS). Recent deep MTL methods for RS (e.g. MMoE, PLE) focus on designing soft gating-based parameter-sharing networks that implicitly learn a generalized representation for each task. However, MTL methods may suffer from performance degeneration when dealing with conflicting tasks, as negative transfer effects can occur on the task-shared bottom representation. This can result in a reduced capacity for MTL methods to capture task-specific characteristics, ultimately impeding their effectiveness and hindering the ability to generalize well on all tasks. In this paper, we focus on the bottom representation learning of MTL in RS and propose the Deep Task-specific Bottom Representation Network (DTRN) to alleviate the negative transfer problem. DTRN obtains task-specific bottom representation explicitly by making each task have its own representation learning network in the bottom representation modeling stage. Specifically, it extracts the user's interests from multiple types of behavior sequences for each task through the parameter-efficient hypernetwork. To further obtain the dedicated representation for each task, DTRN refines the representation of each feature by employing a SENet-like network for each task. The two proposed modules can achieve the purpose of getting task-specific bottom representation to relieve tasks' mutual interference. Moreover, the proposed DTRN is flexible to combine with existing MTL methods. Experiments on one public dataset and one industrial dataset demonstrate the effectiveness of the proposed DTRN.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Audio-is-all-in-one-speech-driven-gesture-synthetics-using-WavLM-pre-trained-model"><a href="#Audio-is-all-in-one-speech-driven-gesture-synthetics-using-WavLM-pre-trained-model" class="headerlink" title="Audio is all in one: speech-driven gesture synthetics using WavLM pre-trained model"></a>Audio is all in one: speech-driven gesture synthetics using WavLM pre-trained model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05995">http://arxiv.org/abs/2308.05995</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fan Zhang, Naye Ji, Fuxing Gao, Siyuan Zhao, Zhaohan Wang, Shunman Li</li>
<li>for: 这篇论文主要目标是提出一种基于扩散的非自回归 трансформа器模型，用于生成基于语音的自然化姿势。</li>
<li>methods: 该模型使用 WavLM 预训练模型提取低级和高级语音信息，并使用适应层 нор方案学习语音信息和附属姿势之间的关系。</li>
<li>results: 对于 Trinity、ZEGGS 和 BEAT 等 dataset，模型能够生成自然化的姿势，并且可以控制姿势的风格和质量。<details>
<summary>Abstract</summary>
The generation of co-speech gestures for digital humans is an emerging area in the field of virtual human creation. Prior research has made progress by using acoustic and semantic information as input and adopting classify method to identify the person's ID and emotion for driving co-speech gesture generation. However, this endeavour still faces significant challenges. These challenges go beyond the intricate interplay between co-speech gestures, speech acoustic, and semantics; they also encompass the complexities associated with personality, emotion, and other obscure but important factors. This paper introduces "diffmotion-v2," a speech-conditional diffusion-based and non-autoregressive transformer-based generative model with WavLM pre-trained model. It can produce individual and stylized full-body co-speech gestures only using raw speech audio, eliminating the need for complex multimodal processing and manually annotated. Firstly, considering that speech audio not only contains acoustic and semantic features but also conveys personality traits, emotions, and more subtle information related to accompanying gestures, we pioneer the adaptation of WavLM, a large-scale pre-trained model, to extract low-level and high-level audio information. Secondly, we introduce an adaptive layer norm architecture in the transformer-based layer to learn the relationship between speech information and accompanying gestures. Extensive subjective evaluation experiments are conducted on the Trinity, ZEGGS, and BEAT datasets to confirm the WavLM and the model's ability to synthesize natural co-speech gestures with various styles.
</details>
<details>
<summary>摘要</summary>
“数字人类创造领域中的同声动作生成是一个emerging领域。先前的研究使用了语音和语义信息作为输入，采用分类方法来确定人的ID和情绪，以驱动同声动作生成。然而，这个努力仍然面临着一些挑战。这些挑战不仅包括语音、语义和同声动作之间的复杂互动，还包括人格、情绪和其他一些重要 yet obscure的因素。本文介绍了一种基于扩散和非autoregressive transformer的generative模型，称为diffmotion-v2。该模型可以通过 Raw speech audio 生成具有个性化和风格化的全身同声动作，无需复杂的多modal处理和手动标注。首先，我们认为语音audio不仅包含语音和语义特征，还拥有人格特征、情绪和更加细微的动作相关信息。因此，我们采用了 WavLM，一个大规模预训练模型，以提取低级和高级语音信息。其次，我们引入了adaptive层norm架构，以学习语音信息和同声动作之间的关系。我们在 Trinity、ZEGGS 和 BEAT 数据集上进行了评估实验，以确认 WavLM 和模型的能力生成自然的同声动作。”
</details></li>
</ul>
<hr>
<h2 id="Defensive-Perception-Estimation-and-Monitoring-of-Neural-Network-Performance-under-Deployment"><a href="#Defensive-Perception-Estimation-and-Monitoring-of-Neural-Network-Performance-under-Deployment" class="headerlink" title="Defensive Perception: Estimation and Monitoring of Neural Network Performance under Deployment"></a>Defensive Perception: Estimation and Monitoring of Neural Network Performance under Deployment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06299">http://arxiv.org/abs/2308.06299</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hendrik Vogt, Stefan Buehler, Mark Schutera</li>
<li>for: 本研究旨在 Addressing the issue of unnoticed catastrophic deployment and domain shift in neural networks for semantic segmentation in autonomous driving.</li>
<li>methods: 我们的方法基于 deep learning-based perception for autonomous driving 是 uncertain 的，并且可以通过 Monte Carlo Dropout 方法来 estimating epistemic uncertainty. 我们的方法不需要修改已经部署的神经网络，并且可以保证预期的模型性能。</li>
<li>results: 我们的方法可以 estimate neural network performance，并且可以 monitoring 和 notification of entering domains of reduced neural network performance under deployment. 我们还提出了一些新的方法来改进应用在部署设置下，包括减少计算成本和限制估计噪声。 最后，我们示出了我们的方法在多种不同的部署转移 relevante to autonomous driving 中的应用，如夜晚、雨天或雪天等。 总的来说，我们的方法在部署设置下有很大的潜力，可以实现 operational design domain recognition via uncertainty，并且可以提供 defensive perception、safe state triggers、 warning notifications 和 feedback for testing or development and adaptation of the perception stack.<details>
<summary>Abstract</summary>
In this paper, we propose a method for addressing the issue of unnoticed catastrophic deployment and domain shift in neural networks for semantic segmentation in autonomous driving. Our approach is based on the idea that deep learning-based perception for autonomous driving is uncertain and best represented as a probability distribution. As autonomous vehicles' safety is paramount, it is crucial for perception systems to recognize when the vehicle is leaving its operational design domain, anticipate hazardous uncertainty, and reduce the performance of the perception system. To address this, we propose to encapsulate the neural network under deployment within an uncertainty estimation envelope that is based on the epistemic uncertainty estimation through the Monte Carlo Dropout approach. This approach does not require modification of the deployed neural network and guarantees expected model performance. Our defensive perception envelope has the capability to estimate a neural network's performance, enabling monitoring and notification of entering domains of reduced neural network performance under deployment. Furthermore, our envelope is extended by novel methods to improve the application in deployment settings, including reducing compute expenses and confining estimation noise. Finally, we demonstrate the applicability of our method for multiple different potential deployment shifts relevant to autonomous driving, such as transitions into the night, rainy, or snowy domain. Overall, our approach shows great potential for application in deployment settings and enables operational design domain recognition via uncertainty, which allows for defensive perception, safe state triggers, warning notifications, and feedback for testing or development and adaptation of the perception stack.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种方法来解决深度学习基于自动驾驶的神经网络中的不良发展和领域转移问题。我们的方法基于神经网络的不确定性，即神经网络在自动驾驶中的观测是一个可能性 Distribution 的。由于自动驾驶的安全性 paramount，因此神经网络的观测系统必须能够识别自动车辆离开操作设计域，预测危险不确定性，并降低神经网络的性能。为此，我们提议将神经网络在部署过程中包裹在一个不确定性估计封装中，该封装基于 Monte Carlo Dropout 方法来估计神经网络的不确定性。这种方法不需要修改已部署的神经网络，并保证预期的模型性能。我们的防御观测封装具有估计神经网络性能的能力，可以监测和通知部署过程中神经网络性能下降。此外，我们还提出了一些新的方法来改进部署设置中的应用，包括减少计算成本和限制估计噪声。最后，我们示例了我们的方法在多种不同的部署转移中的应用，如夜晚、雨天或雪天等。总之，我们的方法在部署设置中具有潜力，可以实现操作设计域识别，并提供了防御观测、安全状态触发器、警示通知和测试或开发和适应观测堆的反馈。
</details></li>
</ul>
<hr>
<h2 id="TrajPAC-Towards-Robustness-Verification-of-Pedestrian-Trajectory-Prediction-Models"><a href="#TrajPAC-Towards-Robustness-Verification-of-Pedestrian-Trajectory-Prediction-Models" class="headerlink" title="TrajPAC: Towards Robustness Verification of Pedestrian Trajectory Prediction Models"></a>TrajPAC: Towards Robustness Verification of Pedestrian Trajectory Prediction Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05985">http://arxiv.org/abs/2308.05985</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zl-helios/trajpac">https://github.com/zl-helios/trajpac</a></li>
<li>paper_authors: Liang Zhang, Nathaniel Xu, Pengfei Yang, Gaojie Jin, Cheng-Chao Huang, Lijun Zhang</li>
<li>for: 本研究旨在提高自动驾驶车辆的安全性，即使在拥有不同的情况下进行预测。</li>
<li>methods: 本研究使用了一种可靠的拟合正确（PAC）框架，以确保方法的稳定性和可靠性。</li>
<li>results: 本研究对四种state-of-the-art trajectory prediction模型进行了robustness测试，并通过了TrajPAC工具的评估。同时，研究还探讨了影响robustness性表现的多种因素。<details>
<summary>Abstract</summary>
Robust pedestrian trajectory forecasting is crucial to developing safe autonomous vehicles. Although previous works have studied adversarial robustness in the context of trajectory forecasting, some significant issues remain unaddressed. In this work, we try to tackle these crucial problems. Firstly, the previous definitions of robustness in trajectory prediction are ambiguous. We thus provide formal definitions for two kinds of robustness, namely label robustness and pure robustness. Secondly, as previous works fail to consider robustness about all points in a disturbance interval, we utilise a probably approximately correct (PAC) framework for robustness verification. Additionally, this framework can not only identify potential counterexamples, but also provides interpretable analyses of the original methods. Our approach is applied using a prototype tool named TrajPAC. With TrajPAC, we evaluate the robustness of four state-of-the-art trajectory prediction models -- Trajectron++, MemoNet, AgentFormer, and MID -- on trajectories from five scenes of the ETH/UCY dataset and scenes of the Stanford Drone Dataset. Using our framework, we also experimentally study various factors that could influence robustness performance.
</details>
<details>
<summary>摘要</summary>
Robust pedestrian trajectory forecasting是Autonomous Vehicle的关键技能。 Previous works have studied adversarial robustness in the context of trajectory forecasting, but some significant issues remain unaddressed. In this work, we try to tackle these crucial problems.Firstly, the previous definitions of robustness in trajectory prediction are ambiguous. We thus provide formal definitions for two kinds of robustness, namely label robustness and pure robustness.Secondly, previous works fail to consider robustness about all points in a disturbance interval. We utilize a probably approximately correct (PAC) framework for robustness verification. This framework not only identifies potential counterexamples but also provides interpretable analyses of the original methods.Our approach is applied using a prototype tool named TrajPAC. With TrajPAC, we evaluate the robustness of four state-of-the-art trajectory prediction models -- Trajectron++, MemoNet, AgentFormer, and MID -- on trajectories from five scenes of the ETH/UCY dataset and scenes of the Stanford Drone Dataset. Using our framework, we also experimentally study various factors that could influence robustness performance.
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Explanations-of-Multi-agent-Optimization-Solutions"><a href="#Contrastive-Explanations-of-Multi-agent-Optimization-Solutions" class="headerlink" title="Contrastive Explanations of Multi-agent Optimization Solutions"></a>Contrastive Explanations of Multi-agent Optimization Solutions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05984">http://arxiv.org/abs/2308.05984</a></li>
<li>repo_url: None</li>
<li>paper_authors: Parisa Zehtabi, Alberto Pozanco, Ayala Bloch, Daniel Borrajo, Sarit Kraus</li>
<li>for: 提供了一种适用于多代理优化问题的域独立方法来获取冲突解释。</li>
<li>methods: 该方法包括生成一个新的解决方案，并将该解决方案与原始解决方案进行比较，以便高亮差异。</li>
<li>results: 计算机实验和用户研究表明，该方法可以为大型多代理优化问题提供有用的冲突解释，并使人类用户对原始解决方案的满意度提高。<details>
<summary>Abstract</summary>
In many real-world scenarios, agents are involved in optimization problems. Since most of these scenarios are over-constrained, optimal solutions do not always satisfy all agents. Some agents might be unhappy and ask questions of the form ``Why does solution $S$ not satisfy property $P$?''. In this paper, we propose MAoE, a domain-independent approach to obtain contrastive explanations by (i) generating a new solution $S^\prime$ where the property $P$ is enforced, while also minimizing the differences between $S$ and $S^\prime$; and (ii) highlighting the differences between the two solutions. Such explanations aim to help agents understanding why the initial solution is better than what they expected. We have carried out a computational evaluation that shows that MAoE can generate contrastive explanations for large multi-agent optimization problems. We have also performed an extensive user study in four different domains that shows that, after being presented with these explanations, humans' satisfaction with the original solution increases.
</details>
<details>
<summary>摘要</summary>
在许多实际场景中，代理人经常参与优化问题。由于大多数场景是过Constraint的，优化解决方案不总能满足所有代理人。一些代理人可能不满意并提问“为什么解决方案 $S$ 不满足属性 $P”？在这篇论文中，我们提议了MAoE，一种适用于各种领域的途径，通过（i）生成一个新的解决方案 $S^\prime$，使属性 $P$ 得到满足，同时尽量减少 $S$ 和 $S^\prime$ 之间的差异；以及（ii）高亮显示这两个解决方案之间的差异。这些解释的目的是帮助代理人理解初始解决方案是如何比预期更好。我们进行了大量计算评估，证明了MAoE可以为大规模多代理人优化问题生成对比性的解释。我们还进行了四个不同领域的用户研究，发现，在被给予这些解释后，人们对初始解决方案的满意度增加。
</details></li>
</ul>
<hr>
<h2 id="Face-Encryption-via-Frequency-Restricted-Identity-Agnostic-Attacks"><a href="#Face-Encryption-via-Frequency-Restricted-Identity-Agnostic-Attacks" class="headerlink" title="Face Encryption via Frequency-Restricted Identity-Agnostic Attacks"></a>Face Encryption via Frequency-Restricted Identity-Agnostic Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05983">http://arxiv.org/abs/2308.05983</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Dong, Rui Wang, Siyuan Liang, Aishan Liu, Lihua Jing</li>
<li>for: 防止face recognition系统的敏感资讯泄露</li>
<li>methods: 利用频率限制identity-agnostic（FRIA）框架实现隐藏人脸图像</li>
<li>results: 实验结果显示FRIA可以实现高比例的黑盒攻击成功率（96%），并且在实际应用中显示出实际的应用前景。<details>
<summary>Abstract</summary>
Billions of people are sharing their daily live images on social media everyday. However, malicious collectors use deep face recognition systems to easily steal their biometric information (e.g., faces) from these images. Some studies are being conducted to generate encrypted face photos using adversarial attacks by introducing imperceptible perturbations to reduce face information leakage. However, existing studies need stronger black-box scenario feasibility and more natural visual appearances, which challenge the feasibility of privacy protection. To address these problems, we propose a frequency-restricted identity-agnostic (FRIA) framework to encrypt face images from unauthorized face recognition without access to personal information. As for the weak black-box scenario feasibility, we obverse that representations of the average feature in multiple face recognition models are similar, thus we propose to utilize the average feature via the crawled dataset from the Internet as the target to guide the generation, which is also agnostic to identities of unknown face recognition systems; in nature, the low-frequency perturbations are more visually perceptible by the human vision system. Inspired by this, we restrict the perturbation in the low-frequency facial regions by discrete cosine transform to achieve the visual naturalness guarantee. Extensive experiments on several face recognition models demonstrate that our FRIA outperforms other state-of-the-art methods in generating more natural encrypted faces while attaining high black-box attack success rates of 96%. In addition, we validate the efficacy of FRIA using real-world black-box commercial API, which reveals the potential of FRIA in practice. Our codes can be found in https://github.com/XinDong10/FRIA.
</details>
<details>
<summary>摘要</summary>
每天，数十亿人在社交媒体上分享每日生活图片。然而，恶意收集者使用深度人脸识别系统抽取这些图片中的生物信息（例如，脸部）。一些研究在生成加密的人脸照片方面进行了努力，但现有研究受到强制黑盒场景可行性和更自然的视觉效果的挑战。为了解决这些问题，我们提出了频率限制人anonymous（FRIA）框架，用于加密不经授权的人脸识别，不需要个人信息。在弱黑盒场景下，我们发现了多个人脸识别模型的表示相似，因此我们提议使用这些表示作为准则，帮助生成加密人脸照片。此外，我们还采用了快播扩散变换来限制低频脸部区域中的杂散变换，以保证视觉自然性。我们的实验表明，FRIA可以在多个人脸识别模型上达到96%的黑盒攻击成功率，同时生成更自然的加密人脸照片。此外，我们还验证了FRIA的可行性，使用了实际的黑盒商用API。codes可以在https://github.com/XinDong10/FRIA中找到。
</details></li>
</ul>
<hr>
<h2 id="CyberForce-A-Federated-Reinforcement-Learning-Framework-for-Malware-Mitigation"><a href="#CyberForce-A-Federated-Reinforcement-Learning-Framework-for-Malware-Mitigation" class="headerlink" title="CyberForce: A Federated Reinforcement Learning Framework for Malware Mitigation"></a>CyberForce: A Federated Reinforcement Learning Framework for Malware Mitigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05978">http://arxiv.org/abs/2308.05978</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Feng, Alberto Huertas Celdran, Pedro Miguel Sanchez Sanchez, Jan Kreischer, Jan von der Assen, Gerome Bovet, Gregorio Martinez Perez, Burkhard Stiller</li>
<li>for: 提高互联网物联网设备的网络安全性</li>
<li>methods: 使用联邦强化学习（FRL）和设备指纹识别技术，采用分布式强化学习来采集和私有地确定适用于防御零日攻击的最佳防御策略</li>
<li>results: 在一个真实的互联网物联网平台上进行了一组实验，证明了CyberForce可以高精度地学习适合防御零日攻击的最佳防御策略，并且在所有客户端受到所有攻击时，FRL Agent比中央RL Agent更快速地训练和选择合适的防御策略。在不同的客户端遭受不同的攻击时，CyberForce客户端可以从其他客户端中获得知识并采用相似的攻击行为。此外，CyberForce还显示了强大的数据欺诈攻击Robustness。<details>
<summary>Abstract</summary>
The expansion of the Internet-of-Things (IoT) paradigm is inevitable, but vulnerabilities of IoT devices to malware incidents have become an increasing concern. Recent research has shown that the integration of Reinforcement Learning with Moving Target Defense (MTD) mechanisms can enhance cybersecurity in IoT devices. Nevertheless, the numerous new malware attacks and the time that agents take to learn and select effective MTD techniques make this approach impractical for real-world IoT scenarios. To tackle this issue, this work presents CyberForce, a framework that employs Federated Reinforcement Learning (FRL) to collectively and privately determine suitable MTD techniques for mitigating diverse zero-day attacks. CyberForce integrates device fingerprinting and anomaly detection to reward or penalize MTD mechanisms chosen by an FRL-based agent. The framework has been evaluated in a federation consisting of ten devices of a real IoT platform. A pool of experiments with six malware samples affecting the devices has demonstrated that CyberForce can precisely learn optimum MTD mitigation strategies. When all clients are affected by all attacks, the FRL agent exhibits high accuracy and reduced training time when compared to a centralized RL agent. In cases where different clients experience distinct attacks, the CyberForce clients gain benefits through the transfer of knowledge from other clients and similar attack behavior. Additionally, CyberForce showcases notable robustness against data poisoning attacks.
</details>
<details>
<summary>摘要</summary>
互联网物料（IoT）的扩展是不可避免的，但是IoT设备对于恶意软件攻击的脆弱性已经成为一个增加的 Concern。 latest research 表明，将强化学习与移动目标防御（MTD）机制相结合可以强化IoT设备的防护。然而，新的恶意软件攻击和代理人执行时间使得这种方法在实际的IoT应用中不实际。为解决这个问题，这个研究呈现了CyberForce框架，这个框架使用联邦强化学习（FRL）来集体和私有地决定适当的MTD策略以避免多种零日攻击。CyberForce框架具有设备识别和偏见检测，将选择的MTD机制作为FRL基于的代理人奖励或惩罚。在一个真实的IoT平台上的十台设备组成的联邦中进行了评估。一系列实验显示，CyberForce可以精确地学习适当的MTD防御策略。当所有客户端受到所有攻击时，FRL代理人比中央RL代理人更高精度和较少的训练时间。在不同的客户端遭受不同的攻击时，CyberForce客户端从其他客户端和相似的攻击行为中获得了知识转移。此外，CyberForce还表现出了杰出的抗毒血统特性。
</details></li>
</ul>
<hr>
<h2 id="Tweet-Sentiment-Extraction-using-Viterbi-Algorithm-with-Transfer-Learning"><a href="#Tweet-Sentiment-Extraction-using-Viterbi-Algorithm-with-Transfer-Learning" class="headerlink" title="Tweet Sentiment Extraction using Viterbi Algorithm with Transfer Learning"></a>Tweet Sentiment Extraction using Viterbi Algorithm with Transfer Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05973">http://arxiv.org/abs/2308.05973</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Zied130/Tweet_Sentiment-">https://github.com/Zied130/Tweet_Sentiment-</a></li>
<li>paper_authors: Zied Baklouti</li>
<li>for: 本研究旨在Identifying tweet sentence中的情感部分。</li>
<li>methods: 该研究基于Modified Viterbi algorithm，并引入了信任分数和向量作为内部评估指标。</li>
<li>results: 研究发现，通过调整非 Parametric 模型，可以获得高度解释性的结果，并且信任分数向量可以准确地表示模型对最不确定预测状态的不确idence。<details>
<summary>Abstract</summary>
Tweet sentiment extraction extracts the most significant portion of the sentence, determining whether the sentiment is positive or negative. This research aims to identify the part of tweet sentences that strikes any emotion. To reach this objective, we continue improving the Viterbi algorithm previously modified by the author to make it able to receive pre-trained model parameters. We introduce the confidence score and vector as two indicators responsible for evaluating the model internally before assessing the final results. We then present a method to fine-tune this nonparametric model. We found that the model gets highly explainable as the confidence score vector reveals precisely where the least confidence predicted states are and if the modifications approved ameliorate the confidence score or if the tuning is going in the wrong direction.
</details>
<details>
<summary>摘要</summary>
《推文情感EXTraction》是一项研究，旨在从推文句子中提取情感的最重要部分，以确定情感是正面或负面。为达到这个目标，我们继续改进了作者已经修改过的维特比 алгоритм，以使其能够接受预训练模型参数。我们引入了信任分数和向量作为两个评估模型的内部指标，以评估模型的性能。然后，我们提出了一种方法来细化这种非 Parametric 模型。我们发现，模型在信任分数向量的指导下变得非常可解释，并且可以准确地描述最不信任的预测状态和修改是否有助于提高信任分数。
</details></li>
</ul>
<hr>
<h2 id="An-Encoder-Decoder-Approach-for-Packing-Circles"><a href="#An-Encoder-Decoder-Approach-for-Packing-Circles" class="headerlink" title="An Encoder-Decoder Approach for Packing Circles"></a>An Encoder-Decoder Approach for Packing Circles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07335">http://arxiv.org/abs/2308.07335</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akshay Kiran Jose, Gangadhar Karevvanavar, Rajshekhar V Bhat</li>
<li>for: 本研究旨在解决一个多年来吸引了广泛关注的封装问题，即将小型对象封装在大型对象中，并且要求小型对象不可以相互重叠或者尽量减少重叠。</li>
<li>methods: 本研究提出了一种新的编码器-解码器架构，包括编码器块、扰动块和解码器块，用于封装同形圆形在大型圆形中。该方法中，编码器接受一个圆形的标识符作为输入，并通过一个归一化层输出圆心，扰动层添加了控制的扰动，使圆心不能超过小圆形的半径，而解码器接受扰动后的圆心作为输入，并估算出封装的圆形标识符。</li>
<li>results: 该方法可以对高维度和不同形状的对象进行封装，并且可以提供竞争性的性能 compared to 经典方法。<details>
<summary>Abstract</summary>
The problem of packing smaller objects within a larger object has been of interest since decades. In these problems, in addition to the requirement that the smaller objects must lie completely inside the larger objects, they are expected to not overlap or have minimum overlap with each other. Due to this, the problem of packing turns out to be a non-convex problem, obtaining whose optimal solution is challenging. As such, several heuristic approaches have been used for obtaining sub-optimal solutions in general, and provably optimal solutions for some special instances. In this paper, we propose a novel encoder-decoder architecture consisting of an encoder block, a perturbation block and a decoder block, for packing identical circles within a larger circle. In our approach, the encoder takes the index of a circle to be packed as an input and outputs its center through a normalization layer, the perturbation layer adds controlled perturbations to the center, ensuring that it does not deviate beyond the radius of the smaller circle to be packed, and the decoder takes the perturbed center as input and estimates the index of the intended circle for packing. We parameterize the encoder and decoder by a neural network and optimize it to reduce an error between the decoder's estimated index and the actual index of the circle provided as input to the encoder. The proposed approach can be generalized to pack objects of higher dimensions and different shapes by carefully choosing normalization and perturbation layers. The approach gives a sub-optimal solution and is able to pack smaller objects within a larger object with competitive performance with respect to classical methods.
</details>
<details>
<summary>摘要</summary>
“ Packing smaller objects within a larger object 已经是多年来的研究问题。在这些问题中，除了要求小 objet完全嵌入大 objet 之外，还需要避免它们之间的重叠或最小化重叠。由于这个原因， packing 问题变得非 convex 的，获得优化的解决方案具有挑战性。因此，许多启发法被用来获得不优化的解决方案，以及对特殊情况下的可证优化解决方案。在这篇论文中，我们提出了一种新的编码器-解码器架构，包括编码器块、扰动块和解码器块，用于嵌入 identical circles  within a larger circle。在我们的方法中，编码器接受一个圆的索引作为输入，并通过normalization layer输出圆心，perturbation layer添加了控制的扰动，确保圆心不会超过小圆的半径，而解码器接受扰动后的圆心作为输入，并估计圆的索引。我们将编码器和解码器参数化为神经网络，并优化它以降低神经网络的输出与实际输入圆的索引之间的错误。我们的方法可以扩展到嵌入高维度和不同形状的对象，通过合适的 normalization 和扰动层来进行parameterization。我们的方法可以提供竞争性的性能，并且可以嵌入小对象 within a larger object 中。”
</details></li>
</ul>
<hr>
<h2 id="Decentralised-Governance-for-Foundation-Model-based-Systems-Exploring-the-Role-of-Blockchain-in-Responsible-AI"><a href="#Decentralised-Governance-for-Foundation-Model-based-Systems-Exploring-the-Role-of-Blockchain-in-Responsible-AI" class="headerlink" title="Decentralised Governance for Foundation Model based Systems: Exploring the Role of Blockchain in Responsible AI"></a>Decentralised Governance for Foundation Model based Systems: Exploring the Role of Blockchain in Responsible AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05962">http://arxiv.org/abs/2308.05962</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Liu, Qinghua Lu, Liming Zhu, Hye-Young Paik</li>
<li>for: 本研究旨在探讨基础模型 Based AI 系统的治理问题，以确保其可靠性和避免滥用，并对人类、社会和环境造成害。</li>
<li>methods: 本研究采用了 eight 个治理挑战，涵盖基础模型 Based AI 系统的三个基本维度：决策权、激励和责任。此外，研究还探讨了使用区块链技术来解决这些挑战的可能性。</li>
<li>results: 研究表明，使用区块链技术可以实现基础模型 Based AI 系统的分布式治理，并提高其可靠性和安全性。<details>
<summary>Abstract</summary>
Foundation models are increasingly attracting interest worldwide for their distinguished capabilities and potential to perform a wide variety of tasks. Nevertheless, people are concerned about whether foundation model based AI systems are properly governed to ensure trustworthiness of foundation model based AI systems and to prevent misuse that could harm humans, society and the environment. In this paper, we identify eight governance challenges in the entire lifecycle of foundation model based AI systems regarding the three fundamental dimensions of governance: decision rights, incentives, and accountability. Furthermore, we explore the potential of blockchain as a solution to address the challenges by providing a distributed ledger to facilitate decentralised governance. We present an architecture that demonstrates how blockchain can be leveraged to realise governance in foundation model based AI systems.
</details>
<details>
<summary>摘要</summary>
基础模型在全球引起了越来越多的关注，因为它们具有突出的能力和可以执行各种任务。然而，人们担心基础模型基于的AI系统是否得到了适当的管理，以确保该系统的可靠性和避免滥用，以避免对人类、社会和环境造成伤害。在这篇论文中，我们认为基础模型基于AI系统的管理存在八个挑战，这些挑战分布在三个基本维度上：决策权、激励和责任。此外，我们还探讨了使用区块链解决这些挑战的可能性，并提出了一种架构，以示如何使用区块链实现基础模型基于AI系统的管理。
</details></li>
</ul>
<hr>
<h2 id="BOLAA-Benchmarking-and-Orchestrating-LLM-augmented-Autonomous-Agents"><a href="#BOLAA-Benchmarking-and-Orchestrating-LLM-augmented-Autonomous-Agents" class="headerlink" title="BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous Agents"></a>BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05960">http://arxiv.org/abs/2308.05960</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/salesforce/bolaa">https://github.com/salesforce/bolaa</a></li>
<li>paper_authors: Zhiwei Liu, Weiran Yao, Jianguo Zhang, Le Xue, Shelby Heinecke, Rithesh Murthy, Yihao Feng, Zeyuan Chen, Juan Carlos Niebles, Devansh Arpit, Ran Xu, Phil Mui, Huan Wang, Caiming Xiong, Silvio Savarese</li>
<li>for: 这篇论文旨在比较不同类型的自主代理（LAA）和大语言模型（LLM）的可比性，以及提出一种新的多代理管理策略，以提高LAA在各种决策和多步逻辑环境中的表现。</li>
<li>methods: 论文使用了多种代理体系和LLM脊梁，并进行了广泛的 simulations  validate  LAAs 的性能。</li>
<li>results: 研究结果表明，BOLAA 可以在各种环境中提高 LAAs 的表现，并且可以提供可靠的量化建议 для LAA 的设计和 LLMS 的选择。<details>
<summary>Abstract</summary>
The massive successes of large language models (LLMs) encourage the emerging exploration of LLM-augmented Autonomous Agents (LAAs). An LAA is able to generate actions with its core LLM and interact with environments, which facilitates the ability to resolve complex tasks by conditioning on past interactions such as observations and actions. Since the investigation of LAA is still very recent, limited explorations are available. Therefore, we provide a comprehensive comparison of LAA in terms of both agent architectures and LLM backbones. Additionally, we propose a new strategy to orchestrate multiple LAAs such that each labor LAA focuses on one type of action, \textit{i.e.} BOLAA, where a controller manages the communication among multiple agents. We conduct simulations on both decision-making and multi-step reasoning environments, which comprehensively justify the capacity of LAAs. Our performance results provide quantitative suggestions for designing LAA architectures and the optimal choice of LLMs, as well as the compatibility of both. We release our implementation code of LAAs to the public at \url{https://github.com/salesforce/BOLAA}.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的巨大成功激发了LAA的潜在探索（LLM-augmented Autonomous Agents）。LAA可以通过核心LLM生成动作并与环境交互，从而实现根据过去交互的条件来解决复杂任务。由于LAA的探索还很新，有限的探索结果 disponible。因此，我们提供了LAA的全面比较，包括代理建筑和LLM脊梁。此外，我们提议一种新的策略，使得每个劳动LAA专注于一种类型的动作，即BOLAA，其中一个控制器负责多个代理之间的交流。我们在决策和多步逻辑环境中进行了模拟，全面证明了LAAs的能力。我们的性能结果提供了LLA的建 architecture和LLM的优选，以及这两者之间的兼容性。我们在GitHub上公开了LAAs的实现代码，请参考\url{https://github.com/salesforce/BOLAA}.
</details></li>
</ul>
<hr>
<h2 id="FoodSAM-Any-Food-Segmentation"><a href="#FoodSAM-Any-Food-Segmentation" class="headerlink" title="FoodSAM: Any Food Segmentation"></a>FoodSAM: Any Food Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05938">http://arxiv.org/abs/2308.05938</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jamesjg/foodsam">https://github.com/jamesjg/foodsam</a></li>
<li>paper_authors: Xing Lan, Jiayi Lyu, Hanyu Jiang, Kun Dong, Zehai Niu, Yi Zhang, Jian Xue</li>
<li>for: 这篇论文探讨了Segment Anything Model（SAM）在食物图像分割中的零基础能力。</li>
<li>methods: 作者提出了一种新的框架，即FoodSAM，用于增强SAM生成的mask的semantic segmentation质量。此外，作者还提出了一种基于独立个体的思想，对食物图像进行实例分割。</li>
<li>results: 广泛的实验表明FoodSAM可以有效地分割食物项目，并且可以在多种级别进行分割。此外，FoodSAM还可以实现实例、�anoptic和Promptable segmentation，是首次在食物图像分割领域实现这些功能的工作。<details>
<summary>Abstract</summary>
In this paper, we explore the zero-shot capability of the Segment Anything Model (SAM) for food image segmentation. To address the lack of class-specific information in SAM-generated masks, we propose a novel framework, called FoodSAM. This innovative approach integrates the coarse semantic mask with SAM-generated masks to enhance semantic segmentation quality. Besides, we recognize that the ingredients in food can be supposed as independent individuals, which motivated us to perform instance segmentation on food images. Furthermore, FoodSAM extends its zero-shot capability to encompass panoptic segmentation by incorporating an object detector, which renders FoodSAM to effectively capture non-food object information. Drawing inspiration from the recent success of promptable segmentation, we also extend FoodSAM to promptable segmentation, supporting various prompt variants. Consequently, FoodSAM emerges as an all-encompassing solution capable of segmenting food items at multiple levels of granularity. Remarkably, this pioneering framework stands as the first-ever work to achieve instance, panoptic, and promptable segmentation on food images. Extensive experiments demonstrate the feasibility and impressing performance of FoodSAM, validating SAM's potential as a prominent and influential tool within the domain of food image segmentation. We release our code at https://github.com/jamesjg/FoodSAM.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们探讨Segment Anything Model（SAM）在食品图像分割中的零类能力。为了 Address SAM生成的masks中缺乏类别特定信息，我们提出了一种新的框架，叫做FoodSAM。这种创新的approach integrates the coarse semantic mask with SAM-generated masks，以提高semantic segmentation的质量。此外，我们认为食品中的 ингредиënces可以 viewed as independent individuals，这使我们能够在食品图像上进行实例分割。此外，FoodSAM还扩展了零类能力，以包括泛素分割，通过添加一个对象检测器，使FoodSAM能够有效地捕捉非食品对象信息。 Drawing inspiration from the recent success of promptable segmentation，我们也 extend FoodSAM to promptable segmentation，支持多种提示变体。因此，FoodSAM emerges as an all-encompassing solution capable of segmenting food items at multiple levels of granularity。值得一提的是，这是首次实现在食品图像上实现实例、泛素和可提示分割的工作。extensive experiments demonstrate the feasibility and impressive performance of FoodSAM，证明SAM的潜在力量以及其作为食品图像分割领域的引用工具。我们在https://github.com/jamesjg/FoodSAM上发布了我们的代码。
</details></li>
</ul>
<hr>
<h2 id="A-Deep-Recurrent-Reinforcement-Learning-Method-for-Intelligent-AutoScaling-of-Serverless-Functions"><a href="#A-Deep-Recurrent-Reinforcement-Learning-Method-for-Intelligent-AutoScaling-of-Serverless-Functions" class="headerlink" title="A Deep Recurrent-Reinforcement Learning Method for Intelligent AutoScaling of Serverless Functions"></a>A Deep Recurrent-Reinforcement Learning Method for Intelligent AutoScaling of Serverless Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05937">http://arxiv.org/abs/2308.05937</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siddharth Agarwal, Maria A. Rodriguez, Rajkumar Buyya</li>
<li>for:  Function autoscaling in cloud environments, specifically for IoT-edge data processing and anomaly detection.</li>
<li>methods:  Model-free Recurrent RL agent and Proximal Policy Optimization (PPO) algorithm.</li>
<li>results:  Improved throughput, function execution, and accounted for more function instances compared to commercially used threshold-based function autoscaling.Here’s the full text in Simplified Chinese:</li>
<li>for: 函数自适应缩放在云环境中，尤其是 для IoT-edge数据处理和异常检测。</li>
<li>methods: 模型自由Recurrent RL代理和Proximal Policy Optimization（PPO）算法。</li>
<li>results: 提高通过率、函数执行率和负担更多的函数实例相比于商业使用的阈值基于函数自适应缩放。<details>
<summary>Abstract</summary>
Function-as-a-Service (FaaS) introduces a lightweight, function-based cloud execution model that finds its relevance in applications like IoT-edge data processing and anomaly detection. While CSP offer a near-infinite function elasticity, these applications often experience fluctuating workloads and stricter performance constraints. A typical CSP strategy is to empirically determine and adjust desired function instances, "autoscaling", based on monitoring-based thresholds such as CPU or memory, to cope with demand and performance. However, threshold configuration either requires expert knowledge, historical data or a complete view of environment, making autoscaling a performance bottleneck lacking an adaptable solution.RL algorithms are proven to be beneficial in analysing complex cloud environments and result in an adaptable policy that maximizes the expected objectives. Most realistic cloud environments usually involve operational interference and have limited visibility, making them partially observable. A general solution to tackle observability in highly dynamic settings is to integrate Recurrent units with model-free RL algorithms and model a decision process as a POMDP. Therefore, in this paper, we investigate a model-free Recurrent RL agent for function autoscaling and compare it against the model-free Proximal Policy Optimisation (PPO) algorithm. We explore the integration of a LSTM network with the state-of-the-art PPO algorithm to find that under our experimental and evaluation settings, recurrent policies were able to capture the environment parameters and show promising results for function autoscaling. We further compare a PPO-based autoscaling agent with commercially used threshold-based function autoscaling and posit that a LSTM-based autoscaling agent is able to improve throughput by 18%, function execution by 13% and account for 8.4% more function instances.
</details>
<details>
<summary>摘要</summary>
Function-as-a-Service (FaaS) 引入了一种轻量级、功能基于云执行模型，在 IoT-edge 数据处理和异常检测等应用中发挥作用。而 CSP 提供了近乎无限的功能灵活性，但这些应用经常遇到波动性的工作负荷和更严格的性能限制。一般 CSP 策略是通过实际观察数据或历史数据来确定和调整所需的功能实例数量，以适应需求和性能。但是，这种策略通常需要专业知识、历史数据或完整的环境视图，从而导致自适应缩放成性能瓶颈。RL 算法已经在分析复杂云环境方面展现出了有利的特点，因此在这篇论文中，我们将 investigate 一种基于 POMDP 的模型自由 RL 代理来解决函数自适应缩放问题。我们将比较使用 PPO 算法和 LSTM 网络来模型决策过程，并发现在我们的实验和评估环境下，循环策略能够捕捉环境参数并显示出扎实的结果。我们进一步比较了使用 PPO 算法进行自适应缩放的代理和商业使用的阈值基于自适应缩放，并论证 LSTM 基于的自适应缩放代理能够提高吞吐量by 18%、功能执行by 13% 和覆盖8.4%更多的功能实例。
</details></li>
</ul>
<hr>
<h2 id="LittleMu-Deploying-an-Online-Virtual-Teaching-Assistant-via-Heterogeneous-Sources-Integration-and-Chain-of-Teach-Prompts"><a href="#LittleMu-Deploying-an-Online-Virtual-Teaching-Assistant-via-Heterogeneous-Sources-Integration-and-Chain-of-Teach-Prompts" class="headerlink" title="LittleMu: Deploying an Online Virtual Teaching Assistant via Heterogeneous Sources Integration and Chain of Teach Prompts"></a>LittleMu: Deploying an Online Virtual Teaching Assistant via Heterogeneous Sources Integration and Chain of Teach Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05935">http://arxiv.org/abs/2308.05935</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thu-keg/vta">https://github.com/thu-keg/vta</a></li>
<li>paper_authors: Shangqing Tu, Zheyuan Zhang, Jifan Yu, Chunyang Li, Siyu Zhang, Zijun Yao, Lei Hou, Juanzi Li</li>
<li>for: 这篇论文旨在提供一个基于少量标注数据的虚拟MOOC教学助手，以支持在线学习。</li>
<li>methods: 该系统包括两个互动模块：一是结构化、半结构化和无结构化知识源的集成，以提供准确的答案；另一个是通过大规模预训练模型的“链式教学”示例，处理复杂的未收集问题。</li>
<li>results: 作者在线测试和实际投入中证明了该系统的性能，并在XuetangX MOOC平台上服务了超过80,000名用户，处理了超过300,000个问题。<details>
<summary>Abstract</summary>
Teaching assistants have played essential roles in the long history of education. However, few MOOC platforms are providing human or virtual teaching assistants to support learning for massive online students due to the complexity of real-world online education scenarios and the lack of training data. In this paper, we present a virtual MOOC teaching assistant, LittleMu with minimum labeled training data, to provide question answering and chit-chat services. Consisting of two interactive modules of heterogeneous retrieval and language model prompting, LittleMu first integrates structural, semi- and unstructured knowledge sources to support accurate answers for a wide range of questions. Then, we design delicate demonstrations named "Chain of Teach" prompts to exploit the large-scale pre-trained model to handle complex uncollected questions. Except for question answering, we develop other educational services such as knowledge-grounded chit-chat. We test the system's performance via both offline evaluation and online deployment. Since May 2020, our LittleMu system has served over 80,000 users with over 300,000 queries from over 500 courses on XuetangX MOOC platform, which continuously contributes to a more convenient and fair education. Our code, services, and dataset will be available at https://github.com/THU-KEG/VTA.
</details>
<details>
<summary>摘要</summary>
教学助手在教育历史中扮演了关键角色，但目前许多MOOC平台没有提供人工或虚拟教学助手来支持在线学习者，这主要因为在线教育场景复杂，缺乏培训数据。在这篇论文中，我们提出了一个名为“小慕”的虚拟MOOC教学助手，可以提供问答和聊天服务。“小慕”包括两个互动模块：一是结构化、半结构化和无结构化知识源的集成，以支持各种问题的准确答案。其次，我们设计了细腻的示例名为“链条教”，以利用大规模预训练模型来处理复杂的未收集问题。除了问答外，我们还开发了其他教育服务，如知识基于聊天。我们对系统的性能进行了线上评估和下载测试。自2020年5月以来，我们的“小慕”系统已经为超过80,000名用户提供了超过300,000个问题的回答，从超过500门课程中获得了XuetangX MOOC平台的线上执行。我们的代码、服务和数据将在https://github.com/THU-KEG/VTA上提供。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Phenotype-Recognition-in-Clinical-Notes-Using-Large-Language-Models-PhenoBCBERT-and-PhenoGPT"><a href="#Enhancing-Phenotype-Recognition-in-Clinical-Notes-Using-Large-Language-Models-PhenoBCBERT-and-PhenoGPT" class="headerlink" title="Enhancing Phenotype Recognition in Clinical Notes Using Large Language Models: PhenoBCBERT and PhenoGPT"></a>Enhancing Phenotype Recognition in Clinical Notes Using Large Language Models: PhenoBCBERT and PhenoGPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06294">http://arxiv.org/abs/2308.06294</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingye Yang, Cong Liu, Wendy Deng, Da Wu, Chunhua Weng, Yunyun Zhou, Kai Wang</li>
<li>for: 本研究旨在开发一种基于transformer架构的大语言模型（LLM），以便自动检测临床现象术语，包括HPO未ocument的术语。</li>
<li>methods: 我们开发了两种模型：PhenoBCBERT和PhenoGPT。PhenoBCBERT使用 Bio+Clinical BERT 作为预训模型，而 PhenoGPT 则可以从多种 GPT 模型中 initialize，包括开源版本如 GPT-J、Falcon 和 LLaMA，以及关闭源版本如 GPT-3 和 GPT-3.5。</li>
<li>results: 我们发现我们的方法可以从临床观察纪录中提取更多的现象术语，包括 novel 的术语不受 HPO 规范。我们还进行了生物医学文献中的案例研究，以示新现象信息的识别和提取。我们比较了现有的 BERT 基本的 versus GPT 基本的模型，包括模型架构、内存使用、速度、准确率和隐私保护等多个方面。<details>
<summary>Abstract</summary>
We hypothesize that large language models (LLMs) based on the transformer architecture can enable automated detection of clinical phenotype terms, including terms not documented in the HPO. In this study, we developed two types of models: PhenoBCBERT, a BERT-based model, utilizing Bio+Clinical BERT as its pre-trained model, and PhenoGPT, a GPT-based model that can be initialized from diverse GPT models, including open-source versions such as GPT-J, Falcon, and LLaMA, as well as closed-source versions such as GPT-3 and GPT-3.5. We compared our methods with PhenoTagger, a recently developed HPO recognition tool that combines rule-based and deep learning methods. We found that our methods can extract more phenotype concepts, including novel ones not characterized by HPO. We also performed case studies on biomedical literature to illustrate how new phenotype information can be recognized and extracted. We compared current BERT-based versus GPT-based models for phenotype tagging, in multiple aspects including model architecture, memory usage, speed, accuracy, and privacy protection. We also discussed the addition of a negation step and an HPO normalization layer to the transformer models for improved HPO term tagging. In conclusion, PhenoBCBERT and PhenoGPT enable the automated discovery of phenotype terms from clinical notes and biomedical literature, facilitating automated downstream tasks to derive new biological insights on human diseases.
</details>
<details>
<summary>摘要</summary>
We compared current BERT-based versus GPT-based models for phenotype tagging in multiple aspects, including model architecture, memory usage, speed, accuracy, and privacy protection. We also discussed the addition of a negation step and an HPO normalization layer to the transformer models for improved HPO term tagging. In conclusion, PhenoBCBERT and PhenoGPT enable the automated discovery of phenotype terms from clinical notes and biomedical literature, facilitating automated downstream tasks to derive new biological insights on human diseases.Translation notes:* "HPO" is translated as "人类疾病诊断 Ontology" (human disease diagnosis ontology)* "Phenotype" is translated as "诊断特征" (diagnostic feature)* "Clinical notes" is translated as "医疗记录" (medical records)* "Biomedical literature" is translated as "生物医学文献" (biomedical literature)* "BERT" is translated as " Bio+Clinical BERT" (Bio+Clinical BERT)* "GPT" is translated as " GPT-based model" (GPT-based model)* "PhenoTagger" is translated as "HPO识别工具" (HPO recognition tool)* "Rule-based" is translated as "规则基于的" (rule-based)* "Deep learning" is translated as "深度学习" (deep learning)* "Memory usage" is translated as "内存使用" (memory usage)* "Speed" is translated as "速度" (speed)* "Accuracy" is translated as "准确率" (accuracy)* "Privacy protection" is translated as "隐私保护" (privacy protection)* "Negation step" is translated as "否定步骤" (negation step)* "HPO normalization layer" is translated as "HPO正常化层" (HPO normalization layer)
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Team-Based-Navigation-A-Review-of-Deep-Reinforcement-Learning-Techniques-for-Multi-Agent-Pathfinding"><a href="#Learning-to-Team-Based-Navigation-A-Review-of-Deep-Reinforcement-Learning-Techniques-for-Multi-Agent-Pathfinding" class="headerlink" title="Learning to Team-Based Navigation: A Review of Deep Reinforcement Learning Techniques for Multi-Agent Pathfinding"></a>Learning to Team-Based Navigation: A Review of Deep Reinforcement Learning Techniques for Multi-Agent Pathfinding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05893">http://arxiv.org/abs/2308.05893</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaehoon Chung, Jamil Fayyad, Younes Al Younes, Homayoun Najjaran</li>
<li>For: 本论文主要探讨了多 Agent Pathfinding（MAPF）领域中 Deep Reinforcement Learning（DRL）的应用，并提供了一个综合的评估 metric 来评估不同的 MAPF 算法。* Methods: 本文使用了 DRL 技术来解决 MAPF 中的复杂问题，并提供了一个综合的评估 metric 来评估不同的 MAPF 算法。* Results: 本文提供了一个综合的评估 metric 来评估不同的 MAPF 算法，并介绍了 Model-based DRL 作为未来研究的可能性，以及其所需的基础理解。<details>
<summary>Abstract</summary>
Multi-agent pathfinding (MAPF) is a critical field in many large-scale robotic applications, often being the fundamental step in multi-agent systems. The increasing complexity of MAPF in complex and crowded environments, however, critically diminishes the effectiveness of existing solutions. In contrast to other studies that have either presented a general overview of the recent advancements in MAPF or extensively reviewed Deep Reinforcement Learning (DRL) within multi-agent system settings independently, our work presented in this review paper focuses on highlighting the integration of DRL-based approaches in MAPF. Moreover, we aim to bridge the current gap in evaluating MAPF solutions by addressing the lack of unified evaluation metrics and providing comprehensive clarification on these metrics. Finally, our paper discusses the potential of model-based DRL as a promising future direction and provides its required foundational understanding to address current challenges in MAPF. Our objective is to assist readers in gaining insight into the current research direction, providing unified metrics for comparing different MAPF algorithms and expanding their knowledge of model-based DRL to address the existing challenges in MAPF.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="DF2-Distribution-Free-Decision-Focused-Learning"><a href="#DF2-Distribution-Free-Decision-Focused-Learning" class="headerlink" title="DF2: Distribution-Free Decision-Focused Learning"></a>DF2: Distribution-Free Decision-Focused Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05889">http://arxiv.org/abs/2308.05889</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lingkai Kong, Wenhao Mu, Jiaming Cui, Yuchen Zhuang, B. Aditya Prakash, Bo Dai, Chao Zhang</li>
<li>for: 这篇论文是关于解决预测然后优化问题的决策尝试学（DFL）方法中的三个瓶颈的研究。</li>
<li>methods: 该论文提出了一种新的分布自由决策尝试学方法（DF2），该方法可以解决预测模型与优化目标之间的模型匹配错误、样本平均approximation错误和梯度approximation错误。</li>
<li>results: 该论文通过在一个 sintetic 问题、一个风力发电拍卖问题和一个非几何疫苗分布问题上进行测试，证明了 DF2 的有效性。<details>
<summary>Abstract</summary>
Decision-focused learning (DFL) has recently emerged as a powerful approach for predict-then-optimize problems by customizing a predictive model to a downstream optimization task. However, existing end-to-end DFL methods are hindered by three significant bottlenecks: model mismatch error, sample average approximation error, and gradient approximation error. Model mismatch error stems from the misalignment between the model's parameterized predictive distribution and the true probability distribution. Sample average approximation error arises when using finite samples to approximate the expected optimization objective. Gradient approximation error occurs as DFL relies on the KKT condition for exact gradient computation, while most methods approximate the gradient for backpropagation in non-convex objectives. In this paper, we present DF2 -- the first \textit{distribution-free} decision-focused learning method explicitly designed to address these three bottlenecks. Rather than depending on a task-specific forecaster that requires precise model assumptions, our method directly learns the expected optimization function during training. To efficiently learn the function in a data-driven manner, we devise an attention-based model architecture inspired by the distribution-based parameterization of the expected objective. Our method is, to the best of our knowledge, the first to address all three bottlenecks within a single model. We evaluate DF2 on a synthetic problem, a wind power bidding problem, and a non-convex vaccine distribution problem, demonstrating the effectiveness of DF2.
</details>
<details>
<summary>摘要</summary>
决策关注学习（DFL）是一种有力的方法，用于预测然后优化问题，通过适应下游优化任务中的预测模型。然而，现有的端到端DFL方法受到三大瓶颈：模型匹配错误、样本平均预测错误和梯度近似错误。模型匹配错误来自预测模型参数化预测分布与真实概率分布的不一致。样本平均预测错误发生在使用有限样本来近似优化目标函数的时候。梯度近似错误由于DFL依赖于KKT条件来确定梯度，而大多数方法在非 convex 目标函数中使用梯度近似来进行反向传播。在这篇论文中，我们提出了DF2方法——首个不偏 towards任务特定预测器的分布自由决策关注学习方法。相比于基于任务特定预测器的方法，我们的方法直接在训练中学习预期优化函数。为效率地学习函数，我们设计了一种注意力基于分布参数化的预测模型。我们的方法是，到目前为止，第一个同时解决三大瓶颈的方法。我们在一个 sintetic 问题、一个风力发电拍卖问题和一个非 convex vaccine distribution问题上评估了DF2方法，并证明了其效果。
</details></li>
</ul>
<hr>
<h2 id="Shared-Memory-contention-aware-Concurrent-DNN-Execution-for-Diversely-Heterogeneous-System-on-Chips"><a href="#Shared-Memory-contention-aware-Concurrent-DNN-Execution-for-Diversely-Heterogeneous-System-on-Chips" class="headerlink" title="Shared Memory-contention-aware Concurrent DNN Execution for Diversely Heterogeneous System-on-Chips"></a>Shared Memory-contention-aware Concurrent DNN Execution for Diversely Heterogeneous System-on-Chips</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05869">http://arxiv.org/abs/2308.05869</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ismet Dagli, Mehmet Belviranli</li>
<li>for: 这篇论文是为了提出一种新的策略来管理移动和自动化系统中的多个工作负荷，以提高系统的性能和资源利用率。</li>
<li>methods: 该论文使用了一种新的策略，即HaX-CoNN，来映射具有不同加速器的concurrently执行深度神经网络（DNN）推理任务到SoC中的多种加速器中。该策略考虑了每层执行特性、共享内存（SM）竞争和间接加速器转换，以找到最佳调度。</li>
<li>results: 实验结果表明，HaX-CoNN可以减少SM竞争量达45%，并且可以提高延迟和总吞吐量相对于现有方法的32%和29%。<details>
<summary>Abstract</summary>
Two distinguishing features of state-of-the-art mobile and autonomous systems are 1) there are often multiple workloads, mainly deep neural network (DNN) inference, running concurrently and continuously; and 2) they operate on shared memory system-on-chips (SoC) that embed heterogeneous accelerators tailored for specific operations. State-of-the-art lacks efficient performance and resource management techniques necessary to either maximize total system throughput or minimize end-to-end workload latency. In this work, we propose HaX-CoNN, a novel scheme that characterizes and maps layers in concurrently executing DNN inference workloads to a diverse set of accelerators within a SoC. Our scheme uniquely takes per-layer execution characteristics, shared memory (SM) contention, and inter-accelerator transitions into account to find optimal schedules. We evaluate HaX-CoNN on NVIDIA Orin, NVIDIA Xavier, and Qualcomm Snapdragon 865 SoCs. Our experimental results indicate that HaX-CoNN minimizes memory contention by up to 45% and can improve latency and total throughput by up to 32% and 29%, respectively, compared to the state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
两个特点 distinguishing state-of-the-art 移动和自动化系统是：1）经常有多个工作负载，主要是深度神经网络（DNN）推理，同时并发运行；2）它们运行在共享内存系统在板（SoC）中，该系统嵌入特化为特定操作的多种加速器。现状缺乏有效的性能和资源管理技术，以最大化总系统吞吐量或最小化终端工作负载延迟。在这种工作中，我们提出了 HaX-CoNN 方案，它将在同时执行的 DNN 推理工作负载中映射层到 SoC 中的多种加速器中。我们的方案独特地考虑每层执行特性、共享内存（SM）竞争以及 между加速器的转换，以找到最佳时间表。我们对 NVIDIA Orin、NVIDIA Xavier 和 Qualcomm Snapdragon 865 SoC 进行了实验。我们的实验结果表明，HaX-CoNN 可以最大化内存竞争减少至 45%，并可以提高延迟和总吞吐量相对于现状方法的 32% 和 29%。
</details></li>
</ul>
<hr>
<h2 id="Unleashing-the-Strengths-of-Unlabeled-Data-in-Pan-cancer-Abdominal-Organ-Quantification-the-FLARE22-Challenge"><a href="#Unleashing-the-Strengths-of-Unlabeled-Data-in-Pan-cancer-Abdominal-Organ-Quantification-the-FLARE22-Challenge" class="headerlink" title="Unleashing the Strengths of Unlabeled Data in Pan-cancer Abdominal Organ Quantification: the FLARE22 Challenge"></a>Unleashing the Strengths of Unlabeled Data in Pan-cancer Abdominal Organ Quantification: the FLARE22 Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05862">http://arxiv.org/abs/2308.05862</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/junma11/flare">https://github.com/junma11/flare</a></li>
<li>paper_authors: Jun Ma, Yao Zhang, Song Gu, Cheng Ge, Shihao Ma, Adamo Young, Cheng Zhu, Kangkang Meng, Xin Yang, Ziyan Huang, Fan Zhang, Wentao Liu, YuanKe Pan, Shoujin Huang, Jiacheng Wang, Mingze Sun, Weixin Xu, Dengqiang Jia, Jae Won Choi, Natália Alves, Bram de Wilde, Gregor Koehler, Yajun Wu, Manuel Wiesenfarth, Qiongjie Zhu, Guoqiang Dong, Jian He, the FLARE Challenge Consortium, Bo Wang</li>
<li>for: 这篇论文的目的是要探讨自动化腹部疾病诊断和治疗规划中的量化器官评估。</li>
<li>methods: 这篇论文使用了许多人工智能（AI）算法，以测试它们在实际世界中的多元国际设定下的精度和效率。</li>
<li>results: 这篇论文发现了一些AI算法可以实现高度的准确性和效率，并且可以在不同的种族、疾病、阶段和生产商的CT扫描 immagini中实现一致性。这些算法还可以将腹部器官的生物特征自动提取出来，这是传统manual measurement的劳动密集的领域。<details>
<summary>Abstract</summary>
Quantitative organ assessment is an essential step in automated abdominal disease diagnosis and treatment planning. Artificial intelligence (AI) has shown great potential to automatize this process. However, most existing AI algorithms rely on many expert annotations and lack a comprehensive evaluation of accuracy and efficiency in real-world multinational settings. To overcome these limitations, we organized the FLARE 2022 Challenge, the largest abdominal organ analysis challenge to date, to benchmark fast, low-resource, accurate, annotation-efficient, and generalized AI algorithms. We constructed an intercontinental and multinational dataset from more than 50 medical groups, including Computed Tomography (CT) scans with different races, diseases, phases, and manufacturers. We independently validated that a set of AI algorithms achieved a median Dice Similarity Coefficient (DSC) of 90.0\% by using 50 labeled scans and 2000 unlabeled scans, which can significantly reduce annotation requirements. The best-performing algorithms successfully generalized to holdout external validation sets, achieving a median DSC of 89.5\%, 90.9\%, and 88.3\% on North American, European, and Asian cohorts, respectively. They also enabled automatic extraction of key organ biology features, which was labor-intensive with traditional manual measurements. This opens the potential to use unlabeled data to boost performance and alleviate annotation shortages for modern AI models.
</details>
<details>
<summary>摘要</summary>
《量化器官评估是自动肝脏疾病诊断和治疗规划的关键步骤。人工智能（AI）已经在这一过程中表现出了很大的潜力。然而，现有的大多数AI算法仍然依赖于许多专家注释，而且缺乏实际世界多国场景中的全面评估和精度。为了解决这些局限性，我们组织了2022年FLARE挑战，这是肝脏分析领域最大的挑战至今。我们构建了跨国和多国的数据集，包括不同的种族、疾病、阶段和制造商的 Computed Tomography（CT）扫描图像。我们独立验证了一组AI算法可以在50个医疗机构的50个标注图像和2000个无标注图像的基础上达到 médiane 的 dice相似度（DSC）90.0%。这些算法可以减少注释要求，并在占据外验集中保持高精度。最佳算法还能够自动提取关键器官生物特征，这是传统的手动测量很劳累。这开启了使用无标注数据来提高性能的可能性，并解决了现代AI模型的注释缺乏问题。》
</details></li>
</ul>
<hr>
<h2 id="Are-We-Closing-the-Loop-Yet-Gaps-in-the-Generalizability-of-VIS4ML-Research"><a href="#Are-We-Closing-the-Loop-Yet-Gaps-in-the-Generalizability-of-VIS4ML-Research" class="headerlink" title="Are We Closing the Loop Yet? Gaps in the Generalizability of VIS4ML Research"></a>Are We Closing the Loop Yet? Gaps in the Generalizability of VIS4ML Research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06290">http://arxiv.org/abs/2308.06290</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hariharan Subramonyam, Jessica Hullman</li>
<li>for: 这个论文的目的是探讨机器学习（ML）领域的人工智能（AI）研究，以帮助专家更好地发展、理解和改进机器学习模型。</li>
<li>methods: 研究人员使用了交互式可视化技术来让专家更好地理解机器学习组件，并使用了人类知识来支持人类在loop任务。</li>
<li>results: 研究人员发现，目前的VIS4ML研究范围和实践中的应用Scope有一定的差距，许多论文的结论通常是基于非代表性的场景、少量的ML专家和已知的数据集进行验证，而且存在一些关键依赖关系和不充分评估的问题。<details>
<summary>Abstract</summary>
Visualization for machine learning (VIS4ML) research aims to help experts apply their prior knowledge to develop, understand, and improve the performance of machine learning models. In conceiving VIS4ML systems, researchers characterize the nature of human knowledge to support human-in-the-loop tasks, design interactive visualizations to make ML components interpretable and elicit knowledge, and evaluate the effectiveness of human-model interchange. We survey recent VIS4ML papers to assess the generalizability of research contributions and claims in enabling human-in-the-loop ML. Our results show potential gaps between the current scope of VIS4ML research and aspirations for its use in practice. We find that while papers motivate that VIS4ML systems are applicable beyond the specific conditions studied, conclusions are often overfitted to non-representative scenarios, are based on interactions with a small set of ML experts and well-understood datasets, fail to acknowledge crucial dependencies, and hinge on decisions that lack justification. We discuss approaches to close the gap between aspirations and research claims and suggest documentation practices to report generality constraints that better acknowledge the exploratory nature of VIS4ML research.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Knowledge-Propagation-over-Conditional-Independence-Graphs"><a href="#Knowledge-Propagation-over-Conditional-Independence-Graphs" class="headerlink" title="Knowledge Propagation over Conditional Independence Graphs"></a>Knowledge Propagation over Conditional Independence Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05857">http://arxiv.org/abs/2308.05857</a></li>
<li>repo_url: None</li>
<li>paper_authors: Urszula Chajewska, Harsh Shrivastava</li>
<li>for: 这个论文主要是为了提出知识传播算法来处理 conditional independence 图（CI graph）。</li>
<li>methods: 该论文使用了一种基于 undirected graph 的方法，用于模型特征之间的相互关系。</li>
<li>results: 实验结果表明，该算法在公开available的 Cora 和 PubMed 数据集上比现有技术更高效。<details>
<summary>Abstract</summary>
Conditional Independence (CI) graph is a special type of a Probabilistic Graphical Model (PGM) where the feature connections are modeled using an undirected graph and the edge weights show the partial correlation strength between the features. Since the CI graphs capture direct dependence between features, they have been garnering increasing interest within the research community for gaining insights into the systems from various domains, in particular discovering the domain topology. In this work, we propose algorithms for performing knowledge propagation over the CI graphs. Our experiments demonstrate that our techniques improve upon the state-of-the-art on the publicly available Cora and PubMed datasets.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译为简化中文。<</SYS>> condition independence（CI）图是一种特殊的概率图模型（PGM），其特征连接使用无向图表示，边重量表示特征之间的半相关强度。由于 CI 图表示直接相关性 между特征，因此在不同领域研究中吸引了越来越多的关注，尤其是发现领域结构。在这种工作中，我们提出了在 CI 图上进行知识传播的算法。我们的实验结果表明，我们的技术在公共可用的 Cora 和 PubMed 数据集上超过了现状的水平。
</details></li>
</ul>
<hr>
<h2 id="Seed-Kernel-Counting-using-Domain-Randomization-and-Object-Tracking-Neural-Networks"><a href="#Seed-Kernel-Counting-using-Domain-Randomization-and-Object-Tracking-Neural-Networks" class="headerlink" title="Seed Kernel Counting using Domain Randomization and Object Tracking Neural Networks"></a>Seed Kernel Counting using Domain Randomization and Object Tracking Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05846">http://arxiv.org/abs/2308.05846</a></li>
<li>repo_url: None</li>
<li>paper_authors: Venkat Margapuri, Prapti Thapaliya, Mitchell Neilsen</li>
<li>For: The paper is written for the seed production industry, specifically for small-scale seed production firms who cannot afford high-priced mechanized seed kernel counters.* Methods: The paper proposes the use of object tracking neural network models, such as YOLO, to estimate cereal yield inexpensively. The authors also use synthetic imagery as a feasible substitute to train neural networks for object tracking.* Results: The paper demonstrates the use of a low-cost mechanical hopper, trained YOLOv8 neural network model, and object tracking algorithms on StrongSORT and ByteTrack to estimate cereal yield from videos. The results show an accuracy of 95.2% and 93.2% for Soy and Wheat respectively using the StrongSORT algorithm, and an accuracy of 96.8% and 92.4% for Soy and Wheat respectively using the ByteTrack algorithm.Here are the three points in Simplified Chinese text:* For: 这篇论文是为小规模种子生产公司写的，以便他们不能负担高价的机器化种子果实计数器。* Methods: 论文提出使用对象跟踪神经网络模型，如YOLO，来低成本地测量小麦和大豢的果实量。作者还使用Synthetic imagery作为可行的代替方法来训练神经网络。* Results: 论文实践了一种低成本的机械搅拌器、训练过的YOLOv8神经网络模型和对象跟踪算法在StrongSORT和ByteTrack上测量小麦和大豢的果实量。结果显示，使用StrongSORT算法可达95.2%和93.2%的准确率，使用ByteTrack算法可达96.8%和92.4%的准确率。<details>
<summary>Abstract</summary>
High-throughput phenotyping (HTP) of seeds, also known as seed phenotyping, is the comprehensive assessment of complex seed traits such as growth, development, tolerance, resistance, ecology, yield, and the measurement of parameters that form more complex traits. One of the key aspects of seed phenotyping is cereal yield estimation that the seed production industry relies upon to conduct their business. While mechanized seed kernel counters are available in the market currently, they are often priced high and sometimes outside the range of small scale seed production firms' affordability. The development of object tracking neural network models such as You Only Look Once (YOLO) enables computer scientists to design algorithms that can estimate cereal yield inexpensively. The key bottleneck with neural network models is that they require a plethora of labelled training data before they can be put to task. We demonstrate that the use of synthetic imagery serves as a feasible substitute to train neural networks for object tracking that includes the tasks of object classification and detection. Furthermore, we propose a seed kernel counter that uses a low-cost mechanical hopper, trained YOLOv8 neural network model, and object tracking algorithms on StrongSORT and ByteTrack to estimate cereal yield from videos. The experiment yields a seed kernel count with an accuracy of 95.2\% and 93.2\% for Soy and Wheat respectively using the StrongSORT algorithm, and an accuray of 96.8\% and 92.4\% for Soy and Wheat respectively using the ByteTrack algorithm.
</details>
<details>
<summary>摘要</summary>
高通量现象评估（HTP）的种子也称为种子现象评估，是全面评估复杂种子特征，如生长、发展、耐受、抗性、生态学、产量等参数的评估。种子生产行业很需要产量预测，以便进行业务。目前市场上有机器化种子坚果数计算机，但它们往往很昂贵，小规模种子生产公司可能无法负担。基于对象跟踪神经网络模型，如一下只看一次（YOLO），计算科学家可以设计便宜的算法来预测小麦和豫 corn 的产量。然而，神经网络模型的主要瓶颈是需要大量标注训练数据。我们表明，使用合成图像作为可行的替代方案，可以用于训练对象跟踪神经网络模型，包括对象分类和检测任务。此外，我们提议一种使用低成本机械吸盘、训练过 YOLOv8 神经网络模型和对象跟踪算法的种子坚果计数器，用于从视频中预测小麦和豫 corn 的产量。实验结果表明，使用 StrongSORT 算法和 ByteTrack 算法，可以准确地预测小麦和豫 corn 的产量，准确率分别为 95.2% 和 93.2%，以及 96.8% 和 92.4%。
</details></li>
</ul>
<hr>
<h2 id="DiLogics-Creating-Web-Automation-Programs-With-Diverse-Logics"><a href="#DiLogics-Creating-Web-Automation-Programs-With-Diverse-Logics" class="headerlink" title="DiLogics: Creating Web Automation Programs With Diverse Logics"></a>DiLogics: Creating Web Automation Programs With Diverse Logics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05828">http://arxiv.org/abs/2308.05828</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kevin Pu, Jim Yang, Angel Yuan, Minyi Ma, Rui Dong, Xinyu Wang, Yan Chen, Tovi Grossman</li>
<li>For: The paper is written for knowledge workers who frequently encounter repetitive web data entry tasks and want to increase their productivity through web automation.* Methods: The paper presents a programming-by-demonstration system called DiLogics, which uses natural language processing (NLP) to assist users in creating web automation programs that can handle diverse specifications.* Results: The paper shows that non-experts can effectively use DiLogics to create automation programs that fulfill diverse input instructions, and that DiLogics provides an efficient, intuitive, and expressive method for developing web automation programs satisfying diverse specifications.Here’s the same information in Simplified Chinese:* For: 论文是为知识工作者所写，他们经常遇到重复的网络数据录入任务，想通过网络自动化提高工作效率。* Methods: 论文提出了一种基于示例示出的编程系统——DiLogics，通过自然语言处理（NLP）助手，帮助用户创建满足多样化要求的网络自动化程序。* Results: 论文表明，非专业人员可以有效使用 DiLogics 创建满足多样化输入指令的自动化程序，而 DiLogics 提供了高效、直观、表达力强的网络自动化程序开发方法。<details>
<summary>Abstract</summary>
Knowledge workers frequently encounter repetitive web data entry tasks, like updating records or placing orders. Web automation increases productivity, but translating tasks to web actions accurately and extending to new specifications is challenging. Existing tools can automate tasks that perform the same logical trace of UI actions (e.g., input text in each field in order), but do not support tasks requiring different executions based on varied input conditions. We present DiLogics, a programming-by-demonstration system that utilizes NLP to assist users in creating web automation programs that handle diverse specifications. DiLogics first semantically segments input data to structured task steps. By recording user demonstrations for each step, DiLogics generalizes the web macros to novel but semantically similar task requirements. Our evaluation showed that non-experts can effectively use DiLogics to create automation programs that fulfill diverse input instructions. DiLogics provides an efficient, intuitive, and expressive method for developing web automation programs satisfying diverse specifications.
</details>
<details>
<summary>摘要</summary>
知识工作者经常遇到重复的网络数据入力任务，如更新记录或发送订单。网络自动化可以提高生产力，但将任务翻译为网络动作accurately并扩展到新规范是挑战。现有工具可以自动执行同样的逻辑Trace of UI actions（例如，输入文本在每个字段中输入），但不支持基于不同输入条件的任务。我们提出了DiLogics，一个基于Programming-by-Demonstration的系统，使用自然语言处理（NLP）帮助用户创建满足多样化要求的网络自动化程序。DiLogics首先将输入数据semanticallySegmented into结构化任务步骤。通过记录用户示例 для每个步骤，DiLogics将网络 macro扩展到新的，但semantically similar的任务要求。我们的评估显示，非专家可以有效地使用DiLogics创建自动化程序，满足多样化的输入指令。DiLogics提供了高效、直观、表达力强的方法 для开发满足多样化要求的网络自动化程序。
</details></li>
</ul>
<hr>
<h2 id="Encode-Store-Retrieve-Enhancing-Memory-Augmentation-through-Language-Encoded-Egocentric-Perception"><a href="#Encode-Store-Retrieve-Enhancing-Memory-Augmentation-through-Language-Encoded-Egocentric-Perception" class="headerlink" title="Encode-Store-Retrieve: Enhancing Memory Augmentation through Language-Encoded Egocentric Perception"></a>Encode-Store-Retrieve: Enhancing Memory Augmentation through Language-Encoded Egocentric Perception</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05822">http://arxiv.org/abs/2308.05822</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junxiao Shen, John Dudley, Per Ola Kristensson</li>
<li>for: 增强人类记忆能力，尤其是对于长期内存和生活记忆。</li>
<li>methods: 利用增强现实头戴式显示器捕捉和保存生活视频，并使用自然语言编码将其存储在矢量数据库中。</li>
<li>results: 比较出色的结果，BLEU分数达8.3，超过了传统机器学习模型的3.4-5.8分数。在用户研究中，人工系统的响应得分为4.13&#x2F;5，而人类参与者的得分为2.46&#x2F;5。<details>
<summary>Abstract</summary>
We depend on our own memory to encode, store, and retrieve our experiences. However, memory lapses can occur. One promising avenue for achieving memory augmentation is through the use of augmented reality head-mounted displays to capture and preserve egocentric videos, a practice commonly referred to as life logging. However, a significant challenge arises from the sheer volume of video data generated through life logging, as the current technology lacks the capability to encode and store such large amounts of data efficiently. Further, retrieving specific information from extensive video archives requires substantial computational power, further complicating the task of quickly accessing desired content. To address these challenges, we propose a memory augmentation system that involves leveraging natural language encoding for video data and storing them in a vector database. This approach harnesses the power of large vision language models to perform the language encoding process. Additionally, we propose using large language models to facilitate natural language querying. Our system underwent extensive evaluation using the QA-Ego4D dataset and achieved state-of-the-art results with a BLEU score of 8.3, outperforming conventional machine learning models that scored between 3.4 and 5.8. Additionally, in a user study, our system received a higher mean response score of 4.13/5 compared to the human participants' score of 2.46/5 on real-life episodic memory tasks.
</details>
<details>
<summary>摘要</summary>
我们依赖我们自己的记忆来编码、存储和检索我们的经验。然而，记忆漏洞可能会出现。一种有前途的方法是通过使用扩展现实头戴式显示器捕捉和保存 Egocentric 视频，这种做法通常被称为生活日志。然而，大量视频数据的生成带来了现有技术的存储和编码问题，特别是在检索广泛的视频存档中寻找特定内容的计算机力量很大，使得快速访问感兴趣的内容变得复杂。为解决这些挑战，我们提议一种增强记忆系统，该系统利用自然语言编码器将视频数据存储在 вектор数据库中。这种方法利用大量视力语言模型来实现语言编码过程。此外，我们还提议使用大量语言模型来促进自然语言查询。我们的系统在使用 QA-Ego4D 数据集进行了广泛的评估，并取得了当前最佳成绩，BLEU 分数为 8.3，超越了传统机器学习模型的分数范围 между 3.4 和 5.8。此外，在用户研究中，我们的系统得到了用户平均回答分数为 4.13/5，而人类参与者的平均回答分数为 2.46/5 在真实生活记忆任务中。
</details></li>
</ul>
<hr>
<h2 id="Neural-Progressive-Meshes"><a href="#Neural-Progressive-Meshes" class="headerlink" title="Neural Progressive Meshes"></a>Neural Progressive Meshes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05741">http://arxiv.org/abs/2308.05741</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yun-Chun Chen, Vladimir G. Kim, Noam Aigerman, Alec Jacobson</li>
<li>for: efficiently transmitting large geometric data (e.g., 3D meshes) over the Internet</li>
<li>methods: subdivision-based encoder-decoder architecture trained on a large collection of surfaces, with progressive transmission of residual features</li>
<li>results: outperforms baselines in terms of compression ratio and reconstruction quality<details>
<summary>Abstract</summary>
The recent proliferation of 3D content that can be consumed on hand-held devices necessitates efficient tools for transmitting large geometric data, e.g., 3D meshes, over the Internet. Detailed high-resolution assets can pose a challenge to storage as well as transmission bandwidth, and level-of-detail techniques are often used to transmit an asset using an appropriate bandwidth budget. It is especially desirable for these methods to transmit data progressively, improving the quality of the geometry with more data. Our key insight is that the geometric details of 3D meshes often exhibit similar local patterns even across different shapes, and thus can be effectively represented with a shared learned generative space. We learn this space using a subdivision-based encoder-decoder architecture trained in advance on a large collection of surfaces. We further observe that additional residual features can be transmitted progressively between intermediate levels of subdivision that enable the client to control the tradeoff between bandwidth cost and quality of reconstruction, providing a neural progressive mesh representation. We evaluate our method on a diverse set of complex 3D shapes and demonstrate that it outperforms baselines in terms of compression ratio and reconstruction quality.
</details>
<details>
<summary>摘要</summary>
现在有许多手持式设备可以播放3D内容，这些内容的大量几何数据（如3D网格）的传输效率变得非常重要。高分辨率资产可能会占用很多存储空间和传输带宽，而level-of-detail技术可以将资产分解为不同的级别，以适应不同的带宽预算。我们的关键发现是，3D网格的几何细节经常会在不同的形状之间展现相似的地方性特征，因此可以使用共享学习生成空间来有效表示它们。我们使用了分割基于的编码器-解码器架构来学习这个空间，并在大量的表面上进行预先训练。此外，我们还发现可以在间接级别之间进行进程式传输额外的剩余特征，使客户端可以控制带宽成本和重建质量之间的交易，提供神经进程式网格表示。我们对一个多样化的3D形状集合进行了评估，并证明了我们的方法在压缩率和重建质量两个方面都超过了基准值。
</details></li>
</ul>
<hr>
<h2 id="AudioLDM-2-Learning-Holistic-Audio-Generation-with-Self-supervised-Pretraining"><a href="#AudioLDM-2-Learning-Holistic-Audio-Generation-with-Self-supervised-Pretraining" class="headerlink" title="AudioLDM 2: Learning Holistic Audio Generation with Self-supervised Pretraining"></a>AudioLDM 2: Learning Holistic Audio Generation with Self-supervised Pretraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05734">http://arxiv.org/abs/2308.05734</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/haoheliu/AudioLDM2">https://github.com/haoheliu/AudioLDM2</a></li>
<li>paper_authors: Haohe Liu, Qiao Tian, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Yuping Wang, Wenwu Wang, Yuxuan Wang, Mark D. Plumbley</li>
<li>for: 这篇论文是针对不同类型的声音生成模型的设计，包括speech、音乐和 зву效，并提出一个框架，让这些模型共享同一个学习方法。</li>
<li>methods: 这个框架使用了一个称为“语言音频”（LOA）的通用表现，将任何声音转换为LOA，然后使用GPT-2模型处理自我监督学习。在生成过程中，我们使用了一个潜在扩散模型，将任何modalities转换为LOA。</li>
<li>results: 实验结果显示，这个框架可以实现新的州际表现或与前一代方法竞争的性能，并且具有内在学习能力和可重用的自我监督学习模型。代码和示例可以在<a target="_blank" rel="noopener" href="https://audioldm.github.io/audioldm2%E8%8E%B7%E5%8F%96%E3%80%82">https://audioldm.github.io/audioldm2获取。</a><details>
<summary>Abstract</summary>
Although audio generation shares commonalities across different types of audio, such as speech, music, and sound effects, designing models for each type requires careful consideration of specific objectives and biases that can significantly differ from those of other types. To bring us closer to a unified perspective of audio generation, this paper proposes a framework that utilizes the same learning method for speech, music, and sound effect generation. Our framework introduces a general representation of audio, called language of audio (LOA). Any audio can be translated into LOA based on AudioMAE, a self-supervised pre-trained representation learning model. In the generation process, we translate any modalities into LOA by using a GPT-2 model, and we perform self-supervised audio generation learning with a latent diffusion model conditioned on LOA. The proposed framework naturally brings advantages such as in-context learning abilities and reusable self-supervised pretrained AudioMAE and latent diffusion models. Experiments on the major benchmarks of text-to-audio, text-to-music, and text-to-speech demonstrate new state-of-the-art or competitive performance to previous approaches. Our demo and code are available at https://audioldm.github.io/audioldm2.
</details>
<details>
<summary>摘要</summary>
尽管各种音频之间存在共同之处，如语音、音乐和音效，但设计模型时需要仔细考虑每种类型的特定目标和偏见，这些偏见可能与其他类型的偏见存在很大差异。为了带我们更近于一个统一的音频生成视角，这篇论文提出了一个框架，该框架利用同一种学习方法来生成语音、音乐和音效。我们的框架称之为语音语言（LOA）框架。任何音频都可以根据AudioMAE自supervised预训练表示学习模型转化为LOA表示。在生成过程中，我们使用GPT-2模型将任何模态转化为LOA，并使用conditioned on LOA的隐式扩散模型进行自主学习。我们的提议的框架自然带来了在上下文学习能力和可 reuse自supervised AudioMAE和隐藏扩散模型的优点。我们的实验在文本到音频、文本到音乐和文本到语音的主要标准测试集上达到了新的状态ucker或竞争性的表现。您可以在https://audioldm.github.io/audioldm2上获取我们的demo和代码。
</details></li>
</ul>
<hr>
<h2 id="PDE-Refiner-Achieving-Accurate-Long-Rollouts-with-Neural-PDE-Solvers"><a href="#PDE-Refiner-Achieving-Accurate-Long-Rollouts-with-Neural-PDE-Solvers" class="headerlink" title="PDE-Refiner: Achieving Accurate Long Rollouts with Neural PDE Solvers"></a>PDE-Refiner: Achieving Accurate Long Rollouts with Neural PDE Solvers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05732">http://arxiv.org/abs/2308.05732</a></li>
<li>repo_url: None</li>
<li>paper_authors: Phillip Lippe, Bastiaan S. Veeling, Paris Perdikaris, Richard E. Turner, Johannes Brandstetter</li>
<li>for: 这个论文的目的是提出一种基于深度神经网络的 partial differential equation（PDE）解决方案，以提高解决PDE问题的计算效率和准确性。</li>
<li>methods: 这个论文使用了一种基于扩展的 diffusion 模型，通过多步增强过程来更好地模型 PDE 解的所有频率成分。</li>
<li>results: 论文通过对复杂的液体动力学 benchmark 进行验证，表明 PDE-Refiner 可以在稳定和准确的情况下进行 Rollout 操作，并且可以超过现有的神经网络、数值和神经数值模型。  additionally, PDE-Refiner 可以大幅提高数据效率，并且可以准确地评估模型的预测不确定性。<details>
<summary>Abstract</summary>
Time-dependent partial differential equations (PDEs) are ubiquitous in science and engineering. Recently, mostly due to the high computational cost of traditional solution techniques, deep neural network based surrogates have gained increased interest. The practical utility of such neural PDE solvers relies on their ability to provide accurate, stable predictions over long time horizons, which is a notoriously hard problem. In this work, we present a large-scale analysis of common temporal rollout strategies, identifying the neglect of non-dominant spatial frequency information, often associated with high frequencies in PDE solutions, as the primary pitfall limiting stable, accurate rollout performance. Based on these insights, we draw inspiration from recent advances in diffusion models to introduce PDE-Refiner; a novel model class that enables more accurate modeling of all frequency components via a multistep refinement process. We validate PDE-Refiner on challenging benchmarks of complex fluid dynamics, demonstrating stable and accurate rollouts that consistently outperform state-of-the-art models, including neural, numerical, and hybrid neural-numerical architectures. We further demonstrate that PDE-Refiner greatly enhances data efficiency, since the denoising objective implicitly induces a novel form of spectral data augmentation. Finally, PDE-Refiner's connection to diffusion models enables an accurate and efficient assessment of the model's predictive uncertainty, allowing us to estimate when the surrogate becomes inaccurate.
</details>
<details>
<summary>摘要</summary>
时间依赖的partial differential equations (PDEs) 在科学和工程中很普遍。近年来，主要因为传统解决方案的计算成本高涨，深度神经网络基于的surrogate 获得了更多的关注。但是，实际应用中，这些神经PDE解决器的实用性受到长时间预测稳定和准确的限制。在这个工作中，我们对常见的时间推送策略进行大规模分析，发现了忽略非主导空间频率信息的问题，这常常与PDE解决中高频信号相关。基于这些发现，我们从Diffusion模型中灵感获得了PDE-Refiner;一种新的模型类，可以更好地模型所有频率成分，通过多步精度提升过程。我们验证了PDE-Refiner在复杂的液体动力学benchmark上，表明其可以在稳定和准确的情况下进行长时间推送。此外，PDE-Refiner可以大幅提高数据效率，因为净化目标意味着一种新的spectral data augmentation。最后，PDE-Refiner的连接到Diffusion模型使得可以准确和有效地评估模型的预测不确定性，从而估计模型在不准确的情况下。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Integration-of-Prediction-and-Planning-in-Deep-Learning-Based-Automated-Driving-Systems-A-Review"><a href="#Rethinking-Integration-of-Prediction-and-Planning-in-Deep-Learning-Based-Automated-Driving-Systems-A-Review" class="headerlink" title="Rethinking Integration of Prediction and Planning in Deep Learning-Based Automated Driving Systems: A Review"></a>Rethinking Integration of Prediction and Planning in Deep Learning-Based Automated Driving Systems: A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05731">http://arxiv.org/abs/2308.05731</a></li>
<li>repo_url: None</li>
<li>paper_authors: Steffen Hagedorn, Marcel Hallgarten, Martin Stoll, Alexandru Condurache</li>
<li>for: 提高自动驾驶系统的安全性、效率和舒适性。</li>
<li>methods: 使用深度学习模型进行预测和规划，并将两者 integrate 为一个互相关联的模型。</li>
<li>results: 通过对现有的模型进行系统性的审视和分析，提供了关于不同集成方法的研究 gap 和未来挑战，以及指出了未来研究的潜在方向。<details>
<summary>Abstract</summary>
Automated driving has the potential to revolutionize personal, public, and freight mobility. Besides the enormous challenge of perception, i.e. accurately perceiving the environment using available sensor data, automated driving comprises planning a safe, comfortable, and efficient motion trajectory. To promote safety and progress, many works rely on modules that predict the future motion of surrounding traffic. Modular automated driving systems commonly handle prediction and planning as sequential separate tasks. While this accounts for the influence of surrounding traffic on the ego-vehicle, it fails to anticipate the reactions of traffic participants to the ego-vehicle's behavior. Recent works suggest that integrating prediction and planning in an interdependent joint step is necessary to achieve safe, efficient, and comfortable driving. While various models implement such integrated systems, a comprehensive overview and theoretical understanding of different principles are lacking. We systematically review state-of-the-art deep learning-based prediction, planning, and integrated prediction and planning models. Different facets of the integration ranging from model architecture and model design to behavioral aspects are considered and related to each other. Moreover, we discuss the implications, strengths, and limitations of different integration methods. By pointing out research gaps, describing relevant future challenges, and highlighting trends in the research field, we identify promising directions for future research.
</details>
<details>
<summary>摘要</summary>
自动驾驶技术有可能改变人类、公共和货物运输的方式。除了巨大的感知挑战以外，自动驾驶还包括规划一个安全、舒适和高效的动力轨迹。为了促进安全和进步，许多研究都是通过模块来预测周围交通的未来运动来实现。这些模块通常处理预测和规划作为独立的两个任务。although this approach takes into account the influence of surrounding traffic on the ego-vehicle, it fails to anticipate the reactions of traffic participants to the ego-vehicle's behavior。Recent works suggest that integrating prediction and planning in an interdependent joint step is necessary to achieve safe, efficient, and comfortable driving。various models have implemented such integrated systems, but a comprehensive overview and theoretical understanding of different principles are lacking。we systematically review state-of-the-art deep learning-based prediction, planning, and integrated prediction and planning models。different aspects of the integration, such as model architecture, model design, and behavioral aspects, are considered and related to each other。furthermore, we discuss the implications, strengths, and limitations of different integration methods。by pointing out research gaps, describing relevant future challenges, and highlighting trends in the research field, we identify promising directions for future research。
</details></li>
</ul>
<hr>
<h2 id="Testing-GPT-4-with-Wolfram-Alpha-and-Code-Interpreter-plug-ins-on-math-and-science-problems"><a href="#Testing-GPT-4-with-Wolfram-Alpha-and-Code-Interpreter-plug-ins-on-math-and-science-problems" class="headerlink" title="Testing GPT-4 with Wolfram Alpha and Code Interpreter plug-ins on math and science problems"></a>Testing GPT-4 with Wolfram Alpha and Code Interpreter plug-ins on math and science problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05713">http://arxiv.org/abs/2308.05713</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ernest Davis, Scott Aaronson</li>
<li>for: 测试GPT-4语言模型在科学和数学领域的105个原始问题中的能力，包括高中和大学水平。</li>
<li>methods: 使用Wolfram Alpha和Code Interpreter插件。</li>
<li>results: 测试结果表明，插件可以显著提高GPT解决这些问题的能力，但仍有“接口”失败，即GPT有问题表述问题以获得有用的答案。<details>
<summary>Abstract</summary>
This report describes a test of the large language model GPT-4 with the Wolfram Alpha and the Code Interpreter plug-ins on 105 original problems in science and math, at the high school and college levels, carried out in June-August 2023. Our tests suggest that the plug-ins significantly enhance GPT's ability to solve these problems. Having said that, there are still often "interface" failures; that is, GPT often has trouble formulating problems in a way that elicits useful answers from the plug-ins. Fixing these interface failures seems like a central challenge in making GPT a reliable tool for college-level calculation problems.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:这份报告描述了在6月至8月2023年，使用GPT-4语言模型和Wolfram Alpha和Code Interpreter插件，对105个科学和数学问题进行了测试，这些问题分别来自高中和大学水平。我们的测试表明，插件可以大大提高GPT的解决这些问题的能力。然而，还有很多“接口”失败，即GPT在提问时未能得到有用的答案。解决这些接口失败是让GPT成为大学水平计算问题的可靠工具的中心挑战。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Potential-of-World-Models-for-Anomaly-Detection-in-Autonomous-Driving"><a href="#Exploring-the-Potential-of-World-Models-for-Anomaly-Detection-in-Autonomous-Driving" class="headerlink" title="Exploring the Potential of World Models for Anomaly Detection in Autonomous Driving"></a>Exploring the Potential of World Models for Anomaly Detection in Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05701">http://arxiv.org/abs/2308.05701</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Bogdoll, Lukas Bosch, Tim Joseph, Helen Gremmelmaier, Yitian Yang, J. Marius Zöllner</li>
<li>for: 本研究旨在探讨世界模型如何应用于自动驾驶系统中的异常检测。</li>
<li>methods: 本研究使用世界模型来检测自动驾驶系统中的异常。</li>
<li>results: 本研究提供了世界模型在自动驾驶系统中异常检测的概述，并将各个组件与前期异常检测研究相关联，以便进一步探讨这一领域。<details>
<summary>Abstract</summary>
In recent years there have been remarkable advancements in autonomous driving. While autonomous vehicles demonstrate high performance in closed-set conditions, they encounter difficulties when confronted with unexpected situations. At the same time, world models emerged in the field of model-based reinforcement learning as a way to enable agents to predict the future depending on potential actions. This led to outstanding results in sparse reward and complex control tasks. This work provides an overview of how world models can be leveraged to perform anomaly detection in the domain of autonomous driving. We provide a characterization of world models and relate individual components to previous works in anomaly detection to facilitate further research in the field.
</details>
<details>
<summary>摘要</summary>
In this work, we explore how world models can be used for anomaly detection in the field of autonomous driving. We provide a comprehensive overview of world models and how they can be applied to detect anomalies in this domain. Additionally, we relate individual components of world models to previous works in anomaly detection, providing a foundation for further research in this area.
</details></li>
</ul>
<hr>
<h2 id="SSLRec-A-Self-Supervised-Learning-Library-for-Recommendation"><a href="#SSLRec-A-Self-Supervised-Learning-Library-for-Recommendation" class="headerlink" title="SSLRec: A Self-Supervised Learning Library for Recommendation"></a>SSLRec: A Self-Supervised Learning Library for Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05697">http://arxiv.org/abs/2308.05697</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hkuds/sslrec">https://github.com/hkuds/sslrec</a></li>
<li>paper_authors: Xubin Ren, Lianghao Xia, Yuhao Yang, Wei Wei, Tianle Wang, Xuheng Cai, Chao Huang</li>
<li>for: This paper is written to address the lack of unified frameworks for evaluating self-supervised learning (SSL) recommendation algorithms across different domains.</li>
<li>methods: The paper introduces SSLRec, a novel benchmark platform that provides a standardized, flexible, and comprehensive framework for evaluating various SSL-enhanced recommenders. The platform features a modular architecture and a complete set of data augmentation and self-supervised toolkits.</li>
<li>results: The paper provides a comprehensive set of state-of-the-art SSL-enhanced recommendation models across different scenarios, enabling researchers to evaluate these cutting-edge models and drive further innovation in the field. The paper also simplifies the process of training and evaluating different recommendation models with consistent and fair settings.<details>
<summary>Abstract</summary>
Self-supervised learning (SSL) has gained significant interest in recent years as a solution to address the challenges posed by sparse and noisy data in recommender systems. Despite the growing number of SSL algorithms designed to provide state-of-the-art performance in various recommendation scenarios (e.g., graph collaborative filtering, sequential recommendation, social recommendation, KG-enhanced recommendation), there is still a lack of unified frameworks that integrate recommendation algorithms across different domains. Such a framework could serve as the cornerstone for self-supervised recommendation algorithms, unifying the validation of existing methods and driving the design of new ones. To address this gap, we introduce SSLRec, a novel benchmark platform that provides a standardized, flexible, and comprehensive framework for evaluating various SSL-enhanced recommenders. The SSLRec library features a modular architecture that allows users to easily evaluate state-of-the-art models and a complete set of data augmentation and self-supervised toolkits to help create SSL recommendation models with specific needs. Furthermore, SSLRec simplifies the process of training and evaluating different recommendation models with consistent and fair settings. Our SSLRec platform covers a comprehensive set of state-of-the-art SSL-enhanced recommendation models across different scenarios, enabling researchers to evaluate these cutting-edge models and drive further innovation in the field. Our implemented SSLRec framework is available at the source code repository https://github.com/HKUDS/SSLRec.
</details>
<details>
<summary>摘要</summary>
自我监督学习（SSL）在过去几年内受到了广泛关注，以解决推荐系统中稀缺和噪音数据的挑战。虽然有一大量的SSL算法，用于在不同领域提供状态对抗性的表现（如图像协同推荐、序列推荐、社交推荐、知识 graphs 增强推荐），但是还没有一个统一的框架，可以将推荐算法集成到不同领域。这样的框架可以作为推荐算法的基础，统一验证现有方法，并驱动新的方法的设计。为解决这个差距，我们介绍了SSLRec，一个新的测试平台，它提供了标准化、灵活、全面的评估推荐算法的框架。SSLRec 库具有可扩展的架构，allowing users to easily evaluate state-of-the-art models，并且提供了完整的数据增强和自我监督工具kit，帮助用户创建特定需求的SSL推荐模型。此外，SSLRec 简化了不同推荐模型的训练和评估过程，使得模型的评估具有共同和公正的设置。我们的SSLRec 平台覆盖了不同enario 中的 cutting-edge SSL-enhanced recommendation models， allowing researchers to evaluate these models and drive further innovation in the field。我们实现的SSLRec 框架可以在 <https://github.com/HKUDS/SSLRec> 上获取。
</details></li>
</ul>
<hr>
<h2 id="Hard-No-Box-Adversarial-Attack-on-Skeleton-Based-Human-Action-Recognition-with-Skeleton-Motion-Informed-Gradient"><a href="#Hard-No-Box-Adversarial-Attack-on-Skeleton-Based-Human-Action-Recognition-with-Skeleton-Motion-Informed-Gradient" class="headerlink" title="Hard No-Box Adversarial Attack on Skeleton-Based Human Action Recognition with Skeleton-Motion-Informed Gradient"></a>Hard No-Box Adversarial Attack on Skeleton-Based Human Action Recognition with Skeleton-Motion-Informed Gradient</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05681">http://arxiv.org/abs/2308.05681</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luyg45/hardnoboxattack">https://github.com/luyg45/hardnoboxattack</a></li>
<li>paper_authors: Zhengzhi Lu, He Wang, Ziyi Chang, Guoan Yang, Hubert P. H. Shum</li>
<li>for: 这 paper 的目的是证明 skeleton-based 人员活动识别方法存在敏感性，并提出一种新的攻击任务，即攻击者没有访问受试者模型或训练数据或标签。</li>
<li>methods: 这 paper 使用了一种新的攻击方法，即基于动作演示的攻击方法，称为 SMI 梯度。这种攻击方法可以在不知情的情况下对skeleton-based 人员活动识别模型进行攻击。</li>
<li>results:  experiments 表明，这种攻击方法可以对skeleton-based 人员活动识别模型造成实际的威胁，并且可以在不知情的情况下进行 Transfer-based 和 black-box 攻击。此外，这种攻击方法还可以提高攻击样本的可识别性和透明度。<details>
<summary>Abstract</summary>
Recently, methods for skeleton-based human activity recognition have been shown to be vulnerable to adversarial attacks. However, these attack methods require either the full knowledge of the victim (i.e. white-box attacks), access to training data (i.e. transfer-based attacks) or frequent model queries (i.e. black-box attacks). All their requirements are highly restrictive, raising the question of how detrimental the vulnerability is. In this paper, we show that the vulnerability indeed exists. To this end, we consider a new attack task: the attacker has no access to the victim model or the training data or labels, where we coin the term hard no-box attack. Specifically, we first learn a motion manifold where we define an adversarial loss to compute a new gradient for the attack, named skeleton-motion-informed (SMI) gradient. Our gradient contains information of the motion dynamics, which is different from existing gradient-based attack methods that compute the loss gradient assuming each dimension in the data is independent. The SMI gradient can augment many gradient-based attack methods, leading to a new family of no-box attack methods. Extensive evaluation and comparison show that our method imposes a real threat to existing classifiers. They also show that the SMI gradient improves the transferability and imperceptibility of adversarial samples in both no-box and transfer-based black-box settings.
</details>
<details>
<summary>摘要</summary>
近期，基于骨架的人体活动识别方法被发现容易受到敌意攻击。然而，这些攻击方法具有限制性的要求，包括受害者（white-box攻击）、训练数据（transfer-based攻击）或模型查询频繁（black-box攻击）。这些要求都是非常困难，这引发了对攻击性能的评估。在这篇论文中，我们表明了这种漏洞的存在。为此，我们倡议了一个新的攻击任务：攻击者无法访问受害者模型或训练数据或标签。我们称之为“硬无框攻击”。我们首先学习了一个动作拟合空间，并定义了一种对抗损失函数来计算一个新的攻击方向，称之为“骨动作信息”（SMI）损失函数。我们的损失函数含有动作动力信息，与现有的梯度基于攻击方法不同。SMI损失函数可以增强许多梯度基于攻击方法，导致一个新的无框攻击家族。我们进行了广泛的评估和比较，并证明了我们的方法对现有分类器具有真正的威胁。此外，我们还证明了SMI损失函数可以提高黑框和转移黑框无框攻击的可读性和隐蔽性。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Deep-Learning-Approaches-to-Predict-Person-and-Vehicle-Trips-An-Analysis-of-NHTS-Data"><a href="#Exploring-Deep-Learning-Approaches-to-Predict-Person-and-Vehicle-Trips-An-Analysis-of-NHTS-Data" class="headerlink" title="Exploring Deep Learning Approaches to Predict Person and Vehicle Trips: An Analysis of NHTS Data"></a>Exploring Deep Learning Approaches to Predict Person and Vehicle Trips: An Analysis of NHTS Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05665">http://arxiv.org/abs/2308.05665</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kojo Adu-Gyamfi, Sharma Anuj<br>for: 这项研究的目的是探讨深度学习技术在交通规划预测中的潜在应用，以提高交通规划的准确性和可靠性。methods: 这项研究使用了全国家庭旅行调查（NHTS）数据集，开发了一个深度学习模型来预测人坐标和车辆坐标。该模型利用了NHTS数据中的庞大信息，捕捉了复杂的非线性关系，从而超越了传统模型的性能。results: 该研究发现，使用深度学习模型可以实现人坐标预测的准确率达98%，车辆坐标预测的准确率达96%，与传统交通规划模型相比有显著提高。这表明深度学习在交通规划预测中具有潜在的应用价值。<details>
<summary>Abstract</summary>
Modern transportation planning relies heavily on accurate predictions of person and vehicle trips. However, traditional planning models often fail to account for the intricacies and dynamics of travel behavior, leading to less-than-optimal accuracy in these predictions. This study explores the potential of deep learning techniques to transform the way we approach trip predictions, and ultimately, transportation planning. Utilizing a comprehensive dataset from the National Household Travel Survey (NHTS), we developed and trained a deep learning model for predicting person and vehicle trips. The proposed model leverages the vast amount of information in the NHTS data, capturing complex, non-linear relationships that were previously overlooked by traditional models. As a result, our deep learning model achieved an impressive accuracy of 98% for person trip prediction and 96% for vehicle trip estimation. This represents a significant improvement over the performances of traditional transportation planning models, thereby demonstrating the power of deep learning in this domain. The implications of this study extend beyond just more accurate predictions. By enhancing the accuracy and reliability of trip prediction models, planners can formulate more effective, data-driven transportation policies, infrastructure, and services. As such, our research underscores the need for the transportation planning field to embrace advanced techniques like deep learning. The detailed methodology, along with a thorough discussion of the results and their implications, are presented in the subsequent sections of this paper.
</details>
<details>
<summary>摘要</summary>
现代交通规划强调准确预测人员和车辆行程。然而，传统的规划模型经常忽略旅行行为的细节和动态特征，导致预测不准确。这项研究探讨使用深度学习技术改变交通规划方法的潜在性。我们使用全国家庭旅行调查（NHTS）数据集，开发和训练了深度学习模型，以预测人员和车辆行程。我们的提案的模型利用NHTS数据中的庞大信息，捕捉复杂的非线性关系，从而超过传统模型的性能。因此，我们的深度学习模型在人员行程预测中达到了98%的准确率，在车辆行程估算中达到了96%的准确率。这表明深度学习在这个领域具有显著的优势。这些结果不仅表明了深度学习模型的更高准确性和可靠性，还有助于交通规划师们制定更有效的数据驱动的交通政策、基础设施和服务。因此，我们的研究证明了交通规划领域应该采用高级技术如深度学习。详细的方法和结果的讨论，以及其影响，在后续章节中提供。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/11/cs.AI_2023_08_11/" data-id="clpxp03u10024fm88dgcf8ums" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_08_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/11/cs.CL_2023_08_11/" class="article-date">
  <time datetime="2023-08-11T11:00:00.000Z" itemprop="datePublished">2023-08-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/11/cs.CL_2023_08_11/">cs.CL - 2023-08-11</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Weakly-Supervised-Text-Classification-on-Free-Text-Comments-in-Patient-Reported-Outcome-Measures"><a href="#Weakly-Supervised-Text-Classification-on-Free-Text-Comments-in-Patient-Reported-Outcome-Measures" class="headerlink" title="Weakly Supervised Text Classification on Free Text Comments in Patient-Reported Outcome Measures"></a>Weakly Supervised Text Classification on Free Text Comments in Patient-Reported Outcome Measures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06199">http://arxiv.org/abs/2308.06199</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anna-Grace Linton, Vania Dimitrova, Amy Downing, Richard Wagland, Adam Glaser</li>
<li>for: 这个论文是为了分析患有Rectal cancer的病人的自报症状数据中的FTC数据，以提高健康质量生活（HRQoL）的评估。</li>
<li>methods: 这个论文使用了五种weakly supervised text classification（WSTC）技术来分类医疗领域特定的FTC数据，以提取报告的健康相关质量生活（HRQoL）主题。</li>
<li>results: 研究发现，使用WSTC技术可以在医疗领域特定的FTC数据中提取健康相关质量生活（HRQoL）主题，但是模型精度和主题之间存在差异。<details>
<summary>Abstract</summary>
Free text comments (FTC) in patient-reported outcome measures (PROMs) data are typically analysed using manual methods, such as content analysis, which is labour-intensive and time-consuming. Machine learning analysis methods are largely unsupervised, necessitating post-analysis interpretation. Weakly supervised text classification (WSTC) can be a valuable method of analysis to classify domain-specific text data in which there is limited labelled data. In this paper, we apply five WSTC techniques to FTC in PROMs data to identify health-related quality of life (HRQoL) themes reported by colorectal cancer patients. The WSTC methods label all the themes mentioned in the FTC. The results showed moderate performance on the PROMs data, mainly due to the precision of the models, and variation between themes. Evaluation of the classification performance illustrated the potential and limitations of keyword based WSTC to label PROMs FTC when labelled data is limited.
</details>
<details>
<summary>摘要</summary>
免费文本评论（FTC）在患者报告的结果数据中通常使用手动方法进行分析，如内容分析，这是费时费力的。机器学习分析方法是无监督的，需要后期分析。弱监督文本分类（WSTC）可以是分析domain特有文本数据的有价值方法，在这篇论文中，我们将WSTC技术应用于患者报告中的FTC，以确定患者报告中的健康相关质量生活（HRQoL）主题。WSTC方法将FTC中所提到的所有主题标注。结果表明，在PROMs数据上，WSTC方法的性能较差，主要是因为模型精度和主题之间的变化。对分类性能的评估表明了关键词基于WSTC的标注PROMs FTC的潜在和局限性。
</details></li>
</ul>
<hr>
<h2 id="Assessing-Guest-Nationality-Composition-from-Hotel-Reviews"><a href="#Assessing-Guest-Nationality-Composition-from-Hotel-Reviews" class="headerlink" title="Assessing Guest Nationality Composition from Hotel Reviews"></a>Assessing Guest Nationality Composition from Hotel Reviews</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06175">http://arxiv.org/abs/2308.06175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fabian Gröger, Marc Pouly, Flavia Tinner, Leif Brandes</li>
<li>for: 这个论文是为了研究如何使用机器学习来监测和评估具体客户来源的酒店业务竞争力。</li>
<li>methods: 该论文使用了预训练的嵌入和堆式LSTM层来提取文本评论中的客户国籍信息，以动态评估和监测具体客户来源的变化。</li>
<li>results: 研究发现，使用简单的架构可以提供更好的性能和时间成本比例，而不是使用更复杂的语言模型。<details>
<summary>Abstract</summary>
Many hotels target guest acquisition efforts to specific markets in order to best anticipate individual preferences and needs of their guests. Likewise, such strategic positioning is a prerequisite for efficient marketing budget allocation. Official statistics report on the number of visitors from different countries, but no fine-grained information on the guest composition of individual businesses exists. There is, however, growing interest in such data from competitors, suppliers, researchers and the general public. We demonstrate how machine learning can be leveraged to extract references to guest nationalities from unstructured text reviews in order to dynamically assess and monitor the dynamics of guest composition of individual businesses. In particular, we show that a rather simple architecture of pre-trained embeddings and stacked LSTM layers provides a better performance-runtime tradeoff than more complex state-of-the-art language models.
</details>
<details>
<summary>摘要</summary>
Many hotels target their guest acquisition efforts at specific markets in order to best anticipate the individual preferences and needs of their guests. Likewise, this strategic positioning is a prerequisite for efficient marketing budget allocation. Official statistics report on the number of visitors from different countries, but there is no fine-grained information on the guest composition of individual businesses. However, there is growing interest in such data from competitors, suppliers, researchers, and the general public. We demonstrate how machine learning can be leveraged to extract references to guest nationalities from unstructured text reviews in order to dynamically assess and monitor the dynamics of guest composition of individual businesses. In particular, we show that a relatively simple architecture of pre-trained embeddings and stacked LSTM layers provides a better performance-runtime tradeoff than more complex state-of-the-art language models.Here's a word-for-word translation of the text into Simplified Chinese: muchos 酒店target their 客源取得 efforts at specific markets in order to best anticipate the individual preferences and needs of their guests. Likewise, this strategic positioning is a prerequisite for efficient marketing budget allocation. Official statistics report on the number of visitors from different countries, but there is no fine-grained information on the guest composition of individual businesses. However, there is growing interest in such data from competitors, suppliers, researchers, and the general public. We demonstrate how machine learning can be leveraged to extract references to guest nationalities from unstructured text reviews in order to dynamically assess and monitor the dynamics of guest composition of individual businesses. In particular, we show that a relatively simple architecture of pre-trained embeddings and stacked LSTM layers provides a better performance-runtime tradeoff than more complex state-of-the-art language models.
</details></li>
</ul>
<hr>
<h2 id="Task-Conditioned-BERT-for-Joint-Intent-Detection-and-Slot-filling"><a href="#Task-Conditioned-BERT-for-Joint-Intent-Detection-and-Slot-filling" class="headerlink" title="Task Conditioned BERT for Joint Intent Detection and Slot-filling"></a>Task Conditioned BERT for Joint Intent Detection and Slot-filling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06165">http://arxiv.org/abs/2308.06165</a></li>
<li>repo_url: None</li>
<li>paper_authors: Diogo Tavares, Pedro Azevedo, David Semedo, Ricardo Sousa, João Magalhães</li>
<li>for: 本研究旨在解决对话系统中的不可预测用户意图和多个插槽的多样性问题，以实现对话状态的跟踪和用户喜好的理解。</li>
<li>methods: 该研究提出了一种基于Transformer编码器的原则性模型，通过在多个任务上训练模型，并通过丰富的输入来conditioning模型。</li>
<li>results: 实验结果表明，通过conditioning模型在多个对话推理任务上的输入，可以实现对MultiWOZ数据集上的同时意图和插槽检测的提高，具体是3.2%、10.8%和14.4%。此外，在真实的Farfetch客户对话中，提出的conditioned BERT也可以在对话中实现高度的共同目标和意图检测性能。<details>
<summary>Abstract</summary>
Dialogue systems need to deal with the unpredictability of user intents to track dialogue state and the heterogeneity of slots to understand user preferences. In this paper we investigate the hypothesis that solving these challenges as one unified model will allow the transfer of parameter support data across the different tasks. The proposed principled model is based on a Transformer encoder, trained on multiple tasks, and leveraged by a rich input that conditions the model on the target inferences. Conditioning the Transformer encoder on multiple target inferences over the same corpus, i.e., intent and multiple slot types, allows learning richer language interactions than a single-task model would be able to. In fact, experimental results demonstrate that conditioning the model on an increasing number of dialogue inference tasks leads to improved results: on the MultiWOZ dataset, the joint intent and slot detection can be improved by 3.2\% by conditioning on intent, 10.8\% by conditioning on slot and 14.4\% by conditioning on both intent and slots. Moreover, on real conversations with Farfetch costumers, the proposed conditioned BERT can achieve high joint-goal and intent detection performance throughout a dialogue.
</details>
<details>
<summary>摘要</summary>
对话系统需要面对用户意图的不可预测性和插槽的多样性，以便理解用户偏好。在这篇论文中，我们研究了假设：将这些挑战作为一个统一的模型来处理，可以在不同任务之间传递参数支持数据。我们提出的原则性的模型基于Transformer编码器，在多个任务上训练，并且使用丰富的输入来 condition the model 于目标推理。conditioning the Transformer encoder 于多个对话推理任务上的同一个 корпу斯（例如，意图和多个插槽类型），可以学习更加丰富的语言互动。实际结果表明，conditioning the model 于增加的对话推理任务可以提高结果：在MultiWOZ dataset上，联合意图和插槽检测可以提高3.2%，联合插槽和意图检测可以提高10.8%，而联合意图和插槽检测可以提高14.4%。此外，使用conditioned BERT在真实的对话中，可以 achieve high joint-goal和意图检测性能。
</details></li>
</ul>
<hr>
<h2 id="Identification-of-the-Relevance-of-Comments-in-Codes-Using-Bag-of-Words-and-Transformer-Based-Models"><a href="#Identification-of-the-Relevance-of-Comments-in-Codes-Using-Bag-of-Words-and-Transformer-Based-Models" class="headerlink" title="Identification of the Relevance of Comments in Codes Using Bag of Words and Transformer Based Models"></a>Identification of the Relevance of Comments in Codes Using Bag of Words and Transformer Based Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06144">http://arxiv.org/abs/2308.06144</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sruthisudheer/comment-classification-of-c-code">https://github.com/sruthisudheer/comment-classification-of-c-code</a></li>
<li>paper_authors: Sruthi S, Tanmay Basu</li>
<li>for: 本研究的目的是分类代码段落的注释是否相关。</li>
<li>methods: 本研究使用了不同的特征工程方案和文本分类技术，包括经典的袋子字符模型和基于转换器的模型。</li>
<li>results: 研究发现，使用袋子字符模型在训练集上表现最佳，但模型在训练和测试集上的表现并不理想。<details>
<summary>Abstract</summary>
The Forum for Information Retrieval (FIRE) started a shared task this year for classification of comments of different code segments. This is binary text classification task where the objective is to identify whether comments given for certain code segments are relevant or not. The BioNLP-IISERB group at the Indian Institute of Science Education and Research Bhopal (IISERB) participated in this task and submitted five runs for five different models. The paper presents the overview of the models and other significant findings on the training corpus. The methods involve different feature engineering schemes and text classification techniques. The performance of the classical bag of words model and transformer-based models were explored to identify significant features from the given training corpus. We have explored different classifiers viz., random forest, support vector machine and logistic regression using the bag of words model. Furthermore, the pre-trained transformer based models like BERT, RoBERT and ALBERT were also used by fine-tuning them on the given training corpus. The performance of different such models over the training corpus were reported and the best five models were implemented on the given test corpus. The empirical results show that the bag of words model outperforms the transformer based models, however, the performance of our runs are not reasonably well in both training and test corpus. This paper also addresses the limitations of the models and scope for further improvement.
</details>
<details>
<summary>摘要</summary>
《信息检索论坛（FIRE）》这年开始了代码段评注分类的共同任务。这是一个二分类文本分类任务，目标是判断给定代码段的评注是否相关。印度科学教育研究所 Bhopal（IISERB）的 BioNLP-IISERB 组participated in this task and submitted five runs for five different models. 本文介绍了模型和其他对训练集的重要发现。方法包括不同的特征工程方案和文本分类技术。我们使用了经典的包装词语模型和转换器基于模型，并对训练集进行了不同的特征工程和分类技术的探索。我们还使用了随机森林、支持向量机和梯度回归等分类器，并使用了包装词语模型。此外，我们还使用了预训练的转换器基于模型，如 BERT、RoBERT 和 ALBERT，并将其在训练集上进行了精度训练。对于训练集和测试集，我们报告了不同模型的性能，并选择了最佳五个模型进行实现。实验结果表明，包装词语模型在训练集上表现出色，但我们的运行并没有在训练集和测试集上表现出理想的性能。这篇文章还讨论了模型的限制和改进的可能性。
</details></li>
</ul>
<hr>
<h2 id="Lip2Vec-Efficient-and-Robust-Visual-Speech-Recognition-via-Latent-to-Latent-Visual-to-Audio-Representation-Mapping"><a href="#Lip2Vec-Efficient-and-Robust-Visual-Speech-Recognition-via-Latent-to-Latent-Visual-to-Audio-Representation-Mapping" class="headerlink" title="Lip2Vec: Efficient and Robust Visual Speech Recognition via Latent-to-Latent Visual to Audio Representation Mapping"></a>Lip2Vec: Efficient and Robust Visual Speech Recognition via Latent-to-Latent Visual to Audio Representation Mapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06112">http://arxiv.org/abs/2308.06112</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yasser Abdelaziz Dahou Djilali, Sanath Narayan, Haithem Boussaid, Ebtessam Almazrouei, Merouane Debbah</li>
<li>for: 本文目的是提出一种简单的方法，以便在视频序列中进行语音识别。</li>
<li>methods: 该方法基于学习一个先验模型，将视频序列中的唇形态编码器映射到匹配的音频对应的唇形态编码器，然后使用一个Off-the-shelf Audio Speech Recognition（ASR）模型将生成的音频表示转换为文本。</li>
<li>results: 该方法在LRS3数据集上比前期方法更高效，达到26 WER水平，而且与State-of-the-art（SoTA）方法不同，该模型在VoxCeleb测试集上保持了理想的性能。<details>
<summary>Abstract</summary>
Visual Speech Recognition (VSR) differs from the common perception tasks as it requires deeper reasoning over the video sequence, even by human experts. Despite the recent advances in VSR, current approaches rely on labeled data to fully train or finetune their models predicting the target speech. This hinders their ability to generalize well beyond the training set and leads to performance degeneration under out-of-distribution challenging scenarios. Unlike previous works that involve auxiliary losses or complex training procedures and architectures, we propose a simple approach, named Lip2Vec that is based on learning a prior model. Given a robust visual speech encoder, this network maps the encoded latent representations of the lip sequence to their corresponding latents from the audio pair, which are sufficiently invariant for effective text decoding. The generated audio representation is then decoded to text using an off-the-shelf Audio Speech Recognition (ASR) model. The proposed model compares favorably with fully-supervised learning methods on the LRS3 dataset achieving 26 WER. Unlike SoTA approaches, our model keeps a reasonable performance on the VoxCeleb test set. We believe that reprogramming the VSR as an ASR task narrows the performance gap between the two and paves the way for more flexible formulations of lip reading.
</details>
<details>
<summary>摘要</summary>
视觉语音识别（VSR）与常见的观察任务不同，它需要对视频序列进行更深层次的理解，即使由人类专家也是如此。尽管最近有一些关于VSR的进步，但现有的方法仍然依赖于标注数据来完全训练或调整其模型，这会导致它们在不同于训练集的场景下表现不佳。不同于之前的工作，我们提出了一种简单的方法，即 Lip2Vec，它基于学习一个先验模型。给定一个强大的视觉语音编码器，这个网络将编码后的唇形态封装到它们对应的音频对的 latent 表示中，这些表示是 suficiently 惰性的，以便有效地解码文本。生成的音频表示然后被使用一个购买的 Audio Speech Recognition（ASR）模型来解码为文本。我们提出的模型与完全supervised学习方法在 LRS3 数据集上比较，达到 26 WER 的性能。与 SoTA 方法不同，我们的模型在 VoxCeleb 测试集上保持了合理的性能。我们认为，将 VSR 转换为 ASR 任务，可以减少两者之间的性能差距，并为更flexible的唇读法提供了道路。
</details></li>
</ul>
<hr>
<h2 id="Fly-Swat-or-Cannon-Cost-Effective-Language-Model-Choice-via-Meta-Modeling"><a href="#Fly-Swat-or-Cannon-Cost-Effective-Language-Model-Choice-via-Meta-Modeling" class="headerlink" title="Fly-Swat or Cannon? Cost-Effective Language Model Choice via Meta-Modeling"></a>Fly-Swat or Cannon? Cost-Effective Language Model Choice via Meta-Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06077">http://arxiv.org/abs/2308.06077</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marija Šakota, Maxime Peyrard, Robert West</li>
<li>for: 这个研究是为了提出一个成本效益探索（CELMOC）框架，帮助选择适当的语言模型（LM），以提高整体性能while reducing cost.</li>
<li>methods: 这个研究使用了四个不同大小和成本的语言模型（LM），并使用了一个meta-model来预测每个输入将对哪个LM有好的表现。然后，这个框架将输入分配给预测对应的LM，以提高整体性能while reducing cost.</li>
<li>results: 这个研究发现，使用这个框架可以与使用最大化LM的性能相似，但是可以降低成本63%。这个框架可以帮助研究者和实践者共同储存大量的钱。<details>
<summary>Abstract</summary>
Generative language models (LMs) have become omnipresent across data science. For a wide variety of tasks, inputs can be phrased as natural language prompts for an LM, from whose output the solution can then be extracted. LM performance has consistently been increasing with model size - but so has the monetary cost of querying the ever larger models. Importantly, however, not all inputs are equally hard: some require larger LMs for obtaining a satisfactory solution, whereas for others smaller LMs suffice. Based on this fact, we design a framework for Cost-Effective Language Model Choice (CELMOC). Given a set of inputs and a set of candidate LMs, CELMOC judiciously assigns each input to an LM predicted to do well on the input according to a so-called meta-model, aiming to achieve high overall performance at low cost. The cost-performance trade-off can be flexibly tuned by the user. Options include, among others, maximizing total expected performance (or the number of processed inputs) while staying within a given cost budget, or minimizing total cost while processing all inputs. We evaluate CELMOC on 14 datasets covering five natural language tasks, using four candidate LMs of vastly different size and cost. With CELMOC, we match the performance of the largest available LM while achieving a cost reduction of 63%. Via our publicly available library, researchers as well as practitioners can thus save large amounts of money without sacrificing performance.
</details>
<details>
<summary>摘要</summary>
现代语言模型（LM）在数据科学中变得普遍，它们可以用来解决各种任务，并且可以通过自然语言提示来获得答案。然而，LM的性能随模型大小的增加而提高，但是查询费用也在增加。这意味着，不同的输入可能需要不同的LM来获得满意的答案，而不同的LM可能需要不同的费用。基于这一点，我们设计了一个Cost-Effective Language Model Choice（CELMOC）框架。给定一组输入和一组候选LM，CELMOC会judiciously将每个输入分配给一个LM，以便在一个称为meta-model中 predictions of the LM's performance on the input，以实现高效性低成本。用户可以通过调整cost-performance贸易来灵活地调整成本-性能平衡。选项包括最大化总预期性能（或处理输入数）的成本不超过一定预算，或者最小化成本的情况下处理所有输入。我们在14个数据集上进行了五种自然语言任务的测试，使用四种不同的LM，并证明了CELMOC可以与最大可用LM的性能匹配，同时实现63%的成本减少。通过我们公开提供的库，研究人员和实践者都可以大幅降低成本，无需牺牲性能。
</details></li>
</ul>
<hr>
<h2 id="A-Case-Study-on-Context-Encoding-in-Multi-Encoder-based-Document-Level-Neural-Machine-Translation"><a href="#A-Case-Study-on-Context-Encoding-in-Multi-Encoder-based-Document-Level-Neural-Machine-Translation" class="headerlink" title="A Case Study on Context Encoding in Multi-Encoder based Document-Level Neural Machine Translation"></a>A Case Study on Context Encoding in Multi-Encoder based Document-Level Neural Machine Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06063">http://arxiv.org/abs/2308.06063</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ramakrishna Appicharla, Baban Gain, Santanu Pal, Asif Ekbal</li>
<li>for: 研究人员希望了解多个encoder模型在不同的上下文情况下的表现，以提高模型在翻译中的准确性。</li>
<li>methods: 研究人员使用多个encoder模型，并对其进行训练，以便在不同的上下文情况下进行翻译。</li>
<li>results: 研究人员发现，even when the context is random, the model can still perform well on the ContraPro test set。此外，研究人员还发现，混合选择的上下文和随机上下文的设置通常比其他设置更好。<details>
<summary>Abstract</summary>
Recent studies have shown that the multi-encoder models are agnostic to the choice of context, and the context encoder generates noise which helps improve the models in terms of BLEU score. In this paper, we further explore this idea by evaluating with context-aware pronoun translation test set by training multi-encoder models trained on three different context settings viz, previous two sentences, random two sentences, and a mix of both as context. Specifically, we evaluate the models on the ContraPro test set to study how different contexts affect pronoun translation accuracy. The results show that the model can perform well on the ContraPro test set even when the context is random. We also analyze the source representations to study whether the context encoder generates noise. Our analysis shows that the context encoder provides sufficient information to learn discourse-level information. Additionally, we observe that mixing the selected context (the previous two sentences in this case) and the random context is generally better than the other settings.
</details>
<details>
<summary>摘要</summary>
近期研究表明，多encoder模型对选择 контекст无关，context encoder生成噪声可以提高模型在BLEU分数上的表现。在这篇论文中，我们进一步探究这个想法，通过训练基于三种不同context设置的多encoder模型，并在ContraPro测试集上评估其表现。结果显示，模型可以在随机context下表现良好。我们还分析了源表示，确定context encoder是否生成噪声。我们的分析表明，context encoder提供了足够的信息来学习论坛水平信息。此外，我们发现混合选定context（在这种情况下是前两句）和随机context通常比其他设置更好。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Picture-Description-Speech-for-Dementia-Detection-using-Image-text-Alignment"><a href="#Evaluating-Picture-Description-Speech-for-Dementia-Detection-using-Image-text-Alignment" class="headerlink" title="Evaluating Picture Description Speech for Dementia Detection using Image-text Alignment"></a>Evaluating Picture Description Speech for Dementia Detection using Image-text Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07933">http://arxiv.org/abs/2308.07933</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youxiang Zhu, Nana Lin, Xiaohui Liang, John A. Batsis, Robert M. Roth, Brian MacWhinney</li>
<li>for: 检测诊断阿尔茨海默病（dementia）</li>
<li>methods: 利用图像描述文本对预处理样本，并利用大型预训练图像文本对适应模型。</li>
<li>results: 提出了首个利用图像和描述文本输入，并利用图像文本对适应模型的诊断阿尔茨海默病模型，实现了state-of-the-art表现，检测精度达83.44%，高于文本只基eline模型的79.91%。<details>
<summary>Abstract</summary>
Using picture description speech for dementia detection has been studied for 30 years. Despite the long history, previous models focus on identifying the differences in speech patterns between healthy subjects and patients with dementia but do not utilize the picture information directly. In this paper, we propose the first dementia detection models that take both the picture and the description texts as inputs and incorporate knowledge from large pre-trained image-text alignment models. We observe the difference between dementia and healthy samples in terms of the text's relevance to the picture and the focused area of the picture. We thus consider such a difference could be used to enhance dementia detection accuracy. Specifically, we use the text's relevance to the picture to rank and filter the sentences of the samples. We also identified focused areas of the picture as topics and categorized the sentences according to the focused areas. We propose three advanced models that pre-processed the samples based on their relevance to the picture, sub-image, and focused areas. The evaluation results show that our advanced models, with knowledge of the picture and large image-text alignment models, achieve state-of-the-art performance with the best detection accuracy at 83.44%, which is higher than the text-only baseline model at 79.91%. Lastly, we visualize the sample and picture results to explain the advantages of our models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="PIPPA-A-Partially-Synthetic-Conversational-Dataset"><a href="#PIPPA-A-Partially-Synthetic-Conversational-Dataset" class="headerlink" title="PIPPA: A Partially Synthetic Conversational Dataset"></a>PIPPA: A Partially Synthetic Conversational Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05884">http://arxiv.org/abs/2308.05884</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tear Gosling, Alpin Dale, Yinhe Zheng</li>
<li>for: 本研究旨在提供一个基于人工智能的会话和扮演数据集，以便研究人工智能系统在角色扮演场景中的发展。</li>
<li>methods: 本研究使用了社区驱动的协同募集方法，吸引了一群游戏爱好者参与，共创造出了超过100万句话的会话记录，分布在26,000个对话会话中。</li>
<li>results: 本研究提供了一个名为PIPPA的半 sintetic数据集，该数据集包含了丰富的会话记录，可以为研究人工智能系统在角色扮演场景中的发展提供一个重要的资源。<details>
<summary>Abstract</summary>
With the emergence of increasingly powerful large language models, there is a burgeoning interest in leveraging these models for casual conversation and role-play applications. However, existing conversational and role-playing datasets often fail to capture the diverse and nuanced interactions typically exhibited by real-world role-play participants. To address this limitation and contribute to the rapidly growing field, we introduce a partially-synthetic dataset named PIPPA (Personal Interaction Pairs between People and AI). PIPPA is a result of a community-driven crowdsourcing effort involving a group of role-play enthusiasts. The dataset comprises over 1 million utterances that are distributed across 26,000 conversation sessions and provides a rich resource for researchers and AI developers to explore and refine conversational AI systems in the context of role-play scenarios.
</details>
<details>
<summary>摘要</summary>
“大型语言模型的出现使得人们对协谈和角色扮演应用的兴趣增加。然而，现有的协谈和角色扮演数据集常常无法捕捉真实世界角色扮演者之间的多样化和细节化互动。为解决这个限制，我们介绍了一个名为PIPPA（人工智能与人际互动对）的半人工数据集。PIPPA是由社区营运的调询员们组成的一个志工团队所创建的，数据集包含了过百万句说话，分布在26,000个对话会议中，并提供了一个丰富的资源供研究人员和AI开发者们探索和检验协谈AI系统在角色扮演场景中的表现。”Note that Simplified Chinese is the standard writing system used in mainland China, while Traditional Chinese is used in Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="EXPRESSO-A-Benchmark-and-Analysis-of-Discrete-Expressive-Speech-Resynthesis"><a href="#EXPRESSO-A-Benchmark-and-Analysis-of-Discrete-Expressive-Speech-Resynthesis" class="headerlink" title="EXPRESSO: A Benchmark and Analysis of Discrete Expressive Speech Resynthesis"></a>EXPRESSO: A Benchmark and Analysis of Discrete Expressive Speech Resynthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05725">http://arxiv.org/abs/2308.05725</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tu Anh Nguyen, Wei-Ning Hsu, Antony D’Avirro, Bowen Shi, Itai Gat, Maryam Fazel-Zarani, Tal Remez, Jade Copet, Gabriel Synnaeve, Michael Hassid, Felix Kreuk, Yossi Adi, Emmanuel Dupoux</li>
<li>for: 这个论文是为了研究textless speech synthesis的高质量表达方法和数据集。</li>
<li>methods: 这篇论文使用了自适应学习的低比特率精炼单元来重新生成高质量的speech，并使用了26种自然的善意表达方式来生成各种不同的表达方式。</li>
<li>results: 该论文 introduce了一个高质量的表达型speech数据集，包括了读出的speech和自由对话，并提供了一个表达 benchmark 来评估不同自适应精炼单元的表达质量。<details>
<summary>Abstract</summary>
Recent work has shown that it is possible to resynthesize high-quality speech based, not on text, but on low bitrate discrete units that have been learned in a self-supervised fashion and can therefore capture expressive aspects of speech that are hard to transcribe (prosody, voice styles, non-verbal vocalization). The adoption of these methods is still limited by the fact that most speech synthesis datasets are read, severely limiting spontaneity and expressivity. Here, we introduce Expresso, a high-quality expressive speech dataset for textless speech synthesis that includes both read speech and improvised dialogues rendered in 26 spontaneous expressive styles. We illustrate the challenges and potentials of this dataset with an expressive resynthesis benchmark where the task is to encode the input in low-bitrate units and resynthesize it in a target voice while preserving content and style. We evaluate resynthesis quality with automatic metrics for different self-supervised discrete encoders, and explore tradeoffs between quality, bitrate and invariance to speaker and style. All the dataset, evaluation metrics and baseline models are open source
</details>
<details>
<summary>摘要</summary>
最近的研究表明，可以基于低比特率独立单元进行高质量的语音重synthesis，而这些单元可以在自我超vised的方式下学习，因此可以捕捉到语音中的表达特征，如语音调、声音风格和非语音 vocalization。然而，这些方法的应用仍受到大多数语音合成数据集是阅读的限制，这限制了它们的自由和表达力。在这里，我们介绍Expresso，一个高质量的自由语音合成数据集，包括了阅读语音和自由对话，并在26种自由表达风格中进行了rendering。我们描述了这个数据集的挑战和潜力，并通过一个表达 benchark来评估重synthesis质量，其中任务是将输入编码成低比特率单元并在目标声音中重synthesize，保持内容和风格不变。我们使用自动测试 метри来评估重synthesis质量，并探讨了不同自动测试 метри的tradeoffs，以及bitrate、 speaker和风格的不变性。所有的数据集、评估 метри和基eline模型都是开源的。
</details></li>
</ul>
<hr>
<h2 id="A-Preliminary-Study-of-the-Intrinsic-Relationship-between-Complexity-and-Alignment"><a href="#A-Preliminary-Study-of-the-Intrinsic-Relationship-between-Complexity-and-Alignment" class="headerlink" title="A Preliminary Study of the Intrinsic Relationship between Complexity and Alignment"></a>A Preliminary Study of the Intrinsic Relationship between Complexity and Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05696">http://arxiv.org/abs/2308.05696</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alibabaresearch/damo-convai">https://github.com/alibabaresearch/damo-convai</a></li>
<li>paper_authors: Yingxiu Zhao, Bowen Yu, Binyuan Hui, Haiyang Yu, Fei Huang, Yongbin Li, Nevin L. Zhang</li>
<li>for: 提高大型自然语言模型（LLMs）在开放领域指令数据上的训练，以实现与终端任务和用户偏好更好地对齐。</li>
<li>methods: 通过控制指令数据的复杂性来提高性能。提出了一种名为“tree-instruct”的方法，通过添加指令 semantic tree 中指定的节点数来生成新的指令数据，并通过调整添加的节点数来控制Difficulty Level。</li>
<li>results: 通过实验发现，增加复杂性可以持续提高性能，例如使用1,000个指令数据和10个节点可以提高胜率24%。同时发现，相同的字符数预算下，一些复杂的指令可以超越多样但简单的指令。此外，训练课程指令调整可能并不是预期的结果，关键在于增加复杂性。<details>
<summary>Abstract</summary>
Training large language models (LLMs) with open-domain instruction data has yielded remarkable success in aligning to end tasks and user preferences. Extensive research has highlighted that enhancing the quality and diversity of instruction data consistently improves performance. However, the impact of data complexity, as a crucial metric, remains relatively unexplored in three aspects: (1) scaling law, where the sustainability of performance improvements with increasing complexity is uncertain, (2) additional tokens, whether the improvement brought by complexity comes from introducing more training tokens, and (3) curriculum tuning, where the potential advantages of incorporating instructions ranging from easy to difficult are not yet fully understood. In this paper, we propose \textit{tree-instruct} to systematically enhance the complexity of instruction data in a controllable manner. This approach adds a specified number of nodes into the instruction semantic tree, yielding new instruction data based on the modified tree. By adjusting the number of added nodes, we can control the difficulty level in the modified instruction data. Our preliminary experiments reveal the following insights: (1) Increasing complexity consistently leads to sustained performance improvements. For instance, using 1,000 instruction data and 10 nodes resulted in a substantial 24\% increase in win rate. (2) Under the same token budget, a few complex instructions outperform diverse yet simple instructions. (3) Curriculum instruction tuning might not yield the anticipated results; focusing on increasing complexity appears to be the key.
</details>
<details>
<summary>摘要</summary>
训练大型自然语言模型（LLM）与开放领域指令数据有着惊人的成功，它们可以很好地适应到任务和用户喜好。广泛的研究表明，提高指令数据质量和多样性可以一直提高性能。然而，数据复杂性的影响，作为一个关键指标，还没有得到充分的探索。特别是，有三个方面的研究仍然不够：（1）扩展法律，是否可以长期维持性能提高的可靠性，（2）新的特征 Tokens 是否真的带来了性能提高，以及（3）课程调整，是否可以通过从易到Difficult的指令进行调整来获得更好的效果。本文提出了一种名为“tree-instruct”的方法，可以系统地提高指令数据的复杂性。这种方法在指令semantic树中添加指定的节点数量，从而生成新的指令数据。通过调整添加的节点数量，可以控制修改后的指令数据的难度水平。我们的初步实验发现了以下结论：（1）增加复杂性一直会持续提高性能。例如，使用 1,000 个指令数据和 10 个节点，可以获得大量的 24% 的提高。（2）在同一个token预算下，一些复杂的指令会超过多样但简单的指令。（3）课程调整可能并不会带来预期的结果，而是关注增加复杂性才是关键。
</details></li>
</ul>
<hr>
<h2 id="Finding-Already-Debunked-Narratives-via-Multistage-Retrieval-Enabling-Cross-Lingual-Cross-Dataset-and-Zero-Shot-Learning"><a href="#Finding-Already-Debunked-Narratives-via-Multistage-Retrieval-Enabling-Cross-Lingual-Cross-Dataset-and-Zero-Shot-Learning" class="headerlink" title="Finding Already Debunked Narratives via Multistage Retrieval: Enabling Cross-Lingual, Cross-Dataset and Zero-Shot Learning"></a>Finding Already Debunked Narratives via Multistage Retrieval: Enabling Cross-Lingual, Cross-Dataset and Zero-Shot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05680">http://arxiv.org/abs/2308.05680</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iknoor Singh, Carolina Scarton, Xingyi Song, Kalina Bontcheva</li>
<li>for: This paper aims to detect stories that have already been debunked to reduce the manual efforts of professional fact-checkers and slow the spread of misinformation.</li>
<li>methods: The paper creates a novel dataset for cross-lingual retrieval of already debunked narratives using tweets as queries to a database of fact-checking articles. It also presents an extensive experiment to benchmark fine-tuned and off-the-shelf multilingual pre-trained Transformer models for this task.</li>
<li>results: The results show that the task of cross-lingual retrieval of already debunked narratives is challenging, and off-the-shelf Transformer models fail to outperform a strong lexical-based baseline (BM25). However, the paper’s multistage retrieval framework is robust and outperforms BM25 in most scenarios, enabling cross-domain and zero-shot learning without significantly harming the model’s performance.<details>
<summary>Abstract</summary>
The task of retrieving already debunked narratives aims to detect stories that have already been fact-checked. The successful detection of claims that have already been debunked not only reduces the manual efforts of professional fact-checkers but can also contribute to slowing the spread of misinformation. Mainly due to the lack of readily available data, this is an understudied problem, particularly when considering the cross-lingual task, i.e. the retrieval of fact-checking articles in a language different from the language of the online post being checked. This paper fills this gap by (i) creating a novel dataset to enable research on cross-lingual retrieval of already debunked narratives, using tweets as queries to a database of fact-checking articles; (ii) presenting an extensive experiment to benchmark fine-tuned and off-the-shelf multilingual pre-trained Transformer models for this task; and (iii) proposing a novel multistage framework that divides this cross-lingual debunk retrieval task into refinement and re-ranking stages. Results show that the task of cross-lingual retrieval of already debunked narratives is challenging and off-the-shelf Transformer models fail to outperform a strong lexical-based baseline (BM25). Nevertheless, our multistage retrieval framework is robust, outperforming BM25 in most scenarios and enabling cross-domain and zero-shot learning, without significantly harming the model's performance.
</details>
<details>
<summary>摘要</summary>
该任务是检索已经证伪的故事，目的是检测已经被证实的故事。成功检测已经证伪的故事不仅可以减少专业 фактоCheckers的手动努力，还可以减slow下迷信的传播。然而，由于数据的不Ready availability，这是一个未得到充分研究的问题，特别是跨语言任务，即在不同语言的 онлайн文章被检查时， retrieve fact-checking articles。这篇论文填补了这一漏洞，通过以下三个方面：1. 创建了一个新的数据集，用于启发研究跨语言检索已经证伪的故事，使用推文作为查询语。2. 进行了广泛的实验，以评估 fine-tuned 和 off-the-shelf 多语言预训练Transformer模型的表现。3. 提出了一个多Stage框架，将跨语言检索已经证伪的故事任务分为两个阶段：精细化阶段和重新排序阶段。结果显示，跨语言检索已经证伪的故事是一个具有挑战性的任务，off-the-shelf Transformer模型无法超过一个强的字符基本模型（BM25）。然而，我们的多Stage Retrieval框架是可靠的，在大多数场景下超过 BM25，并且具有跨频域和零shot学习能力，无需明显危害模型性能。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/11/cs.CL_2023_08_11/" data-id="clpxp03wf00a6fm88a5wgb8pt" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/69/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/68/">68</a><a class="page-number" href="/page/69/">69</a><span class="page-number current">70</span><a class="page-number" href="/page/71/">71</a><a class="page-number" href="/page/72/">72</a><span class="space">&hellip;</span><a class="page-number" href="/page/98/">98</a><a class="extend next" rel="next" href="/page/71/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">67</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">82</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">147</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
