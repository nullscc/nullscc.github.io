
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/32/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-eess.SP_2023_10_09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/09/eess.SP_2023_10_09/" class="article-date">
  <time datetime="2023-10-09T08:00:00.000Z" itemprop="datePublished">2023-10-09</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/09/eess.SP_2023_10_09/">eess.SP - 2023-10-09</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Extended-Reality-via-Cooperative-NOMA-in-Hybrid-Cloud-Mobile-Edge-Computing-Networks"><a href="#Extended-Reality-via-Cooperative-NOMA-in-Hybrid-Cloud-Mobile-Edge-Computing-Networks" class="headerlink" title="Extended Reality via Cooperative NOMA in Hybrid Cloud&#x2F;Mobile-Edge Computing Networks"></a>Extended Reality via Cooperative NOMA in Hybrid Cloud&#x2F;Mobile-Edge Computing Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06874">http://arxiv.org/abs/2310.06874</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robert-Jeron Reifert, Hayssam Dahrouj, Aydin Sezgin</li>
<li>for: 这篇论文旨在解决未来的扩展现实（XR）应用程序中的资源消耗性任务问题，通过融合中央云（CC）、边缘计算（EC）和无人机（UAV）的能力，以提高XR应用程序的质量体验。</li>
<li>methods: 该论文提出了一种基于协同非对称多接入（Co-NOMA）的无人机协助混合云&#x2F;移动边计算架构，以提高XR设备的质量体验。它还提出了一个权衡系统吞吐率和公平性的最大化问题，以确定计算和通信资源的分配和链接选择策略。</li>
<li>results: 该论文通过实验结果表明，提出的算法可以最大化系统吞吐率，同时保证系统公平性，并且在实际网络约束下（如能源消耗和延迟）下实现分布式实现。<details>
<summary>Abstract</summary>
Extended reality (XR) applications often perform resource-intensive tasks, which are computed remotely, a process that prioritizes the latency criticality aspect. To this end, this paper shows that through leveraging the power of the central cloud (CC), the close proximity of edge computers (ECs), and the flexibility of uncrewed aerial vehicles (UAVs), a UAV-aided hybrid cloud/mobile-edge computing architecture promises to handle the intricate requirements of future XR applications. In this context, this paper distinguishes between two types of XR devices, namely, strong and weak devices. The paper then introduces a cooperative non-orthogonal multiple access (Co-NOMA) scheme, pairing strong and weak devices, so as to aid the XR devices quality-of-user experience by intelligently selecting either the direct or the relay links toward the weak XR devices. A sum logarithmic-rate maximization problem is, thus, formulated so as to jointly determine the computation and communication resources, and link-selection strategy as a means to strike a trade-off between the system throughput and fairness. Subject to realistic network constraints, e.g., power consumption and delay, the optimization problem is then solved iteratively via discrete relaxations, successive-convex approximation, and fractional programming, an approach which can be implemented in a distributed fashion across the network. Simulation results validate the proposed algorithms performance in terms of log-rate maximization, delay-sensitivity, scalability, and runtime performance. The practical distributed Co-NOMA implementation is particularly shown to offer appreciable benefits over traditional multiple access and NOMA methods, highlighting its applicability in decentralized XR systems.
</details>
<details>
<summary>摘要</summary>
现实扩展（XR）应用程序通常执行资源密集的任务，这些任务通常在远程计算，以优先级顺序处理。为了实现这一目标，这篇论文提出了一种通过中央云（CC）、边缘计算（EC）和无人机（UAV）的 гибрид云/边缘计算架构来处理未来XR应用程序的复杂需求。在这个上下文中，这篇论文将XR设备分为两类：强设备和弱设备。论文然后引入了合作非对称多访问（Co-NOMA）方案，将强设备和弱设备相互协作，以提高XR设备用户体验质量。为了提高系统吞吐量和公平性，论文提出了一个总日志arithmic-rate最大化问题，以联合确定计算和通信资源，以及链接选择策略。充分考虑了现实网络约束，例如电力消耗和延迟，优化问题可以通过抽象relaxation、Successive-Convex Approximation和分数程序来解决。实际应用中，这种分布式Co-NOMA实现可以提供较高的日志率最大化、延迟敏感度、可扩展性和运行时性能。
</details></li>
</ul>
<hr>
<h2 id="Decomposition-Based-Interference-Management-Framework-for-Local-6G-Networks"><a href="#Decomposition-Based-Interference-Management-Framework-for-Local-6G-Networks" class="headerlink" title="Decomposition Based Interference Management Framework for Local 6G Networks"></a>Decomposition Based Interference Management Framework for Local 6G Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05809">http://arxiv.org/abs/2310.05809</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samitha Gunarathne, Thushan Sivalingam, Nurul Huda Mahmood, Nandana Rajatheva, Matti Latva-Aho</li>
<li>for: 本研究旨在提出一种智能干扰管理框架，用于 garantía de calidad de servicio （QoS）的 ultra-reliable low latency communications （URLLC）应用。</li>
<li>methods: 提议的算法包括了先进的信号预处理技术——empirical mode decomposition（EMD），然后使用序列-到-一个变换器算法进行预测每个分解成分的干扰电平。预测后，使用预测结果来估算未来信号干扰比例，并将资源分配以 garantía高可靠性。最后，基于预测的干扰信号，进行干扰抑制方案的研究。</li>
<li>results: 对于两种基准算法，提议的序列-到-一个变换器模型显示了其 robustness 性。与基准方案相比，提议方案可以降低平均квадратиче差误差值（RMSE）值，最高降低55%。<details>
<summary>Abstract</summary>
Managing inter-cell interference is among the major challenges in a wireless network, more so when strict quality of service needs to be guaranteed such as in ultra-reliable low latency communications (URLLC) applications. This study introduces a novel intelligent interference management framework for a local 6G network that allocates resources based on interference prediction. The proposed algorithm involves an advanced signal pre-processing technique known as empirical mode decomposition followed by prediction of each decomposed component using the sequence-to-one transformer algorithm. The predicted interference power is then used to estimate future signal-to-interference plus noise ratio, and subsequently allocate resources to guarantee the high reliability required by URLLC applications. Finally, an interference cancellation scheme is explored based on the predicted interference signal with the transformer model. The proposed sequence-to-one transformer model exhibits its robustness for interference prediction. The proposed scheme is numerically evaluated against two baseline algorithms, and is found that the root mean squared error is reduced by up to 55% over a baseline scheme.
</details>
<details>
<summary>摘要</summary>
管理间细胞干扰是无线网络中的一个主要挑战，尤其是在需要保证严格的服务质量，如在超低延迟低功率通信（URLLC）应用中。本研究提出了一种新的智能干扰管理框架，用于本地6G网络资源分配。该算法包括一种高级的信号预处理技术known as empirical mode decomposition，然后使用序列到一转换器算法预测每个分解成分。预测的干扰功率然后用于估算未来信号干扰 plus noise ratio，并在保证URLLC应用所需的高可靠性的情况下分配资源。最后，基于预测的干扰信号，探讨了一种干扰抵消方案，使用转换器模型。提出的序列到一转换器模型在干扰预测中展现了其强健性。与两个基线算法进行比较，研究发现，使用该方案可以将根mean squared error降低到55%以下。
</details></li>
</ul>
<hr>
<h2 id="Computation-Limited-Signals-A-Channel-Capacity-Regime-Constrained-by-Computational-Complexity"><a href="#Computation-Limited-Signals-A-Channel-Capacity-Regime-Constrained-by-Computational-Complexity" class="headerlink" title="Computation-Limited Signals: A Channel Capacity Regime Constrained by Computational Complexity"></a>Computation-Limited Signals: A Channel Capacity Regime Constrained by Computational Complexity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05794">http://arxiv.org/abs/2310.05794</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saulo Queiroz, João P. Vilela, Edmundo Monteiro</li>
<li>for: 这篇论文探讨了计算限制（comp-limited）信号，即通信容量 régime中的计算时间复杂度开销是关键约束，而不是功率或带宽。</li>
<li>methods: 作者提出了一种新的数学框架，基于信息理论和计算复杂度的概念，以 relate 容量和时间复杂度。特别是，作者定义了一个名为算法容量的指标，表示在一个符号中模式化的比特数和通信符号转换所需的最低时间复杂度之间的比例。</li>
<li>results: 作者通过设置此指标为函数Channel资源，分类了一个给定的信号设计是comp-limited的。作者还提供了一个使用例子，表明无线OFDM传输器是comp-limited， Unless the lower-bound计算复杂度 of N-point DFT问题为 $\Omega(N)$，这是计算机科学中的一个开放问题。<details>
<summary>Abstract</summary>
In this letter, we introduce the computational-limited (comp-limited) signals, a communication capacity regime in which the signal time computational complexity overhead is the key constraint -- rather than power or bandwidth -- to the overall communication capacity. To relate capacity and time complexity, we propose a novel mathematical framework that builds on concepts of information theory and computational complexity. In particular, the algorithmic capacity stands for the ratio between the upper-bound number of bits modulated in a symbol and the lower-bound time complexity required to turn these bits into a communication symbol. By setting this ratio as function of the channel resources, we classify a given signal design as comp-limited if its algorithmic capacity nullifies as the channel resources grow. As a use-case, we show that an uncoded OFDM transmitter is comp-limited unless the lower-bound computational complexity of the N-point DFT problem verifies as $\Omega(N)$, which remains an open challenge in theoretical computer science.
</details>
<details>
<summary>摘要</summary>
文中，我们介绍了计算限制（comp-limited）信号，它是通信容量 Régime 中的一个条件，其中信号时间计算复杂度成本是主要的限制因素，而不是功率或带宽。为了将容量和时间复杂度相关联，我们提出了一个新的数学框架，基于信息理论和计算复杂度。具体来说，算法容量表示每个符号中模ulated的最高位数与转化这些位数为通信符号所需的最低时间复杂度之比。通过将这个比率设置为通道资源函数，我们可以将一个给定的信号设计分类为comp-limited。作为一个使用情况，我们显示了一个未编码的OFDM发送器是comp-limited， Unless the lower-bound computational complexity of the N-point DFT problem verifies as $\Omega(N)$, which remains an open challenge in theoretical computer science.Note: "计算限制" (comp-limited) is a term used to describe a communication system where the computational complexity of the signal processing is the primary limiting factor, rather than power or bandwidth.
</details></li>
</ul>
<hr>
<h2 id="Physical-Layer-Security-in-a-Private-5G-Network-for-Industrial-and-Mobility-Application"><a href="#Physical-Layer-Security-in-a-Private-5G-Network-for-Industrial-and-Mobility-Application" class="headerlink" title="Physical Layer Security in a Private 5G Network for Industrial and Mobility Application"></a>Physical Layer Security in a Private 5G Network for Industrial and Mobility Application</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05525">http://arxiv.org/abs/2310.05525</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shivraj Hanumant Gonde, Christoph Frisch, Svetoslav Duhovnikov, Martin Kubisch, Thomas Meyerhoff, Dominic Schupke</li>
<li>for: This paper is written for organizations that operate Private 5G networks in industrial environments, particularly those that require secure communication between devices.</li>
<li>methods: The paper uses Physical Layer Key Generation (PLKG) to generate a symmetric secret key between two nodes in the presence of a potential passive eavesdropper.</li>
<li>results: The paper demonstrates the establishment of a long-term symmetric key between an aerial vehicle and IT infrastructure in a manufacturing environment, using the radio interface of the Private 5G network.<details>
<summary>Abstract</summary>
Cellular communication technologies such as 5G are deployed on a large scale around the world. Compared to other communication technologies such as WiFi, Bluetooth, or Ultra Wideband, the 5G communication standard describes support for a large variety of use cases, e.g., Internet of Things, vehicular, industrial, and campus-wide communications. An organization can operate a Private 5G network to provide connectivity to devices in their manufacturing environment. Physical Layer Key Generation (PLKG) is a method to generate a symmetric secret on two nodes despite the presence of a potential passive eavesdropper. To the best of our knowledge, this work is one of the first to implement PLKG in a real Private 5G network. Therefore, it highlights the possibility of integrating PLKG in the communication technology highly relevant for industrial applications. This paper exemplifies the establishment of a long-term symmetric key between an aerial vehicle and IT infrastructure both located in a manufacturing environment and communicating via the radio interface of the Private 5G network.
</details>
<details>
<summary>摘要</summary>
fifth-generation 无线通信技术（5G）在全球范围内大规模部署。相比其他通信技术，如 WiFi、蓝牙或超宽带，5G 通信标准支持各种使用场景，如物联网、交通、工业和校园通信。组织可以运行专用5G网络，以提供制造环境中设备的连接性。物理层密钥生成（PLKG）是一种生成两个节点之间的同步密钥，即使存在可能的潜在窃听者。根据我们所知，这是首次在实际专用5G网络中实现PLKG。因此，它高亮了在工业应用中集成PLKG的可能性。这篇论文示例了在制造环境中的空中车和信息基础设施之间通过专用5G网络的广播 интер脑界面建立长期同步密钥。
</details></li>
</ul>
<hr>
<h2 id="MEDUSA-Scalable-Biometric-Sensing-in-the-Wild-through-Distributed-MIMO-Radars"><a href="#MEDUSA-Scalable-Biometric-Sensing-in-the-Wild-through-Distributed-MIMO-Radars" class="headerlink" title="MEDUSA: Scalable Biometric Sensing in the Wild through Distributed MIMO Radars"></a>MEDUSA: Scalable Biometric Sensing in the Wild through Distributed MIMO Radars</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05507">http://arxiv.org/abs/2310.05507</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yilong Li, Ramanujan K Sheshadri, Karthik Sundaresan, Eugene Chai, Suman Banerjee</li>
<li>for: 这个研究旨在开发一个基于激光的生命 Parameter 监测系统，以提供不断的无接触式生命 Parameter 监测和医疗应用。</li>
<li>methods: 这个系统使用了一种新的快速宽频UWB激光系统，具有自适应和可调的子网络。系统利用了分布式MIMO网络的多标的优点，以提供在实际世界中的生命 Parameter 监测。</li>
<li>results: 这个研究获得了20%的平均提升，相比于使用商业激光感知器的现有系统。这证明了MEDUSA的空间多标优点，包括目标和环境动态的监测在 familier和未知内部环境中。<details>
<summary>Abstract</summary>
Radar-based techniques for detecting vital signs have shown promise for continuous contactless vital sign sensing and healthcare applications. However, real-world indoor environments face significant challenges for existing vital sign monitoring systems. These include signal blockage in non-line-of-sight (NLOS) situations, movement of human subjects, and alterations in location and orientation. Additionally, these existing systems failed to address the challenge of tracking multiple targets simultaneously. To overcome these challenges, we present MEDUSA, a novel coherent ultra-wideband (UWB) based distributed multiple-input multiple-output (MIMO) radar system, especially it allows users to customize and disperse the $16 \times 16$ into sub-arrays. MEDUSA takes advantage of the diversity benefits of distributed yet wirelessly synchronized MIMO arrays to enable robust vital sign monitoring in real-world and daily living environments where human targets are moving and surrounded by obstacles. We've developed a scalable, self-supervised contrastive learning model which integrates seamlessly with our hardware platform. Each attention weight within the model corresponds to a specific antenna pair of Tx and Rx. The model proficiently recovers accurate vital sign waveforms by decomposing and correlating the mixed received signals, including comprising human motion, mobility, noise, and vital signs. Through extensive evaluations involving 21 participants and over 200 hours of collected data (3.75 TB in total, with 1.89 TB for static subjects and 1.86 TB for moving subjects), MEDUSA's performance has been validated, showing an average gain of 20% compared to existing systems employing COTS radar sensors. This demonstrates MEDUSA's spatial diversity gain for real-world vital sign monitoring, encompassing target and environmental dynamics in familiar and unfamiliar indoor environments.
</details>
<details>
<summary>摘要</summary>
采用雷达技术探测生命 Parameters 已经展示了不间断无接触的生命参数监测和医疗应用的搭建。然而，现实世界室内环境对现有生命参数监测系统带来了重大挑战。这些挑战包括雷达信号屏蔽（NLOS）情况下的信号干扰、人体活动的移动和位置和方向的变化。此外，现有系统无法同时跟踪多个目标。为了解决这些挑战，我们提出了MEDUSA，一种新的干扰频率ultra-wideband（UWB）基于分布式多输入多输出（MIMO）雷达系统。MEDUSA利用分布式 yet wirelessly synchronized MIMO数组的多样性优势，以实现robust生命参数监测在现实生活环境中， где人类目标在移动并围绕障碍物。我们开发了一种可扩展的自适应强化学习模型，该模型与我们的硬件平台集成了良好。每个注意力量在模型中对应于特定的天线对（Tx和Rx）。模型能够高效地提取生命参数波形，通过分解和相关处理混合接收信号，包括人体运动、 mobilicity、噪声和生命参数。经过了21名参与者和超过200小时的数据收集（总共3.75TB，其中1.89TB为静止目标和1.86TB为移动目标），MEDUSA的性能已经被验证，显示与现有系统使用商业雷达传感器相比，MEDUSA具有20%的平均提升。这表明MEDUSA在实际世界中具有空间多样性增强，包括目标和环境动态在 familiarn和未知室内环境中。
</details></li>
</ul>
<hr>
<h2 id="Affine-Frequency-Division-Multiplexing-With-Index-Modulation"><a href="#Affine-Frequency-Division-Multiplexing-With-Index-Modulation" class="headerlink" title="Affine Frequency Division Multiplexing With Index Modulation"></a>Affine Frequency Division Multiplexing With Index Modulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05475">http://arxiv.org/abs/2310.05475</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiwei Tao, Miaowen Wen, Yao Ge, Jun Li</li>
<li>for: 这个论文是为了研究一种基于振荡信号的多Provider frequency division multiplexing（AFDM）系统，并提出一种基于AFDM系统的索引编码（IM）方案。</li>
<li>methods: 该论文使用了AFDM系统的框架，并在DAF频域中使用了活动状态来实现索引编码。具体来说， authors将分割DAF域中的子符号，并考虑了本地化和分布式策略。</li>
<li>results: 该论文通过closed-form的极限紧张upper bound来证明IM方案的性能，并通过计算机实验证明了该方案的优越性。results show that index bits have stronger diversity protection than modulated bits even when the full diversity condition of AFDM is not satisfied.<details>
<summary>Abstract</summary>
Affine frequency division multiplexing (AFDM) is a new multicarrier technique based on chirp signals tailored for high-mobility communications, which can achieve full diversity. In this paper, we propose an index modulation (IM) scheme based on the framework of AFDM systems, named AFDM-IM. In the proposed AFDM-IM scheme, the information bits are carried by the activation state of the subsymbols in discrete affine Fourier (DAF) domain in addition to the conventional constellation symbols. To efficiently perform IM, we divide the subsymbols in DAF domain into several groups and consider both the localized and distributed strategies. An asymptotically tight upper bound on the average bit error rate (BER) of the maximum-likelihood detection in the existence of channel estimation errors is derived in closed-form. Computer simulations are carried out to evaluate the performance of the proposed AFDM-IM scheme, whose results corroborate its superiority over the benchmark schemes in the linear time-varying channels. We also evaluate the BER performance of the index and modulated bits for the AFDM-IM scheme with and without satisfying the full diversity condition of AFDM. The results show that the index bits have a stronger diversity protection than the modulated bits even when the full diversity condition of AFDM is not satisfied.
</details>
<details>
<summary>摘要</summary>
“Affine频率分多普通方式”（AFDM）是一种基于滑动信号的新多个 carriers 技术，适用于高移动通信，可以实现全多态性。在这篇论文中，我们提出了一个基于 AFDM 系统框架的指标修征（IM）方案，称为 AFDM-IM。在我们的提案中，信息位元被传递到 AFDM 系统中的几个批次中，并且在这些批次中使用传统的折衣符号。为了有效地实现 IM，我们在 DAF 领域中分割 subsymbols 成多个群体，并考虑了本地化和分散的两种策略。我们 derive 了一个对应于最大可能性探测的对应几何率（BER）的封闭式上界，并将其与 computer simulations 进行评估。结果显示，我们的 AFDM-IM 方案在线性时间变化频率对应于更高的性能。我们还评估了 AFDM-IM 方案中的指标位元和修征位元的 BER 性能，并发现指标位元在 AFDM 的全多态性不满足时仍然具有更强的多态保护。
</details></li>
</ul>
<hr>
<h2 id="Waveform-Design-for-MIMO-OFDM-Integrated-Sensing-and-Communication-System-An-Information-Theoretical-Approach"><a href="#Waveform-Design-for-MIMO-OFDM-Integrated-Sensing-and-Communication-System-An-Information-Theoretical-Approach" class="headerlink" title="Waveform Design for MIMO-OFDM Integrated Sensing and Communication System: An Information Theoretical Approach"></a>Waveform Design for MIMO-OFDM Integrated Sensing and Communication System: An Information Theoretical Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05444">http://arxiv.org/abs/2310.05444</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiqing Wei, Jinghui Piao, Xin Yuan, Huici Wu, J. Andrew Zhang, Zhiyong Feng, Lin Wang, Ping Zhang</li>
<li>for: 这篇论文主要探讨了integration sensing and communication（ISAC）系统中波形设计的问题，以及在5G-A和6G移动通信系统中ISAC技术的应用。</li>
<li>methods: 本论文使用了信息论中的统一性能指标，即相互信息（MI），来度量多普逻盘ISAC系统中的感知和通信性能。然后，提出了最优波形设计方案，以最大化感知MI、通信MI和权衡感知和通信MI的加权和。</li>
<li>results: 优化结果通过Monte Carlo伪陷 simulations进行验证。本研究提供了有效的封闭式表达式，使得MIMO-OFDM ISAC系统能够实现平衡的感知和通信性能。<details>
<summary>Abstract</summary>
Integrated sensing and communication (ISAC) is regarded as the enabling technology in the future 5th-Generation-Advanced (5G-A) and 6th-Generation (6G) mobile communication system. ISAC waveform design is critical in ISAC system. However, the difference of the performance metrics between sensing and communication brings challenges for the ISAC waveform design. This paper applies the unified performance metrics in information theory, namely mutual information (MI), to measure the communication and sensing performance in multicarrier ISAC system. In multi-input multi-output orthogonal frequency division multiplexing (MIMO-OFDM) ISAC system, we first derive the sensing and communication MI with subcarrier correlation and spatial correlation. Then, we propose optimal waveform designs for maximizing the sensing MI, communication MI and the weighted sum of sensing and communication MI, respectively. The optimization results are validated by Monte Carlo simulations. Our work provides effective closed-form expressions for waveform design, enabling the realization of MIMO-OFDM ISAC system with balanced performance in communication and sensing.
</details>
<details>
<summary>摘要</summary>
Integrated sensing and communication (ISAC) 被视为未来 fifth-generation advanced (5G-A) 和 sixth-generation (6G) 移动通信系统的关键技术。 ISAC 波形设计是 ISAC 系统的关键。然而，传感和通信性能的不同会对 ISAC 波形设计带来挑战。本文使用信息理论中的共聚性指标（MI）来度量传感和通信性能。在多个输入多个输出的orthogonal frequency division multiplexing (MIMO-OFDM) ISAC 系统中，我们首先计算传感和通信 MI 的相互关系。然后，我们提出了最佳波形设计，以最大化传感 MI、通信 MI 和权重总和传感和通信 MI。我们的工作提供了有效的关闭式表达式，使得 MIMO-OFDM ISAC 系统可以实现平衡的传感和通信性能。Note: Please note that the translation is in Simplified Chinese, and the word order and grammar may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="A-Stochastic-Particle-Variational-Bayesian-Inference-Inspired-Deep-Unfolding-Network-for-Non-Convex-Parameter-Estimation"><a href="#A-Stochastic-Particle-Variational-Bayesian-Inference-Inspired-Deep-Unfolding-Network-for-Non-Convex-Parameter-Estimation" class="headerlink" title="A Stochastic Particle Variational Bayesian Inference Inspired Deep-Unfolding Network for Non-Convex Parameter Estimation"></a>A Stochastic Particle Variational Bayesian Inference Inspired Deep-Unfolding Network for Non-Convex Parameter Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05382">http://arxiv.org/abs/2310.05382</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhixiang Hu, An Liu, Minjian Zhao</li>
<li>for: 这个研究旨在提供一个高维度非对称参数估计的方法，以应对未来无线网络中的普遍感知服务需求。</li>
<li>methods: 本研究提出了一个平行数位粒子统计量 bayesian inference（PSPVBI）算法，并将其融合到深度 unfolding 网络中（DU），以提高算法的速度和精度。</li>
<li>results: 实验结果显示，LPSPVBI 算法在无线感知应用中的参数估计比现有方法高精度。<details>
<summary>Abstract</summary>
Future wireless networks are envisioned to provide ubiquitous sensing services, which also gives rise to a substantial demand for high-dimensional non-convex parameter estimation, i.e., the associated likelihood function is non-convex and contains numerous local optima. Variational Bayesian inference (VBI) provides a powerful tool for modeling complex estimation problems and reasoning with prior information, but poses a long-standing challenge on computing intractable posteriori distributions. Most existing variational methods generally rely on assumptions about specific distribution families to derive closed-form solutions, and are difficult to apply in high-dimensional, non-convex scenarios. Given these challenges, firstly, we propose a parallel stochastic particle variational Bayesian inference (PSPVBI) algorithm. Thanks to innovations such as particle approximation, additional updates of particle positions, and parallel stochastic successive convex approximation (PSSCA), PSPVBI can flexibly drive particles to fit the posteriori distribution with acceptable complexity, yielding high-precision estimates of the target parameters. Furthermore, additional speedup can be obtained by deep-unfolding (DU) the PSPVBI algorithm. Specifically, superior hyperparameters are learned to dramatically reduce the number of algorithmic iterations. In this PSPVBI-induced Deep-Unfolding Networks, some techniques related to gradient computation, data sub-sampling, differentiable sampling, and generalization ability are also employed to facilitate the practical deployment. Finally, we apply the LPSPVBI to solve several important parameter estimation problems in wireless sensing scenarios. Simulations indicate that the LPSPVBI algorithm outperforms existing solutions.
</details>
<details>
<summary>摘要</summary>
将来的无线网络将提供 ubique 感知服务，导致高维非拟合参数估计的巨大需求，即相关的可能函数是非拟合的和含有多个局部最优点。基本 Bayesian 推理 (VB) 提供了模拟复杂估计问题和使用先验信息进行理据处理的强大工具，但计算不可靠的后验分布却成为了长期挑战。大多数现有的变量方法通常假设特定的分布家族，从而得到关闭式解决方案，而在高维、非拟合情况下困难应用。为解决这些挑战，我们首先提出了并行随机粒子变量 Bayesian 推理（PSPVBI）算法。因为增加了粒子方法、粒子位置更新和并行随机Successive Convex Approximation（PSSCA）等创新，PSPVBI可以灵活地使粒子适应 posteriori 分布，得到高精度的参数估计。此外，通过深度 unfolding（DU）的技术，我们可以进一步提高算法的速度。具体来说，我们通过学习超过参数，减少算法迭代数量，实现了在 PSPVBI 中的深度 unfolding。在 PSPVBI induced Deep-Unfolding Networks 中，我们还使用了一些相关的梯度计算、数据子抽样、可导采样和通用能力等技术，以便实际应用。最后，我们通过 LPSPVBI 算法解决了无线感知场景中的一些重要参数估计问题。 simulation 结果表明，LPSPVBI 算法在现有解决方案中具有优势。
</details></li>
</ul>
<hr>
<h2 id="Distortion-Aware-Phase-Retrieval-Receiver-for-High-Order-QAM-Transmission-with-Carrierless-Intensity-Only-Measurements"><a href="#Distortion-Aware-Phase-Retrieval-Receiver-for-High-Order-QAM-Transmission-with-Carrierless-Intensity-Only-Measurements" class="headerlink" title="Distortion-Aware Phase Retrieval Receiver for High-Order QAM Transmission with Carrierless Intensity-Only Measurements"></a>Distortion-Aware Phase Retrieval Receiver for High-Order QAM Transmission with Carrierless Intensity-Only Measurements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05314">http://arxiv.org/abs/2310.05314</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanzi Huang, Haoshuo Chen, Qi Gao, Yetian Huang, Nicolas K. Fontaine, Mikael Mazur, Lauren Dallachiesa, Roland Ryf, Zhengxuan Li, Yingxiong Song</li>
<li>for:  investigate high-order quadrature amplitude modulation (QAM) signals transmission with carrierless and intensity-only measurements, and improve precision of phase retrieval (PR) algorithm.</li>
<li>methods:  propose distortion-aware PR scheme with training and reconstruction stages, estimate and emulate distortion caused by channel impairments, improve agreement between estimated and measured amplitudes.</li>
<li>results:  experimentally demonstrate 50-GBaud 16QAM and 32QAM signals transmission over 40km and 80km SSMF spans, achieve BERs below 6.25% HD-FEC and 25% SD-FEC thresholds, and achieve post-FEC data rate of up to 140 Gb&#x2F;s with optimal pilot symbol ratio of 20%.<details>
<summary>Abstract</summary>
We experimentally investigate transmitting high-order quadrature amplitude modulation (QAM) signals with carrierless and intensity-only measurements with phase retrieval (PR) receiving techniques. The intensity errors during measurement, including noise and distortions, are found to be a limiting factor for the precise convergence of the PR algorithm. To improve the PR reconstruction accuracy, we propose a distortion-aware PR scheme comprising both training and reconstruction stages. By estimating and emulating the distortion caused by various channel impairments, the proposed scheme enables enhanced agreement between the estimated and measured amplitudes throughout the PR iteration, thus resulting in improved reconstruction performance to support high-order QAM transmission. With the aid of proposed techniques, we experimentally demonstrate 50-GBaud 16QAM and 32QAM signals transmitting through a standard single-mode optical fiber (SSMF) span of 40 and 80 km, and achieve bit error rates (BERs) below the 6.25% hard decision (HD)-forward error correction (FEC) and 25% soft decision (SD)-FEC thresholds for the two modulation formats, respectively. By tuning the pilot symbol ratio and applying concatenated coding, we also demonstrate that a post-FEC data rate of up to 140 Gb/s can be achieved for both distances at an optimal pilot symbol ratio of 20%.
</details>
<details>
<summary>摘要</summary>
我们实验性地研究了在无载波和强度仅测量下传输高阶 quadrature amplitude modulation（QAM）信号。测量过程中的强度错误，包括噪声和扭曲，被发现是精度恢复 алгоритм的限制因素。为了提高恢复精度，我们提议了一种考虑到频率响应的扭曲恢复方案，包括训练和重建两个阶段。通过估算和模拟各种通道缺陷所引起的扭曲，该方案可以在PR迭代过程中实现更好的吻合，从而提高恢复性能，以支持高阶QAM传输。通过我们的技术，我们实验性地在标准单模光纤（SSMF） span 40和80公里上传输了50Gbps 16QAM和32QAM信号，并在这两种模ulation format中达到了Below the 6.25% hard decision（HD）forward error correction（FEC）和25% soft decision（SD）FEC的下限。通过调整示例符号比例和 concatenated coding，我们还示出了在这两个距离上达到140Gb/s的后FEC数据速率。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/09/eess.SP_2023_10_09/" data-id="clp869u9f01fjk5881ex8bjsj" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_10_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/08/cs.SD_2023_10_08/" class="article-date">
  <time datetime="2023-10-08T15:00:00.000Z" itemprop="datePublished">2023-10-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/08/cs.SD_2023_10_08/">cs.SD - 2023-10-08</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="VITS-based-Singing-Voice-Conversion-System-with-DSPGAN-post-processing-for-SVCC2023"><a href="#VITS-based-Singing-Voice-Conversion-System-with-DSPGAN-post-processing-for-SVCC2023" class="headerlink" title="VITS-based Singing Voice Conversion System with DSPGAN post-processing for SVCC2023"></a>VITS-based Singing Voice Conversion System with DSPGAN post-processing for SVCC2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05118">http://arxiv.org/abs/2310.05118</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiquan Zhou, Meng Chen, Yi Lei, Jihua Zhu, Weifeng Zhao</li>
<li>for: 这项研究的目的是为SVCC2023提供一个系统，以便在 singing voice conversion 领域实现高质量的音频转换。</li>
<li>methods: 该系统包括三个模块：特征提取器、声音转换器和后处理器。特征提取器使用 HuBERT 模型提取 singing voice 中的 F0 轨迹和 speaker-independent 语言内容。声音转换器使用 target speaker 的声音特征、F0 和语言内容来生成目标speaker 的波形。此外，为了进一步提高音质，我们还使用了一个精度调整的 DSPGAN  vocoder。</li>
<li>results: 在 official challenge 结果中，我们的系统在 cross-domain 任务中表现出色，得分第1和第2位，分别在自然性和相似性两个指标上。此外，我们还进行了一些缓解分析，以证明我们的系统设计的有效性。<details>
<summary>Abstract</summary>
This paper presents the T02 team's system for the Singing Voice Conversion Challenge 2023 (SVCC2023). Our system entails a VITS-based SVC model, incorporating three modules: a feature extractor, a voice converter, and a post-processor. Specifically, the feature extractor provides F0 contours and extracts speaker-independent linguistic content from the input singing voice by leveraging a HuBERT model. The voice converter is employed to recompose the speaker timbre, F0, and linguistic content to generate the waveform of the target speaker. Besides, to further improve the audio quality, a fine-tuned DSPGAN vocoder is introduced to re-synthesise the waveform. Given the limited target speaker data, we utilize a two-stage training strategy to adapt the base model to the target speaker. During model adaptation, several tricks, such as data augmentation and joint training with auxiliary singer data, are involved. Official challenge results show that our system achieves superior performance, especially in the cross-domain task, ranking 1st and 2nd in naturalness and similarity, respectively. Further ablation justifies the effectiveness of our system design.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Partial-Rank-Similarity-Minimization-Method-for-Quality-MOS-Prediction-of-Unseen-Speech-Synthesis-Systems-in-Zero-Shot-and-Semi-supervised-setting"><a href="#Partial-Rank-Similarity-Minimization-Method-for-Quality-MOS-Prediction-of-Unseen-Speech-Synthesis-Systems-in-Zero-Shot-and-Semi-supervised-setting" class="headerlink" title="Partial Rank Similarity Minimization Method for Quality MOS Prediction of Unseen Speech Synthesis Systems in Zero-Shot and Semi-supervised setting"></a>Partial Rank Similarity Minimization Method for Quality MOS Prediction of Unseen Speech Synthesis Systems in Zero-Shot and Semi-supervised setting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05078">http://arxiv.org/abs/2310.05078</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nii-yamagishilab/partial_rank_similarity">https://github.com/nii-yamagishilab/partial_rank_similarity</a></li>
<li>paper_authors: Hemant Yadav, Erica Cooper, Junichi Yamagishi, Sunayana Sitaram, Rajiv Ratn Shah</li>
<li>for: 这项研究旨在提出一种新的质量 mean opinion score（MOS）预测函数，用于评估未经见过的语音合成系统的质量。</li>
<li>methods: 该函数measure相对位置的相似性，而不是实际的MOS值，通过测量partial rank similarity（PRS）而不是L1损失函数。</li>
<li>results: 实验表明，PRS在零shot和半supervised设定下表现出色，与真实值更高度相关，而MSE和linear correlation coefficient metric可能不适用于评估MOS预测模型。<details>
<summary>Abstract</summary>
This paper introduces a novel objective function for quality mean opinion score (MOS) prediction of unseen speech synthesis systems. The proposed function measures the similarity of relative positions of predicted MOS values, in a mini-batch, rather than the actual MOS values. That is the partial rank similarity is measured (PRS) rather than the individual MOS values as with the L1 loss. Our experiments on out-of-domain speech synthesis systems demonstrate that the PRS outperforms L1 loss in zero-shot and semi-supervised settings, exhibiting stronger correlation with ground truth. These findings highlight the importance of considering rank order, as done by PRS, when training MOS prediction models. We also argue that mean squared error and linear correlation coefficient metrics may be unreliable for evaluating MOS prediction models. In conclusion, PRS-trained models provide a robust framework for evaluating speech quality and offer insights for developing high-quality speech synthesis systems. Code and models are available at github.com/nii-yamagishilab/partial_rank_similarity/
</details>
<details>
<summary>摘要</summary>
这份论文介绍了一种新的评价函数，用于预测未看过的语音合成系统的质量 mean opinion score（MOS）。提出的函数测量在一个小批次中预测的MOS值相对位置的相似性，而不是实际的MOS值。即使使用了partial rank similarity（PRS）而不是L1损失，我们的实验表明，PRS在零批次和半指导学习 Setting 中表现更好，与基准数据 exhibit stronger correlation。这些发现反映了考虑 rank order 的重要性，当训练 MOS 预测模型时。我们还认为 mean squared error 和 linear correlation coefficient  metrics 可能不可靠地评价 MOS 预测模型。 conclusion，PRS 训练的模型提供了一种robust的 speech quality 评价框架，并且为开发高质量语音合成系统提供了意见。代码和模型可以在github.com/nii-yamagishilab/partial_rank_similarity/ 找到。
</details></li>
</ul>
<hr>
<h2 id="SALT-Distinguishable-Speaker-Anonymization-Through-Latent-Space-Transformation"><a href="#SALT-Distinguishable-Speaker-Anonymization-Through-Latent-Space-Transformation" class="headerlink" title="SALT: Distinguishable Speaker Anonymization Through Latent Space Transformation"></a>SALT: Distinguishable Speaker Anonymization Through Latent Space Transformation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05051">http://arxiv.org/abs/2310.05051</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bakerbunker/salt">https://github.com/bakerbunker/salt</a></li>
<li>paper_authors: Yuanjun Lv, Jixun Yao, Peikun Chen, Hongbin Zhou, Heng Lu, Lei Xie</li>
<li>for: 隐藏发音人的身份，保持语音质量和可理解性。</li>
<li>methods: 基于隐藏空间转换的发音人匿名系统（SALT），包括自主学习特征提取器和随机抽取多个发音人和其权重，并通过 interpolate 实现发音人匿名。同时，我们还 explore 了扩展方法以提高假发音人的多样性。</li>
<li>results: 在 Voice Privacy Challenge 数据集上，我们的系统实现了最佳的匿名度指标，同时保持语音质量和可理解性。<details>
<summary>Abstract</summary>
Speaker anonymization aims to conceal a speaker's identity without degrading speech quality and intelligibility. Most speaker anonymization systems disentangle the speaker representation from the original speech and achieve anonymization by averaging or modifying the speaker representation. However, the anonymized speech is subject to reduction in pseudo speaker distinctiveness, speech quality and intelligibility for out-of-distribution speaker. To solve this issue, we propose SALT, a Speaker Anonymization system based on Latent space Transformation. Specifically, we extract latent features by a self-supervised feature extractor and randomly sample multiple speakers and their weights, and then interpolate the latent vectors to achieve speaker anonymization. Meanwhile, we explore the extrapolation method to further extend the diversity of pseudo speakers. Experiments on Voice Privacy Challenge dataset show our system achieves a state-of-the-art distinctiveness metric while preserving speech quality and intelligibility. Our code and demo is availible at https://github.com/BakerBunker/SALT .
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="PromptSpeaker-Speaker-Generation-Based-on-Text-Descriptions"><a href="#PromptSpeaker-Speaker-Generation-Based-on-Text-Descriptions" class="headerlink" title="PromptSpeaker: Speaker Generation Based on Text Descriptions"></a>PromptSpeaker: Speaker Generation Based on Text Descriptions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05001">http://arxiv.org/abs/2310.05001</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongmao Zhang, Guanghou Liu, Yi Lei, Yunlin Chen, Hao Yin, Lei Xie, Zhifei Li</li>
<li>for: 这项研究旨在实现文本描述基于的发音人生成（text-guided speaker generation），即通过文本描述控制发音人生成过程。</li>
<li>methods: 该研究提出了一种名为PromptSpeaker的文本指导发音人生成系统，该系统包括提取器、零批量VITS和Glow模型。提取器预测基于文本描述的含义表示，并从这个分布中采样以获取 semantic representation。Glow模型将含义表示转换成发音人表示，而零批量VITS最后将发音人表示转换成真实的发音。</li>
<li>results: 研究证明PromptSpeaker可以生成与训练集外的新发音人，并且synthetic speaker voice具有相对合理的主观匹配质量。<details>
<summary>Abstract</summary>
Recently, text-guided content generation has received extensive attention. In this work, we explore the possibility of text description-based speaker generation, i.e., using text prompts to control the speaker generation process. Specifically, we propose PromptSpeaker, a text-guided speaker generation system. PromptSpeaker consists of a prompt encoder, a zero-shot VITS, and a Glow model, where the prompt encoder predicts a prior distribution based on the text description and samples from this distribution to obtain a semantic representation. The Glow model subsequently converts the semantic representation into a speaker representation, and the zero-shot VITS finally synthesizes the speaker's voice based on the speaker representation. We verify that PromptSpeaker can generate speakers new from the training set by objective metrics, and the synthetic speaker voice has reasonable subjective matching quality with the speaker prompt.
</details>
<details>
<summary>摘要</summary>
近些时间，文本指导内容生成已经受到了广泛关注。在这项工作中，我们探索了使用文本描述来控制发音生成过程的可能性。具体来说，我们提出了PromptSpeaker，一种文本指导的发音生成系统。PromptSpeaker包括一个描述符编码器、一个零拟合VITS和一个Glow模型，其中描述符编码器根据文本描述预测一个优先分布，并从这个分布中采样以获取一个semantic表示。Glow模型然后将semantic表示转化为发音表示，零拟合VITS最后将发音表示转化为声音。我们证明了PromptSpeaker可以新生成不同于训练集的发音，并且synthetic声音具有合理的主观匹配质量与发音描述。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/08/cs.SD_2023_10_08/" data-id="clp869u3g010ek5889svpff3w" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_10_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/08/cs.CV_2023_10_08/" class="article-date">
  <time datetime="2023-10-08T13:00:00.000Z" itemprop="datePublished">2023-10-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/08/cs.CV_2023_10_08/">cs.CV - 2023-10-08</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Progressive-Neural-Compression-for-Adaptive-Image-Offloading-under-Timing-Constraints"><a href="#Progressive-Neural-Compression-for-Adaptive-Image-Offloading-under-Timing-Constraints" class="headerlink" title="Progressive Neural Compression for Adaptive Image Offloading under Timing Constraints"></a>Progressive Neural Compression for Adaptive Image Offloading under Timing Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05306">http://arxiv.org/abs/2310.05306</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rickywrq/Progressive-Neural-Compression">https://github.com/rickywrq/Progressive-Neural-Compression</a></li>
<li>paper_authors: Ruiqi Wang, Hanyang Liu, Jiaming Qiu, Moran Xu, Roch Guerin, Chenyang Lu</li>
<li>for: 这篇论文旨在提出一种适应性的进步神经压缩方法，以提高机器学习应用程序在边缘服务器上的推论性能，并且在网络带宽不稳定的情况下进行图像卸载。</li>
<li>methods: 本篇论文使用了进步神经压缩（PNC）方法，并使用了多目标减少自适应器来训练对应的图像压缩策略，以便根据可用带宽进行图像卸载。</li>
<li>results: 相比于现有的神经压缩方法和传统压缩方法，PNC方法可以提高机器学习应用程序的推论性能，并且可以适应网络带宽不稳定的情况。<details>
<summary>Abstract</summary>
IoT devices are increasingly the source of data for machine learning (ML) applications running on edge servers. Data transmissions from devices to servers are often over local wireless networks whose bandwidth is not just limited but, more importantly, variable. Furthermore, in cyber-physical systems interacting with the physical environment, image offloading is also commonly subject to timing constraints. It is, therefore, important to develop an adaptive approach that maximizes the inference performance of ML applications under timing constraints and the resource constraints of IoT devices. In this paper, we use image classification as our target application and propose progressive neural compression (PNC) as an efficient solution to this problem. Although neural compression has been used to compress images for different ML applications, existing solutions often produce fixed-size outputs that are unsuitable for timing-constrained offloading over variable bandwidth. To address this limitation, we train a multi-objective rateless autoencoder that optimizes for multiple compression rates via stochastic taildrop to create a compression solution that produces features ordered according to their importance to inference performance. Features are then transmitted in that order based on available bandwidth, with classification ultimately performed using the (sub)set of features received by the deadline. We demonstrate the benefits of PNC over state-of-the-art neural compression approaches and traditional compression methods on a testbed comprising an IoT device and an edge server connected over a wireless network with varying bandwidth.
</details>
<details>
<summary>摘要</summary>
互联网物品（IoT）正在日益成为机器学习（ML）应用程序的数据来源。从设备到服务器的数据传输通常是通过本地无线网络，带宽是有限的，更重要的是可变的。在融合物理环境的应用中，图像传输也会受到时间限制。因此，发展一个适应的方法可以在时间限制和互联网设备的资源限制下，提高机器学习应用的推论性能。在这篇文章中，我们使用图像分类作为我们的目标应用，并提出进步的神经压缩（PNC）方法来解决这个问题。尽管神经压缩已经用于不同的机器学习应用中图像压缩，但现有的解决方案通常会生成固定大小的出力，这不适合时间限制下的图像传输。为解决这个限制，我们训练了多个目标自适应的减少顶峰网络，以便在不同的压缩率下选择适合的压缩率，并生成按照推论性能的重要性排序的特征。这些特征会根据可用带宽进行传输，并最终通过在时间限制下接收的（子）集特征进行分类。我们在一个包括IoT设备和边缘服务器之间的无线网络上进行了实验，评估PNC方法与现有的神经压缩方法和传统压缩方法的比较。
</details></li>
</ul>
<hr>
<h2 id="GestSync-Determining-who-is-speaking-without-a-talking-head"><a href="#GestSync-Determining-who-is-speaking-without-a-talking-head" class="headerlink" title="GestSync: Determining who is speaking without a talking head"></a>GestSync: Determining who is speaking without a talking head</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05304">http://arxiv.org/abs/2310.05304</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Sindhu-Hegde/gestsync">https://github.com/Sindhu-Hegde/gestsync</a></li>
<li>paper_authors: Sindhu B Hegde, Andrew Zisserman</li>
<li>For: The paper is written for determining if a person’s gestures are correlated with their speech or not, and exploring the use of self-supervised learning for this task.* Methods: The paper introduces a dual-encoder model for the task of Gesture-Sync, and compares the performance of different input representations, including RGB frames, keypoint images, and keypoint vectors.* Results: The paper shows that the model can be trained using self-supervised learning alone, and evaluates its performance on the LRS3 dataset. Additionally, the paper demonstrates applications of Gesture-Sync for audio-visual synchronisation and determining who is the speaker in a crowd without seeing their faces.<details>
<summary>Abstract</summary>
In this paper we introduce a new synchronisation task, Gesture-Sync: determining if a person's gestures are correlated with their speech or not. In comparison to Lip-Sync, Gesture-Sync is far more challenging as there is a far looser relationship between the voice and body movement than there is between voice and lip motion. We introduce a dual-encoder model for this task, and compare a number of input representations including RGB frames, keypoint images, and keypoint vectors, assessing their performance and advantages. We show that the model can be trained using self-supervised learning alone, and evaluate its performance on the LRS3 dataset. Finally, we demonstrate applications of Gesture-Sync for audio-visual synchronisation, and in determining who is the speaker in a crowd, without seeing their faces. The code, datasets and pre-trained models can be found at: \url{https://www.robots.ox.ac.uk/~vgg/research/gestsync}.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们引入了一个新的同步任务，即Gesture-Sync：判断一个人的手势与其说话是否相关。与Lip-Sync相比，Gesture-Sync更加具有挑战性，因为voice和身体运动之间的关系比lip motion和语音之间的关系更加松散。我们介绍了一种双encoder模型来解决这个任务，并比较了不同的输入表示，包括RGB帧、关键点图像和关键点向量，评估其性能和优势。我们表明该模型可以通过自我超视了学习来训练，并评估其性能在LRS3 dataset上。最后，我们展示了Gesture-Sync在audio-visual同步和推测人声的场景中的应用，以及不看到人脸时推测说话人的场景。相关代码、数据集和预训练模型可以在以下链接中找到：\url{https://www.robots.ox.ac.uk/~vgg/research/gestsync}.
</details></li>
</ul>
<hr>
<h2 id="Image-Compression-and-Decompression-Framework-Based-on-Latent-Diffusion-Model-for-Breast-Mammography"><a href="#Image-Compression-and-Decompression-Framework-Based-on-Latent-Diffusion-Model-for-Breast-Mammography" class="headerlink" title="Image Compression and Decompression Framework Based on Latent Diffusion Model for Breast Mammography"></a>Image Compression and Decompression Framework Based on Latent Diffusion Model for Breast Mammography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05299">http://arxiv.org/abs/2310.05299</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/neogeoss/EMBED_Mammo_Models">https://github.com/neogeoss/EMBED_Mammo_Models</a></li>
<li>paper_authors: InChan Hwang, MinJae Woo</li>
<li>for: 这个研究旨在开发一个新的医疗图像压缩和解压缩框架，利用隐藏扩散模型（LDM）。LDM比过去的浊度扩散概率模型（DDPM）有更好的图像质量和较少的计算资源，在图像解压缩过程中。</li>
<li>methods: 这个研究使用了隐藏扩散模型（LDM）和Torchvision进行图像扩大，并考虑了医疗图像数据的应用。</li>
<li>results: 实验结果显示，这种方法比传统文件压缩算法更好，并且训练使用解压缩档案的 convolutional neural network（CNN）模型与使用原始图像档案训练的模型相比，表现相似。此外，这种方法还可以将医疗图像数据压缩到较小的大小，以便在医疗设备中储存。研究的影响范围包括医疗图像压缩中的噪声reduction和替代复杂波纹基于损失less压缩算法。<details>
<summary>Abstract</summary>
This research presents a novel framework for the compression and decompression of medical images utilizing the Latent Diffusion Model (LDM). The LDM represents advancement over the denoising diffusion probabilistic model (DDPM) with a potential to yield superior image quality while requiring fewer computational resources in the image decompression process. A possible application of LDM and Torchvision for image upscaling has been explored using medical image data, serving as an alternative to traditional image compression and decompression algorithms. The experimental outcomes demonstrate that this approach surpasses a conventional file compression algorithm, and convolutional neural network (CNN) models trained with decompressed files perform comparably to those trained with original image files. This approach also significantly reduces dataset size so that it can be distributed with a smaller size, and medical images take up much less space in medical devices. The research implications extend to noise reduction in lossy compression algorithms and substitute for complex wavelet-based lossless algorithms.
</details>
<details>
<summary>摘要</summary>
这项研究提出了一种新的压缩和解压缩医疗图像框架，利用Latent Diffusion Model（LDM）。LDM比denoising diffusion probabilistic model（DDPM）有更多的进步，可以提供更高质量的图像，同时需要更少的计算资源进行图像解压缩。研究人员还探索了使用LDM和Torchvision进行图像缩放，作为传统图像压缩和解压缩算法的替代方案。实验结果表明，这种方法比传统文件压缩算法更好，并且使用解压缩文件训练的 convolutional neural network（CNN）模型与原始图像文件训练的模型性能相似。此外，这种方法还可以减少数据集大小，使得它可以更加容易地分布，医疗设备中的医疗图像占用的空间也更加小。研究的影响扩展到了lossy压缩算法中的噪声减少和lossless压缩算法中的复杂波лет特征。
</details></li>
</ul>
<hr>
<h2 id="MSight-An-Edge-Cloud-Infrastructure-based-Perception-System-for-Connected-Automated-Vehicles"><a href="#MSight-An-Edge-Cloud-Infrastructure-based-Perception-System-for-Connected-Automated-Vehicles" class="headerlink" title="MSight: An Edge-Cloud Infrastructure-based Perception System for Connected Automated Vehicles"></a>MSight: An Edge-Cloud Infrastructure-based Perception System for Connected Automated Vehicles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05290">http://arxiv.org/abs/2310.05290</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rusheng Zhang, Depu Meng, Shengyin Shen, Zhengxia Zou, Houqiang Li, Henry X. Liu</li>
<li>for: 这篇论文是为了探讨Connected Automated Vehicle（CAV）应用中的道路边缘感知技术。</li>
<li>methods: 这篇论文使用了路侧感知系统MSight，实现了实时车辆检测、定位、追踪和短期路径预测。</li>
<li>results: 评估结果显示MSight系统能够维持车道精度，并且具有几乎 zero latency，这表明了这个系统在CAV安全性和效率方面的应用潜力。<details>
<summary>Abstract</summary>
As vehicular communication and networking technologies continue to advance, infrastructure-based roadside perception emerges as a pivotal tool for connected automated vehicle (CAV) applications. Due to their elevated positioning, roadside sensors, including cameras and lidars, often enjoy unobstructed views with diminished object occlusion. This provides them a distinct advantage over onboard perception, enabling more robust and accurate detection of road objects. This paper presents MSight, a cutting-edge roadside perception system specifically designed for CAVs. MSight offers real-time vehicle detection, localization, tracking, and short-term trajectory prediction. Evaluations underscore the system's capability to uphold lane-level accuracy with minimal latency, revealing a range of potential applications to enhance CAV safety and efficiency. Presently, MSight operates 24/7 at a two-lane roundabout in the City of Ann Arbor, Michigan.
</details>
<details>
<summary>摘要</summary>
“自动驾驶汽车（CAV）技术继续发展，基础设施上的路面感知emerges as a pivotal tool。由于路面感知器的高位置，包括摄像头和激光测距仪，通常有较好的视野和降低物体遮挡，这提供了它们与车辆上的感知相比，更加稳定和准确地检测道路上的物体。本文介绍了MSight，一个特有的路面感知系统，专门设计供CAV使用。MSight在实时提供车辆检测、定位、追踪和短期预测。评估结果显示MSight可以保持车道精度，并具有最小延迟。这些应用可能增加CAV的安全和效率。目前，MSight在美国密歇根州安那堤市的一个二轮圆环上运行24小时。”
</details></li>
</ul>
<hr>
<h2 id="The-Emergence-of-Reproducibility-and-Consistency-in-Diffusion-Models"><a href="#The-Emergence-of-Reproducibility-and-Consistency-in-Diffusion-Models" class="headerlink" title="The Emergence of Reproducibility and Consistency in Diffusion Models"></a>The Emergence of Reproducibility and Consistency in Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05264">http://arxiv.org/abs/2310.05264</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huijie Zhang, Jinfan Zhou, Yifu Lu, Minzhe Guo, Liyue Shen, Qing Qu</li>
<li>for: 这个论文的目的是探索Diffusion模型中的一种常见现象，即“一致性模型重复性”。</li>
<li>methods: 作者采用了大量实验和分析方法，包括使用不同的模型架构和训练策略，来研究Diffusion模型的一致性模型重复性。</li>
<li>results: 研究发现，Diffusion模型在不同的训练 regime中都具有一定的一致性模型重复性，其中包括“记忆化 режим”和“泛化 режим”。此外，作者还发现这种一致性模型重复性可以在多种Diffusion模型的变种中找到，例如Conditional Diffusion模型、用于解决反向问题的Diffusion模型以及精度调整后的Diffusion模型。<details>
<summary>Abstract</summary>
Recently, diffusion models have emerged as powerful deep generative models, showcasing cutting-edge performance across various applications such as image generation, solving inverse problems, and text-to-image synthesis. These models generate new data (e.g., images) by transforming random noise inputs through a reverse diffusion process. In this work, we uncover a distinct and prevalent phenomenon within diffusion models in contrast to most other generative models, which we refer to as ``consistent model reproducibility''. To elaborate, our extensive experiments have consistently shown that when starting with the same initial noise input and sampling with a deterministic solver, diffusion models tend to produce nearly identical output content. This consistency holds true regardless of the choices of model architectures and training procedures. Additionally, our research has unveiled that this exceptional model reproducibility manifests in two distinct training regimes: (i) ``memorization regime,'' characterized by a significantly overparameterized model which attains reproducibility mainly by memorizing the training data; (ii) ``generalization regime,'' in which the model is trained on an extensive dataset, and its reproducibility emerges with the model's generalization capabilities. Our analysis provides theoretical justification for the model reproducibility in ``memorization regime''. Moreover, our research reveals that this valuable property generalizes to many variants of diffusion models, including conditional diffusion models, diffusion models for solving inverse problems, and fine-tuned diffusion models. A deeper understanding of this phenomenon has the potential to yield more interpretable and controllable data generative processes based on diffusion models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Structure-Preserving-Instance-Segmentation-via-Skeleton-Aware-Distance-Transform"><a href="#Structure-Preserving-Instance-Segmentation-via-Skeleton-Aware-Distance-Transform" class="headerlink" title="Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform"></a>Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05262">http://arxiv.org/abs/2310.05262</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zudi Lin, Donglai Wei, Aarush Gupta, Xingyu Liu, Deqing Sun, Hanspeter Pfister</li>
<li>for:  Histopathology image segmentation</li>
<li>methods:  Skeleton-aware distance transform (SDT) combining object skeleton and distance transform</li>
<li>results:  State-of-the-art performance in histopathology image segmentation<details>
<summary>Abstract</summary>
Objects with complex structures pose significant challenges to existing instance segmentation methods that rely on boundary or affinity maps, which are vulnerable to small errors around contacting pixels that cause noticeable connectivity change. While the distance transform (DT) makes instance interiors and boundaries more distinguishable, it tends to overlook the intra-object connectivity for instances with varying width and result in over-segmentation. To address these challenges, we propose a skeleton-aware distance transform (SDT) that combines the merits of object skeleton in preserving connectivity and DT in modeling geometric arrangement to represent instances with arbitrary structures. Comprehensive experiments on histopathology image segmentation demonstrate that SDT achieves state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
对于具有复杂结构的对象，现有的实例分割方法，例如基于边界或相互关系图的方法，会面临 significiant 挑战。这些方法容易受到小范围内的像素错误所致的连接性变化的影响，从而导致分割不准确。而距离变换（DT）可以使实例的内部和边界更加可识别，但是它通常会忽略内部对象的连接性，导致不同宽度的实例进行过分割。为解决这些挑战，我们提议使用skeleton-aware distance transform（SDT），这种方法结合了对象骨架的优点，以保持连接性，并且利用距离变换来模型几何关系，以更好地表示具有自由结构的实例。我们对历史病理图像分割进行了广泛的实验，结果表明，SDT可以达到当前最佳性能。
</details></li>
</ul>
<hr>
<h2 id="SCANet-Scene-Complexity-Aware-Network-for-Weakly-Supervised-Video-Moment-Retrieval"><a href="#SCANet-Scene-Complexity-Aware-Network-for-Weakly-Supervised-Video-Moment-Retrieval" class="headerlink" title="SCANet: Scene Complexity Aware Network for Weakly-Supervised Video Moment Retrieval"></a>SCANet: Scene Complexity Aware Network for Weakly-Supervised Video Moment Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05241">http://arxiv.org/abs/2310.05241</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sunjae Yoon, Gwanhyeong Koo, Dahyun Kim, Chang D. Yoo</li>
<li>for: 本研究旨在提高视频oment Retrieval（VMR）系统的精度和效率，通过在多个视频中检索具有相同语言查询的时刻点。</li>
<li>methods: 本研究提出了一种新的Scene Complexity Aware Network（SCANet），该网络能够评估多个视频中场景的复杂性，并根据场景的复杂性生成适应性的提案。</li>
<li>results: 实验结果表明，使用SCANet网络可以在三个检索标准（Charades-STA、ActivityNet、TVR）上达到状态级性能，并且 demonstarted the effectiveness of incorporating scene complexity in VMR systems.<details>
<summary>Abstract</summary>
Video moment retrieval aims to localize moments in video corresponding to a given language query. To avoid the expensive cost of annotating the temporal moments, weakly-supervised VMR (wsVMR) systems have been studied. For such systems, generating a number of proposals as moment candidates and then selecting the most appropriate proposal has been a popular approach. These proposals are assumed to contain many distinguishable scenes in a video as candidates. However, existing proposals of wsVMR systems do not respect the varying numbers of scenes in each video, where the proposals are heuristically determined irrespective of the video. We argue that the retrieval system should be able to counter the complexities caused by varying numbers of scenes in each video. To this end, we present a novel concept of a retrieval system referred to as Scene Complexity Aware Network (SCANet), which measures the `scene complexity' of multiple scenes in each video and generates adaptive proposals responding to variable complexities of scenes in each video. Experimental results on three retrieval benchmarks (i.e., Charades-STA, ActivityNet, TVR) achieve state-of-the-art performances and demonstrate the effectiveness of incorporating the scene complexity.
</details>
<details>
<summary>摘要</summary>
视频瞬间检索目标是将视频中对应给给定语言查询的时刻点进行本地化。为了避免对时间点的标注成本昂贵，弱监督视频检索（wsVMR）系统得到了研究。这些系统通常采取生成多个提议作为时刻点候选者，然后选择最合适的提议。这些提议假设视频中包含许多可识别的场景。然而，现有的wsVMR系统提议不尊重每个视频中场景的数量，这些提议是基于视频的规则随意确定的。我们认为检索系统应该能够对每个视频中场景的复杂性进行应对。为此，我们提出了一种新的检索系统，即场景复杂度意识网络（SCANet），它在多个视频中场景的复杂性测量并生成适应场景复杂性的提议。实验结果在三个检索标准 benchmark（i.e., Charades-STA、ActivityNet、TVR）上达到了状态的最佳性能，并证明了包含场景复杂度的 incorporation 的效iveness。
</details></li>
</ul>
<hr>
<h2 id="Latent-Diffusion-Model-for-Medical-Image-Standardization-and-Enhancement"><a href="#Latent-Diffusion-Model-for-Medical-Image-Standardization-and-Enhancement" class="headerlink" title="Latent Diffusion Model for Medical Image Standardization and Enhancement"></a>Latent Diffusion Model for Medical Image Standardization and Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05237">http://arxiv.org/abs/2310.05237</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Selim, Jie Zhang, Faraneh Fathi, Michael A. Brooks, Ge Wang, Guoqiang Yu, Jin Chen</li>
<li>for: 这篇论文的目的是提出一个新的数据构造模型，以对 computed tomography (CT) 图像进行标准化，以提高医学研究中的可比性和精确性。</li>
<li>methods: 这篇论文使用了一个名为 DiffusionCT 的新型数据构造模型，该模型在 latent space 中将不同的 CT 图像转换为标准化的图像，以提高医学研究中的可比性和精确性。</li>
<li>results: 这篇论文的实验结果显示，DiffusionCT 可以对 CT 图像进行高品质的标准化，并且可以降低 SPAD 图像中的噪声，进一步验证了 DiffusionCT 的有效性。<details>
<summary>Abstract</summary>
Computed tomography (CT) serves as an effective tool for lung cancer screening, diagnosis, treatment, and prognosis, providing a rich source of features to quantify temporal and spatial tumor changes. Nonetheless, the diversity of CT scanners and customized acquisition protocols can introduce significant inconsistencies in texture features, even when assessing the same patient. This variability poses a fundamental challenge for subsequent research that relies on consistent image features. Existing CT image standardization models predominantly utilize GAN-based supervised or semi-supervised learning, but their performance remains limited. We present DiffusionCT, an innovative score-based DDPM model that operates in the latent space to transform disparate non-standard distributions into a standardized form. The architecture comprises a U-Net-based encoder-decoder, augmented by a DDPM model integrated at the bottleneck position. First, the encoder-decoder is trained independently, without embedding DDPM, to capture the latent representation of the input data. Second, the latent DDPM model is trained while keeping the encoder-decoder parameters fixed. Finally, the decoder uses the transformed latent representation to generate a standardized CT image, providing a more consistent basis for downstream analysis. Empirical tests on patient CT images indicate notable improvements in image standardization using DiffusionCT. Additionally, the model significantly reduces image noise in SPAD images, further validating the effectiveness of DiffusionCT for advanced imaging tasks.
</details>
<details>
<summary>摘要</summary>
computed tomography (CT) serve as an effective tool for lung cancer screening, diagnosis, treatment, and prognosis, providing a rich source of features to quantify temporal and spatial tumor changes. However, the diversity of CT scanners and customized acquisition protocols can introduce significant inconsistencies in texture features, even when assessing the same patient. This variability poses a fundamental challenge for subsequent research that relies on consistent image features. Existing CT image standardization models predominantly utilize GAN-based supervised or semi-supervised learning, but their performance remains limited. We present DiffusionCT, an innovative score-based DDPM model that operates in the latent space to transform disparate non-standard distributions into a standardized form. The architecture comprises a U-Net-based encoder-decoder, augmented by a DDPM model integrated at the bottleneck position. First, the encoder-decoder is trained independently, without embedding DDPM, to capture the latent representation of the input data. Second, the latent DDPM model is trained while keeping the encoder-decoder parameters fixed. Finally, the decoder uses the transformed latent representation to generate a standardized CT image, providing a more consistent basis for downstream analysis. Empirical tests on patient CT images indicate notable improvements in image standardization using DiffusionCT. Additionally, the model significantly reduces image noise in SPAD images, further validating the effectiveness of DiffusionCT for advanced imaging tasks.
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Cross-Dataset-Performance-of-Distracted-Driving-Detection-With-Score-Softmax-Classifier"><a href="#Enhancing-Cross-Dataset-Performance-of-Distracted-Driving-Detection-With-Score-Softmax-Classifier" class="headerlink" title="Enhancing Cross-Dataset Performance of Distracted Driving Detection With Score-Softmax Classifier"></a>Enhancing Cross-Dataset Performance of Distracted Driving Detection With Score-Softmax Classifier</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05202">http://arxiv.org/abs/2310.05202</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/congduan-hnu/ssoftmax">https://github.com/congduan-hnu/ssoftmax</a></li>
<li>paper_authors: Cong Duan, Zixuan Liu, Jiahao Xia, Minghai Zhang, Jiacai Liao, Libo Cao</li>
<li>for: 这个研究旨在提高车上司机的实时监控，以预测分心、疲劳和潜在危险。</li>
<li>methods: 我们引入了Score-Softmax分类器，以解决跨数据集短cut learning问题，并且利用人类评价模式设计了二维超级监管矩阵。</li>
<li>results: 我们的研究表明，Score-Softmax分类器可以提高跨数据集表现，并且比传统方法更好地结合多个数据集。<details>
<summary>Abstract</summary>
Deep neural networks enable real-time monitoring of in-vehicle driver, facilitating the timely prediction of distractions, fatigue, and potential hazards. This technology is now integral to intelligent transportation systems. Recent research has exposed unreliable cross-dataset end-to-end driver behavior recognition due to overfitting, often referred to as ``shortcut learning", resulting from limited data samples. In this paper, we introduce the Score-Softmax classifier, which addresses this issue by enhancing inter-class independence and Intra-class uncertainty. Motivated by human rating patterns, we designed a two-dimensional supervisory matrix based on marginal Gaussian distributions to train the classifier. Gaussian distributions help amplify intra-class uncertainty while ensuring the Score-Softmax classifier learns accurate knowledge. Furthermore, leveraging the summation of independent Gaussian distributed random variables, we introduced a multi-channel information fusion method. This strategy effectively resolves the multi-information fusion challenge for the Score-Softmax classifier. Concurrently, we substantiate the necessity of transfer learning and multi-dataset combination. We conducted cross-dataset experiments using the SFD, AUCDD-V1, and 100-Driver datasets, demonstrating that Score-Softmax improves cross-dataset performance without modifying the model architecture. This provides a new approach for enhancing neural network generalization. Additionally, our information fusion approach outperforms traditional methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Improving-Discriminative-Multi-Modal-Learning-with-Large-Scale-Pre-Trained-Models"><a href="#Improving-Discriminative-Multi-Modal-Learning-with-Large-Scale-Pre-Trained-Models" class="headerlink" title="Improving Discriminative Multi-Modal Learning with Large-Scale Pre-Trained Models"></a>Improving Discriminative Multi-Modal Learning with Large-Scale Pre-Trained Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05193">http://arxiv.org/abs/2310.05193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenzhuang Du, Yue Zhao, Chonghua Liao, Jiacheng You, Jie Fu, Hang Zhao</li>
<li>for: 这种研究旨在更好地利用大规模预训练的uni-modal模型，以提高多模态学习的表现。</li>
<li>methods: 这种方法使用预训练的uni-modal模型，并将其作为初始模型进行多模态联合训练，以增强模式之间的适应性。</li>
<li>results: 研究表明，这种方法可以提高多模态模型的总表现，特别是在一些任务中，even when fine-tuned with only uni-modal data。<details>
<summary>Abstract</summary>
This paper investigates how to better leverage large-scale pre-trained uni-modal models to further enhance discriminative multi-modal learning. Even when fine-tuned with only uni-modal data, these models can outperform previous multi-modal models in certain tasks. It's clear that their incorporation into multi-modal learning would significantly improve performance. However, multi-modal learning with these models still suffers from insufficient learning of uni-modal features, which weakens the resulting multi-modal model's generalization ability. While fine-tuning uni-modal models separately and then aggregating their predictions is straightforward, it doesn't allow for adequate adaptation between modalities, also leading to sub-optimal results. To this end, we introduce Multi-Modal Low-Rank Adaptation learning (MMLoRA). By freezing the weights of uni-modal fine-tuned models, adding extra trainable rank decomposition matrices to them, and subsequently performing multi-modal joint training, our method enhances adaptation between modalities and boosts overall performance. We demonstrate the effectiveness of MMLoRA on three dataset categories: audio-visual (e.g., AVE, Kinetics-Sound, CREMA-D), vision-language (e.g., MM-IMDB, UPMC Food101), and RGB-Optical Flow (UCF101).
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="HOD-A-Benchmark-Dataset-for-Harmful-Object-Detection"><a href="#HOD-A-Benchmark-Dataset-for-Harmful-Object-Detection" class="headerlink" title="HOD: A Benchmark Dataset for Harmful Object Detection"></a>HOD: A Benchmark Dataset for Harmful Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05192">http://arxiv.org/abs/2310.05192</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/poori-nuna/hod-benchmark-dataset">https://github.com/poori-nuna/hod-benchmark-dataset</a></li>
<li>paper_authors: Eungyeom Ha, Heemook Kim, Sung Chul Hong, Dongbin Na<br>for: 这个论文的目标是开发自动识别危险内容的系统，以防止在在线服务平台上传播危险内容。methods: 这个研究使用了最新的计算机视觉技术，包括使用最新的对象检测架构和大量的数据集来训练模型。results: 研究人员通过实验表明，使用提议的数据集和方法可以准确地检测在线服务平台上的危险内容，并且可以在实时应用中提供有效的识别结果。<details>
<summary>Abstract</summary>
Recent multi-media data such as images and videos have been rapidly spread out on various online services such as social network services (SNS). With the explosive growth of online media services, the number of image content that may harm users is also growing exponentially. Thus, most recent online platforms such as Facebook and Instagram have adopted content filtering systems to prevent the prevalence of harmful content and reduce the possible risk of adverse effects on users. Unfortunately, computer vision research on detecting harmful content has not yet attracted attention enough. Users of each platform still manually click the report button to recognize patterns of harmful content they dislike when exposed to harmful content. However, the problem with manual reporting is that users are already exposed to harmful content. To address these issues, our research goal in this work is to develop automatic harmful object detection systems for online services. We present a new benchmark dataset for harmful object detection. Unlike most related studies focusing on a small subset of object categories, our dataset addresses various categories. Specifically, our proposed dataset contains more than 10,000 images across 6 categories that might be harmful, consisting of not only normal cases but also hard cases that are difficult to detect. Moreover, we have conducted extensive experiments to evaluate the effectiveness of our proposed dataset. We have utilized the recently proposed state-of-the-art (SOTA) object detection architectures and demonstrated our proposed dataset can be greatly useful for the real-time harmful object detection task. The whole source codes and datasets are publicly accessible at https://github.com/poori-nuna/HOD-Benchmark-Dataset.
</details>
<details>
<summary>摘要</summary>
近年来多媒体数据如图片和视频在不同的在线服务平台上迅速扩散，如社交媒体服务（SNS）。随着在线媒体服务的快速发展，具有可能伤害用户的图像内容的数量也在增长 exponentially。因此，现代在线平台如Facebook和Instagram已经采用内容筛选系统来防止有害内容的普及和减少可能的用户伤害的风险。然而，计算机视觉研究检测有害内容还没有吸引到够多的关注。用户们仍然通过手动报告按钮来认识他们看到的有害内容。然而，手动报告的问题在于用户已经曝露在有害内容中。为解决这些问题，我们在这项工作中的研究目标是开发自动检测有害对象系统。我们提出了一个新的比较 dataset，与大多数相关研究一样，我们的 dataset 覆盖了多个对象类别，并且包含了超过 10,000 个图像，这些图像包括不只是正常情况，还有一些困难检测的情况。此外，我们进行了广泛的实验，以评估我们提出的 dataset 的有效性。我们使用了最新的 state-of-the-art 对象检测架构，并证明了我们的 dataset 可以在实时有害对象检测任务中具有很高的有用性。整个源代码和数据集都可以在 <https://github.com/poori-nuna/HOD-Benchmark-Dataset> 上公开访问。
</details></li>
</ul>
<hr>
<h2 id="AANet-Aggregation-and-Alignment-Network-with-Semi-hard-Positive-Sample-Mining-for-Hierarchical-Place-Recognition"><a href="#AANet-Aggregation-and-Alignment-Network-with-Semi-hard-Positive-Sample-Mining-for-Hierarchical-Place-Recognition" class="headerlink" title="AANet: Aggregation and Alignment Network with Semi-hard Positive Sample Mining for Hierarchical Place Recognition"></a>AANet: Aggregation and Alignment Network with Semi-hard Positive Sample Mining for Hierarchical Place Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05184">http://arxiv.org/abs/2310.05184</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Lu-Feng/AANet">https://github.com/Lu-Feng/AANet</a></li>
<li>paper_authors: Feng Lu, Lijun Zhang, Shuting Dong, Baifan Chen, Chun Yuan<br>for:* 这种paper是为了提出一种高效的视觉场景识别方法，用于Robotics中的位置定位。methods:* 该方法使用了两个阶段的 hierarchical two-stage VPR 方法，首先使用全局特征进行全局搜索，然后使用本地特征进行再排序。* 该方法还提出了一种 Dynamically Aligning Local Features (DALF) 算法，用于在空间约束下对本地特征进行对齐。results:* 对四个常用的 VPR 数据集进行了广泛的实验，结果显示，提出的 AANet 可以比一些现有的状态作准的方法更高效，同时占用的时间更少。<details>
<summary>Abstract</summary>
Visual place recognition (VPR) is one of the research hotspots in robotics, which uses visual information to locate robots. Recently, the hierarchical two-stage VPR methods have become popular in this field due to the trade-off between accuracy and efficiency. These methods retrieve the top-k candidate images using the global features in the first stage, then re-rank the candidates by matching the local features in the second stage. However, they usually require additional algorithms (e.g. RANSAC) for geometric consistency verification in re-ranking, which is time-consuming. Here we propose a Dynamically Aligning Local Features (DALF) algorithm to align the local features under spatial constraints. It is significantly more efficient than the methods that need geometric consistency verification. We present a unified network capable of extracting global features for retrieving candidates via an aggregation module and aligning local features for re-ranking via the DALF alignment module. We call this network AANet. Meanwhile, many works use the simplest positive samples in triplet for weakly supervised training, which limits the ability of the network to recognize harder positive pairs. To address this issue, we propose a Semi-hard Positive Sample Mining (ShPSM) strategy to select appropriate hard positive images for training more robust VPR networks. Extensive experiments on four benchmark VPR datasets show that the proposed AANet can outperform several state-of-the-art methods with less time consumption. The code is released at https://github.com/Lu-Feng/AANet.
</details>
<details>
<summary>摘要</summary>
Visual地位识别（VPR）是机器人学研究的热点之一，它利用视觉信息来定位机器人。近年来，层次分解两个阶段VPR方法在这个领域得到了广泛应用，因为它们可以平衡精度和效率。这些方法首先使用全局特征来检索top-k候选图像，然后在第二阶段使用本地特征进行重新排序。然而，它们通常需要额外的算法（例如RANSAC）来验证几何一致性，这会占用大量时间。我们提出了一种 Dinamically Aligning Local Features（DALF）算法，可以在空间约束下对本地特征进行对齐。与需要几何一致性验证的方法相比，它更高效。我们提出了一种能够提取全局特征并对本地特征进行对齐的网络，我们称之为AANet。另外，许多工作使用最简单的三个图像作为弱有监督训练的正例，这限制了网络的识别能力。为了解决这个问题，我们提出了一种 Semi-hard Positive Sample Mining（ShPSM）策略，可以选择适当的困难正例图像进行训练更加 Robust VPR 网络。我们在四个常用的VPR数据集上进行了广泛的实验，结果显示，我们的AANet可以超过一些状态OF-the-art方法，并且占用更少的时间。代码可以在https://github.com/Lu-Feng/AANet上下载。
</details></li>
</ul>
<hr>
<h2 id="ITRE-Low-light-Image-Enhancement-Based-on-Illumination-Transmission-Ratio-Estimation"><a href="#ITRE-Low-light-Image-Enhancement-Based-on-Illumination-Transmission-Ratio-Estimation" class="headerlink" title="ITRE: Low-light Image Enhancement Based on Illumination Transmission Ratio Estimation"></a>ITRE: Low-light Image Enhancement Based on Illumination Transmission Ratio Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05158">http://arxiv.org/abs/2310.05158</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Wang, Yihong Wang, Tong Liu, Xiubao Sui, Qian Chen</li>
<li>for: 提高低光照图像的品质</li>
<li>methods: 使用Retinex方法，包括分色域聚类、初始照明传输矩阵计算、基础模型生成和终端检查等步骤，以避免噪声、 artifacts 和过度曝光</li>
<li>results: 对比state-of-the-art方法，本方法在降低噪声、避免 artifacts、控制曝光水平方面具有优越的表现<details>
<summary>Abstract</summary>
Noise, artifacts, and over-exposure are significant challenges in the field of low-light image enhancement. Existing methods often struggle to address these issues simultaneously. In this paper, we propose a novel Retinex-based method, called ITRE, which suppresses noise and artifacts from the origin of the model, prevents over-exposure throughout the enhancement process. Specifically, we assume that there must exist a pixel which is least disturbed by low light within pixels of same color. First, clustering the pixels on the RGB color space to find the Illumination Transmission Ratio (ITR) matrix of the whole image, which determines that noise is not over-amplified easily. Next, we consider ITR of the image as the initial illumination transmission map to construct a base model for refined transmission map, which prevents artifacts. Additionally, we design an over-exposure module that captures the fundamental characteristics of pixel over-exposure and seamlessly integrate it into the base model. Finally, there is a possibility of weak enhancement when inter-class distance of pixels with same color is too small. To counteract this, we design a Robust-Guard module that safeguards the robustness of the image enhancement process. Extensive experiments demonstrate the effectiveness of our approach in suppressing noise, preventing artifacts, and controlling over-exposure level simultaneously. Our method performs superiority in qualitative and quantitative performance evaluations by comparing with state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
噪声、artefacts和过度曝光是低光照图像增强领域中的主要挑战。现有方法 oftentimes 难以同时解决这些问题。在这篇论文中，我们提出了一种新的Retinex基于的方法，称为ITRE，该方法可以在图像增强过程中对噪声和artefacts进行控制，同时避免过度曝光。具体来说，我们假设在图像中存在一个最少受到低光照的像素，我们可以通过RGB色彩空间的聚类来找到整个图像的照明传输矩阵（ITR），该矩阵确定了噪声不易过度增强。接着，我们将ITR矩阵作为图像的初始照明传输地图，并将其用于构建基本模型，以避免artefacts。此外，我们还设计了一个过度曝光模块，该模块可以融合到基本模型中，以捕捉图像过度曝光的基本特征。最后，当相同色彩的像素间距离过小时，可能会出现弱化效果。为了解决这个问题，我们设计了一个Robust-Guard模块，以保证图像增强过程的稳定性。广泛的实验表明，我们的方法可以同时控制噪声、artefacts和过度曝光水平，并且在质量和量化性能评价中表现出优于状态艺术方法。
</details></li>
</ul>
<hr>
<h2 id="LocoNeRF-A-NeRF-based-Approach-for-Local-Structure-from-Motion-for-Precise-Localization"><a href="#LocoNeRF-A-NeRF-based-Approach-for-Local-Structure-from-Motion-for-Precise-Localization" class="headerlink" title="LocoNeRF: A NeRF-based Approach for Local Structure from Motion for Precise Localization"></a>LocoNeRF: A NeRF-based Approach for Local Structure from Motion for Precise Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05134">http://arxiv.org/abs/2310.05134</a></li>
<li>repo_url: None</li>
<li>paper_authors: Artem Nenashev, Mikhail Kurenkov, Andrei Potapov, Iana Zhura, Maksim Katerishich, Dzmitry Tsetserukou</li>
<li>for: 提高视觉定位精度， addresses the limitations of global SfM and the challenges of local SfM.</li>
<li>methods: 使用Neural Radiance Fields (NeRF) instead of image databases for storage, and sampling reference images around the prior query position for further improvements.</li>
<li>results: 比ground truth有0.068米的准确性，但是数据库大小减少至160MB，比COLMAP的400MB有所降低。 Additionally, the ablation study shows the impact of using reference images from the NeRF reconstruction.<details>
<summary>Abstract</summary>
Visual localization is a critical task in mobile robotics, and researchers are continuously developing new approaches to enhance its efficiency. In this article, we propose a novel approach to improve the accuracy of visual localization using Structure from Motion (SfM) techniques. We highlight the limitations of global SfM, which suffers from high latency, and the challenges of local SfM, which requires large image databases for accurate reconstruction. To address these issues, we propose utilizing Neural Radiance Fields (NeRF), as opposed to image databases, to cut down on the space required for storage. We suggest that sampling reference images around the prior query position can lead to further improvements. We evaluate the accuracy of our proposed method against ground truth obtained using LIDAR and Advanced Lidar Odometry and Mapping in Real-time (A-LOAM), and compare its storage usage against local SfM with COLMAP in the conducted experiments. Our proposed method achieves an accuracy of 0.068 meters compared to the ground truth, which is slightly lower than the most advanced method COLMAP, which has an accuracy of 0.022 meters. However, the size of the database required for COLMAP is 400 megabytes, whereas the size of our NeRF model is only 160 megabytes. Finally, we perform an ablation study to assess the impact of using reference images from the NeRF reconstruction.
</details>
<details>
<summary>摘要</summary>
“视觉本地化是移动 роботика中的一项关键任务，研究人员不断开发新的方法来提高其效率。在这篇文章中，我们提出一种新的方法来提高视觉本地化的准确性，使用结构从运动（SfM）技术。我们指出了全球SfM的局限性，即高延迟，以及本地SfM的挑战，即需要大量图像数据库以实现准确重建。为了解决这些问题，我们提议使用神经辐射场（NeRF），而不是图像数据库，来减少存储空间。我们建议在提前查询位置附近采样参考图像可以获得进一步改进。我们对我们提议的方法与实际数据进行评估，并与使用COLMAP的本地SfM进行比较。我们的方法准确性为0.068米，轻微低于最先进的方法COLMAP准确性（0.022米）。然而，COLMAP需要400兆字节的数据库，而我们的NeRF模型只需160兆字节。最后，我们进行了一个ablation研究，以评估使用NeRF重建中的参考图像的影响。”
</details></li>
</ul>
<hr>
<h2 id="Geometry-Aware-Field-to-field-Transformations-for-3D-Semantic-Segmentation"><a href="#Geometry-Aware-Field-to-field-Transformations-for-3D-Semantic-Segmentation" class="headerlink" title="Geometry Aware Field-to-field Transformations for 3D Semantic Segmentation"></a>Geometry Aware Field-to-field Transformations for 3D Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05133">http://arxiv.org/abs/2310.05133</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/USTCPCS/CVPR2018_attention">https://github.com/USTCPCS/CVPR2018_attention</a></li>
<li>paper_authors: Dominik Hollidt, Clinton Wang, Polina Golland, Marc Pollefeys</li>
<li>for: 实现3D内容Semantic Segmentation仅基于2D监控，使用神经辉照场（NeRF）。</li>
<li>methods: 通过获取表面点云的特征，实现了几何簇的储存，实现了3D理解。通过伪类型自动编码学习，实现了几何簇的数据储存。</li>
<li>results: 获得了几何簇的储存，并且可以实现几何簇的3D理解。<details>
<summary>Abstract</summary>
We present a novel approach to perform 3D semantic segmentation solely from 2D supervision by leveraging Neural Radiance Fields (NeRFs). By extracting features along a surface point cloud, we achieve a compact representation of the scene which is sample-efficient and conducive to 3D reasoning. Learning this feature space in an unsupervised manner via masked autoencoding enables few-shot segmentation. Our method is agnostic to the scene parameterization, working on scenes fit with any type of NeRF.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法，可以通过使用神经辐射场（NeRF）来完成3Dsemantic segmentation，仅基于2D监督。我们通过对表面点云中提取特征来获得一个减少的表示形式，该形式是效率的样本和适合3D理解。通过在掩码自动编码中学习这个特征空间，我们实现了几个shot分类。我们的方法对于场景参数化是无关的，可以应用于任何类型的NeRF场景。Note: "神经辐射场" (NeRF) is a Chinese term that refers to Neural Radiance Fields.
</details></li>
</ul>
<hr>
<h2 id="Bidirectional-Knowledge-Reconfiguration-for-Lightweight-Point-Cloud-Analysis"><a href="#Bidirectional-Knowledge-Reconfiguration-for-Lightweight-Point-Cloud-Analysis" class="headerlink" title="Bidirectional Knowledge Reconfiguration for Lightweight Point Cloud Analysis"></a>Bidirectional Knowledge Reconfiguration for Lightweight Point Cloud Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05125">http://arxiv.org/abs/2310.05125</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peipei Li, Xing Cui, Yibo Hu, Man Zhang, Ting Yao, Tao Mei</li>
<li>for: 本研究旨在提高点云分析的计算机系统过载问题，使其可以在移动或边缘设备上应用。</li>
<li>methods: 本文提出了特征缩减技术来降低点云模型的计算负担。 Specifically, we propose bidirectional knowledge reconfiguration (BKR) to distill informative contextual knowledge from the teacher to the student.</li>
<li>results: 我们的方法在shape classification、part segmentation和semantic segmentation benchmarks上表现出了超越性和优势，demonstrating the universality and superiority of our method.<details>
<summary>Abstract</summary>
Point cloud analysis faces computational system overhead, limiting its application on mobile or edge devices. Directly employing small models may result in a significant drop in performance since it is difficult for a small model to adequately capture local structure and global shape information simultaneously, which are essential clues for point cloud analysis. This paper explores feature distillation for lightweight point cloud models. To mitigate the semantic gap between the lightweight student and the cumbersome teacher, we propose bidirectional knowledge reconfiguration (BKR) to distill informative contextual knowledge from the teacher to the student. Specifically, a top-down knowledge reconfiguration and a bottom-up knowledge reconfiguration are developed to inherit diverse local structure information and consistent global shape knowledge from the teacher, respectively. However, due to the farthest point sampling in most point cloud models, the intermediate features between teacher and student are misaligned, deteriorating the feature distillation performance. To eliminate it, we propose a feature mover's distance (FMD) loss based on optimal transportation, which can measure the distance between unordered point cloud features effectively. Extensive experiments conducted on shape classification, part segmentation, and semantic segmentation benchmarks demonstrate the universality and superiority of our method.
</details>
<details>
<summary>摘要</summary>
点云分析面临计算系统开销限制其在移动或边缘设备上应用。直接采用小型模型可能会导致显著性能下降，因为小型模型很难同时捕捉点云中的本地结构和全局形态信息，这些信息是点云分析的关键决定因素。本文探讨了降简点云模型的技术。为了减少教师和学生之间的Semantic gap，我们提出了双向知识重新配置（BKR），将教师知识中的有用Contextual information遗传给学生。具体来说，我们开发了从教师到学生的顶部知识重新配置和从学生到教师的底部知识重新配置，以继承教师的多样化本地结构信息和一致的全局形态知识。然而，由于多数点云模型中的远点抽样，学生和教师之间的中间特征不对Alignment，这会降低feature distillation的性能。为了解决这个问题，我们提出了基于最优运输的特征移动距离（FMD）损失，可以有效度量不同点云特征之间的距离。我们对shape classification、部分 segmentation和semantic segmentation benchmark进行了广泛的实验，结果表明我们的方法在 universality 和优势性方面具有出色的表现。
</details></li>
</ul>
<hr>
<h2 id="Cross-domain-Robust-Deepfake-Bias-Expansion-Network-for-Face-Forgery-Detection"><a href="#Cross-domain-Robust-Deepfake-Bias-Expansion-Network-for-Face-Forgery-Detection" class="headerlink" title="Cross-domain Robust Deepfake Bias Expansion Network for Face Forgery Detection"></a>Cross-domain Robust Deepfake Bias Expansion Network for Face Forgery Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05124">http://arxiv.org/abs/2310.05124</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weihua Liu, Lin Li, Chaochao Lin, Said Boumaraf<br>for: 这篇论文旨在提高人脸假制检测的安全性，尤其是面对深度推假技术的威胁。methods: 本论文提出了一种解决方案——跨频率坚定偏差扩展网络（BENet），通过使用自动编码器重建输入face，保持真实face的均衡性，同时选择性地增强假face与其原始样本之间的差异。results: 对比于现有方法，BENet在内部和跨频率测试中表现出色，能够有效地检测深度推假face。此外，BENet还 incorporates 一种矩阵注意力（LSA）模块，可以更好地捕捉异常的假制特征。<details>
<summary>Abstract</summary>
The rapid advancement of deepfake technologies raises significant concerns about the security of face recognition systems. While existing methods leverage the clues left by deepfake techniques for face forgery detection, malicious users may intentionally manipulate forged faces to obscure the traces of deepfake clues and thereby deceive detection tools. Meanwhile, attaining cross-domain robustness for data-based methods poses a challenge due to potential gaps in the training data, which may not encompass samples from all relevant domains. Therefore, in this paper, we introduce a solution - a Cross-Domain Robust Bias Expansion Network (BENet) - designed to enhance face forgery detection. BENet employs an auto-encoder to reconstruct input faces, maintaining the invariance of real faces while selectively enhancing the difference between reconstructed fake faces and their original counterparts. This enhanced bias forms a robust foundation upon which dependable forgery detection can be built. To optimize the reconstruction results in BENet, we employ a bias expansion loss infused with contrastive concepts to attain the aforementioned objective. In addition, to further heighten the amplification of forged clues, BENet incorporates a Latent-Space Attention (LSA) module. This LSA module effectively captures variances in latent features between the auto-encoder's encoder and decoder, placing emphasis on inconsistent forgery-related information. Furthermore, BENet incorporates a cross-domain detector with a threshold to determine whether the sample belongs to a known distribution. The correction of classification results through the cross-domain detector enables BENet to defend against unknown deepfake attacks from cross-domain. Extensive experiments demonstrate the superiority of BENet compared with state-of-the-art methods in intra-database and cross-database evaluations.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "The rapid advancement of deepfake technologies raises significant concerns about the security of face recognition systems. While existing methods leverage the clues left by deepfake techniques for face forgery detection, malicious users may intentionally manipulate forged faces to obscure the traces of deepfake clues and thereby deceive detection tools. Meanwhile, attaining cross-domain robustness for data-based methods poses a challenge due to potential gaps in the training data, which may not encompass samples from all relevant domains. Therefore, in this paper, we introduce a solution - a Cross-Domain Robust Bias Expansion Network (BENet) - designed to enhance face forgery detection. BENet employs an auto-encoder to reconstruct input faces, maintaining the invariance of real faces while selectively enhancing the difference between reconstructed fake faces and their original counterparts. This enhanced bias forms a robust foundation upon which dependable forgery detection can be built. To optimize the reconstruction results in BENet, we employ a bias expansion loss infused with contrastive concepts to attain the aforementioned objective. In addition, to further heighten the amplification of forged clues, BENet incorporates a Latent-Space Attention (LSA) module. This LSA module effectively captures variances in latent features between the auto-encoder's encoder and decoder, placing emphasis on inconsistent forgery-related information. Furthermore, BENet incorporates a cross-domain detector with a threshold to determine whether the sample belongs to a known distribution. The correction of classification results through the cross-domain detector enables BENet to defend against unknown deepfake attacks from cross-domain. Extensive experiments demonstrate the superiority of BENet compared with state-of-the-art methods in intra-database and cross-database evaluations." into Simplified Chinese.Here's the translation:“深刻的深伪技术的快速发展，对人脸识别系统的安全带来重要的忧虑。现有的方法可以利用深伪技术留下的伪证据来检测伪证，但是黑客可能会故意修改伪证，以隐藏深伪的迹象，并且欺骗检测工具。同时，为了实现跨领域Robustness，资料基础方法面临着潜在的领域差异问题，这些训练数据可能不包括所有 relevance 的领域。因此，在这篇论文中，我们提出了一个解决方案——跨领域Robust Bias Expansion Network（BENet），用于增强伪证检测。BENet 使用 auto-encoder 重建输入的脸部，保持真实脸部的不变性，同时选择性地强化伪证的重建和原始对比。这个强化的偏见形成了可靠的基础，以便建立可靠的伪证检测。为了优化 BENet 中的重建结果，我们使用了偏见扩展损失，融合了相对概念，以达到这个目标。此外，BENet 还包括一个 Latent-Space Attention（LSA）模组，这个模组可以有效地捕捉 auto-encoder 的Encoder 和 Decoder 之间的潜在特征差异，将注意力集中在伪证相关的不一致信息上。此外，BENet 还包括一个跨领域检测器，以决定样本是否属于已知分布。通过跨领域检测器进行样本的修正，使得 BENet 能够防止不知道的深伪攻击。实验结果显示，BENet 与现有的方法相比，在内部资料和跨部资料评估中表现出色。”
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Multi-Domain-Knowledge-Networks-for-Chest-X-ray-Report-Generation"><a href="#Dynamic-Multi-Domain-Knowledge-Networks-for-Chest-X-ray-Report-Generation" class="headerlink" title="Dynamic Multi-Domain Knowledge Networks for Chest X-ray Report Generation"></a>Dynamic Multi-Domain Knowledge Networks for Chest X-ray Report Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05119">http://arxiv.org/abs/2310.05119</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weihua Liu, Youyuan Xue, Chaochao Lin, Said Boumaraf</li>
<li>for: This paper aims to address the challenges of automatically generating radiology diagnostic reports, particularly the imbalance in data distribution between normal and abnormal samples, by proposing a Dynamic Multi-Domain Knowledge (DMDK) network.</li>
<li>methods: The proposed DMDK network consists of four modules: Chest Feature Extractor (CFE), Dynamic Knowledge Extractor (DKE), Specific Knowledge Extractor (SKE), and Multi-knowledge Integrator (MKI) module. The network utilizes dynamic disease topic labels, domain-specific dynamic knowledge graphs, and multi-knowledge integration to mitigate data biases and enhance interpretability.</li>
<li>results: The proposed method was extensively evaluated on two widely used datasets (IU X-Ray and MIMIC-CXR) and achieved state-of-the-art performance in all evaluation metrics, outperforming previous models.<details>
<summary>Abstract</summary>
The automated generation of radiology diagnostic reports helps radiologists make timely and accurate diagnostic decisions while also enhancing clinical diagnostic efficiency. However, the significant imbalance in the distribution of data between normal and abnormal samples (including visual and textual biases) poses significant challenges for a data-driven task like automatically generating diagnostic radiology reports. Therefore, we propose a Dynamic Multi-Domain Knowledge(DMDK) network for radiology diagnostic report generation. The DMDK network consists of four modules: Chest Feature Extractor(CFE), Dynamic Knowledge Extractor(DKE), Specific Knowledge Extractor(SKE), and Multi-knowledge Integrator(MKI) module. Specifically, the CFE module is primarily responsible for extracting the unprocessed visual medical features of the images. The DKE module is responsible for extracting dynamic disease topic labels from the retrieved radiology diagnostic reports. We then fuse the dynamic disease topic labels with the original visual features of the images to highlight the abnormal regions in the original visual features to alleviate the visual data bias problem. The SKE module expands upon the conventional static knowledge graph to mitigate textual data biases and amplify the interpretability capabilities of the model via domain-specific dynamic knowledge graphs. The MKI distills all the knowledge and generates the final diagnostic radiology report. We performed extensive experiments on two widely used datasets, IU X-Ray and MIMIC-CXR. The experimental results demonstrate the effectiveness of our method, with all evaluation metrics outperforming previous state-of-the-art models.
</details>
<details>
<summary>摘要</summary>
自动生成 radiology 诊断报告可以帮助 radiologist 更快、更准确地作出诊断决策，同时提高临床诊断效率。然而，数据分布的巨大偏度问题（包括视觉和文本偏见）对于数据驱动的任务如自动生成 radiology 诊断报告来说是一个挑战。因此，我们提出了动态多Domain知识（DMDK）网络，用于 radiology 诊断报告生成。DMDK 网络由四个模块组成：胸部特征提取器（CFE）、动态知识提取器（DKE）、特定知识提取器（SKE）和多知识 интегратор（MKI）模块。具体来说，CFE 模块主要负责从未处理的医学影像中提取视觉特征。DKE 模块负责从检索到的 radiology 诊断报告中提取动态疾病话题标签。我们将这些动态疾病话题标签与原始视觉特征相结合，以减少视觉数据偏见问题。SKE 模块在传统的静止知识图中增强了文本数据偏见问题，并通过域pecific的动态知识图来提高模型的解释能力。MKI 模块将所有的知识integrirated，并生成最终的 radiology 诊断报告。我们在 IU X-Ray 和 MIMIC-CXR 两个广泛使用的数据集上进行了广泛的实验，结果表明我们的方法效果很高，所有评价指标都高于之前的状态对照模型。
</details></li>
</ul>
<hr>
<h2 id="Lightweight-In-Context-Tuning-for-Multimodal-Unified-Models"><a href="#Lightweight-In-Context-Tuning-for-Multimodal-Unified-Models" class="headerlink" title="Lightweight In-Context Tuning for Multimodal Unified Models"></a>Lightweight In-Context Tuning for Multimodal Unified Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05109">http://arxiv.org/abs/2310.05109</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yixin Chen, Shuai Zhang, Boran Han, Jiaya Jia</li>
<li>For: The paper aims to address the challenges of in-context learning (ICL) in multimodal tasks, specifically the difficulty of extrapolating from contextual examples to perform ICL as more modalities are added.* Methods: The proposed solution is called MultiModal In-conteXt Tuning (M$^2$IXT), a lightweight module that incorporates an expandable context window to incorporate various labeled examples of multiple modalities. The module can be prepended to various multimodal unified models and trained via a mixed-tasks strategy to enable rapid few-shot adaption on multiple tasks and datasets.* Results: The paper shows that M$^2$IXT can significantly boost the few-shot ICL performance (e.g., 18% relative increase for OFA) and achieve state-of-the-art results across various tasks, including visual question answering, image captioning, visual grounding, and visual entailment, while being considerably small in terms of model parameters.<details>
<summary>Abstract</summary>
In-context learning (ICL) involves reasoning from given contextual examples. As more modalities comes, this procedure is becoming more challenging as the interleaved input modalities convolutes the understanding process. This is exemplified by the observation that multimodal models often struggle to effectively extrapolate from contextual examples to perform ICL. To address these challenges, we introduce MultiModal In-conteXt Tuning (M$^2$IXT), a lightweight module to enhance the ICL capabilities of multimodal unified models. The proposed M$^2$IXT module perceives an expandable context window to incorporate various labeled examples of multiple modalities (e.g., text, image, and coordinates). It can be prepended to various multimodal unified models (e.g., OFA, Unival, LLaVA) of different architectures and trained via a mixed-tasks strategy to enable rapid few-shot adaption on multiple tasks and datasets. When tuned on as little as 50K multimodal data, M$^2$IXT can boost the few-shot ICL performance significantly (e.g., 18\% relative increase for OFA), and obtained state-of-the-art results across an array of tasks including visual question answering, image captioning, visual grounding, and visual entailment, while being considerably small in terms of model parameters (e.g., $\sim$$20\times$ smaller than Flamingo or MMICL), highlighting the flexibility and effectiveness of M$^2$IXT as a multimodal in-context learner.
</details>
<details>
<summary>摘要</summary>
宽 Context 学习（ICL） involve 从Contextual例子进行推理。随着更多modalities来临，这个过程变得更加挑战，因为交错的输入modalities会混淆理解过程。这被示示于多modal模型在 Contextual例子上进行ICL时的表现不佳。为了解决这些挑战，我们介绍 MultiModal In-conteXt Tuning（M$^2$IXT）模块，用于提高多modal unified模型的ICL能力。该模块可以预pend于多modal unified模型（例如OFA、Unival、LLaVA）的不同架构上，并通过混合任务策略进行训练，以实现快速少量数据适应。当与50000个多modal数据进行训练时，M$^2$IXT可以显著提高ICL性能（例如18%相对提高），并在视觉问答、图像描述、视觉固定、视觉包容等任务上达到了状态之册的结果，而且模型参数很小（例如$\sim$$20\times$ smaller than Flamingo或MMICL），这说明M$^2$IXT具有多modal Contextual学习的灵活性和效果。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Representations-through-Heterogeneous-Self-Supervised-Learning"><a href="#Enhancing-Representations-through-Heterogeneous-Self-Supervised-Learning" class="headerlink" title="Enhancing Representations through Heterogeneous Self-Supervised Learning"></a>Enhancing Representations through Heterogeneous Self-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05108">http://arxiv.org/abs/2310.05108</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhong-Yu Li, Bo-Wen Yin, Shanghua Gao, Yongxiang Liu, Li Liu, Ming-Ming Cheng</li>
<li>for: 提高自我超vised学习中基础模型的表示质量，通过在基础模型上添加不同架构的辅助头来增强表示能力。</li>
<li>methods: 提出了一种基于不同架构的辅助头的自我超vised学习方法（HSSL），通过让基础模型学习辅助头的不同架构来增强表示能力，而不需要Structural changes。</li>
<li>results: 通过多种不同的基础模型和辅助头组合，对多个下游任务（包括图像分类、 semantic segmentation、instance segmentation、object detection）进行了实验，并发现了基础模型的表示质量随着辅助头的架构差异增加而提高。此外，还提出了一种快速找到最适合基础模型学习的辅助头的搜索策略和一些简单 yet effective的方法来扩大模型差异。<details>
<summary>Abstract</summary>
Incorporating heterogeneous representations from different architectures has facilitated various vision tasks, e.g., some hybrid networks combine transformers and convolutions. However, complementarity between such heterogeneous architectures has not been well exploited in self-supervised learning. Thus, we propose Heterogeneous Self-Supervised Learning (HSSL), which enforces a base model to learn from an auxiliary head whose architecture is heterogeneous from the base model. In this process, HSSL endows the base model with new characteristics in a representation learning way without structural changes. To comprehensively understand the HSSL, we conduct experiments on various heterogeneous pairs containing a base model and an auxiliary head. We discover that the representation quality of the base model moves up as their architecture discrepancy grows. This observation motivates us to propose a search strategy that quickly determines the most suitable auxiliary head for a specific base model to learn and several simple but effective methods to enlarge the model discrepancy. The HSSL is compatible with various self-supervised methods, achieving superior performances on various downstream tasks, including image classification, semantic segmentation, instance segmentation, and object detection. Our source code will be made publicly available.
</details>
<details>
<summary>摘要</summary>
将不同架构的表示结合在一起已经提高了许多视觉任务的性能，例如混合网络将转换器和卷积结合使用。然而，这些不同架构之间的补做性未得到了自我超vised学习中的充分利用。因此，我们提出了多样化自我超vised学习（HSSL），它要求基本模型从auxiliary头中学习，auxiliary头的架构与基本模型不同。在这个过程中，HSSL使得基本模型学习新的特征，不需要结构性改变。为了全面了解HSSL，我们在不同的 heterogeneous对中进行了实验，发现当基本模型和auxiliary头的架构差异增大时，基本模型的表示质量会提高。这一观察使我们提出了一种快速找到最适合基本模型学习的auxiliary头的搜索策略，以及一些简单 yet effective的方法来扩大模型差异。HSSL可以与多种自我超vised方法结合使用，在多种下游任务上达到了更高的性能，包括图像分类、 semantic segmentation、实例 segmentation和对象检测。我们将代码公开发布。
</details></li>
</ul>
<hr>
<h2 id="OV-PARTS-Towards-Open-Vocabulary-Part-Segmentation"><a href="#OV-PARTS-Towards-Open-Vocabulary-Part-Segmentation" class="headerlink" title="OV-PARTS: Towards Open-Vocabulary Part Segmentation"></a>OV-PARTS: Towards Open-Vocabulary Part Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05107">http://arxiv.org/abs/2310.05107</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/openrobotlab/ov_parts">https://github.com/openrobotlab/ov_parts</a></li>
<li>paper_authors: Meng Wei, Xiaoyu Yue, Wenwei Zhang, Shu Kong, Xihui Liu, Jiangmiao Pang</li>
<li>for: 本研究旨在提出一个开 vocabulary part segmentation（OV-PARTS）benchmark，以探索在实世界中具有多元定义的部分构成的挑战。</li>
<li>methods: 本研究使用了两个公开可用的数据集：Pascal-Part-116和ADE20K-Part-234，并提出了三个特定任务：通用零基准部分分类、跨数据集部分分类和少量基准部分分类，以探索模型对于不同定义的部分的数据分类能力。</li>
<li>results: 本研究通过实验分析了两种现有的物件水平OVSS方法的适用性，并提供了一个精确的数据集和代码，以便未来研究者可以在OV-PARTS领域进行更多的探索和创新。<details>
<summary>Abstract</summary>
Segmenting and recognizing diverse object parts is a crucial ability in applications spanning various computer vision and robotic tasks. While significant progress has been made in object-level Open-Vocabulary Semantic Segmentation (OVSS), i.e., segmenting objects with arbitrary text, the corresponding part-level research poses additional challenges. Firstly, part segmentation inherently involves intricate boundaries, while limited annotated data compounds the challenge. Secondly, part segmentation introduces an open granularity challenge due to the diverse and often ambiguous definitions of parts in the open world. Furthermore, the large-scale vision and language models, which play a key role in the open vocabulary setting, struggle to recognize parts as effectively as objects. To comprehensively investigate and tackle these challenges, we propose an Open-Vocabulary Part Segmentation (OV-PARTS) benchmark. OV-PARTS includes refined versions of two publicly available datasets: Pascal-Part-116 and ADE20K-Part-234. And it covers three specific tasks: Generalized Zero-Shot Part Segmentation, Cross-Dataset Part Segmentation, and Few-Shot Part Segmentation, providing insights into analogical reasoning, open granularity and few-shot adapting abilities of models. Moreover, we analyze and adapt two prevailing paradigms of existing object-level OVSS methods for OV-PARTS. Extensive experimental analysis is conducted to inspire future research in leveraging foundational models for OV-PARTS. The code and dataset are available at https://github.com/OpenRobotLab/OV_PARTS.
</details>
<details>
<summary>摘要</summary>
Segmenting and recognizing diverse object parts is a crucial ability in various computer vision and robotic tasks. Although significant progress has been made in object-level Open-Vocabulary Semantic Segmentation (OVSS), segmenting objects with arbitrary text, the corresponding part-level research poses additional challenges. Firstly, part segmentation involves intricate boundaries, and limited annotated data makes it more challenging. Secondly, part segmentation introduces an open granularity challenge due to the diverse and often ambiguous definitions of parts in the open world. Moreover, large-scale vision and language models, which play a key role in the open vocabulary setting, struggle to recognize parts as effectively as objects.To comprehensively investigate and tackle these challenges, we propose an Open-Vocabulary Part Segmentation (OV-PARTS) benchmark. OV-PARTS includes refined versions of two publicly available datasets: Pascal-Part-116 and ADE20K-Part-234. It covers three specific tasks: Generalized Zero-Shot Part Segmentation, Cross-Dataset Part Segmentation, and Few-Shot Part Segmentation, providing insights into analogical reasoning, open granularity, and few-shot adapting abilities of models. Moreover, we analyze and adapt two prevailing paradigms of existing object-level OVSS methods for OV-PARTS. Extensive experimental analysis is conducted to inspire future research in leveraging foundational models for OV-PARTS. The code and dataset are available at https://github.com/OpenRobotLab/OV_PARTS.
</details></li>
</ul>
<hr>
<h2 id="Cross-head-mutual-Mean-Teaching-for-semi-supervised-medical-image-segmentation"><a href="#Cross-head-mutual-Mean-Teaching-for-semi-supervised-medical-image-segmentation" class="headerlink" title="Cross-head mutual Mean-Teaching for semi-supervised medical image segmentation"></a>Cross-head mutual Mean-Teaching for semi-supervised medical image segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05082">http://arxiv.org/abs/2310.05082</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leesoon1984/cmmt-net">https://github.com/leesoon1984/cmmt-net</a></li>
<li>paper_authors: Wei Li, Ruifeng Bian, Wenyi Zhao, Weijin Xu, Huihua Yang</li>
<li>for: 提高 semi-supervised medical image segmentation 的精度和一致性</li>
<li>methods: 提出了一种新的 Cross-head mutual mean-teaching Network (CMMT-Net)，包括 teacher-student 师生网络和 pseudo label 生成等技术，以提高自教学和一致学习的性能</li>
<li>results: 实验结果显示，CMMT-Net 在三个公共可用的数据集上取得了前所未有的提高，在不同的 semi-supervised enario中均表现出色<details>
<summary>Abstract</summary>
Semi-supervised medical image segmentation (SSMIS) has witnessed substantial advancements by leveraging limited labeled data and abundant unlabeled data. Nevertheless, existing state-of-the-art (SOTA) methods encounter challenges in accurately predicting labels for the unlabeled data, giving rise to disruptive noise during training and susceptibility to erroneous information overfitting. Moreover, applying perturbations to inaccurate predictions further reduces consistent learning. To address these concerns, we propose a novel Cross-head mutual mean-teaching Network (CMMT-Net) incorporated strong-weak data augmentation, thereby benefitting both self-training and consistency learning. Specifically, our CMMT-Net consists of both teacher-student peer networks with a share encoder and dual slightly different decoders, and the pseudo labels generated by one mean teacher head are adopted to supervise the other student branch to achieve a mutual consistency. Furthermore, we propose mutual virtual adversarial training (MVAT) to smooth the decision boundary and enhance feature representations. To diversify the consistency training samples, we employ Cross-Set CutMix strategy, which also helps address distribution mismatch issues. Notably, CMMT-Net simultaneously implements data, feature, and network perturbations, amplifying model diversity and generalization performance. Experimental results on three publicly available datasets indicate that our approach yields remarkable improvements over previous SOTA methods across various semi-supervised scenarios. Code and logs will be available at https://github.com/Leesoon1984/CMMT-Net.
</details>
<details>
<summary>摘要</summary>
semi-supervised医学图像分割（SSMIS）在过去几年中取得了重大进步，通过利用有限的标注数据和庞大的无标注数据。然而，现有的状态之 искусственный智能（SOTA）方法在准确地预测无标注数据的标签时遇到了挑战，从而导致训练过程中的干扰和模型过拟合。此外，在应用扰动后，模型的学习不稳定。为解决这些问题，我们提出了一种新的交叉头同义启发网络（CMMT-Net），具有强弱数据增强和一致学习。具体来说，我们的CMMT-Net包括一个共享encoder和两个不同的decoder，其中一个用于生成 pseudo标签，另一个用于自我训练。此外，我们还提出了一种mutual virtual adversarial training（MVAT），以缓解决决策边界的问题，并提高特征表示。为了多样化一致学习样本，我们采用了 Cross-Set CutMix策略，这也有助于解决分布匹配问题。值得注意的是，CMMT-Net同时实现了数据、特征和网络扰动，从而扩大模型多样性和总体性能。我们的方法在三个公开的数据集上进行了实验，并取得了前所未有的提高。代码和日志将在https://github.com/Leesoon1984/CMMT-Net上提供。
</details></li>
</ul>
<hr>
<h2 id="Language-driven-Open-Vocabulary-Keypoint-Detection-for-Animal-Body-and-Face"><a href="#Language-driven-Open-Vocabulary-Keypoint-Detection-for-Animal-Body-and-Face" class="headerlink" title="Language-driven Open-Vocabulary Keypoint Detection for Animal Body and Face"></a>Language-driven Open-Vocabulary Keypoint Detection for Animal Body and Face</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05056">http://arxiv.org/abs/2310.05056</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Zhang, Kaipeng Zhang, Lumin Xu, Shenqi Lai, Wenqi Shao, Nanning Zheng, Ping Luo, Yu Qiao</li>
<li>for: 提出开放词汇键点检测（OVKD）任务，以用文本提示来ocalize任意种类的键点。</li>
<li>methods: 提出Open-Vocabulary Keypoint Detection with Semantic-feature Matching（KDSM）方法，利用视觉和语言模型来利用文本和视觉之间的关系，从而实现键点检测。</li>
<li>results: 实验表明，我们提出的组件带来了显著性能提升，而我们的总方法在OVKD中达到了非常出色的结果，甚至在零例学习方式下超过了现状卷积检测方法。<details>
<summary>Abstract</summary>
Current approaches for image-based keypoint detection on animal (including human) body and face are limited to specific keypoints and species. We address the limitation by proposing the Open-Vocabulary Keypoint Detection (OVKD) task. It aims to use text prompts to localize arbitrary keypoints of any species. To accomplish this objective, we propose Open-Vocabulary Keypoint Detection with Semantic-feature Matching (KDSM), which utilizes both vision and language models to harness the relationship between text and vision and thus achieve keypoint detection through associating text prompt with relevant keypoint features. Additionally, KDSM integrates domain distribution matrix matching and some special designs to reinforce the relationship between language and vision, thereby improving the model's generalizability and performance. Extensive experiments show that our proposed components bring significant performance improvements, and our overall method achieves impressive results in OVKD. Remarkably, our method outperforms the state-of-the-art few-shot keypoint detection methods using a zero-shot fashion. We will make the source code publicly accessible.
</details>
<details>
<summary>摘要</summary>
当前对人体和面部图像中的关键点检测方法受限于特定关键点和种类。我们解决这一限制，提出开放词汇关键点检测（OVKD）任务。该任务的目标是使用文本提示来确定任意种类的关键点。为 достичь这一目标，我们提出了开放词汇关键点检测与semantic特征匹配（KDSM）方法，该方法利用视觉和语言模型来利用视觉和文本之间的关系，从而通过文本提示与相关的关键点特征相匹配来实现关键点检测。此外，KDSM还 integrates域名分布矩阵匹配和一些特殊的设计，以强化语言和视觉之间的关系，从而提高模型的普适性和性能。我们的实验表明，我们提出的组件带来了显著的性能提升，而我们的总方法在OVKD中实现了卓越的成绩，并且在零shot模式下超越了现有的状态对抗方法。我们将将源代码公开访问。
</details></li>
</ul>
<hr>
<h2 id="FairTune-Optimizing-Parameter-Efficient-Fine-Tuning-for-Fairness-in-Medical-Image-Analysis"><a href="#FairTune-Optimizing-Parameter-Efficient-Fine-Tuning-for-Fairness-in-Medical-Image-Analysis" class="headerlink" title="FairTune: Optimizing Parameter Efficient Fine Tuning for Fairness in Medical Image Analysis"></a>FairTune: Optimizing Parameter Efficient Fine Tuning for Fairness in Medical Image Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05055">http://arxiv.org/abs/2310.05055</a></li>
<li>repo_url: None</li>
<li>paper_authors: Raman Dutt, Ondrej Bohdal, Sotirios A. Tsaftaris, Timothy Hospedales</li>
<li>for: 这个论文目标是提高机器学习模型的鲁棒性和公平性，特别是在具有伦理敏感性的应用领域，如医学诊断。</li>
<li>methods: 这篇论文使用了两级优化方法来解决公平学习问题，即在验证集上优化学习策略以确保公平性，并在更新参数时考虑公平性。</li>
<li>results: 论文的实验结果表明，使用 FairTune 框架可以提高医学图像 datasets 上的公平性。<details>
<summary>Abstract</summary>
Training models with robust group fairness properties is crucial in ethically sensitive application areas such as medical diagnosis. Despite the growing body of work aiming to minimise demographic bias in AI, this problem remains challenging. A key reason for this challenge is the fairness generalisation gap: High-capacity deep learning models can fit all training data nearly perfectly, and thus also exhibit perfect fairness during training. In this case, bias emerges only during testing when generalisation performance differs across subgroups. This motivates us to take a bi-level optimisation perspective on fair learning: Optimising the learning strategy based on validation fairness. Specifically, we consider the highly effective workflow of adapting pre-trained models to downstream medical imaging tasks using parameter-efficient fine-tuning (PEFT) techniques. There is a trade-off between updating more parameters, enabling a better fit to the task of interest vs. fewer parameters, potentially reducing the generalisation gap. To manage this tradeoff, we propose FairTune, a framework to optimise the choice of PEFT parameters with respect to fairness. We demonstrate empirically that FairTune leads to improved fairness on a range of medical imaging datasets.
</details>
<details>
<summary>摘要</summary>
训练模型具有坚固的群体公平性质iels是在伦理敏感的应用领域，如医疗诊断，是非常重要的。despite the growing body of work aiming to minimize demographic bias in AI, this problem remains challenging. A key reason for this challenge is the fairness generalization gap: high-capacity deep learning models can fit all training data nearly perfectly, and thus also exhibit perfect fairness during training. In this case, bias emerges only during testing when generalization performance differs across subgroups. This motivates us to take a bi-level optimization perspective on fair learning: optimizing the learning strategy based on validation fairness. Specifically, we consider the highly effective workflow of adapting pre-trained models to downstream medical imaging tasks using parameter-efficient fine-tuning (PEFT) techniques. There is a trade-off between updating more parameters, enabling a better fit to the task of interest vs. fewer parameters, potentially reducing the generalization gap. To manage this tradeoff, we propose FairTune, a framework to optimize the choice of PEFT parameters with respect to fairness. We demonstrate empirically that FairTune leads to improved fairness on a range of medical imaging datasets.Here's the text with some additional information about the translation:I used the Google Translate API to translate the text into Simplified Chinese. The translation is in the "Simplified Chinese" language setting, which is the standard writing system used in mainland China.Please note that the translation may not be perfect, and there may be some nuances or cultural references that are lost in translation. Additionally, the translation may not be suitable for all audiences, especially in formal or professional settings. It's always a good idea to double-check the translation with a native speaker or a professional translator to ensure accuracy and cultural appropriateness.
</details></li>
</ul>
<hr>
<h2 id="Low-Resolution-Self-Attention-for-Semantic-Segmentation"><a href="#Low-Resolution-Self-Attention-for-Semantic-Segmentation" class="headerlink" title="Low-Resolution Self-Attention for Semantic Segmentation"></a>Low-Resolution Self-Attention for Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05026">http://arxiv.org/abs/2310.05026</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuhuan-wu/LRFormer">https://github.com/yuhuan-wu/LRFormer</a></li>
<li>paper_authors: Yu-Huan Wu, Shi-Chen Zhang, Yun Liu, Le Zhang, Xin Zhan, Daquan Zhou, Jiashi Feng, Ming-Ming Cheng, Liangli Zhen<br>for:LRFormer is designed for semantic segmentation tasks, specifically to improve the efficiency of vision transformers while maintaining performance.methods:LRFormer uses a Low-Resolution Self-Attention (LRSA) mechanism to capture global context at a reduced computational cost, along with 3x3 depth-wise convolutions to capture fine details in the high-resolution space.results:LRFormer outperforms state-of-the-art models on the ADE20K, COCO-Stuff, and Cityscapes datasets.<details>
<summary>Abstract</summary>
Semantic segmentation tasks naturally require high-resolution information for pixel-wise segmentation and global context information for class prediction. While existing vision transformers demonstrate promising performance, they often utilize high resolution context modeling, resulting in a computational bottleneck. In this work, we challenge conventional wisdom and introduce the Low-Resolution Self-Attention (LRSA) mechanism to capture global context at a significantly reduced computational cost. Our approach involves computing self-attention in a fixed low-resolution space regardless of the input image's resolution, with additional 3x3 depth-wise convolutions to capture fine details in the high-resolution space. We demonstrate the effectiveness of our LRSA approach by building the LRFormer, a vision transformer with an encoder-decoder structure. Extensive experiments on the ADE20K, COCO-Stuff, and Cityscapes datasets demonstrate that LRFormer outperforms state-of-the-art models. The code will be made available at https://github.com/yuhuan-wu/LRFormer.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译为简化字符的中文。</SYS>>semantic segmentation任务自然需要高分辨率信息进行像素精度分割和全局上下文信息进行类型预测。而现有的视觉变换器经常使用高分辨率上下文模型，导致计算扰乱。在这项工作中，我们挑战传统的观点，并引入低分辨率自我关注（LRSA）机制，以 capture全局上下文，但是计算成本明显减少。我们的方法是在固定的低分辨率空间内计算自我关注，并在高分辨率空间内进行3x3深度感知 convolution来捕捉细节。我们建立了LRFormer，一个具有encoder-decoder结构的视觉变换器。我们的实验表明，LRFormer在ADE20K、COCO-Stuff和Cityscapes datasets上表现出色，超过了当前最佳模型。代码将在https://github.com/yuhuan-wu/LRFormer上提供。
</details></li>
</ul>
<hr>
<h2 id="Single-Stage-Warped-Cloth-Learning-and-Semantic-Contextual-Attention-Feature-Fusion-for-Virtual-TryOn"><a href="#Single-Stage-Warped-Cloth-Learning-and-Semantic-Contextual-Attention-Feature-Fusion-for-Virtual-TryOn" class="headerlink" title="Single Stage Warped Cloth Learning and Semantic-Contextual Attention Feature Fusion for Virtual TryOn"></a>Single Stage Warped Cloth Learning and Semantic-Contextual Attention Feature Fusion for Virtual TryOn</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05024">http://arxiv.org/abs/2310.05024</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanhita Pathak, Vinay Kaushik, Brejesh Lall</li>
<li>for: 提供一种基于图像的虚拟尝试服装系统，使得用户可以在图像上尝试不同的服装。</li>
<li>methods: 提posed a novel single-stage framework that implicitly learns garment warping and body synthesis from target pose keypoints, using a semantic-contextual fusion attention module and a lightweight linear attention framework to address misalignment and artifacts.</li>
<li>results: 比较 existed methods有更高的效率和质量，提供更可靠和真实的虚拟尝试体验。<details>
<summary>Abstract</summary>
Image-based virtual try-on aims to fit an in-shop garment onto a clothed person image. Garment warping, which aligns the target garment with the corresponding body parts in the person image, is a crucial step in achieving this goal. Existing methods often use multi-stage frameworks to handle clothes warping, person body synthesis and tryon generation separately or rely on noisy intermediate parser-based labels. We propose a novel single-stage framework that implicitly learns the same without explicit multi-stage learning. Our approach utilizes a novel semantic-contextual fusion attention module for garment-person feature fusion, enabling efficient and realistic cloth warping and body synthesis from target pose keypoints. By introducing a lightweight linear attention framework that attends to garment regions and fuses multiple sampled flow fields, we also address misalignment and artifacts present in previous methods. To achieve simultaneous learning of warped garment and try-on results, we introduce a Warped Cloth Learning Module. WCLM uses segmented warped garments as ground truth, operating within a single-stage paradigm. Our proposed approach significantly improves the quality and efficiency of virtual try-on methods, providing users with a more reliable and realistic virtual try-on experience. We evaluate our method on the VITON dataset and demonstrate its state-of-the-art performance in terms of both qualitative and quantitative metrics.
</details>
<details>
<summary>摘要</summary>
文本翻译：图像基于的虚拟试穿目标是将店内衣服适应到披衣人像中的不同姿势。衣服扭曲是实现这个目标的关键步骤，但现有方法frequently使用多个阶段框架来处理衣服扭曲、人体身体合成和试穿生成。我们提出了一种新的单阶段框架，不需要显式的多阶段学习。我们的方法利用了一种新的 semantics-contextual fusion attention模块来拼接衣服和人体特征，从而实现高效和真实的衣服扭曲和人体合成。我们还引入了一种轻量级的线性注意机制，用于衣服区域的注意和多个抽取流场的融合，以解决先前方法中的偏移和零散。为了同时学习扭曲衣服和试穿结果，我们引入了扭曲衣服学习模块（WCLM）。WCLM使用分割后的扭曲衣服作为真实数据，在单阶段框架中进行学习。我们的提议方法可以大幅提高虚拟试穿方法的质量和效率，为用户提供更可靠和更真实的虚拟试穿体验。我们在VITON数据集上进行了评估，并证明了我们的方法在质量和量化指标上具有当前领域的state-of-the-art性。
</details></li>
</ul>
<hr>
<h2 id="Detecting-Abnormal-Health-Conditions-in-Smart-Home-Using-a-Drone"><a href="#Detecting-Abnormal-Health-Conditions-in-Smart-Home-Using-a-Drone" class="headerlink" title="Detecting Abnormal Health Conditions in Smart Home Using a Drone"></a>Detecting Abnormal Health Conditions in Smart Home Using a Drone</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05012">http://arxiv.org/abs/2310.05012</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pronob Kumar Barman</li>
<li>for: 本研究旨在开发一种智能化的跌倒检测系统，以帮助年轻和老年人独立生活。</li>
<li>methods: 该系统使用视觉基于的跌倒监测，通过图像或视频分割和物体检测方法，以实现跌倒的识别。</li>
<li>results: 研究结果表明，该系统可以准确地识别跌倒物体，准确率为0.9948。<details>
<summary>Abstract</summary>
Nowadays, detecting aberrant health issues is a difficult process. Falling, especially among the elderly, is a severe concern worldwide. Falls can result in deadly consequences, including unconsciousness, internal bleeding, and often times, death. A practical and optimal, smart approach of detecting falling is currently a concern. The use of vision-based fall monitoring is becoming more common among scientists as it enables senior citizens and those with other health conditions to live independently. For tracking, surveillance, and rescue, unmanned aerial vehicles use video or image segmentation and object detection methods. The Tello drone is equipped with a camera and with this device we determined normal and abnormal behaviors among our participants. The autonomous falling objects are classified using a convolutional neural network (CNN) classifier. The results demonstrate that the systems can identify falling objects with a precision of 0.9948.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Data-Augmentation-through-Pseudolabels-in-Automatic-Region-Based-Coronary-Artery-Segmentation-for-Disease-Diagnosis"><a href="#Data-Augmentation-through-Pseudolabels-in-Automatic-Region-Based-Coronary-Artery-Segmentation-for-Disease-Diagnosis" class="headerlink" title="Data Augmentation through Pseudolabels in Automatic Region Based Coronary Artery Segmentation for Disease Diagnosis"></a>Data Augmentation through Pseudolabels in Automatic Region Based Coronary Artery Segmentation for Disease Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05990">http://arxiv.org/abs/2310.05990</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sandesh Pokhrel, Sanjay Bhandari, Eduard Vazquez, Yash Raj Shrestha, Binod Bhattarai</li>
<li>for: 诊断心血管疾病（CAD）的准确性和效率有很大提高的需求，但现有的诊断方法往往困难和资源占用。在这种情况下，分割arteries的技术成为一种帮助临床专业人员作出准确诊断的工具。</li>
<li>methods: 本研究使用 pseudolabels 作为数据增强技术，以提高基eline Yolo 模型的性能。</li>
<li>results: 在验证集中，使用 pseudolabels 增强基eline Yolo 模型的 F1 分数提高了 9%，在测试集中提高了 3%。<details>
<summary>Abstract</summary>
Coronary Artery Diseases(CADs) though preventable are one of the leading causes of death and disability. Diagnosis of these diseases is often difficult and resource intensive. Segmentation of arteries in angiographic images has evolved as a tool for assistance, helping clinicians in making accurate diagnosis. However, due to the limited amount of data and the difficulty in curating a dataset, the task of segmentation has proven challenging. In this study, we introduce the idea of using pseudolabels as a data augmentation technique to improve the performance of the baseline Yolo model. This method increases the F1 score of the baseline by 9% in the validation dataset and by 3% in the test dataset.
</details>
<details>
<summary>摘要</summary>
心血管疾病(CAD) 虽可预防，但它是死亡和残疾的主要原因之一。诊断这种疾病的困难和资源占用。artery segmentation in angiographic images has evolved as a tool for assistance, helping clinicians make accurate diagnoses. However, due to limited data and difficulty in curating a dataset, the task of segmentation has proven challenging. In this study, we introduce the idea of using pseudolabels as a data augmentation technique to improve the performance of the baseline Yolo model. This method increases the F1 score of the baseline by 9% in the validation dataset and by 3% in the test dataset.Note: "心血管" (xīn xuè màn) is a shortened form of "心血管疾病" (xīn xuè màn jì bìng), which means "cardiovascular disease" in Chinese.
</details></li>
</ul>
<hr>
<h2 id="Building-an-Open-Vocabulary-Video-CLIP-Model-with-Better-Architectures-Optimization-and-Data"><a href="#Building-an-Open-Vocabulary-Video-CLIP-Model-with-Better-Architectures-Optimization-and-Data" class="headerlink" title="Building an Open-Vocabulary Video CLIP Model with Better Architectures, Optimization and Data"></a>Building an Open-Vocabulary Video CLIP Model with Better Architectures, Optimization and Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05010">http://arxiv.org/abs/2310.05010</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wengzejia1/open-vclip">https://github.com/wengzejia1/open-vclip</a></li>
<li>paper_authors: Zuxuan Wu, Zejia Weng, Wujian Peng, Xitong Yang, Ang Li, Larry S. Davis, Yu-Gang Jiang<br>for:* The paper aims to adapt Contrastive Language-Image Pretraining (CLIP) for zero-shot video recognition, with the goal of identifying novel actions and events in videos.methods:* The proposed method, called Open-VCLIP++, modifies CLIP to capture spatial-temporal relationships in videos, and leverages a technique called Interpolated Weight Optimization to improve generalization.* The method also utilizes large language models to produce fine-grained video descriptions, which are aligned with video features to facilitate a better transfer of CLIP to the video domain.results:* The proposed method achieves zero-shot accuracy scores of 88.1%, 58.7%, and 81.2% on UCF, HMDB, and Kinetics-600 datasets respectively, outperforming the best-performing alternative methods by 8.5%, 8.2%, and 12.3%.* The method also delivers competitive video-to-text and text-to-video retrieval performance on the MSR-VTT video-text retrieval dataset, with substantially less fine-tuning data compared to other methods.<details>
<summary>Abstract</summary>
Despite significant results achieved by Contrastive Language-Image Pretraining (CLIP) in zero-shot image recognition, limited effort has been made exploring its potential for zero-shot video recognition. This paper presents Open-VCLIP++, a simple yet effective framework that adapts CLIP to a strong zero-shot video classifier, capable of identifying novel actions and events during testing. Open-VCLIP++ minimally modifies CLIP to capture spatial-temporal relationships in videos, thereby creating a specialized video classifier while striving for generalization. We formally demonstrate that training Open-VCLIP++ is tantamount to continual learning with zero historical data. To address this problem, we introduce Interpolated Weight Optimization, a technique that leverages the advantages of weight interpolation during both training and testing. Furthermore, we build upon large language models to produce fine-grained video descriptions. These detailed descriptions are further aligned with video features, facilitating a better transfer of CLIP to the video domain. Our approach is evaluated on three widely used action recognition datasets, following a variety of zero-shot evaluation protocols. The results demonstrate that our method surpasses existing state-of-the-art techniques by significant margins. Specifically, we achieve zero-shot accuracy scores of 88.1%, 58.7%, and 81.2% on UCF, HMDB, and Kinetics-600 datasets respectively, outpacing the best-performing alternative methods by 8.5%, 8.2%, and 12.3%. We also evaluate our approach on the MSR-VTT video-text retrieval dataset, where it delivers competitive video-to-text and text-to-video retrieval performance, while utilizing substantially less fine-tuning data compared to other methods. Code is released at https://github.com/wengzejia1/Open-VCLIP.
</details>
<details>
<summary>摘要</summary>
尽管尝试语言图像预训练（CLIP）在零shot图像识别中取得了显著的成果，但是对其在零shot视频识别方面的潜在能力尚未得到了充分的探讨。本文提出了Open-VCLIP++框架，这是一种简单而有效的方法，可以将CLIP adapted into a strong zero-shot video classifier，能够在测试中识别新的动作和事件。Open-VCLIP++只需要微小地修改CLIP，以捕捉视频中的空间-时间关系，从而创造一个特циализирован的视频分类器，同时尽量保持泛化能力。我们正式证明，在训练Open-VCLIP++时， Equivalent to continual learning with zero historical data。为解决这个问题，我们提出了 interpolated weight optimization 技术，该技术利用在训练和测试中 weight interpolation 的优势。此外，我们基于大型自然语言模型来生成细腻的视频描述，这些描述与视频特征进行了更好的匹配，以便更好地将CLIP转移到视频领域。我们的方法在三个常用的动作识别数据集上进行了评估，采用了多种零shot评估协议。结果表明，我们的方法在 UCF、HMDB 和 Kinetics-600 数据集上的零shot精度分别达到了 88.1%、58.7% 和 81.2%，与最佳替代方法相比提高了8.5%、8.2% 和 12.3%。我们还在 MSR-VTT 视频-文本检索数据集上评估了我们的方法，其在视频-文本和文本-视频检索中表现竞争力强，而且使用的微型 fine-tuning 数据量相比其他方法更少。代码可以在 <https://github.com/wengzejia1/Open-VCLIP> 上下载。
</details></li>
</ul>
<hr>
<h2 id="Symmetrical-Linguistic-Feature-Distillation-with-CLIP-for-Scene-Text-Recognition"><a href="#Symmetrical-Linguistic-Feature-Distillation-with-CLIP-for-Scene-Text-Recognition" class="headerlink" title="Symmetrical Linguistic Feature Distillation with CLIP for Scene Text Recognition"></a>Symmetrical Linguistic Feature Distillation with CLIP for Scene Text Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04999">http://arxiv.org/abs/2310.04999</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wzx99/clipocr">https://github.com/wzx99/clipocr</a></li>
<li>paper_authors: Zixiao Wang, Hongtao Xie, Yuxin Wang, Jianjun Xu, Boqiang Zhang, Yongdong Zhang</li>
<li>for: 这种研究旨在探讨CLIP模型在场景文本识别（STR）领域的潜力，并提出了一种新的对称语言特征泵化框架（CLIP-OCR），用于利用CLIP模型中的视觉和语言知识。</li>
<li>methods: 该研究提出了一种对称特征泵化策略（SDS），该策略可以更好地捕捉CLIP模型中的语言知识。具体来说，通过将CLIP图像encoder与反转的CLIP文本encoder串联起来，建立了一个对称结构，该结构包括一个图像到文本的特征流，这个特征流包括了视觉和语言信息。</li>
<li>results: 实验结果表明，CLIP-OCR可以在六个流行的STR benchmark上达到93.8%的平均准确率。<details>
<summary>Abstract</summary>
In this paper, we explore the potential of the Contrastive Language-Image Pretraining (CLIP) model in scene text recognition (STR), and establish a novel Symmetrical Linguistic Feature Distillation framework (named CLIP-OCR) to leverage both visual and linguistic knowledge in CLIP. Different from previous CLIP-based methods mainly considering feature generalization on visual encoding, we propose a symmetrical distillation strategy (SDS) that further captures the linguistic knowledge in the CLIP text encoder. By cascading the CLIP image encoder with the reversed CLIP text encoder, a symmetrical structure is built with an image-to-text feature flow that covers not only visual but also linguistic information for distillation.Benefiting from the natural alignment in CLIP, such guidance flow provides a progressive optimization objective from vision to language, which can supervise the STR feature forwarding process layer-by-layer.Besides, a new Linguistic Consistency Loss (LCL) is proposed to enhance the linguistic capability by considering second-order statistics during the optimization. Overall, CLIP-OCR is the first to design a smooth transition between image and text for the STR task.Extensive experiments demonstrate the effectiveness of CLIP-OCR with 93.8% average accuracy on six popular STR benchmarks.Code will be available at https://github.com/wzx99/CLIPOCR.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们探索了CLIP模型在文本识别（STR）领域的潜力，并提出了一种新的对称语言特征精炼框架（CLIP-OCR），用于利用CLIP模型的视觉和语言知识。与前者CLIP基于方法主要关注视觉编码特征泛化，我们提议一种对称精炼策略（SDS），进一步捕捉CLIP文本编码器中的语言知识。通过将CLIP图像编码器与反转CLIP文本编码器串联起来，建立了一个对称结构，涵盖了图像和文本之间的视觉和语言信息 для精炼。由于CLIP自然的对称性，这种导向流提供了一个逐层优化目标，从视觉到语言，可以超visually guided feature forwarding process layer by layer.此外，我们还提出了一种新的语言一致损失（LCL），以提高语言能力，通过考虑第二阶段统计信息进行优化。总的来说，CLIP-OCR是STR任务中首次设计了图像和文本之间的平滑过渡。我们的实验结果显示，CLIP-OCR在六个流行的STR benchmark上获得了93.8%的平均准确率。代码将在https://github.com/wzx99/CLIPOCR上公开。
</details></li>
</ul>
<hr>
<h2 id="SemST-Semantically-Consistent-Multi-Scale-Image-Translation-via-Structure-Texture-Alignment"><a href="#SemST-Semantically-Consistent-Multi-Scale-Image-Translation-via-Structure-Texture-Alignment" class="headerlink" title="SemST: Semantically Consistent Multi-Scale Image Translation via Structure-Texture Alignment"></a>SemST: Semantically Consistent Multi-Scale Image Translation via Structure-Texture Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04995">http://arxiv.org/abs/2310.04995</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ganning Zhao, Wenhui Cui, Suya You, C. -C. Jay Kuo</li>
<li>for: 本研究旨在提出一种能够维护semantic consistency的无监督图像到图像（I2I）翻译方法，以 Addressing the challenge of content discrepancy in I2I translation.</li>
<li>methods: 本方法使用对比学习和最大化输入和输出之间的共聚信息，以降低semantic distortion。另外，一种多尺度方法也是引入，以提高翻译性能。</li>
<li>results: 实验表明，本方法能够有效地减少semantic distortion，并 achieve state-of-the-art performance。此外，在域适应（DA）中应用SemST也被证明可以作为semantic segmentation任务的有利预处理。<details>
<summary>Abstract</summary>
Unsupervised image-to-image (I2I) translation learns cross-domain image mapping that transfers input from the source domain to output in the target domain while preserving its semantics. One challenge is that different semantic statistics in source and target domains result in content discrepancy known as semantic distortion. To address this problem, a novel I2I method that maintains semantic consistency in translation is proposed and named SemST in this work. SemST reduces semantic distortion by employing contrastive learning and aligning the structural and textural properties of input and output by maximizing their mutual information. Furthermore, a multi-scale approach is introduced to enhance translation performance, thereby enabling the applicability of SemST to domain adaptation in high-resolution images. Experiments show that SemST effectively mitigates semantic distortion and achieves state-of-the-art performance. Also, the application of SemST to domain adaptation (DA) is explored. It is demonstrated by preliminary experiments that SemST can be utilized as a beneficial pre-training for the semantic segmentation task.
</details>
<details>
<summary>摘要</summary>
<<SYS>>设想系统：源领域到目标领域的无监督图像译换学习映射输入源领域的图像到目标领域的图像，保持 Semantics 的含义。一个挑战是源领域和目标领域的含义统计不同，导致内容偏差，即semantic distortion。为解决这个问题，本文提出了一种新的I2I方法，命名为SemST，该方法保持了Semantics的一致性。SemST通过对输入和输出的结构和文本特征进行匹配，使得它们之间的信息共同性最大化，从而减少semantic distortion。此外，本文还提出了一种多尺度方法，以提高译换性能，使得SemST可以应用于高分辨率图像的领域适应。实验表明，SemST可以有效地减少semantic distortion，并达到领域顶尖性能。此外，本文还探讨了SemST的应用于领域适应（DA），并通过初步实验表明，SemST可以作为semantic segmentation任务的有利预训练。
</details></li>
</ul>
<hr>
<h2 id="VisionFM-a-Multi-Modal-Multi-Task-Vision-Foundation-Model-for-Generalist-Ophthalmic-Artificial-Intelligence"><a href="#VisionFM-a-Multi-Modal-Multi-Task-Vision-Foundation-Model-for-Generalist-Ophthalmic-Artificial-Intelligence" class="headerlink" title="VisionFM: a Multi-Modal Multi-Task Vision Foundation Model for Generalist Ophthalmic Artificial Intelligence"></a>VisionFM: a Multi-Modal Multi-Task Vision Foundation Model for Generalist Ophthalmic Artificial Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04992">http://arxiv.org/abs/2310.04992</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianing Qiu, Jian Wu, Hao Wei, Peilun Shi, Minqing Zhang, Yunyun Sun, Lin Li, Hanruo Liu, Hongyi Liu, Simeng Hou, Yuyang Zhao, Xuehui Shi, Junfang Xian, Xiaoxia Qu, Sirui Zhu, Lijie Pan, Xiaoniao Chen, Xiaojia Zhang, Shuai Jiang, Kebing Wang, Chenlong Yang, Mingqiang Chen, Sujie Fan, Jianhua Hu, Aiguo Lv, Hui Miao, Li Guo, Shujun Zhang, Cheng Pei, Xiaojuan Fan, Jianqin Lei, Ting Wei, Junguo Duan, Chun Liu, Xiaobo Xia, Siqi Xiong, Junhong Li, Benny Lo, Yih Chung Tham, Tien Yin Wong, Ningli Wang, Wu Yuan</li>
<li>for: 这个论文是为了开发一个基于340万张眼科图像的基础模型，用于推动多种眼科人工智能应用程序。</li>
<li>methods: 该模型使用了340万张眼科图像，涵盖了各种眼科疾病、图像设备和人口类型，进行预训练。</li>
<li>results: 模型在12种常见眼科疾病的诊断中与专业医生的合作诊断性能相当或更高，并在新的大规模眼科疾病诊断数据集和检测数据集上表现出优异性能。<details>
<summary>Abstract</summary>
We present VisionFM, a foundation model pre-trained with 3.4 million ophthalmic images from 560,457 individuals, covering a broad range of ophthalmic diseases, modalities, imaging devices, and demography. After pre-training, VisionFM provides a foundation to foster multiple ophthalmic artificial intelligence (AI) applications, such as disease screening and diagnosis, disease prognosis, subclassification of disease phenotype, and systemic biomarker and disease prediction, with each application enhanced with expert-level intelligence and accuracy. The generalist intelligence of VisionFM outperformed ophthalmologists with basic and intermediate levels in jointly diagnosing 12 common ophthalmic diseases. Evaluated on a new large-scale ophthalmic disease diagnosis benchmark database, as well as a new large-scale segmentation and detection benchmark database, VisionFM outperformed strong baseline deep neural networks. The ophthalmic image representations learned by VisionFM exhibited noteworthy explainability, and demonstrated strong generalizability to new ophthalmic modalities, disease spectrum, and imaging devices. As a foundation model, VisionFM has a large capacity to learn from diverse ophthalmic imaging data and disparate datasets. To be commensurate with this capacity, in addition to the real data used for pre-training, we also generated and leveraged synthetic ophthalmic imaging data. Experimental results revealed that synthetic data that passed visual Turing tests, can also enhance the representation learning capability of VisionFM, leading to substantial performance gains on downstream ophthalmic AI tasks. Beyond the ophthalmic AI applications developed, validated, and demonstrated in this work, substantial further applications can be achieved in an efficient and cost-effective manner using VisionFM as the foundation.
</details>
<details>
<summary>摘要</summary>
我们介绍VisionFM，一个基础模型，通过340万张眼科图像和560457名个人数据进行预训练，覆盖了广泛的眼科疾病、模式、成像设备和人口学。预训练后，VisionFM提供了一个基础，以推动多种眼科人工智能应用程序，如疾病检测和诊断、疾病诊断、疾病类型分 subclassification和系统生物标志和疾病预测，每个应用程序都受到专家水平的智能和准确性的提高。VisionFM的通用智能超过了基本和中级水平的眼科医生，在共同诊断12种常见眼科疾病方面表现出色。在一个新的大规模眼科疾病诊断benchmark数据集和一个新的大规模分割和检测benchmark数据集上进行评估，VisionFM表现出色，并超越了强基线深度神经网络。眼科图像学习的VisionFM表现出了值得注意的解释性，并在新的眼科模式、疾病谱和成像设备上表现出了强大的普适性。作为基础模型，VisionFM具有大量学习眼科成像数据和多种数据集的能力。为了与这种能力相符，我们不仅使用了实际数据进行预训练，还生成并利用了合理的 synthetic眼科成像数据。实验结果表明，通过visual Turing测试，合理的synthetic数据也可以提高VisionFM的表征学习能力，导致下游眼科人工智能任务的性能提高。除了在本工作中开发、验证和示例的眼科人工智能应用程序外，VisionFM可以在高效和cost-effective的方式实现更多的应用。
</details></li>
</ul>
<hr>
<h2 id="Video-Teller-Enhancing-Cross-Modal-Generation-with-Fusion-and-Decoupling"><a href="#Video-Teller-Enhancing-Cross-Modal-Generation-with-Fusion-and-Decoupling" class="headerlink" title="Video-Teller: Enhancing Cross-Modal Generation with Fusion and Decoupling"></a>Video-Teller: Enhancing Cross-Modal Generation with Fusion and Decoupling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04991">http://arxiv.org/abs/2310.04991</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haogeng Liu, Qihang Fan, Tingkai Liu, Linjie Yang, Yunzhe Tao, Huaibo Huang, Ran He, Hongxia Yang</li>
<li>for: 这篇论文旨在提出一种视频语言基础模型，以便进行视频描述生成任务。</li>
<li>methods: 该模型使用多模态融合和精细的模态对齐来显著提高视频描述生成的效果。它利用冻结预训练的视觉和语言模块，并在描述生成过程中使用大型自然语言模型来生成 concise 和 elaborate 的视频描述。</li>
<li>results: 实验结果表明，该模型可以准确地理解视频内容，并生成 coherent 和精细的语言描述。 fine-grained 模态对齐目标可以提高模型的能力（4% 提高 CIDEr 分数在 MSR-VTT），仅需训练参数增加 13%，并在推理过程中不增加额外成本。<details>
<summary>Abstract</summary>
This paper proposes Video-Teller, a video-language foundation model that leverages multi-modal fusion and fine-grained modality alignment to significantly enhance the video-to-text generation task. Video-Teller boosts the training efficiency by utilizing frozen pretrained vision and language modules. It capitalizes on the robust linguistic capabilities of large language models, enabling the generation of both concise and elaborate video descriptions. To effectively integrate visual and auditory information, Video-Teller builds upon the image-based BLIP-2 model and introduces a cascaded Q-Former which fuses information across frames and ASR texts. To better guide video summarization, we introduce a fine-grained modality alignment objective, where the cascaded Q-Former's output embedding is trained to align with the caption/summary embedding created by a pretrained text auto-encoder. Experimental results demonstrate the efficacy of our proposed video-language foundation model in accurately comprehending videos and generating coherent and precise language descriptions. It is worth noting that the fine-grained alignment enhances the model's capabilities (4% improvement of CIDEr score on MSR-VTT) with only 13% extra parameters in training and zero additional cost in inference.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:这篇论文提出了 Video-Teller，一种基于视频-语言基础模型，通过多modal融合和精细模态对接来备受提高视频到文本生成任务的能力。Video-Teller 通过冻结预训练的视觉和语言模块来提高训练效率。它利用大语言模型的 Robust 语言功能，以便生成 Both concise 和 elaborate 的视频描述。为了有效地 инте格ри视觉和听觉信息，Video-Teller 基于 BLIP-2 模型，并引入一个卷积扩展器，将信息融合到帧和 ASR 文本之间。为了更好地引导视频摘要，我们引入了精细模态对接目标，使得卷积扩展器的输出嵌入与预训练文本自动编码器创建的 Caption/Summary 嵌入进行对接。实验结果表明我们提出的视频语言基础模型在准确理解视频并生成准确和精细的语言描述方面具有很高的能力。值得注意的是，精细对接对模型的能力提高带来了4%的 CIDEr 分数提高（在 MSR-VTT 上），只需要在训练中增加13%的参数，并在推理时间中没有额外的成本。
</details></li>
</ul>
<hr>
<h2 id="Compositional-Semantics-for-Open-Vocabulary-Spatio-semantic-Representations"><a href="#Compositional-Semantics-for-Open-Vocabulary-Spatio-semantic-Representations" class="headerlink" title="Compositional Semantics for Open Vocabulary Spatio-semantic Representations"></a>Compositional Semantics for Open Vocabulary Spatio-semantic Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04981">http://arxiv.org/abs/2310.04981</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robin Karlsson, Francisco Lepe-Salazar, Kazuya Takeda</li>
<li>for: 本研究旨在实现无需人工指令的通用移动机器人，通过大语言模型（LLM）和感知语言模型（VLM）来实现常识世界知识和理性计划。</li>
<li>methods: 本研究提出了幽领结构嵌入〈z<em>〉，实现可询问的空间 semantics 表示。通过数学证明和实验验证，〈z</em>〉可以在任意集Z中找到最佳中心，并且可以由梯度下降优化从视觉外观和单词描述中学习。</li>
<li>results: 实验结果显示，〈z<em>〉可以表示到100个高维度嵌入中的10个 semantics，并且可以提高非关连的开 vocabulary segmentation性能。使用CLIP和SBERT嵌入空间的实验结果显示，一个简单的 dense VLM可以在COCO-Stuff dataset上学习〈z</em>〉，实现181个相互关联的 semantics 的构成。<details>
<summary>Abstract</summary>
General-purpose mobile robots need to complete tasks without exact human instructions. Large language models (LLMs) is a promising direction for realizing commonsense world knowledge and reasoning-based planning. Vision-language models (VLMs) transform environment percepts into vision-language semantics interpretable by LLMs. However, completing complex tasks often requires reasoning about information beyond what is currently perceived. We propose latent compositional semantic embeddings z* as a principled learning-based knowledge representation for queryable spatio-semantic memories. We mathematically prove that z* can always be found, and the optimal z* is the centroid for any set Z. We derive a probabilistic bound for estimating separability of related and unrelated semantics. We prove that z* is discoverable by iterative optimization by gradient descent from visual appearance and singular descriptions. We experimentally verify our findings on four embedding spaces incl. CLIP and SBERT. Our results show that z* can represent up to 10 semantics encoded by SBERT, and up to 100 semantics for ideal uniformly distributed high-dimensional embeddings. We demonstrate that a simple dense VLM trained on the COCO-Stuff dataset can learn z* for 181 overlapping semantics by 42.23 mIoU, while improving conventional non-overlapping open-vocabulary segmentation performance by +3.48 mIoU compared with a popular SOTA model.
</details>
<details>
<summary>摘要</summary>
通用移动机器人需要完成任务无需准确的人类指令。大型语言模型（LLM）是实现通用世界知识和理由预测的可能性的方向。视觉语言模型（VLM）将环境感知转化为可解释的视觉语言 semantics，但完成复杂任务通常需要对现有信息之外的信息进行推理。我们提议使用幽默的 composer semantic embedding z* 作为可学习基于知识表示的原则。我们 математичеamente 证明 z* 总是可以找到，并且最佳 z* 是 Z 集合中的中心。我们 derive 一个 probabilistic bound 用于估计相关和不相关 semantics 之间的分化程度。我们证明 z* 可以通过迭代的梯度下降从视觉特征和 singular descriptions 进行学习。我们在 CLIP 和 SBERT 等四个 embedding space 上进行实验，并证明 z* 可以表示 SBERT 中的 10 个 semantics，以及高维 embedding 中的 100 个 semantics。我们还示出了一个简单的 dense VLM 在 COCO-Stuff 数据集上可以通过 z* 学习 181 个 overlap semantics，而且提高了非 overlap segmentation 性能。
</details></li>
</ul>
<hr>
<h2 id="Learning-Many-to-Many-Mapping-for-Unpaired-Real-World-Image-Super-resolution-and-Downscaling"><a href="#Learning-Many-to-Many-Mapping-for-Unpaired-Real-World-Image-Super-resolution-and-Downscaling" class="headerlink" title="Learning Many-to-Many Mapping for Unpaired Real-World Image Super-resolution and Downscaling"></a>Learning Many-to-Many Mapping for Unpaired Real-World Image Super-resolution and Downscaling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04964">http://arxiv.org/abs/2310.04964</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wanjie Sun, Zhenzhong Chen</li>
<li>for: 这篇论文旨在提出一种不需要对比的单图超解析（SISR）方法，用于处理真实世界中的图像，因为现有的大多数不监督的实世界SISR方法采用了两个阶段训练策略，首先将高分辨率图像转换成低分辨率图像，然后在监督下训练超解析模型。</li>
<li>methods: 该方法提出了一种名为SDFlow的图像下采样和超解析模型，该模型同时学习了 bidirectional 多对多 mapping  между实世界低分辨率图像和高分辨率图像，无需对比。SDFlow 通过分离图像内容和降解信息在幂空间中，使得低分辨率图像和高分辨率图像的内容信息分布在共同的幂空间中匹配。</li>
<li>results: 实验结果表明，SDFlow 可以生成多个真实和可见的低分辨率图像和高分辨率图像，并且能够Quantitatively and qualitatively improve the performance of real-world image super-resolution.<details>
<summary>Abstract</summary>
Learning based single image super-resolution (SISR) for real-world images has been an active research topic yet a challenging task, due to the lack of paired low-resolution (LR) and high-resolution (HR) training images. Most of the existing unsupervised real-world SISR methods adopt a two-stage training strategy by synthesizing realistic LR images from their HR counterparts first, then training the super-resolution (SR) models in a supervised manner. However, the training of image degradation and SR models in this strategy are separate, ignoring the inherent mutual dependency between downscaling and its inverse upscaling process. Additionally, the ill-posed nature of image degradation is not fully considered. In this paper, we propose an image downscaling and SR model dubbed as SDFlow, which simultaneously learns a bidirectional many-to-many mapping between real-world LR and HR images unsupervisedly. The main idea of SDFlow is to decouple image content and degradation information in the latent space, where content information distribution of LR and HR images is matched in a common latent space. Degradation information of the LR images and the high-frequency information of the HR images are fitted to an easy-to-sample conditional distribution. Experimental results on real-world image SR datasets indicate that SDFlow can generate diverse realistic LR and SR images both quantitatively and qualitatively.
</details>
<details>
<summary>摘要</summary>
学习基于单个图像超分辨 (SISR) 对实际世界图像进行研究是一个活跃的研究话题，但是是一个具有挑战性的任务，因为缺乏匹配的低分辨率 (LR) 和高分辨率 (HR) 训练图像。大多数现有的无监督实际世界 SISR 方法采用了两个阶段训练策略，先将实际LR图像Synthesize into HR counterparts，然后在监督性训练SR模型。但是，训练图像减退和其 inverse upscaling 过程中的相互依赖关系未被考虑，同时不完全考虑图像减退的不定性。在这篇论文中，我们提出了一种名为SDFlow的图像减退和SR模型，可以同时学习实际LR和HR图像之间的 bidirectional many-to-many 映射，无需监督。主要思想是在幂空间中分离图像内容和减退信息，LR图像的内容信息和HR图像的高频信息在幂空间中匹配。LR图像的减退信息和HR图像的高频信息被 fitted 到一个易于样本的 conditional distribution。实验结果表明，SDFlow可以生成多样化的实际LR和SR图像，并且具有较高的量化和质量指标。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/08/cs.CV_2023_10_08/" data-id="clp869ty300kzk588d9zf5bz8" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_10_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/08/cs.AI_2023_10_08/" class="article-date">
  <time datetime="2023-10-08T12:00:00.000Z" itemprop="datePublished">2023-10-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/08/cs.AI_2023_10_08/">cs.AI - 2023-10-08</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Optimizing-Solution-Samplers-for-Combinatorial-Problems-The-Landscape-of-Policy-Gradient-Methods"><a href="#Optimizing-Solution-Samplers-for-Combinatorial-Problems-The-Landscape-of-Policy-Gradient-Methods" class="headerlink" title="Optimizing Solution-Samplers for Combinatorial Problems: The Landscape of Policy-Gradient Methods"></a>Optimizing Solution-Samplers for Combinatorial Problems: The Landscape of Policy-Gradient Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05309">http://arxiv.org/abs/2310.05309</a></li>
<li>repo_url: None</li>
<li>paper_authors: Constantine Caramanis, Dimitris Fotakis, Alkis Kalavasis, Vasilis Kontonis, Christos Tzamos</li>
<li>for: 这 paper 的目的是提供一种新的理论框架，用于分析 Deep Neural Networks 和 Reinforcement Learning 方法在解决复杂的 combinatorial 问题时的效果。</li>
<li>methods: 这 paper 使用的方法包括使用 Deep Neural Network 作为解决方案生成器，并通过 gradient-based 方法（例如策略梯度）进行训练，以获得更好的解决方案分布。</li>
<li>results: 本 paper 的主要贡献是提供了一个答案，证明 Deep Neural Networks 和 Reinforcement Learning 方法可以有效地解决 combinatorial 问题，包括 Max- 和 Min-Cut、Max-$k$-CSP、最大权重双向匹配和旅行商问题。此外，这 paper 还介绍了一种新的规范过程，用于改进 vanilla gradient descent，并提供了理论和实验证明，这种方法可以解决消失梯度问题和避免坏的站点点。<details>
<summary>Abstract</summary>
Deep Neural Networks and Reinforcement Learning methods have empirically shown great promise in tackling challenging combinatorial problems. In those methods a deep neural network is used as a solution generator which is then trained by gradient-based methods (e.g., policy gradient) to successively obtain better solution distributions. In this work we introduce a novel theoretical framework for analyzing the effectiveness of such methods. We ask whether there exist generative models that (i) are expressive enough to generate approximately optimal solutions; (ii) have a tractable, i.e, polynomial in the size of the input, number of parameters; (iii) their optimization landscape is benign in the sense that it does not contain sub-optimal stationary points. Our main contribution is a positive answer to this question. Our result holds for a broad class of combinatorial problems including Max- and Min-Cut, Max-$k$-CSP, Maximum-Weight-Bipartite-Matching, and the Traveling Salesman Problem. As a byproduct of our analysis we introduce a novel regularization process over vanilla gradient descent and provide theoretical and experimental evidence that it helps address vanishing-gradient issues and escape bad stationary points.
</details>
<details>
<summary>摘要</summary>
深度神经网络和强化学习方法在解决复杂的 combinatorial 问题方面有广泛的实践经验。在这些方法中，深度神经网络被用作解决生成器，然后通过梯度基本方法（例如策略梯度）进行训练，以逐渐获得更好的解决分布。在这种工作中，我们提出了一个新的理论框架来分析这些方法的效果。我们问题是否存在一些生成模型，满足以下条件：（i）能够生成约等价优的解决方案；（ii） Parameters 的数量是输入数据的线性函数；（iii）优化Landscaper 是柔和的，不含有优化点。我们的主要贡献是给出了一个积极的答案。我们的结果适用于一类复杂的 combinatorial 问题，包括最大批量和最小批量问题、最大-$k$-CSP、最大负载双向匹配和旅行商问题。作为我们的分析的侧重点，我们还提出了一种新的 Regularization 过程，并通过理论和实验证明，它可以帮助解决混合梯度问题和避免坏的站点点。
</details></li>
</ul>
<hr>
<h2 id="Tailoring-Self-Attention-for-Graph-via-Rooted-Subtrees"><a href="#Tailoring-Self-Attention-for-Graph-via-Rooted-Subtrees" class="headerlink" title="Tailoring Self-Attention for Graph via Rooted Subtrees"></a>Tailoring Self-Attention for Graph via Rooted Subtrees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05296">http://arxiv.org/abs/2310.05296</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lumia-group/subtree-attention">https://github.com/lumia-group/subtree-attention</a></li>
<li>paper_authors: Siyuan Huang, Yunchong Song, Jiayue Zhou, Zhouhan Lin</li>
<li>for: 本文旨在提出一种新的多跳图注意机制，以解决现有图注意机制中的局部注意力和全局注意力的缺陷。</li>
<li>methods: 本文提出了一种名为Subtree Attention（STA）的新型多跳图注意机制，具有跨度更强的能力捕捉长距离信息和细腻的地方信息。STA还提供了一种有理证据的修正方法，以保证STA在极端情况下可以近似于全局注意力。</li>
<li>results: 对于十个节点分类 dataset，STA-based模型表现出色，超越现有的图Transformers和主流 GNNs。<details>
<summary>Abstract</summary>
Attention mechanisms have made significant strides in graph learning, yet they still exhibit notable limitations: local attention faces challenges in capturing long-range information due to the inherent problems of the message-passing scheme, while global attention cannot reflect the hierarchical neighborhood structure and fails to capture fine-grained local information. In this paper, we propose a novel multi-hop graph attention mechanism, named Subtree Attention (STA), to address the aforementioned issues. STA seamlessly bridges the fully-attentional structure and the rooted subtree, with theoretical proof that STA approximates the global attention under extreme settings. By allowing direct computation of attention weights among multi-hop neighbors, STA mitigates the inherent problems in existing graph attention mechanisms. Further we devise an efficient form for STA by employing kernelized softmax, which yields a linear time complexity. Our resulting GNN architecture, the STAGNN, presents a simple yet performant STA-based graph neural network leveraging a hop-aware attention strategy. Comprehensive evaluations on ten node classification datasets demonstrate that STA-based models outperform existing graph transformers and mainstream GNNs. The code is available at https://github.com/LUMIA-Group/SubTree-Attention.
</details>
<details>
<summary>摘要</summary>
注意机制在图学习中已经取得了重要进展，但它们仍然存在显著的限制：当地注意力不能够捕捉远程信息，因为消息传递方案的内在问题，而全局注意力则不能够反映层次结构和细化的地方信息。在这篇论文中，我们提出了一种新的多趟图注意机制，名为子树注意（STA），以解决以上问题。STA可以准确地计算多趟邻居之间的注意力权重，并且有理论证明，STA可以在极端情况下近似于全局注意。我们还提出了一种高效的STA实现方式，通过使用核函数软max，实现了线性时间复杂度。我们的结果是一种简单又高性能的STA-基于GNN，称为STAGNN，它利用跳跃注意策略来实现多趟图注意。我们对十个节点分类 datasets进行了广泛的评估，发现STA-based模型比现有的图transformer和主流GNN都有更好的性能。代码可以在https://github.com/LUMIA-Group/SubTree-Attention中下载。
</details></li>
</ul>
<hr>
<h2 id="Generalizable-Error-Modeling-for-Search-Relevance-Data-Annotation-Tasks"><a href="#Generalizable-Error-Modeling-for-Search-Relevance-Data-Annotation-Tasks" class="headerlink" title="Generalizable Error Modeling for Search Relevance Data Annotation Tasks"></a>Generalizable Error Modeling for Search Relevance Data Annotation Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05286">http://arxiv.org/abs/2310.05286</a></li>
<li>repo_url: None</li>
<li>paper_authors: Heinrich Peters, Alireza Hashemi, James Rae</li>
<li>for: 这篇论文旨在提高机器学习和人工智能系统的质量，具体来说是针对搜索 relevance 标注任务进行预测错误模型的建立和评估。</li>
<li>methods: 该论文使用了一种预测错误模型，并在三个产业级 ML 应用（音乐流媒体、视频流媒体、移动应用）中进行了实践。</li>
<li>results: 论文显示了预测错误模型可以在不同应用中具有moderate的模型性能（AUC&#x3D;0.65-0.75），并且该模型在不同应用之间具有良好的泛化性。此外，论文还提供了模型解释分析，以便理解预测错误的主要驱动因素。最后，论文还证明了这种模型在审核中的有用性，可以提高数据标注过程中的效率和质量。<details>
<summary>Abstract</summary>
Human data annotation is critical in shaping the quality of machine learning (ML) and artificial intelligence (AI) systems. One significant challenge in this context is posed by annotation errors, as their effects can degrade the performance of ML models. This paper presents a predictive error model trained to detect potential errors in search relevance annotation tasks for three industry-scale ML applications (music streaming, video streaming, and mobile apps) and assesses its potential to enhance the quality and efficiency of the data annotation process. Drawing on real-world data from an extensive search relevance annotation program, we illustrate that errors can be predicted with moderate model performance (AUC=0.65-0.75) and that model performance generalizes well across applications (i.e., a global, task-agnostic model performs on par with task-specific models). We present model explainability analyses to identify which types of features are the main drivers of predictive performance. Additionally, we demonstrate the usefulness of the model in the context of auditing, where prioritizing tasks with high predicted error probabilities considerably increases the amount of corrected annotation errors (e.g., 40% efficiency gains for the music streaming application). These results underscore that automated error detection models can yield considerable improvements in the efficiency and quality of data annotation processes. Thus, our findings reveal critical insights into effective error management in the data annotation process, thereby contributing to the broader field of human-in-the-loop ML.
</details>
<details>
<summary>摘要</summary>
人工数据标注是机器学习（ML）和人工智能（AI）系统的关键因素。一个重要的挑战在这个上是标注错误，因为它们可以降低ML模型的性能。本文介绍了一个预测错误模型，用于检测搜索相关性标注任务中的可能错误，并评估其在三个产业级ML应用（音乐流媒体、视频流媒体和移动应用）中的可能性。基于广泛的搜索相关性标注计划的实际数据，我们示出了预测错误的能力（AUC=0.65-0.75），并证明模型性能可以通过应用之间进行泛化。我们还提供了模型解释分析，以确定预测性能的主要驱动因素。此外，我们还证明了模型在审核中的用途，可以减少标注错误的效率（例如，音乐流媒体应用中的40%效率提升）。这些结果证明了自动错误检测模型可以提供显著改善数据标注过程的效率和质量。因此，我们的发现对人类在ML过程中的循环提供了重要的洞察，并贡献到更广泛的人类-在-loop ML领域。
</details></li>
</ul>
<hr>
<h2 id="Are-Personalized-Stochastic-Parrots-More-Dangerous-Evaluating-Persona-Biases-in-Dialogue-Systems"><a href="#Are-Personalized-Stochastic-Parrots-More-Dangerous-Evaluating-Persona-Biases-in-Dialogue-Systems" class="headerlink" title="Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems"></a>Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05280">http://arxiv.org/abs/2310.05280</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/uclanlp/persona-biases">https://github.com/uclanlp/persona-biases</a></li>
<li>paper_authors: Yixin Wan, Jieyu Zhao, Aman Chadha, Nanyun Peng, Kai-Wei Chang</li>
<li>for: 这项研究旨在探讨对话系统中使用人物模拟的风险，以及这些风险如何影响对话系统的性能和用户体验。</li>
<li>methods: 本研究使用UNIVERSALPERSONA数据集，对四种不同的对话系统进行了比较，并采用了五种评价指标来评估对话系统中人物模拟的偏见。</li>
<li>results: 研究发现，使用人物模拟在对话系统中存在许多偏见，包括不够尊重和不当的回应，这些偏见可能会对用户造成困惑和不良影响。<details>
<summary>Abstract</summary>
Recent advancements in Large Language Models empower them to follow freeform instructions, including imitating generic or specific demographic personas in conversations. We define generic personas to represent demographic groups, such as "an Asian person", whereas specific personas may take the form of specific popular Asian names like "Yumi". While the adoption of personas enriches user experiences by making dialogue systems more engaging and approachable, it also casts a shadow of potential risk by exacerbating social biases within model responses, thereby causing societal harm through interactions with users. In this paper, we systematically study "persona biases", which we define to be the sensitivity of dialogue models' harmful behaviors contingent upon the personas they adopt. We categorize persona biases into biases in harmful expression and harmful agreement, and establish a comprehensive evaluation framework to measure persona biases in five aspects: Offensiveness, Toxic Continuation, Regard, Stereotype Agreement, and Toxic Agreement. Additionally, we propose to investigate persona biases by experimenting with UNIVERSALPERSONA, a systematically constructed persona dataset encompassing various types of both generic and specific model personas. Through benchmarking on four different models -- including Blender, ChatGPT, Alpaca, and Vicuna -- our study uncovers significant persona biases in dialogue systems. Our findings also underscore the pressing need to revisit the use of personas in dialogue agents to ensure safe application.
</details>
<details>
<summary>摘要</summary>
现代大语言模型可以遵循自由式指令，包括模仿 generic或特定民族人物的对话。我们定义了一些通用的人物类型来表示民族组成部分，例如“一个亚洲人”，而特定的人物可能是具体的受欢迎的亚洲名字“玉米”。虽然采用人物可以增加对话系统的互动性和可接近性，但也可能扩大社会偏见在模型响应中，从而对社会造成伤害。在这篇论文中，我们系统地研究了“人物偏见”，定义为对话模型的危险行为与人物相关的敏感性。我们分类人物偏见为表达偏见和同意偏见，并设计了全面的评价框架来测试人物偏见的五个方面：不礼貌、继续恶势力、尊敬、刻板印象同意和恶势力同意。此外，我们还提出了使用 UNIVERSALPERSONA 系统构建的人物数据集，包括各种通用和特定的模型人物。通过对四种不同的模型（包括 Blender、ChatGPT、Alpaca 和 Vicuna）进行比较，我们的研究发现了对话系统中的人物偏见。我们的发现也警示了对人物的使用以确保安全应用的需要。
</details></li>
</ul>
<hr>
<h2 id="Measuring-reasoning-capabilities-of-ChatGPT"><a href="#Measuring-reasoning-capabilities-of-ChatGPT" class="headerlink" title="Measuring reasoning capabilities of ChatGPT"></a>Measuring reasoning capabilities of ChatGPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05993">http://arxiv.org/abs/2310.05993</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrian Groza<br>for: 这个论文的目的是量化 chatGPT 在逻辑任务中生成的逻辑错误。methods: 作者使用了 chatGPT 解决 144 个逻辑题目，并使用 Prover9 和 Mace4 来验证解决方案。results: 作者发现 chatGPT 只能正确解决 7% 的题目，而 BARD 则可以正确解决 5% 的题目。此外，作者还发现 chatGPT 生成的解决方案中包含了 67 种逻辑错误，平均每个逻辑任务中包含 7 种错误。<details>
<summary>Abstract</summary>
I shall quantify the logical faults generated by ChatGPT when applied to reasoning tasks. For experiments, I use the 144 puzzles from the library \url{https://users.utcluj.ro/~agroza/puzzles/maloga}~\cite{groza:fol}. The library contains puzzles of various types, including arithmetic puzzles, logical equations, Sudoku-like puzzles, zebra-like puzzles, truth-telling puzzles, grid puzzles, strange numbers, or self-reference puzzles. The correct solutions for these puzzles were checked using the theorem prover Prover9~\cite{mccune2005release} and the finite models finder Mace4~\cite{mccune2003mace4} based on human-modelling in Equational First Order Logic. A first output of this study is the benchmark of 100 logical puzzles. For this dataset ChatGPT provided both correct answer and justification for 7\% only. %, while BARD for 5\%. Since the dataset seems challenging, the researchers are invited to test the dataset on more advanced or tuned models than ChatGPT3.5 with more crafted prompts. A second output is the classification of reasoning faults conveyed by ChatGPT. This classification forms a basis for a taxonomy of reasoning faults generated by large language models. I have identified 67 such logical faults, among which: inconsistencies, implication does not hold, unsupported claim, lack of commonsense, wrong justification. The 100 solutions generated by ChatGPT contain 698 logical faults. That is on average, 7 fallacies for each reasoning task. A third ouput is the annotated answers of the ChatGPT with the corresponding logical faults. Each wrong statement within the ChatGPT answer was manually annotated, aiming to quantify the amount of faulty text generated by the language model. On average, 26.03\% from the generated text was a logical fault.
</details>
<details>
<summary>摘要</summary>
A second output is the classification of reasoning faults conveyed by ChatGPT. This classification forms a basis for a taxonomy of reasoning faults generated by large language models. I have identified 67 such logical faults, including inconsistencies, implications that do not hold, unsupported claims, lack of common sense, and wrong justifications. The 100 solutions generated by ChatGPT contain 698 logical faults, averaging 7 fallacies for each reasoning task.A third output is the annotated answers of ChatGPT with the corresponding logical faults. Each wrong statement within the ChatGPT answer was manually annotated to quantify the amount of faulty text generated by the language model. On average, 26.03% of the generated text was found to contain logical faults.Note:[1] Groza, F. (2009). Puzzles for Logical Reasoning. Retrieved from <https://users.utcluj.ro/~agroza/puzzles/maloga>[2] McCune, A. (2005). Prover9: A System for Automatic Theorem Proving. Retrieved from <https://www.cs.umd.edu/projects/prover9/>[3] McCune, A. (2003). Mace4: A System for Automatic Finite Model Generation. Retrieved from <https://www.cs.umd.edu/projects/mace4/>
</details></li>
</ul>
<hr>
<h2 id="Transforming-Pixels-into-a-Masterpiece-AI-Powered-Art-Restoration-using-a-Novel-Distributed-Denoising-CNN-DDCNN"><a href="#Transforming-Pixels-into-a-Masterpiece-AI-Powered-Art-Restoration-using-a-Novel-Distributed-Denoising-CNN-DDCNN" class="headerlink" title="Transforming Pixels into a Masterpiece: AI-Powered Art Restoration using a Novel Distributed Denoising CNN (DDCNN)"></a>Transforming Pixels into a Masterpiece: AI-Powered Art Restoration using a Novel Distributed Denoising CNN (DDCNN)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05270">http://arxiv.org/abs/2310.05270</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sankar B., Mukil Saravanan, Kalaivanan Kumar, Siri Dubbaka</li>
<li>For:  restore deteriorated artworks accurately and efficiently* Methods: 使用深度学习和计算机视觉技术，创造一种基于DDCNN的混合模型，可以根据不同的损害程度和类型进行自适应 restauration* Results: 实验表明，该方法可以有效地纠正损害，并保持细节的精度，提高了艺术品的 restauration 质量，比传统方法有更大的超越<details>
<summary>Abstract</summary>
Art restoration is crucial for preserving cultural heritage, but traditional methods have limitations in faithfully reproducing original artworks while addressing issues like fading, staining, and damage. We present an innovative approach using deep learning, specifically Convolutional Neural Networks (CNNs), and Computer Vision techniques to revolutionize art restoration. We start by creating a diverse dataset of deteriorated art images with various distortions and degradation levels. This dataset trains a Distributed Denoising CNN (DDCNN) to remove distortions while preserving intricate details. Our method is adaptable to different distortion types and levels, making it suitable for various deteriorated artworks, including paintings, sketches, and photographs. Extensive experiments demonstrate our approach's efficiency and effectiveness compared to other Denoising CNN models. We achieve a substantial reduction in distortion, transforming deteriorated artworks into masterpieces. Quantitative evaluations confirm our method's superiority over traditional techniques, reshaping the art restoration field and preserving cultural heritage. In summary, our paper introduces an AI-powered solution that combines Computer Vision and deep learning with DDCNN to restore artworks accurately, overcoming limitations and paving the way for future advancements in art restoration.
</details>
<details>
<summary>摘要</summary>
艺术修复是保护文化遗产的关键，但传统方法有限制，无法准确地复制原始艺术作品，同时解决抹涂、损坏等问题。我们提出了一种创新的方法，使用深度学习技术和计算机视觉技术，以推动艺术修复领域的 револю变。我们开始创建一个多样化的褪色艺术图像数据集，用于训练分布式滤清神经网络（DDCNN），以除掉抹涂而保留细节。我们的方法适用于不同类型和水平的抹涂，可以应用于不同的艺术作品，包括画作、素描和照片。我们的实验证明，我们的方法可以减少抹涂，将褪色艺术作品转化为名画。量化评估表明，我们的方法比传统方法更高效，重新定义艺术修复领域，并为未来的艺术修复领域提供了新的发展方向。总之，我们的论文介绍了一种通过计算机视觉和深度学习技术，使用 DDCNN 恢复艺术作品的准确方法，超越传统技术，开拓了未来艺术修复领域的新途径。
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-A-Cutting-Edge-Survey-of-the-Latest-Advancements-and-Applications"><a href="#Federated-Learning-A-Cutting-Edge-Survey-of-the-Latest-Advancements-and-Applications" class="headerlink" title="Federated Learning: A Cutting-Edge Survey of the Latest Advancements and Applications"></a>Federated Learning: A Cutting-Edge Survey of the Latest Advancements and Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05269">http://arxiv.org/abs/2310.05269</a></li>
<li>repo_url: None</li>
<li>paper_authors: Azim Akhtarshenas, Mohammad Ali Vahedifar, Navid Ayoobi, Behrouz Maham, Tohid Alizadeh, Sina Ebrahimi</li>
<li>for: 这份论文主要是为了探讨联盟学习（Federated Learning，FL）在机器学习系统中实现隐私安全性的可能性和挑战。</li>
<li>methods: 这份论文使用了分布式机器学习（Distributed Machine Learning）和封包技术来实现联盟学习，并且进行了评估和比较现有的FL应用，以评估其效率、精度和隐私保护。</li>
<li>results: 这份论文发现了联盟学习可以实现隐私安全性和成本效益，并且发现了一些未解决的问题和挑战，例如资料权益和安全性、资料分布和资料隐私保护等。<details>
<summary>Abstract</summary>
In the realm of machine learning (ML) systems featuring client-host connections, the enhancement of privacy security can be effectively achieved through federated learning (FL) as a secure distributed ML methodology. FL effectively integrates cloud infrastructure to transfer ML models onto edge servers using blockchain technology. Through this mechanism, it guarantees the streamlined processing and data storage requirements of both centralized and decentralized systems, with an emphasis on scalability, privacy considerations, and cost-effective communication. In current FL implementations, data owners locally train their models, and subsequently upload the outcomes in the form of weights, gradients, and parameters to the cloud for overall model aggregation. This innovation obviates the necessity of engaging Internet of Things (IoT) clients and participants to communicate raw and potentially confidential data directly with a cloud center. This not only reduces the costs associated with communication networks but also enhances the protection of private data. This survey conducts an analysis and comparison of recent FL applications, aiming to assess their efficiency, accuracy, and privacy protection. However, in light of the complex and evolving nature of FL, it becomes evident that additional research is imperative to address lingering knowledge gaps and effectively confront the forthcoming challenges in this field. In this study, we categorize recent literature into the following clusters: privacy protection, resource allocation, case study analysis, and applications. Furthermore, at the end of each section, we tabulate the open areas and future directions presented in the referenced literature, affording researchers and scholars an insightful view of the evolution of the field.
</details>
<details>
<summary>摘要</summary>
在机器学习（ML）系统中，通过联邦学习（FL）可以有效提高隐私安全性。FL可以将云基础设施与边缘服务器集成，通过块链技术实现模型传输。这种机制可以保证中央化和分布式系统之间的流畅处理和数据存储要求，同时强调扩展性、隐私考虑因素和效率沟通。现在的FL实现中，数据所有者在本地训练模型，然后将结果上传到云中进行总模型聚合。这种创新使得无需将互联网物联网（IoT）客户端和参与者直接与云中心进行明文和潜在敏感数据的直接交流，从而降低了通信网络成本并提高了隐私数据的保护。本文对现有的FL应用进行分析和比较，以评估其效率、准确率和隐私保护。然而，随着FL的复杂和不断演化，显然需要进一步的研究，以解决仍存的知识漏洞并有效地应对未来的挑战。在这个研究中，我们将 recens literature into以下类别：隐私保护、资源分配、案例分析和应用。此外，文章结尾附加了每个部分的开放领域和未来方向，为研究人员和学者提供了深入的视野，了解领域的演化。
</details></li>
</ul>
<hr>
<h2 id="A-Knowledge-Graph-Based-Search-Engine-for-Robustly-Finding-Doctors-and-Locations-in-the-Healthcare-Domain"><a href="#A-Knowledge-Graph-Based-Search-Engine-for-Robustly-Finding-Doctors-and-Locations-in-the-Healthcare-Domain" class="headerlink" title="A Knowledge Graph-Based Search Engine for Robustly Finding Doctors and Locations in the Healthcare Domain"></a>A Knowledge Graph-Based Search Engine for Robustly Finding Doctors and Locations in the Healthcare Domain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05258">http://arxiv.org/abs/2310.05258</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mayank Kejriwal, Hamid Haidarian, Min-Hsueh Chiu, Andy Xiang, Deep Shrestha, Faizan Javed</li>
<li>for: 这篇论文是为了解决医疗领域患者找寻医生和位置的搜索问题而写的。</li>
<li>methods: 该论文使用知识图（KG）来结合 semi-structured 数据的感知模型、自然语言处理技术和结构化查询语言 like SPARQL 和 Cypher 来提供强大的搜索引擎体系。</li>
<li>results:  Early results 表明，该方法可以对复杂查询提供明显更高的覆盖率，无需降低质量。<details>
<summary>Abstract</summary>
Efficiently finding doctors and locations is an important search problem for patients in the healthcare domain, for which traditional information retrieval methods tend not to work optimally. In the last ten years, knowledge graphs (KGs) have emerged as a powerful way to combine the benefits of gleaning insights from semi-structured data using semantic modeling, natural language processing techniques like information extraction, and robust querying using structured query languages like SPARQL and Cypher. In this short paper, we present a KG-based search engine architecture for robustly finding doctors and locations in the healthcare domain. Early results demonstrate that our approach can lead to significantly higher coverage for complex queries without degrading quality.
</details>
<details>
<summary>摘要</summary>
Traditional information retrieval methods tend not to work optimally for efficiently finding doctors and locations in the healthcare domain. In the last ten years, knowledge graphs (KGs) have emerged as a powerful way to combine the benefits of gleaning insights from semi-structured data using semantic modeling, natural language processing techniques like information extraction, and robust querying using structured query languages like SPARQL and Cypher. In this short paper, we present a KG-based search engine architecture for robustly finding doctors and locations in the healthcare domain. Early results demonstrate that our approach can lead to significantly higher coverage for complex queries without degrading quality.Here's the text in Traditional Chinese:传统的资讯搜寻方法在医疗领域中不太能够有效率地找到医生和位置。过去十年，知识图表（KGs）已经emerged as a powerful way to combine the benefits of gleaning insights from semi-structured data using semantic modeling, natural language processing techniques like information extraction, and robust querying using structured query languages like SPARQL and Cypher。在这篇短篇论文中，我们呈现了一个基于KG的搜索引擎架构，用于在医疗领域中强健地找到医生和位置。初步结果显示，我们的方法可以导致复杂的查询得到更高的覆盖率，而不会降低品质。
</details></li>
</ul>
<hr>
<h2 id="Persis-A-Persian-Font-Recognition-Pipeline-Using-Convolutional-Neural-Networks"><a href="#Persis-A-Persian-Font-Recognition-Pipeline-Using-Convolutional-Neural-Networks" class="headerlink" title="Persis: A Persian Font Recognition Pipeline Using Convolutional Neural Networks"></a>Persis: A Persian Font Recognition Pipeline Using Convolutional Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05255">http://arxiv.org/abs/2310.05255</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mehrdad-dev/persis">https://github.com/mehrdad-dev/persis</a></li>
<li>paper_authors: Mehrdad Mohammadian, Neda Maleki, Tobias Olsson, Fredrik Ahlgren</li>
<li>for: 这篇论文是为了解决视觉字体识别（VFR）系统中的波斯字体识别问题。</li>
<li>methods: 该论文使用卷积神经网络（CNN）来解决这个问题，并使用了新的公共可用数据集来训练模型。</li>
<li>results: 根据论文的结果，提出的管道可以达到78.0%的顶部准确率，89.1%的IDPL-PFOD数据集准确率，以及94.5%的KAFD数据集准确率。  Additionally, the average time spent in the entire pipeline for one sample of the proposed datasets is 0.54 seconds and 0.017 seconds for CPU and GPU, respectively.<details>
<summary>Abstract</summary>
What happens if we encounter a suitable font for our design work but do not know its name? Visual Font Recognition (VFR) systems are used to identify the font typeface in an image. These systems can assist graphic designers in identifying fonts used in images. A VFR system also aids in improving the speed and accuracy of Optical Character Recognition (OCR) systems. In this paper, we introduce the first publicly available datasets in the field of Persian font recognition and employ Convolutional Neural Networks (CNN) to address this problem. The results show that the proposed pipeline obtained 78.0% top-1 accuracy on our new datasets, 89.1% on the IDPL-PFOD dataset, and 94.5% on the KAFD dataset. Furthermore, the average time spent in the entire pipeline for one sample of our proposed datasets is 0.54 and 0.017 seconds for CPU and GPU, respectively. We conclude that CNN methods can be used to recognize Persian fonts without the need for additional pre-processing steps such as feature extraction, binarization, normalization, etc.
</details>
<details>
<summary>摘要</summary>
如果我们在设计工作中遇到一种适合的字体，但是不知道它的名称，可以使用视觉字体识别（VFR）系统来识别字体类型。这些系统可以帮助图形设计师在图像中识别字体。VFR 系统还可以提高光学字符识别（OCR）系统的速度和准确性。在这篇论文中，我们介绍了字体识别领域的第一个公共可用数据集，并使用卷积神经网络（CNN）解决这个问题。结果显示，我们的提案的管道取得了78.0%的顶部一准确率，89.1%的IDPL-PFOD数据集和94.5%的KAFD数据集。此外，我们的整个管道中对一个样本的平均时间为0.54秒和0.017秒，分别是CPU和GPU上的。我们 conclude 的是，CNN 方法可以用来识别波斯字体，不需要额外的预处理步骤，如特征提取、二进制化、Normalization等。
</details></li>
</ul>
<hr>
<h2 id="Explainable-Claim-Verification-via-Knowledge-Grounded-Reasoning-with-Large-Language-Models"><a href="#Explainable-Claim-Verification-via-Knowledge-Grounded-Reasoning-with-Large-Language-Models" class="headerlink" title="Explainable Claim Verification via Knowledge-Grounded Reasoning with Large Language Models"></a>Explainable Claim Verification via Knowledge-Grounded Reasoning with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05253">http://arxiv.org/abs/2310.05253</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wang2226/folk">https://github.com/wang2226/folk</a></li>
<li>paper_authors: Haoran Wang, Kai Shu</li>
<li>for: 验证宣称的可靠性，对抗虚假信息的扩散。</li>
<li>methods: 使用First-Order-Logic-Guided Knowledge-Grounded (FOLK) Reasoning，无需人工标注数据，可以验证复杂的宣称，并生成可读的解释。</li>
<li>results: 在三个不同的数据集上，FOLK 已经超越强基eline，并且可以提供清晰的解释，帮助人工验证者更好地理解模型的决策过程。<details>
<summary>Abstract</summary>
Claim verification plays a crucial role in combating misinformation. While existing works on claim verification have shown promising results, a crucial piece of the puzzle that remains unsolved is to understand how to verify claims without relying on human-annotated data, which is expensive to create at a large scale. Additionally, it is important for models to provide comprehensive explanations that can justify their decisions and assist human fact-checkers. This paper presents First-Order-Logic-Guided Knowledge-Grounded (FOLK) Reasoning that can verify complex claims and generate explanations without the need for annotated evidence using Large Language Models (LLMs). FOLK leverages the in-context learning ability of LLMs to translate the claim into a First-Order-Logic (FOL) clause consisting of predicates, each corresponding to a sub-claim that needs to be verified. Then, FOLK performs FOL-Guided reasoning over a set of knowledge-grounded question-and-answer pairs to make veracity predictions and generate explanations to justify its decision-making process. This process makes our model highly explanatory, providing clear explanations of its reasoning process in human-readable form. Our experiment results indicate that FOLK outperforms strong baselines on three datasets encompassing various claim verification challenges. Our code and data are available.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>研究人员认为，确认说法的重要作用在抵御谎言中扮演着关键角色。 although existing works on claim verification have shown promising results, a crucial piece of the puzzle that remains unsolved is to understand how to verify claims without relying on expensive human-annotated data. In addition, it is important for models to provide comprehensive explanations that can justify their decisions and assist human fact-checkers. This paper presents First-Order-Logic-Guided Knowledge-Grounded (FOLK) Reasoning, which can verify complex claims and generate explanations without the need for annotated evidence using Large Language Models (LLMs). FOLK leverages the in-context learning ability of LLMs to translate the claim into a First-Order-Logic (FOL) clause consisting of predicates, each corresponding to a sub-claim that needs to be verified. Then, FOLK performs FOL-Guided reasoning over a set of knowledge-grounded question-and-answer pairs to make veracity predictions and generate explanations to justify its decision-making process. This process makes our model highly explanatory, providing clear explanations of its reasoning process in human-readable form. Our experiment results indicate that FOLK outperforms strong baselines on three datasets encompassing various claim verification challenges. Our code and data are available.
</details></li>
</ul>
<hr>
<h2 id="In-Context-Convergence-of-Transformers"><a href="#In-Context-Convergence-of-Transformers" class="headerlink" title="In-Context Convergence of Transformers"></a>In-Context Convergence of Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05249">http://arxiv.org/abs/2310.05249</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Huang, Yuan Cheng, Yingbin Liang</li>
<li>for: 这个论文研究了一层转换器在梯度下降训练下的学习动力学，以便在不需要参数调整的情况下解决未看过的任务。</li>
<li>methods: 该论文使用了梯度下降训练方法，并对一层转换器的软max注意力进行研究。</li>
<li>results: 研究发现，对于具有平衡或不平衡特征的数据，转换器在梯度下降训练下可以在不同阶段达到近Zero预测错误的finite-time收敛保证。<details>
<summary>Abstract</summary>
Transformers have recently revolutionized many domains in modern machine learning and one salient discovery is their remarkable in-context learning capability, where models can solve an unseen task by utilizing task-specific prompts without further parameters fine-tuning. This also inspired recent theoretical studies aiming to understand the in-context learning mechanism of transformers, which however focused only on linear transformers. In this work, we take the first step toward studying the learning dynamics of a one-layer transformer with softmax attention trained via gradient descent in order to in-context learn linear function classes. We consider a structured data model, where each token is randomly sampled from a set of feature vectors in either balanced or imbalanced fashion. For data with balanced features, we establish the finite-time convergence guarantee with near-zero prediction error by navigating our analysis over two phases of the training dynamics of the attention map. More notably, for data with imbalanced features, we show that the learning dynamics take a stage-wise convergence process, where the transformer first converges to a near-zero prediction error for the query tokens of dominant features, and then converges later to a near-zero prediction error for the query tokens of under-represented features, respectively via one and four training phases. Our proof features new techniques for analyzing the competing strengths of two types of attention weights, the change of which determines different training phases.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>现代机器学习中，变换器最近对许多领域进行了革命性的改变，其中一个吸引人的发现是它们在未经参数调整的情况下可以解决未看过的任务，这也激发了最近的理论研究，旨在理解变换器的在场景学习机制。然而，这些研究仅专注于线性变换器。在这项工作中，我们首先研究了一层变换器，通过梯度下降来学习线性函数类型。我们考虑了一种结构化数据模型，其中每个token是随机选择的特征向量集中的一个元素。对于具有平衡特征的数据，我们证明了在 finite-time 内 convergence guarantee，并且预测错误几乎为零。而对于具有不平衡特征的数据，我们显示了一个stage-wise convergence进程，变换器首先对查询符号的主要特征 converge 到 near-zero prediction error，然后在后四个训练阶段 convergence 到 under-represented 特征上的查询符号的 near-zero prediction error。我们的证明利用了一些新的分析技术，以确定不同训练阶段中的关键因素。
</details></li>
</ul>
<hr>
<h2 id="ChatRadio-Valuer-A-Chat-Large-Language-Model-for-Generalizable-Radiology-Report-Generation-Based-on-Multi-institution-and-Multi-system-Data"><a href="#ChatRadio-Valuer-A-Chat-Large-Language-Model-for-Generalizable-Radiology-Report-Generation-Based-on-Multi-institution-and-Multi-system-Data" class="headerlink" title="ChatRadio-Valuer: A Chat Large Language Model for Generalizable Radiology Report Generation Based on Multi-institution and Multi-system Data"></a>ChatRadio-Valuer: A Chat Large Language Model for Generalizable Radiology Report Generation Based on Multi-institution and Multi-system Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05242">http://arxiv.org/abs/2310.05242</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyang Zhong, Wei Zhao, Yutong Zhang, Yi Pan, Peixin Dong, Zuowei Jiang, Xiaoyan Kui, Youlan Shang, Li Yang, Yaonai Wei, Longtao Yang, Hao Chen, Huan Zhao, Yuxiao Liu, Ning Zhu, Yiwei Li, Yisong Wang, Jiaqi Yao, Jiaqi Wang, Ying Zeng, Lei He, Chao Zheng, Zhixue Zhang, Ming Li, Zhengliang Liu, Haixing Dai, Zihao Wu, Lu Zhang, Shu Zhang, Xiaoyan Cai, Xintao Hu, Shijie Zhao, Xi Jiang, Xin Zhang, Xiang Li, Dajiang Zhu, Lei Guo, Dinggang Shen, Junwei Han, Tianming Liu, Jun Liu, Tuo Zhang</li>
<li>For: 这个研究旨在解决医疗影像分析中的报告生成问题，以实现诊断过程中的量化分析。* Methods: 这个研究使用了大型自然语言模型（LLM），发展了一个适应器“ChatRadio-Valuer”，以自动生成医疗影像报告。* Results: 研究结果显示，ChatRadio-Valuer在医疗影像报告中诊断疾病的能力高于现有的模型，特别是与ChatGPT和GPT-4等模型相比。<details>
<summary>Abstract</summary>
Radiology report generation, as a key step in medical image analysis, is critical to the quantitative analysis of clinically informed decision-making levels. However, complex and diverse radiology reports with cross-source heterogeneity pose a huge generalizability challenge to the current methods under massive data volume, mainly because the style and normativity of radiology reports are obviously distinctive among institutions, body regions inspected and radiologists. Recently, the advent of large language models (LLM) offers great potential for recognizing signs of health conditions. To resolve the above problem, we collaborate with the Second Xiangya Hospital in China and propose ChatRadio-Valuer based on the LLM, a tailored model for automatic radiology report generation that learns generalizable representations and provides a basis pattern for model adaptation in sophisticated analysts' cases. Specifically, ChatRadio-Valuer is trained based on the radiology reports from a single institution by means of supervised fine-tuning, and then adapted to disease diagnosis tasks for human multi-system evaluation (i.e., chest, abdomen, muscle-skeleton, head, and maxillofacial $\&$ neck) from six different institutions in clinical-level events. The clinical dataset utilized in this study encompasses a remarkable total of \textbf{332,673} observations. From the comprehensive results on engineering indicators, clinical efficacy and deployment cost metrics, it can be shown that ChatRadio-Valuer consistently outperforms state-of-the-art models, especially ChatGPT (GPT-3.5-Turbo) and GPT-4 et al., in terms of the diseases diagnosis from radiology reports. ChatRadio-Valuer provides an effective avenue to boost model generalization performance and alleviate the annotation workload of experts to enable the promotion of clinical AI applications in radiology reports.
</details>
<details>
<summary>摘要</summary>
医学影像分析中的 radiology 报告生成是医疗决策中的关键步骤，但是复杂和多样的 radiology 报告带有跨源差异性，对当前方法来说是一个巨大普适性挑战。这是因为 radiology 报告的风格和标准性在不同机构、身体区域和 radiologist 之间存在显著差异。然而，最近的大语言模型（LLM）的出现带来了识别健康状况的潜在可能性。为解决这个问题，我们与中国第二医学院合作，并提出了基于 LLM 的 ChatRadio-Valuer 自动 radiology 报告生成模型，该模型学习普适表示和提供基本模式 для模型适应复杂分析员的情况。具体来说，ChatRadio-Valuer 通过单机构的 radiology 报告进行监督微调训练，然后在多个机构的疾病诊断任务中进行人类多系统评估。这些临床数据的总量为 \textbf{332,673} 个观察。根据工程指标、临床效果和部署成本度量，可以看出，ChatRadio-Valuer 在疾病诊断方面与现有模型，特别是 ChatGPT（GPT-3.5-Turbo）和 GPT-4 等模型，表现出色，尤其是在 radiology 报告中诊断疾病。ChatRadio-Valuer 为临床 AI 应用提供了一个有效的通路，以提高模型普适性性和减轻专家的标注工作负担，以便推动临床 AI 应用的普及。
</details></li>
</ul>
<hr>
<h2 id="MindfulDiary-Harnessing-Large-Language-Model-to-Support-Psychiatric-Patients’-Journaling"><a href="#MindfulDiary-Harnessing-Large-Language-Model-to-Support-Psychiatric-Patients’-Journaling" class="headerlink" title="MindfulDiary: Harnessing Large Language Model to Support Psychiatric Patients’ Journaling"></a>MindfulDiary: Harnessing Large Language Model to Support Psychiatric Patients’ Journaling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05231">http://arxiv.org/abs/2310.05231</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taewan Kim, Seolyeong Bae, Hyun Ah Kim, Su-woo Lee, Hwajung Hong, Chanmo Yang, Young-Ho Kim</li>
<li>for: 帮助心理病人每天记录经验，并帮助心理医生更好地理解患者的思想和日常情境。</li>
<li>methods: 使用大语言模型（LLM）和移动应用程序，实现了患者每天的自由对话记录，并遵循专业指导方针。</li>
<li>results: 经四周的场景研究，发现 MindfulDiary 可以帮助患者日常记录更详细和系统化，同时帮助心理医生更好地理解患者的思想和日常情境，有助于提高心理医疗效果。<details>
<summary>Abstract</summary>
In the mental health domain, Large Language Models (LLMs) offer promising new opportunities, though their inherent complexity and low controllability have raised questions about their suitability in clinical settings. We present MindfulDiary, a mobile journaling app incorporating an LLM to help psychiatric patients document daily experiences through conversation. Designed in collaboration with mental health professionals (MHPs), MindfulDiary takes a state-based approach to safely comply with the experts' guidelines while carrying on free-form conversations. Through a four-week field study involving 28 patients with major depressive disorder and five psychiatrists, we found that MindfulDiary supported patients in consistently enriching their daily records and helped psychiatrists better empathize with their patients through an understanding of their thoughts and daily contexts. Drawing on these findings, we discuss the implications of leveraging LLMs in the mental health domain, bridging the technical feasibility and their integration into clinical settings.
</details>
<details>
<summary>摘要</summary>
在心理健康领域，大型自然语言模型（LLM）提供了新的机遇，但其内置的复杂性和控制性问题引起了许多关于其在临床设置中适用性的问题。我们介绍了一款名为 MindfulDiary的移动日记应用程序，该应用程序通过与心理医生（MHP）合作，使用 LLM 帮助心理病人每天记录他们的经验。我们在28名主观抑郁症患者和5名心理医生参与的四周实验中发现，MindfulDiary 可以帮助患者日常记录更加详细，并帮助心理医生更好地理解他们的患者的思想和日常背景。根据这些发现，我们讨论了在心理健康领域利用 LLM 的意义，把技术可行性和其在临床设置中的集成相结合。
</details></li>
</ul>
<hr>
<h2 id="Physics-aware-Machine-Learning-Revolutionizes-Scientific-Paradigm-for-Machine-Learning-and-Process-based-Hydrology"><a href="#Physics-aware-Machine-Learning-Revolutionizes-Scientific-Paradigm-for-Machine-Learning-and-Process-based-Hydrology" class="headerlink" title="Physics-aware Machine Learning Revolutionizes Scientific Paradigm for Machine Learning and Process-based Hydrology"></a>Physics-aware Machine Learning Revolutionizes Scientific Paradigm for Machine Learning and Process-based Hydrology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05227">http://arxiv.org/abs/2310.05227</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qingsong Xu, Yilei Shi, Jonathan Bamber, Ye Tuo, Ralf Ludwig, Xiao Xiang Zhu<br>for:physics-aware machine learning (PaML) is introduced as a transformative approach to overcome the barrier between hydrology and machine learning, and to revolutionize both fields.methods:the review includes a comprehensive analysis of existing PaML methodologies that integrate prior physical knowledge or physics-based modeling into machine learning, including physical data-guided ML, physics-informed ML, physics-embedded ML, and physics-aware hybrid learning.results:the review highlights the most promising and challenging directions for different objectives and PaML methods in hydrology, including rainfall-runoff hydrological processes and hydrodynamic processes. Additionally, a new PaML-based hydrology platform, termed HydroPML, is released as a foundation for hydrological applications, which enhances the explainability and causality of machine learning and lays the groundwork for the digital water cycle’s realization.<details>
<summary>Abstract</summary>
Accurate hydrological understanding and water cycle prediction are crucial for addressing scientific and societal challenges associated with the management of water resources, particularly under the dynamic influence of anthropogenic climate change. Existing reviews predominantly concentrate on the development of machine learning (ML) in this field, yet there is a clear distinction between hydrology and ML as separate paradigms. Here, we introduce physics-aware ML as a transformative approach to overcome the perceived barrier and revolutionize both fields. Specifically, we present a comprehensive review of the physics-aware ML methods, building a structured community (PaML) of existing methodologies that integrate prior physical knowledge or physics-based modeling into ML. We systematically analyze these PaML methodologies with respect to four aspects: physical data-guided ML, physics-informed ML, physics-embedded ML, and physics-aware hybrid learning. PaML facilitates ML-aided hypotheses, accelerating insights from big data and fostering scientific discoveries. We first conduct a systematic review of hydrology in PaML, including rainfall-runoff hydrological processes and hydrodynamic processes, and highlight the most promising and challenging directions for different objectives and PaML methods. Finally, a new PaML-based hydrology platform, termed HydroPML, is released as a foundation for hydrological applications. HydroPML enhances the explainability and causality of ML and lays the groundwork for the digital water cycle's realization. The HydroPML platform is publicly available at https://hydropml.github.io/.
</details>
<details>
<summary>摘要</summary>
Accurate hydrological understanding和水ecycle prediction是管理水资源的科学和社会挑战中的关键，尤其是在人类活动导致的气候变化的影响下。现有的评论主要集中在机器学习（ML）的发展中，但是有一个明确的分界线：水文和ML为两个不同的思维框架。我们介绍了一种将物理知识integrated into ML的新方法，即物理意识ML（PaML），以超越这一障碍并重塑两个领域。我们对PaML方法进行了系统性的分析，包括物理数据驱动ML、物理信息驱动ML、物理嵌入ML和物理意识混合学习。PaML方法可以加速大数据的学习和探索，并促进科学发现。我们首先对PaML在水文领域进行了系统性的评论，包括雨水径流过程和 hidrodynamic过程，并将不同目标和PaML方法中最有前途和挑战的方向 highlighted。最后，我们发布了一个基于PaML的水文平台，称为HydroPML，以提高ML的解释性和因果关系，并为数字水ecycle的实现奠定基础。HydroPML平台publicly available at <https://hydropml.github.io/>。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Semiotics-Networks-Representing-Awareness"><a href="#Interpretable-Semiotics-Networks-Representing-Awareness" class="headerlink" title="Interpretable Semiotics Networks Representing Awareness"></a>Interpretable Semiotics Networks Representing Awareness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05212">http://arxiv.org/abs/2310.05212</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Kupeev, Eyal Nitcany</li>
<li>for: 这个论文描述了一种计算模型，用于跟踪和模拟人类对物体的感知和communication中的表达。</li>
<li>methods: 该模型包括两个关键组件（’observed’和’seen’），与计算机视觉术语(‘encoding’和’decoding’)相关。这些元素结合形成了 semiotic networks，用于模拟人类对物体的感知和communication中的意识。</li>
<li>results: 作者在多个实验中证明了这个模型的可见性，并且在小训练数据集上，该模型的复合网络超过了单独的分类网络的性能。未来的工作将利用这个模型，以更好地理解人类communication和个人表达。<details>
<summary>Abstract</summary>
Humans perceive objects daily and communicate their perceptions using various channels. Here, we describe a computational model that track and simulate objects' perception, and their representations as they pass in communication.   We describe two key components of our internal representation ('observed' and 'seen') and relate them to familiar computer vision terms (encoding and decoding). These elements joined together to form semiotic networks, which simulate awareness in object perception and human communication.   Nowadays, most neural networks are uninterpretable. On the other hand, our model is free from this disadvantages. We performed several experiments and demonstrated the visibility of our model.   We describe how our network may be used as preprocessing unit to any classification network. In our experiments the compound network overperforms in average the classification network at datasets with small training data.   Future work would leverage our model to gain better understanding of human communications and personal representations.
</details>
<details>
<summary>摘要</summary>
人们日常接触物体，通过不同的渠道传达自己的感知。我们描述了一种计算模型，可以跟踪和模拟物体的感知和表达，以及它们在交流中的表现。我们描述了两个关键组成部分('观察'和'看到')，与 familar computer vision terms（编码和解码）相关。这些元素结合形成了 semiotic networks，可以模拟人类对物体感知和communication的意识。现在，大多数神经网络都是不可解释的。然而，我们的模型免受这些缺点。我们进行了多个实验，并证明了我们的模型的可见性。我们描述了如何使用我们的网络作为任何分类网络的预处理单元，并在实验中发现了compound network在小训练数据集上的超越性。未来的工作将利用我们的模型，更好地理解人类通信和个人表示。
</details></li>
</ul>
<hr>
<h2 id="TILFA-A-Unified-Framework-for-Text-Image-and-Layout-Fusion-in-Argument-Mining"><a href="#TILFA-A-Unified-Framework-for-Text-Image-and-Layout-Fusion-in-Argument-Mining" class="headerlink" title="TILFA: A Unified Framework for Text, Image, and Layout Fusion in Argument Mining"></a>TILFA: A Unified Framework for Text, Image, and Layout Fusion in Argument Mining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05210">http://arxiv.org/abs/2310.05210</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hkust-knowcomp/tilfa">https://github.com/hkust-knowcomp/tilfa</a></li>
<li>paper_authors: Qing Zong, Zhaowei Wang, Baixuan Xu, Tianshi Zheng, Haochen Shi, Weiqi Wang, Yangqiu Song, Ginny Y. Wong, Simon See</li>
<li>for: 本研究旨在分析作者的立场（Argument Mining）</li>
<li>methods: 该研究使用了一种新的框架—TILFA（约文本、图像和布局融合框架），可以处理混合数据（文本和图像），并且可以理解文本以及检测图像中的光学字符和布局细节</li>
<li>results: 该模型在Argumentative Stance Classification子 зада务中显著超过了现有的基eline，为知识共享（KnowComp）团队赢得了第一名<details>
<summary>Abstract</summary>
A main goal of Argument Mining (AM) is to analyze an author's stance. Unlike previous AM datasets focusing only on text, the shared task at the 10th Workshop on Argument Mining introduces a dataset including both text and images. Importantly, these images contain both visual elements and optical characters. Our new framework, TILFA (A Unified Framework for Text, Image, and Layout Fusion in Argument Mining), is designed to handle this mixed data. It excels at not only understanding text but also detecting optical characters and recognizing layout details in images. Our model significantly outperforms existing baselines, earning our team, KnowComp, the 1st place in the leaderboard of Argumentative Stance Classification subtask in this shared task.
</details>
<details>
<summary>摘要</summary>
主要目标之一的Argument Mining（AM）是分析作者的态度。与过去的AM数据集仅专注于文本的情况不同，这个共同任务在10个Argument Mining工作坊中引入了包括文本和图像的数据集。重要的是，这些图像包含视觉元素和光学字符。我们的新框架TILFA（文本、图像和布局融合在Argument Mining中的一体化框架）针对这种混合数据进行处理。它不仅能够理解文本，还能探测光学字符和图像中的布局细节。我们的模型在Argumentative Stance Classification子任务中显著超越了现有的基线，让我们的团队 KnowComp 在领导板块中获得第一名。
</details></li>
</ul>
<hr>
<h2 id="Scaling-Laws-of-RoPE-based-Extrapolation"><a href="#Scaling-Laws-of-RoPE-based-Extrapolation" class="headerlink" title="Scaling Laws of RoPE-based Extrapolation"></a>Scaling Laws of RoPE-based Extrapolation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05209">http://arxiv.org/abs/2310.05209</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/OpenLMLab/scaling-rope">https://github.com/OpenLMLab/scaling-rope</a></li>
<li>paper_authors: Xiaoran Liu, Hang Yan, Shuo Zhang, Chenxin An, Xipeng Qiu, Dahua Lin</li>
<li>for: 本文主要研究了基于Rotary Position Embedding（RoPE）的大型自然语言模型（LLM）的推断能力。</li>
<li>methods: 本文提出了一种基于RoPE的推断方法，包括修改RoPE的基数和提供长文本练习。</li>
<li>results: 本文在16K训练长度下，通过调整RoPE的基数和练习文本长度，实现了在1000000上下文长度内的推断。同时，本文还提出了一种periodic perspective下的扩展法则，以描述推断性能与基数和练习文本长度之间的关系。<details>
<summary>Abstract</summary>
The extrapolation capability of Large Language Models (LLMs) based on Rotary Position Embedding is currently a topic of considerable interest. The mainstream approach to addressing extrapolation with LLMs involves modifying RoPE by replacing 10000, the rotary base of $\theta_n={10000}^{-2n/d}$ in the original RoPE, with a larger value and providing longer fine-tuning text. In this work, we first observe that fine-tuning a RoPE-based LLM with either a smaller or larger base in pre-training context length could significantly enhance its extrapolation performance. After that, we propose \textbf{\textit{Scaling Laws of RoPE-based Extrapolation}, a unified framework from the periodic perspective, to describe the relationship between the extrapolation performance and base value as well as tuning context length. In this process, we also explain the origin of the RoPE-based extrapolation issue by \textbf{\textit{critical dimension for extrapolation}. Besides these observations and analyses, we achieve extrapolation up to 1 million context length within only 16K training length on LLaMA2 7B and 13B.
</details>
<details>
<summary>摘要</summary>
Currently, the ability of Large Language Models (LLMs) to extrapolate is a topic of great interest. The mainstream approach to improving extrapolation with LLMs is to modify Rotary Position Embedding (RoPE) by replacing the rotary base of $\theta_n={10000}^{-2n/d}$ with a larger value and providing longer fine-tuning text. In this study, we find that fine-tuning a RoPE-based LLM with a smaller or larger base in the pre-training context length can significantly enhance its extrapolation performance. We then propose the \textbf{\textit{Scaling Laws of RoPE-based Extrapolation}, a unified framework from a periodic perspective, to describe the relationship between the extrapolation performance and base value as well as tuning context length. In this process, we also explain the origin of the RoPE-based extrapolation issue by \textbf{\textit{critical dimension for extrapolation}. Furthermore, we achieve extrapolation up to 1 million context length within only 16K training length on LLaMA2 7B and 13B.Note:* "Rotary Position Embedding" (RoPE) is translated as "旋转位嵌入" (Fánzàng wèi yù) in Simplified Chinese.* "Large Language Models" (LLMs) is translated as "大型语言模型" (dàxí yǔyán módelǐ) in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Quantifying-Zero-shot-Coordination-Capability-with-Behavior-Preferring-Partners"><a href="#Quantifying-Zero-shot-Coordination-Capability-with-Behavior-Preferring-Partners" class="headerlink" title="Quantifying Zero-shot Coordination Capability with Behavior Preferring Partners"></a>Quantifying Zero-shot Coordination Capability with Behavior Preferring Partners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05208">http://arxiv.org/abs/2310.05208</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xihuai Wang, Shao Zhang, Wenhao Zhang, Wentao Dong, Jingxiao Chen, Ying Wen, Weinan Zhang</li>
<li>for: 评估 Zero-shot coordination（ZSC）能力的可靠、全面和高效的评估方法。</li>
<li>methods: 提出了一种基于“理想完整”评估伙伴的评估方法，包括构建“完整”评估伙伴和Multi-dimensional度量指标BR-Prox。</li>
<li>results: 使用提出的评估方法重新评估了强大的ZSC方法在Overcooked环境中的性能，结果显示一些最常用的布局下，不同ZSC方法的性能差异不明显。此外，评估的ZSC方法需要生成更多和更高性能的训练伙伴。<details>
<summary>Abstract</summary>
Zero-shot coordination (ZSC) is a new challenge focusing on generalizing learned coordination skills to unseen partners. Existing methods train the ego agent with partners from pre-trained or evolving populations. The agent's ZSC capability is typically evaluated with a few evaluation partners, including human and agent, and reported by mean returns. Current evaluation methods for ZSC capability still need to improve in constructing diverse evaluation partners and comprehensively measuring the ZSC capability. We aim to create a reliable, comprehensive, and efficient evaluation method for ZSC capability. We formally define the ideal 'diversity-complete' evaluation partners and propose the best response (BR) diversity, which is the population diversity of the BRs to the partners, to approximate the ideal evaluation partners. We propose an evaluation workflow including 'diversity-complete' evaluation partners construction and a multi-dimensional metric, the Best Response Proximity (BR-Prox) metric. BR-Prox quantifies the ZSC capability as the performance similarity to each evaluation partner's approximate best response, demonstrating generalization capability and improvement potential. We re-evaluate strong ZSC methods in the Overcooked environment using the proposed evaluation workflow. Surprisingly, the results in some of the most used layouts fail to distinguish the performance of different ZSC methods. Moreover, the evaluated ZSC methods must produce more diverse and high-performing training partners. Our proposed evaluation workflow calls for a change in how we efficiently evaluate ZSC methods as a supplement to human evaluation.
</details>
<details>
<summary>摘要</summary>
Zero-shot coordination (ZSC) 是一个新的挑战，旨在将已经学习的协调技能应用到未见过的伙伴上。现有的方法将自己作为主体Agent训练的伙伴来自预先训练或进化的人类和机器人 population。主体Agent的 ZSC 能力通常是通过一些评估伙伴，包括人类和机器人，并由平均回应报告。现有的评估方法 для ZSC 能力仍然需要改进，以建立多样化的评估伙伴和全面地衡量 ZSC 能力。我们希望创建一个可靠、全面和高效的评估方法。我们正式定义了理想的 '多样化完整' 评估伙伴，并提出了最佳回应多样性（BR 多样性），它是评估伙伴的 Population 多样性的最佳回应。我们提出了一个评估工作流程，包括 '多样化完整' 评估伙伴的建构和多维度度量，即最佳回应距离度量（BR-Prox）。BR-Prox 量化 ZSC 能力为对每个评估伙伴的近似最佳回应的性能相似度，显示了扩展性和改进潜力。我们在 Overcooked 环境中重新评估了强大 ZSC 方法，结果显示，在一些最常用的布局中，不能区分不同 ZSC 方法的表现。此外，评估 ZSC 方法的伙伴必须生成更多和更高性能的训练伙伴。我们的提出的评估工作流程将对 ZSC 方法的评估作为补充 human 评估。
</details></li>
</ul>
<hr>
<h2 id="Boosting-Facial-Action-Unit-Detection-Through-Jointly-Learning-Facial-Landmark-Detection-and-Domain-Separation-and-Reconstruction"><a href="#Boosting-Facial-Action-Unit-Detection-Through-Jointly-Learning-Facial-Landmark-Detection-and-Domain-Separation-and-Reconstruction" class="headerlink" title="Boosting Facial Action Unit Detection Through Jointly Learning Facial Landmark Detection and Domain Separation and Reconstruction"></a>Boosting Facial Action Unit Detection Through Jointly Learning Facial Landmark Detection and Domain Separation and Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05207">http://arxiv.org/abs/2310.05207</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziqiao Shang, Li Yu</li>
<li>for: 这篇研究旨在提出一个新的 facial action unit (AU) 检测框架，以便在无标的面部图像中进行supervised检测。</li>
<li>methods: 这篇研究使用多任务学习，将AU领域分类和重建、面部标志检测共享同structural facial extraction模组的 Parameters。此外，提出了一个基于对照学习的新Feature alignment方案，加入了四个中途supervisors，以促进特征重建过程。</li>
<li>results: 实验结果显示，该方法在两个benchmark上具有较高的精度和稳定性，较之前所有方法有所提高。<details>
<summary>Abstract</summary>
Recently how to introduce large amounts of unlabeled facial images in the wild into supervised Facial Action Unit (AU) detection frameworks has become a challenging problem. In this paper, we propose a new AU detection framework where multi-task learning is introduced to jointly learn AU domain separation and reconstruction and facial landmark detection by sharing the parameters of homostructural facial extraction modules. In addition, we propose a new feature alignment scheme based on contrastive learning by simple projectors and an improved contrastive loss, which adds four additional intermediate supervisors to promote the feature reconstruction process. Experimental results on two benchmarks demonstrate our superiority against the state-of-the-art methods for AU detection in the wild.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="GEAR-A-GPU-Centric-Experience-Replay-System-for-Large-Reinforcement-Learning-Models"><a href="#GEAR-A-GPU-Centric-Experience-Replay-System-for-Large-Reinforcement-Learning-Models" class="headerlink" title="GEAR: A GPU-Centric Experience Replay System for Large Reinforcement Learning Models"></a>GEAR: A GPU-Centric Experience Replay System for Large Reinforcement Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05205">http://arxiv.org/abs/2310.05205</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bigrl-team/gear">https://github.com/bigrl-team/gear</a></li>
<li>paper_authors: Hanjing Wang, Man-Kit Sit, Congjie He, Ying Wen, Weinan Zhang, Jun Wang, Yaodong Yang, Luo Mai</li>
<li>for: 这篇论文旨在开发一种分布式、GPU-中心的经验回忆系统（GEAR），用于执行扩展的强化学习（RL），并使用大 sequences 模型（如 transformers）。</li>
<li>methods: GEAR 使用了一种优化的内存管理策略，使得 GPU 服务器的内存资源（包括主机内存和设备内存）可以有效地管理经验数据。此外，它还实现了分布式的 GPU 设备来快速执行不同的经验选择策略，从而缓解计算瓶颈。 GEAR 还使用了 GPU 加速器来收集经验数据，并使用零复制访问主机内存和远程指定内存访问来提高通信效率。</li>
<li>results: 根据集群实验结果，GEAR 可以与 Reverb 相比，在训练 state-of-the-art 大 RL 模型时达到6倍的性能水平。<details>
<summary>Abstract</summary>
This paper introduces a distributed, GPU-centric experience replay system, GEAR, designed to perform scalable reinforcement learning (RL) with large sequence models (such as transformers). With such models, existing systems such as Reverb face considerable bottlenecks in memory, computation, and communication. GEAR, however, optimizes memory efficiency by enabling the memory resources on GPU servers (including host memory and device memory) to manage trajectory data. Furthermore, it facilitates decentralized GPU devices to expedite various trajectory selection strategies, circumventing computational bottlenecks. GEAR is equipped with GPU kernels capable of collecting trajectories using zero-copy access to host memory, along with remote-directed-memory access over InfiniBand, improving communication efficiency. Cluster experiments have shown that GEAR can achieve performance levels up to 6x greater than Reverb when training state-of-the-art large RL models. GEAR is open-sourced at https://github.com/bigrl-team/gear.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文介绍了一种分布式、GPU中心的经验回放系统GEAR，用于执行可扩展的 reinforcement learning（RL），并且可以使用大型序列模型（如转换器）。现有的系统如Reverb，在内存、计算和通信方面都会遇到严重的瓶颈。然而，GEAR通过启用 GPU 服务器上的内存资源（包括主机内存和设备内存）来管理轨迹数据，从而提高了内存效率。此外，它还可以让分布式的 GPU 设备优先级化不同的轨迹选择策略，以避免计算瓶颈。GEAR 具有可收集轨迹的 GPU kernels，使用零复制访问主机内存，以及通过 InfiniBand 进行远程指定内存访问，以提高通信效率。在分布式集群实验中，GEAR 可以与 Reverb 训练state-of-the-art 大型 RL 模型时， achieve 性能水平高达 6 倍。GEAR 开源在 <https://github.com/bigrl-team/gear>。
</details></li>
</ul>
<hr>
<h2 id="GMMFormer-Gaussian-Mixture-Model-based-Transformer-for-Efficient-Partially-Relevant-Video-Retrieval"><a href="#GMMFormer-Gaussian-Mixture-Model-based-Transformer-for-Efficient-Partially-Relevant-Video-Retrieval" class="headerlink" title="GMMFormer: Gaussian-Mixture-Model based Transformer for Efficient Partially Relevant Video Retrieval"></a>GMMFormer: Gaussian-Mixture-Model based Transformer for Efficient Partially Relevant Video Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05195">http://arxiv.org/abs/2310.05195</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuting Wang, Jinpeng Wang, Bin Chen, Ziyun Zeng, Shu-Tao Xia</li>
<li>For: This paper is written for partially relevant video retrieval (PRVR), which aims to find untrimmed videos containing pertinent moments in a database.* Methods: The paper proposes a novel method called GMMFormer, which models clip representations implicitly using a Gaussian-Mixture-Model (GMM) and Transformer architecture. The method incorporates Gaussian-Mixture-Model constraints during frame interactions to focus each frame on its adjacent frames, generating representations that contain multi-scale clip information.* Results: The paper demonstrates the superiority and efficiency of GMMFormer through extensive experiments on three large-scale video datasets (TVR, ActivityNet Captions, and Charades-STA). The results show that GMMFormer outperforms existing PRVR methods and achieves better efficiency by reducing the storage overhead and improving the embedding space.<details>
<summary>Abstract</summary>
Given a text query, partially relevant video retrieval (PRVR) seeks to find untrimmed videos containing pertinent moments in a database. For PRVR, clip modeling is essential to capture the partial relationship between texts and videos. Current PRVR methods adopt scanning-based clip construction to achieve explicit clip modeling, which is information-redundant and requires a large storage overhead. To solve the efficiency problem of PRVR methods, this paper proposes GMMFormer, a \textbf{G}aussian-\textbf{M}ixture-\textbf{M}odel based Trans\textbf{former} which models clip representations implicitly. During frame interactions, we incorporate Gaussian-Mixture-Model constraints to focus each frame on its adjacent frames instead of the whole video. Then generated representations will contain multi-scale clip information, achieving implicit clip modeling. In addition, PRVR methods ignore semantic differences between text queries relevant to the same video, leading to a sparse embedding space. We propose a query diverse loss to distinguish these text queries, making the embedding space more intensive and contain more semantic information. Extensive experiments on three large-scale video datasets (\ie, TVR, ActivityNet Captions, and Charades-STA) demonstrate the superiority and efficiency of GMMFormer.
</details>
<details>
<summary>摘要</summary>
During frame interactions, we incorporate Gaussian-Mixture-Model constraints to focus each frame on its adjacent frames instead of the whole video. This allows generated representations to contain multi-scale clip information, achieving implicit clip modeling. Additionally, PRVR methods ignore semantic differences between text queries relevant to the same video, leading to a sparse embedding space. We propose a query diverse loss to distinguish these text queries, making the embedding space more intense and contain more semantic information.Extensive experiments on three large-scale video datasets (TVR, ActivityNet Captions, and Charades-STA) demonstrate the superiority and efficiency of GMMFormer.
</details></li>
</ul>
<hr>
<h2 id="Factuality-Challenges-in-the-Era-of-Large-Language-Models"><a href="#Factuality-Challenges-in-the-Era-of-Large-Language-Models" class="headerlink" title="Factuality Challenges in the Era of Large Language Models"></a>Factuality Challenges in the Era of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05189">http://arxiv.org/abs/2310.05189</a></li>
<li>repo_url: None</li>
<li>paper_authors: Isabelle Augenstein, Timothy Baldwin, Meeyoung Cha, Tanmoy Chakraborty, Giovanni Luca Ciampaglia, David Corney, Renee DiResta, Emilio Ferrara, Scott Hale, Alon Halevy, Eduard Hovy, Heng Ji, Filippo Menczer, Ruben Miguez, Preslav Nakov, Dietram Scheufele, Shivam Sharma, Giovanni Zagni</li>
<li>for: 本研究旨在探讨Generative AI技术的发展以及其对社会的影响，尤其是LLMs技术的潜在的威胁和风险。</li>
<li>methods: 本研究采用了文献综述和讨论的方法，检视了现有的LLMs技术和其应用场景，并分析了这些技术的潜在的威胁和风险。</li>
<li>results: 本研究发现了一些LLMs技术的潜在威胁和风险，包括生成假信息和假 profiles，以及恶意利用这些技术来欺诈用户。同时，本研究还提出了一些可能的解决方案，如实施技术审核和评估机制，提高用户的AI理解水平，以及进行更多的研究和规范。<details>
<summary>Abstract</summary>
The emergence of tools based on Large Language Models (LLMs), such as OpenAI's ChatGPT, Microsoft's Bing Chat, and Google's Bard, has garnered immense public attention. These incredibly useful, natural-sounding tools mark significant advances in natural language generation, yet they exhibit a propensity to generate false, erroneous, or misleading content -- commonly referred to as "hallucinations." Moreover, LLMs can be exploited for malicious applications, such as generating false but credible-sounding content and profiles at scale. This poses a significant challenge to society in terms of the potential deception of users and the increasing dissemination of inaccurate information. In light of these risks, we explore the kinds of technological innovations, regulatory reforms, and AI literacy initiatives needed from fact-checkers, news organizations, and the broader research and policy communities. By identifying the risks, the imminent threats, and some viable solutions, we seek to shed light on navigating various aspects of veracity in the era of generative AI.
</details>
<details>
<summary>摘要</summary>
LLM（大语言模型）技术的出现，如OpenAI的ChatGPT、Microsoft的Bing Chat以及Google的Bard，吸引了广泛的公众关注。这些极其有用、自然 звуча的工具表现出了对自然语言生成的重要进步，但它们往往会生成错误、误导性的内容，通常被称为“幻见”。此外，LLM可能会被恶用于黑客活动，如大规模生成假 pero Credible-sounding内容和 Profile。这对社会带来了误导用户的风险，以及假信息的扩散。为了面对这些挑战，我们需要从事实核查、法规改革以及人工智能文化培训等方面来解决这些问题。我们希望通过识别风险、危机点以及可行的解决方案，为在生成AI时的真实性提供指南。
</details></li>
</ul>
<hr>
<h2 id="Evolutionary-Retrosynthetic-Route-Planning"><a href="#Evolutionary-Retrosynthetic-Route-Planning" class="headerlink" title="Evolutionary Retrosynthetic Route Planning"></a>Evolutionary Retrosynthetic Route Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05186">http://arxiv.org/abs/2310.05186</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yan Zhang, Hao Hao, Xiao He, Shuanhu Gao, Aimin Zhou</li>
<li>for: 本研究目的是提出一种基于进化算法的多步反Synthesis路径规划方法，以解决现有的反Synthesis问题。</li>
<li>methods: 该方法首先将反Synthesis问题转化为优化问题，定义搜索空间和操作。此外，为提高搜索效率， parallel 策略被实现。</li>
<li>results: 对四种产品的实验结果表明，相比较 Monte Carlo tree search 算法，EA 可以Significantly 减少单步模型的调用数（均减少53.9%），搜索三个解决方案的时间减少83.9%，并同时提高可行搜索路径的数量（增加5倍）。<details>
<summary>Abstract</summary>
Molecular retrosynthesis is a significant and complex problem in the field of chemistry, however, traditional manual synthesis methods not only need well-trained experts but also are time-consuming. With the development of big data and machine learning, artificial intelligence (AI) based retrosynthesis is attracting more attention and is becoming a valuable tool for molecular retrosynthesis. At present, Monte Carlo tree search is a mainstream search framework employed to address this problem. Nevertheless, its search efficiency is compromised by its large search space. Therefore, we propose a novel approach for retrosynthetic route planning based on evolutionary optimization, marking the first use of Evolutionary Algorithm (EA) in the field of multi-step retrosynthesis. The proposed method involves modeling the retrosynthetic problem into an optimization problem, defining the search space and operators. Additionally, to improve the search efficiency, a parallel strategy is implemented. The new approach is applied to four case products, and is compared with Monte Carlo tree search. The experimental results show that, in comparison to the Monte Carlo tree search algorithm, EA significantly reduces the number of calling single-step model by an average of 53.9%. The time required to search three solutions decreased by an average of 83.9%, and the number of feasible search routes increases by 5 times.
</details>
<details>
<summary>摘要</summary>
分子逆synthesis是化学领域中的一个重要和复杂问题，但传统的手动合成方法不仅需要高水平的专业人员，还需要很长的时间。随着大数据和机器学习的发展，人工智能（AI）基于的逆synthesis在这一问题上吸引了更多的注意力，成为化学领域的一种有价值的工具。目前，蒙特卡洛tree搜索是逆synthesis搜索框架的主流，但它的搜索效率受到搜索空间的限制。因此，我们提出了一种基于进化优化的新方法，标志着多步逆synthesis中Evolutionary Algorithm（EA）的首次应用。该方法包括将逆synthesis问题转化为优化问题，定义搜索空间和运算符。此外，为了提高搜索效率，并行策略被实现。新方法在四种case продуkttest中应用，并与蒙特卡洛tree搜索进行比较。实验结果表明，相比蒙特卡洛tree搜索算法，EA可以平均减少单步模型的呼び出数量53.9%，搜索三个解决方案所需的时间减少83.9%，并同时提高可行搜索路径的数量5倍。
</details></li>
</ul>
<hr>
<h2 id="Text2NKG-Fine-Grained-N-ary-Relation-Extraction-for-N-ary-relational-Knowledge-Graph-Construction"><a href="#Text2NKG-Fine-Grained-N-ary-Relation-Extraction-for-N-ary-relational-Knowledge-Graph-Construction" class="headerlink" title="Text2NKG: Fine-Grained N-ary Relation Extraction for N-ary relational Knowledge Graph Construction"></a>Text2NKG: Fine-Grained N-ary Relation Extraction for N-ary relational Knowledge Graph Construction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05185">http://arxiv.org/abs/2310.05185</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lhrlab/text2nkg">https://github.com/lhrlab/text2nkg</a></li>
<li>paper_authors: Haoran Luo, Haihong E, Yuhao Yang, Tianyu Yao, Yikai Guo, Zichen Tang, Wentai Zhang, Kaiyang Wan, Shiyao Peng, Meina Song, Wei Lin</li>
<li>for: 这篇论文旨在构建基于文本的n-ary关系知识图（NKG），以便更好地表达现实世界中的多元关系。</li>
<li>methods: 本文提出了一种新的细化n-ary关系抽取方法，使用 span-tuple classification 和 heteo-ordered merging 技术来实现不同的n-ary关系抽取。</li>
<li>results: 实验结果表明，Text2NKG 比前一代模型提高了 nearly 20% 的 $F_1$ 分数在 Hyper-relational schema 中的细化n-ary关系抽取任务上。<details>
<summary>Abstract</summary>
Beyond traditional binary relational facts, n-ary relational knowledge graphs (NKGs) are comprised of n-ary relational facts containing more than two entities, which are closer to real-world facts with broader applications. However, the construction of NKGs still significantly relies on manual labor, and n-ary relation extraction still remains at a course-grained level, which is always in a single schema and fixed arity of entities. To address these restrictions, we propose Text2NKG, a novel fine-grained n-ary relation extraction framework for n-ary relational knowledge graph construction. We introduce a span-tuple classification approach with hetero-ordered merging to accomplish fine-grained n-ary relation extraction in different arity. Furthermore, Text2NKG supports four typical NKG schemas: hyper-relational schema, event-based schema, role-based schema, and hypergraph-based schema, with high flexibility and practicality. Experimental results demonstrate that Text2NKG outperforms the previous state-of-the-art model by nearly 20\% points in the $F_1$ scores on the fine-grained n-ary relation extraction benchmark in the hyper-relational schema. Our code and datasets are publicly available.
</details>
<details>
<summary>摘要</summary>
traditional binary relational facts beyond, n-ary relational knowledge graphs (NKGs) comprised of n-ary relational facts containing more than two entities, closer to real-world facts with broader applications. However, the construction of NKGs still significantly relies on manual labor, and n-ary relation extraction still remains at a course-grained level, which is always in a single schema and fixed arity of entities. To address these restrictions, we propose Text2NKG, a novel fine-grained n-ary relation extraction framework for n-ary relational knowledge graph construction. We introduce a span-tuple classification approach with hetero-ordered merging to accomplish fine-grained n-ary relation extraction in different arity. Furthermore, Text2NKG supports four typical NKG schemas: hyper-relational schema, event-based schema, role-based schema, and hypergraph-based schema, with high flexibility and practicality. Experimental results demonstrate that Text2NKG outperforms the previous state-of-the-art model by nearly 20\% points in the $F_1$ scores on the fine-grained n-ary relation extraction benchmark in the hyper-relational schema. Our code and datasets are publicly available.Here's the breakdown of the translation:* "traditional binary relational facts" becomes "传统二元关系知识"* "n-ary relational knowledge graphs" becomes "n-ary关系知识图"* "n-ary relational facts" becomes "n-ary关系事实"* "broader applications" becomes "更广泛的应用"* "manual labor" becomes "手动劳动"* "course-grained level" becomes "粗粒度层"* "single schema" becomes "单一 schema"* "fixed arity of entities" becomes " fixes 实体数量"* "Text2NKG" becomes "文本到 NKG"* "span-tuple classification" becomes " span-tuple 分类"* "hetero-ordered merging" becomes "异质顺序合并"* "fine-grained n-ary relation extraction" becomes "细化 n-ary 关系提取"* "n-ary relation extraction benchmark" becomes "n-ary 关系提取指标"* "hyper-relational schema" becomes "超过关系 schema"* "event-based schema" becomes "事件基于 schema"* "role-based schema" becomes "角色基于 schema"* "hypergraph-based schema" becomes "超graph基于 schema"* "high flexibility and practicality" becomes "高灵活性和实用性"* "previous state-of-the-art model" becomes "前一代模型"* "nearly 20\% points" becomes "约 20\% 的点数"Note that the translation is in Simplified Chinese, which is the most widely used variety of Chinese. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Optimizing-Large-Language-Models-to-Expedite-the-Development-of-Smart-Contracts"><a href="#Optimizing-Large-Language-Models-to-Expedite-the-Development-of-Smart-Contracts" class="headerlink" title="Optimizing Large Language Models to Expedite the Development of Smart Contracts"></a>Optimizing Large Language Models to Expedite the Development of Smart Contracts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05178">http://arxiv.org/abs/2310.05178</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nii Osae Osae Dade, Margaret Lartey-Quaye, Emmanuel Teye-Kofi Odonkor, Paul Ammah</li>
<li>For: The paper aims to help developers build decentralized applications (dApps) on blockchain networks by introducing MazzumaGPT, a large language model that can generate smart contract code and improve development productivity.* Methods: The paper uses a large language model called MazzumaGPT, which is optimized for generating smart contract code. The model is fine-tuned and evaluated for functional correctness.* Results: The paper reports on the performance of MazzumaGPT in generating smart contract code and improving development productivity. The results show that the model can generate correct code and improve development efficiency. However, the paper also acknowledges some limitations and broader impacts of the research.Here is the same information in Simplified Chinese:* For: 本研究旨在帮助开发者在区块链网络上建立分布式应用程序（dApps），通过引入MazzumaGPT大语言模型，生成智能合约代码并提高开发效率。* Methods: 本研究使用MazzumaGPT大语言模型，该模型是为生成智能合约代码优化。模型进行了精度调整和功能正确性评估。* Results: 本研究报告MazzumaGPT模型在生成智能合约代码和提高开发效率方面的性能。结果显示，模型可以生成正确的代码并提高开发效率，但也存在一些限制和更广泛的影响。<details>
<summary>Abstract</summary>
Programming has always been at the heart of technological innovation in the 21st century. With the advent of blockchain technologies and the proliferation of web3 paradigms of decentralised applications, smart contracts have been very instrumental in enabling developers to build applications that reside on decentralised blockchains. Despite the huge interest and potential of smart contracts, there is still a significant knowledge and skill gap that developers need to cross in order to build web3 applications. In light of this, we introduce MazzumaGPT, a large language model that has been optimised to generate smart contract code and aid developers to scaffold development and improve productivity. As part of this research, we outline the optimisation and fine-tuning parameters, evaluate the model's performance on functional correctness and address the limitations and broader impacts of our research.
</details>
<details>
<summary>摘要</summary>
Programming 一直是现代科技创新的核心在21世纪。随着区块链技术的出现和分布式应用程序的普及，智能合约帮助开发者建立在分布式区块链上的应用程序。虽然智能合约具有巨大的潜在利益和潜力，但开发者仍然需要跨越一定的知识和技能差距来构建Web3应用程序。为了解决这个问题，我们介绍MazzumaGPT，一个优化的大语言模型，可以生成智能合约代码，帮助开发者快速构建和改进开发。在这项研究中，我们详细介绍优化和细调参数，评估模型的性能并讨论我们的研究的局限性和更广泛的影响。
</details></li>
</ul>
<hr>
<h2 id="GSLB-The-Graph-Structure-Learning-Benchmark"><a href="#GSLB-The-Graph-Structure-Learning-Benchmark" class="headerlink" title="GSLB: The Graph Structure Learning Benchmark"></a>GSLB: The Graph Structure Learning Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05174">http://arxiv.org/abs/2310.05174</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gsl-benchmark/gslb">https://github.com/gsl-benchmark/gslb</a></li>
<li>paper_authors: Zhixun Li, Liang Wang, Xin Sun, Yifan Luo, Yanqiao Zhu, Dingshuo Chen, Yingtao Luo, Xiangxin Zhou, Qiang Liu, Shu Wu, Liang Wang, Jeffrey Xu Yu</li>
<li>for: 本研究的目的是为Graph Structure Learning (GSL)提供一个系统的分析和评估，以便更好地理解GSL在不同情况下的表现。</li>
<li>methods: 本研究使用了20种不同的图 dataset和16种不同的 GSL 算法，并进行了系统的性能分析和比较。</li>
<li>results: 研究发现，GSL 在 node-level 和 graph-level 任务中表现出色，并且在鲁棒学习和模型复杂度方面也有出色的表现。<details>
<summary>Abstract</summary>
Graph Structure Learning (GSL) has recently garnered considerable attention due to its ability to optimize both the parameters of Graph Neural Networks (GNNs) and the computation graph structure simultaneously. Despite the proliferation of GSL methods developed in recent years, there is no standard experimental setting or fair comparison for performance evaluation, which creates a great obstacle to understanding the progress in this field. To fill this gap, we systematically analyze the performance of GSL in different scenarios and develop a comprehensive Graph Structure Learning Benchmark (GSLB) curated from 20 diverse graph datasets and 16 distinct GSL algorithms. Specifically, GSLB systematically investigates the characteristics of GSL in terms of three dimensions: effectiveness, robustness, and complexity. We comprehensively evaluate state-of-the-art GSL algorithms in node- and graph-level tasks, and analyze their performance in robust learning and model complexity. Further, to facilitate reproducible research, we have developed an easy-to-use library for training, evaluating, and visualizing different GSL methods. Empirical results of our extensive experiments demonstrate the ability of GSL and reveal its potential benefits on various downstream tasks, offering insights and opportunities for future research. The code of GSLB is available at: https://github.com/GSL-Benchmark/GSLB.
</details>
<details>
<summary>摘要</summary>
“几年前，Graph Structure Learning（GSL）已经吸引了很多注意，因为它可以同时优化Graph Neural Networks（GNNs）的参数和计算图структура。不过，过去几年发展的GSL方法中，没有一个通用的实验设置或公平的比较方法，这导致了理解这个领域的进步受到了很大的阻碍。为了填补这个空白，我们系统地分析了GSL在不同的场景下的表现，并开发了一个全面的Graph Structure Learning Benchmark（GSLB），收集了20个多标的图数据和16种不同的GSL算法。具体来说，GSLB系统地探讨了GSL的特点在三个维度上：有效性、韧性和复杂度。我们对现今的State-of-the-art GSL算法进行了node-和graph-水平的任务，并分析了它们在Robust Learning和模型复杂度上的表现。此外，为了促进可重现性的研究，我们开发了一个容易使用的库，可以用于训练、评估和显示不同的GSL方法。我们的广泛的实验结果显示了GSL的能力，并给出了不同下游任务的可能性和未来研究的方向。GSLB的代码可以在：https://github.com/GSL-Benchmark/GSLB中找到。”
</details></li>
</ul>
<hr>
<h2 id="Multi-Ship-Tracking-by-Robust-Similarity-metric"><a href="#Multi-Ship-Tracking-by-Robust-Similarity-metric" class="headerlink" title="Multi-Ship Tracking by Robust Similarity metric"></a>Multi-Ship Tracking by Robust Similarity metric</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05171">http://arxiv.org/abs/2310.05171</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongyu Zhao, Gongming Wei, Yang Xiao, Xianglei Xing</li>
<li>for: 提高多船跟踪（MST）技术的应用于海上情况意识和自动船 Navigation System 的发展。</li>
<li>methods: 通过在多目标跟踪（MOT）算法中使用最小几何形态的拟合来提高跟踪性能。</li>
<li>results: 通过将TIoU metricintegrated into state-of-the-art object tracking frameworks, such as DeepSort and ByteTrack, achieving improvements in tracking performance.<details>
<summary>Abstract</summary>
Multi-ship tracking (MST) as a core technology has been proven to be applied to situational awareness at sea and the development of a navigational system for autonomous ships. Despite impressive tracking outcomes achieved by multi-object tracking (MOT) algorithms for pedestrian and vehicle datasets, these models and techniques exhibit poor performance when applied to ship datasets. Intersection of Union (IoU) is the most popular metric for computing similarity used in object tracking. The low frame rates and severe image shake caused by wave turbulence in ship datasets often result in minimal, or even zero, Intersection of Union (IoU) between the predicted and detected bounding boxes. This issue contributes to frequent identity switches of tracked objects, undermining the tracking performance. In this paper, we address the weaknesses of IoU by incorporating the smallest convex shapes that enclose both the predicted and detected bounding boxes. The calculation of the tracking version of IoU (TIoU) metric considers not only the size of the overlapping area between the detection bounding box and the prediction box, but also the similarity of their shapes. Through the integration of the TIoU into state-of-the-art object tracking frameworks, such as DeepSort and ByteTrack, we consistently achieve improvements in the tracking performance of these frameworks.
</details>
<details>
<summary>摘要</summary>
多船跟踪（MST）作为核心技术已被应用于海上情况意识和自动船 Navigation System 的开发。 despite impressive tracking outcomes achieved by multi-object tracking（MOT）算法for pedestrian and vehicle datasets， these models and techniques exhibit poor performance when applied to ship datasets。 Intersection of Union（IoU）是计算相似性的最受欢迎度量， However， the low frame rates and severe image shake caused by wave turbulence in ship datasets often result in minimal, or even zero, Intersection of Union（IoU）between the predicted and detected bounding boxes。 This issue contributes to frequent identity switches of tracked objects, undermining the tracking performance。 In this paper， we address the weaknesses of IoU by incorporating the smallest convex shapes that enclose both the predicted and detected bounding boxes。 The calculation of the tracking version of IoU（TIoU）metric considers not only the size of the overlapping area between the detection bounding box and the prediction box， but also the similarity of their shapes。 Through the integration of the TIoU into state-of-the-art object tracking frameworks， such as DeepSort and ByteTrack， we consistently achieve improvements in the tracking performance of these frameworks。
</details></li>
</ul>
<hr>
<h2 id="DeepQTest-Testing-Autonomous-Driving-Systems-with-Reinforcement-Learning-and-Real-world-Weather-Data"><a href="#DeepQTest-Testing-Autonomous-Driving-Systems-with-Reinforcement-Learning-and-Real-world-Weather-Data" class="headerlink" title="DeepQTest: Testing Autonomous Driving Systems with Reinforcement Learning and Real-world Weather Data"></a>DeepQTest: Testing Autonomous Driving Systems with Reinforcement Learning and Real-world Weather Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05170">http://arxiv.org/abs/2310.05170</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/simula-complex/deepqtest">https://github.com/simula-complex/deepqtest</a></li>
<li>paper_authors: Chengjie Lu, Tao Yue, Man Zhang, Shaukat Ali</li>
<li>for: 这个论文的目的是提出一种基于强化学习的自动驾驶系统测试方法，以确保自动驾驶系统的安全性。</li>
<li>methods: 这种测试方法使用强化学习的深度Q学习算法，以学习环境配置，并采用了三种安全和舒适度量来构建奖励函数。</li>
<li>results: 对于三个比较基线，深度Q测试表现出显著更高的效果，能够更好地激发自动驾驶系统的异常行为，并确保测试场景的现实性。<details>
<summary>Abstract</summary>
Autonomous driving systems (ADSs) are capable of sensing the environment and making driving decisions autonomously. These systems are safety-critical, and testing them is one of the important approaches to ensure their safety. However, due to the inherent complexity of ADSs and the high dimensionality of their operating environment, the number of possible test scenarios for ADSs is infinite. Besides, the operating environment of ADSs is dynamic, continuously evolving, and full of uncertainties, which requires a testing approach adaptive to the environment. In addition, existing ADS testing techniques have limited effectiveness in ensuring the realism of test scenarios, especially the realism of weather conditions and their changes over time. Recently, reinforcement learning (RL) has demonstrated great potential in addressing challenging problems, especially those requiring constant adaptations to dynamic environments. To this end, we present DeepQTest, a novel ADS testing approach that uses RL to learn environment configurations with a high chance of revealing abnormal ADS behaviors. Specifically, DeepQTest employs Deep Q-Learning and adopts three safety and comfort measures to construct the reward functions. To ensure the realism of generated scenarios, DeepQTest defines a set of realistic constraints and introduces real-world weather conditions into the simulated environment. We employed three comparison baselines, i.e., random, greedy, and a state-of-the-art RL-based approach DeepCOllision, for evaluating DeepQTest on an industrial-scale ADS. Evaluation results show that DeepQTest demonstrated significantly better effectiveness in terms of generating scenarios leading to collisions and ensuring scenario realism compared with the baselines. In addition, among the three reward functions implemented in DeepQTest, Time-To-Collision is recommended as the best design according to our study.
</details>
<details>
<summary>摘要</summary>
自动驾驶系统（ADS）具有感知环境和做出自主驾驶决策的能力。这些系统的安全性非常重要，测试是确保其安全的重要方法。然而，由于ADS的内在复杂性和操作环境的高维度，测试场景的数量是无限的。此外，ADS的操作环境是动态不断变化的，充满不确定性，需要适应环境的测试方法。此外，现有的ADS测试技术对测试场景的真实性具有有限的效果，特别是天气变化和时间的变化。最近，人工智能学习（RL）已经在解决复杂问题方面表现出了极大的潜力。为此，我们提出了 DeepQTest，一种基于RL学习环境配置，以高概率暴露ADS异常行为的测试方法。具体来说，DeepQTest使用深度Q学习并采用了三种安全和舒适度量来定义奖励函数。为保证生成的场景的真实性，DeepQTest定义了一组真实的约束和将实际天气条件引入模拟环境中。我们对ADS进行了三种比较基准，即随机、积极和当前State-of-the-art RL基于approach DeepCOllision，以评估DeepQTest的效果。评估结果表明，DeepQTest与基准相比显著地提高了导致碰撞的场景生成和场景真实性的效果。此外，我们对DeepQTest中实现的三种奖励函数进行了研究，并确定了时间到碰撞为最佳设计。
</details></li>
</ul>
<hr>
<h2 id="Hieros-Hierarchical-Imagination-on-Structured-State-Space-Sequence-World-Models"><a href="#Hieros-Hierarchical-Imagination-on-Structured-State-Space-Sequence-World-Models" class="headerlink" title="Hieros: Hierarchical Imagination on Structured State Space Sequence World Models"></a>Hieros: Hierarchical Imagination on Structured State Space Sequence World Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05167">http://arxiv.org/abs/2310.05167</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/snagnar/hieros">https://github.com/snagnar/hieros</a></li>
<li>paper_authors: Paul Mattes, Rainer Schlosser, Ralf Herbrich</li>
<li>for: 本研究旨在提高现代深度强化学习（DRL）算法的样本效率。</li>
<li>methods: 我们提出了一种层次策略（Hieros），该策略使用了S5层来学习时间抽象的世界表示，并在幂 espacio 中预测下一个世界状态。</li>
<li>results: 我们的方法在Atari 100k Benchmark上的平均和中位数正常化人工分数中超过了现有的状态势。此外，我们的提出的世界模型能够准确预测复杂的动力学。此外，我们还发现了Hieros在探索方面的优势。<details>
<summary>Abstract</summary>
One of the biggest challenges to modern deep reinforcement learning (DRL) algorithms is sample efficiency. Many approaches learn a world model in order to train an agent entirely in imagination, eliminating the need for direct environment interaction during training. However, these methods often suffer from either a lack of imagination accuracy, exploration capabilities, or runtime efficiency. We propose Hieros, a hierarchical policy that learns time abstracted world representations and imagines trajectories at multiple time scales in latent space. Hieros uses an S5 layer-based world model, which predicts next world states in parallel during training and iteratively during environment interaction. Due to the special properties of S5 layers, our method can train in parallel and predict next world states iteratively during imagination. This allows for more efficient training than RNN-based world models and more efficient imagination than Transformer-based world models.   We show that our approach outperforms the state of the art in terms of mean and median normalized human score on the Atari 100k benchmark, and that our proposed world model is able to predict complex dynamics very accurately. We also show that Hieros displays superior exploration capabilities compared to existing approaches.
</details>
<details>
<summary>摘要</summary>
一个现代深度奖励学习（DRL）算法的主要挑战是样本效率。许多方法尝试通过在幻想中训练代理人，从而消除直接环境互动的需要。然而，这些方法经常受到幻想准确性、探索能力或运行效率的限制。我们提出了 Hieros，一种层次策略，该策略在幻想中预测时间抽象的世界表示和轨迹，并在多个时间尺度上进行幻想。Heros使用基于 S5 层的世界模型，该模型在训练和环境互动过程中并行地预测下一个世界状态。由于 S5 层的特殊性，我们的方法可以并行地训练和在幻想中预测下一个世界状态。这使得我们的方法比 RNN 类世界模型更高效，并且比 Transformer 类世界模型更高效。我们表明，我们的方法在 Atari 100k 测试集上的平均和中位数normalized human score比 state of the art 高，并且我们提出的世界模型能够准确预测复杂的动力学。此外，我们还证明 Hieros 在探索方面表现出优于现有的方法。
</details></li>
</ul>
<hr>
<h2 id="MenatQA-A-New-Dataset-for-Testing-the-Temporal-Comprehension-and-Reasoning-Abilities-of-Large-Language-Models"><a href="#MenatQA-A-New-Dataset-for-Testing-the-Temporal-Comprehension-and-Reasoning-Abilities-of-Large-Language-Models" class="headerlink" title="MenatQA: A New Dataset for Testing the Temporal Comprehension and Reasoning Abilities of Large Language Models"></a>MenatQA: A New Dataset for Testing the Temporal Comprehension and Reasoning Abilities of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05157">http://arxiv.org/abs/2310.05157</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/weiyifan1023/MenatQA">https://github.com/weiyifan1023/MenatQA</a></li>
<li>paper_authors: Yifan Wei, Yisong Su, Huanhuan Ma, Xiaoyan Yu, Fangyu Lei, Yuanzhe Zhang, Jun Zhao, Kang Liu</li>
<li>for: This paper aims to evaluate the time comprehension and reasoning abilities of large language models (LLMs) and investigate potential improvement strategies.</li>
<li>methods: The paper constructs a benchmark task called Multiple Sensitive Factors Time QA (MenatQA) that tests LLMs’ performance on three temporal factors (scope factor, order factor, counterfactual factor) with a total of 2,853 samples.</li>
<li>results: Most LLMs fall behind smaller temporal reasoning models in terms of performance on the MenatQA task, particularly in handling temporal biases and utilizing external information. The paper also explores potential improvement strategies such as devising specific prompts and leveraging external tools.<details>
<summary>Abstract</summary>
Large language models (LLMs) have shown nearly saturated performance on many natural language processing (NLP) tasks. As a result, it is natural for people to believe that LLMs have also mastered abilities such as time understanding and reasoning. However, research on the temporal sensitivity of LLMs has been insufficiently emphasized. To fill this gap, this paper constructs Multiple Sensitive Factors Time QA (MenatQA), which encompasses three temporal factors (scope factor, order factor, counterfactual factor) with total 2,853 samples for evaluating the time comprehension and reasoning abilities of LLMs. This paper tests current mainstream LLMs with different parameter sizes, ranging from billions to hundreds of billions. The results show most LLMs fall behind smaller temporal reasoning models with different degree on these factors. In specific, LLMs show a significant vulnerability to temporal biases and depend heavily on the temporal information provided in questions. Furthermore, this paper undertakes a preliminary investigation into potential improvement strategies by devising specific prompts and leveraging external tools. These approaches serve as valuable baselines or references for future research endeavors.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在自然语言处理（NLP）任务上已经显示出几乎满足性的性能。因此，人们对于 LLM 的时间理解和推理能力的拥有有了误解。然而，对于 LLN 的时间敏感性的研究却得不到充分的关注。为了填补这个空白，这篇文章建立了多重敏感因素时间问答（MenatQA），包括三个时间因素（范围因素、次序因素、Counterfactual因素），共有2,853个样本，用于评估 LLM 的时间理解和推理能力。这篇文章测试了现代主流 LLM 的不同参数大小，从十亿到百亿。结果显示，大多数 LLM 落后于不同程度的时间推理模型。具体来说，LLM 对于时间偏见具有重要的敏感性，并且对于时间提供的问题中的时间信息依赖很大。此外，这篇文章进行了初步的改进策略研究，包括设计特定的提示和使用外部工具。这些方法可以作为未来研究的基础或参考。
</details></li>
</ul>
<hr>
<h2 id="Toolink-Linking-Toolkit-Creation-and-Using-through-Chain-of-Solving-on-Open-Source-Model"><a href="#Toolink-Linking-Toolkit-Creation-and-Using-through-Chain-of-Solving-on-Open-Source-Model" class="headerlink" title="Toolink: Linking Toolkit Creation and Using through Chain-of-Solving on Open-Source Model"></a>Toolink: Linking Toolkit Creation and Using through Chain-of-Solving on Open-Source Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05155">http://arxiv.org/abs/2310.05155</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qiancheng0/toolink">https://github.com/qiancheng0/toolink</a></li>
<li>paper_authors: Cheng Qian, Chenyan Xiong, Zhenghao Liu, Zhiyuan Liu</li>
<li>for: The paper aims to develop a comprehensive framework for task-solving using tool-based chain-of-solving (CoS) approach, with the goal of leveraging smaller, open-sourced models for adaptability.</li>
<li>methods: The proposed framework, called Toolink, creates a toolkit and integrates planning and calling of tools through a CoS approach. The authors validate the efficacy of Toolink on ChatGPT and curate a CoS dataset (CoS-GPT) for task-solving. They finetune the LLaMA-7B model to create LLaMA-CoS, a powerful open-source model with advanced tool-planning and tool-calling capabilities.</li>
<li>results: The evaluation on diverse tasks from BIG-bench shows that LLaMA-CoS matches the CoS ability of ChatGPT while surpassing the chain-of-thought approach in performance. The study also demonstrates the generalization of LLaMA-CoS to unseen tasks and its capability in using toolkits not explicitly tailored for the target task, affirming its robustness in real-world scenarios.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have demonstrated remarkable progress in utilizing tools, but their closed-source nature and high inference costs pose limitations on their adaptability, necessitating a valid method that leverages smaller, open-sourced models. In this paper, we introduce Toolink, a comprehensive framework that performs task-solving by first creating a toolkit and then integrating the planning and calling of tools through a chain-of-solving (CoS) approach. We first validate the efficacy of Toolink in harnessing the model's creativity and CoS ability on ChatGPT. Subsequently, we curate CoS-GPT, a chain-of-solving dataset designed for tool-using, and finetune the LLaMA-7B model. It results in LLaMA-CoS, a powerful open-source model with advanced tool-planning and tool-calling capabilities. Evaluation on diverse tasks from BIG-bench demonstrates its CoS ability matches that of ChatGPT while its performance surpasses the chain-of-thought approach. Further studies highlight the generalization of LLaMA-CoS to unseen tasks and showcase its capability in using toolkits not explicitly tailored for the target task, affirming its robustness in real-world scenarios. All codes and data are released.
</details>
<details>
<summary>摘要</summary>
Here is the translation in Simplified Chinese:大型语言模型（LLMs）已经展示出具有优异的进步，但它们的关闭源代码和高推论成本导致它们的适应性有限，需要一个有效的方法来应用小型开源模型。在这篇文章中，我们介绍Toolink，一个完整的框架，通过链式解决（CoS）方法来实现任务解决。我们首先验证Toolink在ChatGPT上的有效性，然后创建CoS-GPT dataset，并调整LLaMA-7B模型。它将实现LLaMA-CoS，一个开源模型，拥有进步的工具规划和工具呼叫能力。我们从BIG-bench中的多个任务进行评估，发现LLaMA-CoS的CoS能力与ChatGPT相似，并且其表现超过链式思维方法。此外，我们还进行了进一步的研究，证明LLaMA-CoS具有对未见任务的普遍性和在不同的工具集上的可行性，这证明了它在实际情况中的可靠性。所有代码和数据都是公开发布。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Model-LLM-as-a-System-of-Multiple-Expert-Agents-An-Approach-to-solve-the-Abstraction-and-Reasoning-Corpus-ARC-Challenge"><a href="#Large-Language-Model-LLM-as-a-System-of-Multiple-Expert-Agents-An-Approach-to-solve-the-Abstraction-and-Reasoning-Corpus-ARC-Challenge" class="headerlink" title="Large Language Model (LLM) as a System of Multiple Expert Agents: An Approach to solve the Abstraction and Reasoning Corpus (ARC) Challenge"></a>Large Language Model (LLM) as a System of Multiple Expert Agents: An Approach to solve the Abstraction and Reasoning Corpus (ARC) Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05146">http://arxiv.org/abs/2310.05146</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tanchongmin/arc-challenge">https://github.com/tanchongmin/arc-challenge</a></li>
<li>paper_authors: John Chong Min Tan, Mehul Motani</li>
<li>for: 解决Abstraction and Reasoning Corpus(ARC)挑战，使用大型自然语言模型(LLM)作为多个专家系统。</li>
<li>methods: 使用LLM的灵活性，通过零shot、几shot、上下文固定的提示，让LLM解决多种新任务。首先将输入图像转化为多种适合的文本抽象空间，然后利用LLM的协同力量，Derive输入-输出关系，并将其映射到动作形式的工作程序，类似于Voyager &#x2F; Ghost in MineCraft。 Additionally, use iterative environmental feedback to guide LLMs to solve the task.</li>
<li>results: 使用提posed方法解决111个训练集问题中的50个(45%)，只需三个抽象空间 - 网格、对象和像素。我们认为，通过添加更多抽象空间和学习动作，我们将能够解决更多问题。<details>
<summary>Abstract</summary>
We attempt to solve the Abstraction and Reasoning Corpus (ARC) Challenge using Large Language Models (LLMs) as a system of multiple expert agents. Using the flexibility of LLMs to be prompted to do various novel tasks using zero-shot, few-shot, context-grounded prompting, we explore the feasibility of using LLMs to solve the ARC Challenge. We firstly convert the input image into multiple suitable text-based abstraction spaces. We then utilise the associative power of LLMs to derive the input-output relationship and map this to actions in the form of a working program, similar to Voyager / Ghost in the MineCraft. In addition, we use iterative environmental feedback in order to guide LLMs to solve the task. Our proposed approach achieves 50 solves out of 111 training set problems (45%) with just three abstraction spaces - grid, object and pixel - and we believe that with more abstraction spaces and learnable actions, we will be able to solve more.
</details>
<details>
<summary>摘要</summary>
我们尝试使用大型自然语言模型（LLM）解决抽象和逻辑 Corpora（ARC）挑战，以多个专家代理系统的形式进行解决。通过使用 LLM 的灵活性，我们可以使其响应各种新任务，使用零上下文、几上下文、上下文固定的提示，探索使用 LLM 解决 ARC 挑战的可能性。首先，我们将输入图像转换为多个适合的文本基于抽象空间。然后，我们利用 LLMS 的协同力来推导输入-输出关系，并将其映射到作为工作程序的动作，类似于 Voyager / Ghost 在 MineCraft 中。此外，我们使用迭代环境反馈，以引导 LLMS 解决任务。我们的提议方法已经实现了 50 个训练集问题（45%）的解决，只使用了三个抽象空间 - 网格、对象和像素 - 并我们认为，通过添加更多的抽象空间和学习动作，我们将能够解决更多的问题。
</details></li>
</ul>
<hr>
<h2 id="NeuralFastLAS-Fast-Logic-Based-Learning-from-Raw-Data"><a href="#NeuralFastLAS-Fast-Logic-Based-Learning-from-Raw-Data" class="headerlink" title="NeuralFastLAS: Fast Logic-Based Learning from Raw Data"></a>NeuralFastLAS: Fast Logic-Based Learning from Raw Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05145">http://arxiv.org/abs/2310.05145</a></li>
<li>repo_url: None</li>
<li>paper_authors: Theo Charalambous, Yaniv Aspis, Alessandra Russo</li>
<li>for: 本研究旨在提出一种可扩展和高效的综合方法，即NeuralFastLAS，用于同时训练神经网络和符号学习器。</li>
<li>methods: NeuralFastLAS使用一种新的约束优化技术，通过学习一个 posterior distribution 来提高训练稳定性。</li>
<li>results: 实验结果表明，NeuralFastLAS可以在数学和逻辑任务中达到状态革命级别的准确率，训练时间比其他同时训练神经网络和符号学习器的方法快到两个数量级。<details>
<summary>Abstract</summary>
Symbolic rule learners generate interpretable solutions, however they require the input to be encoded symbolically. Neuro-symbolic approaches overcome this issue by mapping raw data to latent symbolic concepts using a neural network. Training the neural and symbolic components jointly is difficult, due to slow and unstable learning, hence many existing systems rely on hand-engineered rules to train the network. We introduce NeuralFastLAS, a scalable and fast end-to-end approach that trains a neural network jointly with a symbolic learner. For a given task, NeuralFastLAS computes a relevant set of rules, proved to contain an optimal symbolic solution, trains a neural network using these rules, and finally finds an optimal symbolic solution to the task while taking network predictions into account. A key novelty of our approach is learning a posterior distribution on rules while training the neural network to improve stability during training. We provide theoretical results for a sufficient condition on network training to guarantee correctness of the final solution. Experimental results demonstrate that NeuralFastLAS is able to achieve state-of-the-art accuracy in arithmetic and logical tasks, with a training time that is up to two orders of magnitude faster than other jointly trained neuro-symbolic methods.
</details>
<details>
<summary>摘要</summary>
symbolic rule learners 可以生成可读解释的解决方案，但是它们需要输入数据被编码成符号形式。 neural-symbolic 方法可以将原始数据映射到隐藏的符号概念上使用神经网络，从而解决这个问题。 然而，在培aujointly trained neural and symbolic components 的问题上，存在慢速和不稳定的学习问题，因此许多现有系统通常采用手工设计规则来训练网络。我们介绍NeuralFastLAS，一种可扩展和快速的终端方法，可以同时训练神经网络和符号学习器。对于给定任务，NeuralFastLAS 可以计算一个相关的规则集，证明其中包含最优的符号解决方案，使用这些规则来训练神经网络，并最终找到一个包含神经网络预测的最优符号解决方案。我们的方法的一个新特点是在培aujointly trained neural and symbolic components 时，学习一个 posterior distribution  sobre rules 以提高培aujoint training 的稳定性。我们提供了理论结果，证明在网络训练时满足某些条件下，可以保证最终解决方案的正确性。实验结果表明，NeuralFastLAS 能够在数学和逻辑任务中达到领先的准确率，并且培aujoint training 时间比其他同时训练的神经网络和符号学习器方法快到两个数量级。
</details></li>
</ul>
<hr>
<h2 id="ZooPFL-Exploring-Black-box-Foundation-Models-for-Personalized-Federated-Learning"><a href="#ZooPFL-Exploring-Black-box-Foundation-Models-for-Personalized-Federated-Learning" class="headerlink" title="ZooPFL: Exploring Black-box Foundation Models for Personalized Federated Learning"></a>ZooPFL: Exploring Black-box Foundation Models for Personalized Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05143">http://arxiv.org/abs/2310.05143</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/personalizedfl">https://github.com/microsoft/personalizedfl</a></li>
<li>paper_authors: Wang Lu, Hao Yu, Jindong Wang, Damien Teney, Haohan Wang, Yiqiang Chen, Qiang Yang, Xing Xie, Xiangyang Ji</li>
<li>for: 这篇论文旨在解决个性化 Federated Learning (FL) 中资源有限的问题，包括数据、计算和通信成本，以及访问模型的限制。</li>
<li>methods: 该论文提出了一种名为 ZOOPFL 的方法，使用零阶优化解决分布偏移问题，并使用简单 yet effective 的线性投影进行个性化。此外，它还使用输入修复来投影预测值。</li>
<li>results: 广泛的实验表明，ZOOPFL 可以有效地应用于黑盒基模型上的 FL 任务，并且可以提高个性化的精度。<details>
<summary>Abstract</summary>
When personalized federated learning (FL) meets large foundation models, new challenges arise from various limitations in resources. In addition to typical limitations such as data, computation, and communication costs, access to the models is also often limited. This paper endeavors to solve both the challenges of limited resources and personalization. i.e., distribution shifts between clients. To do so, we propose a method named ZOOPFL that uses Zeroth-Order Optimization for Personalized Federated Learning. ZOOPFL avoids direct interference with the foundation models and instead learns to adapt its inputs through zeroth-order optimization. In addition, we employ simple yet effective linear projections to remap its predictions for personalization. To reduce the computation costs and enhance personalization, we propose input surgery to incorporate an auto-encoder with low-dimensional and client-specific embeddings. We provide theoretical support for ZOOPFL to analyze its convergence. Extensive empirical experiments on computer vision and natural language processing tasks using popular foundation models demonstrate its effectiveness for FL on black-box foundation models.
</details>
<details>
<summary>摘要</summary>
当个性化联合学习（FL）遇到大规模基础模型时，新的挑战出现，包括不同限制的资源。除了典型的限制，如数据、计算和通信成本外，对模型的访问也经常受限。这篇论文旨在解决限制资源和个性化的两个挑战。即分布shift between客户端。为此，我们提出了一种方法名为ZOOPFL，它使用零阶优化进行个性化联合学习。ZOOPFL避免直接干扰基础模型，而是通过零阶优化学习适应输入。此外，我们使用简单 yet有效的线性映射来重新映射其预测。为了减少计算成本并提高个性化，我们提议输入手术，其中包括一个低维度的自动encoder和客户端特定的嵌入。我们提供了对ZOOPFL的理论支持，以分析其相对稳定性。我们对计算机视觉和自然语言处理任务使用了流行的基础模型进行了广泛的实验，以证明ZOOPFL在黑盒基础模型上的有效性。
</details></li>
</ul>
<hr>
<h2 id="Harnessing-the-Power-of-Large-Language-Models-for-Empathetic-Response-Generation-Empirical-Investigations-and-Improvements"><a href="#Harnessing-the-Power-of-Large-Language-Models-for-Empathetic-Response-Generation-Empirical-Investigations-and-Improvements" class="headerlink" title="Harnessing the Power of Large Language Models for Empathetic Response Generation: Empirical Investigations and Improvements"></a>Harnessing the Power of Large Language Models for Empathetic Response Generation: Empirical Investigations and Improvements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05140">http://arxiv.org/abs/2310.05140</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yushan Qian, Wei-Nan Zhang, Ting Liu</li>
<li>for: 这个论文主要研究了大语言模型（LLMs）在建立和谐社会关系中的应用效果，以及如何使用LLMs提高对话的同理能力。</li>
<li>methods: 本文提出了三种改进方法，包括semantically similar in-context learning、two-stage interactive generation和知识库的组合。</li>
<li>results: 广泛的实验表明，LLMs可以在我们提出的方法的帮助下显著提高对话的同理能力，并在自动和人类评价中达到了领先水平。此外，我们还探讨了GPT-4可以模拟人类评价者的可能性。<details>
<summary>Abstract</summary>
Empathetic dialogue is an indispensable part of building harmonious social relationships and contributes to the development of a helpful AI. Previous approaches are mainly based on fine small-scale language models. With the advent of ChatGPT, the application effect of large language models (LLMs) in this field has attracted great attention. This work empirically investigates the performance of LLMs in generating empathetic responses and proposes three improvement methods of semantically similar in-context learning, two-stage interactive generation, and combination with the knowledge base. Extensive experiments show that LLMs can significantly benefit from our proposed methods and is able to achieve state-of-the-art performance in both automatic and human evaluations. Additionally, we explore the possibility of GPT-4 simulating human evaluators.
</details>
<details>
<summary>摘要</summary>
帮助AI的发展，对话是不可或缺的一部分。以前的方法主要基于细致语言模型。随着ChatGPT的出现，大语言模型（LLMs）在这一领域的应用效果吸引了广泛的关注。本研究employs three improvement methods of semantically similar in-context learning, two-stage interactive generation, and combination with the knowledge base to investigate the performance of LLMs in generating empathetic responses. Our extensive experiments show that LLMs can significantly benefit from our proposed methods and achieve state-of-the-art performance in both automatic and human evaluations. In addition, we explore the possibility of GPT-4 simulating human evaluators.
</details></li>
</ul>
<hr>
<h2 id="Maximizing-Utilitarian-and-Egalitarian-Welfare-of-Fractional-Hedonic-Games-on-Tree-like-Graphs"><a href="#Maximizing-Utilitarian-and-Egalitarian-Welfare-of-Fractional-Hedonic-Games-on-Tree-like-Graphs" class="headerlink" title="Maximizing Utilitarian and Egalitarian Welfare of Fractional Hedonic Games on Tree-like Graphs"></a>Maximizing Utilitarian and Egalitarian Welfare of Fractional Hedonic Games on Tree-like Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05139">http://arxiv.org/abs/2310.05139</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tesshu Hanaka, Airi Ikeyama, Hirotaka Ono</li>
<li>for:  Fractional hedonic games are coalition formation games where a player’s utility is determined by the average value they assign to the members of their coalition.</li>
<li>methods:  The paper presents (pseudo)polynomial-time algorithms to compute welfare-maximizing partitions in fractional hedonic games on tree-like graphs, including two types of social welfare measures: utilitarian and egalitarian.</li>
<li>results:  The paper provides a hardness result, demonstrating that the pseudopolynomial-time solvability is the best possible under the assumption P$\neq$NP.<details>
<summary>Abstract</summary>
Fractional hedonic games are coalition formation games where a player's utility is determined by the average value they assign to the members of their coalition. These games are a variation of graph hedonic games, which are a class of coalition formation games that can be succinctly represented. Due to their applicability in network clustering and their relationship to graph hedonic games, fractional hedonic games have been extensively studied from various perspectives. However, finding welfare-maximizing partitions in fractional hedonic games is a challenging task due to the nonlinearity of utilities. In fact, it has been proven to be NP-hard and can be solved in polynomial time only for a limited number of graph classes, such as trees. This paper presents (pseudo)polynomial-time algorithms to compute welfare-maximizing partitions in fractional hedonic games on tree-like graphs. We consider two types of social welfare measures: utilitarian and egalitarian. Tree-like graphs refer to graphs with bounded treewidth and block graphs. A hardness result is provided, demonstrating that the pseudopolynomial-time solvability is the best possible under the assumption P$\neq$NP.
</details>
<details>
<summary>摘要</summary>
幂数 Hedonic 游戏是一种协会成员选择游戏，其中玩家的产生 utility 取决于他们所在协会的平均价值。这种游戏是图 Hedonic 游戏的一种变种，可以简洁地表示。由于它们在网络划分和图 Hedonic 游戏之间的关系，幂数 Hedonic 游戏已经得到了广泛的研究。然而，在幂数 Hedonic 游戏中找到最大启用分 partitions 是一项困难的任务，因为价值函数是非线性的。事实上，已经证明了这是 NP-hard 问题，只有在一些图类型，如树，可以在多项时间内解决。本文提出了（假）多项时间算法来计算幂数 Hedonic 游戏中的最大启用分 partitions。我们考虑了两种社会利益度量：utilitarian 和 egalitarian。树状图指的是具有固定树宽度的图和块图。我们还提供了一个困难性结果，证明了 pseudopolynomial-time 可行性是最佳的，即 P ≠ NP 的假设下。
</details></li>
</ul>
<hr>
<h2 id="InstructDET-Diversifying-Referring-Object-Detection-with-Generalized-Instructions"><a href="#InstructDET-Diversifying-Referring-Object-Detection-with-Generalized-Instructions" class="headerlink" title="InstructDET: Diversifying Referring Object Detection with Generalized Instructions"></a>InstructDET: Diversifying Referring Object Detection with Generalized Instructions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05136">http://arxiv.org/abs/2310.05136</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jyfenggogo/instructdet">https://github.com/jyfenggogo/instructdet</a></li>
<li>paper_authors: Ronghao Dang, Jiangyan Feng, Haodong Zhang, Chongjian Ge, Lin Song, Lijun Gong, Chengju Liu, Qijun Chen, Feng Zhu, Rui Zhao, Yibing Song</li>
<li>for: 本文提出了一种数据驱动的对象检测方法（InstructDET），用于基于用户指令（referring expressions，REC）进行对象检测。</li>
<li>methods: 本文使用了基于用户指令的数据驱动方法，并利用了新的视觉语言模型（VLM）和大语言模型（LLM）来生成指令和对象 bounding boxes（bbxs）。</li>
<li>results: 本文通过使用 InstructDET 方法和自制的 InDET  dataset，实现了在标准 REC  dataset 和 InDET 测试集上超越现有方法的对象检测性能。<details>
<summary>Abstract</summary>
We propose InstructDET, a data-centric method for referring object detection (ROD) that localizes target objects based on user instructions. While deriving from referring expressions (REC), the instructions we leverage are greatly diversified to encompass common user intentions related to object detection. For one image, we produce tremendous instructions that refer to every single object and different combinations of multiple objects. Each instruction and its corresponding object bounding boxes (bbxs) constitute one training data pair. In order to encompass common detection expressions, we involve emerging vision-language model (VLM) and large language model (LLM) to generate instructions guided by text prompts and object bbxs, as the generalizations of foundation models are effective to produce human-like expressions (e.g., describing object property, category, and relationship). We name our constructed dataset as InDET. It contains images, bbxs and generalized instructions that are from foundation models. Our InDET is developed from existing REC datasets and object detection datasets, with the expanding potential that any image with object bbxs can be incorporated through using our InstructDET method. By using our InDET dataset, we show that a conventional ROD model surpasses existing methods on standard REC datasets and our InDET test set. Our data-centric method InstructDET, with automatic data expansion by leveraging foundation models, directs a promising field that ROD can be greatly diversified to execute common object detection instructions.
</details>
<details>
<summary>摘要</summary>
我们提出了InstructDET，一种数据驱动的引用物体检测（ROD）方法，它基于用户指令来定位目标对象。而我们所利用的指令不仅来自引用表达（REC），还包括各种用户意图相关的对象检测指令。对于一张图像，我们生成了庞大的指令和对象 bounding box（bbxs），每个指令和对应的bbxs组成一个训练数据对。为了涵盖通用的检测表达，我们利用了趋势感知模型（VLM）和大语言模型（LLM），通过文本提示和对象bbxs来引导生成指令，这些基础模型的泛化效果可以生成人类化表达（例如，描述对象属性、类别和关系）。我们称之为InDET，它包含图像、bbxs和通用指令，这些指令来自基础模型。我们的InDET是基于现有REC dataset和对象检测dataset的扩展，可以通过我们的InstructDET方法将任何图像 WITH object bbxsintegrated。通过使用InDET数据集，我们示出了一个标准ROD模型在标准REC dataset和InDET测试集上的表现比普通方法更高。我们的数据驱动方法InstructDET，通过基于基础模型的自动扩展，指明了一个可能的场景，ROD可以通过各种常见的检测指令执行。
</details></li>
</ul>
<hr>
<h2 id="Are-Emily-and-Greg-Still-More-Employable-than-Lakisha-and-Jamal-Investigating-Algorithmic-Hiring-Bias-in-the-Era-of-ChatGPT"><a href="#Are-Emily-and-Greg-Still-More-Employable-than-Lakisha-and-Jamal-Investigating-Algorithmic-Hiring-Bias-in-the-Era-of-ChatGPT" class="headerlink" title="Are Emily and Greg Still More Employable than Lakisha and Jamal? Investigating Algorithmic Hiring Bias in the Era of ChatGPT"></a>Are Emily and Greg Still More Employable than Lakisha and Jamal? Investigating Algorithmic Hiring Bias in the Era of ChatGPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05135">http://arxiv.org/abs/2310.05135</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akshaj Kumar Veldanda, Fabian Grob, Shailja Thakur, Hammond Pearce, Benjamin Tan, Ramesh Karri, Siddharth Garg</li>
<li>for: 这个研究探讨了大语言模型（LLMs）在算法招聘中的应用，特别是将简历与职业类别相匹配。</li>
<li>methods: 研究使用了场景实验来评估大语言模型对保护属性的偏见（如性别、种族和生育状况）的影响。</li>
<li>results: 研究发现，LLMs在不同的种族和性别下表现一致，但在孕期状况和政治倾向上存在偏见。使用了开源的LLMs进行对比输入解码来探讨可能的偏见源。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) such as GPT-3.5, Bard, and Claude exhibit applicability across numerous tasks. One domain of interest is their use in algorithmic hiring, specifically in matching resumes with job categories. Yet, this introduces issues of bias on protected attributes like gender, race and maternity status. The seminal work of Bertrand & Mullainathan (2003) set the gold-standard for identifying hiring bias via field experiments where the response rate for identical resumes that differ only in protected attributes, e.g., racially suggestive names such as Emily or Lakisha, is compared. We replicate this experiment on state-of-art LLMs (GPT-3.5, Bard, Claude and Llama) to evaluate bias (or lack thereof) on gender, race, maternity status, pregnancy status, and political affiliation. We evaluate LLMs on two tasks: (1) matching resumes to job categories; and (2) summarizing resumes with employment relevant information. Overall, LLMs are robust across race and gender. They differ in their performance on pregnancy status and political affiliation. We use contrastive input decoding on open-source LLMs to uncover potential sources of bias.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）如GPT-3.5、Bard和Claude在多个任务中表现出色。一个有趣的领域是它们在算法招聘中的应用，特别是在匹配简历与职业类别之间。然而，这会引入保护特征如性别、种族和生育状况等的偏见。Bertrand & Mullainathan（2003）的著名研究设置了标准 для识别招聘偏见，通过在实验室中对同样的简历进行比较，以确定它们是否具有保护特征。我们在当今最高级的LLMs（GPT-3.5、Bard、Claude和Llama）上重复了这个实验，以评估它们对gender、种族、生育状况、怀孕状况和政治信仰等保护特征的偏见。我们在两个任务上评估LLMs：（1）匹配简历与职业类别之间；和（2）摘要简历中有关雇佣信息。总的来说，LLMs在gender和种族方面都很稳定，但在怀孕状况和政治信仰方面存在差异。我们使用开源LLMs的对比输入解码来探测可能的偏见源。
</details></li>
</ul>
<hr>
<h2 id="ed-cec-improving-rare-word-recognition-using-asr-postprocessing-based-on-error-detection-and-context-aware-error-correction"><a href="#ed-cec-improving-rare-word-recognition-using-asr-postprocessing-based-on-error-detection-and-context-aware-error-correction" class="headerlink" title="ed-cec: improving rare word recognition using asr postprocessing based on error detection and context-aware error correction"></a>ed-cec: improving rare word recognition using asr postprocessing based on error detection and context-aware error correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05129">http://arxiv.org/abs/2310.05129</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiajun He, Zekun Yang, Tomoki Toda</li>
<li>for: 提高自然语言处理（NLP）任务中罕见词的识别精度，以优化下游任务 such as 关键词检测、意图检测和文本概要生成。</li>
<li>methods: 提出了一种基于错误检测和上下文相关知识的ASR后处理方法，通过针对预测出的错误位置进行优化decoding过程，最大化精度while minimizing unnecessary computations。此外，我们还利用罕见词名单提供额外的上下文知识，以便更好地 corrected罕见词。</li>
<li>results: 在五个数据集上实验表明，我们的提议方法可以比前一些方法更好地降低单词错误率（WER），同时保持一定的推理速度，并且在不同的ASR系统上表现出良好的鲁棒性。<details>
<summary>Abstract</summary>
Automatic speech recognition (ASR) systems often encounter difficulties in accurately recognizing rare words, leading to errors that can have a negative impact on downstream tasks such as keyword spotting, intent detection, and text summarization. To address this challenge, we present a novel ASR postprocessing method that focuses on improving the recognition of rare words through error detection and context-aware error correction. Our method optimizes the decoding process by targeting only the predicted error positions, minimizing unnecessary computations. Moreover, we leverage a rare word list to provide additional contextual knowledge, enabling the model to better correct rare words. Experimental results across five datasets demonstrate that our proposed method achieves significantly lower word error rates (WERs) than previous approaches while maintaining a reasonable inference speed. Furthermore, our approach exhibits promising robustness across different ASR systems.
</details>
<details>
<summary>摘要</summary>
自动语音识别（ASR）系统经常遇到罕见词汇识别错误，导致下游任务如关键词检测、意图检测和文本概要 SUMMARIZATION 中的错误。为解决这个挑战，我们提出了一种新的 ASR 后处理方法，旨在提高罕见词汇识别的准确率。我们的方法优化了解码过程，只targeting 预测出的错误位置，最小化无用的计算。此外，我们利用罕见词表来提供额外的contextual knowledge，使模型更好地更正罕见词汇。实验结果 across five datasets 表明，我们提出的方法可以在word error rate（WER）下达到 significanly 更高的准确率，同时保持合理的推理速度。此外，我们的方法在不同的 ASR 系统上也展现出了良好的Robustness。
</details></li>
</ul>
<hr>
<h2 id="Instances-and-Labels-Hierarchy-aware-Joint-Supervised-Contrastive-Learning-for-Hierarchical-Multi-Label-Text-Classification"><a href="#Instances-and-Labels-Hierarchy-aware-Joint-Supervised-Contrastive-Learning-for-Hierarchical-Multi-Label-Text-Classification" class="headerlink" title="Instances and Labels: Hierarchy-aware Joint Supervised Contrastive Learning for Hierarchical Multi-Label Text Classification"></a>Instances and Labels: Hierarchy-aware Joint Supervised Contrastive Learning for Hierarchical Multi-Label Text Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05128">http://arxiv.org/abs/2310.05128</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/simonucl/HJCL">https://github.com/simonucl/HJCL</a></li>
<li>paper_authors: Simon Chi Lok U, Jie He, Víctor Gutiérrez-Basulto, Jeff Z. Pan</li>
<li>for: 这个研究的目的是解决多个标签分类中的多个标签间的关联性问题。</li>
<li>methods: 这个研究使用了对生成的标签类别进行对照学习，以将文本和标签嵌入更加接近。</li>
<li>results: 实验结果显示，HJCL可以实现了优异的结果，并且显示了对于多个标签分类的效果。<details>
<summary>Abstract</summary>
Hierarchical multi-label text classification (HMTC) aims at utilizing a label hierarchy in multi-label classification. Recent approaches to HMTC deal with the problem of imposing an over-constrained premise on the output space by using contrastive learning on generated samples in a semi-supervised manner to bring text and label embeddings closer. However, the generation of samples tends to introduce noise as it ignores the correlation between similar samples in the same batch. One solution to this issue is supervised contrastive learning, but it remains an underexplored topic in HMTC due to its complex structured labels. To overcome this challenge, we propose $\textbf{HJCL}$, a $\textbf{H}$ierarchy-aware $\textbf{J}$oint Supervised $\textbf{C}$ontrastive $\textbf{L}$earning method that bridges the gap between supervised contrastive learning and HMTC. Specifically, we employ both instance-wise and label-wise contrastive learning techniques and carefully construct batches to fulfill the contrastive learning objective. Extensive experiments on four multi-path HMTC datasets demonstrate that HJCL achieves promising results and the effectiveness of Contrastive Learning on HMTC.
</details>
<details>
<summary>摘要</summary>
Here's the translation in Simplified Chinese: Hierarchical Multi-label Text Classification (HMTC) targets 使用标签层次结构在多标签分类中。现有的 HMTC 方法面临着通过使用半supervised contrastive learning 来违规输出空间的问题，这会使文本和标签嵌入更加紧密。然而，生成样本通常会引入噪声，因为它们忽略了同一个批处理中的相似样本之间的相关性。一种解决这个问题的方法是使用supervised contrastive learning，但它在 HMTC 中尚未得到充分发挥。为了 bridge 这两种方法之间的差异，我们提出了 Hierarchy-aware Joint Supervised Contrastive Learning (HJCL) 方法。特别是，我们使用了 both instance-wise 和 label-wise contrastive learning 技术，并且细心地构造批处理来满足对做对的 contrastive learning 目标。广泛的实验表明，HJCL 在四个多路 HMTC 数据集上达到了可塑性和 HMTC 中的对比学习的效果。
</details></li>
</ul>
<hr>
<h2 id="UReader-Universal-OCR-free-Visually-situated-Language-Understanding-with-Multimodal-Large-Language-Model"><a href="#UReader-Universal-OCR-free-Visually-situated-Language-Understanding-with-Multimodal-Large-Language-Model" class="headerlink" title="UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model"></a>UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05126">http://arxiv.org/abs/2310.05126</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lukeforeveryoung/ureader">https://github.com/lukeforeveryoung/ureader</a></li>
<li>paper_authors: Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Guohai Xu, Chenliang Li, Junfeng Tian, Qi Qian, Ji Zhang, Qin Jin, Liang He, Xin Alex Lin, Fei Huang</li>
<li>for: 这个研究旨在提出一个universal OCR-free visually-situated language understanding模型，以便在文档、表格、图表、自然图像和网页 screenshot 等多种类型的视觉文本中进行语言理解。</li>
<li>methods: 本研究使用 Multimodal Large Language Model (MLLM)，并将其训练为可以进行多种类型的视觉文本理解任务，包括文档、表格、图表、自然图像和网页 screenshot 等。此外，研究者还将两个辅助任务添加到模型中，以增强模型的视觉文本和 semantics 理解能力。</li>
<li>results: 根据研究结果，这个单一模型可以在8个不同类型的视觉文本理解任务中实现state-of-the-art的性能，不需要进行下游训练。此外，研究者还发现这个模型可以对高分辨率的图像进行有效的处理，并且可以快速地处理大量的视觉文本。<details>
<summary>Abstract</summary>
Text is ubiquitous in our visual world, conveying crucial information, such as in documents, websites, and everyday photographs. In this work, we propose UReader, a first exploration of universal OCR-free visually-situated language understanding based on the Multimodal Large Language Model (MLLM). By leveraging the shallow text recognition ability of the MLLM, we only finetuned 1.2% parameters and the training cost is much lower than previous work following domain-specific pretraining and finetuning paradigms. Concretely, UReader is jointly finetuned on a wide range of Visually-situated Language Understanding tasks via a unified instruction format. To enhance the visual text and semantic understanding, we further apply two auxiliary tasks with the same format, namely text reading and key points generation tasks. We design a shape-adaptive cropping module before the encoder-decoder architecture of MLLM to leverage the frozen low-resolution vision encoder for processing high-resolution images. Without downstream finetuning, our single model achieves state-of-the-art ocr-free performance in 8 out of 10 visually-situated language understanding tasks, across 5 domains: documents, tables, charts, natural images, and webpage screenshots. Codes and instruction-tuning datasets will be released.
</details>
<details>
<summary>摘要</summary>
文本在我们的视觉世界中 ubique, 传递重要信息，如文档、网站和日常照片。在这项工作中，我们提出了 UReader，一种首次探索的无需 OCR 的通用视觉语言理解基于多模态大语言模型（MLLM）。我们利用 MLLM 的浅文本认知能力，只需要 finetune 1.2% 的参数，训练成本远低于先前的领域特定预训练和 fine-tuning 方法。具体来说，UReader 是通过一种统一的指令格式进行联合训练多种视觉语言理解任务。为了增强视觉文本和 semantics 理解，我们还应用了两个辅助任务，即文本读取和关键点生成任务。我们设计了适应形式的截取模块，以便使用冻结的低分辨率视觉Encoder 处理高分辨率图像。无需下游训练，我们的单个模型在 8 个视觉语言理解任务中 achievement state-of-the-art OCR-free 性能，覆盖 5 个领域：文档、表格、图表、自然图像和网页截屏。我们将代码和 instrucion-tuning 数据集发布。
</details></li>
</ul>
<hr>
<h2 id="Distribution-Based-Trajectory-Clustering"><a href="#Distribution-Based-Trajectory-Clustering" class="headerlink" title="Distribution-Based Trajectory Clustering"></a>Distribution-Based Trajectory Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05123">http://arxiv.org/abs/2310.05123</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/IsolationKernel/TIDKC">https://github.com/IsolationKernel/TIDKC</a></li>
<li>paper_authors: Zi Jing Wang, Ye Zhu, Kai Ming Ting</li>
<li>for:  trajectory clustering, 探索 trajectory 数据中的共同模式</li>
<li>methods: 使用 Isolation Distributional Kernel (IDK) 作为主要工具，以实现 trajectory 相似度测量和归类</li>
<li>results: 比较传统和深度学习基于距离度量的方法，IDK 能够更好地捕捉 trajectory 中复杂的结构，并且提供了更高效和稳定的归类性能。<details>
<summary>Abstract</summary>
Trajectory clustering enables the discovery of common patterns in trajectory data. Current methods of trajectory clustering rely on a distance measure between two points in order to measure the dissimilarity between two trajectories. The distance measures employed have two challenges: high computational cost and low fidelity. Independent of the distance measure employed, existing clustering algorithms have another challenge: either effectiveness issues or high time complexity. In this paper, we propose to use a recent Isolation Distributional Kernel (IDK) as the main tool to meet all three challenges. The new IDK-based clustering algorithm, called TIDKC, makes full use of the distributional kernel for trajectory similarity measuring and clustering. TIDKC identifies non-linearly separable clusters with irregular shapes and varied densities in linear time. It does not rely on random initialisation and is robust to outliers. An extensive evaluation on 7 large real-world trajectory datasets confirms that IDK is more effective in capturing complex structures in trajectories than traditional and deep learning-based distance measures. Furthermore, the proposed TIDKC has superior clustering performance and efficiency to existing trajectory clustering algorithms.
</details>
<details>
<summary>摘要</summary>
trajectory clustering可以揭示行程数据中的共同模式。现有的行程 clustering方法都基于两点之间的距离度量来衡量行程之间的不同。现有的距离度量面临两个挑战：高计算成本和低准确性。独立于选择的距离度量，现有的归类算法又面临另一个挑战：效果不佳或高时间复杂度。在本文中，我们提议使用最近的隔离分布 kernel（IDK）作为主要工具，以解决这三个挑战。我们称之为 TIDKC 归类算法。 TIDKC 利用分布 kernel 来衡量行程之间的相似度，并且可以快速地找到非线性分割的弯曲形状和不规则的分布。它不需要随机初始化，并且对异常值有较高的Robustness。我们对 7 个大的实际行程数据集进行了广泛的评估，发现 IDK 可以更好地捕捉行程中的复杂结构，比传统和深度学习基于的距离度量更有效。此外，我们的提议的 TIDKC 归类算法也比现有的行程归类算法有更高的归类性和效率。
</details></li>
</ul>
<hr>
<h2 id="Breaking-Down-Word-Semantics-from-Pre-trained-Language-Models-through-Layer-wise-Dimension-Selection"><a href="#Breaking-Down-Word-Semantics-from-Pre-trained-Language-Models-through-Layer-wise-Dimension-Selection" class="headerlink" title="Breaking Down Word Semantics from Pre-trained Language Models through Layer-wise Dimension Selection"></a>Breaking Down Word Semantics from Pre-trained Language Models through Layer-wise Dimension Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05115">http://arxiv.org/abs/2310.05115</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nayoung Choi</li>
<li>for: 本研究旨在分析BERT中各层中的不同语言知识，以及分离含义的不同方面。</li>
<li>methods: 本研究使用了一种 binary mask 技术，将中间输出规范化到不同层，以便分离含义。</li>
<li>results: 实验结果表明，通过层划分信息可以提高表达效果，而分离含义更进一步提高表达效果。<details>
<summary>Abstract</summary>
Contextual word embeddings obtained from pre-trained language model (PLM) have proven effective for various natural language processing tasks at the word level. However, interpreting the hidden aspects within embeddings, such as syntax and semantics, remains challenging. Disentangled representation learning has emerged as a promising approach, which separates specific aspects into distinct embeddings. Furthermore, different linguistic knowledge is believed to be stored in different layers of PLM. This paper aims to disentangle semantic sense from BERT by applying a binary mask to middle outputs across the layers, without updating pre-trained parameters. The disentangled embeddings are evaluated through binary classification to determine if the target word in two different sentences has the same meaning. Experiments with cased BERT$_{\texttt{base}$ show that leveraging layer-wise information is effective and disentangling semantic sense further improve performance.
</details>
<details>
<summary>摘要</summary>
Contextual word embeddings obtained from pre-trained language model (PLM) have proven effective for various natural language processing tasks at the word level. However, interpreting the hidden aspects within embeddings, such as syntax and semantics, remains challenging. Disentangled representation learning has emerged as a promising approach, which separates specific aspects into distinct embeddings. Furthermore, different linguistic knowledge is believed to be stored in different layers of PLM. This paper aims to disentangle semantic sense from BERT by applying a binary mask to middle outputs across the layers, without updating pre-trained parameters. The disentangled embeddings are evaluated through binary classification to determine if the target word in two different sentences has the same meaning. Experiments with cased BERT$_{\texttt{base}$ show that leveraging layer-wise information is effective and disentangling semantic sense further improves performance.Here's the translation in Traditional Chinese:Contextual word embeddings obtained from pre-trained language model (PLM) have proven effective for various natural language processing tasks at the word level. However, interpreting the hidden aspects within embeddings, such as syntax and semantics, remains challenging. Disentangled representation learning has emerged as a promising approach, which separates specific aspects into distinct embeddings. Furthermore, different linguistic knowledge is believed to be stored in different layers of PLM. This paper aims to disentangle semantic sense from BERT by applying a binary mask to middle outputs across the layers, without updating pre-trained parameters. The disentangled embeddings are evaluated through binary classification to determine if the target word in two different sentences has the same meaning. Experiments with cased BERT$_{\texttt{base}$ show that leveraging layer-wise information is effective and disentangling semantic sense further improves performance.
</details></li>
</ul>
<hr>
<h2 id="Zero-Shot-Detection-of-Machine-Generated-Codes"><a href="#Zero-Shot-Detection-of-Machine-Generated-Codes" class="headerlink" title="Zero-Shot Detection of Machine-Generated Codes"></a>Zero-Shot Detection of Machine-Generated Codes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05103">http://arxiv.org/abs/2310.05103</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/baoguangsheng/fast-detect-gpt">https://github.com/baoguangsheng/fast-detect-gpt</a></li>
<li>paper_authors: Xianjun Yang, Kexun Zhang, Haifeng Chen, Linda Petzold, William Yang Wang, Wei Cheng</li>
<li>for: 本研究旨在提出一种不需要训练的方法，用于检测 LLMS 生成的代码，以避免这些代码的不当使用而带来的风险。</li>
<li>methods: 我们修改了之前的零批文本检测方法 DetectGPT（Mitchell et al., 2023），使用一个代理白盒模型来估算最右侧的字符的概率，以便识别由语言模型生成的代码片断。</li>
<li>results: 我们通过对 CodeContest 和 APPS 数据集的 python 代码进行了广泛的实验，并demonstrated 我们的方法可以在 text-davinci-003、GPT-3.5 和 GPT-4 模型上达到领先的检测结果。此外，我们的方法还能够抗 Reynolds 攻击和通用化到 Java 代码。<details>
<summary>Abstract</summary>
This work proposes a training-free approach for the detection of LLMs-generated codes, mitigating the risks associated with their indiscriminate usage. To the best of our knowledge, our research is the first to investigate zero-shot detection techniques applied to code generated by advanced black-box LLMs like ChatGPT. Firstly, we find that existing training-based or zero-shot text detectors are ineffective in detecting code, likely due to the unique statistical properties found in code structures. We then modify the previous zero-shot text detection method, DetectGPT (Mitchell et al., 2023) by utilizing a surrogate white-box model to estimate the probability of the rightmost tokens, allowing us to identify code snippets generated by language models. Through extensive experiments conducted on the python codes of the CodeContest and APPS dataset, our approach demonstrates its effectiveness by achieving state-of-the-art detection results on text-davinci-003, GPT-3.5, and GPT-4 models. Moreover, our method exhibits robustness against revision attacks and generalizes well to Java codes. We also find that the smaller code language model like PolyCoder-160M performs as a universal code detector, outperforming the billion-scale counterpart. The codes will be available at https://github.com/ Xianjun-Yang/Code_detection.git
</details>
<details>
<summary>摘要</summary>
这个研究提出了一种不需要训练的方法，用于检测 LLMs 生成的代码，从而降低这些代码的不当使用所带来的风险。据我们所知，我们的研究是首次应用零shot 检测技术于高级黑盒 LLMs 如 ChatGPT 生成的代码中。我们发现，现有的训练基于或零shot 文本检测器都不能有效地检测代码，可能是因为代码结构的独特统计特性。我们然后对之前的零shot 文本检测方法 DetectGPT（Mitchell et al., 2023）进行修改，通过利用代理白盒模型来估计右侧的最后几个字符的概率，从而识别 LLMs 生成的代码片断。经过对 Python 代码 dataset CodeContest 和 APPS 进行了广泛的实验，我们的方法在 text-davinci-003、GPT-3.5 和 GPT-4 模型上达到了最佳检测结果。此外，我们的方法还能够抗 revision 攻击，并在 Java 代码上显示良好的泛化性。我们还发现，较小的代码语言模型 PolyCoder-160M 可以作为一个通用的代码检测器，超过了一个百亿级模型的性能。代码将在 <https://github.com/Xianjun-Yang/Code_detection.git> 上提供。
</details></li>
</ul>
<hr>
<h2 id="Intelligent-DRL-Based-Adaptive-Region-of-Interest-for-Delay-sensitive-Telemedicine-Applications"><a href="#Intelligent-DRL-Based-Adaptive-Region-of-Interest-for-Delay-sensitive-Telemedicine-Applications" class="headerlink" title="Intelligent DRL-Based Adaptive Region of Interest for Delay-sensitive Telemedicine Applications"></a>Intelligent DRL-Based Adaptive Region of Interest for Delay-sensitive Telemedicine Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05099">http://arxiv.org/abs/2310.05099</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdulrahman Soliman, Amr Mohamed, Elias Yaacoub, Nikhil V. Navkar, Aiman Erbad</li>
<li>for: 本研究旨在提高 телемедицина应用的效率和质量，尤其是在 COVID-19 大流行后。</li>
<li>methods: 本研究使用 Deep Reinforcement Learning（DRL）模型，智能调整 ROI 大小和非 ROI 质量，以适应网络带宽变化。</li>
<li>results: 比较结果表明，DRL 模型可以降低延迟率 by 13%，并保持总质量在可接受范围内。这些发现对 телемедицина应用有很大的价值提升。<details>
<summary>Abstract</summary>
Telemedicine applications have recently received substantial potential and interest, especially after the COVID-19 pandemic. Remote experience will help people get their complex surgery done or transfer knowledge to local surgeons, without the need to travel abroad. Even with breakthrough improvements in internet speeds, the delay in video streaming is still a hurdle in telemedicine applications. This imposes using image compression and region of interest (ROI) techniques to reduce the data size and transmission needs. This paper proposes a Deep Reinforcement Learning (DRL) model that intelligently adapts the ROI size and non-ROI quality depending on the estimated throughput. The delay and structural similarity index measure (SSIM) comparison are used to assess the DRL model. The comparison findings and the practical application reveal that DRL is capable of reducing the delay by 13% and keeping the overall quality in an acceptable range. Since the latency has been significantly reduced, these findings are a valuable enhancement to telemedicine applications.
</details>
<details>
<summary>摘要</summary>
随着 télémedicine 应用的潜在和兴趣的不断增长，尤其是在 COVID-19 大流行之后。远程经验可以帮助人们完成复杂的手术或传输知识到地方外科医生，无需出国。尽管互联网速度有了 significative 的改善，但视频流程延迟仍然是 телеmedicine 应用的一大障碍。为了解决这个问题，这篇论文提出了一种基于深度强化学习（DRL）模型，该模型可以智能调整 ROI 大小和非 ROI 质量，以适应估算的吞吐量。延迟和结构相似度指数（SSIM）比较是用于评估 DRL 模型的。对比结果和实际应用显示，DRL 可以降低延迟约 13%，并保持总质量在可接受范围内。由于延迟得到了重要的减少，这些发现对 télémedicine 应用是有价值的改进。
</details></li>
</ul>
<hr>
<h2 id="How-Reliable-Are-AI-Generated-Text-Detectors-An-Assessment-Framework-Using-Evasive-Soft-Prompts"><a href="#How-Reliable-Are-AI-Generated-Text-Detectors-An-Assessment-Framework-Using-Evasive-Soft-Prompts" class="headerlink" title="How Reliable Are AI-Generated-Text Detectors? An Assessment Framework Using Evasive Soft Prompts"></a>How Reliable Are AI-Generated-Text Detectors? An Assessment Framework Using Evasive Soft Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05095">http://arxiv.org/abs/2310.05095</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tharindu Kumarage, Paras Sheth, Raha Moraffah, Joshua Garland, Huan Liu</li>
<li>for: 本研究旨在评估高性能探测器的可靠性，以响应AI生成文本的滥用问题。</li>
<li>methods: 我们提出了一种新的应对方法，即通过调整PLM的软提示来导致PLM生成”人类化”的文本，以诱导探测器做出错误判断。我们在两步中实现了universal逃脱提示：首先，我们为特定PLM设计了逃脱软提示，然后通过软提示的传输性来将学习到的逃脱软提示传递到另一个PLM上。</li>
<li>results: 我们通过多种PLM在不同写作任务中进行了广泛的实验，并评估了逃脱软提示的效果。结果表明，逃脱软提示能够成功地诱导探测器做出错误判断，并且可以在不同的PLM和写作任务中实现高度的可重复性和稳定性。<details>
<summary>Abstract</summary>
In recent years, there has been a rapid proliferation of AI-generated text, primarily driven by the release of powerful pre-trained language models (PLMs). To address the issue of misuse associated with AI-generated text, various high-performing detectors have been developed, including the OpenAI detector and the Stanford DetectGPT. In our study, we ask how reliable these detectors are. We answer the question by designing a novel approach that can prompt any PLM to generate text that evades these high-performing detectors. The proposed approach suggests a universal evasive prompt, a novel type of soft prompt, which guides PLMs in producing "human-like" text that can mislead the detectors. The novel universal evasive prompt is achieved in two steps: First, we create an evasive soft prompt tailored to a specific PLM through prompt tuning; and then, we leverage the transferability of soft prompts to transfer the learned evasive soft prompt from one PLM to another. Employing multiple PLMs in various writing tasks, we conduct extensive experiments to evaluate the efficacy of the evasive soft prompts in their evasion of state-of-the-art detectors.
</details>
<details>
<summary>摘要</summary>
近年来，人工智能生成文本的迅速扩散，主要受到强大预训练语言模型（PLM）的释放所驱动。为了解决人工智能生成文本的违规问题，许多高性能的检测器被开发出来，包括OpenAI检测器和斯坦福DetectGPT。在我们的研究中，我们问到这些检测器的可靠性。我们回答这个问题，我们设计了一种新的方法，可以让任何PLM生成文本，以逃脱这些高性能的检测器。我们的方法建议一种通用逃脱提示，一种新的软提示，可以导引PLM生成“人类化”的文本，使检测器受到误导。我们的新通用逃脱提示包括两个步骤：首先，我们通过提示调整制定一个逃脱软提示，适应特定PLM；然后，我们利用软提示的传输性，将学习的逃脱软提示从一个PLM传递到另一个PLM。通过多种PLM在不同的写作任务中使用，我们进行了广泛的实验来评估逃脱软提示的有效性。
</details></li>
</ul>
<hr>
<h2 id="Learning-Generalizable-Agents-via-Saliency-Guided-Features-Decorrelation"><a href="#Learning-Generalizable-Agents-via-Saliency-Guided-Features-Decorrelation" class="headerlink" title="Learning Generalizable Agents via Saliency-Guided Features Decorrelation"></a>Learning Generalizable Agents via Saliency-Guided Features Decorrelation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05086">http://arxiv.org/abs/2310.05086</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sili Huang, Yanchao Sun, Jifeng Hu, Siyuan Guo, Hechang Chen, Yi Chang, Lichao Sun, Bo Yang</li>
<li>for: 实现在视觉基于学习（Reinforcement Learning，RL）中agent能够通过环境变化域对环境变化的应对。</li>
<li>methods: 我们提出了Saliency-Guided Features Decorrelation（SGFD），它包括两个核心技术：Random Fourier Functions（RFF）和Saliency map。 RFF用于估计高维度像像的复杂非线性相关，而Saliency map则用于识别变化的特征。 SGFD透过样本重新权重的方式，以降低相关于变化特征的估计相关性，实现特征decorrelation。</li>
<li>results: 我们的实验结果显示，SGFD可以在广泛的试验环境中实现很好的通过率，并在处理任务不相关的变化和任务相关的变化方面具有明显的改善。<details>
<summary>Abstract</summary>
In visual-based Reinforcement Learning (RL), agents often struggle to generalize well to environmental variations in the state space that were not observed during training. The variations can arise in both task-irrelevant features, such as background noise, and task-relevant features, such as robot configurations, that are related to the optimal decisions. To achieve generalization in both situations, agents are required to accurately understand the impact of changed features on the decisions, i.e., establishing the true associations between changed features and decisions in the policy model. However, due to the inherent correlations among features in the state space, the associations between features and decisions become entangled, making it difficult for the policy to distinguish them. To this end, we propose Saliency-Guided Features Decorrelation (SGFD) to eliminate these correlations through sample reweighting. Concretely, SGFD consists of two core techniques: Random Fourier Functions (RFF) and the saliency map. RFF is utilized to estimate the complex non-linear correlations in high-dimensional images, while the saliency map is designed to identify the changed features. Under the guidance of the saliency map, SGFD employs sample reweighting to minimize the estimated correlations related to changed features, thereby achieving decorrelation in visual RL tasks. Our experimental results demonstrate that SGFD can generalize well on a wide range of test environments and significantly outperforms state-of-the-art methods in handling both task-irrelevant variations and task-relevant variations.
</details>
<details>
<summary>摘要</summary>
在视觉基于的回归学习（RL）中，代理人经常难以通过训练不包括的环境变化来泛化良好。这些变化可能来自任务不相关的特征，如背景噪音，也可能来自任务相关的特征，如机器人配置，都与优化的决策相关。为了在这两种情况下实现泛化，代理人需要准确地理解变化特征对决策的影响，即在政策模型中建立真实的关联。然而，由于状态空间中特征之间的自然相关性，这些关联变得杂乱不清晰，使得政策很难分辨它们。为此，我们提出了吸引力引导特征分解（SGFD），通过样本重新权重来消除这些相关性。SGFD包括两种核心技术：Random Fourier Functions（RFF）和Saliency Map。RFF用于估计高维图像中复杂非线性相关性，而Saliency Map则用于标识变化特征。在Saliency Map的引导下，SGFD通过样本重新权重来减少相关性，从而实现特征分解。我们的实验结果表明，SGFD可以在各种测试环境上广泛泛化，并在处理任务不相关的变化和任务相关的变化方面显著超越当前的方法。
</details></li>
</ul>
<hr>
<h2 id="FLatS-Principled-Out-of-Distribution-Detection-with-Feature-Based-Likelihood-Ratio-Score"><a href="#FLatS-Principled-Out-of-Distribution-Detection-with-Feature-Based-Likelihood-Ratio-Score" class="headerlink" title="FLatS: Principled Out-of-Distribution Detection with Feature-Based Likelihood Ratio Score"></a>FLatS: Principled Out-of-Distribution Detection with Feature-Based Likelihood Ratio Score</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05083">http://arxiv.org/abs/2310.05083</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/linhaowei1/flats">https://github.com/linhaowei1/flats</a></li>
<li>paper_authors: Haowei Lin, Yuntian Gu</li>
<li>for: 本文旨在提出一种理论支持的外围样本检测方法，用于帮助NLPT模型在实际应用中更好地识别外围样本。</li>
<li>methods: 本文提出的方法基于likelihood比率的思想，通过对外围分布$\mathcal P_{\textit{out}$和内围分布$\mathcal P_{\textit{in}$的比较，来评估测试样本$\boldsymbol{x}$的”外围性”。而现有的SOTA方法，如Maha和KNN，只计算内围分布$p_{\textit{in}(\boldsymbol{x})$，因此是不优的。</li>
<li>results: 实验表明，提出的FLatS方法可以在 популяр的 benchmark 上建立新的SOTA。此外，FLatS 还可以增强其他OOD检测方法，通过包含外围分布 $p_{\textit{out}(\boldsymbol{x})$ 的估计。<details>
<summary>Abstract</summary>
Detecting out-of-distribution (OOD) instances is crucial for NLP models in practical applications. Although numerous OOD detection methods exist, most of them are empirical. Backed by theoretical analysis, this paper advocates for the measurement of the "OOD-ness" of a test case $\boldsymbol{x}$ through the likelihood ratio between out-distribution $\mathcal P_{\textit{out}$ and in-distribution $\mathcal P_{\textit{in}$. We argue that the state-of-the-art (SOTA) feature-based OOD detection methods, such as Maha and KNN, are suboptimal since they only estimate in-distribution density $p_{\textit{in}(\boldsymbol{x})$. To address this issue, we propose FLatS, a principled solution for OOD detection based on likelihood ratio. Moreover, we demonstrate that FLatS can serve as a general framework capable of enhancing other OOD detection methods by incorporating out-distribution density $p_{\textit{out}(\boldsymbol{x})$ estimation. Experiments show that FLatS establishes a new SOTA on popular benchmarks. Our code is publicly available at https://github.com/linhaowei1/FLatS.
</details>
<details>
<summary>摘要</summary>
检测外部分布（OOD）实例是NLTP模型在实际应用中的关键。虽然有许多OOD检测方法存在，但大多数都是经验的。本文通过理论分析，提出测量测试 caso $\boldsymbol{x}$ 的 "OOD-ness" 通过likelihood比率计算，即在外部分布 $\mathcal P_{\textit{out}$ 和内部分布 $\mathcal P_{\textit{in}$ 之间的比较。我们认为现有的SOTA feature-based OOD检测方法，如Maha和KNN，是不佳的，因为它们只估计内部分布 $p_{\textit{in}(\boldsymbol{x})$。为解决这一问题，我们提出了FLatS，一种理解的OOD检测方法，基于likelihood比率。此外，我们还证明FLatS可以增强其他OOD检测方法，通过包含外部分布 $p_{\textit{out}(\boldsymbol{x})$ 估计。实验表明，FLatS在 популяр的benchmark上建立了新的SOTA。我们的代码在https://github.com/linhaowei1/FLatS上公开。
</details></li>
</ul>
<hr>
<h2 id="“A-Nova-Eletricidade-Aplicacoes-Riscos-e-Tendencias-da-IA-Moderna-–-“The-New-Electricity”-Applications-Risks-and-Trends-in-Current-AI"><a href="#“A-Nova-Eletricidade-Aplicacoes-Riscos-e-Tendencias-da-IA-Moderna-–-“The-New-Electricity”-Applications-Risks-and-Trends-in-Current-AI" class="headerlink" title="“A Nova Eletricidade: Aplicações, Riscos e Tendências da IA Moderna – “The New Electricity”: Applications, Risks, and Trends in Current AI"></a>“A Nova Eletricidade: Aplicações, Riscos e Tendências da IA Moderna – “The New Electricity”: Applications, Risks, and Trends in Current AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18324">http://arxiv.org/abs/2310.18324</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ana L. C. Bazzan, Anderson R. Tavares, André G. Pereira, Cláudio R. Jung, Jacob Scharcanski, Joel Luis Carbonera, Luís C. Lamb, Mariana Recamonde-Mendoza, Thiago L. T. da Silveira, Viviane Moreira<br>for:* The paper is written to provide an overview of the ever-evolving landscape of Artificial Intelligence (AI) and its applications in various sectors of the economy, impacting society and humanity.methods:* The paper analyzes the risks that come with rapid technological progress and future trends in AI, as well as the potential for AI to become a general-purpose technology like electricity.results:* The paper explores the transformative impact of AI on society, with the potential to revolutionize sectors of the economy and impact humanity in the same way that electricity did in the 19th and 20th centuries.<details>
<summary>Abstract</summary>
The thought-provoking analogy between AI and electricity, made by computer scientist and entrepreneur Andrew Ng, summarizes the deep transformation that recent advances in Artificial Intelligence (AI) have triggered in the world. This chapter presents an overview of the ever-evolving landscape of AI, written in Portuguese. With no intent to exhaust the subject, we explore the AI applications that are redefining sectors of the economy, impacting society and humanity. We analyze the risks that may come along with rapid technological progress and future trends in AI, an area that is on the path to becoming a general-purpose technology, just like electricity, which revolutionized society in the 19th and 20th centuries.   A provocativa compara\c{c}\~ao entre IA e eletricidade, feita pelo cientista da computa\c{c}\~ao e empreendedor Andrew Ng, resume a profunda transforma\c{c}\~ao que os recentes avan\c{c}os em Intelig\^encia Artificial (IA) t\^em desencadeado no mundo. Este cap\'itulo apresenta uma vis\~ao geral pela paisagem em constante evolu\c{c}\~ao da IA. Sem pretens\~oes de exaurir o assunto, exploramos as aplica\c{c}\~oes que est\~ao redefinindo setores da economia, impactando a sociedade e a humanidade. Analisamos os riscos que acompanham o r\'apido progresso tecnol\'ogico e as tend\^encias futuras da IA, \'area que trilha o caminho para se tornar uma tecnologia de prop\'osito geral, assim como a eletricidade, que revolucionou a sociedade dos s\'eculos XIX e XX.
</details>
<details>
<summary>摘要</summary>
思想提出的人工智能和电力相似性比喻，由计算机科学家和企业家安드鲁·涅（Andrew Ng）提出，概括了由最近的人工智能技术进步所Trigger的深刻变革。本章介绍了人工智能领域的不断发展，无意尝试涵盖所有方面。我们探讨人工智能在经济、社会和人类生活中的应用，以及可能随着技术进步而出现的风险。我们还分析了人工智能的未来趋势，该领域正在踏上成为一种通用技术的道路，类似于电力在19世纪和20世纪所 triggers 的社会革命。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese is also widely used, especially in Taiwan, Hong Kong, and Macau.
</details></li>
</ul>
<hr>
<h2 id="DialCoT-Meets-PPO-Decomposing-and-Exploring-Reasoning-Paths-in-Smaller-Language-Models"><a href="#DialCoT-Meets-PPO-Decomposing-and-Exploring-Reasoning-Paths-in-Smaller-Language-Models" class="headerlink" title="DialCoT Meets PPO: Decomposing and Exploring Reasoning Paths in Smaller Language Models"></a>DialCoT Meets PPO: Decomposing and Exploring Reasoning Paths in Smaller Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05074">http://arxiv.org/abs/2310.05074</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hccngu/dialcot">https://github.com/hccngu/dialcot</a></li>
<li>paper_authors: Chengcheng Han, Xiaowei Du, Che Zhang, Yixin Lian, Xiang Li, Ming Gao, Baoyuan Wang</li>
<li>for: 提高小语言模型（SLM）的逻辑能力</li>
<li>methods: 对 reasoning 任务进行对话指导，并使用 proximal policy optimization（PPO）算法优化逻辑路径选择</li>
<li>results: 在四个算术逻辑 dataset 上实现了显著性能提升，比前一代竞争者更好<details>
<summary>Abstract</summary>
Chain-of-Thought (CoT) prompting has proven to be effective in enhancing the reasoning capabilities of Large Language Models (LLMs) with at least 100 billion parameters. However, it is ineffective or even detrimental when applied to reasoning tasks in Smaller Language Models (SLMs) with less than 10 billion parameters. To address this limitation, we introduce Dialogue-guided Chain-of-Thought (DialCoT) which employs a dialogue format to generate intermediate reasoning steps, guiding the model toward the final answer. Additionally, we optimize the model's reasoning path selection using the Proximal Policy Optimization (PPO) algorithm, further enhancing its reasoning capabilities. Our method offers several advantages compared to previous approaches. Firstly, we transform the process of solving complex reasoning questions by breaking them down into a series of simpler sub-questions, significantly reducing the task difficulty and making it more suitable for SLMs. Secondly, we optimize the model's reasoning path selection through the PPO algorithm. We conduct comprehensive experiments on four arithmetic reasoning datasets, demonstrating that our method achieves significant performance improvements compared to state-of-the-art competitors.
</details>
<details>
<summary>摘要</summary>
大脑语言模型（LLM）的逻辑能力可以通过链条思维（CoT）提示来提高，但是当应用于少于100亿参数的小语言模型（SLM）时，CoT的效果减弱或者甚至有害。为了解决这个局限性，我们提出了对话引导链条思维（DialCoT），它使用对话格式生成中间逻辑步骤，导引模型到答案。此外，我们使用距离策略优化（PPO）算法优化模型的逻辑路径选择，进一步提高其逻辑能力。我们的方法具有以下优势：首先，我们将复杂的逻辑问题转化为一系列更加简单的子问题，从而大大减轻任务难度，使SLM更适合处理。其次，我们通过PPO算法优化模型的逻辑路径选择，从而提高模型的逻辑能力。我们对四个数学逻辑数据集进行了广泛的实验，显示我们的方法与当前的竞争对手相比有显著的性能提升。
</details></li>
</ul>
<hr>
<h2 id="Video-CSR-Complex-Video-Digest-Creation-for-Visual-Language-Models"><a href="#Video-CSR-Complex-Video-Digest-Creation-for-Visual-Language-Models" class="headerlink" title="Video-CSR: Complex Video Digest Creation for Visual-Language Models"></a>Video-CSR: Complex Video Digest Creation for Visual-Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05060">http://arxiv.org/abs/2310.05060</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tingkai Liu, Yunzhe Tao, Haogeng Liu, Qihang Fan, Ding Zhou, Huaibo Huang, Ran He, Hongxia Yang</li>
<li>for: 这个论文是用来评估视频语言模型的captioning、摘要和检索能力的新任务和人工标注数据集。</li>
<li>methods: 该数据集包含4.8万个YouTube视频clip，每个clip长度在20-60秒之间，覆盖了各种主题和兴趣。每个视频clip都有5个独立的标注caption（1句）和摘要（3-10句）。</li>
<li>results: 给任意选择的视频和其相应的ASR信息，我们评估视频语言模型在caption和摘要生成任务中，以及基于caption和摘要的检索任务中的表现。此外，我们还进行了评估不同的现有评价 metric的alignment with human preferences，并提出了一个基eline模型，以便作为Video-CSR任务的参考点。<details>
<summary>Abstract</summary>
We present a novel task and human annotated dataset for evaluating the ability for visual-language models to generate captions and summaries for real-world video clips, which we call Video-CSR (Captioning, Summarization and Retrieval). The dataset contains 4.8K YouTube video clips of 20-60 seconds in duration and covers a wide range of topics and interests. Each video clip corresponds to 5 independently annotated captions (1 sentence) and summaries (3-10 sentences). Given any video selected from the dataset and its corresponding ASR information, we evaluate visual-language models on either caption or summary generation that is grounded in both the visual and auditory content of the video. Additionally, models are also evaluated on caption- and summary-based retrieval tasks, where the summary-based retrieval task requires the identification of a target video given excerpts of a corresponding summary. Given the novel nature of the paragraph-length video summarization task, we perform extensive comparative analyses of different existing evaluation metrics and their alignment with human preferences. Finally, we propose a foundation model with competitive generation and retrieval capabilities that serves as a baseline for the Video-CSR task. We aim for Video-CSR to serve as a useful evaluation set in the age of large language models and complex multi-modal tasks.
</details>
<details>
<summary>摘要</summary>
我们介绍了一个新的任务和人标注数据集，用于评估视觉语言模型对真实视频片段的captioning、摘要和搜索能力，我们称之为Video-CSR（captioning、摘要和搜索）。该数据集包含4.8万个YouTube视频片段，每个片段长度在20-60秒之间，覆盖了广泛的主题和兴趣。每个视频片段对应着5个独立 annotated captions（1句）和摘要（3-10句）。给任意选择的视频和其相应的ASR信息，我们评估视觉语言模型在caption或摘要生成 tasks中的能力，这些任务都基于视频的视觉和听音内容。此外，我们还评估模型在caption-和摘要基于搜索任务中的能力，其中摘要基于搜索任务需要根据视频摘要的剪辑来标识目标视频。由于文章长度视频摘要任务的新性，我们进行了详细的比较分析不同的评估指标和其与人类偏好的对齐。最后，我们提出了一个基础模型，具有竞争力强的生成和搜索能力，作为Video-CSR任务的基线模型。我们希望Video-CSR能够在大型语言模型和复杂多Modal任务时代发挥作用。
</details></li>
</ul>
<hr>
<h2 id="Learning-Separable-Hidden-Unit-Contributions-for-Speaker-Adaptive-Lip-Reading"><a href="#Learning-Separable-Hidden-Unit-Contributions-for-Speaker-Adaptive-Lip-Reading" class="headerlink" title="Learning Separable Hidden Unit Contributions for Speaker-Adaptive Lip-Reading"></a>Learning Separable Hidden Unit Contributions for Speaker-Adaptive Lip-Reading</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05058">http://arxiv.org/abs/2310.05058</a></li>
<li>repo_url: None</li>
<li>paper_authors: Songtao Luo, Shuang Yang, Shiguang Shan, Xilin Chen</li>
<li>for: 本文提出了一种基于两个观察点的新方法，用于 speaker adaptation  lip reading，目的是提高 lip reading 的精度和稳定性。</li>
<li>methods: 本文使用了 shallow 和 deep 层，将 speaker 的特征分别处理为 two different targets，以便自动学习 separable hidden unit contributions。在 shallow 层中，引入 speaker-adaptive features 来增强 speech content 相关的特征；在 deep 层中，引入 speaker-adaptive features 来抑制 speech content 不相关的噪音。</li>
<li>results: 本文的方法在不同设置下进行了广泛的分析和比较，并 consistently 超过了现有方法的性能。此外，本文还发布了一个新的测试集 CAS-VSR-S68h，以进一步评估在只有几个 speaker 的情况下，但涵盖了大量和多样化的 speech content 的情况下的性能。<details>
<summary>Abstract</summary>
In this paper, we propose a novel method for speaker adaptation in lip reading, motivated by two observations. Firstly, a speaker's own characteristics can always be portrayed well by his/her few facial images or even a single image with shallow networks, while the fine-grained dynamic features associated with speech content expressed by the talking face always need deep sequential networks to represent accurately. Therefore, we treat the shallow and deep layers differently for speaker adaptive lip reading. Secondly, we observe that a speaker's unique characteristics ( e.g. prominent oral cavity and mandible) have varied effects on lip reading performance for different words and pronunciations, necessitating adaptive enhancement or suppression of the features for robust lip reading. Based on these two observations, we propose to take advantage of the speaker's own characteristics to automatically learn separable hidden unit contributions with different targets for shallow layers and deep layers respectively. For shallow layers where features related to the speaker's characteristics are stronger than the speech content related features, we introduce speaker-adaptive features to learn for enhancing the speech content features. For deep layers where both the speaker's features and the speech content features are all expressed well, we introduce the speaker-adaptive features to learn for suppressing the speech content irrelevant noise for robust lip reading. Our approach consistently outperforms existing methods, as confirmed by comprehensive analysis and comparison across different settings. Besides the evaluation on the popular LRW-ID and GRID datasets, we also release a new dataset for evaluation, CAS-VSR-S68h, to further assess the performance in an extreme setting where just a few speakers are available but the speech content covers a large and diversified range.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的lip reading speaker adaptation方法，基于两个观察结论。首先，一个说话人的自己特征可以通过他/她的一些脸部图像或者even a single image with shallow networks来表示得非常好，而言语内容表达在说话脸上的细腻动态特征则需要深度顺序网络来表示准确。因此，我们将浅层和深层处理 differently。其次，我们发现说话人的独特特征（例如嘴巴和下颌）对不同的话语和发音有不同的影响，需要根据不同的话语和发音进行适应增强或减弱这些特征以实现Robust lip reading。基于这两个观察结论，我们提出了利用说话人自己的特征自动学习可分离的隐藏单元贡献，其中浅层的特征与话语内容相关的特征更强，我们引入说话人特征学习以增强话语内容相关的特征。深层的特征则是说话人特征和话语内容相关的特征都很好地表示，我们引入说话人特征学习以减弱话语内容无关的噪音以实现Robust lip reading。我们的方法在不同的设置下 consistently outperform了现有方法，经过了全面的分析和比较。除了在popular LRW-ID和GRID dataset上进行评估外，我们还发布了一个新的测试集，CAS-VSR-S68h，以进一步评估在只有几个说话人的情况下，但是说话内容覆盖了广泛而多样化的情况下的性能。
</details></li>
</ul>
<hr>
<h2 id="FP3O-Enabling-Proximal-Policy-Optimization-in-Multi-Agent-Cooperation-with-Parameter-Sharing-Versatility"><a href="#FP3O-Enabling-Proximal-Policy-Optimization-in-Multi-Agent-Cooperation-with-Parameter-Sharing-Versatility" class="headerlink" title="FP3O: Enabling Proximal Policy Optimization in Multi-Agent Cooperation with Parameter-Sharing Versatility"></a>FP3O: Enabling Proximal Policy Optimization in Multi-Agent Cooperation with Parameter-Sharing Versatility</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05053">http://arxiv.org/abs/2310.05053</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lang Feng, Dong Xing, Junru Zhang, Gang Pan</li>
<li>for: 提高多代理人PPO算法的合作多代理人学习（MARL）理论保证性。</li>
<li>methods: 基于全管道思想，实现多平行优化管道，通过不同的等价分解方法表示代理人之间的连接。</li>
<li>results: FP3O算法在多代理人MuJoCo和StarCraftII任务上表现出色，超过了其他强基eline，并在不同的参数共享配置下展现了强大的可变性。<details>
<summary>Abstract</summary>
Existing multi-agent PPO algorithms lack compatibility with different types of parameter sharing when extending the theoretical guarantee of PPO to cooperative multi-agent reinforcement learning (MARL). In this paper, we propose a novel and versatile multi-agent PPO algorithm for cooperative MARL to overcome this limitation. Our approach is achieved upon the proposed full-pipeline paradigm, which establishes multiple parallel optimization pipelines by employing various equivalent decompositions of the advantage function. This procedure successfully formulates the interconnections among agents in a more general manner, i.e., the interconnections among pipelines, making it compatible with diverse types of parameter sharing. We provide a solid theoretical foundation for policy improvement and subsequently develop a practical algorithm called Full-Pipeline PPO (FP3O) by several approximations. Empirical evaluations on Multi-Agent MuJoCo and StarCraftII tasks demonstrate that FP3O outperforms other strong baselines and exhibits remarkable versatility across various parameter-sharing configurations.
</details>
<details>
<summary>摘要</summary>
现有的多代理PPO算法缺乏扩展 тео리тиче guarantee of PPO to cooperative multi-agent reinforcement learning (MARL) 中的兼容性。在这篇文章中，我们提出了一种新的和灵活的多代理PPO算法，以超越这些限制。我们的方法基于我们提出的全管道 paradigm，该 paradigm在利用多种等价 decompositions of the advantage function 中实现多个并行的优化管道。这个过程成功地表达了多个代理之间的连接，即多个管道之间的连接，使其与不同类型的参数共享兼容。我们提供了强有力的理论基础，以便策略提高，并在后续开发了一种实用的FP3O算法。在Multi-Agent MuJoCo和StarCraftII任务上的实验评估中，FP3O的表现超过了其他强大的基准，并且在不同的参数共享配置下展现出了remarkable的灵活性。
</details></li>
</ul>
<hr>
<h2 id="Learning-Intra-and-Inter-Cell-Differences-for-Accurate-Battery-Lifespan-Prediction-across-Diverse-Conditions"><a href="#Learning-Intra-and-Inter-Cell-Differences-for-Accurate-Battery-Lifespan-Prediction-across-Diverse-Conditions" class="headerlink" title="Learning Intra- and Inter-Cell Differences for Accurate Battery Lifespan Prediction across Diverse Conditions"></a>Learning Intra- and Inter-Cell Differences for Accurate Battery Lifespan Prediction across Diverse Conditions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05052">http://arxiv.org/abs/2310.05052</a></li>
<li>repo_url: None</li>
<li>paper_authors: Han Zhang, Yuqi Li, Shun Zheng, Ziheng Lu, Xiaofan Gui, Wei Xu, Jiang Bian</li>
<li>for: 预测电池寿命的研究具有实际应用价值，尤其是在电池研发中。现有的数据驱动模型大多依靠特定电池的早期电学信号来预测它的寿命。然而，这些模型受限于特定腐食条件，这不仅限制了它们的模型能力，还使其在不同条件下预测腐食的效果减退。因此，这些模型经常错过了可以从另一些条件下的历史数据中获得的全部 beneficial。</li>
<li>methods: 我们引入了一种方法，可以考虑target电池和参照电池之间的差异，无论它们的材料和腐食条件如何。通过这种差异，我们不仅扩大了特征空间，而且开辟了一个通用的电池寿命预测框架。我们的模型结合了inter-和intra-cell差异，在多种条件下表现出了极高的效率和准确率，使用了所有可用的数据集。</li>
<li>results: 我们的方法可以充分利用older电池的数据，使 newer电池可以借鉴过去的电池的经验。这种方法不仅拓宽了电池数据利用策略，还为未来的电池管理系统提供了智能化的基础。<details>
<summary>Abstract</summary>
Battery life prediction holds significant practical value for battery research and development. Currently, many data-driven models rely on early electrical signals from specific target batteries to predict their lifespan. A common shortfall is that most existing methods are developed based on specific aging conditions, which not only limits their model's capability but also diminishes their effectiveness in predicting degradation under varied conditions. As a result, these models often miss out on fully benefiting from the rich historical data available under other conditions. Here, to address above, we introduce an approach that explicitly captures differences between electrical signals of a target battery and a reference battery, irrespective of their materials and aging conditions, to forecast the target battery life. Through this inter-cell difference, we not only enhance the feature space but also pave the way for a universal battery life prediction framework. Remarkably, our model that combines the inter- and intra-cell differences shines across diverse conditions, standing out in its efficiency and accuracy using all accessible datasets. An essential application of our approach is its capability to leverage data from older batteries effectively, enabling newer batteries to capitalize on insights gained from past batteries. This work not only enriches the battery data utilization strategy but also sets the stage for smarter battery management system in the future.
</details>
<details>
<summary>摘要</summary>
预测电池寿命具有重要的实践价值，对电池研发具有重要的意义。目前，许多数据驱动模型依靠特定目标电池早期的电学信号来预测它们的寿命。然而，大多数现有方法都是基于特定腐蚀条件下开发的，这不仅限制了他们的模型能力，而且降低了它们在不同条件下预测腐蚀的效果。因此，这些模型经常会错过利用可用的历史数据来预测腐蚀情况。在这里，我们引入了一种方法，可以跨电池和参照电池之间的差异来预测目标电池寿命。通过这种差异，我们不仅扩大了特征空间，而且开创了一个通用的电池寿命预测框架。另外，我们的模型结合了差异和内部差异，在多种条件下表现出色，高效精准地使用所有可用的数据集。这种方法不仅可以有效地利用较老的电池数据，还可以为未来的电池管理系统提供智能化的基础。
</details></li>
</ul>
<hr>
<h2 id="From-Text-to-Tactic-Evaluating-LLMs-Playing-the-Game-of-Avalon"><a href="#From-Text-to-Tactic-Evaluating-LLMs-Playing-the-Game-of-Avalon" class="headerlink" title="From Text to Tactic: Evaluating LLMs Playing the Game of Avalon"></a>From Text to Tactic: Evaluating LLMs Playing the Game of Avalon</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05036">http://arxiv.org/abs/2310.05036</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jonathanmli/avalon-llm">https://github.com/jonathanmli/avalon-llm</a></li>
<li>paper_authors: Jonathan Light, Min Cai, Sheng Shen, Ziniu Hu</li>
<li>for: 这篇论文探讨了大语言模型代理人（LLM）在游戏《抵抗avalon》中的潜力。</li>
<li>methods: 作者们使用了一个名为AvalonBench的游戏环境，以评估多代理LML模型。这个环境包括avalon游戏环境、基于规则的bot对手和ReAct风格的LML代理人。</li>
<li>results: 作者们的评估结果显示，使用AvalonBench评估LML模型时存在明显的能力差距。例如，使用ChatGPT扮演善良角色时，与基于规则的bot对手扮演邪恶角色的情况下，win rate为22.2%，而使用基于规则的bot扮演善良角色时，win rate为38.2%。<details>
<summary>Abstract</summary>
In this paper, we explore the potential of Large Language Models (LLMs) Agents in playing the strategic social deduction game, Resistance Avalon. Players in Avalon are challenged not only to make informed decisions based on dynamically evolving game phases, but also to engage in discussions where they must deceive, deduce, and negotiate with other players. These characteristics make Avalon a compelling test-bed to study the decision-making and language-processing capabilities of LLM Agents. To facilitate research in this line, we introduce AvalonBench - a comprehensive game environment tailored for evaluating multi-agent LLM Agents. This benchmark incorporates: (1) a game environment for Avalon, (2) rule-based bots as baseline opponents, and (3) ReAct-style LLM agents with tailored prompts for each role. Notably, our evaluations based on AvalonBench highlight a clear capability gap. For instance, models like ChatGPT playing good-role got a win rate of 22.2% against rule-based bots playing evil, while good-role bot achieves 38.2% win rate in the same setting. We envision AvalonBench could be a good test-bed for developing more advanced LLMs (with self-playing) and agent frameworks that can effectively model the layered complexities of such game environments.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们探讨了大语言模型代理人（LLM）在游戏《巨大叛逆：阿瓦隆》中的潜力。游戏中的玩家不仅需要根据不断发展的游戏阶段进行了解的决策，还需要与其他玩家进行交流，包括谎言、推理和谈判。这些特点使得阿瓦隆成为了研究LLM代理人决策和语言处理能力的有力的测试场景。为了促进这种研究，我们介绍了阿瓦隆Bench，一个包含以下三个重要组成部分的游戏环境：（1）阿瓦隆游戏环境，（2）基于规则的 bot 作为基准对手，以及（3）ReAct 风格的 LLM 代理人，每个角色都有适应的提示。我们的评估结果表明，与基于规则的 bot 作为邪恶对手进行比较，ChatGPT 扮演善良角色时的胜率为 22.2%，而基于规则的 bot 扮演善良角色时的胜率为 38.2%。我们认为阿瓦隆Bench 可以成为 LLM 的发展和自适应代理人框架的试验场景。
</details></li>
</ul>
<hr>
<h2 id="Self-Convinced-Prompting-Few-Shot-Question-Answering-with-Repeated-Introspection"><a href="#Self-Convinced-Prompting-Few-Shot-Question-Answering-with-Repeated-Introspection" class="headerlink" title="Self-Convinced Prompting: Few-Shot Question Answering with Repeated Introspection"></a>Self-Convinced Prompting: Few-Shot Question Answering with Repeated Introspection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05035">http://arxiv.org/abs/2310.05035</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haodi Zhang, Min Cai, Xinhe Zhang, Chen Jason Zhang, Rui Mao, Kaishun Wu</li>
<li>for: 提高大语言模型（LLM）的复杂理解和具有技巧使用能力</li>
<li>methods: 使用提前训练的语言模型、询问、检查和修改步骤</li>
<li>results: 实验结果 validate Self-Convince 框架的有效性，与基准值进行比较获得了显著提高<details>
<summary>Abstract</summary>
While large language models (LLMs) such as ChatGPT and PaLM have demonstrated remarkable performance in various language understanding and generation tasks, their capabilities in complex reasoning and intricate knowledge utilization still fall short of human-level proficiency. Recent studies have established the effectiveness of prompts in steering LLMs towards generating desired outputs. Building on these insights, we introduce a novel framework that harnesses the potential of large-scale pre-trained language models, to iteratively enhance performance of the LLMs. Our framework incorporates three components: \textit{Normal CoT}, a \textit{Convincer}, and an \textit{Answerer}. It processes the output of a typical few-shot chain-of-thought prompt, assesses the correctness of the response, scrutinizes the answer, refines the reasoning, and ultimately produces a new solution. Experimental results on the 7 datasets of miscellaneous problems validate the efficacy of the Self-Convince framework, achieving substantial improvements compared to the baselines. This study contributes to the burgeoning body of research focused on integrating pre-trained language models with tailored prompts and iterative refinement processes to augment their performance in complex tasks.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）如ChatGPT和PaLM在不同的语言理解和生成任务中表现出色，但它们在复杂的推理和细节知识利用方面仍然落后人类水平。现在的研究显示了干预提示的效果，可以导引LLM生成所需的出力。基于这些见解，我们提出了一个新的框架，叫做Self-Convince框架。这个框架包含三个 ком成分： Normal CoT、Convincer 和 Answerer。它在一般几步链接思维提示的出力中进行处理，评估回应的正确性，探究答案，删除错误的推理，最终生成一个新的解决方案。实验结果显示，Self-Convince框架在7个多元问题的数据集上实现了重要的改善，较基于点的表现有所提高。这项研究将大型预训语言模型与定制提示和迭代改进过程相结合，以增强它们在复杂任务中的表现。
</details></li>
</ul>
<hr>
<h2 id="Counter-Turing-Test-CT-2-AI-Generated-Text-Detection-is-Not-as-Easy-as-You-May-Think-–-Introducing-AI-Detectability-Index"><a href="#Counter-Turing-Test-CT-2-AI-Generated-Text-Detection-is-Not-as-Easy-as-You-May-Think-–-Introducing-AI-Detectability-Index" class="headerlink" title="Counter Turing Test CT^2: AI-Generated Text Detection is Not as Easy as You May Think – Introducing AI Detectability Index"></a>Counter Turing Test CT^2: AI-Generated Text Detection is Not as Easy as You May Think – Introducing AI Detectability Index</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05030">http://arxiv.org/abs/2310.05030</a></li>
<li>repo_url: None</li>
<li>paper_authors: Megha Chakraborty, S. M Towhidul Islam Tonmoy, S M Mehedi Zaman, Krish Sharma, Niyar R Barman, Chandan Gupta, Shreya Gautam, Tanay Kumar, Vinija Jain, Aman Chadha, Amit P. Sheth, Amitava Das</li>
<li>for: 这篇论文主要旨在评估当前的AI生成文本检测技术的robustness，以及评估不同大小的自然语言处理模型（LLMs）在生成文本检测中的可探测性。</li>
<li>methods: 这篇论文提出了Counter Turing Test（CT^2）作为一个完整的评估AI生成文本检测技术的标准 benchark。它们还提出了一个名为AI Detectability Index（ADI）的指标，用于评估不同大小的LLMs在生成文本检测中的可探测性。</li>
<li>results: 这篇论文的实验结果表明，现有的AI生成文本检测技术在面对CT^2的测试中具有较弱的可探测性。此外，研究发现大型LLMs具有较高的AI Detectability Index（ADI），这意味着它们在生成文本检测中更难被检测。<details>
<summary>Abstract</summary>
With the rise of prolific ChatGPT, the risk and consequences of AI-generated text has increased alarmingly. To address the inevitable question of ownership attribution for AI-generated artifacts, the US Copyright Office released a statement stating that 'If a work's traditional elements of authorship were produced by a machine, the work lacks human authorship and the Office will not register it'. Furthermore, both the US and the EU governments have recently drafted their initial proposals regarding the regulatory framework for AI. Given this cynosural spotlight on generative AI, AI-generated text detection (AGTD) has emerged as a topic that has already received immediate attention in research, with some initial methods having been proposed, soon followed by emergence of techniques to bypass detection. This paper introduces the Counter Turing Test (CT^2), a benchmark consisting of techniques aiming to offer a comprehensive evaluation of the robustness of existing AGTD techniques. Our empirical findings unequivocally highlight the fragility of the proposed AGTD methods under scrutiny. Amidst the extensive deliberations on policy-making for regulating AI development, it is of utmost importance to assess the detectability of content generated by LLMs. Thus, to establish a quantifiable spectrum facilitating the evaluation and ranking of LLMs according to their detectability levels, we propose the AI Detectability Index (ADI). We conduct a thorough examination of 15 contemporary LLMs, empirically demonstrating that larger LLMs tend to have a higher ADI, indicating they are less detectable compared to smaller LLMs. We firmly believe that ADI holds significant value as a tool for the wider NLP community, with the potential to serve as a rubric in AI-related policy-making.
</details>
<details>
<summary>摘要</summary>
随着智能ChatGPT的出现，人工智能生成文本的风险和后果增加了致命地。为了解决人工智能生成文本的所有权归属问题，美国版权办公室发表了一份声明，表示如果文本中的传统元素的作者是机器制造出来的，那么文本就缺乏人类作者，因此不会注册。此外，美国和欧盟政府最近已经起草了关于人工智能的规制框架的初步提案。随着生成AI的焦点逐渐吸引到研究领域，AI生成文本检测（AGTD）已经成为研究的热点，一些初步的方法已经被提出，然后又有人提出了绕过检测的技术。本文介绍了Counter Turing Test（CT^2），一种包含了检测AGTD技术的多种方法的benchmark。我们的实验结果明显地表明，现有的AGTD方法在审查中表现极其脆弱。在政策制定的过程中，评估人工智能生成内容的可检测性是非常重要的。因此，我们提出了人工智能可检测指数（ADI），以评估和排名15种当代LLMs的可检测性水平。我们通过实验证明，大型LLMs tend to have higher ADI， indicating that they are less detectable compared to smaller LLMs。我们认为ADI具有重要的价值，可以作为NLP社区中的工具，并且可能在AI相关的政策制定中扮演重要的角色。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-Large-Language-Models-as-Zero-shot-Relation-Extractors"><a href="#Revisiting-Large-Language-Models-as-Zero-shot-Relation-Extractors" class="headerlink" title="Revisiting Large Language Models as Zero-shot Relation Extractors"></a>Revisiting Large Language Models as Zero-shot Relation Extractors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05028">http://arxiv.org/abs/2310.05028</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guozheng Li, Peng Wang, Wenjun Ke</li>
<li>for: 这个论文主要研究了使用大语言模型（LLM）进行零shot关系EXTRACTION（RE）。</li>
<li>methods: 本研究使用了Chain-of-thought（CoT）技术和summarize-and-ask（\textsc{SumAsk}）提示法来提高零shot RE的性能。</li>
<li>results: 研究发现，\textsc{SumAsk}可以Consistently和Significantly提高LLMs在不同的模型大小、benchmark和设置下的性能。此外，零shot提示与ChatGPT比较或超过了零shot和完全监督方法的性能。LLMs也能够Handle Challenge none-of-the-above（NoTA）关系，但关系性能差异较大。<details>
<summary>Abstract</summary>
Relation extraction (RE) consistently involves a certain degree of labeled or unlabeled data even if under zero-shot setting. Recent studies have shown that large language models (LLMs) transfer well to new tasks out-of-the-box simply given a natural language prompt, which provides the possibility of extracting relations from text without any data and parameter tuning. This work focuses on the study of exploring LLMs, such as ChatGPT, as zero-shot relation extractors. On the one hand, we analyze the drawbacks of existing RE prompts and attempt to incorporate recent prompt techniques such as chain-of-thought (CoT) to improve zero-shot RE. We propose the summarize-and-ask (\textsc{SumAsk}) prompting, a simple prompt recursively using LLMs to transform RE inputs to the effective question answering (QA) format. On the other hand, we conduct comprehensive experiments on various benchmarks and settings to investigate the capabilities of LLMs on zero-shot RE. Specifically, we have the following findings: (i) \textsc{SumAsk} consistently and significantly improves LLMs performance on different model sizes, benchmarks and settings; (ii) Zero-shot prompting with ChatGPT achieves competitive or superior results compared with zero-shot and fully supervised methods; (iii) LLMs deliver promising performance in extracting overlapping relations; (iv) The performance varies greatly regarding different relations. Different from small language models, LLMs are effective in handling challenge none-of-the-above (NoTA) relation.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate_language: zh-CN关系提取（RE）总是需要一定量的标注或未标注数据，即使在零shot设定下。现代语言模型（LLM）可以轻松地在新任务上进行升级，只需要一个自然语言提示，这提供了提取关系从文本中的可能性。本研究将关注使用LLM，如ChatGPT，作为零shot关系提取器的研究。一方面，我们分析了现有RE提示的缺点，并尝试使用最新的提示技术，如链条思维（CoT），来改进零shot RE。我们提出了摘要并问 (\textsc{SumAsk})提示，一种简单的提示，使用LLM recursively将RE输入转换为有效的问答（QA）格式。另一方面，我们对多个benchmark和设置进行了广泛的实验，以研究LLM在零shot RE中的能力。我们得到以下发现：(i) \textsc{SumAsk}在不同的模型大小、benchmark和设置上具有一致性和显著性，提高LLM的表现。(ii) 采用ChatGPT的零shot提示可以与零shot和完全监督方法相比，在不同的任务和设置上达到竞争或更高的性能。(iii) LLM在抽象关系提取方面表现出色，特别是在抽象关系上。(iv) 不同的关系之间的表现差异较大，而LLM在处理抽象关系方面表现更佳。与小语言模型相比，LLM在处理抽象关系方面表现更出色。
</details></li>
</ul>
<hr>
<h2 id="Fully-Spiking-Neural-Network-for-Legged-Robots"><a href="#Fully-Spiking-Neural-Network-for-Legged-Robots" class="headerlink" title="Fully Spiking Neural Network for Legged Robots"></a>Fully Spiking Neural Network for Legged Robots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05022">http://arxiv.org/abs/2310.05022</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoyang Jiang, Qiang Zhang, Jingkai Sun, Renjing Xu</li>
<li>For: The paper aims to improve the performance of legged robots using a novel Spiking Neural Network (SNN) to process body perception signals, achieving better speed and energy consumption, and improved biological interpretability.* Methods: The paper employs a SNN to process legged robots’ perception signals, which offers improved biological interpretability and natural advantages in inference speed and energy consumption compared to traditional artificial neural networks.* Results: The paper achieves outstanding results across a range of simulated terrains, demonstrating the effectiveness of SNN in legged robots.<details>
<summary>Abstract</summary>
In recent years, legged robots based on deep reinforcement learning have made remarkable progress. Quadruped robots have demonstrated the ability to complete challenging tasks in complex environments and have been deployed in real-world scenarios to assist humans. Simultaneously, bipedal and humanoid robots have achieved breakthroughs in various demanding tasks. Current reinforcement learning methods can utilize diverse robot bodies and historical information to perform actions. However, prior research has not emphasized the speed and energy consumption of network inference, as well as the biological significance of the neural networks themselves. Most of the networks employed are traditional artificial neural networks that utilize multilayer perceptrons (MLP). In this paper, we successfully apply a novel Spiking Neural Network (SNN) to process legged robots, achieving outstanding results across a range of simulated terrains. SNN holds a natural advantage over traditional neural networks in terms of inference speed and energy consumption, and their pulse-form processing of body perception signals offers improved biological interpretability. To the best of our knowledge, this is the first work to implement SNN in legged robots.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Compresso-Structured-Pruning-with-Collaborative-Prompting-Learns-Compact-Large-Language-Models"><a href="#Compresso-Structured-Pruning-with-Collaborative-Prompting-Learns-Compact-Large-Language-Models" class="headerlink" title="Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models"></a>Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05015">http://arxiv.org/abs/2310.05015</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/moonlit">https://github.com/microsoft/moonlit</a></li>
<li>paper_authors: Song Guo, Jiahang Xu, Li Lyna Zhang, Mao Yang</li>
<li>for: 这篇研究的目的是为了提高大型语言模型（LLM）的部署，特别是在具有限制的硬件资源的环境下。</li>
<li>methods: 这篇研究使用了一种新的架构，叫做Compresso，它通过与LLM的协作，在训练过程中学习最佳的剪辑决策。Compresso使用了LoRA技术来实现$L_0$规律，并在调询过程中引入了协同提示，以增强整体性能。</li>
<li>results: 根据实验结果，Compresso可以将LLaMA-7B剪辑到5.4B，保持原始性能，甚至在阅读理解测试中超过LLaMA-7B的表现。Compresso比一项基eline的一项单一剪辑方法（one-shot pruning）有更高的表现，在不同的组合比例下，可以达到2.21%, 11.43%, 7.04%, 4.81%更高的分数在 Commonsense Reasoning、Reading Comprehension、MMLU和BBH测试中。<details>
<summary>Abstract</summary>
Despite the remarkable success of Large Language Models (LLMs), the massive size poses significant deployment challenges, particularly on resource-constrained hardware. While existing LLM compression methods focus on quantization, pruning remains relatively unexplored due to the high cost of training-based approaches and data collection challenges. One-shot pruning methods, although cost-effective and data-free, have become dominant in LLM pruning, but lead to performance decline under the structured pruning setting. In this work, we introduce a new paradigm for structurally pruning LLMs, called Compresso. Our approach, through the collaboration of the proposed resource-efficient pruning algorithm and the LLM itself, learns optimal pruning decisions during the training process. Compresso addresses the challenges of expensive training costs and data collection by incorporating Low-Rank Adaptation (LoRA) into the $L_0$ regularization during the instruction tuning process. Then, we further augment the pruning algorithm by introducing a collaborative prompt that fosters collaboration between the LLM and the pruning algorithm, significantly boosting the overall performance. To this end, Compresso prunes LLaMA-7B to 5.4B, maintaining original performance and even surpassing LLaMA-7B in reading comprehension by 2.62%. Extensive experiments demonstrate that Compresso significantly outperforms one-shot pruning baselines across various sparsity ratios, achieving up to 2.21%, 11.43%, 7.04%, and 4.81% higher scores on the commonsense reasoning, reading comprehension, MMLU, and BBH benchmarks, respectively.
</details>
<details>
<summary>摘要</summary>
尽管大型自然语言模型（LLM）已经取得了非常出色的成功，但是它们的巨大大小却对资源有限的硬件 pose 了部署的挑战。现有的 LLM 压缩方法主要集中在量化上，而采用 Training-based 方法和数据收集的压缩方法却受到了高成本和数据收集的挑战。一shot 压缩方法，尽管成本低廉且不需数据，但在结构化压缩设定下会导致性能下降。在这种情况下，我们提出了一种新的 LL 模型结构压缩方法，called Compresso。我们的方法通过与 LL 模型的协作，在训练过程中学习最佳压缩决策。Compresso 通过综合利用 LoRA 技术和 $L_0$ 正则化来解决训练成本和数据收集的挑战。此外，我们还在压缩算法中引入了协同提示，使 LL 模型和压缩算法之间的合作更加紧密，从而提高总性能。因此，Compresso 可以压缩 LLaMA-7B 到 5.4B，保持原有性能，甚至在阅读理解任务上超越 LLaMA-7B 的表现。我们的实验表明，Compresso significantly 高于一shot 压缩基准在不同的稀疏比例上，达到了2.21%、11.43%、7.04%和4.81%的提升。
</details></li>
</ul>
<hr>
<h2 id="The-Reinforce-Policy-Gradient-Algorithm-Revisited"><a href="#The-Reinforce-Policy-Gradient-Algorithm-Revisited" class="headerlink" title="The Reinforce Policy Gradient Algorithm Revisited"></a>The Reinforce Policy Gradient Algorithm Revisited</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05000">http://arxiv.org/abs/2310.05000</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shalabh Bhatnagar</li>
<li>for: 本文提出了一种改进版本的强化策略梯度算法，用于处理无穷状态和动作空间的系统。</li>
<li>methods: 本文使用一种随机搜索方法来估计策略梯度，而不需要Regularity conditions。</li>
<li>results: 本文证明了这种新算法的收敛性，并且在无穷状态和动作空间中的系统上实现了高效的收敛性。In English:</li>
<li>for: The paper proposes an improved version of the reinforcement policy gradient algorithm for systems with infinite state and action spaces.</li>
<li>methods: The paper uses a random search method to estimate the policy gradient, without requiring regularity conditions.</li>
<li>results: The paper proves the convergence of the new algorithm and demonstrates its effectiveness in systems with infinite state and action spaces.<details>
<summary>Abstract</summary>
We revisit the Reinforce policy gradient algorithm from the literature. Note that this algorithm typically works with cost returns obtained over random length episodes obtained from either termination upon reaching a goal state (as with episodic tasks) or from instants of visit to a prescribed recurrent state (in the case of continuing tasks). We propose a major enhancement to the basic algorithm. We estimate the policy gradient using a function measurement over a perturbed parameter by appealing to a class of random search approaches. This has advantages in the case of systems with infinite state and action spaces as it relax some of the regularity requirements that would otherwise be needed for proving convergence of the Reinforce algorithm. Nonetheless, we observe that even though we estimate the gradient of the performance objective using the performance objective itself (and not via the sample gradient), the algorithm converges to a neighborhood of a local minimum. We also provide a proof of convergence for this new algorithm.
</details>
<details>
<summary>摘要</summary>
我们回顾到文献中的增强策略梯度算法。该算法通常与成本返回得到随机长度集所得到的集成任务（如果结束）或从定义状态访问中抽出的循环状态（在续行任务中）。我们提出了一个主要优化，使用功能测量在干扰参数上估计策略梯度，通过一类随机搜寻方法。这具有利陵系统拥有无限州和动作空间的情况下，可以缓和一些常量需求，从而让增强算法的证明 converges。然而，我们观察到，即使使用表现目标自身估计策略梯度（而不是采样梯度），算法仍会趋向一个地方最小值的邻近。我们也提供了该新算法的充分性证明。
</details></li>
</ul>
<hr>
<h2 id="Distantly-Supervised-Joint-Entity-and-Relation-Extraction-with-Noise-Robust-Learning"><a href="#Distantly-Supervised-Joint-Entity-and-Relation-Extraction-with-Noise-Robust-Learning" class="headerlink" title="Distantly-Supervised Joint Entity and Relation Extraction with Noise-Robust Learning"></a>Distantly-Supervised Joint Entity and Relation Extraction with Noise-Robust Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04994">http://arxiv.org/abs/2310.04994</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yul091/denrl">https://github.com/yul091/denrl</a></li>
<li>paper_authors: Yufei Li, Xiao Yu, Yanghong Guo, Yanchi Liu, Haifeng Chen, Cong Liu</li>
<li>for: 这个论文主要用于解决使用远程标注数据进行entity和关系抽象的问题，即使面临着噪声标注的问题。</li>
<li>methods: 该论文提出了一种新的噪声鲁棒方法，包括在序列标注模型中预训练GPT-2，以及使用一种新的噪声鲁棒学习框架，包括一个新的损失函数，惩罚与重要关系模式和实体关系依赖性不一致。</li>
<li>results: 实验结果显示，该方法可以在两个数据集上达到现有状态的 arts 方法的同等或更高的 JOINT 抽象性和噪声减少效果。<details>
<summary>Abstract</summary>
Joint entity and relation extraction is a process that identifies entity pairs and their relations using a single model. We focus on the problem of training these models on distantly-labeled data, which is generated by aligning entity mentions in a text corpus with their corresponding entity and relation types in a knowledge base. One key challenge here is the presence of noisy labels, which arises from both entity and relation annotations, and significantly impair the effectiveness of supervised learning applications. However, existing research primarily addresses only one type of noise, thereby limiting the effectiveness of noise reduction. To fill this gap, we introduce a new noise-robust approach, that 1)~incorporates a pre-trained GPT-2 into a sequence tagging scheme for simultaneous entity and relation detection, and 2)~employs a noise-robust learning framework which includes a new loss function that penalizes inconsistency with both significant relation patterns and entity-relation dependencies, as well as a self-adaptive learning step that iteratively selects and trains on high-quality instances. Experiments on two datasets show that our method outperforms the existing state-of-the-art methods in both joint extraction performance and noise reduction effect.
</details>
<details>
<summary>摘要</summary>
共同实体和关系抽取是一个过程，它通过单一模型标识实体对和其关系。我们关注在训练这些模型的远程标注数据上的问题，这些数据是通过文本库中的实体提及与知识库中的实体和关系类型的对应进行对齐的。一个关键挑战是噪声标注，它来自实体和关系注释，并对监督学习应用产生重要影响。然而，现有研究主要只处理一种噪声，因此限制了噪声减少的效iveness。为了填补这个空白，我们介绍了一种新的噪声Robust Approach，它包括以下两个部分：1. 使用预训练的 GPT-2 在序列标记方案中同时检测实体和关系，以提高实体和关系的同时检测能力。2. 使用一种噪声Robust的学习框架，包括一种新的损失函数，该损失函数考虑实体和关系之间的依赖关系和重要关系模式，以及一种自适应学习步骤，该步骤在高质量实例上进行逐步选择和训练。我们在两个数据集上进行了实验，结果表明，我们的方法在同时检测性能和噪声减少效果方面都超过了现有状态的方法。
</details></li>
</ul>
<hr>
<h2 id="The-Troubling-Emergence-of-Hallucination-in-Large-Language-Models-–-An-Extensive-Definition-Quantification-and-Prescriptive-Remediations"><a href="#The-Troubling-Emergence-of-Hallucination-in-Large-Language-Models-–-An-Extensive-Definition-Quantification-and-Prescriptive-Remediations" class="headerlink" title="The Troubling Emergence of Hallucination in Large Language Models – An Extensive Definition, Quantification, and Prescriptive Remediations"></a>The Troubling Emergence of Hallucination in Large Language Models – An Extensive Definition, Quantification, and Prescriptive Remediations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04988">http://arxiv.org/abs/2310.04988</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vipula Rawte, Swagata Chakraborty, Agnibh Pathak, Anubhav Sarkar, S. M Towhidul Islam Tonmoy, Aman Chadha, Amit P. Sheth, Amitava Das</li>
<li>for: 本研究旨在提供一种细化的幻觉分类方法，以及对幻觉的减轻策略。</li>
<li>methods: 本研究使用了15种当代大语言模型生成75,000个样本，并对其进行了人工标注。此外，本研究还提出了一个幻觉敏感指数（HVI），用于评估和排序不同的大语言模型在生成幻觉方面的敏感度。</li>
<li>results: 本研究对幻觉进行了细化分类，并提出了两种减轻幻觉的方法。<details>
<summary>Abstract</summary>
The recent advancements in Large Language Models (LLMs) have garnered widespread acclaim for their remarkable emerging capabilities. However, the issue of hallucination has parallelly emerged as a by-product, posing significant concerns. While some recent endeavors have been made to identify and mitigate different types of hallucination, there has been a limited emphasis on the nuanced categorization of hallucination and associated mitigation methods. To address this gap, we offer a fine-grained discourse on profiling hallucination based on its degree, orientation, and category, along with offering strategies for alleviation. As such, we define two overarching orientations of hallucination: (i) factual mirage (FM) and (ii) silver lining (SL). To provide a more comprehensive understanding, both orientations are further sub-categorized into intrinsic and extrinsic, with three degrees of severity - (i) mild, (ii) moderate, and (iii) alarming. We also meticulously categorize hallucination into six types: (i) acronym ambiguity, (ii) numeric nuisance, (iii) generated golem, (iv) virtual voice, (v) geographic erratum, and (vi) time wrap. Furthermore, we curate HallucInation eLiciTation (HILT), a publicly available dataset comprising of 75,000 samples generated using 15 contemporary LLMs along with human annotations for the aforementioned categories. Finally, to establish a method for quantifying and to offer a comparative spectrum that allows us to evaluate and rank LLMs based on their vulnerability to producing hallucinations, we propose Hallucination Vulnerability Index (HVI). We firmly believe that HVI holds significant value as a tool for the wider NLP community, with the potential to serve as a rubric in AI-related policy-making. In conclusion, we propose two solution strategies for mitigating hallucinations.
</details>
<details>
<summary>摘要</summary>
最近的大自然语言模型（LLM）的进步得到了广泛的赞誉，但同时也出现了一种问题，即幻觉。幻觉的出现引起了一定的担忧，因为它可能会对语言处理 tasks 产生负面影响。然而，关于幻觉的细分分类和相应的缓解方法尚未得到了足够的重视。为了解决这个问题，我们提出了一种细化的幻觉分类方法，以及一些缓解方法。我们将幻觉分为两类：（i）实际幻觉（FM）和（ii）银色幻觉（SL）。这两类幻觉进一步分为内在和外在两类，并且分为三级幻觉的严重程度：（i）轻度、（ii）中度和（iii）警示级。此外，我们还将幻觉分为六种类型：（i）字符混淆、（ii）数字幻觉、（iii）生成的 GOLEM、（iv）虚拟之声、（v）地理错误和（vi）时间包袋。此外，我们还创建了一个名为 HallucInation eLiciTation（HILT）的公共可用数据集，包含75000个样本，由15种当代LLM生成，以及人工标注的相应类别。最后，我们提出了一种用于评估和排名 LLM 的幻觉抵触指数（HVI）。我们认为 HVI 对 NLP 社区拥有广泛的价值，并可能被用作 AI 相关的政策制定的工具。为了缓解幻觉，我们提出了两种解决方案。
</details></li>
</ul>
<hr>
<h2 id="A-new-economic-and-financial-theory-of-money"><a href="#A-new-economic-and-financial-theory-of-money" class="headerlink" title="A new economic and financial theory of money"></a>A new economic and financial theory of money</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04986">http://arxiv.org/abs/2310.04986</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael E. Glinsky, Sharon Sievert</li>
<li>for: This paper aims to reformulate economic and financial theory to include electronic currencies, and to develop a new view of electronic currency as a transactional equity associated with tangible assets.</li>
<li>methods: The paper uses macroeconomic theory and the fundamental equation of monetary policy to value electronic currencies, and employs multi time scale models to capture true risk. The decision-making process is approached using deep reinforcement learning, generative pretrained transformers, and other methods of artificial intelligence.</li>
<li>results: The paper develops a new view of electronic currency management firms as entities responsible for coordinated monetary and fiscal policies of a substantial sub-economy, and proposes a system response function and DRL&#x2F;GPT&#x2F;AI-based active nonlinear control to stabilize unstable equilibriums in the sub-economy.<details>
<summary>Abstract</summary>
This paper fundamentally reformulates economic and financial theory to include electronic currencies. The valuation of the electronic currencies will be based on macroeconomic theory and the fundamental equation of monetary policy, not the microeconomic theory of discounted cash flows. The view of electronic currency as a transactional equity associated with tangible assets of a sub-economy will be developed, in contrast to the view of stock as an equity associated mostly with intangible assets of a sub-economy. The view will be developed of the electronic currency management firm as an entity responsible for coordinated monetary (electronic currency supply and value stabilization) and fiscal (investment and operational) policies of a substantial (for liquidity of the electronic currency) sub-economy. The risk model used in the valuations and the decision-making will not be the ubiquitous, yet inappropriate, exponential risk model that leads to discount rates, but will be multi time scale models that capture the true risk. The decision-making will be approached from the perspective of true systems control based on a system response function given by the multi scale risk model and system controllers that utilize the Deep Reinforcement Learning, Generative Pretrained Transformers, and other methods of Artificial Intelligence (DRL/GPT/AI). Finally, the sub-economy will be viewed as a nonlinear complex physical system with both stable equilibriums that are associated with short-term exploitation, and unstable equilibriums that need to be stabilized with active nonlinear control based on the multi scale system response functions and DRL/GPT/AI.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Comparative-Analysis-of-Transfer-Learning-in-Deep-Learning-Text-to-Speech-Models-on-a-Few-Shot-Low-Resource-Customized-Dataset"><a href="#Comparative-Analysis-of-Transfer-Learning-in-Deep-Learning-Text-to-Speech-Models-on-a-Few-Shot-Low-Resource-Customized-Dataset" class="headerlink" title="Comparative Analysis of Transfer Learning in Deep Learning Text-to-Speech Models on a Few-Shot, Low-Resource, Customized Dataset"></a>Comparative Analysis of Transfer Learning in Deep Learning Text-to-Speech Models on a Few-Shot, Low-Resource, Customized Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04982">http://arxiv.org/abs/2310.04982</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ze Liu</li>
<li>for: 本研究旨在提高 Text-to-Speech（TTS）synthesis 的质量，使用深度学习，但现代 TTS 模型需要大量数据。因此，本研究强调使用传输学习，特别是几何学习、少量数据和自定义数据集。</li>
<li>methods: 本研究使用了现代 TTS 模型的传输学习能力，进行了系统技术分析，并对几何学习模型进行了实验分析。</li>
<li>results: 研究发现，传输学习可以大幅提高 TTS 模型在紧张数据集上的表现，并且可以找到适合特定数据集的优化模型。这种模型可以在数据稀缺时提供高质量的语音输出。<details>
<summary>Abstract</summary>
Text-to-Speech (TTS) synthesis using deep learning relies on voice quality. Modern TTS models are advanced, but they need large amount of data. Given the growing computational complexity of these models and the scarcity of large, high-quality datasets, this research focuses on transfer learning, especially on few-shot, low-resource, and customized datasets. In this research, "low-resource" specifically refers to situations where there are limited amounts of training data, such as a small number of audio recordings and corresponding transcriptions for a particular language or dialect. This thesis, is rooted in the pressing need to find TTS models that require less training time, fewer data samples, yet yield high-quality voice output. The research evaluates TTS state-of-the-art model transfer learning capabilities through a thorough technical analysis. It then conducts a hands-on experimental analysis to compare models' performance in a constrained dataset. This study investigates the efficacy of modern TTS systems with transfer learning on specialized datasets and a model that balances training efficiency and synthesis quality. Initial hypotheses suggest that transfer learning could significantly improve TTS models' performance on compact datasets, and an optimal model may exist for such unique conditions. This thesis predicts a rise in transfer learning in TTS as data scarcity increases. In the future, custom TTS applications will favour models optimized for specific datasets over generic, data-intensive ones.
</details>
<details>
<summary>摘要</summary>
TEXT-TO-SPEECH（TTS）合成使用深度学习取决于声音质量。现代TTS模型非常先进，但它们需要大量数据。随着这些模型的计算复杂度的增加和数据的罕见性，这些研究将注重传输学习，特别是几个数据集的传输学习。在这种情况下，“低资源”指的是有限的训练数据，例如一小number of audio recording和相应的转录 для一种语言或方言。这个研究是根据找到需要 fewer training time和数据amples的TTS模型的强需求。研究通过深入技术分析评估TTS模型的传输学习能力，然后通过实验分析比较模型在紧张数据集中的表现。这个研究investigatesmodern TTS系统在特殊数据集上的传输学习能力，并预测未来custom TTS应用程序将偏好特定数据集上优化的模型。Note: Simplified Chinese is used here as the translation target, as it is more widely used in mainland China and is the standard form of Chinese used in many online applications. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="MULTISCRIPT-Multimodal-Script-Learning-for-Supporting-Open-Domain-Everyday-Tasks"><a href="#MULTISCRIPT-Multimodal-Script-Learning-for-Supporting-Open-Domain-Everyday-Tasks" class="headerlink" title="MULTISCRIPT: Multimodal Script Learning for Supporting Open Domain Everyday Tasks"></a>MULTISCRIPT: Multimodal Script Learning for Supporting Open Domain Everyday Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04965">http://arxiv.org/abs/2310.04965</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingyuan Qi, Minqian Liu, Ying Shen, Zhiyang Xu, Lifu Huang</li>
<li>for: 提高AI虚拟助手完成日常任务的自动生成脚本能力，特别是对于不熟悉的任务。</li>
<li>methods: 基于多模态视频和文本描述，提出了两个新任务：多模态脚本生成和后续步骤预测。两个任务的输入都是目标任务名和一段完成目标任务的视频示例，输出包括（1）基于视频示例的结构化文本描述，和（2）基于视频示例的后续步骤文本描述。</li>
<li>results: 提出了两种基于大语言模型知识的多模态生成框架，并在MultiScript挑战 задании上实现了显著提高。<details>
<summary>Abstract</summary>
Automatically generating scripts (i.e. sequences of key steps described in text) from video demonstrations and reasoning about the subsequent steps are crucial to the modern AI virtual assistants to guide humans to complete everyday tasks, especially unfamiliar ones. However, current methods for generative script learning rely heavily on well-structured preceding steps described in text and/or images or are limited to a certain domain, resulting in a disparity with real-world user scenarios. To address these limitations, we present a new benchmark challenge -- MultiScript, with two new tasks on task-oriented multimodal script learning: (1) multimodal script generation, and (2) subsequent step prediction. For both tasks, the input consists of a target task name and a video illustrating what has been done to complete the target task, and the expected output is (1) a sequence of structured step descriptions in text based on the demonstration video, and (2) a single text description for the subsequent step, respectively. Built from WikiHow, MultiScript covers multimodal scripts in videos and text descriptions for over 6,655 human everyday tasks across 19 diverse domains. To establish baseline performance on MultiScript, we propose two knowledge-guided multimodal generative frameworks that incorporate the task-related knowledge prompted from large language models such as Vicuna. Experimental results show that our proposed approaches significantly improve over the competitive baselines.
</details>
<details>
<summary>摘要</summary>
现代AI虚拟助手需要自动生成脚本（即文本描述的顺序步骤）从视频示例中，并根据示例视频进行逻辑推理来导引人类完成日常任务，特别是不熟悉的任务。然而，现有的生成脚本学习方法都是基于结构化的前置步骤（文本和/或图像），或者只能在特定领域中进行学习，这导致了与实际用户场景的差距。为了解决这些限制，我们提出了一个新的比赛挑战——MultiScript，包括两个新任务：（1）多媒体脚本生成和（2）后续步骤预测。对于两个任务，输入都是目标任务名和一段完成目标任务的视频示例，并且期望的输出是（1）基于示例视频的结构化文本描述，和（2）一个基于示例视频的文本描述。MultiScript由WikiHow建立，覆盖了视频和文本描述的多媒体脚本 для人类日常任务的19个不同领域，涵盖了6,655个任务。为了确定MultiScript的基准性能，我们提议两种基于大型自然语言模型（如Vicuna）的知识导向多媒体生成框架，实验结果表明，我们的提议方法具有显著的优势。
</details></li>
</ul>
<hr>
<h2 id="LLM4VV-Developing-LLM-Driven-Testsuite-for-Compiler-Validation"><a href="#LLM4VV-Developing-LLM-Driven-Testsuite-for-Compiler-Validation" class="headerlink" title="LLM4VV: Developing LLM-Driven Testsuite for Compiler Validation"></a>LLM4VV: Developing LLM-Driven Testsuite for Compiler Validation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04963">http://arxiv.org/abs/2310.04963</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christian Munley, Aaron Jarmusch, Sunita Chandrasekaran</li>
<li>for: This paper explores the capability of state-of-the-art large language models (LLMs) to automatically generate tests and validate compiler implementations of a directive-based programming paradigm, OpenACC.</li>
<li>methods: The paper employs various prompt engineering techniques, including code templates, retrieval-augmented generation (RAG) with code templates, expressive prompts using RAG with code templates, one-shot examples, and RAG with one-shot examples.</li>
<li>results: The paper investigates the outcome of LLMs-generated tests and analyzes the capabilities of the latest LLMs for code generation.<details>
<summary>Abstract</summary>
Large language models (LLMs) are a new and powerful tool for a wide span of applications involving natural language and demonstrate impressive code generation abilities. In this paper, we explore the capabilitity of state-of-the-art LLMs, including closed-source options like OpenAI GPT-4 and open-source alternatives like Meta AI Codellama, to automatically generate tests and use these tests to validate and verify compiler implementations of a directive-based programming paradigm, OpenACC. Our approach entails exploring various prompt engineering techniques including a code template, retrieval-augmented generation (RAG) with code template, expressive prompt using RAG with code template, one-shot example, and RAG with one-shot example. This paper focusses on (a) exploring the capabilities of the latest LLMs for code generation, (b) investigating prompt and fine tuning methods, and (c) analyzing the outcome of LLMs generated tests
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）是一种新的和强大的工具，可以应用于许多自然语言相关的应用程序。在这篇论文中，我们探讨了当前领先的LLM，包括OpenAI GPT-4和Meta AI Codellama等closed-source选择，以及open-source的选择，用于自动生成测试，并使用这些测试来验证和验证编译器实现的指令式编程方法OpenACC。我们的方法包括使用代码模板、代码检索增强生成（RAG）、表达式提示、一shot示例和RAG与一shot示例等多种提示工程技术。本文主要关注以下三点：1. 探讨最新的LLM代码生成能力2. 探讨提示和精度调整方法3. 分析LLM生成的测试结果
</details></li>
</ul>
<hr>
<h2 id="Safe-Deep-Policy-Adaptation"><a href="#Safe-Deep-Policy-Adaptation" class="headerlink" title="Safe Deep Policy Adaptation"></a>Safe Deep Policy Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08602">http://arxiv.org/abs/2310.08602</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenli Xiao, Tairan He, John Dolan, Guanya Shi</li>
<li>for: 本研究旨在开发一种能够快速适应动态不确定环境的自主 робоット控制框架，同时保证安全性和稳定性。</li>
<li>methods: 本研究使用了policy adaptation基于再归折衔学习（RL），并提出了一种安全防止（Safety Filter）来保证实际世界中的安全性。</li>
<li>results: 实验结果显示，SafeDPA在三个不同的环境中（倒挠杆、Safety Gym和RC Car）具有出色的安全性和任务性能，与现有的基准值进行比较，SafeDPA在不可见干扰的实际世界中展现出了300%的安全率提升。<details>
<summary>Abstract</summary>
A critical goal of autonomy and artificial intelligence is enabling autonomous robots to rapidly adapt in dynamic and uncertain environments. Classic adaptive control and safe control provide stability and safety guarantees but are limited to specific system classes. In contrast, policy adaptation based on reinforcement learning (RL) offers versatility and generalizability but presents safety and robustness challenges. We propose SafeDPA, a novel RL and control framework that simultaneously tackles the problems of policy adaptation and safe reinforcement learning. SafeDPA jointly learns adaptive policy and dynamics models in simulation, predicts environment configurations, and fine-tunes dynamics models with few-shot real-world data. A safety filter based on the Control Barrier Function (CBF) on top of the RL policy is introduced to ensure safety during real-world deployment. We provide theoretical safety guarantees of SafeDPA and show the robustness of SafeDPA against learning errors and extra perturbations. Comprehensive experiments on (1) classic control problems (Inverted Pendulum), (2) simulation benchmarks (Safety Gym), and (3) a real-world agile robotics platform (RC Car) demonstrate great superiority of SafeDPA in both safety and task performance, over state-of-the-art baselines. Particularly, SafeDPA demonstrates notable generalizability, achieving a 300% increase in safety rate compared to the baselines, under unseen disturbances in real-world experiments.
</details>
<details>
<summary>摘要</summary>
“一个重要目标是实现自主机器人快速适应动态和不确定环境。类型的适应控制和安全控制可以提供稳定性和安全保证，但是仅对特定系统类型有效。相比之下，基于征得学习（RL）的政策适应则提供了多样性和普遍性，但是产生了安全和可靠性挑战。我们提出了SafeDPA，一个新的RL和控制框架，同时解决政策适应和安全征得学习的问题。SafeDPA在实验中同时学习适应政策和动力学模型，预测环境配置，并将几何数据进行精确化。我们引入了基于控制障碍函数（CBF）的安全筛选器，以保证在真实世界中的安全运行。我们提供了理论上的安全保证，并证明SafeDPA对学习错误和额外干扰具有Robustness。实验结果显示，SafeDPA在三个不同的应用中具有优秀的安全性和任务性能，比基准设定更高。特别是，SafeDPA在未见到的干扰下 demonstrate了特别的多样性，在真实世界中获得300%的安全率提升。”
</details></li>
</ul>
<hr>
<h2 id="CodeTransOcean-A-Comprehensive-Multilingual-Benchmark-for-Code-Translation"><a href="#CodeTransOcean-A-Comprehensive-Multilingual-Benchmark-for-Code-Translation" class="headerlink" title="CodeTransOcean: A Comprehensive Multilingual Benchmark for Code Translation"></a>CodeTransOcean: A Comprehensive Multilingual Benchmark for Code Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04951">http://arxiv.org/abs/2310.04951</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/weixiangyan/codetransocean">https://github.com/weixiangyan/codetransocean</a></li>
<li>paper_authors: Weixiang Yan, Yuchen Tian, Yunzhe Li, Qian Chen, Wen Wang</li>
<li>for: 这个研究旨在提高代码翻译的质量和维护效率，并满足实际应用中的多元化需求。</li>
<li>methods: 这个研究使用了人工神经网络翻译模型，探索了多种程式语言之间的翻译，包括具有多种程式语言的复杂混合翻译。</li>
<li>results: 研究发现，这些多种程式语言翻译方法可以提高低资源语言的翻译质量和高资源语言的培训效率。此外，研究还提出了一个新的评估指标Debugging Success Rate@K，用于评估翻译后的程式码可行性。<details>
<summary>Abstract</summary>
Recent code translation techniques exploit neural machine translation models to translate source code from one programming language to another to satisfy production compatibility or to improve efficiency of codebase maintenance. Most existing code translation datasets only focus on a single pair of popular programming languages. To advance research on code translation and meet diverse requirements of real-world applications, we construct CodeTransOcean, a large-scale comprehensive benchmark that supports the largest variety of programming languages for code translation. CodeTransOcean consists of three novel multilingual datasets, namely, MultilingualTrans supporting translations between multiple popular programming languages, NicheTrans for translating between niche programming languages and popular ones, and LLMTrans for evaluating executability of translated code by large language models (LLMs). CodeTransOcean also includes a novel cross-framework dataset, DLTrans, for translating deep learning code across different frameworks. We develop multilingual modeling approaches for code translation and demonstrate their great potential in improving the translation quality of both low-resource and high-resource language pairs and boosting the training efficiency. We also propose a novel evaluation metric Debugging Success Rate@K for program-level code translation. Last but not least, we evaluate LLM ChatGPT on our datasets and investigate its potential for fuzzy execution predictions. We build baselines for CodeTransOcean and analyze challenges of code translation for guiding future research. The CodeTransOcean datasets and code are publicly available at https://github.com/WeixiangYAN/CodeTransOcean.
</details>
<details>
<summary>摘要</summary>
现代代码翻译技术利用神经机器翻译模型将源代码从一种编程语言翻译到另一种编程语言，以满足生产兼容性或改善代码维护效率。现有大多数代码翻译数据集只关注单个受欢迎的编程语言对。为了推动代码翻译研究和满足实际应用的多样化需求，我们构建了CodeTransOcean，一个大规模、完整的benchmark，支持最多的编程语言对进行代码翻译。CodeTransOcean包括三个新的多语言数据集：MultilingualTrans、NicheTrans和LLMTrans。MultilingualTrans支持多种受欢迎编程语言之间的翻译，NicheTrans用于将特殊编程语言与受欢迎语言之间翻译，LLMTrans用于通过大语言模型（LLMs）评估翻译后代码的执行可能性。CodeTransOcean还包括一个跨框架数据集DLTrans，用于跨不同框架深度学习代码翻译。我们开发了多语言模型方法，并证明它们在低资源语言对和高资源语言对翻译质量提高和训练效率提高。我们还提出了一个新的评价指标Debugging Success Rate@K，用于评估翻译后代码的可调试性。最后，我们评估了LLM ChatGPT在我们的数据集上的性能，并调查其可能性于软件执行预测。我们建立了CodeTransOcean的基准，并分析了代码翻译的挑战，以帮助未来研究。CodeTransOcean数据集和代码可以在https://github.com/WeixiangYAN/CodeTransOcean上下载。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/08/cs.AI_2023_10_08/" data-id="clp869trn0057k588ewd4cg7i" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_10_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/08/cs.CL_2023_10_08/" class="article-date">
  <time datetime="2023-10-08T11:00:00.000Z" itemprop="datePublished">2023-10-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/08/cs.CL_2023_10_08/">cs.CL - 2023-10-08</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Visual-Storytelling-with-Question-Answer-Plans"><a href="#Visual-Storytelling-with-Question-Answer-Plans" class="headerlink" title="Visual Storytelling with Question-Answer Plans"></a>Visual Storytelling with Question-Answer Plans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05295">http://arxiv.org/abs/2310.05295</a></li>
<li>repo_url: None</li>
<li>paper_authors: Danyang Liu, Mirella Lapata, Frank Keller</li>
<li>for: 本研究旨在生成吸引人的故事，从图像序列中提取有趣的视觉表达。</li>
<li>methods: 该模型将图像序列转化为可进行语言模型解释的视觉预фикс，并使用问题对话对话来选择关键的视觉概念并决定如何将它们组织成一个故事。</li>
<li>results: 自动和人工评估结果表明，蓝图基本模型可以生成更加有趣、有логи、自然的故事，比baseline和现有系统更高效。<details>
<summary>Abstract</summary>
Visual storytelling aims to generate compelling narratives from image sequences. Existing models often focus on enhancing the representation of the image sequence, e.g., with external knowledge sources or advanced graph structures. Despite recent progress, the stories are often repetitive, illogical, and lacking in detail. To mitigate these issues, we present a novel framework which integrates visual representations with pretrained language models and planning. Our model translates the image sequence into a visual prefix, a sequence of continuous embeddings which language models can interpret. It also leverages a sequence of question-answer pairs as a blueprint plan for selecting salient visual concepts and determining how they should be assembled into a narrative. Automatic and human evaluation on the VIST benchmark (Huang et al., 2016) demonstrates that blueprint-based models generate stories that are more coherent, interesting, and natural compared to competitive baselines and state-of-the-art systems.
</details>
<details>
<summary>摘要</summary>
Visual storytelling 目标是从图像序列中生成吸引人的故事。现有的模型通常会将注意力集中在图像序列的表现方面，例如通过与外部知识源或高级graph structures整合。despite recent progress, stories are often repetitive, illogical, and lacking in detail. To address these issues, we propose a novel framework that integrates visual representations with pre-trained language models and planning. Our model translates the image sequence into a visual prefix, a sequence of continuous embeddings that language models can interpret. It also leverages a sequence of question-answer pairs as a blueprint plan for selecting salient visual concepts and determining how they should be assembled into a narrative. Automatic and human evaluation on the VIST benchmark (Huang et al., 2016) shows that blueprint-based models generate stories that are more coherent, interesting, and natural compared to competitive baselines and state-of-the-art systems.
</details></li>
</ul>
<hr>
<h2 id="Hi-Guys-or-Hi-Folks-Benchmarking-Gender-Neutral-Machine-Translation-with-the-GeNTE-Corpus"><a href="#Hi-Guys-or-Hi-Folks-Benchmarking-Gender-Neutral-Machine-Translation-with-the-GeNTE-Corpus" class="headerlink" title="Hi Guys or Hi Folks? Benchmarking Gender-Neutral Machine Translation with the GeNTE Corpus"></a>Hi Guys or Hi Folks? Benchmarking Gender-Neutral Machine Translation with the GeNTE Corpus</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05294">http://arxiv.org/abs/2310.05294</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hlt-mt/fbk-neutr-eval">https://github.com/hlt-mt/fbk-neutr-eval</a></li>
<li>paper_authors: Andrea Piergentili, Beatrice Savoldi, Dennis Fucci, Matteo Negri, Luisa Bentivogli</li>
<li>for:  Addressing the lack of inclusive language in machine translation, particularly in grammatical gender languages.</li>
<li>methods:  Proposing a dedicated benchmark and exploring automated evaluation methods for gender-neutral translation from English to Italian, including a natural, bilingual test set (GeNTE) and a reference-free evaluation approach.</li>
<li>results:  A new, more inclusive approach to machine translation that challenges traditional binary gender assumptions and provides a more accurate assessment of gender-neutral translation.<details>
<summary>Abstract</summary>
Gender inequality is embedded in our communication practices and perpetuated in translation technologies. This becomes particularly apparent when translating into grammatical gender languages, where machine translation (MT) often defaults to masculine and stereotypical representations by making undue binary gender assumptions. Our work addresses the rising demand for inclusive language by focusing head-on on gender-neutral translation from English to Italian. We start from the essentials: proposing a dedicated benchmark and exploring automated evaluation methods. First, we introduce GeNTE, a natural, bilingual test set for gender-neutral translation, whose creation was informed by a survey on the perception and use of neutral language. Based on GeNTE, we then overview existing reference-based evaluation approaches, highlight their limits, and propose a reference-free method more suitable to assess gender-neutral translation.
</details>
<details>
<summary>摘要</summary>
gender inequality 在我们的沟通习惯中存在并在翻译技术中被延续。这种情况特别在翻译到 grammatical gender 语言时变得明显，MT 常 defaults to  masculine 和标准化的表达，从而做出了不当的男性假设。我们的工作解决了包容性语言的增长需求，专注于从英语到意大利语的gender-neutral 翻译。我们从基础开始：提议一个专门的标准和探索自动评估方法。首先，我们介绍了 GeNTE，一个自然、双语测试集 для gender-neutral 翻译，其创建受到了对中性语言的感知和使用的调查。然后，我们概述了现有的参照基础评估方法， highlight  их的局限性，并提出了不需要参照的方法，更适合评估 gender-neutral 翻译。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Pre-Trained-Language-Models-with-Sentence-Position-Embeddings-for-Rhetorical-Roles-Recognition-in-Legal-Opinions"><a href="#Enhancing-Pre-Trained-Language-Models-with-Sentence-Position-Embeddings-for-Rhetorical-Roles-Recognition-in-Legal-Opinions" class="headerlink" title="Enhancing Pre-Trained Language Models with Sentence Position Embeddings for Rhetorical Roles Recognition in Legal Opinions"></a>Enhancing Pre-Trained Language Models with Sentence Position Embeddings for Rhetorical Roles Recognition in Legal Opinions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05276">http://arxiv.org/abs/2310.05276</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anas Belfathi, Nicolas Hernandez, Laura Monceaux</li>
<li>For: 这个研究论文是为了提出一种基于预训练语言模型（PLM）和句子位置信息的新型自动预测辩论角色的模型建模方法。* Methods: 该方法使用了一个简单的模型结构，并使用了LegalEval@SemEval2023 competition上的注释 corpora进行训练。在这个 corpus 中，它们使用了一些特定的预处理技术来提高模型的性能。* Results: 研究人员发现，他们的方法比使用复杂的层次模型在全局上的方法更加简单，具有更低的计算成本。此外，他们还发现，通过在本地上增加更多的注意力，以及将句子位置信息纳入模型中，可以进一步提高结果。<details>
<summary>Abstract</summary>
The legal domain is a vast and complex field that involves a considerable amount of text analysis, including laws, legal arguments, and legal opinions. Legal practitioners must analyze these texts to understand legal cases, research legal precedents, and prepare legal documents. The size of legal opinions continues to grow, making it increasingly challenging to develop a model that can accurately predict the rhetorical roles of legal opinions given their complexity and diversity. In this research paper, we propose a novel model architecture for automatically predicting rhetorical roles using pre-trained language models (PLMs) enhanced with knowledge of sentence position information within a document. Based on an annotated corpus from the LegalEval@SemEval2023 competition, we demonstrate that our approach requires fewer parameters, resulting in lower computational costs when compared to complex architectures employing a hierarchical model in a global-context, yet it achieves great performance. Moreover, we show that adding more attention to a hierarchical model based only on BERT in the local-context, along with incorporating sentence position information, enhances the results.
</details>
<details>
<summary>摘要</summary>
法律领域是一个庞大复杂的领域，涉及到大量的文本分析，包括法律、法律论据和法律意见。法律实践者需要分析这些文本，以理解法律案例，研究法律先例，并准备法律文书。随着法律意见的大小不断增加，以至于开发一个可以准确预测法律意见的模型变得愈加挑战。在这篇研究论文中，我们提出一种新的模型建立方法，使用预训练语言模型（PLMs），并在文本中添加句子位置信息，以自动预测法律意见的文化角色。基于LegalEval@SemEval2023比赛获得的标注词汇集，我们示示了我们的方法需要 fewer parameters，相比较复杂的结构，计算成本更低，同时可以达到高效的表现。此外，我们还证明了在地方上添加更多注意力，以及基于BERT的层次模型，可以提高结果。
</details></li>
</ul>
<hr>
<h2 id="XLS-R-fine-tuning-on-noisy-word-boundaries-for-unsupervised-speech-segmentation-into-words"><a href="#XLS-R-fine-tuning-on-noisy-word-boundaries-for-unsupervised-speech-segmentation-into-words" class="headerlink" title="XLS-R fine-tuning on noisy word boundaries for unsupervised speech segmentation into words"></a>XLS-R fine-tuning on noisy word boundaries for unsupervised speech segmentation into words</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05235">http://arxiv.org/abs/2310.05235</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robin Algayres, Pablo Diego-Simon, Benoit Sagot, Emmanuel Dupoux<br>for: 这个论文的目的是提高无文本支持的语音分割任务的性能。methods: 这个论文使用了最新的自我超vised speech模型，通过精度调整来快速适应新任务，即使在资源匮乏的情况下。它们引入了 semi-supervised learning的想法，使用 XLS-R 模型预测语音分割系统生成的字Boundary。results: 这个论文的方法可以一直提高每种系统的性能，并在五种语言 corpora 上设置了新的状态态�idents，平均提高了130%的 F1 分数。此外，这个系统还可以在无seen语言中进行零shot分割。<details>
<summary>Abstract</summary>
Due to the absence of explicit word boundaries in the speech stream, the task of segmenting spoken sentences into word units without text supervision is particularly challenging. In this work, we leverage the most recent self-supervised speech models that have proved to quickly adapt to new tasks through fine-tuning, even in low resource conditions. Taking inspiration from semi-supervised learning, we fine-tune an XLS-R model to predict word boundaries themselves produced by top-tier speech segmentation systems: DPDP, VG-HuBERT, GradSeg and DP-Parse. Once XLS-R is fine-tuned, it is used to infer new word boundary labels that are used in turn for another fine-tuning step. Our method consistently improves the performance of each system and sets a new state-of-the-art that is, on average 130% higher than the previous one as measured by the F1 score on correctly discovered word tokens on five corpora featuring different languages. Finally, our system can segment speech from languages unseen during fine-tuning in a zero-shot fashion.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Generative-Spoken-Language-Model-based-on-continuous-word-sized-audio-tokens"><a href="#Generative-Spoken-Language-Model-based-on-continuous-word-sized-audio-tokens" class="headerlink" title="Generative Spoken Language Model based on continuous word-sized audio tokens"></a>Generative Spoken Language Model based on continuous word-sized audio tokens</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05224">http://arxiv.org/abs/2310.05224</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robin Algayres, Yossi Adi, Tu Anh Nguyen, Jade Copet, Gabriel Synnaeve, Benoit Sagot, Emmanuel Dupoux</li>
<li>for: 该论文旨在提出一种基于word-size连续值音频嵌入的生成语言模型（GSLM），以便生成多样化和表达力强的语言输出。</li>
<li>methods: 该模型使用了 Lexical Embedding 函数取代 lookup 表格，权重损失函数被替换为对比损失函数，以及多omial 采样被替换为 k-NN 采样。</li>
<li>results: 该模型的表现与基于分组单元 GSLMs 相当，自动度量器和人工评价都表示生成质量高，并且具有五倍的内存效率优势。此外，模型中的嵌入before和after Lexical Embedder 具有phonetics和semantics的可读性。<details>
<summary>Abstract</summary>
In NLP, text language models based on words or subwords are known to outperform their character-based counterparts. Yet, in the speech community, the standard input of spoken LMs are 20ms or 40ms-long discrete units (shorter than a phoneme). Taking inspiration from word-based LM, we introduce a Generative Spoken Language Model (GSLM) based on word-size continuous-valued audio embeddings that can generate diverse and expressive language output. This is obtained by replacing lookup table for lexical types with a Lexical Embedding function, the cross entropy loss by a contrastive loss, and multinomial sampling by k-NN sampling. The resulting model is the first generative language model based on word-size continuous embeddings. Its performance is on par with discrete unit GSLMs regarding generation quality as measured by automatic metrics and subjective human judgements. Moreover, it is five times more memory efficient thanks to its large 200ms units. In addition, the embeddings before and after the Lexical Embedder are phonetically and semantically interpretable.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)在 NLP 中，文本语言模型 based on words or subwords 知道会比其字符基本的对手表现更好。然而，在语音社区中，标准输入的语音LMs 是20ms或40ms短于一个音素的分 discrete units。 drawing inspiration from word-based LM, we introduce a Generative Spoken Language Model (GSLM) based on word-size continuous-valued audio embeddings that can generate diverse and expressive language output. This is obtained by replacing lookup table for lexical types with a Lexical Embedding function, the cross entropy loss by a contrastive loss, and multinomial sampling by k-NN sampling. The resulting model is the first generative language model based on word-size continuous embeddings. Its performance is on par with discrete unit GSLMs regarding generation quality as measured by automatic metrics and subjective human judgements. Moreover, it is five times more memory efficient thanks to its large 200ms units. In addition, the embeddings before and after the Lexical Embedder are phonetically and semantically interpretable.
</details></li>
</ul>
<hr>
<h2 id="Probing-Language-Models-from-A-Human-Behavioral-Perspective"><a href="#Probing-Language-Models-from-A-Human-Behavioral-Perspective" class="headerlink" title="Probing Language Models from A Human Behavioral Perspective"></a>Probing Language Models from A Human Behavioral Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05216">http://arxiv.org/abs/2310.05216</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xintong Wang, Xiaoyu Li, Xingshan Li, Chris Biemann</li>
<li>for: This paper aims to provide a better understanding of how large language models (LLMs) work and how they make predictions.</li>
<li>methods: The authors use eye-tracking measures to correlate with the values produced by LLMs and compare them to those of recurrent neural network-based language models (RNN-LMs). They also analyze the functions of self-attention and gate mechanisms in LLMs.</li>
<li>results: The study finds that LLMs exhibit a distinct prediction pattern compared to RNN-LMs, with a peak in memorization and linguistic knowledge encoding as the number of feed-forward network (FFN) layers increases, followed by a pivot to comprehension capacity. The self-attention mechanisms are found to be distributed across multiple heads, and the gate mechanisms control the flow of information, with some gates promoting and others eliminating information.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have emerged as dominant foundational models in modern NLP. However, the understanding of their prediction process and internal mechanisms, such as feed-forward networks and multi-head self-attention, remains largely unexplored. In this study, we probe LLMs from a human behavioral perspective, correlating values from LLMs with eye-tracking measures, which are widely recognized as meaningful indicators of reading patterns. Our findings reveal that LLMs exhibit a prediction pattern distinct from that of RNN-based LMs. Moreover, with the escalation of FFN layers, the capacity for memorization and linguistic knowledge encoding also surges until it peaks, subsequently pivoting to focus on comprehension capacity. The functions of self-attention are distributed across multiple heads. Lastly, we scrutinize the gate mechanisms, finding that they control the flow of information, with some gates promoting, while others eliminating information.
</details>
<details>
<summary>摘要</summary>
Note:* "Large Language Models" (LLMs) 是现代 NLP 中最具代表性的基础模型，但它们的预测过程和内部机制仍然尚未得到充分的研究。* "feed-forward networks" (FFNs) 是 LLMs 的一种基本结构，它们在预测过程中发挥着重要的作用。* "multi-head self-attention" 是 LLMs 中的一种自注意机制，它可以帮助模型更好地理解语言结构和含义。* "eye-tracking measures" 是一种广泛用于研究人类阅读习惯的方法，它可以反映人们在阅读过程中的注意力和理解程度。* "gate mechanisms" 是 LLMs 中的一种控制信息流动的机制，它可以帮助模型更好地过滤不必要的信息并保留有用信息。
</details></li>
</ul>
<hr>
<h2 id="A-Comparative-Study-of-Voice-Conversion-Models-with-Large-Scale-Speech-and-Singing-Data-The-T13-Systems-for-the-Singing-Voice-Conversion-Challenge-2023"><a href="#A-Comparative-Study-of-Voice-Conversion-Models-with-Large-Scale-Speech-and-Singing-Data-The-T13-Systems-for-the-Singing-Voice-Conversion-Challenge-2023" class="headerlink" title="A Comparative Study of Voice Conversion Models with Large-Scale Speech and Singing Data: The T13 Systems for the Singing Voice Conversion Challenge 2023"></a>A Comparative Study of Voice Conversion Models with Large-Scale Speech and Singing Data: The T13 Systems for the Singing Voice Conversion Challenge 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05203">http://arxiv.org/abs/2310.05203</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryuichi Yamamoto, Reo Yoneyama, Lester Phillip Violeta, Wen-Chin Huang, Tomoki Toda</li>
<li>for: 这个论文targets the singing voice conversion challenge (SVCC) 2023, with a recognition-synthesis approach using self-supervised learning-based representation.</li>
<li>methods: 该方法首先使用公共可用的大规模750小时的语音和唱歌数据进行扩散基于的任意到任意语音转换模型的训练，然后对每个目标唱歌者&#x2F;说话者进行微调。</li>
<li>results: 大规模的听力测试显示，我们的T13系统在SVCC 2023中获得了竞争力强的自然性和说话者相似性，这表明了我们的方法在跨频道SVC中的泛化能力。<details>
<summary>Abstract</summary>
This paper presents our systems (denoted as T13) for the singing voice conversion challenge (SVCC) 2023. For both in-domain and cross-domain English singing voice conversion (SVC) tasks (Task 1 and Task 2), we adopt a recognition-synthesis approach with self-supervised learning-based representation. To achieve data-efficient SVC with a limited amount of target singer/speaker's data (150 to 160 utterances for SVCC 2023), we first train a diffusion-based any-to-any voice conversion model using publicly available large-scale 750 hours of speech and singing data. Then, we finetune the model for each target singer/speaker of Task 1 and Task 2. Large-scale listening tests conducted by SVCC 2023 show that our T13 system achieves competitive naturalness and speaker similarity for the harder cross-domain SVC (Task 2), which implies the generalization ability of our proposed method. Our objective evaluation results show that using large datasets is particularly beneficial for cross-domain SVC.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍我们的系统（简称为T13）在2023年歌唱voice conversions挑战（SVCC）中的应用。对于英语歌唱voice conversions（SVC）的内域和跨域任务（任务1和任务2），我们采用了认知-合成方法，使用自我超vised学习基于表示。为了实现数据精efficient的SVC，我们首先使用公共可用的大规模750小时的说话和唱歌数据来训练一个扩散-based any-to-anyvoice conversions模型。然后，我们对每个目标歌手/说话人进行了微调。SVCC 2023年的大规模听力测试显示，我们的T13系统在跨域SVC（任务2）中实现了竞争性的自然和说话人相似性，这表明了我们提出的方法的泛化能力。我们的目标评价结果表明，使用大量数据对跨域SVC是非常有利的。
</details></li>
</ul>
<hr>
<h2 id="Loose-lips-sink-ships-Mitigating-Length-Bias-in-Reinforcement-Learning-from-Human-Feedback"><a href="#Loose-lips-sink-ships-Mitigating-Length-Bias-in-Reinforcement-Learning-from-Human-Feedback" class="headerlink" title="Loose lips sink ships: Mitigating Length Bias in Reinforcement Learning from Human Feedback"></a>Loose lips sink ships: Mitigating Length Bias in Reinforcement Learning from Human Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05199">http://arxiv.org/abs/2310.05199</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Shen, Rui Zheng, Wenyu Zhan, Jun Zhao, Shihan Dou, Tao Gui, Qi Zhang, Xuanjing Huang</li>
<li>for: 这篇论文的目的是如何使用人类反馈来改善大型自然语言模型，使其更好地适应人类和社会价值。</li>
<li>methods: 这篇论文使用了Product-of-Experts（PoE）技术，将奖励模型分为两部分：主要专家关注人类意图，而偏见专家则targets the identification and capture of length bias。另外，为了进一步提高偏见的学习，我们导入了扰动 INTO the bias-focused expert, disrupting the flow of semantic information。</li>
<li>results: 实验结果显示，我们的方法可以改善语言模型的性能，不受序列长度的影响。<details>
<summary>Abstract</summary>
Reinforcement learning from human feedback serves as a crucial bridge, aligning large language models with human and societal values. This alignment requires a vast corpus of human feedback to learn a reward model, which is subsequently used to finetune language models. However, we have identified that the reward model often finds shortcuts to bypass its intended objectives, misleadingly assuming that humans prefer longer responses. The emergence of length bias often induces the model to favor longer outputs, yet it doesn't equate to an increase in helpful information within these outputs. In this paper, we propose an innovative solution, applying the Product-of-Experts (PoE) technique to separate reward modeling from the influence of sequence length. In our framework, the main expert concentrates on understanding human intents, while the biased expert targets the identification and capture of length bias. To further enhance the learning of bias, we introduce perturbations into the bias-focused expert, disrupting the flow of semantic information. Experimental results validate the effectiveness of our approach, indicating that language model performance is improved, irrespective of sequence length.
</details>
<details>
<summary>摘要</summary>
大language模型可以通过人类反馈来进行强化学习，这种反馈可以帮助模型与人类和社会价值观念相Alignment。为了学习奖励模型，需要一个大量的人类反馈，然后使用这个奖励模型来精化语言模型。然而，我们发现奖励模型经常会寻找短cut的缺点，假设人类更喜欢 longer responses。这种Length bias会导致模型偏好 longer outputs，但这并不意味着这些输出中含有更多的有用信息。在这篇论文中，我们提出了一种创新的解决方案，通过Product-of-Experts（PoE）技术分离奖励模型和序列长度的影响。在我们的框架中，主专家专注于理解人类意图，而偏好专家则targets the identification and capture of length bias。为了进一步增强偏好的学习，我们引入了对偏好专家中的干扰，使得 semantic information的流动被中断。实验结果证明了我们的方法的有效性，表明语言模型的性能不受序列长度的限制。
</details></li>
</ul>
<hr>
<h2 id="FABRIC-Automated-Scoring-and-Feedback-Generation-for-Essays"><a href="#FABRIC-Automated-Scoring-and-Feedback-Generation-for-Essays" class="headerlink" title="FABRIC: Automated Scoring and Feedback Generation for Essays"></a>FABRIC: Automated Scoring and Feedback Generation for Essays</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05191">http://arxiv.org/abs/2310.05191</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jieun Han, Haneul Yoo, Junho Myung, Minsun Kim, Hyunseung Lim, Yoonsu Kim, Tak Yeon Lee, Hwajung Hong, Juho Kim, So-Yeon Ahn, Alice Oh</li>
<li>for: 这个论文是为了提供一种自动生成英语写作评分的工具，以帮助学生和教师在写作课程中更好地评分和反馈写作。</li>
<li>methods: 该论文使用了一种管道模型，包括DREsS、CASE和EssayCoT三部分。DREsS是一个基于标准的写作评分数据集，CASE是一种伪造策略，可以提高模型的准确率。EssayCoT是一种写作思维推荐策略，可以根据模型预测的分数提供更好的反馈。</li>
<li>results: 论文表明，使用新的数据集DREsS和伪造策略CASE可以提高模型的准确率，并且使用EssayCoT可以提供更好的反馈。论文还表明，学生和教师对新的评分和反馈表示满意，评分和反馈的帮助程度也得到了提升。<details>
<summary>Abstract</summary>
Automated essay scoring (AES) provides a useful tool for students and instructors in writing classes by generating essay scores in real-time. However, previous AES models do not provide more specific rubric-based scores nor feedback on how to improve the essays, which can be even more important than the overall scores for learning. We present FABRIC, a pipeline to help students and instructors in English writing classes by automatically generating 1) the overall scores, 2) specific rubric-based scores, and 3) detailed feedback on how to improve the essays. Under the guidance of English education experts, we chose the rubrics for the specific scores as content, organization, and language. The first component of the FABRIC pipeline is DREsS, a real-world Dataset for Rubric-based Essay Scoring (DREsS). The second component is CASE, a Corruption-based Augmentation Strategy for Essays, with which we can improve the accuracy of the baseline model by 45.44%. The third component is EssayCoT, the Essay Chain-of-Thought prompting strategy which uses scores predicted from the AES model to generate better feedback. We evaluate the effectiveness of the new dataset DREsS and the augmentation strategy CASE quantitatively and show significant improvements over the models trained with existing datasets. We evaluate the feedback generated by EssayCoT with English education experts to show significant improvements in the helpfulness of the feedback across all rubrics. Lastly, we evaluate the FABRIC pipeline with students in a college English writing class who rated the generated scores and feedback with an average of 6 on the Likert scale from 1 to 7.
</details>
<details>
<summary>摘要</summary>
自动化文章评分（AES）为学生和教师写作课程提供了一个有用的工具，可以在实时生成文章评分。然而，先前的AES模型并不提供更加特定的评分标准和提高文章的细节反馈，这些反馈可能对学习更加重要。我们介绍了FABRIC管道，帮助学生和教师英语写作课程，可以自动生成1）总评分，2）特定评分标准，以及3）提高文章的细节反馈。在英语教育专家的指导下，我们选择了评分标准的内容、组织和语言。FABRIC管道的第一个组成部分是DREsS，一个用于评分标准的实际数据集（DREsS）。第二个组成部分是CASE，一种对文章进行恶意增强策略，可以提高基线模型的准确率45.44%。第三个组成部分是EssayCoT，文章链条思维提醒策略，使用AES模型预测的分数来生成更好的反馈。我们评估了新的数据集DREsS和增强策略CASE的效果，并显示了与现有数据集训练的模型相比有显著提高。我们评估EssayCoT生成的反馈与英语教育专家相比，并显示了所有评分标准上的有用性提高。最后，我们评估了FABRIC管道与大学英语写作课程学生的反馈，学生对生成的分数和反馈给出了7分的满分评价。
</details></li>
</ul>
<hr>
<h2 id="Do-Large-Language-Models-Know-about-Facts"><a href="#Do-Large-Language-Models-Know-about-Facts" class="headerlink" title="Do Large Language Models Know about Facts?"></a>Do Large Language Models Know about Facts?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05177">http://arxiv.org/abs/2310.05177</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Xuming Hu, Junzhe Chen, Xiaochuan Li, Yufei Guo, Lijie Wen, Philip S. Yu, Zhijiang Guo</li>
<li>for: 这paper的目的是评估大型自然语言处理模型（LLMs）中的事实知识，以及这些模型是否可以具备真实知识和抵御黑客攻击。</li>
<li>methods: 这paper使用了一个名为Pinocchio的benchmark，包含20000个多样化的事实问题，以评估LLMs中的事实知识。</li>
<li>results: 经过extensive的实验研究发现，现有的LLMs仍然缺乏事实知识，并且存在多种假相关性。<details>
<summary>Abstract</summary>
Large language models (LLMs) have recently driven striking performance improvements across a range of natural language processing tasks. The factual knowledge acquired during pretraining and instruction tuning can be useful in various downstream tasks, such as question answering, and language generation. Unlike conventional Knowledge Bases (KBs) that explicitly store factual knowledge, LLMs implicitly store facts in their parameters. Content generated by the LLMs can often exhibit inaccuracies or deviations from the truth, due to facts that can be incorrectly induced or become obsolete over time. To this end, we aim to comprehensively evaluate the extent and scope of factual knowledge within LLMs by designing the benchmark Pinocchio. Pinocchio contains 20K diverse factual questions that span different sources, timelines, domains, regions, and languages. Furthermore, we investigate whether LLMs are able to compose multiple facts, update factual knowledge temporally, reason over multiple pieces of facts, identify subtle factual differences, and resist adversarial examples. Extensive experiments on different sizes and types of LLMs show that existing LLMs still lack factual knowledge and suffer from various spurious correlations. We believe this is a critical bottleneck for realizing trustworthy artificial intelligence. The dataset Pinocchio and our codes will be publicly available.
</details>
<details>
<summary>摘要</summary>
大型自然语言处理模型（LLM）在最近几年中带来了一系列的性能提升。这些模型在预训练和调教过程中获得的 фактиче知识可以在多种下游任务中使用，如问答和语言生成。不同于传统的知识库（KB），LLM中的知识不是显式存储的，而是通过模型参数的方式隐式存储。由模型生成的内容经常会具有误差或不准确，因为模型可能会 incorrectly induce 或者随着时间的推移而变得过时。为了全面评估 LLM 中的知识范围和深度，我们设计了 Pinocchio  benchmark。Pinocchio 包含 20,000 个多样化的 фактиче问题，这些问题来自不同的来源、时间线、领域、地区和语言。此外，我们还 investigate 了 LLM 是否能够组合多个 фактиче知识、 temporally 更新知识、理解多个知识之间的关系、察看微妙的知识差异以及抵御骚扰示例。我们对不同大小和类型的 LLM 进行了广泛的实验，发现现有 LLM 仍然缺乏知识和受到多种假 correlate 的影响。我们认为这是人工智能实现可信worthy 的核心瓶颈。Pinocchio 数据集和我们的代码将公开发布。
</details></li>
</ul>
<hr>
<h2 id="On-the-Zero-Shot-Generalization-of-Machine-Generated-Text-Detectors"><a href="#On-the-Zero-Shot-Generalization-of-Machine-Generated-Text-Detectors" class="headerlink" title="On the Zero-Shot Generalization of Machine-Generated Text Detectors"></a>On the Zero-Shot Generalization of Machine-Generated Text Detectors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05165">http://arxiv.org/abs/2310.05165</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiao Pu, Jingyu Zhang, Xiaochuang Han, Yulia Tsvetkov, Tianxing He</li>
<li>for: 本研究的目的是检测机器生成的文本，以确定新生成器输出的真实性。</li>
<li>methods: 本研究使用了许多大语言模型生成的数据，并使用神经网络检测器来检测机器生成的文本。</li>
<li>results: 研究发现，使用中等大小的语言模型生成的数据来训练检测器，可以在其他大型模型上实现零基础泛化。这表明，可以通过将中等大小模型的数据作为基础，建立可靠的机器生成文本检测器。<details>
<summary>Abstract</summary>
The rampant proliferation of large language models, fluent enough to generate text indistinguishable from human-written language, gives unprecedented importance to the detection of machine-generated text. This work is motivated by an important research question: How will the detectors of machine-generated text perform on outputs of a new generator, that the detectors were not trained on? We begin by collecting generation data from a wide range of LLMs, and train neural detectors on data from each generator and test its performance on held-out generators. While none of the detectors can generalize to all generators, we observe a consistent and interesting pattern that the detectors trained on data from a medium-size LLM can zero-shot generalize to the larger version. As a concrete application, we demonstrate that robust detectors can be built on an ensemble of training data from medium-sized models.
</details>
<details>
<summary>摘要</summary>
大量的语言模型的蔓延，使得机器生成文本的检测成为了不可或缺的任务。这项工作受到一个重要的研究问题的推动：新生成器输出的机器生成文本检测器如何表现？我们开始sBy collecting generation data from a wide range of LLMs, and training neural detectors on data from each generator, we test the performance of these detectors on held-out generators. While none of the detectors can generalize to all generators, we observe a consistent and interesting pattern that the detectors trained on data from a medium-size LLM can zero-shot generalize to the larger version. As a concrete application, we demonstrate that robust detectors can be built on an ensemble of training data from medium-sized models.Here's the translation breakdown:* 大量 (dà liàng) - large amount* 语言模型 (yǔ yán módel) - language model* 蔓延 (shū yì) - rampant proliferation* 机器生成文本 (jī shì zhì yì wén tǐ) - machine-generated text* 检测 (jiǎn dòu) - detection* 新生成器 (xīn shēng chéng qì) - new generator* 输出 (xū chū) - output* 机器生成文本检测器 (jī shì zhì yì wén tǐ jiàn dòu qì) - machine-generated text detector* none of the detectors can generalize to all generators (zhè yī xiàng qù zhè yī xiàng qù) - none of the detectors can generalize to all generators* medium-size LLM (zhōng xiǎo yǔ yán módel) - medium-size language model* zero-shot generalize (zhè yī xiàng qù) - zero-shot generalize* ensemble (jiān) - ensemble* training data (liào xīng xīng) - training data* robust (dòu lì) - robust* detectors (jiàn dòu qì) - detectors
</details></li>
</ul>
<hr>
<h2 id="An-Investigation-of-LLMs’-Inefficacy-in-Understanding-Converse-Relations"><a href="#An-Investigation-of-LLMs’-Inefficacy-in-Understanding-Converse-Relations" class="headerlink" title="An Investigation of LLMs’ Inefficacy in Understanding Converse Relations"></a>An Investigation of LLMs’ Inefficacy in Understanding Converse Relations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05163">http://arxiv.org/abs/2310.05163</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/3b-group/convre">https://github.com/3b-group/convre</a></li>
<li>paper_authors: Chengwen Qi, Bowen Li, Binyuan Hui, Bailin Wang, Jinyang Li, Jinwang Wu, Yuanjun Laili</li>
<li>for: 本文 investigate LLMs 是否真的理解正式语言的结构化 semantics，通过一个特殊情况——抽象 binary relation。</li>
<li>methods: 本文 introduce 一个新的 benchmark ConvRE，该 benchmark 包含 17 关系和 1240 个 triple 从受欢迎的知识 Graph completion 数据集中提取出来。本 benchmark 包含 two 个任务：Re2Text 和 Text2Re，它们是通过多选问答来评估 LLMs 对关系和相关文本的匹配能力。</li>
<li>results: 经过实验表明，LLMs 经常采用短cut 学习，并且在我们的 proposed benchmark 上仍然遇到挑战。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have achieved remarkable success in many formal language oriented tasks, such as structural data-to-text and semantic parsing. However current benchmarks mostly follow the data distribution of the pre-training data of LLMs. Therefore, a natural question rises that do LLMs really understand the structured semantics of formal languages. In this paper, we investigate this problem on a special case, converse binary relation. We introduce a new benchmark ConvRe focusing on converse relations, which contains 17 relations and 1240 triples extracted from popular knowledge graph completion datasets. Our ConvRE features two tasks, Re2Text and Text2Re, which are formulated as multi-choice question answering to evaluate LLMs' ability to determine the matching between relations and associated text. For the evaluation protocol, apart from different prompting methods, we further introduce variants to the test text and few-shot example text. We conduct experiments on three popular LLM families and have observed various scaling trends. The results suggest that LLMs often resort to shortcut learning and still face challenges on our proposed benchmark.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Recurrent-Neural-Language-Models-as-Probabilistic-Finite-state-Automata"><a href="#Recurrent-Neural-Language-Models-as-Probabilistic-Finite-state-Automata" class="headerlink" title="Recurrent Neural Language Models as Probabilistic Finite-state Automata"></a>Recurrent Neural Language Models as Probabilistic Finite-state Automata</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05161">http://arxiv.org/abs/2310.05161</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anej Svete, Ryan Cotterell</li>
<li>for: 本文研究语言模型（LM）的表示能力和限制，通过使用已知的ormalism来准确地描述LM的能力和限制。</li>
<li>methods: 本文使用了回归神经网络（RNN）LM来研究LM可以表示哪些概率分布。</li>
<li>results: 研究结果表明，简单的RNN可以表示一个子集的概率分布，而且需要至少有 $\Omega\left(N |\Sigma|\right)$ 神经元来表示一个任意决定性 finite-state LM。<details>
<summary>Abstract</summary>
Studying language models (LMs) in terms of well-understood formalisms allows us to precisely characterize their abilities and limitations. Previous work has investigated the representational capacity of recurrent neural network (RNN) LMs in terms of their capacity to recognize unweighted formal languages. However, LMs do not describe unweighted formal languages -- rather, they define probability distributions over strings. In this work, we study what classes of such probability distributions RNN LMs can represent, which allows us to make more direct statements about their capabilities. We show that simple RNNs are equivalent to a subclass of probabilistic finite-state automata, and can thus model a strict subset of probability distributions expressible by finite-state models. Furthermore, we study the space complexity of representing finite-state LMs with RNNs. We show that, to represent an arbitrary deterministic finite-state LM with $N$ states over an alphabet $\Sigma$, an RNN requires $\Omega\left(N |\Sigma|\right)$ neurons. These results present a first step towards characterizing the classes of distributions RNN LMs can represent and thus help us understand their capabilities and limitations.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:研究语言模型（LM）使用已知的形式主义，可以准确地描述它们的能力和局限性。先前的工作已经研究了基于回归神经网络（RNN）的语言模型的表示能力，但是LM不是形式语言的描述，而是一种字符串上的概率分布。在这项工作中，我们研究了RNN可以表示哪些类型的概率分布，这使得我们可以更直接地说明它们的能力。我们显示了简单的RNN等价于一个子集的概率金字塔自动机，因此它们可以模型一 subset of概率分布可以由金字塔自动机表示。此外，我们研究了表示finite-state LM的RNN空间复杂度。我们显示了，要表示一个任意deterministic finite-state LM，需要$\Omega\left(N |\Sigma|\right)$ neuron。这些结果为我们帮助理解RNN可以表示哪些类型的概率分布，并且帮助我们理解它们的能力和局限性。
</details></li>
</ul>
<hr>
<h2 id="From-Data-to-Dialogue-Leveraging-the-Structure-of-Knowledge-Graphs-for-Conversational-Exploratory-Search"><a href="#From-Data-to-Dialogue-Leveraging-the-Structure-of-Knowledge-Graphs-for-Conversational-Exploratory-Search" class="headerlink" title="From Data to Dialogue: Leveraging the Structure of Knowledge Graphs for Conversational Exploratory Search"></a>From Data to Dialogue: Leveraging the Structure of Knowledge Graphs for Conversational Exploratory Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05150">http://arxiv.org/abs/2310.05150</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sebischair/kg-conv-exploratory-search">https://github.com/sebischair/kg-conv-exploratory-search</a></li>
<li>paper_authors: Phillip Schneider, Nils Rehtanz, Kristiina Jokinen, Florian Matthes</li>
<li>for: 这篇研究旨在探索新闻文章中的探索搜寻，以实现对话式搜寻和知识库的融合，从而将结构化和无结构化资料搜寻融合在一起。</li>
<li>methods: 本研究使用了对话式搜寻系统和知识库来支持探索搜寻，并透过自然语言问题来询问新闻文章中的相关资讯。</li>
<li>results: 根据54名参与者的用户研究，这种基于知识库的对话式搜寻系统被证明是有效的，并且提供了开发这类系统的设计假设。<details>
<summary>Abstract</summary>
Exploratory search is an open-ended information retrieval process that aims at discovering knowledge about a topic or domain rather than searching for a specific answer or piece of information. Conversational interfaces are particularly suitable for supporting exploratory search, allowing users to refine queries and examine search results through interactive dialogues. In addition to conversational search interfaces, knowledge graphs are also useful in supporting information exploration due to their rich semantic representation of data items. In this study, we demonstrate the synergistic effects of combining knowledge graphs and conversational interfaces for exploratory search, bridging the gap between structured and unstructured information retrieval. To this end, we propose a knowledge-driven dialogue system for exploring news articles by asking natural language questions and using the graph structure to navigate between related topics. Based on a user study with 54 participants, we empirically evaluate the effectiveness of the graph-based exploratory search and discuss design implications for developing such systems.
</details>
<details>
<summary>摘要</summary>
<SYS>探索搜寻是一种开放式搜寻过程，旨在探索一个主题或领域中的知识而不是寻找具体的答案或信息。对话式 интерфей斯特别适合支持探索搜寻，允许用户通过交互对话来细化查询和检视搜寻结果。此外，知识图也非常有用于支持信息探索，因为它们可以提供丰富的Semantic Representation的数据项。在这项研究中，我们证明了结合知识图和对话式 интерфей斯可以减少结构化和无结构化搜寻之间的差距，并提供一种基于知识的对话系统来探索新闻文章。基于54名参与者的用户研究，我们Empirically评估了图structure-based探索搜寻的效果，并讨论了开发这类系统的设计方面。</SYS>Here's a word-for-word translation of the text into Simplified Chinese:<SYS>探索搜寻是一种开放式搜寻过程，旨在探索一个主题或领域中的知识而不是寻找具体的答案或信息。对话式 интерфей斯特别适合支持探索搜寻，允许用户通过交互对话来细化查询和检视搜寻结果。此外，知识图也非常有用于支持信息探索，因为它们可以提供丰富的Semantic Representation的数据项。在这项研究中，我们证明了结合知识图和对话式 интерфей斯可以减少结构化和无结构化搜寻之间的差距，并提供一种基于知识的对话系统来探索新闻文章。基于54名参与者的用户研究，我们Empirically评估了图structure-based探索搜寻的效果，并讨论了开发这类系统的设计方面。</SYS>
</details></li>
</ul>
<hr>
<h2 id="Retrieval-Generation-Synergy-Augmented-Large-Language-Models"><a href="#Retrieval-Generation-Synergy-Augmented-Large-Language-Models" class="headerlink" title="Retrieval-Generation Synergy Augmented Large Language Models"></a>Retrieval-Generation Synergy Augmented Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05149">http://arxiv.org/abs/2310.05149</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhangyin Feng, Xiaocheng Feng, Dezhi Zhao, Maojin Yang, Bing Qin</li>
<li>for: 提高大型自然语言模型的理解能力和多步逻辑能力</li>
<li>methods: 融合任务相关文档和大型自然语言模型，通过反射-生成协作机制，利用参数化和非参数化知识，找到正确的逻辑路径</li>
<li>results: 在四个问答任务上，经验结果表明我们的方法可以显著提高大型自然语言模型的逻辑能力，并超越先前的基eline。<details>
<summary>Abstract</summary>
Large language models augmented with task-relevant documents have demonstrated impressive performance on knowledge-intensive tasks. However, regarding how to obtain effective documents, the existing methods are mainly divided into two categories. One is to retrieve from an external knowledge base, and the other is to utilize large language models to generate documents. We propose an iterative retrieval-generation collaborative framework. It is not only able to leverage both parametric and non-parametric knowledge, but also helps to find the correct reasoning path through retrieval-generation interactions, which is very important for tasks that require multi-step reasoning. We conduct experiments on four question answering datasets, including single-hop QA and multi-hop QA tasks. Empirical results show that our method significantly improves the reasoning ability of large language models and outperforms previous baselines.
</details>
<details>
<summary>摘要</summary>
大型语言模型，通过与任务相关的文档的协同工作，已经在知识型任务中表现出了惊人的表现。然而，现有的方法主要分为两类：一是从外部知识库中检索，另一是利用大型语言模型生成文档。我们提出了一种迭代检索生成协同框架，不仅能充分利用参数化和非参数化知识，而且能够通过检索生成互动，找到正确的逻辑路径，这对于需要多步逻辑的任务非常重要。我们在四个问答dataset上进行了实验，包括单步QA和多步QA任务。实验结果表明，我们的方法可以显著提高大型语言模型的逻辑能力，并超越先前的基elines。
</details></li>
</ul>
<hr>
<h2 id="Fast-DetectGPT-Efficient-Zero-Shot-Detection-of-Machine-Generated-Text-via-Conditional-Probability-Curvature"><a href="#Fast-DetectGPT-Efficient-Zero-Shot-Detection-of-Machine-Generated-Text-via-Conditional-Probability-Curvature" class="headerlink" title="Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature"></a>Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05130">http://arxiv.org/abs/2310.05130</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/baoguangsheng/fast-detect-gpt">https://github.com/baoguangsheng/fast-detect-gpt</a></li>
<li>paper_authors: Guangsheng Bao, Yanbin Zhao, Zhiyang Teng, Linyi Yang, Yue Zhang<br>for: 本研究旨在分别区别机器生成和人类撰写的内容，以建立可信赖的人工智能系统。methods: 本研究使用了 conditional probability curvature 来显示机器学习模型和人类之间的差异。results: Fast-DetectGPT 比 DetectGPT 更高效，可以在不同的数据集、来源模型和测试环境下提高检测效能，并且可以实现340倍的速度提升。<details>
<summary>Abstract</summary>
Large language models (LLMs) have shown the ability to produce fluent and cogent content, presenting both productivity opportunities and societal risks. To build trustworthy AI systems, it is imperative to distinguish between machine-generated and human-authored content. The leading zero-shot detector, DetectGPT, showcases commendable performance but is marred by its intensive computational costs. In this paper, we introduce the concept of conditional probability curvature to elucidate discrepancies in word choices between LLMs and humans within a given context. Utilizing this curvature as a foundational metric, we present Fast-DetectGPT, an optimized zero-shot detector, which substitutes DetectGPT's perturbation step with a more efficient sampling step. Our evaluations on various datasets, source models, and test conditions indicate that Fast-DetectGPT not only outperforms DetectGPT in both the white-box and black-box settings but also accelerates the detection process by a factor of 340, as detailed in Table 1.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Enhancing-Document-level-Event-Argument-Extraction-with-Contextual-Clues-and-Role-Relevance"><a href="#Enhancing-Document-level-Event-Argument-Extraction-with-Contextual-Clues-and-Role-Relevance" class="headerlink" title="Enhancing Document-level Event Argument Extraction with Contextual Clues and Role Relevance"></a>Enhancing Document-level Event Argument Extraction with Contextual Clues and Role Relevance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05991">http://arxiv.org/abs/2310.05991</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/LWL-cpu/SCPRG-master">https://github.com/LWL-cpu/SCPRG-master</a></li>
<li>paper_authors: Wanlong Liu, Shaohuan Cheng, Dingyi Zeng, Hong Qu</li>
<li>for: 这个论文主要针对的是文档级事件抽象EXTRACTION中的新挑战，即输入长度大、跨句理解。</li>
<li>methods: 我们提出了一种基于Span-trigger-based Contextual Pooling和 latent Role Guidance的SCPRG模型，包括两个新的有效模块，即 Span-Trigger-based Contextual Pooling(STCP)和 Role-based Latent Information Guidance (RLIG)。</li>
<li>results: 我们的SCPRG模型在两个公共数据集上进行了比较，与之前的状态态方法相比，提高了1.13和2.64的F1分数。<details>
<summary>Abstract</summary>
Document-level event argument extraction poses new challenges of long input and cross-sentence inference compared to its sentence-level counterpart. However, most prior works focus on capturing the relations between candidate arguments and the event trigger in each event, ignoring two crucial points: a) non-argument contextual clue information; b) the relevance among argument roles. In this paper, we propose a SCPRG (Span-trigger-based Contextual Pooling and latent Role Guidance) model, which contains two novel and effective modules for the above problem. The Span-Trigger-based Contextual Pooling(STCP) adaptively selects and aggregates the information of non-argument clue words based on the context attention weights of specific argument-trigger pairs from pre-trained model. The Role-based Latent Information Guidance (RLIG) module constructs latent role representations, makes them interact through role-interactive encoding to capture semantic relevance, and merges them into candidate arguments. Both STCP and RLIG introduce no more than 1% new parameters compared with the base model and can be easily applied to other event extraction models, which are compact and transplantable. Experiments on two public datasets show that our SCPRG outperforms previous state-of-the-art methods, with 1.13 F1 and 2.64 F1 improvements on RAMS and WikiEvents respectively. Further analyses illustrate the interpretability of our model.
</details>
<details>
<summary>摘要</summary>
文档级事件语义抽象带来新的挑战，包括长输入和跨句 inference，与句子级对应的模型不同。然而，大多数前作 FoCUS 在事件触发器和候选参与者之间的关系，忽略了两点： a) 非参与者上下文提示信息; b) 参与者角色之间的相关性。在这篇论文中，我们提出了一种SCPRG（Span-trigger-based Contextual Pooling and latent Role Guidance）模型，其包含两个新的有效模块。Span-Trigger-based Contextual Pooling（STCP）模块根据特定参与者-触发器对的上下文注意力权重自适应地选择和聚合非参与者提示词的信息。Role-based Latent Information Guidance（RLIG）模块构建了latent角色表示，使其互相交互编码，捕捉 semantic relevance，并将其与候选参与者结合。STCP和RLIG模块新增 Parameters 不超过1%，可以与基础模型一起使用，并且可以轻松应用于其他事件抽象模型。我们在两个公共数据集上进行了实验，结果显示，我们的SCPRG模型在RAMS和WikiEvents上的F1分别提高1.13和2.64。进一步的分析表明了我们模型的可读性。
</details></li>
</ul>
<hr>
<h2 id="CARLG-Leveraging-Contextual-Clues-and-Role-Correlations-for-Improving-Document-level-Event-Argument-Extraction"><a href="#CARLG-Leveraging-Contextual-Clues-and-Role-Correlations-for-Improving-Document-level-Event-Argument-Extraction" class="headerlink" title="CARLG: Leveraging Contextual Clues and Role Correlations for Improving Document-level Event Argument Extraction"></a>CARLG: Leveraging Contextual Clues and Role Correlations for Improving Document-level Event Argument Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05116">http://arxiv.org/abs/2310.05116</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wanlong Liu, Wenyu Chen, Dingyi Zeng, Li Zhou, Hong Qu</li>
<li>for: 提高文档级事件抽象EXTRACTION的精度。</li>
<li>methods: 提出了一种基于CONTEXTUAL CLUES和ROLE correlation的CARLG模型，包括CONTEXTUAL CLUES Aggregation（CCA）模块和ROLE-based Latent Information Guidance（RLIG）模块，利用上下文注意力权重和角色相互作用编码，从而提高文档级EXTRACTION的精度。</li>
<li>results: 在RAMS、WikiEvents和MLEE datasets上进行了广泛的实验，并证明了CARLG模型的超越性，与之前的状态艺术方法相比，提高了1.26倍、1.22倍和1.98倍的F1分数，同时降低了推理时间 by 31%。<details>
<summary>Abstract</summary>
Document-level event argument extraction (EAE) is a crucial but challenging subtask in information extraction. Most existing approaches focus on the interaction between arguments and event triggers, ignoring two critical points: the information of contextual clues and the semantic correlations among argument roles. In this paper, we propose the CARLG model, which consists of two modules: Contextual Clues Aggregation (CCA) and Role-based Latent Information Guidance (RLIG), effectively leveraging contextual clues and role correlations for improving document-level EAE. The CCA module adaptively captures and integrates contextual clues by utilizing context attention weights from a pre-trained encoder. The RLIG module captures semantic correlations through role-interactive encoding and provides valuable information guidance with latent role representation. Notably, our CCA and RLIG modules are compact, transplantable and efficient, which introduce no more than 1% new parameters and can be easily equipped on other span-base methods with significant performance boost. Extensive experiments on the RAMS, WikiEvents, and MLEE datasets demonstrate the superiority of the proposed CARLG model. It outperforms previous state-of-the-art approaches by 1.26 F1, 1.22 F1, and 1.98 F1, respectively, while reducing the inference time by 31%. Furthermore, we provide detailed experimental analyses based on the performance gains and illustrate the interpretability of our model.
</details>
<details>
<summary>摘要</summary>
文档级事件参数提取（EAE）是信息提取中的关键但是挑战性任务。现有大多数方法强调事件触发器和参数之间的交互，忽略了两个关键点：文档背景信息和参数角色之间的 semantics 相关性。在这篇论文中，我们提出了 CARLG 模型，它由两个模块组成：文档背景信息汇集（CCA）和角色相关信息引导（RLIG）。CCA 模块可以适应地捕捉和 инте integrate 文档背景信息，并通过使用上下文注意力权重从预训练的 encoder 获得上下文注意力权重。RLIG 模块通过角色交互编码来捕捉参数角色之间的 semantics 相关性，并提供有价值的信息引导，使用潜在角色表示。各自CCA和RLIG模块都是紧凑、可移植和高效的，其新增参数不超过 1%，可以轻松地在其他基于宽度的方法上采用，并且可以提高性能。我们在 RAMS、WikiEvents 和 MLEE 数据集上进行了广泛的实验，并证明了我们的 CARLG 模型在这些数据集上的超越性。它与前一个状态的方法相比，提高了 1.26 F1、1.22 F1 和 1.98 F1，同时降低了推理时间 31%。此外，我们还提供了详细的实验分析，以及模型的可读性。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Large-Language-Models-with-Augmented-Instructions-for-Fine-grained-Information-Extraction"><a href="#Benchmarking-Large-Language-Models-with-Augmented-Instructions-for-Fine-grained-Information-Extraction" class="headerlink" title="Benchmarking Large Language Models with Augmented Instructions for Fine-grained Information Extraction"></a>Benchmarking Large Language Models with Augmented Instructions for Fine-grained Information Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05092">http://arxiv.org/abs/2310.05092</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jun Gao, Huan Zhao, Yice Zhang, Wei Wang, Changlong Yu, Ruifeng Xu</li>
<li>for: 本研究旨在探讨大语言模型（LLMs）在自然语言处理中的信息提取 task 中的应用。</li>
<li>methods: 本研究使用了精细化的信息提取标准 benchmark 数据集，并采用了加强的提取规则和输出格式来适应 LLMS 的能力。</li>
<li>results: 我们的研究发现，使用encoder-decoder模型（特别是 T5 和 FLAN-T5）可以在不同的信息类型中具有普适性，而 ChatGPT 则在新任务形态中具有更高的适应性。我们的结果还表明，模型缩放不是决定性的性能因素，architecture、数据多样性和学习技术也具有重要的作用。这项研究为 LLMS 在信息提取中的更加细化和多样化应用提供了道路。<details>
<summary>Abstract</summary>
Information Extraction (IE) is an essential task in Natural Language Processing. Traditional methods have relied on coarse-grained extraction with simple instructions. However, with the emergence of Large Language Models (LLMs), there is a need to adapt IE techniques to leverage the capabilities of these models. This paper introduces a fine-grained IE benchmark dataset tailored for LLMs, employing augmented instructions for each information type, which includes task descriptions, extraction rules, output formats, and examples. Through extensive evaluations, we observe that encoder-decoder models, particularly T5 and FLAN-T5, perform well in generalizing to unseen information types, while ChatGPT exhibits greater adaptability to new task forms. Our results also indicate that performance is not solely dictated by model scale, and highlight the significance of architecture, data diversity, and learning techniques. This work paves the way for a more refined and versatile utilization of LLMs in Information Extraction.
</details>
<details>
<summary>摘要</summary>
信息提取（IE）是自然语言处理中的一项重要任务。传统方法通常采用粗粒度提取，使用简单的指令。然而，随着大语言模型（LLM）的出现，需要对IE技术进行适应。本文介绍了一个适合LLM的细致提取数据集，使用了增强的指令集，包括任务描述、提取规则、输出格式和示例。经过广泛的评估，我们发现使用encoder-decoder模型，特别是T5和FLAN-T5，在未经见情报类型上进行泛化性能良好，而ChatGPT在新任务形式上表现出更大的适应性。我们的结果还表明，性能不 solely 受模型规模的限制，也受到体系、数据多样性和学习技巧的影响。这项工作为LLM在信息提取中更加细致和多样化的使用开出了新的可能性。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Argument-Structure-Extraction-with-Efficient-Leverage-of-Contextual-Information"><a href="#Enhancing-Argument-Structure-Extraction-with-Efficient-Leverage-of-Contextual-Information" class="headerlink" title="Enhancing Argument Structure Extraction with Efficient Leverage of Contextual Information"></a>Enhancing Argument Structure Extraction with Efficient Leverage of Contextual Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05073">http://arxiv.org/abs/2310.05073</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luoxiaoheics/ecase">https://github.com/luoxiaoheics/ecase</a></li>
<li>paper_authors: Yun Luo, Zhen Yang, Fandong Meng, Yingjie Li, Jie Zhou, Yue Zhang</li>
<li>for: 本研究旨在提高对文档中Arguments的结构分析性能。</li>
<li>methods: 我们提出了一种高效的上下文感知ASE模型（ECASE），利用上下文信息来增强模型的表达能力和训练数据。具体来说，我们引入了序列注意力模块和距离权重相似损失函数，以便聚合上下文信息和 argumentative 信息。此外，我们还随机屏蔽了文档中的讨论标识符和句子，以降低模型对特定单词或 menos informative 句子的依赖。</li>
<li>results: 我们在五个不同领域的五个数据集上进行了实验，并确认了我们的模型在这些数据集上的状态知识表现。此外，我们还进行了减少模块的研究，以证明每个模块在我们的模型中的效果。<details>
<summary>Abstract</summary>
Argument structure extraction (ASE) aims to identify the discourse structure of arguments within documents. Previous research has demonstrated that contextual information is crucial for developing an effective ASE model. However, we observe that merely concatenating sentences in a contextual window does not fully utilize contextual information and can sometimes lead to excessive attention on less informative sentences. To tackle this challenge, we propose an Efficient Context-aware ASE model (ECASE) that fully exploits contextual information by enhancing modeling capacity and augmenting training data. Specifically, we introduce a sequence-attention module and distance-weighted similarity loss to aggregate contextual information and argumentative information. Additionally, we augment the training data by randomly masking discourse markers and sentences, which reduces the model's reliance on specific words or less informative sentences. Our experiments on five datasets from various domains demonstrate that our model achieves state-of-the-art performance. Furthermore, ablation studies confirm the effectiveness of each module in our model.
</details>
<details>
<summary>摘要</summary>
Argument structure extraction (ASE) targets to identify the discourse structure of arguments within documents. Previous research has shown that contextual information is crucial for developing an effective ASE model. However, we find that simply concatenating sentences in a contextual window does not fully utilize contextual information and can sometimes lead to excessive attention on less informative sentences. To address this challenge, we propose an Efficient Context-aware ASE model (ECASE) that fully exploits contextual information by enhancing modeling capacity and augmenting training data. Specifically, we introduce a sequence-attention module and distance-weighted similarity loss to aggregate contextual information and argumentative information. Additionally, we augment the training data by randomly masking discourse markers and sentences, which reduces the model's reliance on specific words or less informative sentences. Our experiments on five datasets from various domains demonstrate that our model achieves state-of-the-art performance. Furthermore, ablation studies confirm the effectiveness of each module in our model.Here's the word-for-word translation:Argument structure extraction (ASE) targets to identify the discourse structure of arguments within documents. Previous research has shown that contextual information is crucial for developing an effective ASE model. However, we find that simply concatenating sentences in a contextual window does not fully utilize contextual information and can sometimes lead to excessive attention on less informative sentences. To address this challenge, we propose an Efficient Context-aware ASE model (ECASE) that fully exploits contextual information by enhancing modeling capacity and augmenting training data. Specifically, we introduce a sequence-attention module and distance-weighted similarity loss to aggregate contextual information and argumentative information. Additionally, we augment the training data by randomly masking discourse markers and sentences, which reduces the model's reliance on specific words or less informative sentences. Our experiments on five datasets from various domains demonstrate that our model achieves state-of-the-art performance. Furthermore, ablation studies confirm the effectiveness of each module in our model.
</details></li>
</ul>
<hr>
<h2 id="Unleashing-the-Multilingual-Encoder-Potential-Boosting-Zero-Shot-Performance-via-Probability-Calibration"><a href="#Unleashing-the-Multilingual-Encoder-Potential-Boosting-Zero-Shot-Performance-via-Probability-Calibration" class="headerlink" title="Unleashing the Multilingual Encoder Potential: Boosting Zero-Shot Performance via Probability Calibration"></a>Unleashing the Multilingual Encoder Potential: Boosting Zero-Shot Performance via Probability Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05069">http://arxiv.org/abs/2310.05069</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ercong21/calibration">https://github.com/ercong21/calibration</a></li>
<li>paper_authors: Ercong Nie, Helmut Schmid, Hinrich Schütze</li>
<li>for: 这个论文主要针对Zero-shot和少量示例情景下的多语言任务和语言探测问题。</li>
<li>methods: 这个论文使用预训练多语言encoder模型，通过重写输入示例为cloze风格的问题，直接完成多语言任务或语言探测。这种方法不需要更新模型参数。但是，模型偏好预测频繁出现的标签词，导致性能有限制。为了解决这个问题，这个论文提出了一种简单的准确化方法，并与其他现有技术进行比较。</li>
<li>results: 这个论文使用准确化技术与预训练多语言encoder模型结合，在多种任务中实现了显著性能提升。<details>
<summary>Abstract</summary>
Pretrained multilingual encoder models can directly perform zero-shot multilingual tasks or linguistic probing by reformulating the input examples into cloze-style prompts. This is accomplished by predicting the probabilities of the label words at the masked token position, without requiring any updates to the model parameters. However, the performance of this method is limited by the model's bias toward predicting label words which frequently occurred during the pretraining. These words typically receive high probabilities. To address this issue, we combine the models with calibration techniques which modify the probabilities of label words predicted by the models. We first validate the effectiveness of a proposed simple calibration method together with other existing techniques on monolingual encoders in both zero- and few-shot scenarios. We subsequently employ these calibration techniques on multilingual encoders, resulting in substantial performance improvements across a wide range of tasks.
</details>
<details>
<summary>摘要</summary>
预训练多语言encoder模型可以直接执行零shot多语言任务或语言探测，通过重写输入示例为cloze样式提示。这是通过预测掩码Token位置的标签词概率，不需要更新模型参数。然而，这种方法的性能受到模型对预测常见的标签词的偏好的限制。这些词通常会 Receive高概率预测。为解决这个问题，我们将模型与加拟定技术相结合， modify模型预测标签词的概率。我们首先验证提议的简单加拟定方法，以及其他现有的技术在单语言encoder上的效果。然后，我们在多语言encoder上使用这些加拟定技术， resulting in 广泛任务中的性能提升。
</details></li>
</ul>
<hr>
<h2 id="Guideline-Learning-for-In-context-Information-Extraction"><a href="#Guideline-Learning-for-In-context-Information-Extraction" class="headerlink" title="Guideline Learning for In-context Information Extraction"></a>Guideline Learning for In-context Information Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05066">http://arxiv.org/abs/2310.05066</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chaoxu Pang, Yixuan Cao, Qiang Ding, Ping Luo</li>
<li>for: 提高嵌入式学习（ICL）中的信息提取性能（IE）。</li>
<li>methods: 提出指南学习（GL）框架，在学习阶段自动生成指南，在推断阶段根据错误案例选择有助于ICL的指南。同时，提出基于自我一致性的活动学习方法，提高GL的效率。</li>
<li>results: 在事件提取和关系提取任务上，GL可以显著提高嵌入式IE的性能。<details>
<summary>Abstract</summary>
Large language models (LLMs) can perform a new task by merely conditioning on task instructions and a few input-output examples, without optimizing any parameters. This is called In-Context Learning (ICL). In-context Information Extraction (IE) has recently garnered attention in the research community. However, the performance of In-context IE generally lags behind the state-of-the-art supervised expert models. We highlight a key reason for this shortfall: underspecified task description. The limited-length context struggles to thoroughly express the intricate IE task instructions and various edge cases, leading to misalignment in task comprehension with humans. In this paper, we propose a Guideline Learning (GL) framework for In-context IE which reflectively learns and follows guidelines. During the learning phrase, GL automatically synthesizes a set of guidelines based on a few error cases, and during inference, GL retrieves helpful guidelines for better ICL. Moreover, we propose a self-consistency-based active learning method to enhance the efficiency of GL. Experiments on event extraction and relation extraction show that GL can significantly improve the performance of in-context IE.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="sign-mt-Real-Time-Multilingual-Sign-Language-Translation-Application"><a href="#sign-mt-Real-Time-Multilingual-Sign-Language-Translation-Application" class="headerlink" title="sign.mt: Real-Time Multilingual Sign Language Translation Application"></a>sign.mt: Real-Time Multilingual Sign Language Translation Application</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05064">http://arxiv.org/abs/2310.05064</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amit Moryossef</li>
<li>for: 这个研究旨在为听语和手语之间的交流问题提供解决方案，实现语言通信的协调。</li>
<li>methods: 这个开源应用程序使用了现代的开源模型，包括对话语言模型和手语识别模型，以提供即时多语言对话的转换。</li>
<li>results: 这个应用程序可以实现即时多语言对话的转换，并且提供了自定义的真实人工手语演示，以激发用户参与和满意度。<details>
<summary>Abstract</summary>
This demo paper presents sign.mt, an open-source application pioneering real-time multilingual bi-directional translation between spoken and signed languages. Harnessing state-of-the-art open-source models, this tool aims to address the communication divide between the hearing and the deaf, facilitating seamless translation in both spoken-to-signed and signed-to-spoken translation directions.   Promising reliable and unrestricted communication, sign.mt offers offline functionality, crucial in areas with limited internet connectivity. It further enhances user engagement by offering customizable photo-realistic sign language avatars, thereby encouraging a more personalized and authentic user experience.   Licensed under CC BY-NC-SA 4.0, sign.mt signifies an important stride towards open, inclusive communication. The app can be used, and modified for personal and academic uses, and even supports a translation API, fostering integration into a wider range of applications. However, it is by no means a finished product.   We invite the NLP community to contribute towards the evolution of sign.mt. Whether it be the integration of more refined models, the development of innovative pipelines, or user experience improvements, your contributions can propel this project to new heights. Available at https://sign.mt, it stands as a testament to what we can achieve together, as we strive to make communication accessible to all.
</details>
<details>
<summary>摘要</summary>
这个示例文章介绍了一个开源应用程序，即sign.mt，它实现了实时多语言对话转化，包括口头语言和手语两种语言之间的对话转化。使用现有的开源模型，这工具计划解决听力和耳语之间的沟通差异，为听力和耳语之间的对话提供流畅的翻译。  sign.mt 提供了可靠和无限制的沟通，并且在网络连接性较差的地区具有离线功能。它还提高了用户参与度，通过提供可定制的真实手语人物，使用户感受到更个性化和原始的用户体验。  根据 CC BY-NC-SA 4.0 许可证，sign.mt 表示开放、包容的沟通的重要一步。这个应用程序可以用于个人和学术用途，甚至支持翻译 API，以便更广泛地应用。尽管不是一款完整的产品，但我们邀请 NLP 社区参与 sign.mt 的演进。你的贡献可以使这个项目走向更高的峰点，包括更加精准的模型集成、创新的管道开发和用户体验改进等。可以在 <https://sign.mt> 上获取更多信息。
</details></li>
</ul>
<hr>
<h2 id="BRAINTEASER-Lateral-Thinking-Puzzles-for-Large-Language-Models"><a href="#BRAINTEASER-Lateral-Thinking-Puzzles-for-Large-Language-Models" class="headerlink" title="BRAINTEASER: Lateral Thinking Puzzles for Large Language Models"></a>BRAINTEASER: Lateral Thinking Puzzles for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05057">http://arxiv.org/abs/2310.05057</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifan Jiang, Filip Ilievski, Kaixin Ma, Zhivar Sourati</li>
<li>for: 该论文旨在检验语义理解模型是否具备倾向性思维能力，以及模型是否能够扭转默认知的关系。</li>
<li>methods: 该论文使用了多选问答任务，以检验模型的倾向性思维能力。其中，模型需要从多个选项中选择正确答案，而不是直接回答问题。</li>
<li>results: 研究发现，当前的语义理解模型在倾向性思维任务中表现不佳，与人类表现的 gap 较大。此外，模型在不同的倾向性思维任务中的表现也异常。<details>
<summary>Abstract</summary>
The success of language models has inspired the NLP community to attend to tasks that require implicit and complex reasoning, relying on human-like commonsense mechanisms. While such vertical thinking tasks have been relatively popular, lateral thinking puzzles have received little attention. To bridge this gap, we devise BRAINTEASER: a multiple-choice Question Answering task designed to test the model's ability to exhibit lateral thinking and defy default commonsense associations. We design a three-step procedure for creating the first lateral thinking benchmark, consisting of data collection, distractor generation, and generation of adversarial examples, leading to 1,100 puzzles with high-quality annotations. To assess the consistency of lateral reasoning by models, we enrich BRAINTEASER based on a semantic and contextual reconstruction of its questions. Our experiments with state-of-the-art instruction- and commonsense language models reveal a significant gap between human and model performance, which is further widened when consistency across adversarial formats is considered. We make all of our code and data available to stimulate work on developing and evaluating lateral thinking models.
</details>
<details>
<summary>摘要</summary>
成功的语言模型使得自然语言处理（NLP）社区受到了关注，把注意力转移到需要间接和复杂的理解的任务上。虽然垂直思维任务在某种程度上受到了普遍的关注，但是水平思维拼图得到了少量的关注。为了填补这个差距，我们设计了Brainteaser：一种多选问答任务，旨在测试模型的水平思维能力和脱离默认的共同理解。我们采用了三步过程来创建第一个水平思维标准 benchmark：数据收集、distractor生成和对抗示例生成，共计1,100个高质量注释的拼图。为了评估模型的水平思维一致性，我们对Brainteaser的问题进行了semantic和contextual重建。我们的实验表明，当模型面临水平思维任务时，与人类的表现存在显著的差距，此差距甚至在对抗格式的一致性上受到了进一步的扩大。我们将所有的代码和数据公开，以便激励开发和评估水平思维模型的工作。
</details></li>
</ul>
<hr>
<h2 id="Harnessing-the-Power-of-ChatGPT-in-Fake-News-An-In-Depth-Exploration-in-Generation-Detection-and-Explanation"><a href="#Harnessing-the-Power-of-ChatGPT-in-Fake-News-An-In-Depth-Exploration-in-Generation-Detection-and-Explanation" class="headerlink" title="Harnessing the Power of ChatGPT in Fake News: An In-Depth Exploration in Generation, Detection and Explanation"></a>Harnessing the Power of ChatGPT in Fake News: An In-Depth Exploration in Generation, Detection and Explanation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05046">http://arxiv.org/abs/2310.05046</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Huang, Lichao Sun</li>
<li>For: The paper aims to explore ChatGPT’s proficiency in generating, explaining, and detecting fake news.* Methods: The paper employs four prompt methods to generate fake news samples and obtains nine features to characterize fake news based on ChatGPT’s explanations. It also examines ChatGPT’s capacity to identify fake news and proposes a reason-aware prompt method to improve its performance.* Results: The paper demonstrates that ChatGPT shows commendable performance in detecting fake news, but there is still room for improvement. It also explores the potential extra information that could bolster its effectiveness in detecting fake news.Here are the three key points in Simplified Chinese text:* For: 本研究旨在探讨ChatGPT在生成、解释和检测假新闻方面的能力。* Methods: 本研究使用四种提示方法生成假新闻样本，并通过自我评估和人类评估来证明这些样本的质量。同时，我们从ChatGPT的解释中获取了九个特征来Characterize假新闻，并分析这些特征在多个公共数据集中的分布。* Results: 我们的实验表明，ChatGPT在检测假新闻方面表现了可嘉的表现，但仍有改进的空间。我们还探讨了可能会增强其检测假新闻效果的额外信息。<details>
<summary>Abstract</summary>
The rampant spread of fake news has adversely affected society, resulting in extensive research on curbing its spread. As a notable milestone in large language models (LLMs), ChatGPT has gained significant attention due to its exceptional natural language processing capabilities. In this study, we present a thorough exploration of ChatGPT's proficiency in generating, explaining, and detecting fake news as follows. Generation -- We employ four prompt methods to generate fake news samples and prove the high quality of these samples through both self-assessment and human evaluation. Explanation -- We obtain nine features to characterize fake news based on ChatGPT's explanations and analyze the distribution of these factors across multiple public datasets. Detection -- We examine ChatGPT's capacity to identify fake news. We explore its detection consistency and then propose a reason-aware prompt method to improve its performance. Although our experiments demonstrate that ChatGPT shows commendable performance in detecting fake news, there is still room for its improvement. Consequently, we further probe into the potential extra information that could bolster its effectiveness in detecting fake news.
</details>
<details>
<summary>摘要</summary>
《假新闻的普遍传播对社会造成了不良影响，导致了各方对其散布的研究。作为大型自然语言模型（LLM）的一项重要里程碑，ChatGPT在自然语言处理方面表现出了突出的能力。本研究中，我们对ChatGPT的能力进行了全面探索，具体来说是：生成、解释和检测假新闻。生成——我们使用四种提示方法生成假新闻样本，并通过自我评估和人类评估来证明这些样本的质量。解释——我们从ChatGPT的解释中提取了九个特征来 caracterize假新闻，并分析这些特征在多个公共数据集中的分布。检测——我们检查ChatGPT是否能够识别假新闻。我们首先检查其检测的一致性，然后提出了基于理由的提示方法来提高其性能。虽然我们的实验表明ChatGPT在检测假新闻方面表现出了良好的表现，但还有一些可以提高其效果的空间。因此，我们进一步探索可能会增强其检测假新闻的效iveness的额外信息。
</details></li>
</ul>
<hr>
<h2 id="Walking-Down-the-Memory-Maze-Beyond-Context-Limit-through-Interactive-Reading"><a href="#Walking-Down-the-Memory-Maze-Beyond-Context-Limit-through-Interactive-Reading" class="headerlink" title="Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading"></a>Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05029">http://arxiv.org/abs/2310.05029</a></li>
<li>repo_url: None</li>
<li>paper_authors: Howard Chen, Ramakanth Pasunuru, Jason Weston, Asli Celikyilmaz</li>
<li>for: 这篇论文的目的是提出一种新的长文理解方法，以解决现有的自注意机制受限的问题。</li>
<li>methods: 该方法基于论文自动浏览器，首先将长文处理成摘要节点树，然后根据查询提交，通过 iterative prompting 方式，论文模型在树上寻找相关信息，并在获得足够信息后提供答案。</li>
<li>results: 与基eline方法相比，该方法在长文问答任务上表现出色，并且可以增强解释性，通过在浏览过程中高亮相关的文本段落。<details>
<summary>Abstract</summary>
Large language models (LLMs) have advanced in large strides due to the effectiveness of the self-attention mechanism that processes and compares all tokens at once. However, this mechanism comes with a fundamental issue -- the predetermined context window is bound to be limited. Despite attempts to extend the context window through methods like extrapolating the positional embedding, using recurrence, or selectively retrieving essential parts of the long sequence, long-text understanding continues to be a challenge. We propose an alternative approach which instead treats the LLM as an interactive agent, allowing it to decide how to read the text via iterative prompting. We introduce MemWalker, a method that first processes the long context into a tree of summary nodes. Upon receiving a query, the model navigates this tree in search of relevant information, and responds once it gathers sufficient information. On long-text question answering tasks our method outperforms baseline approaches that use long context windows, recurrence, and retrieval. We show that, beyond effective reading, MemWalker enhances explainability by highlighting the reasoning steps as it interactively reads the text; pinpointing the relevant text segments related to the query.
</details>
<details>
<summary>摘要</summary>
We propose an alternative approach that treats the LLM as an interactive agent, allowing it to decide how to read the text through iterative prompting. We introduce MemWalker, a method that first processes the long context into a tree of summary nodes. When receiving a query, the model navigates this tree to search for relevant information and responds once it has gathered sufficient information.On long-text question answering tasks, our method outperforms baseline approaches that use long context windows, recurrence, and retrieval. Additionally, MemWalker enhances explainability by highlighting the reasoning steps as it interactively reads the text, pinpointing the relevant text segments related to the query.
</details></li>
</ul>
<hr>
<h2 id="Synslator-An-Interactive-Machine-Translation-Tool-with-Online-Learning"><a href="#Synslator-An-Interactive-Machine-Translation-Tool-with-Online-Learning" class="headerlink" title="Synslator: An Interactive Machine Translation Tool with Online Learning"></a>Synslator: An Interactive Machine Translation Tool with Online Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05025">http://arxiv.org/abs/2310.05025</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiayi Wang, Ke Wang, Fengming Zhou, Chengyu Wang, Zhiyong Fu, Zeyu Feng, Yu Zhao, Yuqi Zhang</li>
<li>for: 这篇论文旨在描述一种名为Synslator的计算机助记翻译工具，该工具不仅支持互动翻译（IMT），而且可以在线学习并使用实时翻译记忆。</li>
<li>methods: 该工具使用两种不同的神经翻译模型来处理翻译记忆，以适应不同的部署环境。此外，系统还使用语言模型来提高互动模式下的翻译流畅性。</li>
<li>results: 我们经过评估，确认了在线学习过程中的翻译模型的有效性，并发现使用Synslator的互动功能可以提高翻译效率13%。更多细节可以参考：<a target="_blank" rel="noopener" href="https://youtu.be/K0vRsb2lTt8%E3%80%82">https://youtu.be/K0vRsb2lTt8。</a><details>
<summary>Abstract</summary>
Interactive machine translation (IMT) has emerged as a progression of the computer-aided translation paradigm, where the machine translation system and the human translator collaborate to produce high-quality translations. This paper introduces Synslator, a user-friendly computer-aided translation (CAT) tool that not only supports IMT, but is adept at online learning with real-time translation memories. To accommodate various deployment environments for CAT services, Synslator integrates two different neural translation models to handle translation memories for online learning. Additionally, the system employs a language model to enhance the fluency of translations in an interactive mode. In evaluation, we have confirmed the effectiveness of online learning through the translation models, and have observed a 13% increase in post-editing efficiency with the interactive functionalities of Synslator. A tutorial video is available at:https://youtu.be/K0vRsb2lTt8.
</details>
<details>
<summary>摘要</summary>
协助式机器翻译（IMT）已经成为计算机辅助翻译模式的进化，在这种模式下，机器翻译系统和人类翻译员共同努力以生成高质量翻译。这篇文章介绍了Synslator，一款用户友好的计算机辅助翻译（CAT）工具，不仅支持IMT，而且在线学习 WITH 实时翻译记忆。为满足不同的CAT服务部署环境，Synslator integrate了两种不同的神经翻译模型来处理翻译记忆。此外，系统还使用语言模型来提高交互模式下的翻译流畅性。经评估，我们已经确认了在线学习通过翻译模型的效iveness，并观察到了Synslator的交互功能可以提高翻译效率13%。有关教程视频，请参考：https://youtu.be/K0vRsb2lTt8。
</details></li>
</ul>
<hr>
<h2 id="Hybrid-Quantum-Classical-Machine-Learning-for-Sentiment-Analysis"><a href="#Hybrid-Quantum-Classical-Machine-Learning-for-Sentiment-Analysis" class="headerlink" title="Hybrid Quantum-Classical Machine Learning for Sentiment Analysis"></a>Hybrid Quantum-Classical Machine Learning for Sentiment Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10672">http://arxiv.org/abs/2310.10672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abu Kaisar Mohammad Masum, Anshul Maurya, Dhruthi Sridhar Murthy, Pratibha, Naveed Mahmud</li>
<li>for: 本研究旨在探讨量子计算和经典机器学习的合作在自然语言处理中的可能性，尤其是对大规模数据集中表达的人类情感和意见的情感分析。</li>
<li>methods: 本研究提出了一种混合量子-经典机器学习算法的方法ología，包括量子kernel方法和量子径波变换-基于的分类器，并与经典维度减少技术 such as PCA和Haar wavelet transform进行了集成。</li>
<li>results: 实验结果表明，在减少数据维度后，量子基于的混合算法的性能是稳定和更好于经典方法。<details>
<summary>Abstract</summary>
The collaboration between quantum computing and classical machine learning offers potential advantages in natural language processing, particularly in the sentiment analysis of human emotions and opinions expressed in large-scale datasets. In this work, we propose a methodology for sentiment analysis using hybrid quantum-classical machine learning algorithms. We investigate quantum kernel approaches and variational quantum circuit-based classifiers and integrate them with classical dimension reduction techniques such as PCA and Haar wavelet transform. The proposed methodology is evaluated using two distinct datasets, based on English and Bengali languages. Experimental results show that after dimensionality reduction of the data, performance of the quantum-based hybrid algorithms were consistent and better than classical methods.
</details>
<details>
<summary>摘要</summary>
合作 между量子计算和类别机器学习可以在自然语言处理中提供potential的优势，特别是在大规模数据集中检测人们的情感和意见。在这个工作中，我们提议了一种基于量子-类别机器学习算法的情感分析方法。我们研究了量子kernel方法和量子征值回归-基于分类器，并将其与经典维度减少技术相结合，如PCA和Haar波lets变换。我们对两个不同的数据集进行了实验，一个是英语数据集，另一个是孟加拉语数据集。实验结果表明，在减少数据维度后，量子-基于 hybrid 算法的性能是一致的和更好于经典方法。
</details></li>
</ul>
<hr>
<h2 id="WikiIns-A-High-Quality-Dataset-for-Controlled-Text-Editing-by-Natural-Language-Instruction"><a href="#WikiIns-A-High-Quality-Dataset-for-Controlled-Text-Editing-by-Natural-Language-Instruction" class="headerlink" title="WikiIns: A High-Quality Dataset for Controlled Text Editing by Natural Language Instruction"></a>WikiIns: A High-Quality Dataset for Controlled Text Editing by Natural Language Instruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05009">http://arxiv.org/abs/2310.05009</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/casparswift/wikiins">https://github.com/casparswift/wikiins</a></li>
<li>paper_authors: Xiang Chen, Zheng Li, Xiaojun Wan</li>
<li>for: 本研究targets the problem of controlled text editing by natural language instruction.</li>
<li>methods: 研究者使用了Wikipedia编辑历史数据库，通过批处理和人工纠正来提高数据集的质量，并提出了自动生成大规模“银”训练集的方法。</li>
<li>results: 研究者通过对WikiIns dataset进行分析和实验，得到了一些有价值的结论和编辑INTENTION分析结果。<details>
<summary>Abstract</summary>
Text editing, i.e., the process of modifying or manipulating text, is a crucial step in human writing process. In this paper, we study the problem of controlled text editing by natural language instruction. According to a given instruction that conveys the edit intention and necessary information, an original draft text is required to be revised into a target text. Existing automatically constructed datasets for this task are limited because they do not have informative natural language instruction. The informativeness requires the information contained in the instruction to be enough to produce the revised text. To address this limitation, we build and release WikiIns, a high-quality controlled text editing dataset with improved informativeness. We first preprocess the Wikipedia edit history database to extract the raw data (WikiIns-Raw). Then we crowdsource high-quality validation and test sets, as well as a small-scale training set (WikiIns-Gold). With the high-quality annotated dataset, we further propose automatic approaches to generate a large-scale ``silver'' training set (WikiIns-Silver). Finally, we provide some insightful analysis on our WikiIns dataset, including the evaluation results and the edit intention analysis. Our analysis and the experiment results on WikiIns may assist the ongoing research on text editing. The dataset, source code and annotation guideline are available at https://github.com/CasparSwift/WikiIns.
</details>
<details>
<summary>摘要</summary>
文本编辑，即对文本进行修改或 manipulate 的过程，是人类写作过程中的关键步骤。在这篇论文中，我们研究了基于自然语言指令的控制文本编辑问题。根据一个拥有修改意图和必要信息的自然语言指令，需要将原始稿件文本修改为目标文本。现有的自动生成的这类数据集有限，因为它们没有具有信息的自然语言指令。为了解决这个限制，我们建立了和发布了高质量的控制文本编辑数据集 WikiIns，其中包括改进的信息含量。我们首先从 Wikipedia 编辑历史数据库中提取原始数据（WikiIns-Raw），然后通过人工审核和测试集，以及一小规模的训练集（WikiIns-Gold）来生成高质量验证集。然后，我们提出了一些自动生成大规模“银”训练集（WikiIns-Silver）的方法。最后，我们提供了一些有价值的分析和实验结果，包括我们的 WikiIns 数据集的评价结果和修改意图分析。我们的分析和实验结果可能会帮助当前的文本编辑研究。我们的数据集、源代码和注释指南可以在 GitHub 上找到：https://github.com/CasparSwift/WikiIns。
</details></li>
</ul>
<hr>
<h2 id="MinPrompt-Graph-based-Minimal-Prompt-Data-Augmentation-for-Few-shot-Question-Answering"><a href="#MinPrompt-Graph-based-Minimal-Prompt-Data-Augmentation-for-Few-shot-Question-Answering" class="headerlink" title="MinPrompt: Graph-based Minimal Prompt Data Augmentation for Few-shot Question Answering"></a>MinPrompt: Graph-based Minimal Prompt Data Augmentation for Few-shot Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05007">http://arxiv.org/abs/2310.05007</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiusi Chen, Jyun-Yu Jiang, Wei-Cheng Chang, Cho-Jui Hsieh, Hsiang-Fu Yu, Wei Wang</li>
<li>for: 提高机器问答系统的满意度，使其在几个训练样本不足的情况下达到良好的结果。</li>
<li>methods: 提出了一种基于approximate graph算法和无监督问题生成的最小数据扩充框架，可以有效地提高open-domain QA任务中的精度。</li>
<li>results: 经验result表明，MinPrompt能够与基eline相比或者更好地实现精度，在不同的benchmark datasets上提高F-1分数的提升达27.5%。<details>
<summary>Abstract</summary>
Few-shot question answering (QA) aims at achieving satisfactory results on machine question answering when only a few training samples are available. Recent advances mostly rely on the power of pre-trained large language models (LLMs) and fine-tuning in specific settings. Although the pre-training stage has already equipped LLMs with powerful reasoning capabilities, LLMs still need to be fine-tuned to adapt to specific domains to achieve the best results. In this paper, we propose to select the most informative data for fine-tuning, thereby improving the efficiency of the fine-tuning process with comparative or even better accuracy on the open-domain QA task. We present MinPrompt, a minimal data augmentation framework for open-domain QA based on an approximate graph algorithm and unsupervised question generation. We transform the raw text into a graph structure to build connections between different factual sentences, then apply graph algorithms to identify the minimal set of sentences needed to cover the most information in the raw text. We then generate QA pairs based on the identified sentence subset and train the model on the selected sentences to obtain the final model. Empirical results on several benchmark datasets and theoretical analysis show that MinPrompt is able to achieve comparable or better results than baselines with a high degree of efficiency, bringing improvements in F-1 scores by up to 27.5%.
</details>
<details>
<summary>摘要</summary>
几个示例问答（QA）目标在机器问答中实现满意的结果，只需要几个训练样本。现代进步主要依靠大型自然语言模型（LLM）的力量和特定设置的精细调整。虽然预训练阶段已经把LLM们具备了强大的推理能力，但LLM们仍需要调整以适应特定领域以达到最佳结果。在这篇论文中，我们提议选择最有用的数据进行调整，从而提高调整过程的效率，同时保持比较或更好的准确率在开放领域QA任务中。我们提出了一个名为MinPrompt的最小数据扩展框架，基于approximate graph算法和无监督问题生成。我们将原始文本转换成图结构，建立不同事实句子之间的连接，然后应用图算法选择最小的句子集，以覆盖raw文本中的最多信息。我们然后根据选择的句子集生成QA对，并在选择的句子上训练模型，从而获得最终模型。实验结果表明，MinPrompt可以与基准相比或更好的达到准确率，提高F-1分数的提升达27.5%。
</details></li>
</ul>
<hr>
<h2 id="Self-Knowledge-Guided-Retrieval-Augmentation-for-Large-Language-Models"><a href="#Self-Knowledge-Guided-Retrieval-Augmentation-for-Large-Language-Models" class="headerlink" title="Self-Knowledge Guided Retrieval Augmentation for Large Language Models"></a>Self-Knowledge Guided Retrieval Augmentation for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05002">http://arxiv.org/abs/2310.05002</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/THUNLP-MT/SKR">https://github.com/THUNLP-MT/SKR</a></li>
<li>paper_authors: Yile Wang, Peng Li, Maosong Sun, Yang Liu</li>
<li>for: 提高大语言模型（LLM）的性能，不需要任务特定的精度调整。</li>
<li>methods: 使用自我认知指导的检索增强（SKR）方法，让 LLM 能够识别自己所知道和所不知道，并适应新问题。</li>
<li>results: SKR 在多个数据集上表现出色，比 chain-of-thought 和完整检索基本方法高效，使用 InstructGPT 或 ChatGPT 进行评估。<details>
<summary>Abstract</summary>
Large language models (LLMs) have shown superior performance without task-specific fine-tuning. Despite the success, the knowledge stored in the parameters of LLMs could still be incomplete and difficult to update due to the computational costs. As complementary, retrieval-based methods can offer non-parametric world knowledge and improve the performance on tasks such as question answering. However, we find that the retrieved knowledge does not always help and even has a negative impact on original responses occasionally. To better make use of both internal knowledge and external world knowledge, we investigate eliciting the model's ability to recognize what they know and do not know (which is also called self-knowledge) and propose Self-Knowledge guided Retrieval augmentation (SKR), a simple yet effective method which can let LLMs refer to the questions they have previously encountered and adaptively call for external resources when dealing with new questions. We evaluate SKR on multiple datasets and demonstrate that it outperforms chain-of-thought based and fully retrieval-based methods by using either InstructGPT or ChatGPT.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="TopicAdapt-An-Inter-Corpora-Topics-Adaptation-Approach"><a href="#TopicAdapt-An-Inter-Corpora-Topics-Adaptation-Approach" class="headerlink" title="TopicAdapt- An Inter-Corpora Topics Adaptation Approach"></a>TopicAdapt- An Inter-Corpora Topics Adaptation Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04978">http://arxiv.org/abs/2310.04978</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pritom Saha Akash, Trisha Das, Kevin Chen-Chuan Chang</li>
<li>for: 本研究提出了一种基于神经网络的话题模型，用于改进话题模型在实际场景中的表现。</li>
<li>methods: 本研究使用了一种基于神经网络的话题模型，可以从相关的源корpus中挖掘有用的话题，同时还可以在目标корpus中找到缺失的话题。</li>
<li>results: 实验结果表明，提出的话题模型在多个不同领域的数据集上具有较高的表现，比对state-of-the-art话题模型更好。<details>
<summary>Abstract</summary>
Topic models are popular statistical tools for detecting latent semantic topics in a text corpus. They have been utilized in various applications across different fields. However, traditional topic models have some limitations, including insensitivity to user guidance, sensitivity to the amount and quality of data, and the inability to adapt learned topics from one corpus to another. To address these challenges, this paper proposes a neural topic model, TopicAdapt, that can adapt relevant topics from a related source corpus and also discover new topics in a target corpus that are absent in the source corpus. The proposed model offers a promising approach to improve topic modeling performance in practical scenarios. Experiments over multiple datasets from diverse domains show the superiority of the proposed model against the state-of-the-art topic models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Exploring-the-Usage-of-Chinese-Pinyin-in-Pretraining"><a href="#Exploring-the-Usage-of-Chinese-Pinyin-in-Pretraining" class="headerlink" title="Exploring the Usage of Chinese Pinyin in Pretraining"></a>Exploring the Usage of Chinese Pinyin in Pretraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04960">http://arxiv.org/abs/2310.04960</a></li>
<li>repo_url: None</li>
<li>paper_authors: Baojun Wang, Kun Xu, Lifeng Shang</li>
<li>for: 这篇论文主要是为了提高中文语音识别错误稳定性。</li>
<li>methods: 这篇论文使用了多种预训练方法，包括使用字符和拼音并行预训练，以增强错误识别的稳定性。</li>
<li>results: 实验结果表明，这种新预训练方法可以提高中文语音识别模型的稳定性，并且在公共错误纠正数据集上达到了最高的表现。<details>
<summary>Abstract</summary>
Unlike alphabetic languages, Chinese spelling and pronunciation are different. Both characters and pinyin take an important role in Chinese language understanding. In Chinese NLP tasks, we almost adopt characters or words as model input, and few works study how to use pinyin. However, pinyin is essential in many scenarios, such as error correction and fault tolerance for ASR-introduced errors. Most of these errors are caused by the same or similar pronunciation words, and we refer to this type of error as SSP(the same or similar pronunciation) errors for short. In this work, we explore various ways of using pinyin in pretraining models and propose a new pretraining method called PmBERT. Our method uses characters and pinyin in parallel for pretraining. Through delicate pretraining tasks, the characters and pinyin representation are fused, which can enhance the error tolerance for SSP errors. We do comprehensive experiments and ablation tests to explore what makes a robust phonetic enhanced Chinese language model. The experimental results on both the constructed noise-added dataset and the public error-correction dataset demonstrate that our model is more robust compared to SOTA models.
</details>
<details>
<summary>摘要</summary>
不同的字母语言和中文拼写、发音之间存在差异。中文NLU任务中，大多数作品是直接使用字符或词作为模型输入，而忽略了拼音。然而，拼音在许多场景中具有重要性，如错误纠正和ASR引入错误的稳定性。大多数这些错误是由同或相似的发音单词引起的，我们称这种错误为SSP（同或相似的发音）错误。在这种工作中，我们探索了使用拼音的不同方法，并提出了一种新的预训练方法called PmBERT。我们的方法在平行预训练中使用字符和拼音，通过细腻的预训练任务，字符和拼音表示被融合，从而提高了SSP错误的承受能力。我们进行了广泛的实验和割除测试，以探索使一个强大的中文语言模型具有哪些特点。实验结果表明，我们的模型在constructed noise-added dataset和公共错误纠正dataset上比SOTA模型更加稳定。
</details></li>
</ul>
<hr>
<h2 id="Towards-Better-Chain-of-Thought-Prompting-Strategies-A-Survey"><a href="#Towards-Better-Chain-of-Thought-Prompting-Strategies-A-Survey" class="headerlink" title="Towards Better Chain-of-Thought Prompting Strategies: A Survey"></a>Towards Better Chain-of-Thought Prompting Strategies: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04959">http://arxiv.org/abs/2310.04959</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihan Yu, Liang He, Zhen Wu, Xinyu Dai, Jiajun Chen</li>
<li>for: 本文旨在探讨Chain-of-Thought（CoT）提示Strategy的效果，并系统地分析其关键因素以及如何更好地应用于不同应用场景。</li>
<li>methods: 本文通过审查广泛的当前研究，提供了系统的和全面的分析，涵盖了CoT提示的各种因素的影响，以及如何更好地应用其在不同应用场景。</li>
<li>results: 本文提出了一些挑战和未来发展方向，以帮助读者更好地理解和应用CoT提示。<details>
<summary>Abstract</summary>
Chain-of-Thought (CoT), a step-wise and coherent reasoning chain, shows its impressive strength when used as a prompting strategy for large language models (LLM). Recent years, the prominent effect of CoT prompting has attracted emerging research. However, there still lacks of a systematic summary about key factors of CoT prompting and comprehensive guide for prompts utilizing. For a deeper understanding about CoT prompting, we survey on a wide range of current research, presenting a systematic and comprehensive analysis on several factors that may influence the effect of CoT prompting, and introduce how to better apply it in different applications under these discussions. We further analyze the challenges and propose some future directions about CoT prompting. This survey could provide an overall reference on related research.
</details>
<details>
<summary>摘要</summary>
Chain-of-Thought（CoT），一种逐步逻辑推理链，在大语言模型（LLM）中作为提示策略显示出了惊人的力量。近年来，CoT提示的明显效果吸引了学术界的关注。然而，当前还缺乏一个系统化的总结和完整的指南，用于解释CoT提示的关键因素和如何更好地应用它们。为了深入了解CoT提示，我们在广泛的当前研究中进行了系统化和完整的分析，并对各种因素的影响进行了分析，以及如何在不同应用中更好地使用它们。我们还分析了挑战和提出了未来的发展方向。这种调查可以为相关研究提供一个总体参考。Note: Please note that the translation is in Simplified Chinese, and some words or phrases may have different translations in Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Domain-Knowledge-Graph-Construction-Via-A-Simple-Checker"><a href="#Domain-Knowledge-Graph-Construction-Via-A-Simple-Checker" class="headerlink" title="Domain Knowledge Graph Construction Via A Simple Checker"></a>Domain Knowledge Graph Construction Via A Simple Checker</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04949">http://arxiv.org/abs/2310.04949</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yueling Zeng, Li-C. Wang</li>
<li>for: 这项研究的目的是为Semiconductor chip设计公司提供一种基于语言模型的知识图构建方法，以满足公司的两个重要考虑因素：保密性和可扩展性。</li>
<li>methods: 本文提出了一种oracle-checker方法，利用GPT3.5的力量来解决知识图构建问题。该方法包括一个验证过程，用于检查域专家的背景知识是否已经满足了构建知识图的需求。</li>
<li>results: 本文使用RISC-V无权ISA规范为例，解释了关键想法和讨论了实践中的 oracle-checker方法的可能性。<details>
<summary>Abstract</summary>
With the availability of large language models, there is a growing interest for semiconductor chip design companies to leverage the technologies. For those companies, deployment of a new methodology must include two important considerations: confidentiality and scalability. In this context, this work tackles the problem of knowledge graph construction from hardware-design domain texts. We propose an oracle-checker scheme to leverage the power of GPT3.5 and demonstrate that the essence of the problem is in distillation of domain expert's background knowledge. Using RISC-V unprivileged ISA specification as an example, we explain key ideas and discuss practicality of our proposed oracle-checker approach.
</details>
<details>
<summary>摘要</summary>
现在大型语言模型成为可用的，半导体封包设计公司开始关注这些技术的应用。为这些公司而办理新方法时，需要考虑两个重要因素：保密和可扩展性。在这个上下文中，本文解决半导体设计领域文本知识图构建的问题。我们提议使用GPT3.5的力量，并证明知识的核心问题在封包专家背景知识的精炼中。使用RISC-V不具有特权ISA规范为例，我们介绍关键想法并讨论我们的 oracle-checker方法的实用性。
</details></li>
</ul>
<hr>
<h2 id="TEMPO-Prompt-based-Generative-Pre-trained-Transformer-for-Time-Series-Forecasting"><a href="#TEMPO-Prompt-based-Generative-Pre-trained-Transformer-for-Time-Series-Forecasting" class="headerlink" title="TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting"></a>TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04948">http://arxiv.org/abs/2310.04948</a></li>
<li>repo_url: None</li>
<li>paper_authors: Defu Cao, Furong Jia, Sercan O Arik, Tomas Pfister, Yixiang Zheng, Wen Ye, Yan Liu</li>
<li>for: 本研究旨在开发一种新的时间序列表示学习框架，以提高时间序列预测的准确性。</li>
<li>methods: 该框架基于两个关键的强制性理念：（一）分解复杂的时间序列任务中的趋势、季度和差异部分的交互作用；以及（二）通过选择性的提示来促进非站点时间序列的分布适应。</li>
<li>results: 对多个时间序列benchmark datasets进行实验，TEMPO模型表现出了与现有方法相比的显著性能提升，不仅在标准的指导学习 Setting中，而且在未经见过数据集和多模式输入的情况下也能够获得出色的表现。这一结果表明TEMPO具有成为基础模型构建框架的潜力。<details>
<summary>Abstract</summary>
The past decade has witnessed significant advances in time series modeling with deep learning. While achieving state-of-the-art results, the best-performing architectures vary highly across applications and domains. Meanwhile, for natural language processing, the Generative Pre-trained Transformer (GPT) has demonstrated impressive performance via training one general-purpose model across various textual datasets. It is intriguing to explore whether GPT-type architectures can be effective for time series, capturing the intrinsic dynamic attributes and leading to significant accuracy improvements. In this paper, we propose a novel framework, TEMPO, that can effectively learn time series representations. We focus on utilizing two essential inductive biases of the time series task for pre-trained models: (i) decomposition of the complex interaction between trend, seasonal and residual components; and (ii) introducing the selection-based prompts to facilitate distribution adaptation in non-stationary time series. TEMPO expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains. Our experiments demonstrate the superior performance of TEMPO over state-of-the-art methods on a number of time series benchmark datasets. This performance gain is observed not only in standard supervised learning settings but also in scenarios involving previously unseen datasets as well as in scenarios with multi-modal inputs. This compelling finding highlights TEMPO's potential to constitute a foundational model-building framework.
</details>
<details>
<summary>摘要</summary>
过去一个 décennie  witnessed significant advances in time series modeling with deep learning. Although the best-performing architectures vary greatly across applications and domains, the Generative Pre-trained Transformer (GPT) has demonstrated impressive performance by training one general-purpose model across various textual datasets. It is intriguing to explore whether GPT-type architectures can be effective for time series, capturing the intrinsic dynamic attributes and leading to significant accuracy improvements.In this paper, we propose a novel framework, TEMPO, that can effectively learn time series representations. We focus on utilizing two essential inductive biases of the time series task for pre-trained models: (i) decomposing the complex interaction between trend, seasonal, and residual components; and (ii) introducing selection-based prompts to facilitate distribution adaptation in non-stationary time series. TEMPO expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains. Our experiments demonstrate the superior performance of TEMPO over state-of-the-art methods on a number of time series benchmark datasets. This performance gain is observed not only in standard supervised learning settings but also in scenarios involving previously unseen datasets as well as in scenarios with multi-modal inputs. This compelling finding highlights TEMPO's potential to constitute a foundational model-building framework.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/08/cs.CL_2023_10_08/" data-id="clp869ttv00d2k5888jvg64f9" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_10_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/08/cs.LG_2023_10_08/" class="article-date">
  <time datetime="2023-10-08T10:00:00.000Z" itemprop="datePublished">2023-10-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/08/cs.LG_2023_10_08/">cs.LG - 2023-10-08</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Adversarial-Attacks-on-Combinatorial-Multi-Armed-Bandits"><a href="#Adversarial-Attacks-on-Combinatorial-Multi-Armed-Bandits" class="headerlink" title="Adversarial Attacks on Combinatorial Multi-Armed Bandits"></a>Adversarial Attacks on Combinatorial Multi-Armed Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05308">http://arxiv.org/abs/2310.05308</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rishab Balasubramanian, Jiawei Li, Prasad Tadepalli, Huazheng Wang, Qingyun Wu, Haoyu Zhao</li>
<li>for: 这 paper 研究了对 Combinatorial Multi-armed Bandits (CMAB) 的奖伪攻击。</li>
<li>methods: 这 paper 提供了一个 suficient 和 necessary condition for the attackability of CMAB, 以及一个攻击算法 для可攻击的 CMAB 实例。</li>
<li>results: 这 paper 发现了一个意外的事实，即攻击 CMAB 实例的可能性还取决于敌方知道或不知道该实例的环境。这意味着在实际应用中，对 CMAB 的攻击非常困难，并且无法找到一个通用的攻击策略。这paper 通过实验 validate 了这些理论发现。<details>
<summary>Abstract</summary>
We study reward poisoning attacks on Combinatorial Multi-armed Bandits (CMAB). We first provide a sufficient and necessary condition for the attackability of CMAB, which depends on the intrinsic properties of the corresponding CMAB instance such as the reward distributions of super arms and outcome distributions of base arms. Additionally, we devise an attack algorithm for attackable CMAB instances. Contrary to prior understanding of multi-armed bandits, our work reveals a surprising fact that the attackability of a specific CMAB instance also depends on whether the bandit instance is known or unknown to the adversary. This finding indicates that adversarial attacks on CMAB are difficult in practice and a general attack strategy for any CMAB instance does not exist since the environment is mostly unknown to the adversary. We validate our theoretical findings via extensive experiments on real-world CMAB applications including probabilistic maximum covering problem, online minimum spanning tree, cascading bandits for online ranking, and online shortest path.
</details>
<details>
<summary>摘要</summary>
我们研究了对 combinatorial multi-armed bandit (CMAB) 的奖励毒攻击。我们首先提供了 CMAB 的攻击可行性必要和 suficient condition，这取决于 CMAB 实例中的奖励分布和结果分布。此外，我们还设计了一种攻击算法 для可攻击 CMAB 实例。与先前对多重抓拍机器人的理解不同，我们发现了一个意外的事实，即 CMAB 实例的攻击可行性还取决于敌方知道或不知道 CMAB 实例的情况。这一发现表明了在实践中对 CMAB 进行攻击是困难的，并且没有一个通用的攻击策略可以应用于任何 CMAB 实例，因为环境多数是不知道的。我们验证了我们的理论发现Result via 广泛的实验，包括probabilistic maximum covering problem、online minimum spanning tree、cascading bandits for online ranking和online shortest path。
</details></li>
</ul>
<hr>
<h2 id="Successive-Data-Injection-in-Conditional-Quantum-GAN-Applied-to-Time-Series-Anomaly-Detection"><a href="#Successive-Data-Injection-in-Conditional-Quantum-GAN-Applied-to-Time-Series-Anomaly-Detection" class="headerlink" title="Successive Data Injection in Conditional Quantum GAN Applied to Time Series Anomaly Detection"></a>Successive Data Injection in Conditional Quantum GAN Applied to Time Series Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05307">http://arxiv.org/abs/2310.05307</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benjamin Kalfon, Soumaya Cherkaoui, Jean-Frédéric Laprade, Ola Ahmad, Shengrui Wang</li>
<li>for: 这个论文主要针对的是如何使用量子生成器网络（QGAN）进行异常检测，尤其是在通信网络中采集的时间序列数据上。</li>
<li>methods: 这篇论文提出了一种新的高维编码方法，named Successive Data Injection（SuDaI），以便在量子状态中扩展更大的数据空间，从而适应更高维的时间序列数据。</li>
<li>results: 该方法可以在高维时间序列数据上进行异常检测，并且可以在其他类型的高维时间序列数据上应用，因此开 up了多个应用领域。<details>
<summary>Abstract</summary>
Classical GAN architectures have shown interesting results for solving anomaly detection problems in general and for time series anomalies in particular, such as those arising in communication networks. In recent years, several quantum GAN architectures have been proposed in the literature. When detecting anomalies in time series using QGANs, huge challenges arise due to the limited number of qubits compared to the size of the data. To address these challenges, we propose a new high-dimensional encoding approach, named Successive Data Injection (SuDaI). In this approach, we explore a larger portion of the quantum state than that in the conventional angle encoding, the method used predominantly in the literature, through repeated data injections into the quantum state. SuDaI encoding allows us to adapt the QGAN for anomaly detection with network data of a much higher dimensionality than with the existing known QGANs implementations. In addition, SuDaI encoding applies to other types of high-dimensional time series and can be used in contexts beyond anomaly detection and QGANs, opening up therefore multiple fields of application.
</details>
<details>
<summary>摘要</summary>
传统的GAN架构在检测异常问题上有诸多有趣的结果，特别是在通信网络中出现的时间序列异常问题。在过去几年，一些量子GAN架构在 литературе中被提出。在使用QGAN检测时间序列异常时，面临着很大的挑战，主要是因为量子状态的限制比数据集大得多。为解决这些挑战，我们提出了一种新的高维码编码方法，名为Successive Data Injection（SuDaI）。在SuDaI编码中，我们可以更好地探索量子状态的更大部分，而不是在文献中主要使用的角度编码方法。这使得我们可以通过重复数据注入到量子状态来适应QGAN检测高维时间序列数据的问题。此外，SuDaI编码还适用于其他类型的高维时间序列和不同的应用场景，因此开放了多个应用领域。
</details></li>
</ul>
<hr>
<h2 id="Clustering-Three-Way-Data-with-Outliers"><a href="#Clustering-Three-Way-Data-with-Outliers" class="headerlink" title="Clustering Three-Way Data with Outliers"></a>Clustering Three-Way Data with Outliers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05288">http://arxiv.org/abs/2310.05288</a></li>
<li>repo_url: None</li>
<li>paper_authors: Katharine M. Clark, Paul D. McNicholas</li>
<li>for:  clustering matrix-variate normal data with outliers</li>
<li>methods: 使用分布subset log-likelihoods， extends OCLUST algorithm to matrix-variate normal data，使用迭代方法检测和剔除异常点</li>
<li>results: 可以有效地检测和剔除matrix-variate normal data中的异常点<details>
<summary>Abstract</summary>
Matrix-variate distributions are a recent addition to the model-based clustering field, thereby making it possible to analyze data in matrix form with complex structure such as images and time series. Due to its recent appearance, there is limited literature on matrix-variate data, with even less on dealing with outliers in these models. An approach for clustering matrix-variate normal data with outliers is discussed. The approach, which uses the distribution of subset log-likelihoods, extends the OCLUST algorithm to matrix-variate normal data and uses an iterative approach to detect and trim outliers.
</details>
<details>
<summary>摘要</summary>
矩阵变量分布是现代模型基 clustering 领域的新添加，可以处理矩阵数据形式的复杂结构，如图像和时间序列。由于其新的出现，关于矩阵变量数据的文献非常有限，甚至更少关于处理异常值在这些模型中。一种用于矩阵变量正态数据 clustering 和异常值排除的方法被讨论。该方法基于分布subset log-likelihood的分布，对矩阵变量数据进行了扩展，并使用迭代法排除异常值。
</details></li>
</ul>
<hr>
<h2 id="Learning-force-laws-in-many-body-systems"><a href="#Learning-force-laws-in-many-body-systems" class="headerlink" title="Learning force laws in many-body systems"></a>Learning force laws in many-body systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05273">http://arxiv.org/abs/2310.05273</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wyu54/many-body-force-infer">https://github.com/wyu54/many-body-force-infer</a></li>
<li>paper_authors: Wentao Yu, Eslam Abdelaleem, Ilya Nemenman, Justin C. Burton</li>
<li>for: The paper is written to demonstrate a machine learning (ML) approach for discovering force laws in dusty plasma experiments.</li>
<li>methods: The paper uses 3D particle trajectories to train an ML model that incorporates physical intuition to infer the effective non-reciprocal forces between particles, accounting for inherent symmetries and non-identical particles.</li>
<li>results: The model accurately learns the force laws and extracts each particle’s mass and charge, with an accuracy of R^2 &gt; 0.99, indicating new physics in dusty plasma beyond the resolution of current theories and demonstrating the potential of ML-powered approaches for guiding new routes of scientific discovery in many-body systems.Here’s the same information in Simplified Chinese text:</li>
<li>for: 该文章用于演示一种基于机器学习（ML）的方法，用于在尘晶体实验中发现力法律。</li>
<li>methods: 该文章使用3D particulate轨迹来训练一个ML模型，该模型具有物理直觉，以推导粒子之间的有效非对称力，并考虑粒子之间的自旋Symmetry和不同的粒子。</li>
<li>results: 模型具有R^2&gt;0.99的准确性，表明尘晶体中存在跟当前理论不同的新物理现象，并证明ML能力可以导引科学发现的新路径。<details>
<summary>Abstract</summary>
Scientific laws describing natural systems may be more complex than our intuition can handle, and thus how we discover laws must change. Machine learning (ML) models can analyze large quantities of data, but their structure should match the underlying physical constraints to provide useful insight. Here we demonstrate a ML approach that incorporates such physical intuition to infer force laws in dusty plasma experiments. Trained on 3D particle trajectories, the model accounts for inherent symmetries and non-identical particles, accurately learns the effective non-reciprocal forces between particles, and extracts each particle's mass and charge. The model's accuracy (R^2 > 0.99) points to new physics in dusty plasma beyond the resolution of current theories and demonstrates how ML-powered approaches can guide new routes of scientific discovery in many-body systems.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Simplifying-GNN-Performance-with-Low-Rank-Kernel-Models"><a href="#Simplifying-GNN-Performance-with-Low-Rank-Kernel-Models" class="headerlink" title="Simplifying GNN Performance with Low Rank Kernel Models"></a>Simplifying GNN Performance with Low Rank Kernel Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05250">http://arxiv.org/abs/2310.05250</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lucianoavinas/lowrank-gnn-kernels">https://github.com/lucianoavinas/lowrank-gnn-kernels</a></li>
<li>paper_authors: Luciano Vinas, Arash A. Amini</li>
<li>for: 本研究 revisits recent spectral GNN approaches to semi-supervised node classification (SSNC), 提出许多现代GNN架构可能过度设计。</li>
<li>methods: 研究使用非 Parametric estimation 技术在 spectral 频谱中应用，代替许多深度学习引用 GNN 设计。这些传统技术适用于各种图类型，达到了许多常见 SSNC benchmark 的状态 arts 性能。</li>
<li>results: 研究表明，近期 GNN 方法的性能改进部分归功于评估方法的转变。此外，对 GNN  спектраль滤波技术的各种超参数进行了ablative study。代码可以在 <a target="_blank" rel="noopener" href="https://github.com/lucianoAvinas/lowrank-gnn-kernels">https://github.com/lucianoAvinas/lowrank-gnn-kernels</a> 找到。<details>
<summary>Abstract</summary>
We revisit recent spectral GNN approaches to semi-supervised node classification (SSNC). We posit that many of the current GNN architectures may be over-engineered. Instead, simpler, traditional methods from nonparametric estimation, applied in the spectral domain, could replace many deep-learning inspired GNN designs. These conventional techniques appear to be well suited for a variety of graph types reaching state-of-the-art performance on many of the common SSNC benchmarks. Additionally, we show that recent performance improvements in GNN approaches may be partially attributed to shifts in evaluation conventions. Lastly, an ablative study is conducted on the various hyperparameters associated with GNN spectral filtering techniques. Code available at: https://github.com/lucianoAvinas/lowrank-gnn-kernels
</details>
<details>
<summary>摘要</summary>
我们回到最近的спектルールGraph Neural Network（GNN）方法，用于半supervised node classification（SSNC）。我们认为许多现有的GNN架构可能是过工程。相反，更简单的传统方法，应用于spectral domain，可以取代许多深度学习灵感的GNN设计。这些传统技术适用于多种граф型，可以达到多数常见的SSNC benchmark中的state-of-the-art表现。此外，我们显示出最近GNN方法的性能提升部分可以归因于评估惯例的变化。最后，我们进行了GNNspectral filtering技术各种参数的ablative study。code可以在以下github上取得：https://github.com/lucianoAvinas/lowrank-gnn-kernels。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Kernel-Flexibility-via-Learning-Asymmetric-Locally-Adaptive-Kernels"><a href="#Enhancing-Kernel-Flexibility-via-Learning-Asymmetric-Locally-Adaptive-Kernels" class="headerlink" title="Enhancing Kernel Flexibility via Learning Asymmetric Locally-Adaptive Kernels"></a>Enhancing Kernel Flexibility via Learning Asymmetric Locally-Adaptive Kernels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05236">http://arxiv.org/abs/2310.05236</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hefansjtu/labrbf_kernel">https://github.com/hefansjtu/labrbf_kernel</a></li>
<li>paper_authors: Fan He, Mingzhen He, Lei Shi, Xiaolin Huang, Johan A. K. Suykens</li>
<li>for: 这篇论文的目的是提高基域学习的灵活性，通过使用可训练的本地适应宽度（LAB）来增强径basis函数（RBF）kernels。</li>
<li>methods: 这篇论文提出了一种新的非对称基域函数（Asymmetric Kernel Ridge Regression，AKRR）框架，并引入了一种循环基域学习算法来训练可训练的本地适应宽度。</li>
<li>results: 实验结果表明，提出的方法可以在实际 dataset 上达到remarkable的性能，比 Nystr&quot;om approximation-based algorithms 更具有扩展性，并且在基域学习方法中显示出较高的准确率，甚至超过了 residual neural networks。<details>
<summary>Abstract</summary>
The lack of sufficient flexibility is the key bottleneck of kernel-based learning that relies on manually designed, pre-given, and non-trainable kernels. To enhance kernel flexibility, this paper introduces the concept of Locally-Adaptive-Bandwidths (LAB) as trainable parameters to enhance the Radial Basis Function (RBF) kernel, giving rise to the LAB RBF kernel. The parameters in LAB RBF kernels are data-dependent, and its number can increase with the dataset, allowing for better adaptation to diverse data patterns and enhancing the flexibility of the learned function. This newfound flexibility also brings challenges, particularly with regards to asymmetry and the need for an efficient learning algorithm. To address these challenges, this paper for the first time establishes an asymmetric kernel ridge regression framework and introduces an iterative kernel learning algorithm. This novel approach not only reduces the demand for extensive support data but also significantly improves generalization by training bandwidths on the available training data. Experimental results on real datasets underscore the remarkable performance of the proposed algorithm, showcasing its superior capability in handling large-scale datasets compared to Nystr\"om approximation-based algorithms. Moreover, it demonstrates a significant improvement in regression accuracy over existing kernel-based learning methods and even surpasses residual neural networks.
</details>
<details>
<summary>摘要</summary>
文中提到的主要瓶须是基于手动设计、预给定、不可学习的kernels的学习系统的缺乏足够的灵活性。为了增强kernel的灵活性，这篇论文引入了Locally-Adaptive-Bandwidths（LAB）作为可学习参数，从而改进了基于卷积函数（RBF）kernel，得到LAB RBF kernel。这些参数随着数据的变化而变化，数量可以随着数据集的增加而增加，以适应多样化的数据模式，从而提高学习的灵活性。然而，这种新的灵活性也带来了挑战，特别是偏 asymmetry和有效的学习算法的需求。为了解决这些挑战，这篇论文首次提出了一种偏 asymmetric kernel ridge regression框架，并引入了一种迭代式 kernel learning算法。这种新的方法不仅可以减少了大量的支持数据，还可以很好地适应不同的数据模式，从而提高了泛化性。实验结果表明，提议的算法在实际数据上表现出色，比 Nystr\"om Approximation-based algorithms更好地处理大规模数据，并且超过了基于kernel的学习方法和 residual neural networks 的准确率。
</details></li>
</ul>
<hr>
<h2 id="Global-Convergence-of-Policy-Gradient-Methods-in-Reinforcement-Learning-Games-and-Control"><a href="#Global-Convergence-of-Policy-Gradient-Methods-in-Reinforcement-Learning-Games-and-Control" class="headerlink" title="Global Convergence of Policy Gradient Methods in Reinforcement Learning, Games and Control"></a>Global Convergence of Policy Gradient Methods in Reinforcement Learning, Games and Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05230">http://arxiv.org/abs/2310.05230</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shicong Cen, Yuejie Chi</li>
<li>for: 政策梯度方法，用于Sequential Decision Making中寻找政策优化。</li>
<li>methods: 使用首选信息来最大化价值函数。</li>
<li>results: 最近的进展包括对政策梯度方法的全球最优性保证，以及对重要问题参数的finite-time收敛率。<details>
<summary>Abstract</summary>
Policy gradient methods, where one searches for the policy of interest by maximizing the value functions using first-order information, become increasingly popular for sequential decision making in reinforcement learning, games, and control. Guaranteeing the global optimality of policy gradient methods, however, is highly nontrivial due to nonconcavity of the value functions. In this exposition, we highlight recent progresses in understanding and developing policy gradient methods with global convergence guarantees, putting an emphasis on their finite-time convergence rates with regard to salient problem parameters.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Accelerating-Machine-Learning-Primitives-on-Commodity-Hardware"><a href="#Accelerating-Machine-Learning-Primitives-on-Commodity-Hardware" class="headerlink" title="Accelerating Machine Learning Primitives on Commodity Hardware"></a>Accelerating Machine Learning Primitives on Commodity Hardware</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05218">http://arxiv.org/abs/2310.05218</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roman Snytsar</li>
<li>for: 这篇论文是用于探讨深度神经网络（DNN）中的滑动窗口卷积技术，并对其进行了广泛的研究和评估。</li>
<li>methods: 本论文使用了滑动窗口卷积技术来提高深度神经网络的训练和推理效率，并对其进行了广泛的研究和评估。</li>
<li>results: 研究结果表明，使用滑动窗口卷积技术可以减少内存占用和提高计算效率，并在CPU和专门设计的硬件加速器上实现显著的速度提升。这种技术可能会推动AI在低功耗和低内存设备上的广泛应用，无需特殊硬件。<details>
<summary>Abstract</summary>
Sliding Window Sum algorithms have been successfully used for training and inference of Deep Neural Networks. We have shown before how both pooling and convolution 1-D primitives could be expressed as sliding sums and evaluated by the compute kernels with a shared structure. In this paper, we present an extensive study of the Sliding Window convolution technique as a more efficient alternative to the commonly used General Matrix Multiplication (GEMM) based convolution in Deep Neural Networks (DNNs). The Sliding Window technique addresses the memory bloating problem and demonstrates a significant speedup in 2-D convolution. We explore the performance of this technique on a range of implementations, including custom kernels for specific filter sizes. Our results suggest that the Sliding Window computation kernels can outperform GEMM-based convolution on a CPU and even on dedicated hardware accelerators. This could promote a wider adoption of AI on low-power and low-memory devices without the need for specialized hardware. We also discuss the compatibility of model compression methods and optimized network architectures with the Sliding Window technique, encouraging further research in these areas.
</details>
<details>
<summary>摘要</summary>
Sliding Window Sum算法已经成功地应用于深度神经网络的训练和推理。我们之前已经证明了抽象和卷积1-D primitives可以表示为滑动和计算缓存的共享结构。在这篇论文中，我们进行了广泛的滑动窗口卷积技术的研究，作为深度神经网络中常用的普通矩阵乘法（GEMM）基于卷积的更有效的替代方案。滑动窗口技术解决了内存膨胀问题，并在2-D卷积中显示出了明显的速度提升。我们对不同的实现进行了探索，包括特定的缓存器大小的自定义kernels。我们的结果表明，滑动窗口计算kernels可以在CPU和专门的硬件加速器上超越GEMM-基于卷积。这可能会推动AI在低功耗和低内存设备上的更广泛应用，无需特殊硬件。我们还讨论了模型压缩方法和优化网络架构与滑动窗口技术的相容性，鼓励进一步的研究在这些领域。
</details></li>
</ul>
<hr>
<h2 id="Towards-Optimizing-with-Large-Language-Models"><a href="#Towards-Optimizing-with-Large-Language-Models" class="headerlink" title="Towards Optimizing with Large Language Models"></a>Towards Optimizing with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05204">http://arxiv.org/abs/2310.05204</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Pei-Fu Guo, Ying-Hsuan Chen, Yun-Da Tsai, Shou-De Lin</li>
<li>for: 这个研究是为了评估LLMs在不同任务和数据大小下的优化能力。</li>
<li>methods: 这个研究使用了交互式提示法， LLMS需要在每个优化步骤中从过去生成的解决方案中生成新的解决方案，然后评估这些新的解决方案的值。研究者还引入了三种综合评估任务性能的指标，这些指标适用于评估LLM在各种优化任务中的表现，并且对测试样本的变化更加敏感。</li>
<li>results: 研究发现，当处理小样本时，LLMs表现出强大的优化能力，但是对数据大小和值的影响表明需要进一步研究LLM在优化任务中的表现。<details>
<summary>Abstract</summary>
In this work, we conduct an assessment of the optimization capabilities of LLMs across various tasks and data sizes. Each of these tasks corresponds to unique optimization domains, and LLMs are required to execute these tasks with interactive prompting. That is, in each optimization step, the LLM generates new solutions from the past generated solutions with their values, and then the new solutions are evaluated and considered in the next optimization step. Additionally, we introduce three distinct metrics for a comprehensive assessment of task performance from various perspectives. These metrics offer the advantage of being applicable for evaluating LLM performance across a broad spectrum of optimization tasks and are less sensitive to variations in test samples. By applying these metrics, we observe that LLMs exhibit strong optimization capabilities when dealing with small-sized samples. However, their performance is significantly influenced by factors like data size and values, underscoring the importance of further research in the domain of optimization tasks for LLMs.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们进行了 LLMS 的优化能力评估，涵盖了多种任务和数据大小。每个任务都对应于唯一的优化领域，LLMS 需要在交互式提示下执行这些任务。即在每次优化步骤中，LLMS 从过去生成的解决方案和其值中生成新的解决方案，然后评估并考虑这些新的解决方案。此外，我们引入了三种特征metric来全面评估任务性能从多个角度。这些 metric 可以用于评估 LLMS 在各种优化任务上的性能，并且对测试样本的变化更加敏感。通过应用这些 metric，我们发现 LLMS 在小样本Size 下表现出色，但是它们的表现受到数据大小和值的影响，这 highlights 了进一步研究 LLMS 在优化任务领域的必要性。
</details></li>
</ul>
<hr>
<h2 id="Lifelong-Learning-for-Fog-Load-Balancing-A-Transfer-Learning-Approach"><a href="#Lifelong-Learning-for-Fog-Load-Balancing-A-Transfer-Learning-Approach" class="headerlink" title="Lifelong Learning for Fog Load Balancing: A Transfer Learning Approach"></a>Lifelong Learning for Fog Load Balancing: A Transfer Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05187">http://arxiv.org/abs/2310.05187</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maad Ebrahim, Abdelhakim Senhaji Hafid, Mohamed Riduan Abid</li>
<li>for: 本文旨在提出一种基于Reinforcement Learning（RL）的fog computing环境中的负载均衡（LB）策略，以提高系统性能。</li>
<li>methods: 本文使用privacy-aware RL agents来优化 fog computing环境中的负载均衡，并提出一种生命周期学习框架，使用 Transfer Learning（TL）来减少训练成本和适应环境变化。</li>
<li>results: 本文的实验结果显示，使用TL可以大幅减少RL agents的训练时间和失败概率，并在不同的环境下保持鲁棒性。<details>
<summary>Abstract</summary>
Fog computing emerged as a promising paradigm to address the challenges of processing and managing data generated by the Internet of Things (IoT). Load balancing (LB) plays a crucial role in Fog computing environments to optimize the overall system performance. It requires efficient resource allocation to improve resource utilization, minimize latency, and enhance the quality of service for end-users. In this work, we improve the performance of privacy-aware Reinforcement Learning (RL) agents that optimize the execution delay of IoT applications by minimizing the waiting delay. To maintain privacy, these agents optimize the waiting delay by minimizing the change in the number of queued requests in the whole system, i.e., without explicitly observing the actual number of requests that are queued in each Fog node nor observing the compute resource capabilities of those nodes. Besides improving the performance of these agents, we propose in this paper a lifelong learning framework for these agents, where lightweight inference models are used during deployment to minimize action delay and only retrained in case of significant environmental changes. To improve the performance, minimize the training cost, and adapt the agents to those changes, we explore the application of Transfer Learning (TL). TL transfers the knowledge acquired from a source domain and applies it to a target domain, enabling the reuse of learned policies and experiences. TL can be also used to pre-train the agent in simulation before fine-tuning it in the real environment; this significantly reduces failure probability compared to learning from scratch in the real environment. To our knowledge, there are no existing efforts in the literature that use TL to address lifelong learning for RL-based Fog LB; this is one of the main obstacles in deploying RL LB solutions in Fog systems.
</details>
<details>
<summary>摘要</summary>
FOG计算技术 emerged as a promising paradigm to address the challenges of processing and managing data generated by the Internet of Things (IoT). Load balancing (LB) plays a crucial role in FOG computing environments to optimize the overall system performance. It requires efficient resource allocation to improve resource utilization, minimize latency, and enhance the quality of service for end-users. In this work, we improve the performance of privacy-aware Reinforcement Learning (RL) agents that optimize the execution delay of IoT applications by minimizing the waiting delay. To maintain privacy, these agents optimize the waiting delay by minimizing the change in the number of queued requests in the whole system, i.e., without explicitly observing the actual number of requests that are queued in each FOG node nor observing the compute resource capabilities of those nodes. Besides improving the performance of these agents, we propose in this paper a lifelong learning framework for these agents, where lightweight inference models are used during deployment to minimize action delay and only retrained in case of significant environmental changes. To improve the performance, minimize the training cost, and adapt the agents to those changes, we explore the application of Transfer Learning (TL). TL transfers the knowledge acquired from a source domain and applies it to a target domain, enabling the reuse of learned policies and experiences. TL can be also used to pre-train the agent in simulation before fine-tuning it in the real environment; this significantly reduces failure probability compared to learning from scratch in the real environment. To our knowledge, there are no existing efforts in the literature that use TL to address lifelong learning for RL-based Fog LB; this is one of the main obstacles in deploying RL LB solutions in FOG systems.
</details></li>
</ul>
<hr>
<h2 id="Unified-speech-and-gesture-synthesis-using-flow-matching"><a href="#Unified-speech-and-gesture-synthesis-using-flow-matching" class="headerlink" title="Unified speech and gesture synthesis using flow matching"></a>Unified speech and gesture synthesis using flow matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05181">http://arxiv.org/abs/2310.05181</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shivam Mehta, Ruibo Tu, Simon Alexanderson, Jonas Beskow, Éva Székely, Gustav Eje Henter</li>
<li>for: 这篇论文旨在描述一种新的多Modal合成方法，可以同时生成语音和手势动作。</li>
<li>methods: 该方法使用优化交通流行为匹配（OT-CFM）来联合生成语音和手势动作，而且比前一代更简单，具有更小的内存占用量，同时能够捕捉语音和手势的联合分布，从而生成两个模态的动作。</li>
<li>results: 该方法在论文中被证明可以生成更自然的语音和更人工的手势动作，并且在单模和多模测试中也表现出更高的合理性。<details>
<summary>Abstract</summary>
As text-to-speech technologies achieve remarkable naturalness in read-aloud tasks, there is growing interest in multimodal synthesis of verbal and non-verbal communicative behaviour, such as spontaneous speech and associated body gestures. This paper presents a novel, unified architecture for jointly synthesising speech acoustics and skeleton-based 3D gesture motion from text, trained using optimal-transport conditional flow matching (OT-CFM). The proposed architecture is simpler than the previous state of the art, has a smaller memory footprint, and can capture the joint distribution of speech and gestures, generating both modalities together in one single process. The new training regime, meanwhile, enables better synthesis quality in much fewer steps (network evaluations) than before. Uni- and multimodal subjective tests demonstrate improved speech naturalness, gesture human-likeness, and cross-modal appropriateness compared to existing benchmarks.
</details>
<details>
<summary>摘要</summary>
As text-to-speech technologies achieve remarkable naturalness in read-aloud tasks, there is growing interest in multimodal synthesis of verbal and non-verbal communicative behavior, such as spontaneous speech and associated body gestures. This paper presents a novel, unified architecture for jointly synthesizing speech acoustics and skeleton-based 3D gesture motion from text, trained using optimal-transport conditional flow matching (OT-CFM). The proposed architecture is simpler than the previous state of the art, has a smaller memory footprint, and can capture the joint distribution of speech and gestures, generating both modalities together in one single process. The new training regime, meanwhile, enables better synthesis quality in much fewer steps (network evaluations) than before. Uni- and multimodal subjective tests demonstrate improved speech naturalness, gesture human-likeness, and cross-modal appropriateness compared to existing benchmarks.Here's the translation breakdown:* As text-to-speech technologies achieve remarkable naturalness in read-aloud tasks: 文本读取技术已经达到了很高的自然性水平。* there is growing interest in multimodal synthesis of verbal and non-verbal communicative behavior: 人们对于涉及语音和非语音通信行为的多模态合成表示越来越大的兴趣。* such as spontaneous speech and associated body gestures: 例如，自然的语音和相关的身体姿势。* This paper presents a novel, unified architecture for jointly synthesizing speech acoustics and skeleton-based 3D gesture motion from text: 本文提出了一种新的、统一的架构，用于从文本中同时合成语音和基于骨架的3D手势动作。* trained using optimal-transport conditional flow matching (OT-CFM): 使用优化交通Conditional Flow匹配（OT-CFM）进行训练。* The proposed architecture is simpler than the previous state of the art, has a smaller memory footprint, and can capture the joint distribution of speech and gestures: 提出的架构比前一代更简单，占用更少的内存空间，并能够捕捉语音和手势的共同分布。* generating both modalities together in one single process: 一起生成两种Modalities。* The new training regime, meanwhile, enables better synthesis quality in much fewer steps (network evaluations) than before: 新的训练方法可以在更少的步骤（网络评估）中实现更高质量的合成。* Uni- and multimodal subjective tests demonstrate improved speech naturalness, gesture human-likeness, and cross-modal appropriateness compared to existing benchmarks: 单模态和多模态主观测试表明，提出的方法可以提高语音自然性、姿势人类化和交叉模态适应性，相比exist的 referential。
</details></li>
</ul>
<hr>
<h2 id="Distributional-Reinforcement-Learning-with-Online-Risk-awareness-Adaption"><a href="#Distributional-Reinforcement-Learning-with-Online-Risk-awareness-Adaption" class="headerlink" title="Distributional Reinforcement Learning with Online Risk-awareness Adaption"></a>Distributional Reinforcement Learning with Online Risk-awareness Adaption</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05179">http://arxiv.org/abs/2310.05179</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yupeng Wu, Wenjie Huang</li>
<li>for: 本研究旨在提出一种新的分布式RL框架，以快速适应不确定环境中的不同风险水平，以提高RL在安全关键环境中的可靠优化策略。</li>
<li>methods: 该框架基于分布式RL的基础上，通过在线解决一个总变量最小化问题， dynamically选择适度的epistemic风险水平，以满足安全性和稳定性的要求。这里使用了一种 Follow-The-Leader 类型的搜索算法，以及一种特殊修改的损失函数，以实现在线选择风险水平。</li>
<li>results: 对多种任务进行比较，研究发现，DRL-ORA方法在面对不确定环境中表现出色，超过了基于固定风险水平或手动适应风险水平的方法。此外，研究还发现，DRL-ORA方法可以轻松地与多种RL算法结合使用，不需要进行大量的修改。<details>
<summary>Abstract</summary>
The use of reinforcement learning (RL) in practical applications requires considering sub-optimal outcomes, which depend on the agent's familiarity with the uncertain environment. Dynamically adjusting the level of epistemic risk over the course of learning can tactically achieve reliable optimal policy in safety-critical environments and tackle the sub-optimality of a static risk level. In this work, we introduce a novel framework, Distributional RL with Online Risk Adaption (DRL-ORA), which can quantify the aleatory and epistemic uncertainties compositely and dynamically select the epistemic risk levels via solving a total variation minimization problem online. The risk level selection can be efficiently achieved through grid search using a Follow-The-Leader type algorithm, and its offline oracle is related to "satisficing measure" (in the decision analysis community) under a special modification of the loss function. We show multiple classes of tasks where DRL-ORA outperforms existing methods that rely on either a fixed risk level or manually predetermined risk level adaption. Given the simplicity of our modifications, we believe the framework can be easily incorporated into most RL algorithm variants.
</details>
<details>
<summary>摘要</summary>
使用强化学习（RL）在实际应用中需要考虑不理想的结果，这些结果取决于智能体对不确定环境的熟悉程度。随着学习过程中的时间推移， dynamically 调整epistemic 风险水平可以策略性实现可靠的优化策略并解决固定风险水平的不优势。在这项工作中，我们介绍了一种新的框架，分布式RLwith Online Risk Adaptation（DRL-ORA），它可以compositely 量化 aleatory 和 epistemic uncertainties，并在线 solves  total variation minimization problem 来动态选择 epistemic 风险水平。风险水平选择可以高效地通过格子搜索使用 Follow-The-Leader 类型算法进行，其 Offline oracle 与 "满意度量"（在决策分析社区）相关，只是在特定的损失函数修改下进行修改。我们证明了多种任务上，DRL-ORA 可以超过现有的固定风险水平或手动适应风险水平的方法。 compte tenu de la simplicité de nos modifications, nous croyons que le framework peut être facilement intégré dans la plupart des variantes d'algorithme RL.
</details></li>
</ul>
<hr>
<h2 id="Outlier-Weighed-Layerwise-Sparsity-OWL-A-Missing-Secret-Sauce-for-Pruning-LLMs-to-High-Sparsity"><a href="#Outlier-Weighed-Layerwise-Sparsity-OWL-A-Missing-Secret-Sauce-for-Pruning-LLMs-to-High-Sparsity" class="headerlink" title="Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity"></a>Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05175">http://arxiv.org/abs/2310.05175</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luuyin/owl">https://github.com/luuyin/owl</a></li>
<li>paper_authors: Lu Yin, You Wu, Zhenyu Zhang, Cheng-Yu Hsieh, Yaqing Wang, Yiling Jia, Mykola Pechenizkiy, Yi Liang, Zhangyang Wang, Shiwei Liu<br>for:* The paper aims to improve the practical deployment of large language models (LLMs) by applying traditional network pruning techniques.methods:* The paper introduces a novel LLM pruning methodology called Outlier Weighed Layerwise sparsity (OWL), which incorporates non-uniform layerwise sparsity ratios tailored for LLM pruning.results:* The paper demonstrates the distinct advantages offered by OWL over previous methods, achieving a remarkable performance gain of 61.22 and 6.80 perplexity at a high sparsity level of 70%, respectively, compared to the state-of-the-art Wanda and SparseGPT.<details>
<summary>Abstract</summary>
Large Language Models (LLMs), renowned for their remarkable performance, present a challenge due to their colossal model size when it comes to practical deployment. In response to this challenge, efforts have been directed toward the application of traditional network pruning techniques to LLMs, uncovering a massive number of parameters can be pruned in one-shot without hurting performance. Building upon insights gained from pre-LLM models, prevailing LLM pruning strategies have consistently adhered to the practice of uniformly pruning all layers at equivalent sparsity. However, this observation stands in contrast to the prevailing trends observed in the field of vision models, where non-uniform layerwise sparsity typically yields substantially improved results. To elucidate the underlying reasons for this disparity, we conduct a comprehensive analysis of the distribution of token features within LLMs. In doing so, we discover a strong correlation with the emergence of outliers, defined as features exhibiting significantly greater magnitudes compared to their counterparts in feature dimensions. Inspired by this finding, we introduce a novel LLM pruning methodology that incorporates a tailored set of non-uniform layerwise sparsity ratios specifically designed for LLM pruning, termed as Outlier Weighed Layerwise sparsity (OWL). The sparsity ratio of OWL is directly proportional to the outlier ratio observed within each layer, facilitating a more effective alignment between layerwise weight sparsity and outlier ratios. Our empirical evaluation, conducted across the LLaMA-V1 family and OPT, spanning various benchmarks, demonstrates the distinct advantages offered by OWL over previous methods. For instance, our approach exhibits a remarkable performance gain, surpassing the state-of-the-art Wanda and SparseGPT by 61.22 and 6.80 perplexity at a high sparsity level of 70%, respectively.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）因其出色的表现而带来挑战，尤其是在实际应用时遇到模型的巨大大小问题。为了解决这个问题，努力对 LLM 应用传统网络剔除技术，发现可以剔除一个巨大的数据量而无需影响表现。建立在先前的模型中获得的见解上，现有的 LLM 剔除策略一般遵循以剔除所有层的内容的方式，但这与视觉模型中的非均匀层剔除倾向相比，通常会带来更好的结果。为了了解 LLM 中对于剔除的影响，我们进行了一个全面的分析，发现 LLM 中的内容特征分布和异常值的出现有很强的联系。根据这个发现，我们提出了一种新的 LLM 剔除方法，称为非均匀层剔除（OWL）。OWL 的剔除比率与异常值的比率直接相比，从而使得层剔除与异常值的分布更好地匹配。我们对 LLMA-V1 家族和 OPT 进行了实验，覆盖了多个测试benchmark，结果显示我们的方法在剔除率高于 70% 时表现出色，较前者的状态顶峰方法（Wanda和SparseGPT）提高了61.22和6.80的混淆度。
</details></li>
</ul>
<hr>
<h2 id="Investigating-the-Ability-of-PINNs-To-Solve-Burgers’-PDE-Near-Finite-Time-BlowUp"><a href="#Investigating-the-Ability-of-PINNs-To-Solve-Burgers’-PDE-Near-Finite-Time-BlowUp" class="headerlink" title="Investigating the Ability of PINNs To Solve Burgers’ PDE Near Finite-Time BlowUp"></a>Investigating the Ability of PINNs To Solve Burgers’ PDE Near Finite-Time BlowUp</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05169">http://arxiv.org/abs/2310.05169</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dibyakanti Kumar, Anirbit Mukherjee</li>
<li>for: 这个论文旨在investigating the stability of Physics Informed Neural Networks (PINNs) in solving partial differential equations (PDEs) with finite-time blow-ups.</li>
<li>methods: 作者使用了泛化 bound的方法来研究PINNs的稳定性，并通过实验证明了这些 bound 与 neurally found surrogate 的 $\ell_2$-distance有直接的相关性。</li>
<li>results: 研究发现，PINNs 可以准确地探测 finite-time blow-ups，并且可以提供与真实解的 $\ell_2$-distance的评估。<details>
<summary>Abstract</summary>
Physics Informed Neural Networks (PINNs) have been achieving ever newer feats of solving complicated PDEs numerically while offering an attractive trade-off between accuracy and speed of inference. A particularly challenging aspect of PDEs is that there exist simple PDEs which can evolve into singular solutions in finite time starting from smooth initial conditions. In recent times some striking experiments have suggested that PINNs might be good at even detecting such finite-time blow-ups. In this work, we embark on a program to investigate this stability of PINNs from a rigorous theoretical viewpoint. Firstly, we derive generalization bounds for PINNs for Burgers' PDE, in arbitrary dimensions, under conditions that allow for a finite-time blow-up. Then we demonstrate via experiments that our bounds are significantly correlated to the $\ell_2$-distance of the neurally found surrogate from the true blow-up solution, when computed on sequences of PDEs that are getting increasingly close to a blow-up.
</details>
<details>
<summary>摘要</summary>
物理学 Informed Neural Networks (PINNs) 在解决复杂的偏微分方程（PDEs）方面已经取得了不断更新的成就，同时提供了吸引人的准确率和推理速度之间的折衔。特别是，PDEs 中存在一些简单的 PDEs，可以在有限时间内从流体初始条件演化成精炼解。在最近的实验中，有些突出的实验结果表明，PINNs 可能会检测到这种有限时间爆炸。在这项工作中，我们开始了一项研究，以探讨 PINNs 在理论上的稳定性。首先，我们 derive了 PINNs 对于布尔格 PDE 的泛化上限，在任意维度下，以条件 Allowing for finite-time blow-up。然后，我们通过实验表明，我们的上限与 neurally 发现的代理模型在计算 PDE 序列中的 $\ell_2$ 距离是高度相关的，当 PDE 序列在爆炸解 approached 时。
</details></li>
</ul>
<hr>
<h2 id="A-Corrected-Expected-Improvement-Acquisition-Function-Under-Noisy-Observations"><a href="#A-Corrected-Expected-Improvement-Acquisition-Function-Under-Noisy-Observations" class="headerlink" title="A Corrected Expected Improvement Acquisition Function Under Noisy Observations"></a>A Corrected Expected Improvement Acquisition Function Under Noisy Observations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05166">http://arxiv.org/abs/2310.05166</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/han678/correctednoisyei">https://github.com/han678/correctednoisyei</a></li>
<li>paper_authors: Han Zhou, Xingchen Ma, Matthew B Blaschko</li>
<li>for: 该论文主要针对的是 Bayesian 优化中的难题，即在含有噪声的观测中使用预期改进（EI）策略。</li>
<li>methods: 该论文提出了一种基于 Gaussian Process 模型的 EI 策略修正方法，该方法可以考虑噪声的影响，并提供一个包容更多情况的 acquisition function。</li>
<li>results: 该论文通过 theoretically 和实验来证明，该修正方法可以提高 EI 策略在含有噪声的情况下的性能，并且可以在黑盒优化和神经网络模型压缩等问题中提供更好的解决方案。<details>
<summary>Abstract</summary>
Sequential maximization of expected improvement (EI) is one of the most widely used policies in Bayesian optimization because of its simplicity and ability to handle noisy observations. In particular, the improvement function often uses the best posterior mean as the best incumbent in noisy settings. However, the uncertainty associated with the incumbent solution is often neglected in many analytic EI-type methods: a closed-form acquisition function is derived in the noise-free setting, but then applied to the setting with noisy observations. To address this limitation, we propose a modification of EI that corrects its closed-form expression by incorporating the covariance information provided by the Gaussian Process (GP) model. This acquisition function specializes to the classical noise-free result, and we argue should replace that formula in Bayesian optimization software packages, tutorials, and textbooks. This enhanced acquisition provides good generality for noisy and noiseless settings. We show that our method achieves a sublinear convergence rate on the cumulative regret bound under heteroscedastic observation noise. Our empirical results demonstrate that our proposed acquisition function can outperform EI in the presence of noisy observations on benchmark functions for black-box optimization, as well as on parameter search for neural network model compression.
</details>
<details>
<summary>摘要</summary>
纯粹最大化期望提升（EI）是搜索优化中最广泛使用的政策之一，主要是因为它的简单性和能够处理噪音观测的能力。具体来说，提升函数经常使用 posterior mean 作为噪音观测下的最佳启发式。然而，启发式解释中往往忽略了启发式解释中的不确定性信息。为了解决这个限制，我们提议修改 EI，通过在 GP 模型中提供的决定矩阵信息来改进它的关闭形式表达。这个购买函数在噪音观测下特有化，我们认为这个函数应该取代普通的噪音观测下的表达。我们的改进购买函数在各种不同的观测环境下都具有良好的通用性。我们证明了我们的方法在各种不同的观测环境下都能够实现下降的 regret 级别。我们的实验结果表明，我们的提议的购买函数在噪音观测下可以超越 EI 在黑obox 优化和神经网络模型压缩中的性能。
</details></li>
</ul>
<hr>
<h2 id="Transferable-Availability-Poisoning-Attacks"><a href="#Transferable-Availability-Poisoning-Attacks" class="headerlink" title="Transferable Availability Poisoning Attacks"></a>Transferable Availability Poisoning Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05141">http://arxiv.org/abs/2310.05141</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/trustmlrg/transpoison">https://github.com/trustmlrg/transpoison</a></li>
<li>paper_authors: Yiyong Liu, Michael Backes, Xiao Zhang</li>
<li>for: 这个论文旨在攻击机器学习模型的可用性，特别是针对模型在训练数据上的性能。</li>
<li>methods: 这个论文使用了攻击者采用不同的学习算法和攻击策略来降低模型的总测试精度。</li>
<li>results: 论文表明，如果攻击者使用不同的学习方法来攻击模型， тоThen the effectiveness of prior poisoning attacks will be significantly decreased. In addition, the authors propose a transferable poisoning attack that can produce poisoned samples with improved transferability across different learners and even paradigms. Through extensive experiments on benchmark image datasets, the authors show that their transferable poisoning attack can produce poisoned samples with significantly improved transferability.<details>
<summary>Abstract</summary>
We consider availability data poisoning attacks, where an adversary aims to degrade the overall test accuracy of a machine learning model by crafting small perturbations to its training data. Existing poisoning strategies can achieve the attack goal but assume the victim to employ the same learning method as what the adversary uses to mount the attack. In this paper, we argue that this assumption is strong, since the victim may choose any learning algorithm to train the model as long as it can achieve some targeted performance on clean data. Empirically, we observe a large decrease in the effectiveness of prior poisoning attacks if the victim uses a different learning paradigm to train the model and show marked differences in frequency-level characteristics between perturbations generated with respect to different learners and attack methods. To enhance the attack transferability, we propose Transferable Poisoning, which generates high-frequency poisoning perturbations by alternately leveraging the gradient information with two specific algorithms selected from supervised and unsupervised contrastive learning paradigms. Through extensive experiments on benchmark image datasets, we show that our transferable poisoning attack can produce poisoned samples with significantly improved transferability, not only applicable to the two learners used to devise the attack but also for learning algorithms and even paradigms beyond.
</details>
<details>
<summary>摘要</summary>
我们考虑了数据毒化攻击，敌人想要降低机器学习模型的总测试准确率，通过对训练数据进行小幅度的修改。现有的攻击策略可以实现攻击目标，但假设攻击者使用的学习方法与受害者使用的学习方法一样。在这篇论文中，我们认为这是一个强大的假设，因为受害者可以选择任何学习算法来训练模型，只要它可以在干净数据上达到一定的性能目标。我们在实验中观察到，如果受害者使用不同的学习方法来训练模型，攻击效果会减弱很多。为了提高攻击的传送性，我们提议了可传递的毒化攻击，通过交替使用两种特定的算法来生成高频毒化干扰。我们通过对标准图像集进行广泛的实验，证明了我们的可传递毒化攻击可以生成高质量的毒化样本，不仅适用于我们用于制定攻击的两种学习算法，还可以应用于其他学习算法和学习方法。
</details></li>
</ul>
<hr>
<h2 id="How-Graph-Neural-Networks-Learn-Lessons-from-Training-Dynamics-in-Function-Space"><a href="#How-Graph-Neural-Networks-Learn-Lessons-from-Training-Dynamics-in-Function-Space" class="headerlink" title="How Graph Neural Networks Learn: Lessons from Training Dynamics in Function Space"></a>How Graph Neural Networks Learn: Lessons from Training Dynamics in Function Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05105">http://arxiv.org/abs/2310.05105</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenxiao Yang, Qitian Wu, David Wipf, Ruoyu Sun, Junchi Yan</li>
<li>for: 本研究的目的是探讨深度学习模型在更加可读性的方式下进行学习行为。特别是对于图神经网络（GNNs），研究者们已经做出了很多进步，但是还没有充分了解GNNs在优化过程中是否会学习愿景函数。</li>
<li>methods: 研究者们使用了分析框架来研究GNNs的学习动态，并发现GNNs的训练过程可以被重新描述为一种更加熟悉的标签传播框架。此外，研究者们还提出了一种简化并实现GNNs的学习动态的方法，以提高其效率和可读性。</li>
<li>results: 研究者们发现GNNs在不同类型的图上的学习动态，包括同构图和hetrophylic graph，都具有某种程度的相互关联性。此外，研究者们还发现GNNs可以在不同类型的图上学习出高效的函数，并且这些函数可以在新的图上进行泛化。<details>
<summary>Abstract</summary>
A long-standing goal in deep learning has been to characterize the learning behavior of black-box models in a more interpretable manner. For graph neural networks (GNNs), considerable advances have been made in formalizing what functions they can represent, however it remains less clear whether and how GNNs learn desired functions during the optimization process. To fill this critical gap, we study the learning dynamics of GNNs in function space via the analytic framework of overparameterization. In particular, we find that the seemingly complicated training process of GNNs can be re-cast into a more familiar label propagation framework, due to the graph inductive bias implicit in this process. From this vantage point, we provide explanations for why the learned GNN functions successfully generalize and for their pathological behavior on heterophilic graphs, which are consistent with observations. Practically, sparsifying and implementing the learning dynamics lead to a minimalist semi-supervised learning algorithm with the efficiency of classic algorithms and the effectiveness of modern GNNs.
</details>
<details>
<summary>摘要</summary>
deep learning中的一个长期目标是将黑盒模型的学习行为更加解释性地表示。对于图 neural network (GNN)，已经有了许多进步，但是它们是否在优化过程中学习所需的函数仍然不清楚。为了填补这一重要的空白，我们通过过参数化的方法研究 GNN 的学习动态在函数空间。具体来说，我们发现 GNN 的训练过程可以重新划为一种更加熟悉的标签传播框架，这与图适应偏好相关。从这个角度，我们提供了成功泛化和异谱图处理的解释，这与观察相符。在实践中，我们通过减少学习动态和实现来提出一种简洁的半监督学习算法，具有传统算法的效率和现代 GNN 的效果。
</details></li>
</ul>
<hr>
<h2 id="Asymmetrically-Decentralized-Federated-Learning"><a href="#Asymmetrically-Decentralized-Federated-Learning" class="headerlink" title="Asymmetrically Decentralized Federated Learning"></a>Asymmetrically Decentralized Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05093">http://arxiv.org/abs/2310.05093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinglun Li, Miao Zhang, Nan Yin, Quanjun Yin, Li Shen</li>
<li>For: This paper aims to address the communication burden and privacy concerns associated with centralized servers in Federated Learning (FL) by proposing a Decentralized Federated Learning (DFL) algorithm based on asymmetric topologies and the Push-Sum protocol.* Methods: The proposed DFedSGPSM algorithm combines the Sharpness Aware Minimization (SAM) optimizer and local momentum to improve algorithm performance and alleviate local heterogeneous overfitting in FL. The SAM optimizer employs gradient perturbations to generate locally flat models and searches for models with uniformly low loss values, while the local momentum accelerates the optimization process.* Results: The paper demonstrates the superior performance of the proposed DFedSGPSM algorithm compared to state-of-the-art optimizers through extensive experiments on the MNIST, CIFAR10, and CIFAR100 datasets. The theoretical analysis also proves that the algorithm achieves a convergence rate of $\mathcal{O}(\frac{1}{\sqrt{T})$ in a non-convex smooth setting under mild assumptions, and that better topological connectivity achieves tighter upper bounds.<details>
<summary>Abstract</summary>
To address the communication burden and privacy concerns associated with the centralized server in Federated Learning (FL), Decentralized Federated Learning (DFL) has emerged, which discards the server with a peer-to-peer (P2P) communication framework. However, most existing DFL algorithms are based on symmetric topologies, such as ring and grid topologies, which can easily lead to deadlocks and are susceptible to the impact of network link quality in practice. To address these issues, this paper proposes the DFedSGPSM algorithm, which is based on asymmetric topologies and utilizes the Push-Sum protocol to effectively solve consensus optimization problems. To further improve algorithm performance and alleviate local heterogeneous overfitting in Federated Learning (FL), our algorithm combines the Sharpness Aware Minimization (SAM) optimizer and local momentum. The SAM optimizer employs gradient perturbations to generate locally flat models and searches for models with uniformly low loss values, mitigating local heterogeneous overfitting. The local momentum accelerates the optimization process of the SAM optimizer. Theoretical analysis proves that DFedSGPSM achieves a convergence rate of $\mathcal{O}(\frac{1}{\sqrt{T})$ in a non-convex smooth setting under mild assumptions. This analysis also reveals that better topological connectivity achieves tighter upper bounds. Empirically, extensive experiments are conducted on the MNIST, CIFAR10, and CIFAR100 datasets, demonstrating the superior performance of our algorithm compared to state-of-the-art optimizers.
</details>
<details>
<summary>摘要</summary>
为了解决联合服务器在联合学习（FL）中的通信负担和隐私问题，协同联合学习（DFL）已经出现，它抛弃了服务器，使用幂等（P2P）通信框架。然而，大多数现有的DFL算法都基于对称网络 topology，如环和格 topology，这些 topology 可以轻松导致堵塞和因网络连接质量的影响。为解决这些问题，本文提出了DFedSGPSM算法，它基于非对称网络 topology 和Push-Sum协议来有效地解决共识优化问题。为了进一步改进算法性能并避免本地不同类型的过拟合，我们的算法结合了Sharpness Aware Minimization（SAM）优化器和本地冲击。SAM优化器使用梯度偏移来生成本地平滑模型，并在搜索模型的损失值为uniformly low时搜索模型。本地冲击加速了SAM优化器的优化过程。理论分析表明，DFedSGPSM算法在非对称光滑设定下 achieve 收敛速率为 $\mathcal{O}(\frac{1}{\sqrt{T})$ ，其中 T 是迭代次数。此分析还表明，更好的网络连接性可以实现更紧的Upper bound。实验表明，我们的算法在 MNIST、CIFAR10 和 CIFAR100 数据集上进行了广泛的实验，与现状的优化器相比，它的性能显著更高。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-Block-based-Quantisation-What-is-Important-for-Sub-8-bit-LLM-Inference"><a href="#Revisiting-Block-based-Quantisation-What-is-Important-for-Sub-8-bit-LLM-Inference" class="headerlink" title="Revisiting Block-based Quantisation: What is Important for Sub-8-bit LLM Inference?"></a>Revisiting Block-based Quantisation: What is Important for Sub-8-bit LLM Inference?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05079">http://arxiv.org/abs/2310.05079</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chengzhang-98/llm-mixed-q">https://github.com/chengzhang-98/llm-mixed-q</a></li>
<li>paper_authors: Cheng Zhang, Jianyi Cheng, Ilia Shumailov, George A. Constantinides, Yiren Zhao</li>
<li>for: 这个论文的目的是解决大型自然语言模型（LLM）的扩展和缩放问题，以降低计算和存储资源的成本。</li>
<li>methods: 这篇论文使用了统计学和学习Property的分析，发现LLM层的瓶颈在于数值扩展偏移。以此为基础，他们提出了块量化方法，可以有效地减少数值扩展偏移，从计算路径的视角来看。</li>
<li>results: 根据论文的结果，使用了6位减法的量化LLM可以达到$19\times$高的数学密度和$5\times$的存储密度，比浮点32基eline高$2.5\times$的数学密度和$1.2\times$的存储密度。此外，他们还分享了在下游任务上实现 nearly-lossless 4位LLM的技巧，包括活动和重量分布的不一致、优化的精度练习策略以及LLM的统计性质中的更低的量化精度。<details>
<summary>Abstract</summary>
The inference of Large language models (LLMs) requires immense computation and memory resources. To curtail these costs, quantisation has merged as a promising solution, but existing LLM quantisation mainly focuses on 8-bit. In this work, we explore the statistical and learning properties of the LLM layer and attribute the bottleneck of LLM quantisation to numerical scaling offsets. To address this, we adapt block quantisations for LLMs, a family of methods that share scaling factors across packed numbers. Block quantisations efficiently reduce the numerical scaling offsets solely from an arithmetic perspective, without additional treatments in the computational path. Our nearly-lossless quantised 6-bit LLMs achieve a $19\times$ higher arithmetic density and $5\times$ memory density than the float32 baseline, surpassing the prior art 8-bit quantisation by $2.5\times$ in arithmetic density and $1.2\times$ in memory density, without requiring any data calibration or re-training. We also share our insights into sub-8-bit LLM quantisation, including the mismatch between activation and weight distributions, optimal fine-tuning strategies, and a lower quantisation granularity inherent in the statistical properties of LLMs. The latter two tricks enable nearly-lossless 4-bit LLMs on downstream tasks. Our code is open-sourced.
</details>
<details>
<summary>摘要</summary>
Large language models (LLMs) 的推理需要巨量的计算和存储资源。为了降低这些成本，量化已成为一种有前途的解决方案，但现有的 LLM 量化主要集中在8位。在这个工作中，我们研究了 LLM 层的统计和学习特性，并归因 LLM 量化的瓶颈到数字扩大偏移。为解决这个问题，我们采用了块量化技术，这种技术在压缩数据时共享扩大因子。块量化可以高效地减少数字扩大偏移，不需要额外处理在计算路径上。我们的6位量化 LLM 可以达到浮点32基eline的19倍的数学密度和5倍的存储密度，比前一代8位量化的数学密度和存储密度高出2.5倍和1.2倍。此外，我们还分享了在下游任务上实现 nearly-lossless 4位 LLMS 的技巧，包括活动和重量分布不匹配、优化 fine-tuning 策略和 LLMS 的统计性质中的更低的量化精度。这两个技巧使得我们可以在下游任务上实现 nearly-lossless 4位 LLMS。我们的代码已经开源。
</details></li>
</ul>
<hr>
<h2 id="FedFed-Feature-Distillation-against-Data-Heterogeneity-in-Federated-Learning"><a href="#FedFed-Feature-Distillation-against-Data-Heterogeneity-in-Federated-Learning" class="headerlink" title="FedFed: Feature Distillation against Data Heterogeneity in Federated Learning"></a>FedFed: Feature Distillation against Data Heterogeneity in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05077">http://arxiv.org/abs/2310.05077</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/visitworld123/fedfed">https://github.com/visitworld123/fedfed</a></li>
<li>paper_authors: Zhiqin Yang, Yonggang Zhang, Yu Zheng, Xinmei Tian, Hao Peng, Tongliang Liu, Bo Han</li>
<li>for: 这篇论文旨在解决联合学习（Federated Learning，FL）面临的数据不一致问题，即客户端数据的分布差异。</li>
<li>methods: 该论文提出了一种新的方法 called Federated Feature Distillation（FedFed），它将数据分为性能敏感特征（大量对模型性能的贡献）和性能鲁棒特征（对模型性能有限度贡献）。性能敏感特征被全局共享，以减轻数据不一致问题，而性能鲁棒特征被保留在本地。客户端可以使用本地和共享数据来训练模型。</li>
<li>results: 实验表明，FedFed 可以提高模型性能。<details>
<summary>Abstract</summary>
Federated learning (FL) typically faces data heterogeneity, i.e., distribution shifting among clients. Sharing clients' information has shown great potentiality in mitigating data heterogeneity, yet incurs a dilemma in preserving privacy and promoting model performance. To alleviate the dilemma, we raise a fundamental question: \textit{Is it possible to share partial features in the data to tackle data heterogeneity?} In this work, we give an affirmative answer to this question by proposing a novel approach called {\textbf{Fed}erated \textbf{Fe}ature \textbf{d}istillation} (FedFed). Specifically, FedFed partitions data into performance-sensitive features (i.e., greatly contributing to model performance) and performance-robust features (i.e., limitedly contributing to model performance). The performance-sensitive features are globally shared to mitigate data heterogeneity, while the performance-robust features are kept locally. FedFed enables clients to train models over local and shared data. Comprehensive experiments demonstrate the efficacy of FedFed in promoting model performance.
</details>
<details>
<summary>摘要</summary>
通常，联合学习（FL）会面临数据不一致性问题，即客户端数据的分布差异。如果分享客户端信息，可以减轻数据不一致性问题，但是会降低隐私和提高模型性能。为了解决这个矛盾，我们提出了一个基本问题：“是否可以分享数据中的部分特征来解决数据不一致性问题？”在这个工作中，我们给出了一个答案，并提出了一种新的方法 called“联邦特征分离”（FedFed）。具体来说，FedFed将数据分为对性能敏感的特征（即对模型性能具有重要作用）和对性能稳定的特征（即对模型性能具有有限作用）。对性能敏感的特征进行全局分享，以减轻数据不一致性问题，而对性能稳定的特征则保留在本地。FedFed允许客户端通过本地和共享数据来训练模型。我们进行了广泛的实验，并证明了FedFed的效果。
</details></li>
</ul>
<hr>
<h2 id="Towards-Scalable-Wireless-Federated-Learning-Challenges-and-Solutions"><a href="#Towards-Scalable-Wireless-Federated-Learning-Challenges-and-Solutions" class="headerlink" title="Towards Scalable Wireless Federated Learning: Challenges and Solutions"></a>Towards Scalable Wireless Federated Learning: Challenges and Solutions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05076">http://arxiv.org/abs/2310.05076</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yong Zhou, Yuanming Shi, Haibo Zhou, Jingjing Wang, Liqun Fu, Yang Yang</li>
<li>for: 本研究旨在探讨在无线网络中实现可信批处理的分布式机器学习（ Federated Learning，FL）的挑战和解决方案。</li>
<li>methods: 本文提出了两个方面的解决方案：一是通过任务 oriented 模型聚合来提高无线通信缓存性，二是通过计算效率优化来提高资源分配的算法可扩展性。</li>
<li>results: 本文提出了三种任务 oriented 学习算法来提高计算效率，并指出了一些需要进一步研究的问题。<details>
<summary>Abstract</summary>
The explosive growth of smart devices (e.g., mobile phones, vehicles, drones) with sensing, communication, and computation capabilities gives rise to an unprecedented amount of data. The generated massive data together with the rapid advancement of machine learning (ML) techniques spark a variety of intelligent applications. To distill intelligence for supporting these applications, federated learning (FL) emerges as an effective distributed ML framework, given its potential to enable privacy-preserving model training at the network edge. In this article, we discuss the challenges and solutions of achieving scalable wireless FL from the perspectives of both network design and resource orchestration. For network design, we discuss how task-oriented model aggregation affects the performance of wireless FL, followed by proposing effective wireless techniques to enhance the communication scalability via reducing the model aggregation distortion and improving the device participation. For resource orchestration, we identify the limitations of the existing optimization-based algorithms and propose three task-oriented learning algorithms to enhance the algorithmic scalability via achieving computation-efficient resource allocation for wireless FL. We highlight several potential research issues that deserve further study.
</details>
<details>
<summary>摘要</summary>
随着智能设备（如移动电话、汽车、无人机）的激增，生成了历史上无 precedent的数据量。这些大量数据，加上机器学习（ML）技术的快速发展，使得各种智能应用得以实现。为了提取智能，聚合式学习（FL）作为一种有效的分布式ML框架，在网络边缘实现隐私保护的模型训练。本文从网络设计和资源调度两个角度出发，探讨了无线FL的挑战和解决方案。从网络设计角度来看，我们讨论了任务导向的模型聚合如何影响无线FL的性能，然后提出了有效的无线技术来增强通信可扩展性，例如减少模型聚合误差和提高设备参与度。从资源调度角度来看，我们发现现有的优化算法有限制，因此提出了三种任务导向的学习算法，以实现计算效率的资源分配，从而提高无线FL的算法可扩展性。我们还指出了一些需要进一步研究的问题。
</details></li>
</ul>
<hr>
<h2 id="Robust-GBDT-A-Novel-Gradient-Boosting-Model-for-Noise-Robust-Classification"><a href="#Robust-GBDT-A-Novel-Gradient-Boosting-Model-for-Noise-Robust-Classification" class="headerlink" title="Robust-GBDT: A Novel Gradient Boosting Model for Noise-Robust Classification"></a>Robust-GBDT: A Novel Gradient Boosting Model for Noise-Robust Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05067">http://arxiv.org/abs/2310.05067</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luojiaqimath/robust-gbdt">https://github.com/luojiaqimath/robust-gbdt</a></li>
<li>paper_authors: Jiaqi Luo, Yuedong Quan, Shixin Xu</li>
<li>for: 这个研究是为了提出一种可以处理标签杂音的高效搜寻算法，并且可以处理多类分类任务。</li>
<li>methods: 这个研究使用了进步的Gradient Boosting Decision Trees（GBDT）框架，并且引入了一些robust loss functions，以抵消标签杂音的影响。</li>
<li>results: 这个研究发现，使用Robust-GBDT模型可以获得更准确的预测结果，并且可以更好地处理杂音和类别偏心的问题。<details>
<summary>Abstract</summary>
Robust boosting algorithms have emerged as alternative solutions to traditional boosting techniques for addressing label noise in classification tasks. However, these methods have predominantly focused on binary classification, limiting their applicability to multi-class tasks. Furthermore, they encounter challenges with imbalanced datasets, missing values, and computational efficiency. In this paper, we establish that the loss function employed in advanced Gradient Boosting Decision Trees (GBDT), particularly Newton's method-based GBDT, need not necessarily exhibit global convexity. Instead, the loss function only requires convexity within a specific region. Consequently, these GBDT models can leverage the benefits of nonconvex robust loss functions, making them resilient to noise. Building upon this theoretical insight, we introduce a new noise-robust boosting model called Robust-GBDT, which seamlessly integrates the advanced GBDT framework with robust losses. Additionally, we enhance the existing robust loss functions and introduce a novel robust loss function, Robust Focal Loss, designed to address class imbalance. As a result, Robust-GBDT generates more accurate predictions, significantly enhancing its generalization capabilities, especially in scenarios marked by label noise and class imbalance. Furthermore, Robust-GBDT is user-friendly and can easily integrate existing open-source code, enabling it to effectively handle complex datasets while improving computational efficiency. Numerous experiments confirm the superiority of Robust-GBDT over other noise-robust methods.
</details>
<details>
<summary>摘要</summary>
强健的搜索算法已经出现为 tradicional boosting 技术的替代方案，以解决分类任务中的标签噪声问题。然而，这些方法主要集中在 binary 分类中，因此其可用性不高于多类任务。此外，它们还遇到了不均衡数据集、缺失值和计算效率的挑战。在这篇论文中，我们证明了 GBDT 模型使用的损失函数不必必须具有全局凸性。相反，损失函数只需要在特定区域内具有凸性。因此，这些 GBDT 模型可以利用不凸的 robust 损失函数，使其具有抗噪声的能力。基于这一理论发现，我们引入了一种新的噪声Robust GBDT 模型，该模型通过结合高级 GBDT 框架和robust损失函数来实现。此外，我们还改进了现有的robust损失函数，并引入了一种新的 Robust Focal Loss 函数，用于解决类异常现象。这使得 Robust-GBDT 可以生成更加准确的预测结果，从而提高其泛化能力，特别是在标签噪声和类异常的情况下。此外，Robust-GBDT 易于使用，可以轻松地 интеGRATE现有的开源代码，使其可以有效地处理复杂的数据集，同时提高计算效率。许多实验证明了 Robust-GBDT 的优越性。
</details></li>
</ul>
<hr>
<h2 id="Pushing-the-Limits-of-Pre-training-for-Time-Series-Forecasting-in-the-CloudOps-Domain"><a href="#Pushing-the-Limits-of-Pre-training-for-Time-Series-Forecasting-in-the-CloudOps-Domain" class="headerlink" title="Pushing the Limits of Pre-training for Time Series Forecasting in the CloudOps Domain"></a>Pushing the Limits of Pre-training for Time Series Forecasting in the CloudOps Domain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05063">http://arxiv.org/abs/2310.05063</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gerald Woo, Chenghao Liu, Akshat Kumar, Doyen Sahoo</li>
<li>for: 这篇论文旨在提供大规模时间序列预测数据集，以便进一步研究预测模型的预训练和扩展。</li>
<li>methods: 本研究使用云端操作（CloudOps）领域的三个大规模时间序列预测数据集，其中最大的数据集有比利они个观测值，以便进一步研究预训练和扩展时间序列模型。</li>
<li>results: 研究发现，这种预训练方法可以在大规模时间序列预测 task 上 achieve 27% 的错误减少，并且可以与 класиical 学习基eline 相比。<details>
<summary>Abstract</summary>
Time series has been left behind in the era of pre-training and transfer learning. While research in the fields of natural language processing and computer vision are enjoying progressively larger datasets to train massive models, the most popular time series datasets consist of only tens of thousands of time steps, limiting our ability to study the effectiveness of pre-training and scaling. Recent studies have also cast doubt on the need for expressive models and scale. To alleviate these issues, we introduce three large-scale time series forecasting datasets from the cloud operations (CloudOps) domain, the largest having billions of observations, enabling further study into pre-training and scaling of time series models. We build the empirical groundwork for studying pre-training and scaling of time series models and pave the way for future research by identifying a promising candidate architecture. We show that it is a strong zero-shot baseline and benefits from further scaling, both in model and dataset size. Accompanying these datasets and results is a suite of comprehensive benchmark results comparing classical and deep learning baselines to our pre-trained method - achieving a 27% reduction in error on the largest dataset. Code and datasets will be released.
</details>
<details>
<summary>摘要</summary>
时间序列已经被搁置在预训练和传输学习的时代之外。而自然语言处理和计算机视觉领域的研究正在拥有越来越大的数据集来训练庞大模型，而时间序列数据集仅有几万个时间步，限制了我们研究预训练和扩大的能力。最近的研究还把需要表达力强大的模型和扩大的疑问抛弃了出来。为了解决这些问题，我们介绍了三个大规模时间序列预测数据集，来自云操作（CloudOps）领域，最大数据集有数十亿个观察结果，使我们能够进一步研究预训练和扩大时间序列模型的效果。我们建立了时间序列模型预训练和扩大的基础实验，并证明了我们的方法是一个强大的零个shot基线，并且在模型和数据集大小增加时能够减少错误率。我们附加了这些数据集和结果，以及对классиical和深度学习基eline的比较，实现了最大数据集上的27%错误减少。代码和数据集将被公布。
</details></li>
</ul>
<hr>
<h2 id="Online-Learning-in-Contextual-Second-Price-Pay-Per-Click-Auctions"><a href="#Online-Learning-in-Contextual-Second-Price-Pay-Per-Click-Auctions" class="headerlink" title="Online Learning in Contextual Second-Price Pay-Per-Click Auctions"></a>Online Learning in Contextual Second-Price Pay-Per-Click Auctions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05047">http://arxiv.org/abs/2310.05047</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengxiao Zhang, Haipeng Luo</li>
<li>for: 本文研究在上下文敏感的Pay-Per-Click拍卖中进行在线学习，每轮都会收到一些上下文和一组广告，并需要对广告的点击率进行估计，以进行第二价拍卖。learner的目标是尽可能减少她的后悔，定义为她的总收益与一个假设的拍卖策略的差。</li>
<li>methods: 我们首先证明了在$T$轮后，learner可以达到$\sqrt{T}$的后悔，并且这是不可避免的，因为我们的算法和多重投机问题类似。然后，我们引用了最近的上下文敏感投机算法的进步，开发了两种实用的上下文拍卖算法：第一种使用对数权重方案和正方差误差，保持了同样的$\sqrt{T}$后悔 bound，而第二种通过简单的ε-胆策略将问题降到在线回归问题，尽管它的后悔 bound更差。</li>
<li>results: 我们在一个synthetic数据上进行了实验，并证明了我们的算法在实际应用中的有效性和优异性。<details>
<summary>Abstract</summary>
We study online learning in contextual pay-per-click auctions where at each of the $T$ rounds, the learner receives some context along with a set of ads and needs to make an estimate on their click-through rate (CTR) in order to run a second-price pay-per-click auction. The learner's goal is to minimize her regret, defined as the gap between her total revenue and that of an oracle strategy that always makes perfect CTR predictions. We first show that $\sqrt{T}$-regret is obtainable via a computationally inefficient algorithm and that it is unavoidable since our algorithm is no easier than the classical multi-armed bandit problem. A by-product of our results is a $\sqrt{T}$-regret bound for the simpler non-contextual setting, improving upon a recent work of [Feng et al., 2023] by removing the inverse CTR dependency that could be arbitrarily large. Then, borrowing ideas from recent advances on efficient contextual bandit algorithms, we develop two practically efficient contextual auction algorithms: the first one uses the exponential weight scheme with optimistic square errors and maintains the same $\sqrt{T}$-regret bound, while the second one reduces the problem to online regression via a simple epsilon-greedy strategy, albeit with a worse regret bound. Finally, we conduct experiments on a synthetic dataset to showcase the effectiveness and superior performance of our algorithms.
</details>
<details>
<summary>摘要</summary>
我们研究在上下文中的线上学习，在每个 $T$ 轮中，学习者接收一些上下文以及一组广告，并需要对各个广告的点击率（CTR）进行估计，以进行第二价格支付每击广告。学习者的目标是尽量减少她的恨觉，定义为她的总收入与一个假设总是正确地预测 CTR 的 oracle 策略的差距。我们首先证明了 $\sqrt{T}$-恨觉是可以实现的，并且这是不可避免的，因为我们的算法与经典多重武器问题相同。我们的结果还提供了 $\sqrt{T}$-恨觉 bound  для更加简单的非上下文化设定，超过最近的 [Feng et al., 2023] 的研究成果，并将 inverse CTR 依赖项消除。然后，我们借鉴了最近的上下文策略算法的进步，开发了两种实用的上下文拍卖算法：第一种使用几何质数分配方案和乐观方差 Error，保持了同样的 $\sqrt{T}$-恨觉 bound，而第二种将问题降到在线回归问题，通过简单的ε-赫赫策略，尽管它的恨觉 bound 较差。最后，我们在一个 sintetic 数据集上进行了实验，以展示我们的算法的效果和优于性。
</details></li>
</ul>
<hr>
<h2 id="Deep-Reinforcement-Learning-Based-Cross-Layer-Design-in-Terahertz-Mesh-Backhaul-Networks"><a href="#Deep-Reinforcement-Learning-Based-Cross-Layer-Design-in-Terahertz-Mesh-Backhaul-Networks" class="headerlink" title="Deep Reinforcement Learning Based Cross-Layer Design in Terahertz Mesh Backhaul Networks"></a>Deep Reinforcement Learning Based Cross-Layer Design in Terahertz Mesh Backhaul Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05034">http://arxiv.org/abs/2310.05034</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhifeng Hu, Chong Han, Xudong Wang</li>
<li>for: 这个论文是为了解决teraHertz（THz）网络中的跨层路由和长期资源分配问题，以提高未来无线后门系统的可扩展性和可靠性。</li>
<li>methods: 这个论文使用了深度强化学习（DRL）技术，实现跨层设计，包括路由对应和资源分配。在DRL方法中，使用了多任务结构，协助实现能源和子阵列的有效使用。此外，这个方法还使用了层次架构，实现每个基站的特定资源分配和学习知识传递。</li>
<li>results:  simulations 表明，DEFLECT routing 比 minimal hop-count metric 消耗更少的资源，并且不会导致包库损失和第二层延迟。此外，DEFLECT DRL 方法可以在1秒内从破损链路上复原资源有效地。<details>
<summary>Abstract</summary>
Supporting ultra-high data rates and flexible reconfigurability, Terahertz (THz) mesh networks are attractive for next-generation wireless backhaul systems that empower the integrated access and backhaul (IAB). In THz mesh backhaul networks, the efficient cross-layer routing and long-term resource allocation is yet an open problem due to dynamic traffic demands as well as possible link failures caused by the high directivity and high non-line-of-sight (NLoS) path loss of THz spectrum. In addition, unpredictable data traffic and the mixed integer programming property with the NP-hard nature further challenge the effective routing and long-term resource allocation design. In this paper, a deep reinforcement learning (DRL) based cross-layer design in THz mesh backhaul networks (DEFLECT) is proposed, by considering dynamic traffic demands and possible sudden link failures. In DEFLECT, a heuristic routing metric is first devised to facilitate resource efficiency (RE) enhancement regarding energy and sub-array usages. Furthermore, a DRL based resource allocation algorithm is developed to realize long-term RE maximization and fast recovery from broken links. Specifically in the DRL method, the exploited multi-task structure cooperatively benefits joint power and sub-array allocation. Additionally, the leveraged hierarchical architecture realizes tailored resource allocation for each base station and learned knowledge transfer for fast recovery. Simulation results show that DEFLECT routing consumes less resource, compared to the minimal hop-count metric. Moreover, unlike conventional DRL methods causing packet loss and second-level latency, DEFLECT DRL realizes the long-term RE maximization with no packet loss and millisecond-level latency, and recovers resource-efficient backhaul from broken links within 1s.
</details>
<details>
<summary>摘要</summary>
支持超高数据速率和灵活可重新配置，tera兆Hz（THz）网络是下一代无线备用系统的吸引力，它们可以强化集成访问和备用（IAB）。在THz网络中，有效的交叉层路由和长期资源分配仍然是一个开放的问题，因为动态的流量需求以及可能的链接故障，这些链接故障是由THz频谱的高直达性和高非直视线（NLoS）损失引起的。此外，不可预测的数据流量和混合整数编程性，以及NP困难的性质，进一步挑战了有效的路由和长期资源分配设计。在这篇论文中，一种基于深度学习（DRL）的交叉层设计方法（DEFLECT）被提出，该方法考虑了动态的流量需求和可能的突然链接故障。在DEFLECT中，一种帮助提高资源效率（RE）的启发式路由度量被开发，以便更好地利用能量和子频谱资源。此外，一种基于DRL的资源分配算法被开发，以实现长期RE最大化和快速从破断链接恢复。在DRL方法中，通过合作的多任务结构，对于每个基站的共享资源进行了优化。此外，通过利用层次结构，实现了个性化的资源分配和学习知识传递，以便快速恢复资源。 simulation结果表明，DEFLECT路由占用了较少的资源，相比于最小跳数 metric。此外，与传统DRL方法不同，DEFLECT DRL实现了长期RE最大化，无 packet loss和毫秒级延迟，并在1秒内从破断链接恢复资源。
</details></li>
</ul>
<hr>
<h2 id="Compressed-online-Sinkhorn"><a href="#Compressed-online-Sinkhorn" class="headerlink" title="Compressed online Sinkhorn"></a>Compressed online Sinkhorn</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05019">http://arxiv.org/abs/2310.05019</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fengpei Wang, Clarice Poon, Tony Shardlow<br>for:This paper focuses on the use of optimal transport (OT) distances and the Sinkhorn algorithm for large-scale data processing.methods:The paper revisits the online Sinkhorn algorithm introduced by Mensch and Peyr&#39;e in 2020, and improves the convergence analysis with a faster rate under certain parameter choices. Additionally, the paper proposes a compressed online Sinkhorn algorithm that combines measure compression techniques with the online Sinkhorn algorithm.results:The paper provides numerical results to verify the sharpness of the improved convergence rate, as well as practical numerical gains and theoretical guarantees on the efficiency of the compressed online Sinkhorn algorithm.<details>
<summary>Abstract</summary>
The use of optimal transport (OT) distances, and in particular entropic-regularised OT distances, is an increasingly popular evaluation metric in many areas of machine learning and data science. Their use has largely been driven by the availability of efficient algorithms such as the Sinkhorn algorithm. One of the drawbacks of the Sinkhorn algorithm for large-scale data processing is that it is a two-phase method, where one first draws a large stream of data from the probability distributions, before applying the Sinkhorn algorithm to the discrete probability measures. More recently, there have been several works developing stochastic versions of Sinkhorn that directly handle continuous streams of data. In this work, we revisit the recently introduced online Sinkhorn algorithm of [Mensch and Peyr\'e, 2020]. Our contributions are twofold: We improve the convergence analysis for the online Sinkhorn algorithm, the new rate that we obtain is faster than the previous rate under certain parameter choices. We also present numerical results to verify the sharpness of our result. Secondly, we propose the compressed online Sinkhorn algorithm which combines measure compression techniques with the online Sinkhorn algorithm. We provide numerical experiments to show practical numerical gains, as well as theoretical guarantees on the efficiency of our approach.
</details>
<details>
<summary>摘要</summary>
使用最优运输距离（OT）和特别是减 entropy 规范化OT距离作为评价指标，在机器学习和数据科学中越来越受欢迎。其使用主要受到高效算法如沟道算法的支持。然而，沟道算法在大规模数据处理中有一个缺点，即需要先从概率分布中筛选出大量数据，然后应用沟道算法来处理离散概率度量。最近，有几篇论文开发了直接处理连续流数据的Stochastic Sinkhorn算法。在这篇文章中，我们回顾了2020年Mensch和Peyr\'e提出的在线沟道算法。我们的贡献有两点：首先，我们提高了在线沟道算法的收敛分析，新的速率比旧速率在某些参数选择下更快。其次，我们提出了压缩在线沟道算法，该算法结合了度量压缩技术和在线沟道算法。我们提供了数据实验来证明我们的方法具有实际数值优势，以及理论保证。
</details></li>
</ul>
<hr>
<h2 id="Human-in-the-loop-The-future-of-Machine-Learning-in-Automated-Electron-Microscopy"><a href="#Human-in-the-loop-The-future-of-Machine-Learning-in-Automated-Electron-Microscopy" class="headerlink" title="Human-in-the-loop: The future of Machine Learning in Automated Electron Microscopy"></a>Human-in-the-loop: The future of Machine Learning in Automated Electron Microscopy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05018">http://arxiv.org/abs/2310.05018</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sergei V. Kalinin, Yongtao Liu, Arpan Biswas, Gerd Duscher, Utkarsh Pratiush, Kevin Roccapriore, Maxim Ziatdinov, Rama Vasudevan</li>
<li>for: 这篇论文主要是为了介绍机器学习技术在电子顾问中的应用，以及如何通过人工智能自动化实验来提高实验效率和准确性。</li>
<li>methods: 该论文使用的方法包括机器学习算法和APIs，用于实时分析和控制微scopes的数据和操作。</li>
<li>results: 该论文提出了一种新的实验方法，称为人类在循环（hAE），其中人类操作员监督实验的进行，并通过调整机器学习算法的策略来引导实验向特定目标进行。<details>
<summary>Abstract</summary>
Machine learning methods are progressively gaining acceptance in the electron microscopy community for de-noising, semantic segmentation, and dimensionality reduction of data post-acquisition. The introduction of the APIs by major instrument manufacturers now allows the deployment of ML workflows in microscopes, not only for data analytics but also for real-time decision-making and feedback for microscope operation. However, the number of use cases for real-time ML remains remarkably small. Here, we discuss some considerations in designing ML-based active experiments and pose that the likely strategy for the next several years will be human-in-the-loop automated experiments (hAE). In this paradigm, the ML learning agent directly controls beam position and image and spectroscopy acquisition functions, and human operator monitors experiment progression in real- and feature space of the system and tunes the policies of the ML agent to steer the experiment towards specific objectives.
</details>
<details>
<summary>摘要</summary>
Here, we discuss some considerations for designing machine learning-based active experiments and suggest that the most likely approach for the next few years will be human-in-the-loop automated experiments (hAE). In this paradigm, the machine learning agent directly controls the beam position and image and spectroscopy acquisition functions, while the human operator monitors the experiment's progress in real- and feature space and adjusts the policies of the machine learning agent to steer the experiment towards specific objectives.
</details></li>
</ul>
<hr>
<h2 id="Prompt-augmented-Temporal-Point-Process-for-Streaming-Event-Sequence"><a href="#Prompt-augmented-Temporal-Point-Process-for-Streaming-Event-Sequence" class="headerlink" title="Prompt-augmented Temporal Point Process for Streaming Event Sequence"></a>Prompt-augmented Temporal Point Process for Streaming Event Sequence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04993">http://arxiv.org/abs/2310.04993</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yanyanSann/PromptTPP">https://github.com/yanyanSann/PromptTPP</a></li>
<li>paper_authors: Siqiao Xue, Yan Wang, Zhixuan Chu, Xiaoming Shi, Caigao Jiang, Hongyan Hao, Gangwei Jiang, Xiaoyun Feng, James Y. Zhang, Jun Zhou</li>
<li>for: 本研究旨在 Addressing the challenge of continuous monitoring of Neural Temporal Point Processes (TPPs) for streaming event sequences, while ensuring privacy and memory constraints.</li>
<li>methods: 我们提出了一种简单 yet effective 框架 PromptTPP，它将基础 TPP 与一个 continuous-time retrieval prompt pool 集成，以便随着时间流动而学习流行事件序列。</li>
<li>results: 我们在三个实际用户行为数据集上展示了 PromptTPP 的优秀性能，并且在 Privacy 和 Memory 约束下实现了 Continual Learning。<details>
<summary>Abstract</summary>
Neural Temporal Point Processes (TPPs) are the prevalent paradigm for modeling continuous-time event sequences, such as user activities on the web and financial transactions. In real-world applications, event data is typically received in a \emph{streaming} manner, where the distribution of patterns may shift over time. Additionally, \emph{privacy and memory constraints} are commonly observed in practical scenarios, further compounding the challenges. Therefore, the continuous monitoring of a TPP to learn the streaming event sequence is an important yet under-explored problem. Our work paper addresses this challenge by adopting Continual Learning (CL), which makes the model capable of continuously learning a sequence of tasks without catastrophic forgetting under realistic constraints. Correspondingly, we propose a simple yet effective framework, PromptTPP\footnote{Our code is available at {\small \url{ https://github.com/yanyanSann/PromptTPP}}, by integrating the base TPP with a continuous-time retrieval prompt pool. The prompts, small learnable parameters, are stored in a memory space and jointly optimized with the base TPP, ensuring that the model learns event streams sequentially without buffering past examples or task-specific attributes. We present a novel and realistic experimental setup for modeling event streams, where PromptTPP consistently achieves state-of-the-art performance across three real user behavior datasets.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Waveformer-for-modelling-dynamical-systems"><a href="#Waveformer-for-modelling-dynamical-systems" class="headerlink" title="Waveformer for modelling dynamical systems"></a>Waveformer for modelling dynamical systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04990">http://arxiv.org/abs/2310.04990</a></li>
<li>repo_url: None</li>
<li>paper_authors: N Navaneeth, Souvik Chakraborty</li>
<li>for: 学习解析方程的解 solutions</li>
<li>methods: 使用wavelet变换和transformers来捕捉解的空间多尺度行为和远距离动态</li>
<li>results: 在四个数学示例中，waveformer可以准确地学习解析方程的解，并在推算区域中表现出优于现有状态 искусственный智能算法，具体来说，waveformer可以在推算区域中准确预测解的动态行为，并且其性能在推算区域中比现有算法更高出至少一个数量级。<details>
<summary>Abstract</summary>
Neural operators have gained recognition as potent tools for learning solutions of a family of partial differential equations. The state-of-the-art neural operators excel at approximating the functional relationship between input functions and the solution space, potentially reducing computational costs and enabling real-time applications. However, they often fall short when tackling time-dependent problems, particularly in delivering accurate long-term predictions. In this work, we propose "waveformer", a novel operator learning approach for learning solutions of dynamical systems. The proposed waveformer exploits wavelet transform to capture the spatial multi-scale behavior of the solution field and transformers for capturing the long horizon dynamics. We present four numerical examples involving Burgers's equation, KS-equation, Allen Cahn equation, and Navier Stokes equation to illustrate the efficacy of the proposed approach. Results obtained indicate the capability of the proposed waveformer in learning the solution operator and show that the proposed Waveformer can learn the solution operator with high accuracy, outperforming existing state-of-the-art operator learning algorithms by up to an order, with its advantage particularly visible in the extrapolation region
</details>
<details>
<summary>摘要</summary>
Neural operators have gained recognition as powerful tools for learning solutions of a family of partial differential equations. The state-of-the-art neural operators excel at approximating the functional relationship between input functions and the solution space, potentially reducing computational costs and enabling real-time applications. However, they often fall short when tackling time-dependent problems, particularly in delivering accurate long-term predictions. In this work, we propose "waveformer", a novel operator learning approach for learning solutions of dynamical systems. The proposed waveformer exploits wavelet transform to capture the spatial multi-scale behavior of the solution field and transformers for capturing the long horizon dynamics. We present four numerical examples involving Burgers's equation, KS-equation, Allen Cahn equation, and Navier Stokes equation to illustrate the efficacy of the proposed approach. Results obtained indicate the capability of the proposed waveformer in learning the solution operator and show that the proposed Waveformer can learn the solution operator with high accuracy, outperforming existing state-of-the-art operator learning algorithms by up to an order, with its advantage particularly visible in the extrapolation region.Note: Please note that the translation is in Simplified Chinese, and the word order and grammar may be different from Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Data-centric-Graph-Learning-A-Survey"><a href="#Data-centric-Graph-Learning-A-Survey" class="headerlink" title="Data-centric Graph Learning: A Survey"></a>Data-centric Graph Learning: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04987">http://arxiv.org/abs/2310.04987</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng Yang, Deyu Bo, Jixi Liu, Yufei Peng, Boyu Chen, Haoran Dai, Ao Sun, Yue Yu, Yixin Xiao, Qi Zhang, Chunchen Wang, Yuxin Guo, Chuan Shi</li>
<li>for: 本文旨在探讨如何在深度学习时更好地处理图数据，以提高图模型的能力。</li>
<li>methods: 本文使用数据中心的方法，包括修改图数据的方法，以提高图模型的性能。</li>
<li>results: 本文提出了一种基于图学习管道的新分类法，并分析了图数据中的一些潜在问题，以及如何在数据中心的方法下解决这些问题。<details>
<summary>Abstract</summary>
The history of artificial intelligence (AI) has witnessed the significant impact of high-quality data on various deep learning models, such as ImageNet for AlexNet and ResNet. Recently, instead of designing more complex neural architectures as model-centric approaches, the attention of AI community has shifted to data-centric ones, which focuses on better processing data to strengthen the ability of neural models. Graph learning, which operates on ubiquitous topological data, also plays an important role in the era of deep learning. In this survey, we comprehensively review graph learning approaches from the data-centric perspective, and aim to answer two crucial questions: (1) when to modify graph data and (2) how to modify graph data to unlock the potential of various graph models. Accordingly, we propose a novel taxonomy based on the stages in the graph learning pipeline, and highlight the processing methods for different data structures in the graph data, i.e., topology, feature and label. Furthermore, we analyze some potential problems embedded in graph data and discuss how to solve them in a data-centric manner. Finally, we provide some promising future directions for data-centric graph learning.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）的历史见证了高质量数据对各种深度学习模型的重要影响，如ImageNet对AlexNet和ResNet。在最近，AI社区的注意力转移到了数据中心的方法，而不是设计更复杂的神经网络模型。图学习，它在深度学习时代处理普遍的 topological 数据，也扮演着重要的角色。在本综述中，我们从数据中心的角度全面回顾图学习方法，并试图回答两个关键问题：（1）何时修改图数据，以及（2）如何修改图数据以解锁不同图模型的潜力。因此，我们提出了一种新的分类方法，基于图学习管道中的阶段，并高亮了不同数据结构在图数据中的处理方法，即 topological、特征和标签。此外，我们分析了图数据中的一些可能的问题，并讨论了如何在数据中心的方法下解决这些问题。最后，我们提出了一些未来的可能性，以推动数据中心的图学习发展。
</details></li>
</ul>
<hr>
<h2 id="Model-adapted-Fourier-sampling-for-generative-compressed-sensing"><a href="#Model-adapted-Fourier-sampling-for-generative-compressed-sensing" class="headerlink" title="Model-adapted Fourier sampling for generative compressed sensing"></a>Model-adapted Fourier sampling for generative compressed sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04984">http://arxiv.org/abs/2310.04984</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aaron Berk, Simone Brugiapaglia, Yaniv Plan, Matthew Scott, Xia Sheng, Ozgur Yilmaz</li>
<li>for: 研究卷积感知抽象，即从固定单位矩阵随机抽取测量矩阵，DFT为重要特殊情况。</li>
<li>methods: 构建基于模型的采样策略，以提高采样复杂性为$\textit{O}(kd|\boldsymbol{\alpha}|_{2}^{2})$。这由两个步骤组成：首先发展新的非均匀随机采样分布的理论回归保证，然后优化采样分布以最小化采样次数。</li>
<li>results: 提出了一种适用于自然信号类型的采样复杂性，该采样复杂性可以在低卷积频率下实现高准确性。此外，对于代表采样方案进行了实验 validate。<details>
<summary>Abstract</summary>
We study generative compressed sensing when the measurement matrix is randomly subsampled from a unitary matrix (with the DFT as an important special case). It was recently shown that $\textit{O}(kdn\| \boldsymbol{\alpha}\|_{\infty}^{2})$ uniformly random Fourier measurements are sufficient to recover signals in the range of a neural network $G:\mathbb{R}^k \to \mathbb{R}^n$ of depth $d$, where each component of the so-called local coherence vector $\boldsymbol{\alpha}$ quantifies the alignment of a corresponding Fourier vector with the range of $G$. We construct a model-adapted sampling strategy with an improved sample complexity of $\textit{O}(kd\| \boldsymbol{\alpha}\|_{2}^{2})$ measurements. This is enabled by: (1) new theoretical recovery guarantees that we develop for nonuniformly random sampling distributions and then (2) optimizing the sampling distribution to minimize the number of measurements needed for these guarantees. This development offers a sample complexity applicable to natural signal classes, which are often almost maximally coherent with low Fourier frequencies. Finally, we consider a surrogate sampling scheme, and validate its performance in recovery experiments using the CelebA dataset.
</details>
<details>
<summary>摘要</summary>
我们研究生成式压缩感知（Generative Compressed Sensing），当测量矩阵随机抽取 Unitary 矩阵（DFT 为重要特殊情况）时。最近研究表明，$kdn\| \mathbf{\alpha}\|_{\infty}^{2}$ 随机 Fourier 测量可以重建信号，其中 $\mathbf{\alpha}$ 是所谓的本地协同向量，其中每个分量表示对应的 Fourier 向量与 $G$ 函数（$G:\mathbb{R}^k \to \mathbb{R}^n$）的谱的对应。我们构建了适应模型的采样策略，其sample complexity 为 $kd\| \mathbf{\alpha}\|_{2}^{2}$ 测量。这是由以下两个步骤实现的：首先，我们开发了非均匀随机采样分布的新理论恢复保证；其次，我们优化采样分布，以最小化需要的测量数量。这一发展可以应用于自然的信号类型，其通常是高频率下几乎最大的协同。最后，我们考虑了代理采样方案，并通过 CelebA 数据集的实验验证其性能。
</details></li>
</ul>
<hr>
<h2 id="Understanding-the-Robustness-of-Multi-modal-Contrastive-Learning-to-Distribution-Shift"><a href="#Understanding-the-Robustness-of-Multi-modal-Contrastive-Learning-to-Distribution-Shift" class="headerlink" title="Understanding the Robustness of Multi-modal Contrastive Learning to Distribution Shift"></a>Understanding the Robustness of Multi-modal Contrastive Learning to Distribution Shift</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04971">http://arxiv.org/abs/2310.04971</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihao Xue, Siddharth Joshi, Dang Nguyen, Baharan Mirzasoleiman</li>
<li>for: 本文研究了multimodal contrastive learning（MMCL）方法在不同频谱上的表达学习，尤其是CLIP的成功。</li>
<li>methods: 本文使用了rigorous分析方法，探讨了MMCL的robustness机制，发现了两种机制：intra-class contrasting和inter-class feature sharing。</li>
<li>results: 本文的 teorical findings和实验结果表明，rich captions和annotating different types of details可以提高模型的robustness和zero-shot classification accuracy under distribution shift。<details>
<summary>Abstract</summary>
Recently, multimodal contrastive learning (MMCL) approaches, such as CLIP, have achieved a remarkable success in learning representations that are robust against distribution shift and generalize to new domains. Despite the empirical success, the mechanism behind learning such generalizable representations is not understood. In this work, we rigorously analyze this problem and uncover two mechanisms behind MMCL's robustness: \emph{intra-class contrasting}, which allows the model to learn features with a high variance, and \emph{inter-class feature sharing}, where annotated details in one class help learning other classes better. Both mechanisms prevent spurious features that are over-represented in the training data to overshadow the generalizable core features. This yields superior zero-shot classification accuracy under distribution shift. Furthermore, we theoretically demonstrate the benefits of using rich captions on robustness and explore the effect of annotating different types of details in the captions. We validate our theoretical findings through experiments, including a well-designed synthetic experiment and an experiment involving training CLIP on MS COCO and evaluating the model on variations of shifted ImageNet.
</details>
<details>
<summary>摘要</summary>
近期，多模态对照学习（MMCL）方法，如CLIP，在学习对分布变化robust的表示方面取得了非常成功。尽管在实际中取得了成功，但是这种机制的底层原理尚未了解。在这项工作中，我们仔细分析了这个问题，并揭示了MMCL的 robustness的两种机制：内类对照（intra-class contrasting），允许模型学习具有高方差的特征，以及间类特征共享（inter-class feature sharing），其中一个类中的注解细节帮助学习其他类。这两种机制使得模型不会由训练数据中的假样特征所掩蔽。这 führt zu einer superior zero-shot classification accuracy under distribution shift. 此外，我们也 theoretically demonstrate了使用丰富的描述对robustness带来的好处，并 explore了不同类型的描述在描述中的效果。我们 validate our theoretical findings through experiments, including a well-designed synthetic experiment and an experiment involving training CLIP on MS COCO and evaluating the model on variations of shifted ImageNet.
</details></li>
</ul>
<hr>
<h2 id="Improved-Active-Learning-via-Dependent-Leverage-Score-Sampling"><a href="#Improved-Active-Learning-via-Dependent-Leverage-Score-Sampling" class="headerlink" title="Improved Active Learning via Dependent Leverage Score Sampling"></a>Improved Active Learning via Dependent Leverage Score Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04966">http://arxiv.org/abs/2310.04966</a></li>
<li>repo_url: None</li>
<li>paper_authors: Atsushi Shimizu, Xiaoou Cheng, Christopher Musco, Jonathan Weare</li>
<li>for: 这 paper 的目的是提出一种改进的活动学习方法，用于在agnostic（对抗噪声） Setting 中提高学习效果。</li>
<li>methods: 这 paper 使用了marginal leverage score sampling 和 non-independent sampling策略，以提高 espacial coverage 和减少样本数量。具体来说，这 paper 提出了一种基于 pivotal sampling 算法的方法，并在 parametric PDEs 和 uncertainty quantification 中进行了测试。</li>
<li>results: 相比于独立 sampling，这 paper 的方法可以减少到达给定准确率的样本数量，提高了效率。此外，paper 还提供了两个理论结论：一是任何非独立 leveragescore sampling 方法，如果它符合弱一侧 $\ell_{\infty}$ 独立性条件，可以活动学习 $d$ 维线性函数，只需要 $O(d\log d)$ 个样本。这一结论扩展了 recient work on matrix Chernoff bounds under $\ell_{\infty}$ independence，并可能对其他 sampling 策略进行分析。二是，对于重要的多项式回归问题，我们的 pivotal 方法可以获得 $O(d)$ 个样本的 bound。<details>
<summary>Abstract</summary>
We show how to obtain improved active learning methods in the agnostic (adversarial noise) setting by combining marginal leverage score sampling with non-independent sampling strategies that promote spatial coverage. In particular, we propose an easily implemented method based on the pivotal sampling algorithm, which we test on problems motivated by learning-based methods for parametric PDEs and uncertainty quantification. In comparison to independent sampling, our method reduces the number of samples needed to reach a given target accuracy by up to $50\%$. We support our findings with two theoretical results. First, we show that any non-independent leverage score sampling method that obeys a weak one-sided $\ell_{\infty}$ independence condition (which includes pivotal sampling) can actively learn $d$ dimensional linear functions with $O(d\log d)$ samples, matching independent sampling. This result extends recent work on matrix Chernoff bounds under $\ell_{\infty}$ independence, and may be of interest for analyzing other sampling strategies beyond pivotal sampling. Second, we show that, for the important case of polynomial regression, our pivotal method obtains an improved bound of $O(d)$ samples.
</details>
<details>
<summary>摘要</summary>
我们展示了如何在agnostic（反对抗噪）设定中获得改进的活动学习方法，通过融合margin leverage score抽样和非独立抽样策略以提高空间覆盖率。具体而言，我们提出了一个容易实现的方法，基于pivotal抽样算法，并在parametric PDEs和 uncertainty quantification中的问题上进行测试。与独立抽样相比，我们的方法可以降低到 дости���了一定精度的样本数量，低于独立抽样的50%。我们支持我们的结果通过两个理论成果：首先，我们显示任何非独立leverage score抽样方法，只要满足弱一边 $\ell_{\infty}$ 独立性条件（包括pivotal抽样），可以活动地学习 $d$ 维Linear function，只需要 $O(d\log d)$ 样本，与独立抽样相同。这个结果推进了最近matrix Chernoff bounds under $\ell_{\infty}$ 独立性的研究，并可能适用于分析其他抽样策略。其次，我们显示，在重要的多项 regression问题上，我们的pivotal方法可以获得 $O(d)$ 样本的改进 bound。
</details></li>
</ul>
<hr>
<h2 id="Towards-Explainable-Machine-Learning-The-Effectiveness-of-Reservoir-Computing-in-Wireless-Receive-Processing"><a href="#Towards-Explainable-Machine-Learning-The-Effectiveness-of-Reservoir-Computing-in-Wireless-Receive-Processing" class="headerlink" title="Towards Explainable Machine Learning: The Effectiveness of Reservoir Computing in Wireless Receive Processing"></a>Towards Explainable Machine Learning: The Effectiveness of Reservoir Computing in Wireless Receive Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04956">http://arxiv.org/abs/2310.04956</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shashank Jere, Karim Said, Lizhong Zheng, Lingjia Liu<br>for: This paper aims to improve the performance of channel equalization in wireless communications using a learning-based technique called Reservoir Computing (RC) and provide a first principles-based understanding of its operation.methods: The paper uses an echo state network (ESN) as a channel equalizer and incorporates available domain knowledge in the form of wireless channel statistics into the weights of the ESN model. This optimized initialization of the model weights leads to improved receive processing&#x2F;symbol detection performance.results: The paper shows improved performance in receive processing&#x2F;symbol detection through simulations, demonstrating the effectiveness of the proposed approach. This is a first step towards explainable machine learning (XML) and assigning practical model interpretability that can be utilized to improve performance and enhance detection reliability.<details>
<summary>Abstract</summary>
Deep learning has seen a rapid adoption in a variety of wireless communications applications, including at the physical layer. While it has delivered impressive performance in tasks such as channel equalization and receive processing/symbol detection, it leaves much to be desired when it comes to explaining this superior performance. In this work, we investigate the specific task of channel equalization by applying a popular learning-based technique known as Reservoir Computing (RC), which has shown superior performance compared to conventional methods and other learning-based approaches. Specifically, we apply the echo state network (ESN) as a channel equalizer and provide a first principles-based signal processing understanding of its operation. With this groundwork, we incorporate the available domain knowledge in the form of the statistics of the wireless channel directly into the weights of the ESN model. This paves the way for optimized initialization of the ESN model weights, which are traditionally untrained and randomly initialized. Finally, we show the improvement in receive processing/symbol detection performance with this optimized initialization through simulations. This is a first step towards explainable machine learning (XML) and assigning practical model interpretability that can be utilized together with the available domain knowledge to improve performance and enhance detection reliability.
</details>
<details>
<summary>摘要</summary>
深度学习在无线通信应用中得到了迅速的推广，包括物理层。尽管它在channel等化和接收处理/符号检测等任务中表现出色，但它在解释这种超越性表现的问题上留下了很多不满。在这项工作中，我们调查了通道等化的特定任务，通过应用 популяр的学习基于技术——储池计算（RC），该技术在其他学习基于方法和其他学习基于技术上表现出优异。具体来说，我们使用echo state网络（ESN）作为通道等化器，并提供了基于信号处理的基本原理的操作理解。通过这种基础，我们将可用的频率频道知识直接 integrate到ESN模型的权重中。这种方法可以为ESN模型的初始化提供优化，传统上是Random initialization的。最后，我们通过 simulations 表明了增强接收处理/符号检测性能的改进。这是对机器学习（XML）的第一步，它可以让模型解释性得到实践应用，并与可用的频率频道知识结合使用，以提高性能并增强检测可靠性。
</details></li>
</ul>
<hr>
<h2 id="Information-Theoretic-Bounds-on-The-Removal-of-Attribute-Specific-Bias-From-Neural-Networks"><a href="#Information-Theoretic-Bounds-on-The-Removal-of-Attribute-Specific-Bias-From-Neural-Networks" class="headerlink" title="Information-Theoretic Bounds on The Removal of Attribute-Specific Bias From Neural Networks"></a>Information-Theoretic Bounds on The Removal of Attribute-Specific Bias From Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04955">http://arxiv.org/abs/2310.04955</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiazhi Li, Mahyar Khayatkhoei, Jiageng Zhu, Hanchen Xie, Mohamed E. Hussein, Wael AbdAlmageed</li>
<li>for: 本研究旨在探讨避免基于保护特征（如种族、性别、年龄）的神经网络预测中的偏见问题。</li>
<li>methods: 本研究使用了一些有前途的偏见除除法，但它们的局限性尚未得到充分探讨。</li>
<li>results: 研究发现，当数据集中存在强烈的偏见时，现有的偏见除除法只能在数据集中的偏见较弱时提供有效的性能。这些结论告诉我们在小型数据集中使用这些方法可能不够有效，并促使开发能够在强烈偏见情况下提供有效的方法。<details>
<summary>Abstract</summary>
Ensuring a neural network is not relying on protected attributes (e.g., race, sex, age) for predictions is crucial in advancing fair and trustworthy AI. While several promising methods for removing attribute bias in neural networks have been proposed, their limitations remain under-explored. In this work, we mathematically and empirically reveal an important limitation of attribute bias removal methods in presence of strong bias. Specifically, we derive a general non-vacuous information-theoretical upper bound on the performance of any attribute bias removal method in terms of the bias strength. We provide extensive experiments on synthetic, image, and census datasets to verify the theoretical bound and its consequences in practice. Our findings show that existing attribute bias removal methods are effective only when the inherent bias in the dataset is relatively weak, thus cautioning against the use of these methods in smaller datasets where strong attribute bias can occur, and advocating the need for methods that can overcome this limitation.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:保持神经网络不依赖保护属性（例如种族、性别、年龄）的预测是推进公正和可信的人工智能的关键。虽然一些有前途的属性偏见除除法已经被提出，但它们的限制尚未得到充分探讨。在这种工作中，我们数学和实验上 revela了属性偏见除除法的一个重要限制：即偏见强度的影响。我们 derivates一个普遍的非虚空的信息理论上限，用于衡量任何属性偏见除除法的性能。我们在 sintetic、图像和人口普查数据集上进行了广泛的实验，以验证理论上的上限和实际情况中的后果。我们的发现表明，现有的属性偏见除除法方法只有在数据集中的偏见强度较弱时才能够有效，因此对于小型数据集而言，存在强度偏见的情况下使用这些方法可能不太可靠，而需要开发能够超越这种限制的方法。
</details></li>
</ul>
<hr>
<h2 id="A-framework-to-generate-sparsity-inducing-regularizers-for-enhanced-low-rank-matrix-completion"><a href="#A-framework-to-generate-sparsity-inducing-regularizers-for-enhanced-low-rank-matrix-completion" class="headerlink" title="A framework to generate sparsity-inducing regularizers for enhanced low-rank matrix completion"></a>A framework to generate sparsity-inducing regularizers for enhanced low-rank matrix completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04954">http://arxiv.org/abs/2310.04954</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhi-Yong Wang, Hing Cheung So</li>
<li>for: 提出了一种架构，用于生成具有关闭式距离算子的SIR，并应用于矩阵完成低级别矩阵。</li>
<li>methods: 使用了半quadratic优化方法生成相关的regularizers，并使用了alternating direction method of multipliers（ADMM）开发算法。</li>
<li>results: 对约数据进行了extensive numerical experiments，并证明了方法的效果性和Runtime的优势。<details>
<summary>Abstract</summary>
Applying half-quadratic optimization to loss functions can yield the corresponding regularizers, while these regularizers are usually not sparsity-inducing regularizers (SIRs). To solve this problem, we devise a framework to generate an SIR with closed-form proximity operator. Besides, we specify our framework using several commonly-used loss functions, and produce the corresponding SIRs, which are then adopted as nonconvex rank surrogates for low-rank matrix completion. Furthermore, algorithms based on the alternating direction method of multipliers are developed. Extensive numerical results show the effectiveness of our methods in terms of recovery performance and runtime.
</details>
<details>
<summary>摘要</summary>
使半quadratice优化方法应用于损失函数可以得到相应的正则化项，但这些正则化项通常不是简洁化正则化项（SIR）。为解决这个问题，我们提出了一个框架，可以生成具有关闭形式距离运算器的SIR。此外，我们在多种通常使用的损失函数上Specify我们的框架，并生成相应的SIR，这些SIR然后被作为非对称矩阵完成的低级rankSurrogate采用。此外，我们还开发了基于多重方向分数法的算法。广泛的numerical实验表明我们的方法在完成性和运行时间方面具有良好的效果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/08/cs.LG_2023_10_08/" data-id="clp869u0q00svk5886po5d24q" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_10_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/08/eess.SP_2023_10_08/" class="article-date">
  <time datetime="2023-10-08T08:00:00.000Z" itemprop="datePublished">2023-10-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/08/eess.SP_2023_10_08/">eess.SP - 2023-10-08</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="5G-Advanced-Wireless-Channel-Virtualization-and-Resource-Mapping-for-Real-Time-Spectrum-Sharing"><a href="#5G-Advanced-Wireless-Channel-Virtualization-and-Resource-Mapping-for-Real-Time-Spectrum-Sharing" class="headerlink" title="5G Advanced: Wireless Channel Virtualization and Resource Mapping for Real Time Spectrum Sharing"></a>5G Advanced: Wireless Channel Virtualization and Resource Mapping for Real Time Spectrum Sharing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05271">http://arxiv.org/abs/2310.05271</a></li>
<li>repo_url: None</li>
<li>paper_authors: Walaa Alqwider, Aly Sabri Abdalla, Vuk Marojevic</li>
<li>for: 这个论文是为了探讨有效的频率资源利用和共享，以支持重要服务的协调访问。</li>
<li>methods: 本论文提出了一种基于无线通信通道虚拟化的现时频率共享技术，包括虚拟资源映射框架、类型映射和控制信号化。这种技术很少改变协议协议，并且透明于终端用户应用程序。</li>
<li>results: 作者验证了提议的技术，并发现了需要进一步研究设计有效的频率需求或频率使用预测信号的方法。<details>
<summary>Abstract</summary>
The coexistence between active wireless communications and passive RF spectrum use becomes an increasingly important requirement for coordinated spectrum access supporting critical services. The ongoing research and technological progress are focused on effective spectrum utilization including large-scale MIMO and energy efficient and low-power communications, innovative spectrum use and management, and resilient spectrum sharing, just to name a few. This paper introduces a new tool for real time spectrum sharing among emerging cellular networks and passive RF sensing systems used for remote sensing and radio astronomy, among others. Specifically we propose leveraging wireless channel virtualization and propose a virtual-to-physical resource mapping framework, mapping types, and control signaling that extends the current 5G New Radio (NR) specifications. Our technology introduces minimal changes to the protocol and is meant to be transparent to the end user application. We validate the proposed technology by extending a 3GPP compliant 5G NR downlink simulator and identify further research directions where work is needed on designing effective ways to explicitly signal the need for spectrum or spectrum use predictions.
</details>
<details>
<summary>摘要</summary>
“ aktive wireless 通信和温馈 RF 频率使用的共存变得越来越重要，以支持协调的频率访问，以满足重要服务的需求。当前的研究和技术进步都在注重有效的频率利用，包括大规模 MIMO 和能效的低功率通信，创新的频率使用和管理，以及可靠的频率分享。这篇文章介绍了一种新的实时频率分享工具，用于将趋起的Cellular网络和温馈 RF 感知系统连接起来，用于远程感知和天文学等。我们建议利用无线通道虚拟化，并提出了虚拟资源映射框架，类型和控制信号。我们的技术对协议做出了最小的改变，并且透明于应用程序端。我们验证了我们的技术，并确定了进一步的研究方向，包括设计有效的频率需求或频率使用预测的方法。”
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-deep-learning-framework-for-temperature-compensated-damage-assessment-using-ultrasonic-guided-waves-on-edge-device"><a href="#Unsupervised-deep-learning-framework-for-temperature-compensated-damage-assessment-using-ultrasonic-guided-waves-on-edge-device" class="headerlink" title="Unsupervised deep learning framework for temperature-compensated damage assessment using ultrasonic guided waves on edge device"></a>Unsupervised deep learning framework for temperature-compensated damage assessment using ultrasonic guided waves on edge device</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05154">http://arxiv.org/abs/2310.05154</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pankhi Kashyap, Kajal Shivgan, Sheetal Patil, Ramana Raja B, Sagar Mahajan, Sauvik Banerjee, Siddharth Tallur</li>
<li>for: 本研究旨在提出一种轻量级机器学习（TinyML）框架，以实现附加到边缘设备的深度学习模型，从而降低云计算和图形处理器（GPU）的需求，提高Structural Health Monitoring（SHM）系统的可扩展性和可行性。</li>
<li>methods: 本研究使用的方法包括：（1）使用TinyML框架开发轻量级深度学习模型，（2）采用不监督学习方法检测损害，（3）使用Xilinx Artix-7 FPGA进行数据收集和控制，以及边缘推理损害。</li>
<li>results: 本研究的结果表明，使用TinyML框架可以实现轻量级深度学习模型，并且可以在不同温度（0℃-90℃）下进行损害检测，并且可以实现边缘推理。<details>
<summary>Abstract</summary>
Fueled by the rapid development of machine learning (ML) and greater access to cloud computing and graphics processing units (GPUs), various deep learning based models have been proposed for improving performance of ultrasonic guided wave structural health monitoring (GW-SHM) systems, especially to counter complexity and heterogeneity in data due to varying environmental factors (e.g., temperature) and types of damages. Such models typically comprise of millions of trainable parameters, and therefore add to cost of deployment due to requirements of cloud connectivity and processing, thus limiting the scale of deployment of GW-SHM. In this work, we propose an alternative solution that leverages TinyML framework for development of light-weight ML models that could be directly deployed on embedded edge devices. The utility of our solution is illustrated by presenting an unsupervised learning framework for damage detection in honeycomb composite sandwich structure (HCSS) with disbond and delamination type of damages, validated using data generated by finite element (FE) simulations and experiments performed at various temperatures in the range 0{\deg}C to 90{\deg}C. We demonstrate a fully-integrated solution using a Xilinx Artix-7 FPGA for data acquisition and control, and edge-inference of damage.
</details>
<details>
<summary>摘要</summary>
驱动了机器学习（ML）的快速发展以及更多的云计算和图像处理器（GPU）的访问，各种深度学习基于模型已经被提议用于改善ultrasound guided wave结构Integrity monitoring systems（GW-SHM）的性能，特别是面临 complexity和不同环境因素（例如温度）以及不同类型的损害。这些模型通常包含数百万可训练参数，因此增加了部署成本，因为需要云连接和处理，从而限制了GW-SHM的规模。在这种工作中，我们提出了一个替代解决方案，利用TinyML框架 для开发轻量级ML模型，可以直接在嵌入式边缘设备上部署。我们的解决方案的有用性被示例了，通过对受损 Composite sandwich structure（HCSS）中的损害探测进行无监督学习框架，验证了在0℃至90℃的温度范围内进行了数据生成的Finite element（FE） simulations和实验。我们还示出了一个完全 интеGRATED解决方案，使用Xilinx Artix-7 FPGA进行数据收集和控制，以及边缘推理损害。
</details></li>
</ul>
<hr>
<h2 id="Secure-Short-Packet-Transmission-with-Aerial-Relaying-Blocklength-and-Trajectory-Co-Design"><a href="#Secure-Short-Packet-Transmission-with-Aerial-Relaying-Blocklength-and-Trajectory-Co-Design" class="headerlink" title="Secure Short-Packet Transmission with Aerial Relaying: Blocklength and Trajectory Co-Design"></a>Secure Short-Packet Transmission with Aerial Relaying: Blocklength and Trajectory Co-Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05142">http://arxiv.org/abs/2310.05142</a></li>
<li>repo_url: None</li>
<li>paper_authors: Milad Tatar Mamaghani, Xiangyun Zhou, Nan Yang, A. Lee Swindlehurst</li>
<li>for: 提高下一代互联网物联网（IoT）网络中secure短包通信（SPC）系统的总体机密通信性能。</li>
<li>methods: 利用无人飞行器（UAV）作为移动中继器，实现在地面潜在 listened 侦测器的存在下，可靠地和安全地交换干扰性短包。</li>
<li>results: 提出一种低复杂度算法，通过分解原问题为两个互相独立的子问题，以循环迭代法解决，实现优化性能。实验结果显示，提出的设计方案在其他参考方案相比，具有显著的性能改进。<details>
<summary>Abstract</summary>
In this paper, we propose a secure short-packet communication (SPC) system involving an unmanned aerial vehicle (UAV)-aided relay in the presence of a terrestrial passive eavesdropper. The considered system, which is applicable to various next-generation Internet-of-Things (IoT) networks, exploits a UAV as a mobile relay, facilitating the reliable and secure exchange of intermittent short packets between a pair of remote IoT devices with strict latency. Our objective is to improve the overall secrecy throughput performance of the system by carefully designing key parameters such as the coding blocklengths and the UAV trajectory. However, this inherently poses a challenging optimization problem that is difficult to solve optimally. To address the issue, we propose a low-complexity algorithm inspired by the block successive convex approximation approach, where we divide the original problem into two subproblems and solve them alternately until convergence. Numerical results demonstrate that the proposed design achieves significant performance improvements relative to other benchmarks, and offer valuable insights into determining appropriate coding blocklengths and UAV trajectory.
</details>
<details>
<summary>摘要</summary>
在本文中，我们提出了一种安全短包通信（SPC）系统，其中一架不可持续飞行器（UAV）作为中继器，在地面通信过悬挂的听写器存在下进行通信。这种系统适用于多种下一代互联网关系（IoT）网络，通过UAV作为移动中继器，实现了两个远程IoT设备之间的可靠和安全短包交换，并且具有严格的延迟要求。我们的目标是通过精心设计密钥参数，如编码块长度和UAV轨迹，提高整体密钥传输性能。但这会导致一个困难的优化问题，这个问题难以得到最优解。为了解决这个问题，我们提出了一种低复杂度算法，基于短SUCA（successive convex approximation）方法，将原始问题分解成两个互补问题，并在它们之间 alternate 到达循环至收敛。实验结果表明，提案的设计在其他参考模型的基础上具有显著的性能改进，并且为确定合适的编码块长度和UAV轨迹提供了有价值的洞察。
</details></li>
</ul>
<hr>
<h2 id="Decentralized-Federated-Learning-via-MIMO-Over-the-Air-Computation-Consensus-Analysis-and-Performance-Optimization"><a href="#Decentralized-Federated-Learning-via-MIMO-Over-the-Air-Computation-Consensus-Analysis-and-Performance-Optimization" class="headerlink" title="Decentralized Federated Learning via MIMO Over-the-Air Computation: Consensus Analysis and Performance Optimization"></a>Decentralized Federated Learning via MIMO Over-the-Air Computation: Consensus Analysis and Performance Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05075">http://arxiv.org/abs/2310.05075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiyuan Zhai, Xiaojun Yuan, Xin Wang</li>
<li>for: 采用分布式学习方法来处理大量无线数据，提高学习效率和精度。</li>
<li>methods: 使用分布式计算和多Input多Output（MIMO）技术，并提出了一种新的多输入多出力无线分布式学习（MIMO OA-DFL）框架。</li>
<li>results: 通过分析和实验研究，发现通信错误和混合矩阵的spectral gap对学习性能有显著影响，并提出了一种joint通信学习优化问题来优化发射器扬理和混合矩阵。<details>
<summary>Abstract</summary>
Decentralized federated learning (DFL), inherited from distributed optimization, is an emerging paradigm to leverage the explosively growing data from wireless devices in a fully distributed manner.DFL enables joint training of machine learning model under device to device (D2D) communication fashion without the coordination of a parameter server. However, the deployment of wireless DFL is facing some pivotal challenges. Communication is a critical bottleneck due to the required extensive message exchange between neighbor devices to share the learned model. Besides, consensus becomes increasingly difficult as the number of devices grows because there is no available central server to perform coordination. To overcome these difficulties, this paper proposes employing over-the-air computation (Aircomp) to improve communication efficiency by exploiting the superposition property of analog waveform in multi-access channels, and introduce the mixing matrix mechanism to promote consensus using the spectral property of symmetric doubly stochastic matrix. Specifically, we develop a novel multiple-input multiple-output over-the-air DFL (MIMO OA-DFL) framework to study over-the-air DFL problem over MIMO multiple access channels. We conduct a general convergence analysis to quantitatively capture the influence of aggregation weight and communication error on the MIMO OA-DFL performance in \emph{ad hoc} networks. The result shows that the communication error together with the spectral gap of mixing matrix has a significant impact on the learning performance. Based on this, a joint communication-learning optimization problem is formulated to optimize transceiver beamformers and mixing matrix. Extensive numerical experiments are performed to reveal the characteristics of different topologies and demonstrate the substantial learning performance enhancement of our proposed algorithm.
</details>
<details>
<summary>摘要</summary>
《分布式聚合学习（DFL）》是一种利用无线设备上爆发式增长的数据进行完全分布式的学习 paradigm。DFL可以在设备之间（D2D）进行共同训练机器学习模型，而无需协调参数服务器。然而，无线DFL的部署受到一些重要挑战。通信是一个关键的瓶颈，因为需要在邻居设备之间进行广泛的消息交换，以便分享学习模型。此外，在设备数量增加时，协调变得越来越困难，因为没有可用的中央服务器进行协调。为了解决这些困难，本文提出了使用空中计算（Aircomp）来提高通信效率，并利用多元Access通道上的超позиция性来实现。此外，我们还引入了混合矩阵机制来促进协调，使用幂等矩阵的特性来提高协调性。我们开发了一种多输入多输出（MIMO）无线DFL框架，以研究无线DFL问题在MIMO多接收通道上。我们进行了一种通用的叠加分析，以量化协调积分和通信错误对MIMO OA-DFL性能的影响。结果显示，通信错误以及混合矩阵的spectral gap具有显著影响学习性能。基于这些结果，我们提出了一个共同通信学习优化问题，以优化天线扬发器和混合矩阵。我们进行了广泛的数学实验，以描述不同的Topology特性和 demonstate MIMO OA-DFL性能的明显提高。
</details></li>
</ul>
<hr>
<h2 id="Performance-Analysis-of-RIS-Aided-Double-Spatial-Scattering-Modulation-for-mmWave-MIMO-Systems"><a href="#Performance-Analysis-of-RIS-Aided-Double-Spatial-Scattering-Modulation-for-mmWave-MIMO-Systems" class="headerlink" title="Performance Analysis of RIS-Aided Double Spatial Scattering Modulation for mmWave MIMO Systems"></a>Performance Analysis of RIS-Aided Double Spatial Scattering Modulation for mmWave MIMO Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05072">http://arxiv.org/abs/2310.05072</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xusheng Zhu, Wen Chen, Qingqing Wu, Jun Li, Nan Cheng, Fangjiong Chen, Changle Li</li>
<li>for: 这个论文研究了一种基于多Path传播的 millimeter-wave多输入多出力系统中的可重配置智能表面（RIS）-基于double Spatial Scattering Modulation（DSSM）的实用结构。</li>
<li>methods: 该论文提出了一种优化的探测器，其首先根据接收的扩散强度对扩散方向进行模式化，然后采用最大likelihood算法进行进一步的模式化。基于提出的优化探测器，我们 derivated了条件对数对Error probability表达式。</li>
<li>results: 通过两种不同的方法，我们 derivated了准确的数学积分和闭合式表达式，以及Upper bound和 asymptotic表达式。此外，我们还给出了RIS-DSSM方案的多样性 gain。进一步，我们通过组合UPEP和错误位数来获得了Union upper bound of average bit error probability（ABEP）。在实验中，我们验证了所 derivated的Upper bound和asymptotic表达式。发现在提posed system-based phase shift keying（PSK）中，ABEP性能比quadrature amplitude modulation（QAM）好。此外，随着RIS元素的增加，ABEP性能的提高更加明显。<details>
<summary>Abstract</summary>
In this paper, we investigate a practical structure of reconfigurable intelligent surface (RIS)-based double spatial scattering modulation (DSSM) for millimeter-wave (mmWave) multiple-input multiple-output (MIMO) systems. A suboptimal detector is proposed, in which the beam direction is first demodulated according to the received beam strength, and then the remaining information is demodulated by adopting the maximum likelihood algorithm. Based on the proposed suboptimal detector, we derive the conditional pairwise error probability expression. Further, the exact numerical integral and closed-form expressions of unconditional pairwise error probability (UPEP) are derived via two different approaches. To provide more insights, we derive the upper bound and asymptotic expressions of UPEP. In addition, the diversity gain of the RIS-DSSM scheme was also given. Furthermore, the union upper bound of average bit error probability (ABEP) is obtained by combining the UPEP and the number of error bits. Simulation results are provided to validate the derived upper bound and asymptotic expressions of ABEP. We found an interesting phenomenon that the ABEP performance of the proposed system-based phase shift keying is better than that of the quadrature amplitude modulation. Additionally, the performance advantage of ABEP is more significant with the increase in the number of RIS elements.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了一种实用的可重新配置智能表面（RIS）基于Double Spatial Scattering Modulation（DSSM）的 millimeter 波（mmWave）多输入多出力（MIMO）系统。我们提出了一种不优的探测器，其中首先根据接收到的磁场强度将磁场方向进行模式化，然后采用最大 LIKElihood算法进行模式化。基于我们提出的不优探测器，我们 deriv了Conditional Pairwise Error Probability表达式。进一步，我们通过两种不同的方法 deriv了无条件 Pairwise Error Probability（UPEP）的精确数学 интеграル和闭合式表达式。为了提供更多的意见，我们 deriv了UPEP的上界和极限表达式。此外，我们还给出了RIS-DSSM方案的多样性收益。此外，我们通过将UPEP和错误比特数相加来获得了Union Upper Bound of Average Bit Error Probability（ABEP）的Upper bound。我们通过实验 validate了我们 deriv的Upper bound和极限表达式。我们发现了一种有趣的现象，即提posed systembased phase shift keying的ABEP性能比quadrature amplitude modulation更好。此外，ABEP性能的提升程度随RIS元素的增加而更加显著。
</details></li>
</ul>
<hr>
<h2 id="Robust-matrix-completion-via-Novel-M-estimator-Functions"><a href="#Robust-matrix-completion-via-Novel-M-estimator-Functions" class="headerlink" title="Robust matrix completion via Novel M-estimator Functions"></a>Robust matrix completion via Novel M-estimator Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04953">http://arxiv.org/abs/2310.04953</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhi-Yong Wang, Hing Cheung So</li>
<li>for: robust matrix completion</li>
<li>methods:  generates a class of nonconvex functions to down-weight outlier-corrupted observations, and develops efficient algorithms based on these functions</li>
<li>results:  superior recovery accuracy and runtime compared to competitors<details>
<summary>Abstract</summary>
M-estmators including the Welsch and Cauchy have been widely adopted for robustness against outliers, but they also down-weigh the uncontaminated data. To address this issue, we devise a framework to generate a class of nonconvex functions which only down-weigh outlier-corrupted observations. Our framework is then applied to the Welsch, Cauchy and $\ell_p$-norm functions to produce the corresponding robust loss functions. Targeting on the application of robust matrix completion, efficient algorithms based on these functions are developed and their convergence is analyzed. Finally, extensive numerical results demonstrate that the proposed methods are superior to the competitors in terms of recovery accuracy and runtime.
</details>
<details>
<summary>摘要</summary>
M-estimators including Welsch 和 Cauchy 已经广泛采用，但它们也会减小不受污染的数据。为解决这个问题，我们提出了一个框架，生成一类非凸函数，只有在受污染观测值时减小。我们然后应用这个框架到 Welsch、Cauchy 和 $\ell_p$-norm 函数，生成相应的Robust loss函数。我们然后开发了高效的算法，并分析它们的收敛性。最后，我们在大量的数据上进行了丰富的数据测试，并证明了我们的方法在准确率和运行时间上都高于竞争对手。Note: "M-estimators" is translated as "M-估计器" in Simplified Chinese, and "robust loss functions" is translated as "Robust 损失函数".
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/08/eess.SP_2023_10_08/" data-id="clp869u9d01ffk588cw0n36a5" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_10_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/07/cs.SD_2023_10_07/" class="article-date">
  <time datetime="2023-10-07T15:00:00.000Z" itemprop="datePublished">2023-10-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/07/cs.SD_2023_10_07/">cs.SD - 2023-10-07</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="SA-Paraformer-Non-autoregressive-End-to-End-Speaker-Attributed-ASR"><a href="#SA-Paraformer-Non-autoregressive-End-to-End-Speaker-Attributed-ASR" class="headerlink" title="SA-Paraformer: Non-autoregressive End-to-End Speaker-Attributed ASR"></a>SA-Paraformer: Non-autoregressive End-to-End Speaker-Attributed ASR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04863">http://arxiv.org/abs/2310.04863</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yangze Li, Fan Yu, Yuhao Liang, Pengcheng Guo, Mohan Shi, Zhihao Du, Shiliang Zhang, Lei Xie</li>
<li>for: 这个研究是为了提高自动话语识别（ASR）和Speaker Diagnosis（SD）的融合模型，以提高会议记录的准确率。</li>
<li>methods: 该研究使用了一种新的非自然语言模型（Paraformer），并提出了一种 speaker-filling 策略和一种 inter-CTC 策略来提高模型的性能。</li>
<li>results: 实验结果表明，我们的模型在 AliMeeting 数据集上比 cascaded SA-ASR 模型减少了6.1%的相对Speaker-dependent Character Error Rate（SD-CER），并且与 SOTA 联合 AR SA-ASR 模型的 SD-CER 相似（34.8%），但具有1&#x2F;10 RTF。<details>
<summary>Abstract</summary>
Joint modeling of multi-speaker ASR and speaker diarization has recently shown promising results in speaker-attributed automatic speech recognition (SA-ASR).Although being able to obtain state-of-the-art (SOTA) performance, most of the studies are based on an autoregressive (AR) decoder which generates tokens one-by-one and results in a large real-time factor (RTF). To speed up inference, we introduce a recently proposed non-autoregressive model Paraformer as an acoustic model in the SA-ASR model.Paraformer uses a single-step decoder to enable parallel generation, obtaining comparable performance to the SOTA AR transformer models. Besides, we propose a speaker-filling strategy to reduce speaker identification errors and adopt an inter-CTC strategy to enhance the encoder's ability in acoustic modeling. Experiments on the AliMeeting corpus show that our model outperforms the cascaded SA-ASR model by a 6.1% relative speaker-dependent character error rate (SD-CER) reduction on the test set. Moreover, our model achieves a comparable SD-CER of 34.8% with only 1/10 RTF compared with the SOTA joint AR SA-ASR model.
</details>
<details>
<summary>摘要</summary>
joint模型化多speaker ASR和speaker分类有最近显示出色的结果在speaker所有自动语音识别(SA-ASR)中。尽管能够获得状态之arte(SOTA)性能，大多数研究都基于一个autoregressive(AR) decoder，这会导致大量的实时因子(RTF)。为加速推理，我们引入了一个非autoregressive模型Paraformer作为SA-ASR模型中的声学模型。Paraformer使用单步decoder，以便并行生成，与SOTA AR transformer模型相当。此外，我们提出了一种speaker填充策略，以减少speaker认定错误，并采用一种inter-CTC策略，以提高encoder在声学模型中的能力。在AliMeeting corpus上的实验表明，我们的模型比杂合SA-ASR模型的测试集比例6.1%的相对speaker依赖字符错误率(SD-CER)下降。此外，我们的模型与SOTA联合AR SA-ASR模型相当的SD-CER值为34.8%，只需1/10 RTF。
</details></li>
</ul>
<hr>
<h2 id="FM-Tone-Transfer-with-Envelope-Learning"><a href="#FM-Tone-Transfer-with-Envelope-Learning" class="headerlink" title="FM Tone Transfer with Envelope Learning"></a>FM Tone Transfer with Envelope Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04811">http://arxiv.org/abs/2310.04811</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fcaspe/fmtransfer">https://github.com/fcaspe/fmtransfer</a></li>
<li>paper_authors: Franco Caspe, Andrew McPherson, Mark Sandler</li>
<li>for: 本研究旨在控制生成的音频使用 musical instruments，提高表达性和短语表达能力。</li>
<li>methods: 本文提出了 Envelope Learning，一种新的 tone transfer 建模方法，通过在生成参数层次使用教学目标来映射音乐事件。</li>
<li>results: 本研究实现了在实时演奏场景中提高音频表达性、短语表达能力和多样性，并实现了精准地捕捉音频事件的开始和结束。<details>
<summary>Abstract</summary>
Tone Transfer is a novel deep-learning technique for interfacing a sound source with a synthesizer, transforming the timbre of audio excerpts while keeping their musical form content. Due to its good audio quality results and continuous controllability, it has been recently applied in several audio processing tools. Nevertheless, it still presents several shortcomings related to poor sound diversity, and limited transient and dynamic rendering, which we believe hinder its possibilities of articulation and phrasing in a real-time performance context.   In this work, we present a discussion on current Tone Transfer architectures for the task of controlling synthetic audio with musical instruments and discuss their challenges in allowing expressive performances. Next, we introduce Envelope Learning, a novel method for designing Tone Transfer architectures that map musical events using a training objective at the synthesis parameter level. Our technique can render note beginnings and endings accurately and for a variety of sounds; these are essential steps for improving musical articulation, phrasing, and sound diversity with Tone Transfer. Finally, we implement a VST plugin for real-time live use and discuss possibilities for improvement.
</details>
<details>
<summary>摘要</summary>
<<SYS>>采用深度学习技术，音源与 sintizer之间的 Tone Transfer 技术可以改变音频片断的时域特征，同时保持音乐形式内容。由于其音质效果良好和连续可控，因此在多个音频处理工具中应用。然而，它仍然存在许多缺点，如音色缺乏多样性和过渡和动态渲染的限制，这些缺点阻碍了 Tone Transfer 的表达和phrase演奏的可能性。在这项工作中，我们介绍了目前 Tone Transfer 架构的问题，以及控制 synthetic audio 的乐器的挑战。然后，我们介绍了 Envelope Learning，一种新的方法，可以在 synthesis 参数层次上使用 musical event 来训练 Tone Transfer 架构。我们的技术可以准确地渲染音乐事件的开始和结束，并且可以为不同的音频种类实现多样化。这些步骤对于提高 Tone Transfer 的表达、phrase 和音色多样性是关键。最后，我们实现了一个 VST 插件，用于实时现场使用，并讨论了进一步改进的可能性。
</details></li>
</ul>
<hr>
<h2 id="Multi-objective-Progressive-Clustering-for-Semi-supervised-Domain-Adaptation-in-Speaker-Verification"><a href="#Multi-objective-Progressive-Clustering-for-Semi-supervised-Domain-Adaptation-in-Speaker-Verification" class="headerlink" title="Multi-objective Progressive Clustering for Semi-supervised Domain Adaptation in Speaker Verification"></a>Multi-objective Progressive Clustering for Semi-supervised Domain Adaptation in Speaker Verification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04760">http://arxiv.org/abs/2310.04760</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ze Li, Yuke Lin, Ning Jiang, Xiaoyi Qin, Guoqing Zhao, Haiying Wu, Ming Li</li>
<li>for: 这个论文旨在提出一种新的半监督领域对应方法，专门针对语音识别 задачі。</li>
<li>methods: 本文使用限定目标领域的标签数据来 derivate 对应领域的领域特有描述子，并运用InfoMap算法进行嵌入聚类，以更正target领域的伪标签。此外，我们还引入subcenter-purification和进度融合策略来进一步改善伪标签的质量。</li>
<li>results: 本文的提案方法（Multi-objective Progressive Clustering，MoPC）在VoxSRC 2023 track 3的评估集上获得4.95% EER，排名第一名。此外，我们还进行了额外的实验在FFSVC dataset上，取得了良好的结果。<details>
<summary>Abstract</summary>
Utilizing the pseudo-labeling algorithm with large-scale unlabeled data becomes crucial for semi-supervised domain adaptation in speaker verification tasks. In this paper, we propose a novel pseudo-labeling method named Multi-objective Progressive Clustering (MoPC), specifically designed for semi-supervised domain adaptation. Firstly, we utilize limited labeled data from the target domain to derive domain-specific descriptors based on multiple distinct objectives, namely within-graph denoising, intra-class denoising and inter-class denoising. Then, the Infomap algorithm is adopted for embedding clustering, and the descriptors are leveraged to further refine the target domain's pseudo-labels. Moreover, to further improve the quality of pseudo labels, we introduce the subcenter-purification and progressive-merging strategy for label denoising. Our proposed MoPC method achieves 4.95% EER and ranked the 1$^{st}$ place on the evaluation set of VoxSRC 2023 track 3. We also conduct additional experiments on the FFSVC dataset and yield promising results.
</details>
<details>
<summary>摘要</summary>
使用大规模无标签数据的 Pseudo-labeling 算法在语音识别任务中成为了重要的 semi-supervised 领域适应技术。在这篇论文中，我们提出了一种新的 Pseudo-labeling 方法，即 Multi-objective Progressive Clustering (MoPC)，特意设计用于 semi-supervised 领域适应。首先，我们利用目标频道的有限标注数据来 derive 频道特有的描述符，基于多个不同的目标函数，即在 Graph 中的内部干扰、类内干扰和类间干扰。然后，我们采用 Infomap 算法进行嵌入聚类，并使用描述符进一步改进目标频道的 Pseudo-标签。此外，为了进一步提高 Pseudo-标签 的质量，我们引入 subcenter-purification 和 progressive-merging 策略来进行标签干扰。我们的提出的 MoPC 方法在 VoxSRC 2023 评测集上取得了 4.95% EER，并在评测集上排名第一。我们还进行了额外的 FFSVC 数据集的实验，并取得了有优的结果。
</details></li>
</ul>
<hr>
<h2 id="An-Exploration-of-Task-decoupling-on-Two-stage-Neural-Post-Filter-for-Real-time-Personalized-Acoustic-Echo-Cancellation"><a href="#An-Exploration-of-Task-decoupling-on-Two-stage-Neural-Post-Filter-for-Real-time-Personalized-Acoustic-Echo-Cancellation" class="headerlink" title="An Exploration of Task-decoupling on Two-stage Neural Post Filter for Real-time Personalized Acoustic Echo Cancellation"></a>An Exploration of Task-decoupling on Two-stage Neural Post Filter for Real-time Personalized Acoustic Echo Cancellation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04715">http://arxiv.org/abs/2310.04715</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihan Zhang, Jiayao Sun, Xianjun Xia, Ziqian Wang, Xiaopeng Yan, Yijian Xiao, Lei Xie</li>
<li>for: 这个论文旨在探讨personalized acoustic echo cancellation (PAEC)中的任务解耦策略，以及如何使用多尺度本地-全球speaker表示来提高speaker抽象。</li>
<li>methods: 这个论文提出了一种基于两阶段任务解耦post-filter (TDPF)的PAEC方法，并应用了多尺度本地-全球speaker表示来提高speaker抽象。</li>
<li>results: 实验结果表明，任务解耦模型可以比单一联合网络提供更好的性能，而且在任务解耦序列中，优化训练策略可以further提高模型的性能。<details>
<summary>Abstract</summary>
Deep learning based techniques have been popularly adopted in acoustic echo cancellation (AEC). Utilization of speaker representation has extended the frontier of AEC, thus attracting many researchers' interest in personalized acoustic echo cancellation (PAEC). Meanwhile, task-decoupling strategies are widely adopted in speech enhancement. To further explore the task-decoupling approach, we propose to use a two-stage task-decoupling post-filter (TDPF) in PAEC. Furthermore, a multi-scale local-global speaker representation is applied to improve speaker extraction in PAEC. Experimental results indicate that the task-decoupling model can yield better performance than a single joint network. The optimal approach is to decouple the echo cancellation from noise and interference speech suppression. Based on the task-decoupling sequence, optimal training strategies for the two-stage model are explored afterwards.
</details>
<details>
<summary>摘要</summary>
深度学习基于技术已经广泛应用于声学噪声抑制（AEC）领域。通过使用说话者表示方法，AEC的前iers延伸了，因此吸引了许多研究人员的关注。同时，任务解耦策略广泛应用于speech enhancement。为了进一步探索任务解耦方法，我们提议使用两个阶段任务解耦后filter（TDPF）在PAEC中。此外，我们采用多尺度本地-全球说话者表示方法来提高PAEC中的说话者抽取。实验结果表明，任务解耦模型可以在PAEC中提供更好的性能。最佳方法是在任务解耦序列中分离噪声和干扰音频抑制。基于任务解耦序列，我们后续探索了两个阶段模型的优化训练策略。
</details></li>
</ul>
<hr>
<h2 id="Spike-Triggered-Contextual-Biasing-for-End-to-End-Mandarin-Speech-Recognition"><a href="#Spike-Triggered-Contextual-Biasing-for-End-to-End-Mandarin-Speech-Recognition" class="headerlink" title="Spike-Triggered Contextual Biasing for End-to-End Mandarin Speech Recognition"></a>Spike-Triggered Contextual Biasing for End-to-End Mandarin Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04657">http://arxiv.org/abs/2310.04657</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaixun Huang, Ao Zhang, Binbin Zhang, Tianyi Xu, Xingchen Song, Lei Xie</li>
<li>for: 提高自动语音识别（ASR）系统对 Contextual Phrases 的识别性能</li>
<li>methods: 使用 Attention-based Deep Contextual Biasing 方法，同时支持显式和隐式偏袋</li>
<li>results: 实现了对 Contextual Phrases 的显著改进（32.0% 相对 CER 减少），并可以与浅混合方法相互协作以获得更好的结果<details>
<summary>Abstract</summary>
The attention-based deep contextual biasing method has been demonstrated to effectively improve the recognition performance of end-to-end automatic speech recognition (ASR) systems on given contextual phrases. However, unlike shallow fusion methods that directly bias the posterior of the ASR model, deep biasing methods implicitly integrate contextual information, making it challenging to control the degree of bias. In this study, we introduce a spike-triggered deep biasing method that simultaneously supports both explicit and implicit bias. Moreover, both bias approaches exhibit significant improvements and can be cascaded with shallow fusion methods for better results. Furthermore, we propose a context sampling enhancement strategy and improve the contextual phrase filtering algorithm. Experiments on the public WenetSpeech Mandarin biased-word dataset show a 32.0% relative CER reduction compared to the baseline model, with an impressively 68.6% relative CER reduction on contextual phrases.
</details>
<details>
<summary>摘要</summary>
针对给定的上下文表达，基于注意力的深度上下文偏好方法已经证明可以有效提高端到端自动语音识别（ASR）系统的识别性能。与浅层融合方法不同的是，深度偏好方法不直接偏袋ASR模型的 posterior，而是通过隐式地整合上下文信息，使其控制上下文偏好的难度增加。在本研究中，我们提出了触发器驱动的深度偏好方法，可以同时支持显式和隐式偏好。此外，两种偏好方法都展现出了显著的改善，可以与浅层融合方法相互协同使用。此外，我们还提出了上下文采样优化策略和上下文表达过滤算法改进。实验结果表明，在公共的WenetSpeech普通话biased-word数据集上，与基eline模型相比，我们的模型可以 дости到32.0%的相对报告错误率（CER）减少，其中上下文表达减少了68.6%的相对CER。
</details></li>
</ul>
<hr>
<h2 id="Neural2Speech-A-Transfer-Learning-Framework-for-Neural-Driven-Speech-Reconstruction"><a href="#Neural2Speech-A-Transfer-Learning-Framework-for-Neural-Driven-Speech-Reconstruction" class="headerlink" title="Neural2Speech: A Transfer Learning Framework for Neural-Driven Speech Reconstruction"></a>Neural2Speech: A Transfer Learning Framework for Neural-Driven Speech Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04644">http://arxiv.org/abs/2310.04644</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cctn-bci/neural2speech">https://github.com/cctn-bci/neural2speech</a></li>
<li>paper_authors: Jiawei Li, Chunxu Guo, Li Fu, Lu Fan, Edward F. Chang, Yuanning Li</li>
<li>for: 实现直接通过脑电传输机制进行脑语言交流，这是脑机器人界的一个重要目标。</li>
<li>methods: 我们提出了一个名为Neural2Speech的专业转移学习框架，这个框架包括两个不同的训练阶段。首先，我们使用可以在各种语音数据库中找到的语音自动化器进行预训练，以将语音波形从对应的语音表现中解析出来。其次，我们使用小型脑电资料进行适应器的训练，以将脑活动和语音表现相互调整。</li>
<li>results: 我们的提案Neural2Speech可以实现从脑电资料中重建语音，即使只有20分钟的脑电资料。与之比较，我们的方法在语音质量和准确性方面表现出色，较之前的基eline方法更高。<details>
<summary>Abstract</summary>
Reconstructing natural speech from neural activity is vital for enabling direct communication via brain-computer interfaces. Previous efforts have explored the conversion of neural recordings into speech using complex deep neural network (DNN) models trained on extensive neural recording data, which is resource-intensive under regular clinical constraints. However, achieving satisfactory performance in reconstructing speech from limited-scale neural recordings has been challenging, mainly due to the complexity of speech representations and the neural data constraints. To overcome these challenges, we propose a novel transfer learning framework for neural-driven speech reconstruction, called Neural2Speech, which consists of two distinct training phases. First, a speech autoencoder is pre-trained on readily available speech corpora to decode speech waveforms from the encoded speech representations. Second, a lightweight adaptor is trained on the small-scale neural recordings to align the neural activity and the speech representation for decoding. Remarkably, our proposed Neural2Speech demonstrates the feasibility of neural-driven speech reconstruction even with only 20 minutes of intracranial data, which significantly outperforms existing baseline methods in terms of speech fidelity and intelligibility.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本到简化中文。</SYS>>直接通过脑机器 interfaces 的沟通是重构自然语音的重要任务。先前的努力已经探索了使用复杂的深度神经网络（DNN）模型，通过大量神经记录数据进行训练，以实现语音重构。然而，在有限规模神经记录数据的情况下，实现满意的语音重构性能是困难的，主要是因为语音表示的复杂性和神经数据约束。为了解决这些挑战，我们提出了一种基于传输学习的神经驱动 speech reconstruction 框架，称为 Neural2Speech，该框架包括两个不同的训练阶段。首先，一个语音自适应器在可用的语音资料上进行预训练，以解码语音波形从编码的语音表示中。其次，一个轻量级的适配器在小规模神经记录数据上进行训练，以对神经活动和语音表示进行对应。值得注意的是，我们提出的 Neural2Speech 已经在只有 20 分钟的脑内数据上达到了比较出色的语音质量和可读性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/07/cs.SD_2023_10_07/" data-id="clp869u3i010ik588a0zt4qk5" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_10_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/07/cs.CV_2023_10_07/" class="article-date">
  <time datetime="2023-10-07T13:00:00.000Z" itemprop="datePublished">2023-10-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/07/cs.CV_2023_10_07/">cs.CV - 2023-10-07</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="DISCOVER-Making-Vision-Networks-Interpretable-via-Competition-and-Dissection"><a href="#DISCOVER-Making-Vision-Networks-Interpretable-via-Competition-and-Dissection" class="headerlink" title="DISCOVER: Making Vision Networks Interpretable via Competition and Dissection"></a>DISCOVER: Making Vision Networks Interpretable via Competition and Dissection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04929">http://arxiv.org/abs/2310.04929</a></li>
<li>repo_url: None</li>
<li>paper_authors: Konstantinos P. Panousis, Sotirios Chatzis</li>
<li>for: 这个论文旨在提高深度网络的后期解释性，特别是网络解剖。我们的目标是为视觉任务训练的网络提供一个框架，以便更好地发现每个神经元的个人功能。</li>
<li>methods: 我们利用了最新的视觉语言模型和网络层的新概念——随机地方竞争的线性单元。只有小量的层神经元被激活，导致activation sparse度 extremly low（只有$\approx 4%$）。我们的提posed方法可以推断（稀疏）神经元活动模式，使神经元可以activate&#x2F;特化于输入特征。</li>
<li>results: 我们的方法可以保持或改进视觉网络的分类性能，同时实现了一种原则性的文本基于描述和网络神经元表示的框架。在我们的实验中，我们发现：（i）我们的方法可以提高网络的分类性能，（ii）我们的方法可以实现文本基于描述和网络神经元表示的原则性框架。<details>
<summary>Abstract</summary>
Modern deep networks are highly complex and their inferential outcome very hard to interpret. This is a serious obstacle to their transparent deployment in safety-critical or bias-aware applications. This work contributes to post-hoc interpretability, and specifically Network Dissection. Our goal is to present a framework that makes it easier to discover the individual functionality of each neuron in a network trained on a vision task; discovery is performed in terms of textual description generation. To achieve this objective, we leverage: (i) recent advances in multimodal vision-text models and (ii) network layers founded upon the novel concept of stochastic local competition between linear units. In this setting, only a small subset of layer neurons are activated for a given input, leading to extremely high activation sparsity (as low as only $\approx 4\%$). Crucially, our proposed method infers (sparse) neuron activation patterns that enables the neurons to activate/specialize to inputs with specific characteristics, diversifying their individual functionality. This capacity of our method supercharges the potential of dissection processes: human understandable descriptions are generated only for the very few active neurons, thus facilitating the direct investigation of the network's decision process. As we experimentally show, our approach: (i) yields Vision Networks that retain or improve classification performance, and (ii) realizes a principled framework for text-based description and examination of the generated neuronal representations.
</details>
<details>
<summary>摘要</summary>
现代深度网络具有极高的复杂性，其含义难以解释。这成为了在安全关键应用或偏见敏感应用中透明部署的障碍。本研究做出了后期解释性的贡献，特别是网络解剖。我们的目标是提供一种框架，使得在视觉任务上训练的网络中每个神经元的个性功能更易于发现。为达到这个目标，我们利用：（i）视觉语言模型的最新进展和（ii）基于新的精度地方竞争理论的网络层。在这种设置下，只有输入的一小部分神经元被激活，导致活动率极低（只有约4%）。然而，我们的提议方法可以推断（稀疏）神经元活动模式，使神经元能够对特定输入特征进行特殊化，从而拓宽它们的个性功能。这种能力使我们的方法在分解过程中具有很高的可用性：只有活动的神经元被生成为人类可读的描述，从而方便直接检查网络的决策过程。我们的实验表明，我们的方法可以：（i）保持或提高分类性能，和（ii）实现基于文本描述的网络检查和分析的原则性框架。
</details></li>
</ul>
<hr>
<h2 id="DynamicBEV-Leveraging-Dynamic-Queries-and-Temporal-Context-for-3D-Object-Detection"><a href="#DynamicBEV-Leveraging-Dynamic-Queries-and-Temporal-Context-for-3D-Object-Detection" class="headerlink" title="DynamicBEV: Leveraging Dynamic Queries and Temporal Context for 3D Object Detection"></a>DynamicBEV: Leveraging Dynamic Queries and Temporal Context for 3D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05989">http://arxiv.org/abs/2310.05989</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiawei Yao, Yingxin Lai</li>
<li>for: 这个研究的目的是为了提高BEV图像中3D物体检测的精度和效率，并且能够适应复杂的空间时间关系。</li>
<li>methods: 这篇论文提出了一个新的方法 called DynamicBEV，它使用动态查询来替代传统的静止查询，以更好地适应场景中的复杂空间时间关系。这个方法使用K-means clustering和Top-K Attention来协同组合本地和远程特征，以提高数据聚合效果。此外，这篇论文还提出了一个轻量级的时间融合模组（LTFM），用于有效地融合时间上下文，并且降低计算量。最后，这篇论文还使用了一个自定义的多标本损失函数，以确保特征表现的平衡。</li>
<li>results: 根据nuScenes dataset的实验结果，这篇论文确认了DynamicBEV的效果，并且获得了新的州际纪录，这说明了这个方法在Query-based BEV object detection中的优越性。<details>
<summary>Abstract</summary>
3D object detection is crucial for applications like autonomous driving and robotics. While query-based 3D object detection for BEV (Bird's Eye View) images has seen significant advancements, most existing methods follows the paradigm of static query. Such paradigm is incapable of adapting to complex spatial-temporal relationships in the scene. To solve this problem, we introduce a new paradigm in DynamicBEV, a novel approach that employs dynamic queries for BEV-based 3D object detection. In contrast to static queries, the proposed dynamic queries exploit K-means clustering and Top-K Attention in a creative way to aggregate information more effectively from both local and distant feature, which enable DynamicBEV to adapt iteratively to complex scenes. To further boost efficiency, DynamicBEV incorporates a Lightweight Temporal Fusion Module (LTFM), designed for efficient temporal context integration with a significant computation reduction. Additionally, a custom-designed Diversity Loss ensures a balanced feature representation across scenarios. Extensive experiments on the nuScenes dataset validate the effectiveness of DynamicBEV, establishing a new state-of-the-art and heralding a paradigm-level breakthrough in query-based BEV object detection.
</details>
<details>
<summary>摘要</summary>
三维物体检测是自动驾驶和 робо技术中非常重要的应用。而现有的方法都是使用静态查询，这种方法无法适应场景中的复杂空时关系。为解决这个问题，我们提出了一种新的思路：动态查询（DynamicBEV），它利用K-means归一化和Top-K注意力来更有效地从本地和远程特征处理信息，以适应场景的变化。此外，DynamicBEV还包括一个轻量级时间融合模块（LTFM），用于有效地融合时间上下文，并大幅减少计算量。此外，我们还设计了一种自定义的多样性损失函数，以保证不同场景中特征表示的均衡。广泛的实验 validate DynamicBEV 的有效性，成功创造了一个新的静态查询下的 BEV 物体检测状态ixel。
</details></li>
</ul>
<hr>
<h2 id="H-RANSAC-an-algorithmic-variant-for-Homography-image-transform-from-featureless-point-sets-application-to-video-based-football-analytics"><a href="#H-RANSAC-an-algorithmic-variant-for-Homography-image-transform-from-featureless-point-sets-application-to-video-based-football-analytics" class="headerlink" title="$H$-RANSAC, an algorithmic variant for Homography image transform from featureless point sets: application to video-based football analytics"></a>$H$-RANSAC, an algorithmic variant for Homography image transform from featureless point sets: application to video-based football analytics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04912">http://arxiv.org/abs/2310.04912</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gnousias/h-ransac">https://github.com/gnousias/h-ransac</a></li>
<li>paper_authors: George Nousias, Konstantinos Delibasis, Ilias Maglogiannis</li>
<li>for: 这种paper是为了解决图像匹配问题，具体来说是计算图像之间的投影矩阵。</li>
<li>methods: 这种方法使用了一种通用的RANSAC算法，并添加了一些针对特定情况的修改，以提高其精度和可靠性。</li>
<li>results: 这种方法在一个大量的足球赛事图像 dataset 上进行测试，与其他State-of-the-art实现相比，表现出了更高的精度和更多的成功处理的图像对。<details>
<summary>Abstract</summary>
Estimating homography matrix between two images has various applications like image stitching or image mosaicing and spatial information retrieval from multiple camera views, but has been proved to be a complicated problem, especially in cases of radically different camera poses and zoom factors. Many relevant approaches have been proposed, utilizing direct feature based, or deep learning methodologies. In this paper, we propose a generalized RANSAC algorithm, H-RANSAC, to retrieve homography image transformations from sets of points without descriptive local feature vectors and point pairing. We allow the points to be optionally labelled in two classes. We propose a robust criterion that rejects implausible point selection before each iteration of RANSAC, based on the type of the quadrilaterals formed by random point pair selection (convex or concave and (non)-self-intersecting). A similar post-hoc criterion rejects implausible homography transformations is included at the end of each iteration. The expected maximum iterations of $H$-RANSAC are derived for different probabilities of success, according to the number of points per image and per class, and the percentage of outliers. The proposed methodology is tested on a large dataset of images acquired by 12 cameras during real football matches, where radically different views at each timestamp are to be matched. Comparisons with state-of-the-art implementations of RANSAC combined with classic and deep learning image salient point detection indicates the superiority of the proposed $H$-RANSAC, in terms of average reprojection error and number of successfully processed pairs of frames, rendering it the method of choice in cases of image homography alignment with few tens of points, while local features are not available, or not descriptive enough. The implementation of $H$-RANSAC is available in https://github.com/gnousias/H-RANSAC
</details>
<details>
<summary>摘要</summary>
“计算图像之间的Homography矩阵有很多应用，如图像融合或图像拼接，以及从多个摄像头视角中获取空间信息。但是，这个问题在摄像头姿态和缩放因子之间很大的情况下是非常复杂的。许多相关的方法已经被提出，包括直接特征基于方法和深度学习方法。在这篇论文中，我们提出一种通用的RANSAC算法，即H-RANSAC，用于从无特征点Cloud中提取Homography图像变换。我们允许点可选择为两类标签。我们提出了一种robust的检验点选择前每次RANSAC迭代的 критери产生，基于随机点对选择的四边形类型（半球或半球和（非）自交）。此外，我们还包括在每次迭代结束后对不可能的Homography变换进行检验的预后检查。我们预计在不同的成功概率下，H-RANSAC的最大迭代次数可以得出，根据图像点Cloud的大小和每个图像点的类别。我们对一个包含12个摄像头 captured during real football matches的大型图像集进行测试，并与STATE-OF-THE-ART的RANSAC+ classic/深度学习图像突出点检测结合相比。结果表明，我们的H-RANSAC方法在平均 reprojection error 和成功处理图像对的数量方面表现出色，在图像Homography对齐问题中具有优选性， especial when local features are not available or not descriptive enough。H-RANSAC实现可以在https://github.com/gnousias/H-RANSAC ”
</details></li>
</ul>
<hr>
<h2 id="WAIT-Feature-Warping-for-Animation-to-Illustration-video-Translation-using-GANs"><a href="#WAIT-Feature-Warping-for-Animation-to-Illustration-video-Translation-using-GANs" class="headerlink" title="WAIT: Feature Warping for Animation to Illustration video Translation using GANs"></a>WAIT: Feature Warping for Animation to Illustration video Translation using GANs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04901">http://arxiv.org/abs/2310.04901</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/giddyyupp/wait">https://github.com/giddyyupp/wait</a></li>
<li>paper_authors: Samet Hicsonmez, Nermin Samet, Fidan Samet, Oguz Bakir, Emre Akbas, Pinar Duygulu</li>
<li>for: 本研究探讨了一个新的视频到视频翻译领域，旨在将动画电影翻译为原始插图风格。</li>
<li>methods: 我们提出了一种新的视频翻译问题，使用一个未排序的图像集来翻译输入视频。这是一项挑战性的任务，因为我们没有利用视频序列的优势，同时从多个图像中获得一致的风格更加困难。</li>
<li>results: 我们提出了一种基于图像到图像翻译模型的新生成器网络，并使用特征扭曲层来确保视频之间的时间协调。我们通过三个数据集进行质量和量化的比较，证明了我们的方法的效果。代码和预训练模型可以在GitHub上获取。<details>
<summary>Abstract</summary>
In this paper, we explore a new domain for video-to-video translation. Motivated by the availability of animation movies that are adopted from illustrated books for children, we aim to stylize these videos with the style of the original illustrations. Current state-of-the-art video-to-video translation models rely on having a video sequence or a single style image to stylize an input video. We introduce a new problem for video stylizing where an unordered set of images are used. This is a challenging task for two reasons: i) we do not have the advantage of temporal consistency as in video sequences; ii) it is more difficult to obtain consistent styles for video frames from a set of unordered images compared to using a single image.   Most of the video-to-video translation methods are built on an image-to-image translation model, and integrate additional networks such as optical flow, or temporal predictors to capture temporal relations. These additional networks make the model training and inference complicated and slow down the process. To ensure temporal coherency in video-to-video style transfer, we propose a new generator network with feature warping layers which overcomes the limitations of the previous methods. We show the effectiveness of our method on three datasets both qualitatively and quantitatively. Code and pretrained models are available at https://github.com/giddyyupp/wait.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们探讨了一个新的视频到视频翻译领域。我们受到了动画电影的推广，这些电影是根据儿童插画书改编的。我们希望使用原始插画的风格翻译视频。现有的视频到视频翻译模型都是基于单个风格图片或视频序列来翻译输入视频。我们引入了一个新的视频翻译问题，在这个问题中，我们使用一个无序集合的图片来翻译输入视频。这是一个具有两种挑战性：一是我们没有优势的时间一致性，二是从无序图片中获取视频帧的一致风格更加困难。大多数视频到视频翻译方法都是基于图片到图片翻译模型，并且添加了镜像流、时间预测器等额外网络来捕捉时间关系。这些额外网络使模型训练和推理变得复杂，并且慢了过程。为保证视频翻译中的时间一致性，我们提议了一个新的生成器网络，具有特征扭曲层，这种方法超越了之前的方法的局限性。我们通过三个数据集的质量和量的评估，证明了我们的方法的有效性。代码和预训练模型可以在https://github.com/giddyyupp/wait上获取。
</details></li>
</ul>
<hr>
<h2 id="HowToCaption-Prompting-LLMs-to-Transform-Video-Annotations-at-Scale"><a href="#HowToCaption-Prompting-LLMs-to-Transform-Video-Annotations-at-Scale" class="headerlink" title="HowToCaption: Prompting LLMs to Transform Video Annotations at Scale"></a>HowToCaption: Prompting LLMs to Transform Video Annotations at Scale</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04900">http://arxiv.org/abs/2310.04900</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ninatu/howtocaption">https://github.com/ninatu/howtocaption</a></li>
<li>paper_authors: Nina Shvetsova, Anna Kukleva, Xudong Hong, Christian Rupprecht, Bernt Schiele, Hilde Kuehne</li>
<li>for: 这个论文是为了提高text-video模型的性能而设计的。</li>
<li>methods: 该论文使用大型自然语言模型（LLM）来生成视频描述，并通过提取视频字幕的子title来减少人工标注的干扰。</li>
<li>results: 该论文的实验结果表明，使用该方法可以获得大规模无需人工标注的视频描述，并且可以提高text-video retrieval任务的性能。<details>
<summary>Abstract</summary>
Instructional videos are an excellent source for learning multimodal representations by leveraging video-subtitle pairs extracted with automatic speech recognition systems (ASR) from the audio signal in the videos. However, in contrast to human-annotated captions, both speech and subtitles naturally differ from the visual content of the videos and thus provide only noisy supervision for multimodal learning. As a result, large-scale annotation-free web video training data remains sub-optimal for training text-video models. In this work, we propose to leverage the capability of large language models (LLMs) to obtain fine-grained video descriptions aligned with videos. Specifically, we prompt an LLM to create plausible video descriptions based on ASR narrations of the video for a large-scale instructional video dataset. To this end, we introduce a prompting method that is able to take into account a longer text of subtitles, allowing us to capture context beyond a single sentence. To align the captions to the video temporally, we prompt the LLM to generate timestamps for each produced caption based on the subtitles. In this way, we obtain human-style video captions at scale without human supervision. We apply our method to the subtitles of the HowTo100M dataset, creating a new large-scale dataset, HowToCaption. Our evaluation shows that the resulting captions not only significantly improve the performance over many different benchmark datasets for text-video retrieval but also lead to a disentangling of textual narration from the audio, boosting performance in text-video-audio tasks.
</details>
<details>
<summary>摘要</summary>
文章中提到的视频教程是一个非常好的来源 для学习多Modal表示，通过使用自动语音识别系统（ASR）提取视频中的音频信号中的话语和字幕，并利用这些话语和字幕来学习多Modal表示。然而，与人工标注的字幕相比，视频中的话语和字幕与视频的视觉内容之间存在干扰，因此提供了质量不高的指导。因此，大规模无注释网络视频训练数据仍然是训练文本-视频模型的下OPTimal。在这种情况下，我们提议利用大型自然语言模型（LLM）来获取视频描述。 Specifically, we prompt an LLM to create plausible video descriptions based on ASR narrations of the video for a large-scale instructional video dataset. To this end, we introduce a prompting method that is able to take into account a longer text of subtitles, allowing us to capture context beyond a single sentence. To align the captions to the video temporally, we prompt the LLM to generate timestamps for each produced caption based on the subtitles. In this way, we obtain human-style video captions at scale without human supervision. We apply our method to the subtitles of the HowTo100M dataset, creating a new large-scale dataset, HowToCaption. Our evaluation shows that the resulting captions not only significantly improve the performance over many different benchmark datasets for text-video retrieval but also lead to a disentangling of textual narration from the audio, boosting performance in text-video-audio tasks.
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-for-Automated-Mitral-Regurgitation-Detection-from-Cardiac-Imaging"><a href="#Machine-Learning-for-Automated-Mitral-Regurgitation-Detection-from-Cardiac-Imaging" class="headerlink" title="Machine Learning for Automated Mitral Regurgitation Detection from Cardiac Imaging"></a>Machine Learning for Automated Mitral Regurgitation Detection from Cardiac Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04871">http://arxiv.org/abs/2310.04871</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ke Xiao, Erik Learned-Miller, Evangelos Kalogerakis, James Priest, Madalina Fiterau</li>
<li>for:  Mitral regurgitation (MR) 诊断</li>
<li>methods: 使用 semi-supervised 模型 CUSSP，利用标准计算机视觉技术和对比模型，从大量无标签数据中学习，并与专业分类器结合，实现首次自动 MR 诊断系统。</li>
<li>results: 在测试集上，CUSSP 得到了 F1 分数 0.69 和 ROC-AUC 分数 0.88，创造了这个新任务的首个benchmarkresult。<details>
<summary>Abstract</summary>
Mitral regurgitation (MR) is a heart valve disease with potentially fatal consequences that can only be forestalled through timely diagnosis and treatment. Traditional diagnosis methods are expensive, labor-intensive and require clinical expertise, posing a barrier to screening for MR. To overcome this impediment, we propose a new semi-supervised model for MR classification called CUSSP. CUSSP operates on cardiac imaging slices of the 4-chamber view of the heart. It uses standard computer vision techniques and contrastive models to learn from large amounts of unlabeled data, in conjunction with specialized classifiers to establish the first ever automated MR classification system. Evaluated on a test set of 179 labeled -- 154 non-MR and 25 MR -- sequences, CUSSP attains an F1 score of 0.69 and a ROC-AUC score of 0.88, setting the first benchmark result for this new task.
</details>
<details>
<summary>摘要</summary>
Mitral regurgitation (MR) 是一种心脏阀门疾病，有可能有致命的后果，只能通过及时诊断和治疗来抵消。传统诊断方法是昂贵，劳动密集，需要丰富的临床专业知识，这成为了 MR 检测的障碍。为了突破这个障碍，我们提出了一种新的半supervised模型 для MR 分类，称为 CUSSP。CUSSP 运行在心脏成像剖面的4室视图上，使用标准的计算机视觉技术和对比模型，从大量的无标签数据中学习，并与专门的分类器结合以建立首次的自动 MR 分类系统。在一个测试集上进行评估，CUSSP 的 F1 分数为 0.69，ROC-AUC 分数为 0.88，创造了这个新任务的首个benchmark结果。
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Facial-Relationships-and-Feature-Aggregation-for-Multi-Face-Forgery-Detection"><a href="#Exploiting-Facial-Relationships-and-Feature-Aggregation-for-Multi-Face-Forgery-Detection" class="headerlink" title="Exploiting Facial Relationships and Feature Aggregation for Multi-Face Forgery Detection"></a>Exploiting Facial Relationships and Feature Aggregation for Multi-Face Forgery Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04845">http://arxiv.org/abs/2310.04845</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenhao Lin, Fangbin Yi, Hang Wang, Qian Li, Deng Jingyi, Chao Shen</li>
<li>for: 防止多人脸伪造攻击</li>
<li>methods: 使用facial relationships学习模块和全局特征聚合模块</li>
<li>results: 在两个公开的多人脸伪造数据集上实现了状态畅的多人脸伪造检测效果<details>
<summary>Abstract</summary>
Face forgery techniques have emerged as a forefront concern, and numerous detection approaches have been proposed to address this challenge. However, existing methods predominantly concentrate on single-face manipulation detection, leaving the more intricate and realistic realm of multi-face forgeries relatively unexplored. This paper proposes a novel framework explicitly tailored for multi-face forgery detection,filling a critical gap in the current research. The framework mainly involves two modules:(i) a facial relationships learning module, which generates distinguishable local features for each face within images,(ii) a global feature aggregation module that leverages the mutual constraints between global and local information to enhance forgery detection accuracy.Our experimental results on two publicly available multi-face forgery datasets demonstrate that the proposed approach achieves state-of-the-art performance in multi-face forgery detection scenarios.
</details>
<details>
<summary>摘要</summary>
面孔伪造技术已经成为当前研究的突出问题，许多检测方法已经被提议以解决这个挑战。然而，现有的方法主要集中在单个面孔修改检测上，留下更复杂和实际的多面孔伪造场景尚未得到足够的探索。这篇论文提出了一种新的框架，专门针对多面孔伪造检测。该框架主要包括两个模块：（i）一个人脸关系学习模块，生成每个图像中的 distinguishing 本地特征（ii）一个全局特征聚合模块，利用全局和本地信息之间的互补关系来增强伪造检测精度。我们在两个公开available的多面孔伪造数据集上进行了实验，结果表明，我们提出的方法在多面孔伪造检测场景中具有国际之最的表现。
</details></li>
</ul>
<hr>
<h2 id="Extract-Transform-Load-for-Video-Streams"><a href="#Extract-Transform-Load-for-Video-Streams" class="headerlink" title="Extract-Transform-Load for Video Streams"></a>Extract-Transform-Load for Video Streams</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04830">http://arxiv.org/abs/2310.04830</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ferdiko/vetl">https://github.com/ferdiko/vetl</a></li>
<li>paper_authors: Ferdinand Kossmann, Ziniu Wu, Eugenie Lai, Nesime Tatbul, Lei Cao, Tim Kraska, Samuel Madden</li>
<li>for: 这篇论文主要是为了解决大规模视频分析中的存储和查询问题。</li>
<li>methods: 这篇论文提出了一种名为Skyscraper的系统，可以实现自适应的视频抽取、转换和加载（V-ETL）过程，以降低存储和查询成本。Skyscraper使用了缓存和云汇抽象来应对工作负载峰值，并可以自动调整抽取率和分辨率以适应不同的内容。</li>
<li>results: 在实验中，Skyscraper在对视频抽取和转换过程中显著降低了成本，同时也提供了一定的可靠性保证，而现有的最佳实践系统则无法同时实现这两点。<details>
<summary>Abstract</summary>
Social media, self-driving cars, and traffic cameras produce video streams at large scales and cheap cost. However, storing and querying video at such scales is prohibitively expensive. We propose to treat large-scale video analytics as a data warehousing problem: Video is a format that is easy to produce but needs to be transformed into an application-specific format that is easy to query. Analogously, we define the problem of Video Extract-Transform-Load (V-ETL). V-ETL systems need to reduce the cost of running a user-defined V-ETL job while also giving throughput guarantees to keep up with the rate at which data is produced. We find that no current system sufficiently fulfills both needs and therefore propose Skyscraper, a system tailored to V-ETL. Skyscraper can execute arbitrary video ingestion pipelines and adaptively tunes them to reduce cost at minimal or no quality degradation, e.g., by adjusting sampling rates and resolutions to the ingested content. Skyscraper can hereby be provisioned with cheap on-premises compute and uses a combination of buffering and cloud bursting to deal with peaks in workload caused by expensive processing configurations. In our experiments, we find that Skyscraper significantly reduces the cost of V-ETL ingestion compared to adaptions of current SOTA systems, while at the same time giving robustness guarantees that these systems are lacking.
</details>
<details>
<summary>摘要</summary>
社交媒体、自动驾驶车和交通摄像头产生大规模的视频流，但存储和查询这些视频流的成本过高。我们认为将大规模视频分析视为数据存储问题：视频是容易生成的格式，但需要转换为特定应用程序可查询的格式。我们定义视频EXTRACT-TRANSFORM-LOAD（V-ETL）问题。V-ETL系统需要降低运行用户定义的V-ETL任务的成本，同时给予吞吐量保证以满足数据生产的速度。我们发现当前系统无法充分满足这两个需求，因此我们提出了Skyscraper系统。Skyscraper可以执行任意视频入库管道，并动态调整这些管道以降低成本，例如调整采样率和分辨率。Skyscraper可以通过具有低成本的在线计算机和缓存和云冲顶来处理因价格高昂的处理配置而引起的峰值工作负荷。在我们的实验中，我们发现Skyscraper可以与当前SOTA系统相比，对视频入库成本进行显著减少，同时保证这些系统缺乏的可靠性 guarantee。
</details></li>
</ul>
<hr>
<h2 id="How-to-effectively-train-an-ensemble-of-Faster-R-CNN-object-detectors-to-quantify-uncertainty"><a href="#How-to-effectively-train-an-ensemble-of-Faster-R-CNN-object-detectors-to-quantify-uncertainty" class="headerlink" title="How to effectively train an ensemble of Faster R-CNN object detectors to quantify uncertainty"></a>How to effectively train an ensemble of Faster R-CNN object detectors to quantify uncertainty</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04829">http://arxiv.org/abs/2310.04829</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/akola-mbey-denis/efficientensemble">https://github.com/akola-mbey-denis/efficientensemble</a></li>
<li>paper_authors: Denis Mbey Akola, Gianni Franchi<br>for:这个论文提出了一种新的对象检测 ensemble 模型训练方法，具体来说是对 Faster R-CNN 模型进行不确定性估计。methods: authors 提出了训练一个 Region Proposal Network (RPN) 和多个 Fast R-CNN 预测头，以建立一个可靠的深度ensemble网络，用于对象检测中估计不确定性。results: authors 采用这种方法，并通过实验表明，这种方法比naive方法（完全训练所有 $n$ 模型）要快得多。此外， authors 还使用 Ensemble Model’s Expected Calibration Error (ECE) 来估计不确定性。最后， authors 比较了这种模型与 Gaussian YOLOv3 的性能。<details>
<summary>Abstract</summary>
This paper presents a new approach for training two-stage object detection ensemble models, more specifically, Faster R-CNN models to estimate uncertainty. We propose training one Region Proposal Network(RPN)~\cite{https://doi.org/10.48550/arxiv.1506.01497} and multiple Fast R-CNN prediction heads is all you need to build a robust deep ensemble network for estimating uncertainty in object detection. We present this approach and provide experiments to show that this approach is much faster than the naive method of fully training all $n$ models in an ensemble. We also estimate the uncertainty by measuring this ensemble model's Expected Calibration Error (ECE). We then further compare the performance of this model with that of Gaussian YOLOv3, a variant of YOLOv3 that models uncertainty using predicted bounding box coordinates. The source code is released at \url{https://github.com/Akola-Mbey-Denis/EfficientEnsemble}
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种新的对两stage对象检测集成模型进行训练方法，具体来说是对Faster R-CNN模型进行不确定性估计。我们提议在一个Region Proposal Network（RPN）~\cite{https://doi.org/10.48550/arxiv.1506.01497}和多个快速R-CNN预测头上进行训练，这样就能够建立一个robust的深度集成网络，用于对象检测中的不确定性估计。我们介绍了这种方法，并通过实验表明这种方法比Naive方法（即将所有$n$模型在集成中完全训练）要快得多。我们还使用 ensemble模型的预测结果来估计不确定性，并使用Expected Calibration Error（ECE）来测量这个模型的不确定性。最后，我们进一步比较了这个模型的性能与Gaussian YOLOv3模型，这是一种使用预测 bounding box 坐标来模拟不确定性的YOLOv3变体。代码可以在 \url{https://github.com/Akola-Mbey-Denis/EfficientEnsemble} 上下载。
</details></li>
</ul>
<hr>
<h2 id="Comparative-study-of-multi-person-tracking-methods"><a href="#Comparative-study-of-multi-person-tracking-methods" class="headerlink" title="Comparative study of multi-person tracking methods"></a>Comparative study of multi-person tracking methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04825">http://arxiv.org/abs/2310.04825</a></li>
<li>repo_url: None</li>
<li>paper_authors: Denis Mbey Akola</li>
<li>for: 这个论文研究了两种跟踪算法（SORT和Tracktor++），这两种算法在MOT Challenge leaderboard上排名第一位（MOTChallenge网页：<a target="_blank" rel="noopener" href="https://motchallenge.net)./">https://motchallenge.net）。</a></li>
<li>methods: 作者采用了流行的跟踪-by-检测方法，并使用自己训练的人体检测模型（MOT17Det数据集：<a target="_blank" rel="noopener" href="https://motchallenge.net/data/MOT17Det/%EF%BC%89%E5%92%8CMOT17%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%88https://motchallenge.net/data/MOT17/%EF%BC%89%E4%B8%AD%E7%9A%84%E4%BA%BA%E4%BD%93%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B%E6%9D%A5%E9%99%8D%E4%BD%8ETracktor++%E4%B8%AD%E7%9A%84%E5%81%87%E9%87%8D%E5%A4%8D%E8%AD%A6%E7%A4%BA%E3%80%82">https://motchallenge.net/data/MOT17Det/）和MOT17数据集（https://motchallenge.net/data/MOT17/）中的人体识别模型来降低Tracktor++中的假重复警示。</a></li>
<li>results: 实验结果表明，Tracktor++比SORT更好的多人跟踪算法。作者还进行了减少RE-ID网络和运动的贡献对Tracktor++结果的分析，并提供了未来研究的建议。<details>
<summary>Abstract</summary>
This paper presents a study of two tracking algorithms (SORT~\cite{7533003} and Tracktor++~\cite{2019}) that were ranked first positions on the MOT Challenge leaderboard (The MOTChallenge web page: https://motchallenge.net ). The purpose of this study is to discover the techniques used and to provide useful insights about these algorithms in the tracking pipeline that could improve the performance of MOT tracking algorithms. To this end, we adopted the popular tracking-by-detection approach. We trained our own Pedestrian Detection model using the MOT17Det dataset (MOT17Det : https://motchallenge.net/data/MOT17Det/ ). We also used a re-identification model trained on MOT17 dataset (MOT17 : https://motchallenge.net/data/MOT17/ ) for Tracktor++ to reduce the false re-identification alarms. We then present experimental results which shows that Tracktor++ is a better multi-person tracking algorithm than SORT. We also performed ablation studies to discover the contribution of re-identification(RE-ID) network and motion to the results of Tracktor++. We finally conclude by providing some recommendations for future research.
</details>
<details>
<summary>摘要</summary>
Here's the Simplified Chinese translation:这篇论文研究了两种多对象跟踪（MOT）算法（SORT和Tracktor++），它们在MOT Challenge leaderboard上排名第一。研究的目的是了解这些算法使用的技术和提供有用的洞察，以提高MOT跟踪性能。采用了跟踪通过检测的方法，并使用MOT17Det数据集来训练自己的人体检测模型。此外，还使用MOT17数据集来训练Tracktor++中的重复标识模型，以降低假的重复警示。实验结果表明，Tracktor++比SORT更好的多人跟踪算法。此外，还进行了减少RE-ID网络和运动对Tracktor++的贡献的ablation研究。最后，文章提出了一些未来研究的建议。
</details></li>
</ul>
<hr>
<h2 id="Combining-UPerNet-and-ConvNeXt-for-Contrails-Identification-to-reduce-Global-Warming"><a href="#Combining-UPerNet-and-ConvNeXt-for-Contrails-Identification-to-reduce-Global-Warming" class="headerlink" title="Combining UPerNet and ConvNeXt for Contrails Identification to reduce Global Warming"></a>Combining UPerNet and ConvNeXt for Contrails Identification to reduce Global Warming</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04808">http://arxiv.org/abs/2310.04808</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/biluko/2023gric">https://github.com/biluko/2023gric</a></li>
<li>paper_authors: Zhenkuan Wang<br>for: This study focuses on aircraft contrail detection in global satellite images to improve contrail models and mitigate their impact on climate change.methods: An innovative data preprocessing technique for NOAA GOES-16 satellite images is developed, using brightness temperature data from the infrared channel to create false-color images, enhancing model perception. The model selection is based on the UPerNet architecture, implemented using the MMsegmentation library, with the integration of two ConvNeXt configurations for improved performance.results: The approach achieves exceptional results, boasting a high Dice coefficient score, placing it in the top 5% of participating teams.<details>
<summary>Abstract</summary>
Semantic segmentation is a critical tool in computer vision, applied in various domains like autonomous driving and medical imaging. This study focuses on aircraft contrail detection in global satellite images to improve contrail models and mitigate their impact on climate change.An innovative data preprocessing technique for NOAA GOES-16 satellite images is developed, using brightness temperature data from the infrared channel to create false-color images, enhancing model perception. To tackle class imbalance, the training dataset exclusively includes images with positive contrail labels.The model selection is based on the UPerNet architecture, implemented using the MMsegmentation library, with the integration of two ConvNeXt configurations for improved performance. Cross-entropy loss with positive class weights enhances contrail recognition. Fine-tuning employs the AdamW optimizer with a learning rate of $2.5 \times 10^{-4}$.During inference, a multi-model prediction fusion strategy and a contrail determination threshold of 0.75 yield a binary prediction mask. RLE encoding is used for efficient prediction result organization.The approach achieves exceptional results, boasting a high Dice coefficient score, placing it in the top 5\% of participating teams. This underscores the innovative nature of the segmentation model and its potential for enhanced contrail recognition in satellite imagery.For further exploration, the code and models are available on GitHub: \url{https://github.com/biluko/2023GRIC.git}.
</details>
<details>
<summary>摘要</summary>
semantic segmentation是计算机视觉中的一种重要工具，在各种领域中应用，如自动驾驶和医疗影像。这个研究将focus on气象卫星图像中的飞机烟尘迹象检测，以改进烟尘模型并减少对气候变化的影响。在这种情况下，我们开发了一种新的数据预处理技术，使用NOAA GOES-16卫星图像的红外通道的亮度温度数据创建 false-color 图像，提高模型的识别能力。为了解决类别偏斜问题，我们专门使用包含正确烟尘标签的训练集。模型选择基于 UPerNet 架构，通过 MMsegmentation 库的实现，并配置了两种 ConvNeXt 结构以提高性能。使用十字积分损失函数，并将正确类型权重为 2.5 倍 10 ^{-4}。在推理时，我们采用多模型预测融合策略和烟尘确定阈值 0.75，生成二进制预测Mask。使用 RLE 编码器进行有效的预测结果组织。这种方法实现了出色的结果，具有高 dice 系数，位列参赛队伍的前 5%。这表明我们的 segmentation 模型具有创新性，并有可能在卫星图像中提高烟尘识别精度。如果您想了解更多，可以在 GitHub 上查看我们的代码和模型： \url{https://github.com/biluko/2023GRIC.git}。
</details></li>
</ul>
<hr>
<h2 id="Fully-Sparse-Long-Range-3D-Object-Detection-Using-Range-Experts-and-Multimodal-Virtual-Points"><a href="#Fully-Sparse-Long-Range-3D-Object-Detection-Using-Range-Experts-and-Multimodal-Virtual-Points" class="headerlink" title="Fully Sparse Long Range 3D Object Detection Using Range Experts and Multimodal Virtual Points"></a>Fully Sparse Long Range 3D Object Detection Using Range Experts and Multimodal Virtual Points</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04800">http://arxiv.org/abs/2310.04800</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ajinkya Khoche, Laura Pereira Sánchez, Nazre Batool, Sina Sharif Mansouri, Patric Jensfelt</li>
<li>for: 提高自动驾驶车辆的安全性和效率，准确探测和 реаги于远距离对象、障碍物和危险。</li>
<li>methods:  combining two LiDAR based 3D detection networks, one specializing at near to mid-range objects, and one at long-range 3D detection, with Multimodal Virtual Points (MVP) to enrich the data with virtual points.</li>
<li>results:  achieves state-of-the-art performance on the Argoverse2 (AV2) dataset, with improvements at long range.<details>
<summary>Abstract</summary>
3D object detection at long-range is crucial for ensuring the safety and efficiency of self-driving cars, allowing them to accurately perceive and react to objects, obstacles, and potential hazards from a distance. But most current state-of-the-art LiDAR based methods are limited by the sparsity of range sensors, which generates a form of domain gap between points closer to and farther away from the ego vehicle. Another related problem is the label imbalance for faraway objects, which inhibits the performance of Deep Neural Networks at long-range. Although image features could be beneficial for long-range detections, and some recently proposed multimodal methods incorporate image features, they do not scale well computationally at long ranges or are limited by depth estimation accuracy. To address the above limitations, we propose to combine two LiDAR based 3D detection networks, one specializing at near to mid-range objects, and one at long-range 3D detection. To train a detector at long range under a scarce label regime, we further propose to weigh the loss according to the labelled objects' distance from ego vehicle. To mitigate the LiDAR sparsity issue, we leverage Multimodal Virtual Points (MVP), an image based depth completion algorithm, to enrich our data with virtual points. Our method, combining two range experts trained with MVP, which we refer to as RangeFSD, achieves state-of-the-art performance on the Argoverse2 (AV2) dataset, with improvements at long range. The code will be released soon.
</details>
<details>
<summary>摘要</summary>
三维物体探测在长距离下是自动驾驶车辆的安全和效率 Ensuring the safety and efficiency of self-driving cars, allowing them to accurately perceive and react to objects, obstacles, and potential hazards from a distance. However, most current state-of-the-art LiDAR-based methods are limited by the sparsity of range sensors, which creates a domain gap between points closer to and farther away from the ego vehicle. Another related problem is the label imbalance for faraway objects, which hinders the performance of deep neural networks at long range. Although image features could be beneficial for long-range detections, and some recently proposed multimodal methods incorporate image features, they do not scale well computationally at long ranges or are limited by depth estimation accuracy. To address the above limitations, we propose to combine two LiDAR-based 3D detection networks, one specializing in near-to-mid-range objects, and one at long-range 3D detection. To train a detector at long range under a scarce label regime, we further propose to weigh the loss according to the labeled objects' distance from the ego vehicle. To mitigate the LiDAR sparsity issue, we leverage Multimodal Virtual Points (MVP), an image-based depth completion algorithm, to enrich our data with virtual points. Our method, combining two range experts trained with MVP, which we refer to as RangeFSD, achieves state-of-the-art performance on the Argoverse2 (AV2) dataset, with improvements at long range. The code will be released soon.
</details></li>
</ul>
<hr>
<h2 id="HI-SLAM-Monocular-Real-time-Dense-Mapping-with-Hybrid-Implicit-Fields"><a href="#HI-SLAM-Monocular-Real-time-Dense-Mapping-with-Hybrid-Implicit-Fields" class="headerlink" title="HI-SLAM: Monocular Real-time Dense Mapping with Hybrid Implicit Fields"></a>HI-SLAM: Monocular Real-time Dense Mapping with Hybrid Implicit Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04787">http://arxiv.org/abs/2310.04787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Zhang, Tiecheng Sun, Sen Wang, Qing Cheng, Norbert Haala</li>
<li>for: 这个论文旨在提出一种基于神经场的实时单目地图呈现框架，以实现准确和密集的同时定位和地图建模（SLAM）。</li>
<li>methods: 我们的方法将精细的SLAM方法与神经场隐式场合并，并使用多分辨率网格编码和签名距离函数（SDF）表示来生成神经场。这使得我们可以在实时更新地图，并通过在线循环关闭来保持全球一致性。</li>
<li>results: 我们的方法在实验中表现出比现有方法更高的准确率和地图完整性，同时保持实时性。<details>
<summary>Abstract</summary>
In this letter, we present a neural field-based real-time monocular mapping framework for accurate and dense Simultaneous Localization and Mapping (SLAM). Recent neural mapping frameworks show promising results, but rely on RGB-D or pose inputs, or cannot run in real-time. To address these limitations, our approach integrates dense-SLAM with neural implicit fields. Specifically, our dense SLAM approach runs parallel tracking and global optimization, while a neural field-based map is constructed incrementally based on the latest SLAM estimates. For the efficient construction of neural fields, we employ multi-resolution grid encoding and signed distance function (SDF) representation. This allows us to keep the map always up-to-date and adapt instantly to global updates via loop closing. For global consistency, we propose an efficient Sim(3)-based pose graph bundle adjustment (PGBA) approach to run online loop closing and mitigate the pose and scale drift. To enhance depth accuracy further, we incorporate learned monocular depth priors. We propose a novel joint depth and scale adjustment (JDSA) module to solve the scale ambiguity inherent in depth priors. Extensive evaluations across synthetic and real-world datasets validate that our approach outperforms existing methods in accuracy and map completeness while preserving real-time performance.
</details>
<details>
<summary>摘要</summary>
封信中，我们提出了基于神经场的实时单视映射框架，用于高精度和完整的同时地理位和地图（SLAM）。现有的神经映射框架有很好的表现，但它们依赖于RGB-D或 pose输入，或者不能在实时上运行。为了解决这些限制，我们的方法将粗粒度 SLAM 与神经隐藏场 integrate  вместе。具体来说，我们的粗粒度 SLAM 方法在并行跟踪和全局优化时运行，而神经场基于最新的 SLAM 估计构建了地图。为了高效地构建神经场，我们采用多尺度网格编码和签名距离函数（SDF）表示。这使得我们可以保持地图总是最新的，并在全球更新时适时更新。为了保证全球一致性，我们提出了一种高效的 Sim(3) 基于 pose graph bundle adjustment（PGBA）方法来在线进行循环关闭和缓解 pose 和比例偏移。为了进一步提高深度精度，我们引入了学习的单视深度估计。我们提出了一种新的共同深度和比例调整（JDSA）模块，以解决深度估计中的比例歧阱。我们在 sintetic 和实际 datasets 进行了广泛的评估，并证明了我们的方法在准确和地图完整性方面超过现有方法，而且保持实时性。
</details></li>
</ul>
<hr>
<h2 id="IPMix-Label-Preserving-Data-Augmentation-Method-for-Training-Robust-Classifiers"><a href="#IPMix-Label-Preserving-Data-Augmentation-Method-for-Training-Robust-Classifiers" class="headerlink" title="IPMix: Label-Preserving Data Augmentation Method for Training Robust Classifiers"></a>IPMix: Label-Preserving Data Augmentation Method for Training Robust Classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04780">http://arxiv.org/abs/2310.04780</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hzlsaber/IPMix">https://github.com/hzlsaber/IPMix</a></li>
<li>paper_authors: Zhenglin Huang, Xianan Bao, Na Zhang, Qingqi Zhang, Xiaomei Tu, Biao Wu, Xi Yang</li>
<li>For: 提高卷积神经网络的Robustness和纯度之间的质量衡量。* Methods:  integrate three levels of data augmentation (image-level, patch-level, and pixel-level) into a coherent and label-preserving technique to increase the diversity of training data with limited computational overhead, and introduce structural complexity at different levels to generate more diverse images, as well as adopt the random mixing method for multi-scale information fusion.* Results: outperform state-of-the-art corruption robustness on CIFAR-C and ImageNet-C, and significantly improve other safety measures, including robustness to adversarial perturbations, calibration, prediction consistency, and anomaly detection, achieving state-of-the-art or comparable results on several benchmarks, including ImageNet-R, ImageNet-A, and ImageNet-O.<details>
<summary>Abstract</summary>
Data augmentation has been proven effective for training high-accuracy convolutional neural network classifiers by preventing overfitting. However, building deep neural networks in real-world scenarios requires not only high accuracy on clean data but also robustness when data distributions shift. While prior methods have proposed that there is a trade-off between accuracy and robustness, we propose IPMix, a simple data augmentation approach to improve robustness without hurting clean accuracy. IPMix integrates three levels of data augmentation (image-level, patch-level, and pixel-level) into a coherent and label-preserving technique to increase the diversity of training data with limited computational overhead. To further improve the robustness, IPMix introduces structural complexity at different levels to generate more diverse images and adopts the random mixing method for multi-scale information fusion. Experiments demonstrate that IPMix outperforms state-of-the-art corruption robustness on CIFAR-C and ImageNet-C. In addition, we show that IPMix also significantly improves the other safety measures, including robustness to adversarial perturbations, calibration, prediction consistency, and anomaly detection, achieving state-of-the-art or comparable results on several benchmarks, including ImageNet-R, ImageNet-A, and ImageNet-O.
</details>
<details>
<summary>摘要</summary>
<<SYS>>训练高精度卷积神经网络分类器时，数据扩充已被证明是有效的防止过拟合的方法。然而，在实际场景中建立深度神经网络需要不仅在干净数据上达到高精度，还需要在数据分布shift时具有Robustness。而之前的方法认为存在精度和Robustness之间的负面关系，我们提出了IPMix，一种简单的数据扩充方法，可以在有限计算负担下提高训练数据的多样性，而不会增加过拟合。IPMix在图像、patch和像素三级数据扩充方面进行了一体化和标签保持的实现，并通过不同级别的结构复杂度来生成更多样的图像，并采用了随机混合方法来融合多尺度信息。实验表明，IPMix在CIFAR-C和ImageNet-C上的抗损害性能比state-of-the-art高，而且还在ImageNet-R、ImageNet-A和ImageNet-O上显著提高了其他安全指标，包括对抗扰动抑制、均衡、预测一致性和异常检测，达到了或与state-of-the-art相当的结果。<</SYS>>
</details></li>
</ul>
<hr>
<h2 id="TransCC-Transformer-Network-for-Coronary-Artery-CCTA-Segmentation"><a href="#TransCC-Transformer-Network-for-Coronary-Artery-CCTA-Segmentation" class="headerlink" title="TransCC: Transformer Network for Coronary Artery CCTA Segmentation"></a>TransCC: Transformer Network for Coronary Artery CCTA Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04779">http://arxiv.org/abs/2310.04779</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenchu Xu, Meng Li, Xue Wu</li>
<li>for: 这个研究旨在提高 coronary computed tomography angiography (CCTA) 影像的精确分类，以早期检测和治疗 coronary heart disease (CHD)。</li>
<li>methods: 本研究使用 transformer 和卷积神经网络，具有自注意机制，以解决 coronary 分类任务中的两个挑战：一是对标的地方结构带来损害，二是需要同时考虑全局和本地特征。</li>
<li>results: 实验结果显示，TransCC 可以优于现有方法， segmentation 性能平均为 0.730 和 0.582，这些结果证明 TransCC 在 CCTA 影像分类中的有效性。<details>
<summary>Abstract</summary>
The accurate segmentation of Coronary Computed Tomography Angiography (CCTA) images holds substantial clinical value for the early detection and treatment of Coronary Heart Disease (CHD). The Transformer, utilizing a self-attention mechanism, has demonstrated commendable performance in the realm of medical image processing. However, challenges persist in coronary segmentation tasks due to (1) the damage to target local structures caused by fixed-size image patch embedding, and (2) the critical role of both global and local features in medical image segmentation tasks.To address these challenges, we propose a deep learning framework, TransCC, that effectively amalgamates the Transformer and convolutional neural networks for CCTA segmentation. Firstly, we introduce a Feature Interaction Extraction (FIE) module designed to capture the characteristics of image patches, thereby circumventing the loss of semantic information inherent in the original method. Secondly, we devise a Multilayer Enhanced Perceptron (MEP) to augment attention to local information within spatial dimensions, serving as a complement to the self-attention mechanism. Experimental results indicate that TransCC outperforms existing methods in segmentation performance, boasting an average Dice coefficient of 0.730 and an average Intersection over Union (IoU) of 0.582. These results underscore the effectiveness of TransCC in CCTA image segmentation.
</details>
<details>
<summary>摘要</summary>
精准分割 coronary computed tomography angiography (CCTA) 图像具有临床价值，可以早期探测和治疗 coronary heart disease (CHD)。transformer 使用自注意机制，在医疗图像处理领域表现出色。然而，在 coronary 分割任务中仍存在一些挑战，主要是因为 (1) 固定大小图像块嵌入引起的目标地方结构损害，以及 (2) 医疗图像分割任务中的全球和本地特征的重要作用。为Address these challenges, we propose a deep learning framework, TransCC, that effectively combines the Transformer and convolutional neural networks for CCTA segmentation.首先，我们提出了一种Feature Interaction Extraction (FIE)模块，用于捕捉图像块特征，从而绕过原始方法中的含义损失。其次，我们设计了一种多层增强感知机制 (MEP)，用于增强对本地信息的注意力，作为自注意机制的补偿。实验结果表明，TransCC 在分割性能方面表现出色，其中平均 dice 系数为 0.730，平均 intersection over union (IoU) 为 0.582。这些结果证明 TransCC 在 CCTA 图像分割中的效果。
</details></li>
</ul>
<hr>
<h2 id="1st-Place-Solution-of-Egocentric-3D-Hand-Pose-Estimation-Challenge-2023-Technical-Report-A-Concise-Pipeline-for-Egocentric-Hand-Pose-Reconstruction"><a href="#1st-Place-Solution-of-Egocentric-3D-Hand-Pose-Estimation-Challenge-2023-Technical-Report-A-Concise-Pipeline-for-Egocentric-Hand-Pose-Reconstruction" class="headerlink" title="1st Place Solution of Egocentric 3D Hand Pose Estimation Challenge 2023 Technical Report:A Concise Pipeline for Egocentric Hand Pose Reconstruction"></a>1st Place Solution of Egocentric 3D Hand Pose Estimation Challenge 2023 Technical Report:A Concise Pipeline for Egocentric Hand Pose Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04769">http://arxiv.org/abs/2310.04769</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhishan Zhou, Zhi Lv, Shihao Zhou, Minqiang Zou, Tong Wu, Mochen Yu, Yao Tang, Jiajun Liang</li>
<li>for:  Egocentric 3D Hand Pose Estimation challenge</li>
<li>methods:  ViT backbones and simple regressor for 3D keypoints prediction, non-model method for merging multi-view results</li>
<li>results:  12.21mm MPJPE on test dataset, first place in challenge<details>
<summary>Abstract</summary>
This report introduce our work on Egocentric 3D Hand Pose Estimation workshop. Using AssemblyHands, this challenge focuses on egocentric 3D hand pose estimation from a single-view image. In the competition, we adopt ViT based backbones and a simple regressor for 3D keypoints prediction, which provides strong model baselines. We noticed that Hand-objects occlusions and self-occlusions lead to performance degradation, thus proposed a non-model method to merge multi-view results in the post-process stage. Moreover, We utilized test time augmentation and model ensemble to make further improvement. We also found that public dataset and rational preprocess are beneficial. Our method achieved 12.21mm MPJPE on test dataset, achieve the first place in Egocentric 3D Hand Pose Estimation challenge.
</details>
<details>
<summary>摘要</summary>
这份报告介绍我们在 egocentric 3D 手势估计工作坊中的工作。我们使用 AssemblyHands，这个挑战是从单视图图像中进行 egocentric 3D 手势估计。在竞赛中，我们采用基于 ViT 的背bone 和简单的回归器进行 3D 关键点预测，这提供了强大的模型基线。我们注意到手-物体遮挡和自遮挡会导致性能下降，因此我们提议一种非模型方法将多视图结果在后处理阶段合并。此外，我们利用测试时的扩展和模型ensemble来做进一步的改进。我们还发现公共数据集和合理的预处理是有利的。我们的方法在测试集上达到了 12.21mm MPJPE，在 Egocentric 3D Hand Pose Estimation 挑战中获得了第一名。
</details></li>
</ul>
<hr>
<h2 id="CAD-Models-to-Real-World-Images-A-Practical-Approach-to-Unsupervised-Domain-Adaptation-in-Industrial-Object-Classification"><a href="#CAD-Models-to-Real-World-Images-A-Practical-Approach-to-Unsupervised-Domain-Adaptation-in-Industrial-Object-Classification" class="headerlink" title="CAD Models to Real-World Images: A Practical Approach to Unsupervised Domain Adaptation in Industrial Object Classification"></a>CAD Models to Real-World Images: A Practical Approach to Unsupervised Domain Adaptation in Industrial Object Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04757">http://arxiv.org/abs/2310.04757</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dritter-bht/synthnet-transfer-learning">https://github.com/dritter-bht/synthnet-transfer-learning</a></li>
<li>paper_authors: Dennis Ritter, Mike Hemberger, Marc Hönig, Volker Stopp, Erik Rodner, Kristian Hildebrand</li>
<li>for: 本研究系统atica��analyze unsupervised domain adaptation pipelines for object classification in a challenging industrial setting, using only category-labeled CAD models.</li>
<li>methods: 本研究使用的方法包括 domain adaptation pipeline, which achieves SoTA performance on the VisDA benchmark and drastically improves recognition performance on a new open industrial dataset comprised of 102 mechanical parts.</li>
<li>results: 研究结果表明，使用这种方法可以帮助实现state-of-the-art unsupervised domain adaptation in practice,并且提供了一些实践中应用的指南。<details>
<summary>Abstract</summary>
In this paper, we systematically analyze unsupervised domain adaptation pipelines for object classification in a challenging industrial setting. In contrast to standard natural object benchmarks existing in the field, our results highlight the most important design choices when only category-labeled CAD models are available but classification needs to be done with real-world images. Our domain adaptation pipeline achieves SoTA performance on the VisDA benchmark, but more importantly, drastically improves recognition performance on our new open industrial dataset comprised of 102 mechanical parts. We conclude with a set of guidelines that are relevant for practitioners needing to apply state-of-the-art unsupervised domain adaptation in practice. Our code is available at https://github.com/dritter-bht/synthnet-transfer-learning.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们系统地分析了无监督领域适应管道，用于对工业场景中的物体分类。与现有领域中的标准自然物体标准相比，我们的结果显示了领域适应管道的重要设计选择，当仅有类别标注的CAD模型可用，但是需要使用实际图像进行分类。我们的领域适应管道在VisDA标准曲线上实现了领先的性能，而且在我们新提出的102种机械部件的开放 dataset中显著提高了识别性能。我们 conclude with一些实践中应用状态空间领域适应的指南，可以帮助实践者。我们的代码可以在https://github.com/dritter-bht/synthnet-transfer-learning中找到。
</details></li>
</ul>
<hr>
<h2 id="Balancing-stability-and-plasticity-in-continual-learning-the-readout-decomposition-of-activation-change-RDAC-framework"><a href="#Balancing-stability-and-plasticity-in-continual-learning-the-readout-decomposition-of-activation-change-RDAC-framework" class="headerlink" title="Balancing stability and plasticity in continual learning: the readout-decomposition of activation change (RDAC) framework"></a>Balancing stability and plasticity in continual learning: the readout-decomposition of activation change (RDAC) framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04741">http://arxiv.org/abs/2310.04741</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Anthes, Sushrut Thorat, Peter König, Tim C. Kietzmann</li>
<li>for: 本研究旨在解释继续学习（Continual Learning，CL）算法中稳定性和材料性之间的贸易关系，并提供有价值的思路以帮助解决这一问题。</li>
<li>methods: 该研究提出了一种名为Readout-Decomposition of Activation Change（RDAC）框架，该框架可以帮助解释CL算法中稳定性和材料性之间的关系，同时还可以解释在深度非线性神经网络中处理分割CIFAR-110任务时，各种常用的正则化算法（如Synaptic intelligence、Elastic-weight consolidation、Learning without Forgetting）和回忆策略（如Gradient episodic memory、Data replay）的稳定性和材料性之间的贸易关系。</li>
<li>results: 研究发现，GEM和Data replay等回忆策略可以保持稳定性和材料性，而SI、EWC和LwF等正则化算法则在维持稳定性的同时会减少材料性。此外，对一个隐藏层线性神经网络进行分析，我们 derivated一种gradient decomposition算法，可以限制活动变化在先前的读写空间中，以保持高稳定性而不会进一步减少材料性。结果表明，该算法可以维持稳定性无需重要的材料性损失。<details>
<summary>Abstract</summary>
Continual learning (CL) algorithms strive to acquire new knowledge while preserving prior information. However, this stability-plasticity trade-off remains a central challenge. This paper introduces a framework that dissects this trade-off, offering valuable insights into CL algorithms. The Readout-Decomposition of Activation Change (RDAC) framework first addresses the stability-plasticity dilemma and its relation to catastrophic forgetting. It relates learning-induced activation changes in the range of prior readouts to the degree of stability and changes in the null space to the degree of plasticity. In deep non-linear networks tackling split-CIFAR-110 tasks, the framework clarifies the stability-plasticity trade-offs of the popular regularization algorithms Synaptic intelligence (SI), Elastic-weight consolidation (EWC), and learning without Forgetting (LwF), and replay-based algorithms Gradient episodic memory (GEM), and data replay. GEM and data replay preserved stability and plasticity, while SI, EWC, and LwF traded off plasticity for stability. The inability of the regularization algorithms to maintain plasticity was linked to them restricting the change of activations in the null space of the prior readout. Additionally, for one-hidden-layer linear neural networks, we derived a gradient decomposition algorithm to restrict activation change only in the range of the prior readouts, to maintain high stability while not further sacrificing plasticity. Results demonstrate that the algorithm maintained stability without significant plasticity loss. The RDAC framework informs the behavior of existing CL algorithms and paves the way for novel CL approaches. Finally, it sheds light on the connection between learning-induced activation/representation changes and the stability-plasticity dilemma, also offering insights into representational drift in biological systems.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Activate-and-Reject-Towards-Safe-Domain-Generalization-under-Category-Shift"><a href="#Activate-and-Reject-Towards-Safe-Domain-Generalization-under-Category-Shift" class="headerlink" title="Activate and Reject: Towards Safe Domain Generalization under Category Shift"></a>Activate and Reject: Towards Safe Domain Generalization under Category Shift</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04724">http://arxiv.org/abs/2310.04724</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chaoqi Chen, Luyao Tang, Leitian Tao, Hong-Yu Zhou, Yue Huang, Xiaoguang Han, Yizhou Yu</li>
<li>for: 本研究旨在解决深度神经网络在开放世界中实现满意的准确率的问题，特别是在不同领域和物种出现时能够同时探测未知类型样本和知名类型样本的检测问题。</li>
<li>methods: 我们提出了一种Activate and Reject（ART）框架，通过在训练时间期间优化未知类型的概率，然后使用权重平滑来缓解过自信问题。在测试时，我们引入了一种步骤式在线适应方法，通过跨领域最近邻和类prototype信息来预测标签，不需要更新网络参数或使用阈值机制。</li>
<li>results: 我们的实验表明，ART可以提高深度网络的普适能力，对不同的视觉任务进行改进。对于图像分类任务，ART提高了H-score的平均提升率为6.1%，相比之下前一个最佳方法。对于物体检测和 semantic segmentation，我们建立了新的标准 bencmarks，并实现了竞争性的表现。<details>
<summary>Abstract</summary>
Albeit the notable performance on in-domain test points, it is non-trivial for deep neural networks to attain satisfactory accuracy when deploying in the open world, where novel domains and object classes often occur. In this paper, we study a practical problem of Domain Generalization under Category Shift (DGCS), which aims to simultaneously detect unknown-class samples and classify known-class samples in the target domains. Compared to prior DG works, we face two new challenges: 1) how to learn the concept of ``unknown'' during training with only source known-class samples, and 2) how to adapt the source-trained model to unseen environments for safe model deployment. To this end, we propose a novel Activate and Reject (ART) framework to reshape the model's decision boundary to accommodate unknown classes and conduct post hoc modification to further discriminate known and unknown classes using unlabeled test data. Specifically, during training, we promote the response to the unknown by optimizing the unknown probability and then smoothing the overall output to mitigate the overconfidence issue. At test time, we introduce a step-wise online adaptation method that predicts the label by virtue of the cross-domain nearest neighbor and class prototype information without updating the network's parameters or using threshold-based mechanisms. Experiments reveal that ART consistently improves the generalization capability of deep networks on different vision tasks. For image classification, ART improves the H-score by 6.1% on average compared to the previous best method. For object detection and semantic segmentation, we establish new benchmarks and achieve competitive performance.
</details>
<details>
<summary>摘要</summary>
尽管深度神经网络在域内测试点上表现出色，但在开放世界中部署时，它们很难达到满意的准确率。在这篇论文中，我们研究了适用于领域总结下的类别转换问题（DGCS），该问题的目标是同时检测未知类样本并将知道类样本分类到目标领域中。相比于先前的DG工作，我们面临两个新的挑战：1）如何在训练期间学习“未知”的概念，只使用源领域知道类样本；2）如何适应到未经见过的环境中安全地部署模型。为此，我们提出了一种名为Activate and Reject（ART）框架，用于重塑模型的决策边界，以便容纳未知类和进行后续修改以进一步分类知道和未知类样本使用无标注测试数据。在训练期间，我们通过优化未知概率来提高模型对未知类的应答，然后使用权重平滑来缓解过自信问题。在测试时，我们引入了一种步骤式在线适应方法，通过跨领域最近邻和类型范围信息来预测标签，不需要更新网络参数或使用阈值机制。实验表明，ART可以在不同的视觉任务上提高深度网络的总成绩。对于图像分类，ART提高了H-score的平均提升为6.1%，较前一个最佳方法。对于物体检测和semantic segmentation，我们建立了新的benchmark和获得了竞争性的表现。
</details></li>
</ul>
<hr>
<h2 id="Memory-Constrained-Semantic-Segmentation-for-Ultra-High-Resolution-UAV-Imagery"><a href="#Memory-Constrained-Semantic-Segmentation-for-Ultra-High-Resolution-UAV-Imagery" class="headerlink" title="Memory-Constrained Semantic Segmentation for Ultra-High Resolution UAV Imagery"></a>Memory-Constrained Semantic Segmentation for Ultra-High Resolution UAV Imagery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04721">http://arxiv.org/abs/2310.04721</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qi Li, Jiaxin Cai, Yuanlong Yu, Jason Gu, Jia Pan, Wenxi Liu</li>
<li>for: 这篇论文主要目的是解决无人机图像分析中的高分辨率图像分类问题，特别是在具有 GPU 内存限制的 Computational Device 上进行高效的分类。</li>
<li>methods: 本文提出了一个 GPU 内存有效的和高效的框架，实现了本地推理而不需要访问对应像的高分辨率信息。具体来说，我们提出了一个新的空间导向高分辨率查询模组，可以透过查询邻近的潜在嵌入对象来预测每个像素的分类结果，而不需要访问高分辨率信息。此外，我们还提出了一个高效的内存基于的互动方案，以corrrect potential的Semantic Bias 。</li>
<li>results: 在实验中，我们使用了公共的标准库 benchmark 进行评估，并在小型和大型 GPU 内存使用限制下 achieve 了 superior 的表现。<details>
<summary>Abstract</summary>
Amidst the swift advancements in photography and sensor technologies, high-definition cameras have become commonplace in the deployment of Unmanned Aerial Vehicles (UAVs) for diverse operational purposes. Within the domain of UAV imagery analysis, the segmentation of ultra-high resolution images emerges as a substantial and intricate challenge, especially when grappling with the constraints imposed by GPU memory-restricted computational devices. This paper delves into the intricate problem of achieving efficient and effective segmentation of ultra-high resolution UAV imagery, while operating under stringent GPU memory limitation. The strategy of existing approaches is to downscale the images to achieve computationally efficient segmentation. However, this strategy tends to overlook smaller, thinner, and curvilinear regions. To address this problem, we propose a GPU memory-efficient and effective framework for local inference without accessing the context beyond local patches. In particular, we introduce a novel spatial-guided high-resolution query module, which predicts pixel-wise segmentation results with high quality only by querying nearest latent embeddings with the guidance of high-resolution information. Additionally, we present an efficient memory-based interaction scheme to correct potential semantic bias of the underlying high-resolution information by associating cross-image contextual semantics. For evaluation of our approach, we perform comprehensive experiments over public benchmarks and achieve superior performance under both conditions of small and large GPU memory usage limitations. We will release the model and codes in the future.
</details>
<details>
<summary>摘要</summary>
在摄像头和感知技术的快速进步下，高清晰相机在无人航空器（UAV）的应用中变得普遍。在UAV成像分析领域，分解超高清晰图像成为一项困难和复杂的任务，尤其是在面临GPU内存限制的计算设备上。本文探讨如何在GPU内存限制下实现高效和高质量的超高清晰图像分解方法。现有方法的策略是下samples the images to achieve computationally efficient segmentation, but this approach tends to overlook smaller, thinner, and curvilinear regions.为解决这问题，我们提出了一种GPU内存高效和可靠的框架，用于本地推理而无需访问背景信息。具体来说，我们引入了一种新的空间指导高分辨率查询模块，可以通过询问最近的潜在嵌入来预测每个像素的分类结果，并且只需考虑当前局部信息。此外，我们还提出了一种高效的内存基于的交互方案，以 corralling potential semantic bias of the underlying high-resolution information by associating cross-image contextual semantics.为评估我们的方法，我们在公共benchmark上进行了广泛的实验，并在小和大GPU内存使用限制下达到了superior表现。我们将在未来释放模型和代码。
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Survey-on-Deep-Neural-Image-Deblurring"><a href="#A-Comprehensive-Survey-on-Deep-Neural-Image-Deblurring" class="headerlink" title="A Comprehensive Survey on Deep Neural Image Deblurring"></a>A Comprehensive Survey on Deep Neural Image Deblurring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04719">http://arxiv.org/abs/2310.04719</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sajjad Amrollahi Biyouki, Hoon Hwangbo</li>
<li>for: 图像锐化纷和提高图像质量，提高图像的纹理和对象视觉。</li>
<li>methods: 使用深度神经网络，包括盲目和非盲目图像锐化方法。</li>
<li>results: 深度神经网络在图像锐化方面带来了一场大进步，提高了性能指标和数据集的使用。但目前还存在一些挑战和研究空白，未来的研究可能把焦点放在这些领域。<details>
<summary>Abstract</summary>
Image deblurring tries to eliminate degradation elements of an image causing blurriness and improve the quality of an image for better texture and object visualization. Traditionally, prior-based optimization approaches predominated in image deblurring, but deep neural networks recently brought a major breakthrough in the field. In this paper, we comprehensively review the recent progress of the deep neural architectures in both blind and non-blind image deblurring. We outline the most popular deep neural network structures used in deblurring applications, describe their strengths and novelties, summarize performance metrics, and introduce broadly used datasets. In addition, we discuss the current challenges and research gaps in this domain and suggest potential research directions for future works.
</details>
<details>
<summary>摘要</summary>
图像锐化尝试消除图像模糊的因素，提高图像质量，以便更好地看到图像的文字和物体视觉。在过去，基于优先的优化方法曾经主导图像锐化领域，但是最近，深度神经网络在这个领域带来了一场大的突破。在这篇评论中，我们全面回顾了最近深度神经网络在图像锐化中的进步，包括无监控和监控图像锐化。我们列出了最受欢迎的深度神经网络结构，描述了它们的优势和创新，概括性能指标，并介绍了广泛使用的数据集。此外，我们讨论了当前领域的挑战和研究缺陷，并提出了未来研究的可能性。
</details></li>
</ul>
<hr>
<h2 id="Reinforced-UI-Instruction-Grounding-Towards-a-Generic-UI-Task-Automation-API"><a href="#Reinforced-UI-Instruction-Grounding-Towards-a-Generic-UI-Task-Automation-API" class="headerlink" title="Reinforced UI Instruction Grounding: Towards a Generic UI Task Automation API"></a>Reinforced UI Instruction Grounding: Towards a Generic UI Task Automation API</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04716">http://arxiv.org/abs/2310.04716</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhizheng Zhang, Wenxuan Xie, Xiaoyi Zhang, Yan Lu</li>
<li>for:  automatize numerous AI tasks by connecting Large Language Models (LLMs) to various domain-specific models or APIs</li>
<li>methods:  build a multimodal model to ground natural language instructions in given UI screenshots as a generic UI task automation executor, using a visual encoder and a language decoder, and an innovative Reinforcement Learning (RL) based algorithm</li>
<li>results:  outperforms the state-of-the-art methods by a clear margin, showing the potential as a generic UI task automation API<details>
<summary>Abstract</summary>
Recent popularity of Large Language Models (LLMs) has opened countless possibilities in automating numerous AI tasks by connecting LLMs to various domain-specific models or APIs, where LLMs serve as dispatchers while domain-specific models or APIs are action executors. Despite the vast numbers of domain-specific models/APIs, they still struggle to comprehensively cover super diverse automation demands in the interaction between human and User Interfaces (UIs). In this work, we build a multimodal model to ground natural language instructions in given UI screenshots as a generic UI task automation executor. This metadata-free grounding model, consisting of a visual encoder and a language decoder, is first pretrained on well studied document understanding tasks and then learns to decode spatial information from UI screenshots in a promptable way. To facilitate the exploitation of image-to-text pretrained knowledge, we follow the pixel-to-sequence paradigm to predict geometric coordinates in a sequence of tokens using a language decoder. We further propose an innovative Reinforcement Learning (RL) based algorithm to supervise the tokens in such sequence jointly with visually semantic metrics, which effectively strengthens the spatial decoding capability of the pixel-to-sequence paradigm. Extensive experiments demonstrate our proposed reinforced UI instruction grounding model outperforms the state-of-the-art methods by a clear margin and shows the potential as a generic UI task automation API.
</details>
<details>
<summary>摘要</summary>
近年来，大型语言模型（LLM）的流行性打开了许多自动化AI任务的可能性，通过将LLM与不同领域模型或API连接起来，使LLM serve为调度器，而领域特定模型或API则是行动执行者。尽管有很多领域特定模型/API，但它们仍然无法全面覆盖人机交互中的自动化需求。在这种情况下，我们建立了一个多模态模型，将自然语言指令与给定的UIcreenshot相关联，作为一个通用的UI任务自动化执行器。这个没有元数据的grounding模型由视觉编码器和语言解码器组成，首先在已有的文档理解任务上进行预训练，然后学习从UIcreenshot中提取空间信息的decode方法。为了利用图像到文本的预训练知识，我们采用了像素到序列的方法，通过语言解码器预测图像中的几何坐标序列。我们还提出了一种创新的强化学习（RL）算法，以便同时监督序列中的图像Semantic метрик，从而提高图像到序列的空间解码能力。经过广泛的实验，我们的提出的强化UI指令grounding模型超越了当前状态的方法，并显示出了作为通用UI任务自动化API的潜在力量。
</details></li>
</ul>
<hr>
<h2 id="Generalized-Robust-Test-Time-Adaptation-in-Continuous-Dynamic-Scenarios"><a href="#Generalized-Robust-Test-Time-Adaptation-in-Continuous-Dynamic-Scenarios" class="headerlink" title="Generalized Robust Test-Time Adaptation in Continuous Dynamic Scenarios"></a>Generalized Robust Test-Time Adaptation in Continuous Dynamic Scenarios</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04714">http://arxiv.org/abs/2310.04714</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bit-da/rotta">https://github.com/bit-da/rotta</a></li>
<li>paper_authors: Shuang Li, Longhui Yuan, Binhui Xie, Tao Yang<br>for: 这个研究旨在解决实际应用中出现的同时进行构成和标签shift的问题，即在测试过程中，测试数据流中的标签和特征都在不断变化。methods: 这个研究使用的方法包括Robust Parameter Adaptation、recalibration of batch normalization、source knowledge regularization和Bias-Guided Output Adaptation等。这些方法可以帮助模型在测试过程中快速适应测试数据流中的变化。results: 实验结果显示，GRoTTA方法在PTTA设定下具有较高的效果，较以往的竞争者多个项目。这显示GRoTTA方法可以帮助模型在实际应用中更好地适应测试数据流中的变化。<details>
<summary>Abstract</summary>
Test-time adaptation (TTA) adapts the pre-trained models to test distributions during the inference phase exclusively employing unlabeled test data streams, which holds great value for the deployment of models in real-world applications. Numerous studies have achieved promising performance on simplistic test streams, characterized by independently and uniformly sampled test data originating from a fixed target data distribution. However, these methods frequently prove ineffective in practical scenarios, where both continual covariate shift and continual label shift occur simultaneously, i.e., data and label distributions change concurrently and continually over time. In this study, a more challenging Practical Test-Time Adaptation (PTTA) setup is introduced, which takes into account the concurrent presence of continual covariate shift and continual label shift, and we propose a Generalized Robust Test-Time Adaptation (GRoTTA) method to effectively address the difficult problem. We start by steadily adapting the model through Robust Parameter Adaptation to make balanced predictions for test samples. To be specific, firstly, the effects of continual label shift are eliminated by enforcing the model to learn from a uniform label distribution and introducing recalibration of batch normalization to ensure stability. Secondly, the continual covariate shift is alleviated by employing a source knowledge regularization with the teacher-student model to update parameters. Considering the potential information in the test stream, we further refine the balanced predictions by Bias-Guided Output Adaptation, which exploits latent structure in the feature space and is adaptive to the imbalanced label distribution. Extensive experiments demonstrate GRoTTA outperforms the existing competitors by a large margin under PTTA setting, rendering it highly conducive for adoption in real-world applications.
</details>
<details>
<summary>摘要</summary>
测试时适应（TTA）在推理阶段 exclusively使用无标注测试数据流来适应测试分布，这对实际应用中的模型部署具有很大的价值。许多研究已经在简单的测试流中达到了有 promise的性能，但这些方法经常在实际场景中失效，因为测试数据和标签分布在时间上同时发生变化。在本研究中，我们引入了更加具有挑战性的实际测试适应（PTTA）设定，该设定考虑了同时发生的连续变量和连续标签变换，并提出了一种通用鲁棒测试适应（GRoTTA）方法来有效地解决这个困难问题。我们首先通过鲁棒参数适应来稳定地适应测试样本。更 Specifically，我们首先消除了连续标签变换的影响，通过使用均匀标签分布来学习，并通过批量正则化来保证稳定性。其次，我们使用教师模型来更新参数，以 alleviate连续变量变换。另外，我们还利用测试流中的可能信息，通过偏好导向输出适应来细化平衡预测，这里利用了特征空间中的潜在结构，并且是适应偏高标签分布的。我们进行了广泛的实验，结果显示GRoTTA在PTTA设定下高效地击败了现有的竞争对手，这表明GRoTTA在实际应用中具有很高的潜力。
</details></li>
</ul>
<hr>
<h2 id="UFD-PRiME-Unsupervised-Joint-Learning-of-Optical-Flow-and-Stereo-Depth-through-Pixel-Level-Rigid-Motion-Estimation"><a href="#UFD-PRiME-Unsupervised-Joint-Learning-of-Optical-Flow-and-Stereo-Depth-through-Pixel-Level-Rigid-Motion-Estimation" class="headerlink" title="UFD-PRiME: Unsupervised Joint Learning of Optical Flow and Stereo Depth through Pixel-Level Rigid Motion Estimation"></a>UFD-PRiME: Unsupervised Joint Learning of Optical Flow and Stereo Depth through Pixel-Level Rigid Motion Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04712">http://arxiv.org/abs/2310.04712</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuai Yuan, Carlo Tomasi</li>
<li>for: 这篇论文是为了提出一种基于joint training的光流和立体动态模型的方法，以提高光流的精度和详细程度。</li>
<li>methods: 这篇论文使用了一种两个网络 architecture，第一个网络用于 estimate flow和 disparity jointly，而第二个网络用于使用 optical flow作为 Pseudo-labels来估算3D rigid motion和重建 optical flow。最后一个阶段使用了这两个网络的输出进行融合。</li>
<li>results: 这篇论文在 KITTI-2015 测试集上 achieved 7.36% optical flow error，与之前的state-of-the-art 9.38%的错误率相比，提高了大量的精度和详细程度。此外，这种方法也在 stero depth 方面 achieved slightly better or comparable results。<details>
<summary>Abstract</summary>
Both optical flow and stereo disparities are image matches and can therefore benefit from joint training. Depth and 3D motion provide geometric rather than photometric information and can further improve optical flow. Accordingly, we design a first network that estimates flow and disparity jointly and is trained without supervision. A second network, trained with optical flow from the first as pseudo-labels, takes disparities from the first network, estimates 3D rigid motion at every pixel, and reconstructs optical flow again. A final stage fuses the outputs from the two networks. In contrast with previous methods that only consider camera motion, our method also estimates the rigid motions of dynamic objects, which are of key interest in applications. This leads to better optical flow with visibly more detailed occlusions and object boundaries as a result. Our unsupervised pipeline achieves 7.36% optical flow error on the KITTI-2015 benchmark and outperforms the previous state-of-the-art 9.38% by a wide margin. It also achieves slightly better or comparable stereo depth results. Code will be made available.
</details>
<details>
<summary>摘要</summary>
beide 光流和立体差是图像匹配，因此可以从共同训练中受益。深度和3D运动提供geometry rather than photometry信息，可以进一步提高光流。因此，我们设计了一个首先网络，并将流和差 jointly estimated，并在无监督下训练。其次，使用光流从首先网络中得到的pseudo-labels，对差从首先网络中得到，并在每个像素处估计3D刚性运动，并重新计算光流。最后，将两个网络的输出进行融合。与之前的方法只考虑相机运动的情况下，我们的方法还估计了动态对象的刚性运动，这些运动是应用中关键的。这导致了更好的光流，有较明显的 occlusion和物体边界。我们的无监督管道在 KITTI-2015 测试benchmark上取得了7.36%的光流错误率，与之前的状态 искусственного智能 9.38% 的差距非常大。它还实现了与之前或相当的 stero depth 结果。我们将代码公开。
</details></li>
</ul>
<hr>
<h2 id="Multi-scale-MRI-reconstruction-via-dilated-ensemble-networks"><a href="#Multi-scale-MRI-reconstruction-via-dilated-ensemble-networks" class="headerlink" title="Multi-scale MRI reconstruction via dilated ensemble networks"></a>Multi-scale MRI reconstruction via dilated ensemble networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04705">http://arxiv.org/abs/2310.04705</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wendi Ma, Marlon Bran Lorenzana, Wei Dai, Hongfu Sun, Shekhar S. Chandra</li>
<li>for: 本文旨在提出一种高效的多尺度重建网络，用于提高MRI重建图像质量。</li>
<li>methods: 本文使用了扩展 convolutions 技术，并采用了多路分支和堆叠连接来保留分辨率和增加缩放级别。此外，文章还提出了一种复杂值版本，使用复杂 convolutions 来利用phas信息。</li>
<li>results: 实验结果表明，实数版本的模型比常见重建架构和一种多尺度网络更高效，而复杂值版本在phas信息更加强的情况下得到了更好的质量result。<details>
<summary>Abstract</summary>
As aliasing artefacts are highly structural and non-local, many MRI reconstruction networks use pooling to enlarge filter coverage and incorporate global context. However, this inadvertently impedes fine detail recovery as downsampling creates a resolution bottleneck. Moreover, real and imaginary features are commonly split into separate channels, discarding phase information particularly important to high frequency textures. In this work, we introduce an efficient multi-scale reconstruction network using dilated convolutions to preserve resolution and experiment with a complex-valued version using complex convolutions. Inspired by parallel dilated filters, multiple receptive fields are processed simultaneously with branches that see both large structural artefacts and fine local features. We also adopt dense residual connections for feature aggregation to efficiently increase scale and the deep cascade global architecture to reduce overfitting. The real-valued version of this model outperformed common reconstruction architectures as well as a state-of-the-art multi-scale network whilst being three times more efficient. The complex-valued network yielded better qualitative results when more phase information was present.
</details>
<details>
<summary>摘要</summary>
“为了减少别名遗留物的影响，许多MRI重建网络使用抢取来扩大范围和包含全球观点。然而，这会意外地阻碍细节重建，因为下推过滤导致解析瓶颈。此外，实际和想像的特征通常会被分配到不同的通道中，将相位资讯特别是高频率 текстур中的重要资讯排除。在这个工作中，我们引入了一个高效的多値重建网络，使用扩大核心来保留分辨率和实验使用复数值版本使用复数核心。受到平行扩大滤镜的启发，我们的网络可以同时处理多个接收场，处理大规模结构遗留物和细部本地特征。我们还采用紧密的复原连接来汇集特征，以增加缩减的尺度和深度汇集全球架构来降低过滤。实际版本的这个模型比常用重建架构和多値网络更高效，并且三倍更高效。复数值版本的模型在具有更多相位资讯时表现更好。”
</details></li>
</ul>
<hr>
<h2 id="Tree-GPT-Modular-Large-Language-Model-Expert-System-for-Forest-Remote-Sensing-Image-Understanding-and-Interactive-Analysis"><a href="#Tree-GPT-Modular-Large-Language-Model-Expert-System-for-Forest-Remote-Sensing-Image-Understanding-and-Interactive-Analysis" class="headerlink" title="Tree-GPT: Modular Large Language Model Expert System for Forest Remote Sensing Image Understanding and Interactive Analysis"></a>Tree-GPT: Modular Large Language Model Expert System for Forest Remote Sensing Image Understanding and Interactive Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04698">http://arxiv.org/abs/2310.04698</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siqi Du, Shengjun Tang, Weixi Wang, Xiaoming Li, Renzhong Guo</li>
<li>for: 这个论文旨在提高森林远程感知数据的分析效率，通过将大语言模型（LLMs）integrated into forestry remote sensing data workflow。</li>
<li>methods: 该论文提出了一种模块化LLM专家系统，名为Tree-GPT，它将图像理解模块、域知识库和工具链 integrate into LLMs，使其能够理解图像、获得准确的知识、生成代码和在本地环境中进行数据分析。</li>
<li>results: 该论文测试了这些任务，包括搜索、视觉化和机器学习分析，并得到了良好的结果，表明Tree-GPT可以在森林研究和环境科学中动态使用LLMs。<details>
<summary>Abstract</summary>
This paper introduces a novel framework, Tree-GPT, which incorporates Large Language Models (LLMs) into the forestry remote sensing data workflow, thereby enhancing the efficiency of data analysis. Currently, LLMs are unable to extract or comprehend information from images and may generate inaccurate text due to a lack of domain knowledge, limiting their use in forestry data analysis. To address this issue, we propose a modular LLM expert system, Tree-GPT, that integrates image understanding modules, domain knowledge bases, and toolchains. This empowers LLMs with the ability to comprehend images, acquire accurate knowledge, generate code, and perform data analysis in a local environment. Specifically, the image understanding module extracts structured information from forest remote sensing images by utilizing automatic or interactive generation of prompts to guide the Segment Anything Model (SAM) in generating and selecting optimal tree segmentation results. The system then calculates tree structural parameters based on these results and stores them in a database. Upon receiving a specific natural language instruction, the LLM generates code based on a thought chain to accomplish the analysis task. The code is then executed by an LLM agent in a local environment and . For ecological parameter calculations, the system retrieves the corresponding knowledge from the knowledge base and inputs it into the LLM to guide the generation of accurate code. We tested this system on several tasks, including Search, Visualization, and Machine Learning Analysis. The prototype system performed well, demonstrating the potential for dynamic usage of LLMs in forestry research and environmental sciences.
</details>
<details>
<summary>摘要</summary>
The image understanding module extracts structured information from forest remote sensing images by utilizing automatic or interactive generation of prompts to guide the Segment Anything Model (SAM) in generating and selecting optimal tree segmentation results. The system then calculates tree structural parameters based on these results and stores them in a database. Upon receiving a specific natural language instruction, the LLM generates code based on a thought chain to accomplish the analysis task. The code is then executed by an LLM agent in a local environment. For ecological parameter calculations, the system retrieves the corresponding knowledge from the knowledge base and inputs it into the LLM to guide the generation of accurate code.The prototype system was tested on several tasks, including search, visualization, and machine learning analysis, and performed well, demonstrating the potential for dynamic usage of LLMs in forestry research and environmental sciences.
</details></li>
</ul>
<hr>
<h2 id="SeeDS-Semantic-Separable-Diffusion-Synthesizer-for-Zero-shot-Food-Detection"><a href="#SeeDS-Semantic-Separable-Diffusion-Synthesizer-for-Zero-shot-Food-Detection" class="headerlink" title="SeeDS: Semantic Separable Diffusion Synthesizer for Zero-shot Food Detection"></a>SeeDS: Semantic Separable Diffusion Synthesizer for Zero-shot Food Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04689">http://arxiv.org/abs/2310.04689</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lancezpf/seeds">https://github.com/lancezpf/seeds</a></li>
<li>paper_authors: Pengfei Zhou, Weiqing Min, Yang Zhang, Jiajun Song, Ying Jin, Shuqiang Jiang</li>
<li>for:  zeroshot food detection (ZSFD)</li>
<li>methods:  semantic separable diffusion synthesizer (SeeDS) framework, including semantic separable synthesizing module (S$^3$M) and region feature denoising diffusion model (RFDDM)</li>
<li>results:  state-of-the-art ZSFD performance on two food datasets (ZSFooD and UECFOOD-256), and effectiveness on general ZSD datasets (PASCAL VOC and MS COCO)<details>
<summary>Abstract</summary>
Food detection is becoming a fundamental task in food computing that supports various multimedia applications, including food recommendation and dietary monitoring. To deal with real-world scenarios, food detection needs to localize and recognize novel food objects that are not seen during training, demanding Zero-Shot Detection (ZSD). However, the complexity of semantic attributes and intra-class feature diversity poses challenges for ZSD methods in distinguishing fine-grained food classes. To tackle this, we propose the Semantic Separable Diffusion Synthesizer (SeeDS) framework for Zero-Shot Food Detection (ZSFD). SeeDS consists of two modules: a Semantic Separable Synthesizing Module (S$^3$M) and a Region Feature Denoising Diffusion Model (RFDDM). The S$^3$M learns the disentangled semantic representation for complex food attributes from ingredients and cuisines, and synthesizes discriminative food features via enhanced semantic information. The RFDDM utilizes a novel diffusion model to generate diversified region features and enhances ZSFD via fine-grained synthesized features. Extensive experiments show the state-of-the-art ZSFD performance of our proposed method on two food datasets, ZSFooD and UECFOOD-256. Moreover, SeeDS also maintains effectiveness on general ZSD datasets, PASCAL VOC and MS COCO. The code and dataset can be found at https://github.com/LanceZPF/SeeDS.
</details>
<details>
<summary>摘要</summary>
食物检测已成为食物计算中的基本任务，支持多媒体应用程序，包括食物推荐和饮食监测。要处理实际场景，食物检测需要本地化和识别新的食物对象，需要零Instance检测（ZSD）。然而，食物的semantic特征和内部特征多样性带来了ZSD方法在细腻食物类型之间分辨的挑战。为解决这个问题，我们提出了Semantic Separable Diffusion Synthesizer（SeeDS）框架，用于零Instance食物检测（ZSFD）。SeeDS包括两个模块：Semantic Separable Synthesizing Module（S$^3$M）和Region Feature Denoising Diffusion Model（RFDDM）。S$^3$M学习了复杂的食物特征的分离 semantic representation，从原料和菜系中提取出各种Semantic信息，并将其拼接成特征。RFDDM使用了一种新的扩散模型，生成了多样化的区域特征，并通过细腻的合成特征提高了ZSFD的性能。我们的提议方法在两个食物数据集上实现了状态的ZSFD性能，即ZSFooD和UECFOOD-256。此外，SeeDS还保持了在通用ZSD数据集上的有效性，包括PASCAL VOC和MS COCO。codes和数据集可以在https://github.com/LanceZPF/SeeDS中找到。
</details></li>
</ul>
<hr>
<h2 id="PatchProto-Networks-for-Few-shot-Visual-Anomaly-Classification"><a href="#PatchProto-Networks-for-Few-shot-Visual-Anomaly-Classification" class="headerlink" title="PatchProto Networks for Few-shot Visual Anomaly Classification"></a>PatchProto Networks for Few-shot Visual Anomaly Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04688">http://arxiv.org/abs/2310.04688</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jian Wang, Yue Zhuo</li>
<li>for: 本研究は、缺乏异常样本的实际问题に针对的，即几何学学习（FSL）。</li>
<li>methods: 该研究提出了一种基于几何学学习的方法，称为PatchProto网络，它仅提取了异常区域的 CNN特征，作为几何学学习的原型。</li>
<li>results: 对于MVTec-AD数据集，相比基本的几何学学习分类器，PatchProto网络显著提高了几何学学习异常分类精度。<details>
<summary>Abstract</summary>
The visual anomaly diagnosis can automatically analyze the defective products, which has been widely applied in industrial quality inspection. The anomaly classification can classify the defective products into different categories. However, the anomaly samples are hard to access in practice, which impedes the training of canonical machine learning models. This paper studies a practical issue that anomaly samples for training are extremely scarce, i.e., few-shot learning (FSL). Utilizing the sufficient normal samples, we propose PatchProto networks for few-shot anomaly classification. Different from classical FSL methods, PatchProto networks only extract CNN features of defective regions of interest, which serves as the prototypes for few-shot learning. Compared with basic few-shot classifier, the experiment results on MVTec-AD dataset show PatchProto networks significantly improve the few-shot anomaly classification accuracy.
</details>
<details>
<summary>摘要</summary>
《图像异常诊断可以自动分析异常产品，广泛应用于工业质量检测。异常分类可以将异常产品分为不同类别。然而，异常样本在实践中很难获取，这限制了 canonical 机器学习模型的训练。本文研究了实际上异常样本训练很 scarce 的问题，即 few-shot learning（FSL）。使用 suffcient normal samples，我们提议 PatchProto 网络 для few-shot 异常分类。与传统 FSL 方法不同，PatchProto 网络只提取异常区域的 CNN 特征，这些特征 serves 为 few-shot 学习的原型。与基本 FSL 分类器比较，我们在 MVTec-AD 数据集上进行了实验，结果显示 PatchProto 网络可以显著提高 few-shot 异常分类精度。》Note: "异常" (ànòu) in the text refers to "anomaly" or "defect", and "异常样本" (ànòu yàngxì) refers to "anomalous samples" or "defective products".
</details></li>
</ul>
<hr>
<h2 id="High-Visual-Fidelity-Learned-Video-Compression"><a href="#High-Visual-Fidelity-Learned-Video-Compression" class="headerlink" title="High Visual-Fidelity Learned Video Compression"></a>High Visual-Fidelity Learned Video Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04679">http://arxiv.org/abs/2310.04679</a></li>
<li>repo_url: None</li>
<li>paper_authors: Meng Li, Yibo Shi, Jing Wang, Yunqi Huang</li>
<li>for: 提高视频应用的质量，提出高视质量学习视频压缩框架(HVFVC)。</li>
<li>methods: 提出一种新的信任度基于的特征重建方法，以解决新出现的区域重建质量不佳的问题，并使用周期赔偿损失来抑制减采融合操作和优化导致的检查板抖波问题。</li>
<li>results: 对比VVC标准，HVFVC在50%的比特率下达到了出色的感知质量，显著超越VVC标准。<details>
<summary>Abstract</summary>
With the growing demand for video applications, many advanced learned video compression methods have been developed, outperforming traditional methods in terms of objective quality metrics such as PSNR. Existing methods primarily focus on objective quality but tend to overlook perceptual quality. Directly incorporating perceptual loss into a learned video compression framework is nontrivial and raises several perceptual quality issues that need to be addressed. In this paper, we investigated these issues in learned video compression and propose a novel High Visual-Fidelity Learned Video Compression framework (HVFVC). Specifically, we design a novel confidence-based feature reconstruction method to address the issue of poor reconstruction in newly-emerged regions, which significantly improves the visual quality of the reconstruction. Furthermore, we present a periodic compensation loss to mitigate the checkerboard artifacts related to deconvolution operation and optimization. Extensive experiments have shown that the proposed HVFVC achieves excellent perceptual quality, outperforming the latest VVC standard with only 50% required bitrate.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)随着视频应用的增长需求，许多高级学习视频压缩方法已经被开发出来，比传统方法在PSNR等 объекive 质量指标上表现更高。现有方法主要关注对象质量，忽略了人类视觉质量。直接在学习视频压缩框架中引入人类视觉损失是非常困难的，并且会引起一些人类视觉质量问题。在这篇论文中，我们investigated these issues in learned video compression and propose a novel High Visual-Fidelity Learned Video Compression framework (HVFVC). Specifically, we design a novel confidence-based feature reconstruction method to address the issue of poor reconstruction in newly-emerged regions, which significantly improves the visual quality of the reconstruction. Furthermore, we present a periodic compensation loss to mitigate the checkerboard artifacts related to deconvolution operation and optimization. Extensive experiments have shown that the proposed HVFVC achieves excellent perceptual quality, outperforming the latest VVC standard with only 50% required bitrate.
</details></li>
</ul>
<hr>
<h2 id="AG-CRC-Anatomy-Guided-Colorectal-Cancer-Segmentation-in-CT-with-Imperfect-Anatomical-Knowledge"><a href="#AG-CRC-Anatomy-Guided-Colorectal-Cancer-Segmentation-in-CT-with-Imperfect-Anatomical-Knowledge" class="headerlink" title="AG-CRC: Anatomy-Guided Colorectal Cancer Segmentation in CT with Imperfect Anatomical Knowledge"></a>AG-CRC: Anatomy-Guided Colorectal Cancer Segmentation in CT with Imperfect Anatomical Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04677">http://arxiv.org/abs/2310.04677</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rongzhao-zhang/ag-crc">https://github.com/rongzhao-zhang/ag-crc</a></li>
<li>paper_authors: Rongzhao Zhang, Zhian Bai, Ruoying Yu, Wenrao Pang, Lingyun Wang, Lifeng Zhu, Xiaofan Zhang, Huan Zhang, Weiguo Hu</li>
<li>for: 本研究旨在帮助解决肠RECTAL癌（CRC）从 computed tomography（CT）成像中进行精准的肿瘤分 segmentation，通过利用现有的 deep learning 模型生成的多器官分割（MOS）masks 和自适应的学习策略。</li>
<li>methods: 本研究提出了一种新的 Anatomy-Guided 分 segmentation框架，包括获取 MOS masks，提取更加稳定的器官兴趣（OOI）mask，以及一种基于器官的自适应训练 patch sampling 策略和一种基于管道器官的自适应学习方法。</li>
<li>results: 对于 two CRC segmentation 数据集，提出的方法可以 Achieve 5% 到 9% 的 Dice 指标提升，而且ablation study 进一步证明每个提出的组件的有效性。<details>
<summary>Abstract</summary>
When delineating lesions from medical images, a human expert can always keep in mind the anatomical structure behind the voxels. However, although high-quality (though not perfect) anatomical information can be retrieved from computed tomography (CT) scans with modern deep learning algorithms, it is still an open problem how these automatically generated organ masks can assist in addressing challenging lesion segmentation tasks, such as the segmentation of colorectal cancer (CRC). In this paper, we develop a novel Anatomy-Guided segmentation framework to exploit the auto-generated organ masks to aid CRC segmentation from CT, namely AG-CRC. First, we obtain multi-organ segmentation (MOS) masks with existing MOS models (e.g., TotalSegmentor) and further derive a more robust organ of interest (OOI) mask that may cover most of the colon-rectum and CRC voxels. Then, we propose an anatomy-guided training patch sampling strategy by optimizing a heuristic gain function that considers both the proximity of important regions (e.g., the tumor or organs of interest) and sample diversity. Third, we design a novel self-supervised learning scheme inspired by the topology of tubular organs like the colon to boost the model performance further. Finally, we employ a masked loss scheme to guide the model to focus solely on the essential learning region. We extensively evaluate the proposed method on two CRC segmentation datasets, where substantial performance improvement (5% to 9% in Dice) is achieved over current state-of-the-art medical image segmentation models, and the ablation studies further evidence the efficacy of every proposed component.
</details>
<details>
<summary>摘要</summary>
当分类医学图像时，人类专家总是可以保持在脏部结构的背景下。然而，尽管现代深度学习算法可以从 computed tomography（CT）扫描机器学习得到高质量（尚未完美）的解剖信息，但是仍然是一个开放的问题如何使用自动生成的器官涂抹来帮助困难的肿瘤分割任务，例如肿瘤分割。在这篇论文中，我们开发了一种新的 Anatomy-Guided 分割框架，以利用自动生成的器官涂抹来帮助 CT 上肿瘤分割，即 AG-CRC。首先，我们通过现有的 MOS 模型（例如 TotalSegmentor）来获得多个器官分割（MOS）mask，并 derivate 更加robust的器官对象（OOI）涂抹，可以覆盖大部分的肠Rectum和肿瘤 voxels。然后，我们提出了一种基于器官解剖 topology的自适应训练 patch sampling 策略，通过优化一个启发函数来考虑Important region（例如肿瘤或器官对象）的 proximity 和样本多样性。第三，我们设计了一种基于管状器官 topology 的自适应学习方案，以进一步提高模型性能。最后，我们采用了一种 masked loss 方案，以导引模型尽量只学习重要的学习区域。我们对两个 CRC 分割数据集进行了广泛的评估，并实现了现有医学图像分割模型的显著性能提高（5% 到 9% 的 Dice），并且ablation studies 进一步证明了每一个提posed component的有效性。
</details></li>
</ul>
<hr>
<h2 id="EasyPhoto-Your-Smart-AI-Photo-Generator"><a href="#EasyPhoto-Your-Smart-AI-Photo-Generator" class="headerlink" title="EasyPhoto: Your Smart AI Photo Generator"></a>EasyPhoto: Your Smart AI Photo Generator</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04672">http://arxiv.org/abs/2310.04672</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aigc-apps/sd-webui-EasyPhoto">https://github.com/aigc-apps/sd-webui-EasyPhoto</a></li>
<li>paper_authors: Ziheng Wu, Jiaqi Xu, Xinyi Zou, Kunzhe Huang, Xing Shi, Jun Huang</li>
<li>for: 这个论文是为了介绍一个名为EasyPhoto的WebUI插件，它使得用户可以通过提供5-20个相关图像来生成人工智能肖像。</li>
<li>methods: 这个插件使用了Gradio库和Stable Diffusion模型，并通过训练LoRA模型来进行特征提取和生成AI照片。</li>
<li>results: 该插件可以根据用户提供的图像生成多种不同的AI照片，包括使用自定义模板和拓展SDXL模型来生成具有更多元素的图像。Here’s the full text in Simplified Chinese:</li>
<li>for: 这个论文是为了介绍一个名为EasyPhoto的WebUI插件，它使得用户可以通过提供5-20个相关图像来生成人工智能肖像。</li>
<li>methods: 这个插件使用了Gradio库和Stable Diffusion模型，并通过训练LoRA模型来进行特征提取和生成AI照片。</li>
<li>results: 该插件可以根据用户提供的图像生成多种不同的AI照片，包括使用自定义模板和拓展SDXL模型来生成具有更多元素的图像。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Stable Diffusion web UI (SD-WebUI) is a comprehensive project that provides a browser interface based on Gradio library for Stable Diffusion models. In this paper, We propose a novel WebUI plugin called EasyPhoto, which enables the generation of AI portraits. By training a digital doppelganger of a specific user ID using 5 to 20 relevant images, the finetuned model (according to the trained LoRA model) allows for the generation of AI photos using arbitrary templates. Our current implementation supports the modification of multiple persons and different photo styles. Furthermore, we allow users to generate fantastic template image with the strong SDXL model, enhancing EasyPhoto's capabilities to deliver more diverse and satisfactory results. The source code for EasyPhoto is available at: https://github.com/aigc-apps/sd-webui-EasyPhoto. We also support a webui-free version by using diffusers: https://github.com/aigc-apps/EasyPhoto. We are continuously enhancing our efforts to expand the EasyPhoto pipeline, making it suitable for any identification (not limited to just the face), and we enthusiastically welcome any intriguing ideas or suggestions.
</details>
<details>
<summary>摘要</summary>
stable diffusion网UI(SD-WebUI)是一个完整的项目，提供基于Gradio库的浏览器界面 для稳定扩散模型。在这篇论文中，我们提出了一个新的WebUI插件called EasyPhoto，它允许用户通过训练一个特定用户ID的数字双胞虫（使用5-20个相关图片进行训练）来生成人工智能照片。我们的当前实现支持多个人物和不同的照片风格的修改。此外，我们允许用户使用强大的SDXL模型来生成备受喜欢的模板图片，从而提高EasyPhoto的能力，为用户提供更多的多样化和满意的结果。EasyPhoto的源代码位于：https://github.com/aigc-apps/sd-webui-EasyPhoto。我们还支持一个无WebUI版本，使用diffusers：https://github.com/aigc-apps/EasyPhoto。我们 continuous enhance our efforts to expand the EasyPhoto pipeline, making it suitable for any identification（不限于脸部），欢迎任何有趣的想法或建议。
</details></li>
</ul>
<hr>
<h2 id="Visual-Abductive-Reasoning-Meets-Driving-Hazard-Prediction-Problem-Formulation-and-Dataset"><a href="#Visual-Abductive-Reasoning-Meets-Driving-Hazard-Prediction-Problem-Formulation-and-Dataset" class="headerlink" title="Visual Abductive Reasoning Meets Driving Hazard Prediction: Problem Formulation and Dataset"></a>Visual Abductive Reasoning Meets Driving Hazard Prediction: Problem Formulation and Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04671">http://arxiv.org/abs/2310.04671</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dhpr-dataset/dhpr-dataset">https://github.com/dhpr-dataset/dhpr-dataset</a></li>
<li>paper_authors: Korawat Charoenpitaks, Van-Quang Nguyen, Masanori Suganuma, Masahiro Takahashi, Ryoma Niihara, Takayuki Okatani</li>
<li>for: 本研究旨在预测司机驾驶过程中可能会遇到的危险。</li>
<li>methods: 该研究使用单个输入图像， captured by 汽车前视摄像头，进行预测。与传统驾驶危险预测方法不同，这种方法不是通过计算模拟或视频异常检测来实现。而是通过高级推理，从静止图像中预测未来事件。</li>
<li>results: 该研究创建了一个名为 DHPR（驾驶危险预测和推理）数据集，包含15W个摄像头图像和相应的车速、假设危险描述和场景中可见元素。这些数据被人工标注员标注，以便识别危险场景并提供可能在几秒钟后发生的危险描述。研究发现，使用不同方法的基eline表现不佳，标志着该领域还有很多研究空间。<details>
<summary>Abstract</summary>
This paper addresses the problem of predicting hazards that drivers may encounter while driving a car. We formulate it as a task of anticipating impending accidents using a single input image captured by car dashcams. Unlike existing approaches to driving hazard prediction that rely on computational simulations or anomaly detection from videos, this study focuses on high-level inference from static images. The problem needs predicting and reasoning about future events based on uncertain observations, which falls under visual abductive reasoning. To enable research in this understudied area, a new dataset named the DHPR (Driving Hazard Prediction and Reasoning) dataset is created. The dataset consists of 15K dashcam images of street scenes, and each image is associated with a tuple containing car speed, a hypothesized hazard description, and visual entities present in the scene. These are annotated by human annotators, who identify risky scenes and provide descriptions of potential accidents that could occur a few seconds later. We present several baseline methods and evaluate their performance on our dataset, identifying remaining issues and discussing future directions. This study contributes to the field by introducing a novel problem formulation and dataset, enabling researchers to explore the potential of multi-modal AI for driving hazard prediction.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文关注在车辆驾驶中预测司机可能遇到的危险问题，使用车载 видео记录机拍摄的单个输入图像。与现有方法不同，这些方法基于计算机生成的模拟或视频异常检测，而这些研究强调高级推理从静止图像中。问题需要预测和理解未来事件，基于不确定的观察，这属于视觉推理。为促进这个未explored的领域的研究，这篇论文创建了名为DHPR（驾驶危险预测和理解）数据集，该数据集包含15,000个摄像头图像街景，每个图像都有速度、假设的危险描述和场景中可见的视觉元素。这些被人工标注员标注为危险场景和可能在几秒钟后发生的危险描述。这篇论文提出了多种基线方法，并对其表现进行评估，并识别未解决的问题和未来方向。这篇论文对驾驶多模态AI的潜在应用做出了贡献，开发了一个新的问题定义和数据集，为研究人员提供了探索可能的多模态AI驾驶危险预测的机会。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Rank-Onset-Occurring-Offset-Representations-for-Micro-Expression-Recognition"><a href="#Learning-to-Rank-Onset-Occurring-Offset-Representations-for-Micro-Expression-Recognition" class="headerlink" title="Learning to Rank Onset-Occurring-Offset Representations for Micro-Expression Recognition"></a>Learning to Rank Onset-Occurring-Offset Representations for Micro-Expression Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04664">http://arxiv.org/abs/2310.04664</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Zhu, Yuan Zong, Jingang Shi, Cheng Lu, Hongli Chang, Wenming Zheng</li>
<li>for: 本研究强调微表情识别（MER）领域，提出了一种可靠和灵活的深度学习方法，即学习排名开始-发生-结束表示（LTR3O）。</li>
<li>methods: LTR3O方法使用了一种名为3O结构的动态减少大小序列结构，该结构包括开始、发生和结束帧，以表示微表情（ME）。这种结构使得可以后续学习ME特异性特征。LTR3O方法还使用了多个3O表示候选者来对每个ME样本进行排名和补做，以确保3O表示候选者的情感表达能够与大表情（MaM）的分布相一致。</li>
<li>results: 实验结果表明，LTR3O方法在三个常用的ME数据库（CASME II、SMIC、SAMM）上表现出了优于当前状态艺术MER方法的可靠性和灵活性。<details>
<summary>Abstract</summary>
This paper focuses on the research of micro-expression recognition (MER) and proposes a flexible and reliable deep learning method called learning to rank onset-occurring-offset representations (LTR3O). The LTR3O method introduces a dynamic and reduced-size sequence structure known as 3O, which consists of onset, occurring, and offset frames, for representing micro-expressions (MEs). This structure facilitates the subsequent learning of ME-discriminative features. A noteworthy advantage of the 3O structure is its flexibility, as the occurring frame is randomly extracted from the original ME sequence without the need for accurate frame spotting methods. Based on the 3O structures, LTR3O generates multiple 3O representation candidates for each ME sample and incorporates well-designed modules to measure and calibrate their emotional expressiveness. This calibration process ensures that the distribution of these candidates aligns with that of macro-expressions (MaMs) over time. Consequently, the visibility of MEs can be implicitly enhanced, facilitating the reliable learning of more discriminative features for MER. Extensive experiments were conducted to evaluate the performance of LTR3O using three widely-used ME databases: CASME II, SMIC, and SAMM. The experimental results demonstrate the effectiveness and superior performance of LTR3O, particularly in terms of its flexibility and reliability, when compared to recent state-of-the-art MER methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="VLAttack-Multimodal-Adversarial-Attacks-on-Vision-Language-Tasks-via-Pre-trained-Models"><a href="#VLAttack-Multimodal-Adversarial-Attacks-on-Vision-Language-Tasks-via-Pre-trained-Models" class="headerlink" title="VLAttack: Multimodal Adversarial Attacks on Vision-Language Tasks via Pre-trained Models"></a>VLAttack: Multimodal Adversarial Attacks on Vision-Language Tasks via Pre-trained Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04655">http://arxiv.org/abs/2310.04655</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ericyinyzy/VLAttack">https://github.com/ericyinyzy/VLAttack</a></li>
<li>paper_authors: Ziyi Yin, Muchao Ye, Tianrong Zhang, Tianyu Du, Jinguo Zhu, Han Liu, Jinghui Chen, Ting Wang, Fenglong Ma</li>
<li>for: 这篇论文旨在探讨预训练的语音视觉（VL）模型在多modal任务上的敌方性 robustness。</li>
<li>methods: 该论文提出了一种新的实际任务，通过预训练VL模型生成图像和文本杂乱样本，用于攻击黑盒子细化模型。其中，图像级别使用一种新的块级相似攻击策略（BSA）学习图像杂乱，文本级别采用现有的文本攻击策略独立于图像模式攻击。多modal级别则提出了一种迭代交叉搜索攻击（ICSA）方法，在图像和文本两个纬度上不断更新敌意样本。</li>
<li>results: 该论文通过对三种广泛使用的VL预训练模型进行六种任务的攻击，实现了所有任务的攻击成功率最高，而与现有基线相比，表明预训练VL模型的部署存在重要的缺点。<details>
<summary>Abstract</summary>
Vision-Language (VL) pre-trained models have shown their superiority on many multimodal tasks. However, the adversarial robustness of such models has not been fully explored. Existing approaches mainly focus on exploring the adversarial robustness under the white-box setting, which is unrealistic. In this paper, we aim to investigate a new yet practical task to craft image and text perturbations using pre-trained VL models to attack black-box fine-tuned models on different downstream tasks. Towards this end, we propose VLAttack to generate adversarial samples by fusing perturbations of images and texts from both single-modal and multimodal levels. At the single-modal level, we propose a new block-wise similarity attack (BSA) strategy to learn image perturbations for disrupting universal representations. Besides, we adopt an existing text attack strategy to generate text perturbations independent of the image-modal attack. At the multimodal level, we design a novel iterative cross-search attack (ICSA) method to update adversarial image-text pairs periodically, starting with the outputs from the single-modal level. We conduct extensive experiments to attack three widely-used VL pretrained models for six tasks on eight datasets. Experimental results show that the proposed VLAttack framework achieves the highest attack success rates on all tasks compared with state-of-the-art baselines, which reveals a significant blind spot in the deployment of pre-trained VL models. Codes will be released soon.
</details>
<details>
<summary>摘要</summary>
视觉语言（VL）预训模型已经在许多多modal任务上表现出色，但是它们的抗击机制尚未得到全面的探索。现有的方法主要集中在白盒 Setting下进行抗击，这是不现实的。在这篇论文中，我们目的是调查一种新的实用任务，用预训 VL 模型来针对黑盒精度调整模型在不同下游任务上的攻击。为此，我们提出了 VLAttack 攻击框架，用于生成攻击样本。在单modal水平上，我们提出了一种新的块级相似攻击（BSA）策略，用于学习图像攻击，以破坏通用表示。此外，我们采用了现有的文本攻击策略，独立于图像Modal攻击，生成文本攻击。在多modal水平上，我们设计了一种新的迭代交叉搜索攻击（ICSA）方法，用于更新 periodic 的攻击图像文本对，从单modal水平的输出开始。我们对三个广泛使用的 VL 预训模型进行了六个任务的攻击，并对八个数据集进行了广泛的实验。实验结果表明，我们的 VLAttack 框架在所有任务上取得了最高的攻击成功率，超过了现有的基线，这表明预训 VL 模型在部署中存在一定的盲点。代码即将发布。
</details></li>
</ul>
<hr>
<h2 id="X-Transfer-A-Transfer-Learning-Based-Framework-for-Robust-GAN-Generated-Fake-Image-Detection"><a href="#X-Transfer-A-Transfer-Learning-Based-Framework-for-Robust-GAN-Generated-Fake-Image-Detection" class="headerlink" title="X-Transfer: A Transfer Learning-Based Framework for Robust GAN-Generated Fake Image Detection"></a>X-Transfer: A Transfer Learning-Based Framework for Robust GAN-Generated Fake Image Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04639">http://arxiv.org/abs/2310.04639</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Zhang, Hao Chen, Shu Hu, Bin Zhu, Xi Wu, Jinrong Hu, Xin Wang</li>
<li>for: 这个研究旨在提出一个新的生成随机网络侦测算法，以解决在各种领域中的伪造图像问题。</li>
<li>methods: 本研究使用了两个姐妹神经网络，通过平行的梯度传递来强化传播学习。此外，我们还结合了AUC损失函数和梯度估计损失函数，以提高模型的性能。</li>
<li>results: 我们在多个脸部图像dataset上进行了广泛的实验，结果显示我们的模型在传播学习方法上优于常规的传播方法，并且在非脸部dataset上也获得了出色的表现，这证明了我们的模型具有更广泛的应用前景。<details>
<summary>Abstract</summary>
Generative adversarial networks (GANs) have remarkably advanced in diverse domains, especially image generation and editing. However, the misuse of GANs for generating deceptive images raises significant security concerns, including face replacement and fake accounts, which have gained widespread attention. Consequently, there is an urgent need for effective detection methods to distinguish between real and fake images. Some of the current research centers around the application of transfer learning. Nevertheless, it encounters challenges such as knowledge forgetting from the original dataset and inadequate performance when dealing with imbalanced data during training. To alleviate the above issues, this paper introduces a novel GAN-generated image detection algorithm called X-Transfer. This model enhances transfer learning by utilizing two sibling neural networks that employ interleaved parallel gradient transmission. This approach also effectively mitigates the problem of excessive knowledge forgetting. In addition, we combine AUC loss term and cross-entropy loss to enhance the model's performance comprehensively. The AUC loss approximates the AUC metric using WMW statistics, ensuring differentiability and improving the performance of traditional AUC evaluation. We carry out comprehensive experiments on multiple facial image datasets. The results show that our model outperforms the general transferring approach, and the best accuracy achieves 99.04%, which is increased by approximately 10%. Furthermore, we demonstrate excellent performance on non-face datasets, validating its generality and broader application prospects.
</details>
<details>
<summary>摘要</summary>
生成对抗网络（GAN）在多个领域取得了突出的进步，尤其是图像生成和修改。然而，GAN的滥用可能导致生成假图像的安全问题，如人脸替换和假账户，这些问题已经吸引了广泛的关注。因此，有一项急需有效的检测方法来 отличить真实图像和假图像。一些当前的研究集中在应用传输学习。然而，它遇到了一些挑战，如原始数据集中的知识忘记和训练过程中的数据不均衡。为了解决以上问题，本文提出了一种新的GAN生成图像检测算法called X-Transfer。这种模型在使用两个兄弟神经网络之间的平行梯度传输方法来增强传输学习。此外，我们还将AUC损失函数和交叉熵损失函数结合使用，以提高模型的性能。我们对多个人脸图像集进行了广泛的实验，结果显示，我们的模型在普通的传输方法上出色，并且最高准确率达到99.04%，提高了约10%。此外，我们还证明了我们的模型在非人脸图像集上表现出色，证明其通用性和更广泛的应用前景。
</details></li>
</ul>
<hr>
<h2 id="Metadata-Conditioned-Generative-Models-to-Synthesize-Anatomically-Plausible-3D-Brain-MRIs"><a href="#Metadata-Conditioned-Generative-Models-to-Synthesize-Anatomically-Plausible-3D-Brain-MRIs" class="headerlink" title="Metadata-Conditioned Generative Models to Synthesize Anatomically-Plausible 3D Brain MRIs"></a>Metadata-Conditioned Generative Models to Synthesize Anatomically-Plausible 3D Brain MRIs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04630">http://arxiv.org/abs/2310.04630</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Peng, Tomas Bosschieter, Jiahong Ouyang, Robert Paul, Ehsan Adeli, Qingyu Zhao, Kilian M. Pohl</li>
<li>for: 这个论文旨在提高神经成像研究的数据多样性，使用生成AI模型生成年龄和性别相关的Synthetic MRI数据，以便更好地理解大脑结构和功能的变化。</li>
<li>methods: 该论文提出了一种新的生成模型，叫做BrainSynth，可以生成metadata-conditioned的Synthetic MRI数据，并且可以评估这些数据的可读性和生物学可信度。</li>
<li>results: 研究发现，使用BrainSynth生成的Synthetic MRI数据中的大脑区域具有较高的生物学可信度，并且可以帮助提高神经成像研究中的训练效果。<details>
<summary>Abstract</summary>
Generative AI models hold great potential in creating synthetic brain MRIs that advance neuroimaging studies by, for example, enriching data diversity. However, the mainstay of AI research only focuses on optimizing the visual quality (such as signal-to-noise ratio) of the synthetic MRIs while lacking insights into their relevance to neuroscience. To gain these insights with respect to T1-weighted MRIs, we first propose a new generative model, BrainSynth, to synthesize metadata-conditioned (e.g., age- and sex-specific) MRIs that achieve state-of-the-art visual quality. We then extend our evaluation with a novel procedure to quantify anatomical plausibility, i.e., how well the synthetic MRIs capture macrostructural properties of brain regions, and how accurately they encode the effects of age and sex. Results indicate that more than half of the brain regions in our synthetic MRIs are anatomically accurate, i.e., with a small effect size between real and synthetic MRIs. Moreover, the anatomical plausibility varies across cortical regions according to their geometric complexity. As is, our synthetic MRIs can significantly improve the training of a Convolutional Neural Network to identify accelerated aging effects in an independent study. These results highlight the opportunities of using generative AI to aid neuroimaging research and point to areas for further improvement.
</details>
<details>
<summary>摘要</summary>
优化生成AI模型可以创造出高质量的人工大脑MRI图像，从而提高神经成像研究的数据多样性。然而，主流的AI研究仅专注于提高生成图像的视觉质量（如信号噪声比）而忽略了它们对神经科学的意义。为了获得这些意义，我们首先提出了一种新的生成模型——BrainSynth，用于生成基于metadata（如年龄和性别）的 conditioned MRI图像，并达到了当前最高的视觉质量。然后，我们延展我们的评估方法，包括一种新的评估方法来衡量生成图像的吸收性，即评估生成图像是否能够准确捕捉大脑区域的宏结构特征，以及是否能够正确表达年龄和性别的效果。结果显示，我们的生成图像中的大脑区域有超过半数是吸收正确的，即与实际MRI图像之间的效果效果较小。此外，吸收正确性随 cortical 区域的几何复杂性变化。总之，我们的生成图像可以大幅提高一种Convolutional Neural Network的训练，以识别加速年龄效应。这些结果表明使用生成AI可以帮助神经成像研究，并指出了进一步改进的方向。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/07/cs.CV_2023_10_07/" data-id="clp869ty300kxk58872hc54ml" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_10_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/07/cs.AI_2023_10_07/" class="article-date">
  <time datetime="2023-10-07T12:00:00.000Z" itemprop="datePublished">2023-10-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/07/cs.AI_2023_10_07/">cs.AI - 2023-10-07</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Balancing-Specialized-and-General-Skills-in-LLMs-The-Impact-of-Modern-Tuning-and-Data-Strategy"><a href="#Balancing-Specialized-and-General-Skills-in-LLMs-The-Impact-of-Modern-Tuning-and-Data-Strategy" class="headerlink" title="Balancing Specialized and General Skills in LLMs: The Impact of Modern Tuning and Data Strategy"></a>Balancing Specialized and General Skills in LLMs: The Impact of Modern Tuning and Data Strategy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04945">http://arxiv.org/abs/2310.04945</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheng Zhang, Chen Zheng, Da Tang, Ke Sun, Yukun Ma, Yingtong Bu, Xun Zhou, Liang Zhao</li>
<li>for: 这 paper 的目的是为大型语言模型（LLMs）特циализирован的商业化任务进行精细调整和评估。</li>
<li>methods: 这 paper 使用的方法包括：1) 精心混合域外和通用数据进行 fine-tuning，以实现一个优化的权衡 между通用和专业技能; 2) 设计了一个全面的评估框架，包括45个问题，以评估模型在 fonctionally 相关的维度上的表现，如可靠性、一致性和业务影响; 3) 分析模型大小和连续训练对纪录的影响，以导引有效的资源分配。</li>
<li>results: 这 paper 的结果表明，通过采用这些方法，可以很好地权衡通用语言能力和专业技能，并且可以提供有用的指导方针 для商业和研究人员在特циализирован任务上调整 LLMs。<details>
<summary>Abstract</summary>
This paper introduces a multifaceted methodology for fine-tuning and evaluating large language models (LLMs) for specialized monetization tasks. The goal is to balance general language proficiency with domain-specific skills. The methodology has three main components: 1) Carefully blending in-domain and general-purpose data during fine-tuning to achieve an optimal balance between general and specialized capabilities; 2) Designing a comprehensive evaluation framework with 45 questions tailored to assess performance on functionally relevant dimensions like reliability, consistency, and business impact; 3) Analyzing how model size and continual training influence metrics to guide efficient resource allocation during fine-tuning. The paper details the design, data collection, analytical techniques, and results validating the proposed frameworks. It aims to provide businesses and researchers with actionable insights on effectively adapting LLMs for specialized contexts. We also intend to make public the comprehensive evaluation framework, which includes the 45 tailored questions and their respective scoring guidelines, to foster transparency and collaboration in adapting LLMs for specialized tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Blending in-domain and general-purpose data during fine-tuning to achieve an optimal balance between general and specialized capabilities.2. Developing a comprehensive evaluation framework with 45 questions tailored to assess performance on functionally relevant dimensions such as reliability, consistency, and business impact.3. Analyzing how model size and continual training influence metrics to guide efficient resource allocation during fine-tuning.The paper details the design, data collection, analytical techniques, and results validating the proposed frameworks, providing actionable insights for businesses and researchers on effectively adapting LLMs for specialized contexts. Additionally, the comprehensive evaluation framework, including the 45 tailored questions and their respective scoring guidelines, will be made public to foster transparency and collaboration in adapting LLMs for specialized tasks.</details></li>
</ol>
<hr>
<h2 id="Reliable-Test-Time-Adaptation-via-Agreement-on-the-Line"><a href="#Reliable-Test-Time-Adaptation-via-Agreement-on-the-Line" class="headerlink" title="Reliable Test-Time Adaptation via Agreement-on-the-Line"></a>Reliable Test-Time Adaptation via Agreement-on-the-Line</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04941">http://arxiv.org/abs/2310.04941</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eungyeup Kim, Mingjie Sun, Aditi Raghunathan, Zico Kolter</li>
<li>for: 本文主要针对测试时适应（TTA）方法的可靠性和可重复性问题进行研究，尤其是在不同的分布转移下进行适应后模型的评估和调参问题。</li>
<li>methods: 本文使用了多种TTA方法，包括随机抽象、权重学习和卷积神经网络等方法，并进行了广泛的实验研究以评估这些方法的可靠性和可重复性。</li>
<li>results: 本文的研究发现，TTAed模型具有强的同步特征（agreement-on-the-line phenomenon），即在不同的分布转移下，TTAed模型的预测结果呈现出一定的线性关系。基于这个发现，本文提出了一些解决TTA方法的可靠性和可重复性问题的方法，包括无 labels 数据上的OOD性能估计、无标签信息的模型 calibration 和无标签验证数据的hyperparameter 调参等方法。通过广泛的实验研究，本文证明了这些方法可以准确地评估TTA方法的性能和可靠性，并且可以在不同的分布转移下实现模型的适应和calibration。<details>
<summary>Abstract</summary>
Test-time adaptation (TTA) methods aim to improve robustness to distribution shifts by adapting models using unlabeled data from the shifted test distribution. However, there remain unresolved challenges that undermine the reliability of TTA, which include difficulties in evaluating TTA performance, miscalibration after TTA, and unreliable hyperparameter tuning for adaptation. In this work, we make a notable and surprising observation that TTAed models strongly show the agreement-on-the-line phenomenon (Baek et al., 2022) across a wide range of distribution shifts. We find such linear trends occur consistently in a wide range of models adapted with various hyperparameters, and persist in distributions where the phenomenon fails to hold in vanilla models (i.e., before adaptation). We leverage these observations to make TTA methods more reliable in three perspectives: (i) estimating OOD accuracy (without labeled data) to determine when TTA helps and when it hurts, (ii) calibrating TTAed models without label information, and (iii) reliably determining hyperparameters for TTA without any labeled validation data. Through extensive experiments, we demonstrate that various TTA methods can be precisely evaluated, both in terms of their improvements and degradations. Moreover, our proposed methods on unsupervised calibration and hyperparameters tuning for TTA achieve results close to the ones assuming access to ground-truth labels, in terms of both OOD accuracy and calibration error.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>无标签数据来评估 OOD 精度，以确定 TTA 是否有助于或妨碍。2. 使用无标签数据来准确调整 TTAed 模型的误差。3. 使用无标签数据来可靠地确定 TTA 过程中的 гипер参数。我们通过广泛的实验，证明了不同的 TTA 方法可以准确地评估，并且我们的提议的方法可以在无标签数据下达到与 assuming 访问真实标签数据的结果相似的水平，以 both OOD 精度和误差来衡量。</details></li>
</ol>
<hr>
<h2 id="Diff-Transfer-Model-based-Robotic-Manipulation-Skill-Transfer-via-Differentiable-Physics-Simulation"><a href="#Diff-Transfer-Model-based-Robotic-Manipulation-Skill-Transfer-via-Differentiable-Physics-Simulation" class="headerlink" title="Diff-Transfer: Model-based Robotic Manipulation Skill Transfer via Differentiable Physics Simulation"></a>Diff-Transfer: Model-based Robotic Manipulation Skill Transfer via Differentiable Physics Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04930">http://arxiv.org/abs/2310.04930</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuqi Xiang, Feitong Chen, Qinsi Wang, Yang Gang, Xiang Zhang, Xinghao Zhu, Xingyu Liu, Lin Shao</li>
<li>For: 本文旨在提出一种基于差分 физи学仿真的精准转移框架，以便让智能机器人在完成类似 yet novel 任务时，能够快速转移已经熟悉的技能。* Methods: 本文提出了一种新的转移框架，称为 $\textit{Diff-Transfer}$，它利用差分 физи学仿真来效率地转移机器人技能。具体来说， $\textit{Diff-Transfer}$ 在任务空间中找到可行的任务路径，并在每对邻近点上适应已知的动作，以解决另一个子任务。这种适应是通过差分 физи学仿真的梯度信息引导的。* Results: 作者在实验中使用 $\textit{Diff-Transfer}$ 执行了四个复杂的转移任务，包括机器人抓取和移动等，并通过了全面的实验。详细的实验结果和视频可以参考<a target="_blank" rel="noopener" href="https://sites.google.com/view/difftransfer%E3%80%82">https://sites.google.com/view/difftransfer。</a><details>
<summary>Abstract</summary>
The capability to transfer mastered skills to accomplish a range of similar yet novel tasks is crucial for intelligent robots. In this work, we introduce $\textit{Diff-Transfer}$, a novel framework leveraging differentiable physics simulation to efficiently transfer robotic skills. Specifically, $\textit{Diff-Transfer}$ discovers a feasible path within the task space that brings the source task to the target task. At each pair of adjacent points along this task path, which is two sub-tasks, $\textit{Diff-Transfer}$ adapts known actions from one sub-task to tackle the other sub-task successfully. The adaptation is guided by the gradient information from differentiable physics simulations. We propose a novel path-planning method to generate sub-tasks, leveraging $Q$-learning with a task-level state and reward. We implement our framework in simulation experiments and execute four challenging transfer tasks on robotic manipulation, demonstrating the efficacy of $\textit{Diff-Transfer}$ through comprehensive experiments. Supplementary and Videos are on the website https://sites.google.com/view/difftransfer
</details>
<details>
<summary>摘要</summary>
“智能机器人需要具备将已经学习的技能应用于类似 yet novel 任务的能力。在这个工作中，我们介绍了 $\textit{Diff-Transfer}$ 框架，利用可微的物理学习来有效地传递 робо机能力。具体来说，$\textit{Diff-Transfer}$ 发现可以在任务空间中找到可行的任务路径，将源任务转化为目标任务。在每对相邻的任务点上，$\textit{Diff-Transfer}$ 采用已知的动作改进一个子任务，以解决另一个子任务。改进是通过可微的物理学习的梯度信息引导的。我们提出了一种新的路径规划方法，基于 $Q$-学习和任务级状态和奖励。我们在 simulator 中实现了我们的框架，并在机器人 manipulate 等四个具有挑战性的转移任务中进行了实验，证明了 $\textit{Diff-Transfer}$ 的有效性。补充和视频可以在网站 https://sites.google.com/view/difftransfer 上找到。”Note that Simplified Chinese is used here, which is the most widely used variety of Chinese. If you prefer Traditional Chinese, I can provide that version as well.
</details></li>
</ul>
<hr>
<h2 id="Crystal-Introspective-Reasoners-Reinforced-with-Self-Feedback"><a href="#Crystal-Introspective-Reasoners-Reinforced-with-Self-Feedback" class="headerlink" title="Crystal: Introspective Reasoners Reinforced with Self-Feedback"></a>Crystal: Introspective Reasoners Reinforced with Self-Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04921">http://arxiv.org/abs/2310.04921</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liujch1998/crystal">https://github.com/liujch1998/crystal</a></li>
<li>paper_authors: Jiacheng Liu, Ramakanth Pasunuru, Hannaneh Hajishirzi, Yejin Choi, Asli Celikyilmaz</li>
<li>For: The paper aims to improve the performance and interpretability of commonsense reasoning using knowledge-augmented reasoning methods.* Methods: The proposed method, called Crystal, introspects for knowledge statements related to a given question and makes an informed prediction grounded in the previously introspected knowledge. The knowledge introspection and knowledge-grounded reasoning modes of the model are tuned via reinforcement learning to mutually adapt.* Results: Crystal significantly outperforms both the standard supervised finetuning and chain-of-thought distilled methods, and enhances the transparency of the commonsense reasoning process.<details>
<summary>Abstract</summary>
Extensive work has shown that the performance and interpretability of commonsense reasoning can be improved via knowledge-augmented reasoning methods, where the knowledge that underpins the reasoning process is explicitly verbalized and utilized. However, existing implementations, including "chain-of-thought" and its variants, fall short in capturing the introspective nature of knowledge required in commonsense reasoning, and in accounting for the mutual adaptation between the generation and utilization of knowledge. We propose a novel method to develop an introspective commonsense reasoner, Crystal. To tackle commonsense problems, it first introspects for knowledge statements related to the given question, and subsequently makes an informed prediction that is grounded in the previously introspected knowledge. The knowledge introspection and knowledge-grounded reasoning modes of the model are tuned via reinforcement learning to mutually adapt, where the reward derives from the feedback given by the model itself. Experiments show that Crystal significantly outperforms both the standard supervised finetuning and chain-of-thought distilled methods, and enhances the transparency of the commonsense reasoning process. Our work ultimately validates the feasibility and potential of reinforcing a neural model with self-feedback.
</details>
<details>
<summary>摘要</summary>
广泛的研究表明，可以通过增强知识推理的方法来提高常识推理的性能和可读性，其中包括使用知识推理方法，其中的知识是明确地表达出来并利用。但是现有的实现方法，如“链子”和其变体，对于常识推理中的 introspective 知识需求和知识生成和使用之间的互动，缺乏捕捉。我们提出了一种新的方法，即 Crystal，以解决常识问题。它首先 introspects 相关的知识含义，然后根据这些知识做出了 Informed 的预测，这些预测是基于之前 introspected 的知识。知识 introspection 和知识推理模式在模型中被调整通过反射学习，以便相互适应。实验结果显示，Crystal 与标准的监督调整和链子混合方法相比，有着很高的表现。我们的工作终于验证了将 neural 模型与自己的反馈整合的可能性。>>>
</details></li>
</ul>
<hr>
<h2 id="Robust-Network-Pruning-With-Sparse-Entropic-Wasserstein-Regression"><a href="#Robust-Network-Pruning-With-Sparse-Entropic-Wasserstein-Regression" class="headerlink" title="Robust Network Pruning With Sparse Entropic Wasserstein Regression"></a>Robust Network Pruning With Sparse Entropic Wasserstein Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04918">http://arxiv.org/abs/2310.04918</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei You, Hei Victor Cheng</li>
<li>for: 本研究推出了一种针对噪声梯度的高效神经网络剔除技术，通过计算empirical Fisher Information Matrix (FIM)来实现。</li>
<li>methods: 我们引入了一种Entropic Wasserstein regression (EWR)方法，利用最优运输 (OT) 问题的几何特点，可以有效地减少噪声。</li>
<li>results: 我们的方法在不同的网络模型上进行了广泛的实验，与当前最佳方法（SoTA）的网络剔除算法相比，我们的方法在网络大小或目标稀疏性较大时表现更佳，尤其是在噪声存在的情况下，增加约6%的准确率和8%的测试损失。<details>
<summary>Abstract</summary>
This study unveils a cutting-edge technique for neural network pruning that judiciously addresses noisy gradients during the computation of the empirical Fisher Information Matrix (FIM). We introduce an entropic Wasserstein regression (EWR) formulation, capitalizing on the geometric attributes of the optimal transport (OT) problem. This is analytically showcased to excel in noise mitigation by adopting neighborhood interpolation across data points. The unique strength of the Wasserstein distance is its intrinsic ability to strike a balance between noise reduction and covariance information preservation. Extensive experiments performed on various networks show comparable performance of the proposed method with state-of-the-art (SoTA) network pruning algorithms. Our proposed method outperforms the SoTA when the network size or the target sparsity is large, the gain is even larger with the existence of noisy gradients, possibly from noisy data, analog memory, or adversarial attacks. Notably, our proposed method achieves a gain of 6% improvement in accuracy and 8% improvement in testing loss for MobileNetV1 with less than one-fourth of the network parameters remaining.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="On-Accelerating-Diffusion-based-Molecular-Conformation-Generation-in-SE-3-invariant-Space"><a href="#On-Accelerating-Diffusion-based-Molecular-Conformation-Generation-in-SE-3-invariant-Space" class="headerlink" title="On Accelerating Diffusion-based Molecular Conformation Generation in SE(3)-invariant Space"></a>On Accelerating Diffusion-based Molecular Conformation Generation in SE(3)-invariant Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04915">http://arxiv.org/abs/2310.04915</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihan Zhou, Ruiying Liu, Tianshu Yu</li>
<li>for: 本研究旨在加速在SE(3)-不变空间中的分析生成模型，以提高其在实际应用中的效率。</li>
<li>methods: 本研究使用了对现有方法的约错分析，以了解SE(3)-不变空间中的扩散机制。基于这种理解，我们开发了一种新的加速方案，以提高分析生成模型的运行速度。</li>
<li>results: 实验表明，我们的加速方案可以在SE(3)-不变空间中生成高质量的分析结果，与现有方法相比，具有50倍至100倍的速度提升。<details>
<summary>Abstract</summary>
Diffusion-based generative models in SE(3)-invariant space have demonstrated promising performance in molecular conformation generation, but typically require solving stochastic differential equations (SDEs) with thousands of update steps. Till now, it remains unclear how to effectively accelerate this procedure explicitly in SE(3)-invariant space, which greatly hinders its wide application in the real world. In this paper, we systematically study the diffusion mechanism in SE(3)-invariant space via the lens of approximate errors induced by existing methods. Thereby, we develop more precise approximate in SE(3) in the context of projected differential equations. Theoretical analysis is further provided as well as empirical proof relating hyper-parameters with such errors. Altogether, we propose a novel acceleration scheme for generating molecular conformations in SE(3)-invariant space. Experimentally, our scheme can generate high-quality conformations with 50x--100x speedup compared to existing methods.
</details>
<details>
<summary>摘要</summary>
Diffusion-based生成模型在SE(3)-不变空间中表现出了优秀的表现，但通常需要解决数千步隐藏微分方程（SDEs）。直到现在，未知如何明确地加速这个过程，这大大阻碍了它在实际世界中的广泛应用。在这篇论文中，我们系统地研究了SE(3)-不变空间中的扩散机制，通过对现有方法的误差引入的精度来评估。此外，我们还提供了相关的理论分析和实验证明，关于参数与这些误差之间的关系。总之，我们提出了一种新的加速方案，可以在SE(3)-不变空间中高速生成分子结构。实验表明，我们的方案可以比现有方法快速50-100倍，生成高质量的分子结构。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-Zero-Shot-Abilities-of-Vision-Language-Models-on-Video-Understanding-Tasks"><a href="#Analyzing-Zero-Shot-Abilities-of-Vision-Language-Models-on-Video-Understanding-Tasks" class="headerlink" title="Analyzing Zero-Shot Abilities of Vision-Language Models on Video Understanding Tasks"></a>Analyzing Zero-Shot Abilities of Vision-Language Models on Video Understanding Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04914">http://arxiv.org/abs/2310.04914</a></li>
<li>repo_url: None</li>
<li>paper_authors: Avinash Madasu, Anahita Bhiwandiwalla, Vasudev Lal</li>
<li>for: 这些论文的目的是研究基础的多模态模型是否可以适应视频任务，以及这种方法的效果。</li>
<li>methods: 这些论文使用的方法是将图文模型适应视频任务，并对这种方法的性能进行评估。</li>
<li>results: 研究发现，基础的图文模型在视频理解任务上表现出色，特别是在视频识别、视频检索和视频多选任务上。然而，它们在视频问答和视频描述任务上表现较差。这些发现反映了将图文模型适应视频任务的效果。<details>
<summary>Abstract</summary>
Foundational multimodal models pre-trained on large scale image-text pairs or video-text pairs or both have shown strong generalization abilities on downstream tasks. However unlike image-text models, pretraining video-text models is always not feasible due to the difficulty in collecting large-scale clean and aligned data, and exponential computational costs involved in the pretraining phase. Therefore, the pertinent question to ask is: Can image-text models be adapted to video tasks and is there any benefit to using these models over pretraining directly on videos? In this work, we focus on this question by proposing a detailed study on the generalization abilities of image-text models when evaluated on video understanding tasks in a zero-shot setting. We investigate 9 foundational image-text models on a diverse set of video tasks that include video action recognition (video AR), video retrieval (video RT), video question answering (video QA), video multiple choice (video MC) and video captioning (video CP). Our experiments show that image-text models exhibit impressive performance on video AR, video RT and video MC. Furthermore, they perform moderately on video captioning and poorly on video QA. These findings shed a light on the benefits of adapting foundational image-text models to an array of video tasks while avoiding the costly pretraining step.
</details>
<details>
<summary>摘要</summary>
基础多Modal模型在大规模图片文本对或视频文本对上预训练后显示出强大的通用能力。然而，与图片文本模型不同，不可能在预训练视频文本模型，因为收集大规模干净对齐数据的困难，以及预训练阶段的计算成本呈指数增长。因此，关键的问题是：图片文本模型能否适应视频任务，是否有任何优势使用这些模型而不是直接预训练在视频上？在这项工作中，我们关注这个问题，通过对Foundational image-text模型在视频理解任务上的总体能力进行详细的研究。我们对9种基础图片文本模型在多种视频任务上进行了多样化的测试，包括视频动作识别（视频AR）、视频检索（视频RT）、视频问答（视频QA）、视频多选（视频MC）和视频描述（视频CP）。我们的实验结果表明，图片文本模型在视频AR、视频RT和视频MC方面表现出色，而在视频描述和视频QA方面表现较差。这些发现反映了适应多种视频任务的图片文本模型，而不需要费时的预训练步骤。
</details></li>
</ul>
<hr>
<h2 id="Faithful-Knowledge-Graph-Explanations-for-Commonsense-Reasoning"><a href="#Faithful-Knowledge-Graph-Explanations-for-Commonsense-Reasoning" class="headerlink" title="Faithful Knowledge Graph Explanations for Commonsense Reasoning"></a>Faithful Knowledge Graph Explanations for Commonsense Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04910">http://arxiv.org/abs/2310.04910</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weihe Zhai, Arkaitz Zubiaga, Bingquan Liu</li>
<li>for: 本研究旨在提高知识图（KG）基于解释的准确性和可靠性。</li>
<li>methods: 本研究提出了两项主要贡献：首先，我们提出了两种量化指标——图共识度和图准确度——来衡量知识图基于解释的准确性。其次，我们引入了一种新的培训方法，即具有一定的一致性规范的Consistent GNN（CGNN），以提高解释的准确性。</li>
<li>results: 我们的分析表明，使用原始模型预测的方法可能会导致知识图中的预测结果与原始模型预测结果不同。而我们提出的CGNN方法能够提高图共识度和图准确度，这表明了它在生成更准确的解释方面的潜力。<details>
<summary>Abstract</summary>
While fusing language models (LMs) and knowledge graphs (KGs) has become common in commonsense question answering research, enabling faithful chain-of-thought explanations in these models remains an open problem. One major weakness of current KG-based explanation techniques is that they overlook the faithfulness of generated explanations during evaluation. To address this gap, we make two main contributions: (1) We propose and validate two quantitative metrics - graph consistency and graph fidelity - to measure the faithfulness of KG-based explanations. (2) We introduce Consistent GNN (CGNN), a novel training method that adds a consistency regularization term to improve explanation faithfulness. Our analysis shows that predictions from KG often diverge from original model predictions. The proposed CGNN approach boosts consistency and fidelity, demonstrating its potential for producing more faithful explanations. Our work emphasises the importance of explicitly evaluating suggest a path forward for developing architectures for faithful graph-based explanations.
</details>
<details>
<summary>摘要</summary>
当拓展语言模型（LM）和知识图（KG）的研究成为常见的现象，使得 faithful chain-of-thought 解释在这些模型中保持开放问题。现有的 KG 解释技术的一个主要弱点是忽略生成的解释的忠实程度 durante la evaluación。为了解决这个漏洞，我们作出了两个主要贡献：1. 我们提出了两个量化指标 - 图共识性和图准确性 - 来衡量 KG 解释的忠实度。2. 我们介绍了一种新的训练方法，称为 Consistent GNN（CGNN），该方法添加了一个准确性规则来提高解释的忠实度。我们的分析表明，KG 的预测结果与原始模型的预测结果经常存在差异。CGNN 方法可以提高准确性和忠实度，这表明其在生成更 faithful 的解释方面具有潜在的优势。我们的工作强调了评估 KG 解释的忠实度的重要性，并建议一种发展 faithful graph-based 解释体系的可能之路。
</details></li>
</ul>
<hr>
<h2 id="Generative-AI-May-Prefer-to-Present-National-level-Characteristics-of-Cities-Based-on-Stereotypical-Geographic-Impressions-at-the-Continental-Level"><a href="#Generative-AI-May-Prefer-to-Present-National-level-Characteristics-of-Cities-Based-on-Stereotypical-Geographic-Impressions-at-the-Continental-Level" class="headerlink" title="Generative AI May Prefer to Present National-level Characteristics of Cities Based on Stereotypical Geographic Impressions at the Continental Level"></a>Generative AI May Prefer to Present National-level Characteristics of Cities Based on Stereotypical Geographic Impressions at the Continental Level</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04897">http://arxiv.org/abs/2310.04897</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shan Ye</li>
<li>for: 测试中文基于生成人工智能平台“文心一个”的图像渲染能力，以及该平台是否会描绘出不同国家城市景观的多样性。</li>
<li>methods: 通过使用“文心一个”平台生成不同国家城市街景图像，然后对这些图像进行分析和评估，以了解该平台的图像渲染能力和可能存在的偏见。</li>
<li>results: 研究发现，“文心一个”平台生成的图像可能带有大洲水平的偏见，表现出不同国家城市的经济发展水平和现代化程度。此外，这些生成图像不能充分表达不同国家城市的多样性。使用这些图像进行地理教育或宣传活动可能会巩固人们对各国的偏见。<details>
<summary>Abstract</summary>
A simple experiment was conducted to test the ability of the Chinese-based generative artificial intelligence (AI) platform, Wenxin Yige, to render images of urban street views of different countries. The study found that images generated by this AI platform may contain continental-level stereotypes in terms of showing the level of economic development and modernization. Street view images generated from Wenxin Yige do not adequately represent the diverse range of urban landscapes found across different nations. Using these generated images for geography education or outreach initiatives could inadvertently strengthen people's existing stereotypical views about individual countries.
</details>
<details>
<summary>摘要</summary>
一项简单的实验测试了基于中文的生成式人工智能平台“文心易歌”的图像生成能力，以测试它是否能够生成不同国家城市视图的图像。研究发现，由这个AI平台生成的图像可能带有大洲水平的刻板印象，表现出不同国家的经济发展和现代化水平。这些生成的图像无法准确表达不同国家城市的多样化风貌，使用这些图像进行地理教育或宣传活动可能会巩固人们对各国的刻板印象。
</details></li>
</ul>
<hr>
<h2 id="Cell-Tracking-by-detection-using-Elliptical-Bounding-Boxes"><a href="#Cell-Tracking-by-detection-using-Elliptical-Bounding-Boxes" class="headerlink" title="Cell Tracking-by-detection using Elliptical Bounding Boxes"></a>Cell Tracking-by-detection using Elliptical Bounding Boxes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04895">http://arxiv.org/abs/2310.04895</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/LucasKirsten/Deep-Cell-Tracking-EBB">https://github.com/LucasKirsten/Deep-Cell-Tracking-EBB</a></li>
<li>paper_authors: Lucas N. Kirsten, Cláudio R. Jung</li>
<li>for: 这 paper 的目的是提出一种基于经典 tracking-by-detection 模式的细胞检测和跟踪方法，以避免大量的标注数据。</li>
<li>methods: 该方法使用 oriented ellipses 来 aproximate 细胞形状，然后使用通用 oriented object detectors 来在每帧中标识细胞。 global data association 算法使用 probability distance metrics 来explore temporal cell similarity。</li>
<li>results: 该方法可以 achieves detection and tracking results competitively with state-of-the-art techniques that require considerably more extensive data annotation。Here is the summary in English for reference:</li>
<li>for: The purpose of this paper is to propose a new approach based on the classical tracking-by-detection paradigm for cell detection and tracking, which alleviates the need for extensive annotated data.</li>
<li>methods: The method approximates cell shapes as oriented ellipses and uses generic-purpose oriented object detectors to identify cells in each frame. A global data association algorithm explores temporal cell similarity using probability distance metrics.</li>
<li>results: The method achieves detection and tracking results competitively with state-of-the-art techniques that require considerably more extensive data annotation.<details>
<summary>Abstract</summary>
Cell detection and tracking are paramount for bio-analysis. Recent approaches rely on the tracking-by-model evolution paradigm, which usually consists of training end-to-end deep learning models to detect and track the cells on the frames with promising results. However, such methods require extensive amounts of annotated data, which is time-consuming to obtain and often requires specialized annotators. This work proposes a new approach based on the classical tracking-by-detection paradigm that alleviates the requirement of annotated data. More precisely, it approximates the cell shapes as oriented ellipses and then uses generic-purpose oriented object detectors to identify the cells in each frame. We then rely on a global data association algorithm that explores temporal cell similarity using probability distance metrics, considering that the ellipses relate to two-dimensional Gaussian distributions. Our results show that our method can achieve detection and tracking results competitively with state-of-the-art techniques that require considerably more extensive data annotation. Our code is available at: https://github.com/LucasKirsten/Deep-Cell-Tracking-EBB.
</details>
<details>
<summary>摘要</summary>
维度分析中的细胞检测和跟踪是非常重要的。现有的方法大多基于跟踪-by-模型演化 paradigm，通常是通过训练端到终的深度学习模型来检测和跟踪细胞在帧中的承诺果。然而，这些方法需要庞大量的注解数据，它们是时间消耗的和特殊的注解员。本工作提出了一种新的方法，基于经典的跟踪-by-检测 paradigm，可以减少注解数据的需求。更准确地说，我们将细胞形状 aproximated为方向几何体，然后使用通用的方向对象检测器来在每帧中识别细胞。我们然后采用了全球数据协调算法，通过考虑细胞形状相似性的时间序列距离度量，来实现细胞跟踪。我们的结果表明，我们的方法可以与state-of-the-art技术相比，实现检测和跟踪结果，而不需要庞大量的注解数据。我们的代码可以在：https://github.com/LucasKirsten/Deep-Cell-Tracking-EBB中找到。
</details></li>
</ul>
<hr>
<h2 id="Question-focused-Summarization-by-Decomposing-Articles-into-Facts-and-Opinions-and-Retrieving-Entities"><a href="#Question-focused-Summarization-by-Decomposing-Articles-into-Facts-and-Opinions-and-Retrieving-Entities" class="headerlink" title="Question-focused Summarization by Decomposing Articles into Facts and Opinions and Retrieving Entities"></a>Question-focused Summarization by Decomposing Articles into Facts and Opinions and Retrieving Entities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04880">http://arxiv.org/abs/2310.04880</a></li>
<li>repo_url: None</li>
<li>paper_authors: Krutika Sarode, Shashidhar Reddy Javaji, Vishal Kalakonnavar</li>
<li>for: 这个研究旨在利用自然语言处理技术预测股票价格波动，具体来说是早期发现经济、政治、社会和技术变革，以便捕捉市场机会。</li>
<li>methods: 该方法包括从新闻文章中提取突出的事实，然后将这些事实与实体组合成 tuples，并使用这些 tuples 获取市场变化的摘要，最后将所有摘要合并为整个文章的摘要总结。</li>
<li>results: 研究希望通过分析Wikipedia数据和经济学人报道来建立公司和实体之间的关系，并使用大语言模型GPT 3.5来获取摘要和形成最终摘要。研究的最终目标是开发一个全面的系统，为金融分析师和投资者提供更加有知见的决策工具，以便早期发现市场趋势和事件。<details>
<summary>Abstract</summary>
This research focuses on utilizing natural language processing techniques to predict stock price fluctuations, with a specific interest in early detection of economic, political, social, and technological changes that can be leveraged for capturing market opportunities. The proposed approach includes the identification of salient facts and events from news articles, then use these facts to form tuples with entities which can be used to get summaries of market changes for particular entity and then finally combining all the summaries to form a final abstract summary of the whole article. The research aims to establish relationships between companies and entities through the analysis of Wikipedia data and articles from the Economist. Large Language Model GPT 3.5 is used for getting the summaries and also forming the final summary. The ultimate goal of this research is to develop a comprehensive system that can provide financial analysts and investors with more informed decision-making tools by enabling early detection of market trends and events.
</details>
<details>
<summary>摘要</summary>
这项研究探讨了使用自然语言处理技术预测股票价格波动，具体来说是早期检测经济、政治、社会和技术变化，以便捕捉市场机会。提出的方法包括从新闻文章中提取重要的事实，然后将这些事实与实体组合成Tuple，并使用这些Tuple获取市场变化的摘要。最终，将所有摘要合并为总摘要。研究的目标是通过分析Wikipedia数据和经济学人报道来建立公司和实体之间的关系。使用大语言模型GPT 3.5来获取摘要和组合总摘要。该研究的最终目标是开发一个全面的系统，为金融分析师和投资者提供更多的 Informed Decision-making 工具，以便早期检测市场趋势和事件。
</details></li>
</ul>
<hr>
<h2 id="Hybrid-Recommendation-System-using-Graph-Neural-Network-and-BERT-Embeddings"><a href="#Hybrid-Recommendation-System-using-Graph-Neural-Network-and-BERT-Embeddings" class="headerlink" title="Hybrid Recommendation System using Graph Neural Network and BERT Embeddings"></a>Hybrid Recommendation System using Graph Neural Network and BERT Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04878">http://arxiv.org/abs/2310.04878</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shashidhar Reddy Javaji, Krutika Sarode</li>
<li>for: 这种模型是为了提供个性化的动画推荐，以满足不同用户的兴趣和需求。</li>
<li>methods: 该模型使用图神经网络（GNN）和句子转换器嵌入来预测不同用户的动画推荐，同时考虑了动画的特征和用户对不同动画的交互。</li>
<li>results: 该模型不仅可以为用户提供个性化的动画推荐，还可以预测特定用户对某部动画的评分。<details>
<summary>Abstract</summary>
Recommender systems have emerged as a crucial component of the modern web ecosystem. The effectiveness and accuracy of such systems are critical for providing users with personalized recommendations that meet their specific interests and needs. In this paper, we introduce a novel model that utilizes a Graph Neural Network (GNN) in conjunction with sentence transformer embeddings to predict anime recommendations for different users. Our model employs the task of link prediction to create a recommendation system that considers both the features of anime and user interactions with different anime. The hybridization of the GNN and transformer embeddings enables us to capture both inter-level and intra-level features of anime data.Our model not only recommends anime to users but also predicts the rating a specific user would give to an anime. We utilize the GraphSAGE network for model building and weighted root mean square error (RMSE) to evaluate the performance of the model. Our approach has the potential to significantly enhance the accuracy and effectiveness of anime recommendation systems and can be extended to other domains that require personalized recommendations.
</details>
<details>
<summary>摘要</summary>
现代网络生态系统中，推荐系统已成为一种重要的组成部分。推荐系统的有效性和准确性对于为用户提供个性化推荐是非常重要的。在这篇论文中，我们介绍了一种新的模型，该模型利用图神经网络（GNN）和句子转换器嵌入来预测不同用户的动画推荐。我们的模型通过链接预测任务来创建一个考虑用户和动画特征的推荐系统。我们将GNN和句子转换器嵌入结合使用，以便捕捉动画数据中的内部和外部特征。我们的模型不仅为用户推荐动画，还预测特定用户对于某个动画的评分。我们使用GraphSAGE网络进行模型建立，并使用Weighted Root Mean Square Error（RMSE）来评估模型的性能。我们的方法有可能在动画推荐系统的准确性和有效性方面带来显著改进，并可以扩展到其他需要个性化推荐的领域。
</details></li>
</ul>
<hr>
<h2 id="AirIMU-Learning-Uncertainty-Propagation-for-Inertial-Odometry"><a href="#AirIMU-Learning-Uncertainty-Propagation-for-Inertial-Odometry" class="headerlink" title="AirIMU: Learning Uncertainty Propagation for Inertial Odometry"></a>AirIMU: Learning Uncertainty Propagation for Inertial Odometry</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04874">http://arxiv.org/abs/2310.04874</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuheng Qiu, Chen Wang, Xunfei Zhou, Youjie Xia, Sebastian Scherer</li>
<li>for: 增强各种传感器系统的优化融合，例如视觉或LiDAR激光测距仪。</li>
<li>methods: 学习基于方法，考虑感知器的非线性特性和各种传感器模型。</li>
<li>results: 在多个宠 Singles benchmark 和一个大规模直升机数据集上，reduced the drift rate of the inertial odometry by a factor of between 2.2 and 4 times。<details>
<summary>Abstract</summary>
Accurate uncertainty estimation for inertial odometry is the foundation to achieve optimal fusion in multi-sensor systems, such as visual or LiDAR inertial odometry. Prior studies often simplify the assumptions regarding the uncertainty of inertial measurements, presuming fixed covariance parameters and empirical IMU sensor models. However, the inherent physical limitations and non-linear characteristics of sensors are difficult to capture. Moreover, uncertainty may fluctuate based on sensor rates and motion modalities, leading to variations across different IMUs. To address these challenges, we formulate a learning-based method that not only encapsulate the non-linearities inherent to IMUs but also ensure the accurate propagation of covariance in a data-driven manner. We extend the PyPose library to enable differentiable batched IMU integration with covariance propagation on manifolds, leading to significant runtime speedup. To demonstrate our method's adaptability, we evaluate it on several benchmarks as well as a large-scale helicopter dataset spanning over 262 kilometers. The drift rate of the inertial odometry on these datasets is reduced by a factor of between 2.2 and 4 times. Our method lays the groundwork for advanced developments in inertial odometry.
</details>
<details>
<summary>摘要</summary>
准确的不确定性估计是多感器系统中征印底层的基础，以实现最优的融合。过去的研究常常简化了涉及到涨动测量的不确定性的假设，假设IMU传感器的covariance参数是固定的，并使用经验测量模型。然而，涉及到传感器的物理限制和非线性特性很难捕捉。此外，不确定性可能会随着传感器的读取速率和运动模式而变化，导致不同的IMU传感器之间存在差异。为了解决这些挑战，我们提出了一种学习基于的方法，不仅能够捕捉IMU传感器的非线性特性，还能够确保数据驱动的 covariance 的准确传播。我们将 PyPose 库扩展以实现批处理的 IMU 融合，从而实现了显著的运行速度提升。为了证明我们的方法的适应性，我们对多个 benchmark 和一个大规模的直升机数据集进行了评估，数据集涵盖了262公里的距离。在这些数据集上，IMU 的涨动率被降低了2.2-4倍。我们的方法为高级发展征印底层提供了基础。
</details></li>
</ul>
<hr>
<h2 id="Lemur-Integrating-Large-Language-Models-in-Automated-Program-Verification"><a href="#Lemur-Integrating-Large-Language-Models-in-Automated-Program-Verification" class="headerlink" title="Lemur: Integrating Large Language Models in Automated Program Verification"></a>Lemur: Integrating Large Language Models in Automated Program Verification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04870">http://arxiv.org/abs/2310.04870</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoze Wu, Clark Barrett, Nina Narodytska</li>
<li>for:  automated program verification</li>
<li>methods:  combines the power of LLMs and automated reasoners</li>
<li>results:  practical improvements on a set of synthetic and competition benchmarks<details>
<summary>Abstract</summary>
The demonstrated code-understanding capability of LLMs raises the question of whether they can be used for automated program verification, a task that often demands high-level abstract reasoning about program properties, which is challenging for verification tools. We propose a general methodology to combine the power of LLMs and automated reasoners for automated program verification. We formally describe this methodology as a set of derivation rules and prove its soundness. We instantiate the calculus as a sound automated verification procedure, which led to practical improvements on a set of synthetic and competition benchmarks.
</details>
<details>
<summary>摘要</summary>
LLMS 的代码理解能力的示例引起了自动化程序验证的问题，这是一项需要高级抽象逻辑来评估程序属性的任务，而这是自动验证工具的挑战。我们提出了将 LLMS 和自动逻辑工具结合使用的一般方法，并正式描述了这种方法的 derivation 规则，并证明其正确性。我们将这种方法实现为一个sound的自动验证过程，并在一组 sintetic 和竞赛 bencmarks 上实现了实践性的改进。
</details></li>
</ul>
<hr>
<h2 id="ILuvUI-Instruction-tuned-LangUage-Vision-modeling-of-UIs-from-Machine-Conversations"><a href="#ILuvUI-Instruction-tuned-LangUage-Vision-modeling-of-UIs-from-Machine-Conversations" class="headerlink" title="ILuvUI: Instruction-tuned LangUage-Vision modeling of UIs from Machine Conversations"></a>ILuvUI: Instruction-tuned LangUage-Vision modeling of UIs from Machine Conversations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04869">http://arxiv.org/abs/2310.04869</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Jiang, Eldon Schoop, Amanda Swearngin, Jeffrey Nichols</li>
<li>for: 这篇论文目的是为了提高对UI任务的识别能力，并且不需要人类提供纠正注解。</li>
<li>methods: 该方法 combining existing pixel-based methods with a Large Language Model (LLM)，可以应用于任何UI屏幕截图数据集。</li>
<li>results: 该研究生成了335,000个对话示例，并使用它们来练化一个对话型VLM进行UI任务。研究还评估了模型的性能，包括UI元素检测任务、回答质量和多步UI导航和规划等。<details>
<summary>Abstract</summary>
Multimodal Vision-Language Models (VLMs) enable powerful applications from their fused understanding of images and language, but many perform poorly on UI tasks due to the lack of UI training data. In this paper, we adapt a recipe for generating paired text-image training data for VLMs to the UI domain by combining existing pixel-based methods with a Large Language Model (LLM). Unlike prior art, our method requires no human-provided annotations, and it can be applied to any dataset of UI screenshots. We generate a dataset of 335K conversational examples paired with UIs that cover Q&A, UI descriptions, and planning, and use it to fine-tune a conversational VLM for UI tasks. To assess the performance of our model, we benchmark it on UI element detection tasks, evaluate response quality, and showcase its applicability to multi-step UI navigation and planning.
</details>
<details>
<summary>摘要</summary>
多Modal视觉语言模型（VLM）允许强大的应用由图像和语言的融合理解，但许多perform poorly on UI任务由于缺乏UI训练数据。在这篇论文中，我们适应了一个方法来生成图像和文本的对应训练数据 для VLM，并将其应用到UI领域。与先前艺术不同，我们的方法不需要人工提供笔记，并且可以应用于任何UI屏幕截图集。我们生成了335K的对话示例，与UI描述、问答和规划相关，并用它们来精心UI任务。为评估我们模型的性能，我们对UI元素检测任务进行了测试，评估响应质量，并显示了其可应用于多步UI导航和规划。
</details></li>
</ul>
<hr>
<h2 id="ForeSeer-Product-Aspect-Forecasting-Using-Temporal-Graph-Embedding"><a href="#ForeSeer-Product-Aspect-Forecasting-Using-Temporal-Graph-Embedding" class="headerlink" title="ForeSeer: Product Aspect Forecasting Using Temporal Graph Embedding"></a>ForeSeer: Product Aspect Forecasting Using Temporal Graph Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04865">http://arxiv.org/abs/2310.04865</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zixuan Liu, Gaurush Hiranandani, Kun Qian, Eddie W. Huang, Yi Xu, Belinda Zeng, Karthik Subbian, Sheng Wang</li>
<li>for: 预测新产品的未来升级特征</li>
<li>methods: 使用文本挖掘和产品嵌入approach，逐渐在时间产品图上进行训练</li>
<li>results: 与现有方法相比， ForeSeer 在实际 setting 中具有至少49.1%的 AUPRC 提升，并且在产品图和评论特征关联预测方面具有改善。<details>
<summary>Abstract</summary>
Developing text mining approaches to mine aspects from customer reviews has been well-studied due to its importance in understanding customer needs and product attributes. In contrast, it remains unclear how to predict the future emerging aspects of a new product that currently has little review information. This task, which we named product aspect forecasting, is critical for recommending new products, but also challenging because of the missing reviews. Here, we propose ForeSeer, a novel textual mining and product embedding approach progressively trained on temporal product graphs for this novel product aspect forecasting task. ForeSeer transfers reviews from similar products on a large product graph and exploits these reviews to predict aspects that might emerge in future reviews. A key novelty of our method is to jointly provide review, product, and aspect embeddings that are both time-sensitive and less affected by extremely imbalanced aspect frequencies. We evaluated ForeSeer on a real-world product review system containing 11,536,382 reviews and 11,000 products over 3 years. We observe that ForeSeer substantially outperformed existing approaches with at least 49.1\% AUPRC improvement under the real setting where aspect associations are not given. ForeSeer further improves future link prediction on the product graph and the review aspect association prediction. Collectively, Foreseer offers a novel framework for review forecasting by effectively integrating review text, product network, and temporal information, opening up new avenues for online shopping recommendation and e-commerce applications.
</details>
<details>
<summary>摘要</summary>
开发文本挖掘方法来挖掘用户评价中的方面，因为它对理解客户需求和产品特性非常重要。然而，尚未有效地预测新产品未经评价的方面。这个任务，我们称之为产品方面预测，是推荐新产品的关键任务，但也是非常困难的因为缺少评价。在这里，我们提出了 ForeSeer，一种新的文本挖掘和产品嵌入方法，通过在时间维度上进行模糊嵌入来预测未来评价中可能出现的方面。ForeSeer 可以从类似产品上的大规模产品图中传递评价，并利用这些评价来预测未来可能出现的方面。我们的方法的一个新特点是同时提供评价、产品和方面嵌入，这些嵌入不仅时间敏感，也受到方面频率异常高的影响。我们在一个真实的产品评价系统上进行了测试，包括11,536,382篇评价和11,000个产品，覆盖了3年的时间。我们发现 ForeSeer 在实际 Setting 下明显超过了现有方法，至少提高了49.1%的 AUPRC。ForeSeer 还改进了产品图中未来链接预测和评价方面关联预测。总之，ForeSeer 提供了一种新的评价预测框架，通过有效地结合评价文本、产品网络和时间信息，打开了在线购物推荐和电商应用的新 Avenues。
</details></li>
</ul>
<hr>
<h2 id="Uncovering-hidden-geometry-in-Transformers-via-disentangling-position-and-context"><a href="#Uncovering-hidden-geometry-in-Transformers-via-disentangling-position-and-context" class="headerlink" title="Uncovering hidden geometry in Transformers via disentangling position and context"></a>Uncovering hidden geometry in Transformers via disentangling position and context</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04861">http://arxiv.org/abs/2310.04861</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiajunsong629/uncover-hidden-geometry">https://github.com/jiajunsong629/uncover-hidden-geometry</a></li>
<li>paper_authors: Jiajun Song, Yiqiao Zhong</li>
<li>for: This paper aims to provide a simple yet informative decomposition of hidden states (or embeddings) of trained transformers into interpretable components, in order to gain structural insights about input formats in in-context learning and arithmetic tasks.</li>
<li>methods: The authors use a tensor representation of embedding vectors $\boldsymbol{h} \in \mathbb{R}^{C \times T \times d}$ to extract the mean effects and decompose the hidden states into interpretable components, including the global mean vector $\boldsymbol{\mu}$, the mean vectors across contexts and positions $\mathbf{pos}_t$ and $\mathbf{ctx}<em>c$, and the residual vector $\mathbf{resid}</em>{c,t}$.</li>
<li>results: The authors find that the decomposition yields a pervasive mathematical structure across popular transformer architectures and diverse text datasets, including a low-dimensional, continuous, and often spiral shape for the mean vectors across positions, clear cluster structure for the mean vectors across contexts, and mutual incoherence between the mean vectors across positions and contexts. These findings offer structural insights into the input formats of transformers and have implications for in-context learning and arithmetic tasks.<details>
<summary>Abstract</summary>
Transformers are widely used to extract complex semantic meanings from input tokens, yet they usually operate as black-box models. In this paper, we present a simple yet informative decomposition of hidden states (or embeddings) of trained transformers into interpretable components. For any layer, embedding vectors of input sequence samples are represented by a tensor $\boldsymbol{h} \in \mathbb{R}^{C \times T \times d}$. Given embedding vector $\boldsymbol{h}_{c,t} \in \mathbb{R}^d$ at sequence position $t \le T$ in a sequence (or context) $c \le C$, extracting the mean effects yields the decomposition \[ \boldsymbol{h}_{c,t} = \boldsymbol{\mu} + \mathbf{pos}_t + \mathbf{ctx}_c + \mathbf{resid}_{c,t} \] where $\boldsymbol{\mu}$ is the global mean vector, $\mathbf{pos}_t$ and $\mathbf{ctx}_c$ are the mean vectors across contexts and across positions respectively, and $\mathbf{resid}_{c,t}$ is the residual vector. For popular transformer architectures and diverse text datasets, empirically we find pervasive mathematical structure: (1) $(\mathbf{pos}_t)_{t}$ forms a low-dimensional, continuous, and often spiral shape across layers, (2) $(\mathbf{ctx}_c)_c$ shows clear cluster structure that falls into context topics, and (3) $(\mathbf{pos}_t)_{t}$ and $(\mathbf{ctx}_c)_c$ are mutually incoherent -- namely $\mathbf{pos}_t$ is almost orthogonal to $\mathbf{ctx}_c$ -- which is canonical in compressed sensing and dictionary learning. This decomposition offers structural insights about input formats in in-context learning (especially for induction heads) and in arithmetic tasks.
</details>
<details>
<summary>摘要</summary>
transformers 广泛使用来提取输入токен的复杂含义，但它们通常作为黑obox模型运行。在这篇文章中，我们提出了一种简单又有用的隐状态（或嵌入）的解剖法。对于任何层，输入序列样本的嵌入 вектор $\boldsymbol{h} \in \mathbb{R}^{C \times T \times d}$ 可以被表示为tensor。在序列（或上下文） $c \le C$ 中的嵌入 vector $\boldsymbol{h}_{c,t} \in \mathbb{R}^d$ 的mean effect的提取可以得到以下含义的解剖：$$\boldsymbol{h}_{c,t} = \boldsymbol{\mu} + \mathbf{pos}_t + \mathbf{ctx}_c + \mathbf{resid}_{c,t}$$其中，$\boldsymbol{\mu}$ 是全局均值向量，$\mathbf{pos}_t$ 和 $\mathbf{ctx}_c$ 是在位置和上下文中的均值向量，respectively，和 $\mathbf{resid}_{c,t}$ 是剩余向量。对于流行的 transformer 架构和多种文本数据集，我们在实验上发现了一种普遍的数学结构：1. $({\mathbf{pos}_t)_t$ 形成了一个低维、连续、常见的旋转形状 across layers。2. $({\mathbf{ctx}_c)_c$ 显示了明确的群结构，frequently falls into context topics。3. $({\mathbf{pos}_t)_t$ 和 $({\mathbf{ctx}_c)_c$ 是互不协方的 -- namely $\mathbf{pos}_t$ 几乎是 $\mathbf{ctx}_c$ 的orthogonal vector -- which is canonical in compressed sensing and dictionary learning.这种解剖提供了针对输入格式的结构性理解，特别是在 induction heads 中，以及在算术任务中。
</details></li>
</ul>
<hr>
<h2 id="Balancing-utility-and-cognitive-cost-in-social-representation"><a href="#Balancing-utility-and-cognitive-cost-in-social-representation" class="headerlink" title="Balancing utility and cognitive cost in social representation"></a>Balancing utility and cognitive cost in social representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04852">http://arxiv.org/abs/2310.04852</a></li>
<li>repo_url: None</li>
<li>paper_authors: Max Taylor-Davies, Christopher G. Lucas</li>
<li>for: 该论文旨在研究如何为agent构建和维护其所处环境中其他agent的表示，以便更好地完成多个任务。</li>
<li>methods: 论文使用选择性依效为例任务，描述了代理在选择表示信息时的问题，并提出了两种资源受限制的社会表示方法。</li>
<li>results: 论文通过例子示出了如何在资源受限制的情况下选择合适的表示信息，以优化代理在下游任务中的性能。<details>
<summary>Abstract</summary>
To successfully navigate its environment, an agent must construct and maintain representations of the other agents that it encounters. Such representations are useful for many tasks, but they are not without cost. As a result, agents must make decisions regarding how much information they choose to represent about the agents in their environment. Using selective imitation as an example task, we motivate the problem of finding agent representations that optimally trade off between downstream utility and information cost, and illustrate two example approaches to resource-constrained social representation.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:为了成功地 navigate 其环境，一个 Agent 需要构建和维护与其他 Agent 的表示。这些表示对于许多任务都是有用的，但它们不是无成本的。因此，Agent 需要做出如何选择 represent 的信息的决策。使用选择性模仿为例题，我们激发了找到 Agent 表示，并且优化它们与下游用途之间的平衡问题的问题，并示出了两种资源受限制的社会表示的例方法。
</details></li>
</ul>
<hr>
<h2 id="Sub-linear-Regret-in-Adaptive-Model-Predictive-Control"><a href="#Sub-linear-Regret-in-Adaptive-Model-Predictive-Control" class="headerlink" title="Sub-linear Regret in Adaptive Model Predictive Control"></a>Sub-linear Regret in Adaptive Model Predictive Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04842">http://arxiv.org/abs/2310.04842</a></li>
<li>repo_url: None</li>
<li>paper_authors: Damianos Tranos, Alexandre Proutiere</li>
<li>for: 这个论文针对不确定的线性系统进行了适束型预测控制（MPC）。</li>
<li>methods: 这个算法使用了自适束管道（polytopic tubes）和确定性等价原理（certainty-equivalence principle），在线性系统中处理不确定性和状态和输入限制。</li>
<li>results: 这个算法可以确保状态和输入限制，并且有 recursive-feasibility 和渐进稳定性。对比于对系统动力学确知的oracle算法，这个算法的误差不超过 $O(T^{1&#x2F;2 + \epsilon})$，其中 $\epsilon \in (0,1)$ 是设计参数，用于调整惊对性部分的algorithm。<details>
<summary>Abstract</summary>
We consider the problem of adaptive Model Predictive Control (MPC) for uncertain linear-systems with additive disturbances and with state and input constraints. We present STT-MPC (Self-Tuning Tube-based Model Predictive Control), an online algorithm that combines the certainty-equivalence principle and polytopic tubes. Specifically, at any given step, STT-MPC infers the system dynamics using the Least Squares Estimator (LSE), and applies a controller obtained by solving an MPC problem using these estimates. The use of polytopic tubes is so that, despite the uncertainties, state and input constraints are satisfied, and recursive-feasibility and asymptotic stability hold. In this work, we analyze the regret of the algorithm, when compared to an oracle algorithm initially aware of the system dynamics. We establish that the expected regret of STT-MPC does not exceed $O(T^{1/2 + \epsilon})$, where $\epsilon \in (0,1)$ is a design parameter tuning the persistent excitation component of the algorithm. Our result relies on a recently proposed exponential decay of sensitivity property and, to the best of our knowledge, is the first of its kind in this setting. We illustrate the performance of our algorithm using a simple numerical example.
</details>
<details>
<summary>摘要</summary>
我们考虑了适束预测控制（MPC）的问题，这是不确定线性系统中的噪音和外部干扰，并且受到状态和输入范围限制。我们提出了自适束管道基本预测控制（STT-MPC），这是一个在线上算法，它结合了必然等价原理和多topic管道。具体来说，在任何一步中，STT-MPC使用最小二乘估计器（LSE）估算系统动力学，并使用这些估值解决MPC问题。使用多topic管道的好处是，即使存在不确定性，状态和输入范围仍然满足，并且积累可行性和渐进稳定性持续。在这个工作中，我们分析了STT-MPC的幻悔，与一个对系统动力学有认识的oracle算法进行比较。我们证明，STT-MPC的预料 regret不超过$O(T^{1/2 + \epsilon})$，其中$\epsilon \in (0,1)$是一个设计参数，用于调整 persistentexcitation的部分。我们的结果基于最近提出的对敏感度快速衰减的性质，并且，至今为止，这是这个设定中的第一个相关结果。我们使用一个简单的数据示例来说明性能。
</details></li>
</ul>
<hr>
<h2 id="Federated-Self-Supervised-Learning-of-Monocular-Depth-Estimators-for-Autonomous-Vehicles"><a href="#Federated-Self-Supervised-Learning-of-Monocular-Depth-Estimators-for-Autonomous-Vehicles" class="headerlink" title="Federated Self-Supervised Learning of Monocular Depth Estimators for Autonomous Vehicles"></a>Federated Self-Supervised Learning of Monocular Depth Estimators for Autonomous Vehicles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04837">http://arxiv.org/abs/2310.04837</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elton F. de S. Soares, Carlos Alberto V. Campos</li>
<li>for:  Image-based depth estimation for autonomous vehicles in intelligent transportation systems.</li>
<li>methods: Federated learning and deep self-supervision.</li>
<li>results: Near state-of-the-art performance with a test loss below 0.13 and requiring, on average, only 1.5k training steps and up to 0.415 GB of weight data transfer per autonomous vehicle on each round.<details>
<summary>Abstract</summary>
Image-based depth estimation has gained significant attention in recent research on computer vision for autonomous vehicles in intelligent transportation systems. This focus stems from its cost-effectiveness and wide range of potential applications. Unlike binocular depth estimation methods that require two fixed cameras, monocular depth estimation methods only rely on a single camera, making them highly versatile. While state-of-the-art approaches for this task leverage self-supervised learning of deep neural networks in conjunction with tasks like pose estimation and semantic segmentation, none of them have explored the combination of federated learning and self-supervision to train models using unlabeled and private data captured by autonomous vehicles. The utilization of federated learning offers notable benefits, including enhanced privacy protection, reduced network consumption, and improved resilience to connectivity issues. To address this gap, we propose FedSCDepth, a novel method that combines federated learning and deep self-supervision to enable the learning of monocular depth estimators with comparable effectiveness and superior efficiency compared to the current state-of-the-art methods. Our evaluation experiments conducted on Eigen's Split of the KITTI dataset demonstrate that our proposed method achieves near state-of-the-art performance, with a test loss below 0.13 and requiring, on average, only 1.5k training steps and up to 0.415 GB of weight data transfer per autonomous vehicle on each round.
</details>
<details>
<summary>摘要</summary>
Image-based深度估计在计算机视觉领域中获得了广泛关注，尤其是在自动驾驶系统中。这种关注的原因在于它的成本效益和广泛的应用前景。不同于使用两个固定摄像头的双目深度估计方法，单目深度估计方法只需要一个摄像头，这使得它们非常灵活。当前的状态emo approaches for this task leverageself-supervised learning of deep neural networks in conjunction with tasks like pose estimation and semantic segmentation, but none of them have explored the combination of federated learning and self-supervision to train models using unlabeled and private data captured by autonomous vehicles. Federated learning offers notable benefits, including enhanced privacy protection, reduced network consumption, and improved resilience to connectivity issues. To address this gap, we propose FedSCDepth, a novel method that combines federated learning and deep self-supervision to enable the learning of monocular depth estimators with comparable effectiveness and superior efficiency compared to the current state-of-the-art methods. Our evaluation experiments conducted on Eigen's Split of the KITTI dataset demonstrate that our proposed method achieves near state-of-the-art performance, with a test loss below 0.13 and requiring, on average, only 1.5k training steps and up to 0.415 GB of weight data transfer per autonomous vehicle on each round.Here's the translation in Traditional Chinese:Image-based深度估计在计算机视觉领域中获得了广泛关注，尤其是在自动驾驶系统中。这种关注的原因在于它的成本效益和广泛的应用前景。不同于使用两个固定摄像头的双目深度估计方法，单目深度估计方法只需要一个摄像头，这使得它们非常灵活。现今的状态emo approaches for this task leverageself-supervised learning of deep neural networks in conjunction with tasks like pose estimation and semantic segmentation, but none of them have explored the combination of federated learning and self-supervision to train models using unlabeled and private data captured by autonomous vehicles。 Federated learning offers notable benefits, including enhanced privacy protection, reduced network consumption, and improved resilience to connectivity issues。 To address this gap, we propose FedSCDepth, a novel method that combines federated learning and deep self-supervision to enable the learning of monocular depth estimators with comparable effectiveness and superior efficiency compared to the current state-of-the-art methods。 Our evaluation experiments conducted on Eigen's Split of the KITTI dataset demonstrate that our proposed method achieves near state-of-the-art performance, with a test loss below 0.13 and requiring, on average, only 1.5k training steps and up to 0.415 GB of weight data transfer per autonomous vehicle on each round。
</details></li>
</ul>
<hr>
<h2 id="Dual-Grained-Quantization-Efficient-Fine-Grained-Quantization-for-LLM"><a href="#Dual-Grained-Quantization-Efficient-Fine-Grained-Quantization-for-LLM" class="headerlink" title="Dual Grained Quantization: Efficient Fine-Grained Quantization for LLM"></a>Dual Grained Quantization: Efficient Fine-Grained Quantization for LLM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04836">http://arxiv.org/abs/2310.04836</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luoming Zhang, Wen Fei, Weijia Wu, Yefei He, Zhenyu Lou, Hong Zhou</li>
<li>for: This paper aims to improve the efficiency of large language models (LLMs) for real-world applications by introducing a novel quantization method called Dual Grained Quantization (DGQ).</li>
<li>methods: The DGQ method uses a two-phase grid search algorithm to determine the optimal quantization scales for both coarse-grained and fine-grained quantization, and it dequantizes the fine-grained INT4 weight into coarse-grained INT8 representation for efficient matrix multiplication.</li>
<li>results: The experimental results show that DGQ consistently outperforms prior methods across various LLM architectures and tasks, and achieves significant memory reduction and speed gains compared to the A16W4 implementation. Specifically, DGQ achieves $\textbf{1.12}$ $\times$ memory reduction and $\textbf{3.24}$ $\times$ speed gains.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) pose significant hardware challenges related to memory requirements and computational ability. There are two mainstream quantization schemes for LLMs: coarse-grained ($\textit{e.g.,}$ channel-wise) quantization and fine-grained ($\textit{e.g.,}$ group-wise) quantization. Fine-grained quantization has smaller quantization loss, consequently achieving superior performance. However, when applied to weight-activation quantization, it disrupts continuous integer matrix multiplication, leading to inefficient inference. In this paper, we introduce Dual Grained Quantization (DGQ), a novel A8W4 quantization for LLM that maintains superior performance while ensuring fast inference speed. DSQ dequantizes the fine-grained INT4 weight into coarse-grained INT8 representation and preform matrix multiplication using INT8 kernels. Besides, we develop a two-phase grid search algorithm to simplify the determination of fine-grained and coarse-grained quantization scales. We also devise a percentile clipping schema for smoothing the activation outliers without the need for complex optimization techniques. Experimental results demonstrate that DGQ consistently outperforms prior methods across various LLM architectures and a wide range of tasks. Remarkably, by our implemented efficient CUTLASS kernel, we achieve $\textbf{1.12}$ $\times$ memory reduction and $\textbf{3.24}$ $\times$ speed gains comparing A16W4 implementation. These advancements enable efficient deployment of A8W4 LLMs for real-world applications.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）对于内存需求和计算能力带来严重的挑战。现有两种主流的量化方案 для LLM：粗糙化（channel-wise）量化和细糙化（group-wise）量化。细糙化量化具有较小的量化损失，因此可以 дости得更高的性能。然而，当应用到对于量化的量化时，它会破坏连续数字matrix乘法，导致不fficient的推导。在这篇论文中，我们介绍了dual grained量化（DGQ），一种新的A8W4量化方法 для LLM，可以保持高性能的同时确保快速的推导。DGQ将细糙化INT4Weight转换为粗糙化INT8表示，并使用INT8核心进行矩阵乘法。此外，我们开发了一个双阶搜索算法来简化粗糙化和细糙化量化数值的决定。我们还提出了一个百分比剪枝架构内部构件的schema来缓和活动异常值，无需复杂的优化技术。实验结果显示，DGQ与先前的方法相比，在不同的LLM架构和各种任务上具有优秀的性能。特别是，我们通过我们实现的高效的CUTLASS核心，实现了$\textbf{1.12}$ $\times$ 的内存减少和$\textbf{3.24}$ $\times$ 的速度提升，与A16W4实现相比。这些突破创新实现了A8W4 LLM的效率部署。
</details></li>
</ul>
<hr>
<h2 id="On-the-Evolution-of-Knowledge-Graphs-A-Survey-and-Perspective"><a href="#On-the-Evolution-of-Knowledge-Graphs-A-Survey-and-Perspective" class="headerlink" title="On the Evolution of Knowledge Graphs: A Survey and Perspective"></a>On the Evolution of Knowledge Graphs: A Survey and Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04835">http://arxiv.org/abs/2310.04835</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuhui Jiang, Chengjin Xu, Yinghan Shen, Xun Sun, Lumingyuan Tang, Saizhuo Wang, Zhongwu Chen, Yuanzhuo Wang, Jian Guo</li>
<li>for: 本文提供了知识 graphs（KGs）的演化和知识EXTRACTION、理解以及表示技术的全面综述，以及不同类型的KGs在实际应用中的实践案例。</li>
<li>methods: 本文 introduce了不同类型的KGs（静止KGs、动态KGs、时间KGs和事件KGs）的技术和实践应用，以及知识EXTRACTION和理解的方法。</li>
<li>results: 本文提出了未来知识工程的前瞻之处，包括将知识 graphs和大型自然语言模型（LLMs）相结合的潜力，以及知识EXTRACTION、理解和表示的进一步发展。<details>
<summary>Abstract</summary>
Knowledge graphs (KGs) are structured representations of diversified knowledge. They are widely used in various intelligent applications. In this article, we provide a comprehensive survey on the evolution of various types of knowledge graphs (i.e., static KGs, dynamic KGs, temporal KGs, and event KGs) and techniques for knowledge extraction and reasoning. Furthermore, we introduce the practical applications of different types of KGs, including a case study in financial analysis. Finally, we propose our perspective on the future directions of knowledge engineering, including the potential of combining the power of knowledge graphs and large language models (LLMs), and the evolution of knowledge extraction, reasoning, and representation.
</details>
<details>
<summary>摘要</summary>
知识图（KG）是一种结构化表示多样化知识的工具。它在各种智能应用中广泛使用。本文提供了知识图的发展历程（静态KG、动态KG、时间KG和事件KG）和知识抽取和推理技术的总览。此外，我们还介绍了不同类型的KG的实际应用，以及一个金融分析的实例研究。最后，我们提出了未来知识工程的未来方向，包括结合知识图和大型自然语言模型（LLM）的潜力，以及知识抽取、推理和表示的进一步发展。
</details></li>
</ul>
<hr>
<h2 id="Rethink-Baseline-of-Integrated-Gradients-from-the-Perspective-of-Shapley-Value"><a href="#Rethink-Baseline-of-Integrated-Gradients-from-the-Perspective-of-Shapley-Value" class="headerlink" title="Rethink Baseline of Integrated Gradients from the Perspective of Shapley Value"></a>Rethink Baseline of Integrated Gradients from the Perspective of Shapley Value</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04821">http://arxiv.org/abs/2310.04821</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuyang Liu, Zixuan Chen, Ge Shi, Ji Wang, Changjie Fan, Yu Xiong, Runze Wu Yujing Hu, Ze Ji, Yang Gao</li>
<li>for: 解释深度神经网络（DNN）预测结果的原因。</li>
<li>methods: 基于Aumann-Shapley Value的基准设计方法，包括新的Shapley Integrated Gradients（SIG）方法。</li>
<li>results: SIG方法可以更好地估计特征的贡献，提供更一致的解释，并适用于不同应用场景和数据类型。<details>
<summary>Abstract</summary>
Numerous approaches have attempted to interpret deep neural networks (DNNs) by attributing the prediction of DNN to its input features. One of the well-studied attribution methods is Integrated Gradients (IG). Specifically, the choice of baselines for IG is a critical consideration for generating meaningful and unbiased explanations for model predictions in different scenarios. However, current practice of exploiting a single baseline fails to fulfill this ambition, thus demanding multiple baselines. Fortunately, the inherent connection between IG and Aumann-Shapley Value forms a unique perspective to rethink the design of baselines. Under certain hypothesis, we theoretically analyse that a set of baseline aligns with the coalitions in Shapley Value. Thus, we propose a novel baseline construction method called Shapley Integrated Gradients (SIG) that searches for a set of baselines by proportional sampling to partly simulate the computation path of Shapley Value. Simulations on GridWorld show that SIG approximates the proportion of Shapley Values. Furthermore, experiments conducted on various image tasks demonstrate that compared to IG using other baseline methods, SIG exhibits an improved estimation of feature's contribution, offers more consistent explanations across diverse applications, and is generic to distinct data types or instances with insignificant computational overhead.
</details>
<details>
<summary>摘要</summary>
多种方法已经尝试解释深度神经网络（DNN）的预测，其中一种广泛研究的方法是集成梯度（IG）。specifically，选择基线是 kritical consideration for generating meaningful and unbiased explanations for model predictions in different scenarios。然而，现行的单个基线使用方式不能满足这个目标，因此需要多个基线。幸运的是，IG和AUmann-Shapley Value之间的内在连接形成了一个独特的视角，可以重新思考基线的设计。根据某些假设，我们理论分析表明，一组基eline可以与Shapley Value中的联盟相对应。因此，我们提出了一种新的基线建立方法called Shapley Integrated Gradients（SIG），该方法通过质量抽样来寻找一组基eline，以便 partly simulate Shapley Value的计算路径。在GridWorld上的 simulations中，我们发现SIG可以相似地 aproximate Shapley Value的分布。此外，在多个图像任务上进行的实验表明，相比IG使用其他基eline方法，SIG可以更好地评估特征的贡献，提供更一致的解释，并且对于不同的数据类型或实例来说具有无关的计算开销。
</details></li>
</ul>
<hr>
<h2 id="Hacking-Generative-Models-with-Differentiable-Network-Bending"><a href="#Hacking-Generative-Models-with-Differentiable-Network-Bending" class="headerlink" title="Hacking Generative Models with Differentiable Network Bending"></a>Hacking Generative Models with Differentiable Network Bending</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04816">http://arxiv.org/abs/2310.04816</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giacomo Aldegheri, Alina Rogalska, Ahmed Youssef, Eugenia Iofinova</li>
<li>for: 这篇论文是为了探讨如何“黑客”生成模型，让其输出趋离原始训练分布向新的目标。</li>
<li>methods: 这篇论文使用了一种小规模可训练的模块，在生成模型中间层插入并在一些较低的迭代数上训练，保持其余的网络冻结不动。</li>
<li>results: 该方法可以生成具有怪异质量的输出图像，即由原始和新目标之间的矛盾带来的艺术效果。<details>
<summary>Abstract</summary>
In this work, we propose a method to 'hack' generative models, pushing their outputs away from the original training distribution towards a new objective. We inject a small-scale trainable module between the intermediate layers of the model and train it for a low number of iterations, keeping the rest of the network frozen. The resulting output images display an uncanny quality, given by the tension between the original and new objectives that can be exploited for artistic purposes.
</details>
<details>
<summary>摘要</summary>
在这个研究中，我们提出了一种方法，用于“黑客”生成模型，使其输出偏离原始训练分布向新的目标。我们在模型中插入一个小规模可训练的模块，并在几个迭代后冻结整个网络。结果的输出图像具有怪异的质量，它由原始和新的目标之间的紧张关系带来，可以用于艺术目的。
</details></li>
</ul>
<hr>
<h2 id="User’s-Position-Dependent-Strategies-in-Consumer-Generated-Media-with-Monetary-Rewards"><a href="#User’s-Position-Dependent-Strategies-in-Consumer-Generated-Media-with-Monetary-Rewards" class="headerlink" title="User’s Position-Dependent Strategies in Consumer-Generated Media with Monetary Rewards"></a>User’s Position-Dependent Strategies in Consumer-Generated Media with Monetary Rewards</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04805">http://arxiv.org/abs/2310.04805</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shintaro Ueki, Fujio Toriumi, Toshiharu Sugawara</li>
<li>for: This paper aims to help content-sharing platform designers create more effective monetary reward schemes to incentivize user participation and improve content quality.</li>
<li>methods: The authors propose a model that integrates monetary reward schemes into the Social Networking Services (SNS) norms game, and experimentally investigate the impact of different reward schemes on user behavior and content quality.</li>
<li>results: The authors find that different monetary reward schemes have distinct effects on user proactivity and content quality, and that these effects depend on the user’s position in the CGM network. Their findings can help platform designers create more effective reward schemes to improve user engagement and content quality.<details>
<summary>Abstract</summary>
Numerous forms of consumer-generated media (CGM), such as social networking services (SNS), are widely used. Their success relies on users' voluntary participation, often driven by psychological rewards like recognition and connection from reactions by other users. Furthermore, a few CGM platforms offer monetary rewards to users, serving as incentives for sharing items such as articles, images, and videos. However, users have varying preferences for monetary and psychological rewards, and the impact of monetary rewards on user behaviors and the quality of the content they post remains unclear. Hence, we propose a model that integrates some monetary reward schemes into the SNS-norms game, which is an abstraction of CGM. Subsequently, we investigate the effect of each monetary reward scheme on individual agents (users), particularly in terms of their proactivity in posting items and their quality, depending on agents' positions in a CGM network. Our experimental results suggest that these factors distinctly affect the number of postings and their quality. We believe that our findings will help CGM platformers in designing better monetary reward schemes.
</details>
<details>
<summary>摘要</summary>
众多的消费者生成内容（CGM），如社交媒体服务（SNS），广泛使用。它们的成功取决于用户的自愿参与，通常由其他用户的反应所驱动，如认可和连接。此外，一些CGM平台还提供金钱奖励给用户，作为启发共享文章、图片和视频的行为的激励。然而，用户对金钱和心理奖励的偏好不同，以及奖励对用户行为和文章质量的影响仍然不清楚。因此，我们提出一个 integrate some monetary reward schemes into the SNS-norms game的模型，并 investigate the effect of each monetary reward scheme on individual agents (users)，特别是在CGM网络中 agents的位置。我们的实验结果表明，这些因素明显地影响了用户的投稿数量和质量。我们认为，我们的发现将有助于CGM平台的设计。
</details></li>
</ul>
<hr>
<h2 id="Ten-Challenges-in-Industrial-Recommender-Systems"><a href="#Ten-Challenges-in-Industrial-Recommender-Systems" class="headerlink" title="Ten Challenges in Industrial Recommender Systems"></a>Ten Challenges in Industrial Recommender Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04804">http://arxiv.org/abs/2310.04804</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenhua Dong, Jieming Zhu, Weiwen Liu, Ruiming Tang</li>
<li>for: 本研讨会讲解了十个有趣和重要的推荐系统挑战，以帮助RecSys社区创造更好的推荐系统。</li>
<li>methods: 文章介绍了一些适用于推荐系统的技术趋势，包括深度和复杂的模型，如神经网络和预训练语言模型。</li>
<li>results: 文章描述了在实际应用中遇到的一些困难和挑战，以帮助RecSys社区更好地解决这些问题。<details>
<summary>Abstract</summary>
Huawei's vision and mission is to build a fully connected intelligent world. Since 2013, Huawei Noah's Ark Lab has helped many products build recommender systems and search engines for getting the right information to the right users. Every day, our recommender systems serve hundreds of millions of mobile phone users and recommend different kinds of content and services such as apps, news feeds, songs, videos, books, themes, and instant services. The big data and various scenarios provide us with great opportunities to develop advanced recommendation technologies. Furthermore, we have witnessed the technical trend of recommendation models in the past ten years, from the shallow and simple models like collaborative filtering, linear models, low rank models to deep and complex models like neural networks, pre-trained language models. Based on the mission, opportunities and technological trends, we have also met several hard problems in our recommender systems. In this talk, we will share ten important and interesting challenges and hope that the RecSys community can get inspired and create better recommender systems.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="HNS-An-Efficient-Hermite-Neural-Solver-for-Solving-Time-Fractional-Partial-Differential-Equations"><a href="#HNS-An-Efficient-Hermite-Neural-Solver-for-Solving-Time-Fractional-Partial-Differential-Equations" class="headerlink" title="HNS: An Efficient Hermite Neural Solver for Solving Time-Fractional Partial Differential Equations"></a>HNS: An Efficient Hermite Neural Solver for Solving Time-Fractional Partial Differential Equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04789">http://arxiv.org/abs/2310.04789</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hsbhc/hns">https://github.com/hsbhc/hns</a></li>
<li>paper_authors: Jie Hou, Zhiying Ma, Shihui Ying, Ying Li</li>
<li>for: 解决时间分数导函数方程 equations using deep learning techniques</li>
<li>methods: 使用 Hermite interpolation techniques 和 deep neural networks</li>
<li>results: 实验结果显示 HNS 的精度比 L1 方法高，并且在高维enario中也有显著改善。<details>
<summary>Abstract</summary>
Neural network solvers represent an innovative and promising approach for tackling time-fractional partial differential equations by utilizing deep learning techniques. L1 interpolation approximation serves as the standard method for addressing time-fractional derivatives within neural network solvers. However, we have discovered that neural network solvers based on L1 interpolation approximation are unable to fully exploit the benefits of neural networks, and the accuracy of these models is constrained to interpolation errors. In this paper, we present the high-precision Hermite Neural Solver (HNS) for solving time-fractional partial differential equations. Specifically, we first construct a high-order explicit approximation scheme for fractional derivatives using Hermite interpolation techniques, and rigorously analyze its approximation accuracy. Afterward, taking into account the infinitely differentiable properties of deep neural networks, we integrate the high-order Hermite interpolation explicit approximation scheme with deep neural networks to propose the HNS. The experimental results show that HNS achieves higher accuracy than methods based on the L1 scheme for both forward and inverse problems, as well as in high-dimensional scenarios. This indicates that HNS has significantly improved accuracy and flexibility compared to existing L1-based methods, and has overcome the limitations of explicit finite difference approximation methods that are often constrained to function value interpolation. As a result, the HNS is not a simple combination of numerical computing methods and neural networks, but rather achieves a complementary and mutually reinforcing advantages of both approaches. The data and code can be found at \url{https://github.com/hsbhc/HNS}.
</details>
<details>
<summary>摘要</summary>
神经网络解决方法代表了一种创新和有前途的方法，用于解决时间分辨率部分弗散方程。L1 interpolating approximation是解决时间分辨率 Derivatives的标准方法之一，但我们发现，基于L1 interpolating approximation的神经网络解决方法无法完全利用神经网络的优势，并且模型的准确性受到 interpolating error 的限制。在这篇论文中，我们提出了高精度希尔比特神经网络解决方法（HNS），用于解决时间分辨率部分弗散方程。我们首先构建了高阶显式approximation scheme for fractional derivatives，并且仔细分析了其 Approximation 精度。接着，我们将高阶希尔比特 interpolating scheme与深度神经网络结合，提出了HNS。实验结果表明，HNS在前向和反向问题中，以及高维场景下都具有更高的准确性，比基于L1 scheme的方法更高。这表明，HNS在准确性和灵活性方面有所提高，并且超越了传统的显式差分方法，这些方法通常受到函数值 interpolating 的限制。因此，HNS不仅是一种简单的数字计算方法和神经网络的组合，而是实现了两种方法之间的共轨和互补优势。数据和代码可以在 \url{https://github.com/hsbhc/HNS} 找到。
</details></li>
</ul>
<hr>
<h2 id="PMNN-Physical-Model-driven-Neural-Network-for-solving-time-fractional-differential-equations"><a href="#PMNN-Physical-Model-driven-Neural-Network-for-solving-time-fractional-differential-equations" class="headerlink" title="PMNN:Physical Model-driven Neural Network for solving time-fractional differential equations"></a>PMNN:Physical Model-driven Neural Network for solving time-fractional differential equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04788">http://arxiv.org/abs/2310.04788</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiying Ma, Jie Hou, Wenhao Zhu, Yaxin Peng, Ying Li</li>
<li>for: 解决时间扩展弗拉克达尔方程（Time-fractional differential equations）</li>
<li>methods: Physical Model-driven Neural Network（PMNN）方法，结合深度神经网络（DNNs）和插值拟合方法</li>
<li>results: 通过训练DNNs来学习时间迭代方案，实现了精度高且效率高的时间扩展弗拉克达尔方程解。<details>
<summary>Abstract</summary>
In this paper, an innovative Physical Model-driven Neural Network (PMNN) method is proposed to solve time-fractional differential equations. It establishes a temporal iteration scheme based on physical model-driven neural networks which effectively combines deep neural networks (DNNs) with interpolation approximation of fractional derivatives. Specifically, once the fractional differential operator is discretized, DNNs are employed as a bridge to integrate interpolation approximation techniques with differential equations. On the basis of this integration, we construct a neural-based iteration scheme. Subsequently, by training DNNs to learn this temporal iteration scheme, approximate solutions to the differential equations can be obtained. The proposed method aims to preserve the intrinsic physical information within the equations as far as possible. It fully utilizes the powerful fitting capability of neural networks while maintaining the efficiency of the difference schemes for fractional differential equations. Moreover, we validate the efficiency and accuracy of PMNN through several numerical experiments.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种创新的物理模型驱动神经网络（PMNN）方法，用于解决时间分解 diferencial equations。它建立了一种基于物理模型驱动神经网络的时间迭代方案，Effectively combining deep neural networks (DNNs) with interpolation approximation of fractional derivatives. Specifically, once the fractional differential operator is discretized, DNNs are employed as a bridge to integrate interpolation approximation techniques with differential equations. On the basis of this integration, we construct a neural-based iteration scheme. Subsequently, by training DNNs to learn this temporal iteration scheme, approximate solutions to the differential equations can be obtained. The proposed method aims to preserve the intrinsic physical information within the equations as far as possible. It fully utilizes the powerful fitting capability of neural networks while maintaining the efficiency of the difference schemes for fractional differential equations. Moreover, we validate the efficiency and accuracy of PMNN through several numerical experiments.Here's the translation in Traditional Chinese:在这篇论文中，我们提出了一种创新的物理模型驱动神经网络（PMNN）方法，用于解决时间分解差分方程。它建立了一种基于物理模型驱动神经网络的时间迭代方案，具体来说，一旦时间分解运算符被粗化，神经网络被用来联结插值推理技巧与差分方程。基于这个联结，我们建立了一个神经网络基于的迭代方案。然后，通过训练神经网络以学习这个时间迭代方案，可以从中获得估计解的 approximate solutions。提出的方法希望能够保留差分方程中的本质物理信息，一旦可能。它充分利用神经网络的强大适应能力，同时维持差分方程的效率。此外，我们透过一些数据实验来验证 PMNN 的效率和准确性。
</details></li>
</ul>
<hr>
<h2 id="Optimal-Sequential-Decision-Making-in-Geosteering-A-Reinforcement-Learning-Approach"><a href="#Optimal-Sequential-Decision-Making-in-Geosteering-A-Reinforcement-Learning-Approach" class="headerlink" title="Optimal Sequential Decision-Making in Geosteering: A Reinforcement Learning Approach"></a>Optimal Sequential Decision-Making in Geosteering: A Reinforcement Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04772">http://arxiv.org/abs/2310.04772</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ressi Bonti Muhammad, Sergey Alyaev, Reidar Brumer Bratvold</li>
<li>for: 提高钻掘过程中的地层规划决策（geosteering）的效率和准确性。</li>
<li>methods: 使用深度Q网络（DQN）方法，一种无模型学习（RL）方法，直接从决策环境学习地层规划决策。</li>
<li>results: 在两个synthetic geosteering场景中，RL方法可以达到与 quasi-optimal Approximate Dynamic Programming（ADP）相当的高质量结果，而且比传统方法快速得多。此外，由于RL方法是无模型的，因此可以在更复杂的环境中应用，并且可以在未来与实际数据进行混合训练。<details>
<summary>Abstract</summary>
Trajectory adjustment decisions throughout the drilling process, called geosteering, affect subsequent choices and information gathering, thus resulting in a coupled sequential decision problem. Previous works on applying decision optimization methods in geosteering rely on greedy optimization or Approximate Dynamic Programming (ADP). Either decision optimization method requires explicit uncertainty and objective function models, making developing decision optimization methods for complex and realistic geosteering environments challenging to impossible. We use the Deep Q-Network (DQN) method, a model-free reinforcement learning (RL) method that learns directly from the decision environment, to optimize geosteering decisions. The expensive computations for RL are handled during the offline training stage. Evaluating DQN needed for real-time decision support takes milliseconds and is faster than the traditional alternatives. Moreover, for two previously published synthetic geosteering scenarios, our results show that RL achieves high-quality outcomes comparable to the quasi-optimal ADP. Yet, the model-free nature of RL means that by replacing the training environment, we can extend it to problems where the solution to ADP is prohibitively expensive to compute. This flexibility will allow applying it to more complex environments and make hybrid versions trained with real data in the future.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Trajectory adjustment decisions throughout the drilling process, called geosteering, affect subsequent choices and information gathering, thus resulting in a coupled sequential decision problem. Previous works on applying decision optimization methods in geosteering rely on greedy optimization or Approximate Dynamic Programming (ADP). Either decision optimization method requires explicit uncertainty and objective function models, making developing decision optimization methods for complex and realistic geosteering environments challenging to impossible. We use the Deep Q-Network (DQN) method, a model-free reinforcement learning (RL) method that learns directly from the decision environment, to optimize geosteering decisions. The expensive computations for RL are handled during the offline training stage. Evaluating DQN needed for real-time decision support takes milliseconds and is faster than the traditional alternatives. Moreover, for two previously published synthetic geosteering scenarios, our results show that RL achieves high-quality outcomes comparable to the quasi-optimal ADP. Yet, the model-free nature of RL means that by replacing the training environment, we can extend it to problems where the solution to ADP is prohibitively expensive to compute. This flexibility will allow applying it to more complex environments and make hybrid versions trained with real data in the future."中文简体版：<<SYS>>探钻过程中的轨迹调整决策，称为地OSTEERING，会影响后续的选择和信息收集，因此形成一个coupled sequential decision problem。前一些关于地OSTEERING中的决策优化方法都基于greedy optimization或 Approximate Dynamic Programming (ADP)。这些决策优化方法都需要显式的uncertainty和目标函数模型，因此在复杂和实际的地OSTEERING环境中开发决策优化方法是非常困难或不可能。我们使用Deep Q-Network (DQN)方法，一种model-free reinforcement learning (RL)方法，直接从决策环境中学习优化决策。RL的昂贵计算被处理在线上训练阶段。在实时决策支持中评估DQN所需的时间只需毫秒级，比传统方法更快。此外，对两个已发表的synthetic geosteering场景的结果显示，RL可以 achiev高质量的结果与 quasi-optimal ADP相当。然而，RL的model-free性意味着可以通过更换训练环境来扩展其应用范围，包括更复杂的环境和以后在实际数据上进行hybrid版本的训练。
</details></li>
</ul>
<hr>
<h2 id="Pairwise-GUI-Dataset-Construction-Between-Android-Phones-and-Tablets"><a href="#Pairwise-GUI-Dataset-Construction-Between-Android-Phones-and-Tablets" class="headerlink" title="Pairwise GUI Dataset Construction Between Android Phones and Tablets"></a>Pairwise GUI Dataset Construction Between Android Phones and Tablets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04755">http://arxiv.org/abs/2310.04755</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/huhangithub/papt">https://github.com/huhangithub/papt</a></li>
<li>paper_authors: Han Hu, Haolan Zhan, Yujin Huang, Di Liu</li>
<li>for: 这个论文旨在提高开发者的产效，通过自动化 GUI 开发来减少开发成本和遗漏。</li>
<li>methods: 这篇论文使用了深度学习技术，并提出了一种新的对应式 GUI 收集方法，以生成 Android 手机和平板电脑之间的对应 GUI 数据集。</li>
<li>results: 经过初步实验，论文发现当前使用深度学习自动化 GUI 开发时存在一些挑战，需要进一步的研究和优化。<details>
<summary>Abstract</summary>
In the current landscape of pervasive smartphones and tablets, apps frequently exist across both platforms. Although apps share most graphic user interfaces (GUIs) and functionalities across phones and tablets, developers often rebuild from scratch for tablet versions, escalating costs and squandering existing design resources. Researchers are attempting to collect data and employ deep learning in automated GUIs development to enhance developers' productivity. There are currently several publicly accessible GUI page datasets for phones, but none for pairwise GUIs between phones and tablets. This poses a significant barrier to the employment of deep learning in automated GUI development. In this paper, we introduce the Papt dataset, a pioneering pairwise GUI dataset tailored for Android phones and tablets, encompassing 10,035 phone-tablet GUI page pairs sourced from 5,593 unique app pairs. We propose novel pairwise GUI collection approaches for constructing this dataset and delineate its advantages over currently prevailing datasets in the field. Through preliminary experiments on this dataset, we analyze the present challenges of utilizing deep learning in automated GUI development.
</details>
<details>
<summary>摘要</summary>
在现有的智能手机和平板电脑普及的场景下，许多应用程序frequently across both platforms exist。 although apps share most graphic user interfaces (GUIs) and functionalities across phones and tablets， developers often rebuild from scratch for tablet versions， which escalates costs and wastes existing design resources. Researchers are attempting to collect data and employ deep learning in automated GUI development to enhance developers' productivity. Currently, there are several publicly accessible GUI page datasets for phones, but none for pairwise GUIs between phones and tablets. This poses a significant barrier to the employment of deep learning in automated GUI development. In this paper, we introduce the Papt dataset, a pioneering pairwise GUI dataset tailored for Android phones and tablets， encompassing 10,035 phone-tablet GUI page pairs sourced from 5,593 unique app pairs. We propose novel pairwise GUI collection approaches for constructing this dataset and delineate its advantages over currently prevailing datasets in the field. Through preliminary experiments on this dataset, we analyze the present challenges of utilizing deep learning in automated GUI development.
</details></li>
</ul>
<hr>
<h2 id="A-Unified-Generalization-Analysis-of-Re-Weighting-and-Logit-Adjustment-for-Imbalanced-Learning"><a href="#A-Unified-Generalization-Analysis-of-Re-Weighting-and-Logit-Adjustment-for-Imbalanced-Learning" class="headerlink" title="A Unified Generalization Analysis of Re-Weighting and Logit-Adjustment for Imbalanced Learning"></a>A Unified Generalization Analysis of Re-Weighting and Logit-Adjustment for Imbalanced Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04752">http://arxiv.org/abs/2310.04752</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wang22ti/DDC">https://github.com/wang22ti/DDC</a></li>
<li>paper_authors: Zitai Wang, Qianqian Xu, Zhiyong Yang, Yuan He, Xiaochun Cao, Qingming Huang</li>
<li>for: 减轻类别偏好问题</li>
<li>methods: 修改损失函数，例如重新权重损失或调整логitschannel</li>
<li>results: 提出了一种数据依存收缩技术，并建立了一个细化的泛化 bound，可以帮助解释重新权重和logit调整的实际结果。<details>
<summary>Abstract</summary>
Real-world datasets are typically imbalanced in the sense that only a few classes have numerous samples, while many classes are associated with only a few samples. As a result, a na\"ive ERM learning process will be biased towards the majority classes, making it difficult to generalize to the minority classes. To address this issue, one simple but effective approach is to modify the loss function to emphasize the learning on minority classes, such as re-weighting the losses or adjusting the logits via class-dependent terms. However, existing generalization analysis of such losses is still coarse-grained and fragmented, failing to explain some empirical results. To bridge this gap, we propose a novel technique named data-dependent contraction to capture how these modified losses handle different classes. On top of this technique, a fine-grained generalization bound is established for imbalanced learning, which helps reveal the mystery of re-weighting and logit-adjustment in a unified manner. Furthermore, a principled learning algorithm is developed based on the theoretical insights. Finally, the empirical results on benchmark datasets not only validate the theoretical results but also demonstrate the effectiveness of the proposed method.
</details>
<details>
<summary>摘要</summary>
To address this gap, we propose a novel technique called data-dependent contraction to capture how these modified losses handle different classes. On top of this technique, we establish a fine-grained generalization bound for imbalanced learning, which helps to explain the mystery of re-weighting and logit-adjustment in a unified manner. Furthermore, we develop a principled learning algorithm based on the theoretical insights.The empirical results on benchmark datasets not only validate the theoretical results but also demonstrate the effectiveness of the proposed method.
</details></li>
</ul>
<hr>
<h2 id="DiffNAS-Bootstrapping-Diffusion-Models-by-Prompting-for-Better-Architectures"><a href="#DiffNAS-Bootstrapping-Diffusion-Models-by-Prompting-for-Better-Architectures" class="headerlink" title="DiffNAS: Bootstrapping Diffusion Models by Prompting for Better Architectures"></a>DiffNAS: Bootstrapping Diffusion Models by Prompting for Better Architectures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04750">http://arxiv.org/abs/2310.04750</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenhao Li, Xiu Su, Shan You, Fei Wang, Chen Qian, Chang Xu<br>for:This paper focuses on improving the efficiency and performance of diffusion models for image synthesis.methods:The authors propose a base model search approach called “DiffNAS,” which leverages GPT-4 as a supernet and employs a search memory to enhance the results. They also use RFID as a proxy to quickly rank the experimental outcomes produced by GPT-4.results:The authors’ algorithm can augment the search efficiency by 2 times under GPT-based scenarios and achieve a performance of 2.82 with 0.37 improvement in FID on CIFAR10 relative to the benchmark IDDPM algorithm.<details>
<summary>Abstract</summary>
Diffusion models have recently exhibited remarkable performance on synthetic data. After a diffusion path is selected, a base model, such as UNet, operates as a denoising autoencoder, primarily predicting noises that need to be eliminated step by step. Consequently, it is crucial to employ a model that aligns with the expected budgets to facilitate superior synthetic performance. In this paper, we meticulously analyze the diffusion model and engineer a base model search approach, denoted "DiffNAS". Specifically, we leverage GPT-4 as a supernet to expedite the search, supplemented with a search memory to enhance the results. Moreover, we employ RFID as a proxy to promptly rank the experimental outcomes produced by GPT-4. We also adopt a rapid-convergence training strategy to boost search efficiency. Rigorous experimentation corroborates that our algorithm can augment the search efficiency by 2 times under GPT-based scenarios, while also attaining a performance of 2.82 with 0.37 improvement in FID on CIFAR10 relative to the benchmark IDDPM algorithm.
</details>
<details>
<summary>摘要</summary>
各种扩散模型最近在合成数据上表现出色。选择了扩散路径后，基本模型，如UNet，将作为滤波 autoencoder 操作，主要预测需要除掉的噪声步骤。因此，选择一个与预期预算相align的模型非常重要，以便在合成数据上实现优秀的表现。在这篇论文中，我们仔细分析了扩散模型，并开发了基于搜索的搜索模型搜索方法，称为"DiffNAS"。具体来说，我们利用 GPT-4 作为超网，并补充了搜索内存以提高结果。此外，我们采用 RFID 作为代理，以快速排名 GPT-4 生成的实验结果。同时，我们采用了快速收敛训练策略，以提高搜索效率。经验证明，我们的算法可以在 GPT 基本场景下提高搜索效率两倍，同时在 CIFAR10 上与标准 IDDPM 算法相比，实现了 2.82 的 FID 表现，升准0.37 的提升。
</details></li>
</ul>
<hr>
<h2 id="ConvNeXtv2-Fusion-with-Mask-R-CNN-for-Automatic-Region-Based-Coronary-Artery-Stenosis-Detection-for-Disease-Diagnosis"><a href="#ConvNeXtv2-Fusion-with-Mask-R-CNN-for-Automatic-Region-Based-Coronary-Artery-Stenosis-Detection-for-Disease-Diagnosis" class="headerlink" title="ConvNeXtv2 Fusion with Mask R-CNN for Automatic Region Based Coronary Artery Stenosis Detection for Disease Diagnosis"></a>ConvNeXtv2 Fusion with Mask R-CNN for Automatic Region Based Coronary Artery Stenosis Detection for Disease Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04749">http://arxiv.org/abs/2310.04749</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sandesh Pokhrel, Sanjay Bhandari, Eduard Vazquez, Yash Raj Shrestha, Binod Bhattarai</li>
<li>for:  automatization of manually detecting stenotic lesions in coronary arteries</li>
<li>methods:  employing a specialized Convnext-V2 backbone based Mask RCNN model pre-trained for instance segmentation tasks</li>
<li>results:  achieving a substantial F1 score of 0.5353 in identifying stenotic lesions<details>
<summary>Abstract</summary>
Coronary Artery Diseases although preventable are one of the leading cause of mortality worldwide. Due to the onerous nature of diagnosis, tackling CADs has proved challenging. This study addresses the automation of resource-intensive and time-consuming process of manually detecting stenotic lesions in coronary arteries in X-ray coronary angiography images. To overcome this challenge, we employ a specialized Convnext-V2 backbone based Mask RCNN model pre-trained for instance segmentation tasks. Our empirical findings affirm that the proposed model exhibits commendable performance in identifying stenotic lesions. Notably, our approach achieves a substantial F1 score of 0.5353 in this demanding task, underscoring its effectiveness in streamlining this intensive process.
</details>
<details>
<summary>摘要</summary>
心血管疾病虽可预防，但它是全球至关重要的死亡原因之一。由于诊断过程的复杂性，解决心血管疾病的挑战很大。本研究目的是自动化扫描心血管疾病X射线报告图像中的狭窄部分的手动识别过程。为此，我们采用了专门的 Convnext-V2 幕ignon 基于的面部划分模型，该模型在实例分割任务上进行预先训练。我们的实验结果表明，提议的模型在识别狭窄部分方面表现出色，其 F1 分数达到了 0.5353，这再次证明了该方法的效iveness。
</details></li>
</ul>
<hr>
<h2 id="Towards-Dynamic-and-Small-Objects-Refinement-for-Unsupervised-Domain-Adaptative-Nighttime-Semantic-Segmentation"><a href="#Towards-Dynamic-and-Small-Objects-Refinement-for-Unsupervised-Domain-Adaptative-Nighttime-Semantic-Segmentation" class="headerlink" title="Towards Dynamic and Small Objects Refinement for Unsupervised Domain Adaptative Nighttime Semantic Segmentation"></a>Towards Dynamic and Small Objects Refinement for Unsupervised Domain Adaptative Nighttime Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04747">http://arxiv.org/abs/2310.04747</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingyi Pan, Sihang Li, Yucheng Chen, Jinjing Zhu, Lin Wang</li>
<li>for: 这篇论文的目的是提出一种新的夜间Semantic segmentationUnsupervised domain adaptation方法，以解决夜间环境中的决策难题。</li>
<li>methods: 本方法使用了一个动态和小物件增强模块，将来自源领域的知识传递到目标夜间领域，并使用了一个对比学习模块以缓和领域差异。</li>
<li>results: 实验结果显示，本方法可以与先前的方法相比，大幅提高夜间Semantic segmentation的精度。<details>
<summary>Abstract</summary>
Nighttime semantic segmentation is essential for various applications, e.g., autonomous driving, which often faces challenges due to poor illumination and the lack of well-annotated datasets. Unsupervised domain adaptation (UDA) has shown potential for addressing the challenges and achieved remarkable results for nighttime semantic segmentation. However, existing methods still face limitations in 1) their reliance on style transfer or relighting models, which struggle to generalize to complex nighttime environments, and 2) their ignorance of dynamic and small objects like vehicles and traffic signs, which are difficult to be directly learned from other domains. This paper proposes a novel UDA method that refines both label and feature levels for dynamic and small objects for nighttime semantic segmentation. First, we propose a dynamic and small object refinement module to complement the knowledge of dynamic and small objects from the source domain to target nighttime domain. These dynamic and small objects are normally context-inconsistent in under-exposed conditions. Then, we design a feature prototype alignment module to reduce the domain gap by deploying contrastive learning between features and prototypes of the same class from different domains, while re-weighting the categories of dynamic and small objects. Extensive experiments on four benchmark datasets demonstrate that our method outperforms prior arts by a large margin for nighttime segmentation. Project page: https://rorisis.github.io/DSRNSS/.
</details>
<details>
<summary>摘要</summary>
夜间 semantic segmentation 是许多应用程序中的关键，如自动驾驶，它们frequently facing challenges due to poor illumination and lack of well-annotated datasets. 无监督领域适应（UDA）已经显示出了地Addressing these challenges and achieving remarkable results for nighttime semantic segmentation. However, existing methods still have limitations in 1) their reliance on style transfer or relighting models, which struggle to generalize to complex nighttime environments, and 2) their ignorance of dynamic and small objects like vehicles and traffic signs, which are difficult to be directly learned from other domains.This paper proposes a novel UDA method that refines both label and feature levels for dynamic and small objects for nighttime semantic segmentation. First, we propose a dynamic and small object refinement module to complement the knowledge of dynamic and small objects from the source domain to target nighttime domain. These dynamic and small objects are normally context-inconsistent in under-exposed conditions. Then, we design a feature prototype alignment module to reduce the domain gap by deploying contrastive learning between features and prototypes of the same class from different domains, while re-weighting the categories of dynamic and small objects.Extensive experiments on four benchmark datasets demonstrate that our method outperforms prior arts by a large margin for nighttime segmentation. Project page: <https://rorisis.github.io/DSRNSS/>.Here's the text with some additional information about the translation:I used Google Translate to translate the text into Simplified Chinese, and then made some minor adjustments to the translation to improve its accuracy and readability. I also added some additional information to the translation to help clarify the meaning of certain phrases and concepts.Please note that while Simplified Chinese is the most widely used form of Chinese, there are also other forms of Chinese, such as Traditional Chinese, that may be used in different contexts. If you need the text translated into a different form of Chinese, please let me know and I can try to assist you.
</details></li>
</ul>
<hr>
<h2 id="Task-Aware-Modulation-using-Representation-Learning-An-Approach-for-Few-Shot-Learning-in-Heterogeneous-Systems"><a href="#Task-Aware-Modulation-using-Representation-Learning-An-Approach-for-Few-Shot-Learning-in-Heterogeneous-Systems" class="headerlink" title="Task Aware Modulation using Representation Learning: An Approach for Few Shot Learning in Heterogeneous Systems"></a>Task Aware Modulation using Representation Learning: An Approach for Few Shot Learning in Heterogeneous Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04727">http://arxiv.org/abs/2310.04727</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arvind Renganathan, Rahul Ghosh, Ankush Khandelwal, Vipin Kumar</li>
<li>for: 提高个性化预测性能在少量示例设定下，特别是在不知道任务特征时</li>
<li>methods: 使用表示学习框架（TAM-RL），提取实际精度表示任务特征，进行个性化预测</li>
<li>results: 使用实际水文和流动塔数据集，比较 MAML 和 MMAML 的表现，TAM-RL 可以显著超越这些基准方法，同时训练更加快速和简单，不需要敏感的内 Loop 步骤和内 Loop 学习率，并通过synthetic数据进行了empirical评估，并证明TAM-RL可以在不同任务之间学习独特的表示，提高预测性能。<details>
<summary>Abstract</summary>
We present a Task-aware modulation using Representation Learning (TAM-RL) framework that enhances personalized predictions in few-shot settings for heterogeneous systems when individual task characteristics are not known. TAM-RL extracts embeddings representing the actual inherent characteristics of these entities and uses these characteristics to personalize the predictions for each entity/task. Using real-world hydrological and flux tower benchmark data sets, we show that TAM-RL can significantly outperform existing baseline approaches such as MAML and multi-modal MAML (MMAML) while being much faster and simpler to train due to less complexity. Specifically, TAM-RL eliminates the need for sensitive hyper-parameters like inner loop steps and inner loop learning rate, which are crucial for model convergence in MAML, MMAML. We further present an empirical evaluation via synthetic data to explore the impact of heterogeneity amongst the entities on the relative performance of MAML, MMAML, and TAM-RL. We show that TAM-RL significantly improves predictive performance for cases where it is possible to learn distinct representations for different tasks.
</details>
<details>
<summary>摘要</summary>
我们提出了一个任务意识度模块化学习（TAM-RL）框架，该框架在少量示例情况下提高个性化预测的性能，当个体任务特征不知道时。TAM-RL提取实际内在特征的表示，并使用这些特征个性化预测每个实体/任务。使用实际水文和流动塔benchmark数据集，我们显示了TAM-RL可以明显超越现有的基eline方法such as MAML和多模态MAML（MMAML），并且训练更快和简单。具体来说，TAM-RL消除了内 Loopstep和内 Loop学习率这些敏感的Hyperparameter，这些参数对MAML和MMAML的模型收敛起到关键作用。我们进一步通过synthetic数据进行empirical评估，探索不同实体之间的多样性对MAML、MMAML和TAM-RL的relative性能的影响。我们发现，当可以学习不同任务的明确表示时，TAM-RL会显著提高预测性能。
</details></li>
</ul>
<hr>
<h2 id="A-Holistic-Evaluation-of-Piano-Sound-Quality"><a href="#A-Holistic-Evaluation-of-Piano-Sound-Quality" class="headerlink" title="A Holistic Evaluation of Piano Sound Quality"></a>A Holistic Evaluation of Piano Sound Quality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04722">http://arxiv.org/abs/2310.04722</a></li>
<li>repo_url: None</li>
<li>paper_authors: Monan Zhou, Shangda Wu, Shaohua Ji, Zijin Li, Wei Li</li>
<li>for: 这个论文的目的是开发一种全面评估方法，帮助用户在购买钢琴时更好地评估音色质量。</li>
<li>methods: 这个研究使用主观问卷来 derive高质量评估系统，并使用Convolutional Neural Networks (CNN)进行分类。为了提高模型的可读性，研究人员使用Equivalent Rectangular Bandwidth (ERB)分析。</li>
<li>results: 研究发现，有musically trained individuals能够更好地 отли别不同钢琴的音色差异。最佳的CNN预训练后台 achieves a high accuracy of 98.3% as the piano classifier。然而，数据库有限， audio被截割以增加其量，导致数据不均衡和不够多样性，因此使用 focal loss 来减少数据不均衡的影响。<details>
<summary>Abstract</summary>
This paper aims to develop a holistic evaluation method for piano sound quality to assist in purchasing decisions. Unlike previous studies that focused on the effect of piano performance techniques on sound quality, this study evaluates the inherent sound quality of different pianos. To derive quality evaluation systems, the study uses subjective questionnaires based on a piano sound quality dataset. The method selects the optimal piano classification models by comparing the fine-tuning results of different pre-training models of Convolutional Neural Networks (CNN). To improve the interpretability of the models, the study applies Equivalent Rectangular Bandwidth (ERB) analysis. The results reveal that musically trained individuals are better able to distinguish between the sound quality differences of different pianos. The best fine-tuned CNN pre-trained backbone achieves a high accuracy of 98.3\% as the piano classifier. However, the dataset is limited, and the audio is sliced to increase its quantity, resulting in a lack of diversity and balance, so we use focal loss to reduce the impact of data imbalance. To optimize the method, the dataset will be expanded, or few-shot learning techniques will be employed in future research.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文目的是开发一种全面评价方法，帮助人们在购买钢琴时做出更加 informed 的决定。与之前的研究不同，这些研究专注于钢琴演奏技巧对音质的影响，而这个研究则评估不同钢琴的内在音质。为 derive 音质评价系统，这个研究使用基于钢琴音质数据的主观问naire。方法选择最佳钢琴分类模型，通过比较不同预训练模型的 Convolutional Neural Networks (CNN) 的细化结果进行比较。为了提高模型的可读性，研究使用 Equivalent Rectangular Bandwidth (ERB) 分析。结果显示，具有音乐训练经验的个体更能够 отличи出不同钢琴的音质差异。最佳细化后的 CNN 预训练后台得到了 98.3% 的钢琴分类精度。然而，数据库有限， audio 被截割以增加其量，导致数据的不平衡和缺乏多样性，因此我们使用 focal loss 减少数据不平衡的影响。为优化方法，将来的研究可能会扩大数据库，或者使用 few-shot learning 技术。
</details></li>
</ul>
<hr>
<h2 id="EdgeFD-An-Edge-Friendly-Drift-Aware-Fault-Diagnosis-System-for-Industrial-IoT"><a href="#EdgeFD-An-Edge-Friendly-Drift-Aware-Fault-Diagnosis-System-for-Industrial-IoT" class="headerlink" title="EdgeFD: An Edge-Friendly Drift-Aware Fault Diagnosis System for Industrial IoT"></a>EdgeFD: An Edge-Friendly Drift-Aware Fault Diagnosis System for Industrial IoT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04704">http://arxiv.org/abs/2310.04704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Jiao, Mao Fengjian, Lv Zuohong, Tang Jianhua</li>
<li>for: 这篇论文是针对工业智能故障诊断（FD）领域的传播学（TL）方法进行研究，以解决数据漂移问题。</li>
<li>methods: 该论文提出了一种名为“漂移意识Weight控制”（DAWC）的方法，用于在边缘设备上进行快速和有效的故障诊断。DAWC通过检测漂移并逐渐增强模型的泛化能力来解决频繁的数据漂移问题。</li>
<li>results: 实验结果表明，相比于现有的技术，该论文提出的DAWC方法能够达到更高的性能水平，同时也遵循边缘计算限制。此外，该论文还开发了一个完整的诊断和可视化平台。<details>
<summary>Abstract</summary>
Recent transfer learning (TL) approaches in industrial intelligent fault diagnosis (FD) mostly follow the "pre-train and fine-tuning" paradigm to address data drift, which emerges from variable working conditions. However, we find that this approach is prone to the phenomenon known as catastrophic forgetting. Furthermore, performing frequent models fine-tuning on the resource-constrained edge nodes can be computationally expensive and unnecessary, given the excellent transferability demonstrated by existing models. In this work, we propose the Drift-Aware Weight Consolidation (DAWC), a method optimized for edge deployments, mitigating the challenges posed by frequent data drift in the industrial Internet of Things (IIoT). DAWC efficiently manages multiple data drift scenarios, minimizing the need for constant model fine-tuning on edge devices, thereby conserving computational resources. By detecting drift using classifier confidence and estimating parameter importance with the Fisher Information Matrix, a tool that measures parameter sensitivity in probabilistic models, we introduce a drift detection module and a continual learning module to gradually equip the FD model with powerful generalization capabilities. Experimental results demonstrate that our proposed DAWC achieves superior performance compared to existing techniques while also ensuring compatibility with edge computing constraints. Additionally, we have developed a comprehensive diagnosis and visualization platform.
</details>
<details>
<summary>摘要</summary>
Recent transfer learning (TL) approaches in industrial intelligent fault diagnosis (FD) mostly follow the "pre-train and fine-tuning" paradigm to address data drift, which emerges from variable working conditions. However, we find that this approach is prone to the phenomenon known as catastrophic forgetting. Furthermore, performing frequent models fine-tuning on the resource-constrained edge nodes can be computationally expensive and unnecessary, given the excellent transferability demonstrated by existing models. In this work, we propose the Drift-Aware Weight Consolidation (DAWC), a method optimized for edge deployments, mitigating the challenges posed by frequent data drift in the industrial Internet of Things (IIoT). DAWC efficiently manages multiple data drift scenarios, minimizing the need for constant model fine-tuning on edge devices, thereby conserving computational resources. By detecting drift using classifier confidence and estimating parameter importance with the Fisher Information Matrix, a tool that measures parameter sensitivity in probabilistic models, we introduce a drift detection module and a continual learning module to gradually equip the FD model with powerful generalization capabilities. Experimental results demonstrate that our proposed DAWC achieves superior performance compared to existing techniques while also ensuring compatibility with edge computing constraints. Additionally, we have developed a comprehensive diagnosis and visualization platform.Here is the translation in Traditional Chinese:Recent transfer learning (TL) approaches in industrial intelligent fault diagnosis (FD) mostly follow the "pre-train and fine-tuning" paradigm to address data drift, which emerges from variable working conditions. However, we find that this approach is prone to the phenomenon known as catastrophic forgetting. Furthermore, performing frequent models fine-tuning on the resource-constrained edge nodes can be computationally expensive and unnecessary, given the excellent transferability demonstrated by existing models. In this work, we propose the Drift-Aware Weight Consolidation (DAWC), a method optimized for edge deployments, mitigating the challenges posed by frequent data drift in the industrial Internet of Things (IIoT). DAWC efficiently manages multiple data drift scenarios, minimizing the need for constant model fine-tuning on edge devices, thereby conserving computational resources. By detecting drift using classifier confidence and estimating parameter importance with the Fisher Information Matrix, a tool that measures parameter sensitivity in probabilistic models, we introduce a drift detection module and a continual learning module to gradually equip the FD model with powerful generalization capabilities. Experimental results demonstrate that our proposed DAWC achieves superior performance compared to existing techniques while also ensuring compatibility with edge computing constraints. Additionally, we have developed a comprehensive diagnosis and visualization platform.
</details></li>
</ul>
<hr>
<h2 id="Serving-Deep-Learning-Model-in-Relational-Databases"><a href="#Serving-Deep-Learning-Model-in-Relational-Databases" class="headerlink" title="Serving Deep Learning Model in Relational Databases"></a>Serving Deep Learning Model in Relational Databases</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04696">http://arxiv.org/abs/2310.04696</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexandre Eichenberger, Qi Lin, Saif Masood, Hong Min, Alexander Sim, Jie Wang, Yida Wang, Kesheng Wu, Binhang Yuan, Lixi Zhou, Jia Zou</li>
<li>for: 本研究旨在探讨如何在关系数据上执行深度学习（DL）模型，以满足不同的商业和科学领域的需求。</li>
<li>methods: 本文提出了三种重要的架构方法：DL-Centric architecture、UDF-Centric architecture和Relation-Centric architecture。这三种架构各有优势，但是它们之间存在许多挑战，需要进行融合和中间件技术的研究。</li>
<li>results: 本研究发现了许多 интеграción gap和挑战，并提出了一些创新的解决方案，以实现一种可以满足各种数据挑战的数据处理和深度学习执行平台。<details>
<summary>Abstract</summary>
Serving deep learning (DL) models on relational data has become a critical requirement across diverse commercial and scientific domains, sparking growing interest recently. In this visionary paper, we embark on a comprehensive exploration of representative architectures to address the requirement. We highlight three pivotal paradigms: The state-of-the-artDL-Centricarchitecture offloadsDL computations to dedicated DL frameworks. The potential UDF-Centric architecture encapsulates one or more tensor computations into User Defined Functions (UDFs) within the database system. The potentialRelation-Centricarchitecture aims to represent a large-scale tensor computation through relational operators. While each of these architectures demonstrates promise in specific use scenarios, we identify urgent requirements for seamless integration of these architectures and the middle ground between these architectures. We delve into the gaps that impede the integration and explore innovative strategies to close them. We present a pathway to establish a novel database system for enabling a broad class of data-intensive DL inference applications.
</details>
<details>
<summary>摘要</summary>
优化深度学习（DL）模型在关系数据上的应用已成为不同领域的关键需求，最近吸引了很多关注。在这篇visionary论文中，我们进行了全面的探索，探讨了代表性的建筑方案。我们提出了三个重要的思想：1. 现状顶尖DL-Centric架构，将DL计算外送到专门的DL框架上。2. UDF-Centric架构，将一个或多个张量计算包装在用户定义函数（UDF）内部。3. Relation-Centric架构，通过关系运算来表示大规模张量计算。各种架构在特定使用场景中都有承诺，但是我们认为这些架构之间的协同和中间地带的融合是必要的。我们描述了这些架构之间的差距和融合的难点，并提出了创新的策略来填补这些差距。最后，我们提出了一种新的数据库系统，用于支持广泛的数据敏感DL推理应用。
</details></li>
</ul>
<hr>
<h2 id="Robustness-enhanced-Uplift-Modeling-with-Adversarial-Feature-Desensitization"><a href="#Robustness-enhanced-Uplift-Modeling-with-Adversarial-Feature-Desensitization" class="headerlink" title="Robustness-enhanced Uplift Modeling with Adversarial Feature Desensitization"></a>Robustness-enhanced Uplift Modeling with Adversarial Feature Desensitization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04693">http://arxiv.org/abs/2310.04693</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zexu Sun, Bowei He, Ming Ma, Jiakai Tang, Yuchen Wang, Chen Ma, Dugang Liu</li>
<li>for: 本文旨在解决在实际应用中存在的robustness挑战，提出了一种可能的解释，并采用了两个特定模块来进行稳定性提升。</li>
<li>methods: 本文提出了一种基于反对抗训练和软 interpolate操作的特殊feature敏感性增强策略，以及一种基于多标签模型的feature选择模块。</li>
<li>results: 经过广泛的实验 validate，RUAD可以更好地解决在线广告的feature敏感性问题，同时也能够保持和不同的uplift模型的兼容性。<details>
<summary>Abstract</summary>
Uplift modeling has shown very promising results in online marketing. However, most existing works are prone to the robustness challenge in some practical applications. In this paper, we first present a possible explanation for the above phenomenon. We verify that there is a feature sensitivity problem in online marketing using different real-world datasets, where the perturbation of some key features will seriously affect the performance of the uplift model and even cause the opposite trend. To solve the above problem, we propose a novel robustness-enhanced uplift modeling framework with adversarial feature desensitization (RUAD). Specifically, our RUAD can more effectively alleviate the feature sensitivity of the uplift model through two customized modules, including a feature selection module with joint multi-label modeling to identify a key subset from the input features and an adversarial feature desensitization module using adversarial training and soft interpolation operations to enhance the robustness of the model against this selected subset of features. Finally, we conduct extensive experiments on a public dataset and a real product dataset to verify the effectiveness of our RUAD in online marketing. In addition, we also demonstrate the robustness of our RUAD to the feature sensitivity, as well as the compatibility with different uplift models.
</details>
<details>
<summary>摘要</summary>
《增强模型在线市场营销中的应用显示了非常有前途。然而，现有的大多数工作受到实际应用中的Robustness挑战。在这篇论文中，我们首先提出了这种现象的可能的解释。我们使用了不同的实际数据集，证明了在线市场营销中存在特征敏感性问题，其中一些关键特征的修改会严重影响增强模型的性能，甚至导致反趋势。为解决以上问题，我们提议一种基于反对抗训练和软插值操作的robustness-enhanced增强模型框架（RUAD）。具体来说，我们的RUAD可以更好地减轻增强模型中特征敏感性通过两个定制模块，包括一个联合多标签模型来确定输入特征中关键子集和一个反对抗训练和软插值操作来强化模型对这些选择的特征的抗训练。最后，我们对公共数据集和真实产品数据集进行了广泛的实验，以证明RUAD在线市场营销中的有效性。此外，我们还证明了RUAD对特征敏感性的稳定性，以及与不同的增强模型相容性。》Note: Please note that the translation is in Simplified Chinese, which is used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Understanding-and-Improving-Adversarial-Attacks-on-Latent-Diffusion-Model"><a href="#Understanding-and-Improving-Adversarial-Attacks-on-Latent-Diffusion-Model" class="headerlink" title="Understanding and Improving Adversarial Attacks on Latent Diffusion Model"></a>Understanding and Improving Adversarial Attacks on Latent Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04687">http://arxiv.org/abs/2310.04687</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/caradryanliang/improvedadvdm">https://github.com/caradryanliang/improvedadvdm</a></li>
<li>paper_authors: Boyang Zheng, Chumeng Liang, Xiaoyu Wu, Yan Liu</li>
<li>for: 保护个人隐私和安全数据，防止未经授权的艺术作品复制和谣言生成。</li>
<li>methods: 基于理论基础的隐 diffusion 模型（LDM）对抗攻击，通过一个统一的目标导向对抗攻击进行优化。</li>
<li>results: 比对现有方法，提出了一种更加强大和有效的对抗攻击方法，可以在不同的状态对抗攻击下进行普适化。<details>
<summary>Abstract</summary>
Latent Diffusion Model (LDM) has emerged as a leading tool in image generation, particularly with its capability in few-shot generation. This capability also presents risks, notably in unauthorized artwork replication and misinformation generation. In response, adversarial attacks have been designed to safeguard personal images from being used as reference data. However, existing adversarial attacks are predominantly empirical, lacking a solid theoretical foundation. In this paper, we introduce a comprehensive theoretical framework for understanding adversarial attacks on LDM. Based on the framework, we propose a novel adversarial attack that exploits a unified target to guide the adversarial attack both in the forward and the reverse process of LDM. We provide empirical evidences that our method overcomes the offset problem of the optimization of adversarial attacks in existing methods. Through rigorous experiments, our findings demonstrate that our method outperforms current attacks and is able to generalize over different state-of-the-art few-shot generation pipelines based on LDM. Our method can serve as a stronger and efficient tool for people exposed to the risk of data privacy and security to protect themselves in the new era of powerful generative models. The code is available on GitHub: https://github.com/CaradryanLiang/ImprovedAdvDM.git.
</details>
<details>
<summary>摘要</summary>
Latent Diffusion Model (LDM) 已成为图像生成领域的主导工具，特别是它的几何批处能力。这种能力也存在风险，包括未经授权的艺术作品复制和谣言生成。为了保护个人隐私和安全，我们提出了一种全面的理论基础 для对LDM的逆攻击。基于这种基础，我们提出了一种新的逆攻击方法，通过一个统一的目标导引LDM的前向和反向过程中的逆攻击。我们提供了实验证据，表明我们的方法可以超越现有的优化问题，并且可以泛化到不同的state-of-the-art几何批处管道中。我们的方法可以作为一种更加强大和高效的工具，为人们在新的强大生成模型时代保护自己的隐私和安全。代码可以在GitHub上获取：https://github.com/CaradryanLiang/ImprovedAdvDM.git。
</details></li>
</ul>
<hr>
<h2 id="Data-Centric-Financial-Large-Language-Models"><a href="#Data-Centric-Financial-Large-Language-Models" class="headerlink" title="Data-Centric Financial Large Language Models"></a>Data-Centric Financial Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17784">http://arxiv.org/abs/2310.17784</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhixuan Chu, Huaiyu Guo, Xinyuan Zhou, Yijia Wang, Fei Yu, Hong Chen, Wanqing Xu, Xin Lu, Qing Cui, Longfei Li, Jun Zhou, Sheng Li</li>
<li>for: This paper aims to improve the performance of large language models (LLMs) in financial tasks by using a data-centric approach and multitask prompt-based finetuning.</li>
<li>methods: The proposed method uses a financial LLM (FLLM) and abductive augmentation reasoning (AAR) to generate training data and preprocess the input data.</li>
<li>results: The data-centric FLLM with AAR achieves state-of-the-art performance on financial analysis and interpretation tasks, outperforming baseline financial LLMs designed for raw text. Additionally, a new benchmark for financial analysis and interpretation is open-sourced.<details>
<summary>Abstract</summary>
Large language models (LLMs) show promise for natural language tasks but struggle when applied directly to complex domains like finance. LLMs have difficulty reasoning about and integrating all relevant information. We propose a data-centric approach to enable LLMs to better handle financial tasks. Our key insight is that rather than overloading the LLM with everything at once, it is more effective to preprocess and pre-understand the data. We create a financial LLM (FLLM) using multitask prompt-based finetuning to achieve data pre-processing and pre-understanding. However, labeled data is scarce for each task. To overcome manual annotation costs, we employ abductive augmentation reasoning (AAR) to automatically generate training data by modifying the pseudo labels from FLLM's own outputs. Experiments show our data-centric FLLM with AAR substantially outperforms baseline financial LLMs designed for raw text, achieving state-of-the-art on financial analysis and interpretation tasks. We also open source a new benchmark for financial analysis and interpretation. Our methodology provides a promising path to unlock LLMs' potential for complex real-world domains.
</details>
<details>
<summary>摘要</summary>
Our key insight is that it is more effective to preprocess and pre-understand the data rather than overloading the LLM with everything at once. To achieve this, we create a financial LLM (FLLM) using multitask prompt-based finetuning. However, labeled data is scarce for each task, which can be costly and time-consuming to obtain.To overcome this challenge, we employ abductive augmentation reasoning (AAR) to automatically generate training data by modifying the pseudo labels from FLLM's own outputs. This approach allows us to create a large amount of training data without relying on manual annotation.Our experiments show that our data-centric FLLM with AAR substantially outperforms baseline financial LLMs designed for raw text, achieving state-of-the-art performance on financial analysis and interpretation tasks. We also open source a new benchmark for financial analysis and interpretation.Our methodology provides a promising path to unlock LLMs' potential for complex real-world domains like finance. By preprocessing and pre-understanding the data, we can enable LLMs to better handle tasks that require a deep understanding of financial concepts and terminology.
</details></li>
</ul>
<hr>
<h2 id="Automatic-and-Efficient-Customization-of-Neural-Networks-for-ML-Applications"><a href="#Automatic-and-Efficient-Customization-of-Neural-Networks-for-ML-Applications" class="headerlink" title="Automatic and Efficient Customization of Neural Networks for ML Applications"></a>Automatic and Efficient Customization of Neural Networks for ML Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04685">http://arxiv.org/abs/2310.04685</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhan Liu, Chengcheng Wan, Kuntai Du, Henry Hoffmann, Junchen Jiang, Shan Lu, Michael Maire</li>
<li>for: 这种 исследование旨在解决现有的机器学习（ML）API问题，即不同应用程序对ML API输出的不同响应。</li>
<li>methods: 该研究使用了77个实际应用程序，总共使用了6个ML API提供商的API，以探索这些应用程序如何使用ML API输出来影响它们的决策过程。</li>
<li>results: 研究发现，使用ChameleonAPI优化框架可以减少不正确的应用程序决策数量，相比基准值，减少了43%。<details>
<summary>Abstract</summary>
ML APIs have greatly relieved application developers of the burden to design and train their own neural network models -- classifying objects in an image can now be as simple as one line of Python code to call an API. However, these APIs offer the same pre-trained models regardless of how their output is used by different applications. This can be suboptimal as not all ML inference errors can cause application failures, and the distinction between inference errors that can or cannot cause failures varies greatly across applications.   To tackle this problem, we first study 77 real-world applications, which collectively use six ML APIs from two providers, to reveal common patterns of how ML API output affects applications' decision processes. Inspired by the findings, we propose ChameleonAPI, an optimization framework for ML APIs, which takes effect without changing the application source code. ChameleonAPI provides application developers with a parser that automatically analyzes the application to produce an abstract of its decision process, which is then used to devise an application-specific loss function that only penalizes API output errors critical to the application. ChameleonAPI uses the loss function to efficiently train a neural network model customized for each application and deploys it to serve API invocations from the respective application via existing interface. Compared to a baseline that selects the best-of-all commercial ML API, we show that ChameleonAPI reduces incorrect application decisions by 43%.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）API对应用开发者的负担减轻了设计和训练自己的神经网络模型的负担--用一行python代码调用API可以将图像中的对象分类。然而，这些API提供的预训练模型无论如何都是不变的，这可能是优化的问题，因为不同应用程序中ML推断错误的影响不同。为解决这个问题，我们首先研究了77个实际应用程序，它们共用六个ML API从两家提供商，以探索这些应用程序如何使用ML API的输出来影响它们的决策过程。 inspirited by the findings, we propose ChameleonAPI, an optimization framework for ML APIs, which takes effect without changing the application source code. ChameleonAPI提供了一个分析器，可以自动分析应用程序，生成一个摘要，用于生成每个应用程序的特定损失函数，只有API输出错误对应用程序的决策有影响。 ChameleonAPI使用这个损失函数来高效地训练每个应用程序特定的神经网络模型，并通过现有接口部署到应用程序中。相比基准选择所有商业ML API中最佳的选择，我们显示了ChameleonAPI可以将错误应用程序决策减少43%。
</details></li>
</ul>
<hr>
<h2 id="VoiceExtender-Short-utterance-Text-independent-Speaker-Verification-with-Guided-Diffusion-Model"><a href="#VoiceExtender-Short-utterance-Text-independent-Speaker-Verification-with-Guided-Diffusion-Model" class="headerlink" title="VoiceExtender: Short-utterance Text-independent Speaker Verification with Guided Diffusion Model"></a>VoiceExtender: Short-utterance Text-independent Speaker Verification with Guided Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04681">http://arxiv.org/abs/2310.04681</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yayun He, Zuheng Kang, Jianzong Wang, Junqing Peng, Jing Xiao</li>
<li>for: 提高短语音识别性能（Speaker Verification，SV），特别是处理短时间语音信号。</li>
<li>methods: 我们提出了一种新的架构，即VoiceExtender，该架构使用两个导航 diffusion model，包括内置的speaker embedding（SE）导航 diffusion model，以及一个基于扩展的 diffusion model-based sample generator，以使用SE指导来增强语音特征。</li>
<li>results: 我们的方法在VoxCeleb1数据集上进行了广泛的实验，与基准方法相比，我们的方法在短语音Conditions下（0.5, 1.0, 1.5, 2.0秒）实现了相对改善46.1%, 35.7%, 10.4%, 5.7%。<details>
<summary>Abstract</summary>
Speaker verification (SV) performance deteriorates as utterances become shorter. To this end, we propose a new architecture called VoiceExtender which provides a promising solution for improving SV performance when handling short-duration speech signals. We use two guided diffusion models, the built-in and the external speaker embedding (SE) guided diffusion model, both of which utilize a diffusion model-based sample generator that leverages SE guidance to augment the speech features based on a short utterance. Extensive experimental results on the VoxCeleb1 dataset show that our method outperforms the baseline, with relative improvements in equal error rate (EER) of 46.1%, 35.7%, 10.4%, and 5.7% for the short utterance conditions of 0.5, 1.0, 1.5, and 2.0 seconds, respectively.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_language=zh-CN</SYS> spreaker verification (SV) perfomance degenerates as utterances become shorter. To this end, we propose a new architecture called VoiceExtender, which provides a promising solution for improving SV perfomance when handling short-duration speech signals. We use two guided diffusion models, the built-in and the external speaker embedding (SE) guided diffusion model, both of which utilize a diffusion model-based sample generator that leverages SE guidance to augment the speech features based on a short utterance. Extensive experimental results on the VoxCeleb1 dataset show that our method outperforms the baseline, with relative improvements in equal error rate (EER) of 46.1%, 35.7%, 10.4%, and 5.7% for the short utterance conditions of 0.5, 1.0, 1.5, and 2.0 seconds, respectively.
</details></li>
</ul>
<hr>
<h2 id="The-Cost-of-Down-Scaling-Language-Models-Fact-Recall-Deteriorates-before-In-Context-Learning"><a href="#The-Cost-of-Down-Scaling-Language-Models-Fact-Recall-Deteriorates-before-In-Context-Learning" class="headerlink" title="The Cost of Down-Scaling Language Models: Fact Recall Deteriorates before In-Context Learning"></a>The Cost of Down-Scaling Language Models: Fact Recall Deteriorates before In-Context Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04680">http://arxiv.org/abs/2310.04680</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tian Jin, Nolan Clement, Xin Dong, Vaishnavh Nagarajan, Michael Carbin, Jonathan Ragan-Kelley, Gintare Karolina Dziugaite</li>
<li>for: 这个研究探讨了大语言模型（LLM）中缩放参数数量对其核心能力的影响。</li>
<li>methods: 研究使用了两种自然缩放技术：weight pruning和训练小型或大型模型（dense scaling），并对两种核心能力：在预训练中提供的信息回忆和在推理中处理信息进行分析。</li>
<li>results: 研究发现，通过减少模型大小超过30%（via either scaling approach）会显著降低在预训练中提供的信息回忆的能力。然而，减少模型大小60-70%可以保留在context中的多种信息处理方式，包括从长文本中检索答案和从句子中学习参数化函数。这种行为表明缩放模型大小对于信息回忆和在context中学习有不同的影响。<details>
<summary>Abstract</summary>
How does scaling the number of parameters in large language models (LLMs) affect their core capabilities? We study two natural scaling techniques -- weight pruning and simply training a smaller or larger model, which we refer to as dense scaling -- and their effects on two core capabilities of LLMs: (a) recalling facts presented during pre-training and (b) processing information presented in-context during inference. By curating a suite of tasks that help disentangle these two capabilities, we find a striking difference in how these two abilities evolve due to scaling. Reducing the model size by more than 30\% (via either scaling approach) significantly decreases the ability to recall facts seen in pre-training. Yet, a 60--70\% reduction largely preserves the various ways the model can process in-context information, ranging from retrieving answers from a long context to learning parameterized functions from in-context exemplars. The fact that both dense scaling and weight pruning exhibit this behavior suggests that scaling model size has an inherently disparate effect on fact recall and in-context learning.
</details>
<details>
<summary>摘要</summary>
如何在大语言模型（LLM）中缩放参数数量影响其核心能力？我们研究了两种自然缩放技术——重量剪裁和训练小或大模型，我们称之为笛卡尔缩放——对两个LLM核心能力的影响：（a）在预训练时提供的信息回忆和（b）在推理时接受的信息处理。我们编排了一组任务，以帮助分离这两种能力。我们发现，减少模型大小超过30%（通过任何缩放方法）会导致预训练中提供的信息回忆降低。然而，减少模型大小60-70%以上将保留在上下文中的各种信息处理方式，包括从长上下文中检索答案以及从上下文中学习参数化函数。这种行为表明，缩放模型大小对预训练中信息回忆和上下文中信息处理具有不同的影响。两种缩放技术的行为表明，缩放模型大小在预训练中信息回忆和上下文中信息处理之间存在本质差异。
</details></li>
</ul>
<hr>
<h2 id="LauraGPT-Listen-Attend-Understand-and-Regenerate-Audio-with-GPT"><a href="#LauraGPT-Listen-Attend-Understand-and-Regenerate-Audio-with-GPT" class="headerlink" title="LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT"></a>LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04673">http://arxiv.org/abs/2310.04673</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaming Wang, Zhihao Du, Qian Chen, Yunfei Chu, Zhifu Gao, Zerui Li, Kai Hu, Xiaohuan Zhou, Jin Xu, Ziyang Ma, Wen Wang, Siqi Zheng, Chang Zhou, Zhijie Yan, Shiliang Zhang</li>
<li>for: 这篇论文的目的是提出一种基于Transformer框架的通用语言模型，用于音频识别、理解和生成。</li>
<li>methods: 这篇论文使用了一种组合 kontinuous和精确的特征来编码输入音频，然后使用一个大型Decoder-onlyTransformer语言模型进行多任务超级vised学习。</li>
<li>results: 实验表明，LauraGPT在多种音频处理标准准点上达到了或超过现有最佳性能。<details>
<summary>Abstract</summary>
Generative Pre-trained Transformer (GPT) models have achieved remarkable performance on various natural language processing tasks. However, there has been limited research on applying similar frameworks to audio tasks. Previously proposed large language models for audio tasks either lack sufficient quantitative evaluations, or are limited to tasks for recognizing and understanding audio content, or significantly underperform existing state-of-the-art (SOTA) models. In this paper, we propose LauraGPT, a unified GPT model for audio recognition, understanding, and generation. LauraGPT is a versatile language model that can process both audio and text inputs and generate outputs in either modalities. It can perform a wide range of tasks related to content, semantics, paralinguistics, and audio-signal analysis. Some of its noteworthy tasks include automatic speech recognition, speech-to-text translation, text-to-speech synthesis, machine translation, speech enhancement, automated audio captioning, speech emotion recognition, and spoken language understanding. To achieve this goal, we use a combination of continuous and discrete features for audio. We encode input audio into continuous representations using an audio encoder and decode output audio from discrete codec codes. We then fine-tune a large decoder-only Transformer-based language model on multiple audio-to-text, text-to-audio, audio-to-audio, and text-to-text tasks using a supervised multitask learning approach. Extensive experiments show that LauraGPT achieves competitive or superior performance compared to existing SOTA models on various audio processing benchmarks.
</details>
<details>
<summary>摘要</summary>
生成预训练 transformer（GPT）模型在不同的自然语言处理任务上实现了卓越的表现。然而，对于音频任务来说，有限的研究是在应用相似的框架。以前提出的大语言模型 для音频任务 either lack sufficient quantitative evaluations, or 只能完成音频内容认识和理解任务，或者明显地下perform existing state-of-the-art（SOTA）模型。在这篇论文中，我们提出LauraGPT，一个统一的GPT模型，用于音频认识、理解和生成。LauraGPT 是一种灵活的语言模型，可以处理音频和文本输入，并生成输出在不同的modalities。它可以执行各种内容、semantics、paralinguistics和音频信号分析相关的任务。LauraGPT 的一些吸引人的任务包括自动 speech recognition、speech-to-text翻译、文本-speech合成、机器翻译、speech enhancement、自动音频标题、speech emotion recognition和 spoken language understanding。为了实现这个目标，我们使用了一种组合 continuous和 discrete 特征来编码输入音频。我们使用 audio encoder 编码输入音频，并将输出音频解码为 discrete codec codes。然后，我们在多个 audio-to-text、text-to-audio、audio-to-audio和 text-to-text 任务上精度 fine-tune 一个大型 decoder-only Transformer-based language model。经验表明，LauraGPT 在多种音频处理标准准则上实现了竞争或更高的表现。
</details></li>
</ul>
<hr>
<h2 id="Label-free-Node-Classification-on-Graphs-with-Large-Language-Models-LLMS"><a href="#Label-free-Node-Classification-on-Graphs-with-Large-Language-Models-LLMS" class="headerlink" title="Label-free Node Classification on Graphs with Large Language Models (LLMS)"></a>Label-free Node Classification on Graphs with Large Language Models (LLMS)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04668">http://arxiv.org/abs/2310.04668</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/currytang/llmgnn">https://github.com/currytang/llmgnn</a></li>
<li>paper_authors: Zhikai Chen, Haitao Mao, Hongzhi Wen, Haoyu Han, Wei Jin, Haiyang Zhang, Hui Liu, Jiliang Tang</li>
<li>for: 这个研究的目的是发展一个没有标签的节点分类框架，即LLM-GNN，以便在节点资料上进行分类。</li>
<li>methods: 这个研究使用了大型自然语言模型（LLM）和节点神经网络（GNN）两种不同的技术，并融合它们以获得更好的性能。具体来说，LLMs是用来标签一小部分节点，然后GNNs是用来在这些标签下训练来进行预测。</li>
<li>results: 实验结果显示，LLM-GNN可以在广泛的数据集上达到74.9%的精度，而且在训练成本下than 1 dollar。<details>
<summary>Abstract</summary>
In recent years, there have been remarkable advancements in node classification achieved by Graph Neural Networks (GNNs). However, they necessitate abundant high-quality labels to ensure promising performance. In contrast, Large Language Models (LLMs) exhibit impressive zero-shot proficiency on text-attributed graphs. Yet, they face challenges in efficiently processing structural data and suffer from high inference costs. In light of these observations, this work introduces a label-free node classification on graphs with LLMs pipeline, LLM-GNN. It amalgamates the strengths of both GNNs and LLMs while mitigating their limitations. Specifically, LLMs are leveraged to annotate a small portion of nodes and then GNNs are trained on LLMs' annotations to make predictions for the remaining large portion of nodes. The implementation of LLM-GNN faces a unique challenge: how can we actively select nodes for LLMs to annotate and consequently enhance the GNN training? How can we leverage LLMs to obtain annotations of high quality, representativeness, and diversity, thereby enhancing GNN performance with less cost? To tackle this challenge, we develop an annotation quality heuristic and leverage the confidence scores derived from LLMs to advanced node selection. Comprehensive experimental results validate the effectiveness of LLM-GNN. In particular, LLM-GNN can achieve an accuracy of 74.9% on a vast-scale dataset \products with a cost less than 1 dollar.
</details>
<details>
<summary>摘要</summary>
recent years, there have been remarkable advancements in node classification achieved by Graph Neural Networks (GNNs). However, they necessitate abundant high-quality labels to ensure promising performance. In contrast, Large Language Models (LLMs) exhibit impressive zero-shot proficiency on text-attributed graphs. Yet, they face challenges in efficiently processing structural data and suffer from high inference costs. In light of these observations, this work introduces a label-free node classification on graphs with LLMs pipeline, LLM-GNN. It amalgamates the strengths of both GNNs and LLMs while mitigating their limitations. Specifically, LLMs are leveraged to annotate a small portion of nodes, and then GNNs are trained on LLMs' annotations to make predictions for the remaining large portion of nodes. The implementation of LLM-GNN faces a unique challenge: how can we actively select nodes for LLMs to annotate and consequently enhance the GNN training? How can we leverage LLMs to obtain annotations of high quality, representativeness, and diversity, thereby enhancing GNN performance with less cost? To tackle this challenge, we develop an annotation quality heuristic and leverage the confidence scores derived from LLMs to advanced node selection. Comprehensive experimental results validate the effectiveness of LLM-GNN. In particular, LLM-GNN can achieve an accuracy of 74.9% on a vast-scale dataset  with a cost less than 1 dollar.
</details></li>
</ul>
<hr>
<h2 id="HalluciDet-Hallucinating-RGB-Modality-for-Person-Detection-Through-Privileged-Information"><a href="#HalluciDet-Hallucinating-RGB-Modality-for-Person-Detection-Through-Privileged-Information" class="headerlink" title="HalluciDet: Hallucinating RGB Modality for Person Detection Through Privileged Information"></a>HalluciDet: Hallucinating RGB Modality for Person Detection Through Privileged Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04662">http://arxiv.org/abs/2310.04662</a></li>
<li>repo_url: None</li>
<li>paper_authors: Heitor Rapela Medeiros, Fidel A. Guerrero Pena, Masih Aminbeidokhti, Thomas Dubail, Eric Granger, Marco Pedersoli</li>
<li>for: 这个论文是用于对于具有大跨模式转换的Visul recognition任务中的人像检测，以提高检测性能。</li>
<li>methods: 这个论文使用了一种叫做HalluciDet的IR-RGB图像转换模型，这个模型不是将RGB图像转换为IR图像，而是将IR图像转换为一个能够增强物体的新图像表示。</li>
<li>results: 这个论文的实验结果显示，使用HalluciDet模型可以大幅提高人像检测精度，并且比起使用state-of-the-art图像转换方法以及对IR图像进行精度调整，能够更好地适应训练数据的差异。<details>
<summary>Abstract</summary>
A powerful way to adapt a visual recognition model to a new domain is through image translation. However, common image translation approaches only focus on generating data from the same distribution of the target domain. In visual recognition tasks with complex images, such as pedestrian detection on aerial images with a large cross-modal shift in data distribution from Infrared (IR) to RGB images, a translation focused on generation might lead to poor performance as the loss focuses on irrelevant details for the task. In this paper, we propose HalluciDet, an IR-RGB image translation model for object detection that, instead of focusing on reconstructing the original image on the IR modality, is guided directly on reducing the detection loss of an RGB detector, and therefore avoids the need to access RGB data. This model produces a new image representation that enhances the object of interest in the scene and greatly improves detection performance. We empirically compare our approach against state-of-the-art image translation methods as well as with the commonly used fine-tuning on IR, and show that our method improves detection accuracy in most cases, by exploiting the privileged information encoded in a pre-trained RGB detector.
</details>
<details>
<summary>摘要</summary>
一种强大的方法是通过图像翻译来适应新领域的视觉识别模型。然而，常见的图像翻译方法只关注生成数据与目标领域的同分布。在复杂的视觉任务中，如人员检测在航空图像上的红外（IR）到RGB图像之间的大跨模态差，一种专注于生成的翻译方法可能会导致性能下降，因为损失将关注无关于任务的细节。在这篇论文中，我们提出了HalluciDet，一种用于对象检测的IR-RGB图像翻译模型。这种模型不是专注于重建原始IR图像，而是通过直接减少RGB检测器的检测损失来指导，因此不需要访问RGB数据。这种模型生成的新图像表示法可以增强Scene中的对象，并大幅提高检测性能。我们对比了我们的方法与现有的图像翻译方法以及通常使用的RGB数据练习，并证明了我们的方法在大多数情况下可以提高检测精度，通过利用预训练的RGB检测器中嵌入的特权信息。
</details></li>
</ul>
<hr>
<h2 id="Do-self-supervised-speech-and-language-models-extract-similar-representations-as-human-brain"><a href="#Do-self-supervised-speech-and-language-models-extract-similar-representations-as-human-brain" class="headerlink" title="Do self-supervised speech and language models extract similar representations as human brain?"></a>Do self-supervised speech and language models extract similar representations as human brain?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04645">http://arxiv.org/abs/2310.04645</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peili Chen, Linyang He, Li Fu, Lu Fan, Edward F. Chang, Yuanning Li</li>
<li>for: 这个论文主要研究了 SSL 模型在语言理解中的表现，以及它们与大脑活动的相似性。</li>
<li>methods: 研究者使用了两种代表性 SSL 模型，即 Wav2Vec2.0 和 GPT-2，来评估大脑预测性能。</li>
<li>results: 研究发现，这两种模型在 auditory cortex 中都能准确预测语音响应，并且它们之间的大脑预测相互吻合。另外，共享的语音上下文信息在这两种模型中占据了大脑活动变化的主要贡献，超过静态 semantics 和 lower-level acoustic-phonetic 信息。这些结果表明 SSL 模型中的语音上下文表示 converge 到大脑 beneath speech perception 的网络，并且它们与大脑的语言处理机制相似。<details>
<summary>Abstract</summary>
Speech and language models trained through self-supervised learning (SSL) demonstrate strong alignment with brain activity during speech and language perception. However, given their distinct training modalities, it remains unclear whether they correlate with the same neural aspects. We directly address this question by evaluating the brain prediction performance of two representative SSL models, Wav2Vec2.0 and GPT-2, designed for speech and language tasks. Our findings reveal that both models accurately predict speech responses in the auditory cortex, with a significant correlation between their brain predictions. Notably, shared speech contextual information between Wav2Vec2.0 and GPT-2 accounts for the majority of explained variance in brain activity, surpassing static semantic and lower-level acoustic-phonetic information. These results underscore the convergence of speech contextual representations in SSL models and their alignment with the neural network underlying speech perception, offering valuable insights into both SSL models and the neural basis of speech and language processing.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用自我超vision学习（SSL）训练的语音和语言模型在语音和语言识别过程中具有强的对应性。然而，由于它们的不同训练方式，是否与同样的神经元方面相关仍然未知。我们直接解决这个问题，通过评估两个代表性SSL模型——Wav2Vec2.0和GPT-2——在语音和语言任务中的脑预测性能。我们发现，这两个模型在听觉核心区域中准确预测语音回应，并且它们的脑预测显示了显著的相关性。值得注意的是，在Wav2Vec2.0和GPT-2之间共享的语音上下文信息占主要的解释变量的比重，超过静态semantic和低级别的语音学习信息。这些结果表明SSL模型中的语音上下文表示具有共同性，与语音识别神经网络下的神经元表示相吻合，为SSL模型和语音和语言处理的神经基础提供了有价值的发现。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Anonymization-of-Swiss-Federal-Supreme-Court-Rulings"><a href="#Automatic-Anonymization-of-Swiss-Federal-Supreme-Court-Rulings" class="headerlink" title="Automatic Anonymization of Swiss Federal Supreme Court Rulings"></a>Automatic Anonymization of Swiss Federal Supreme Court Rulings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04632">http://arxiv.org/abs/2310.04632</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joel Niklaus, Robin Mamié, Matthias Stürmer, Daniel Brunner, Marcel Gygli</li>
<li>for: 本研究旨在提高释放法院判决的公众发布需要遵循适当的匿名化方法，以保护所有参与人员，如果必要。</li>
<li>methods: 本研究使用现有系统， комбиines 不同的传统计算方法和人工专家。我们在本研究中增强了现有的匿名化软件，使用大量标注的实体需要匿名化。我们比较了基于 BERT 的模型和基于域内数据预处理的模型。</li>
<li>results: 我们的结果表明，使用域内数据预处理模型可以进一步提高 F1 分数，高于现有模型的提升。我们的研究示例了，结合现有的匿名化方法，如常见表达式，与机器学习结合可以进一步减少人工劳动，提高自动建议。<details>
<summary>Abstract</summary>
Releasing court decisions to the public relies on proper anonymization to protect all involved parties, where necessary. The Swiss Federal Supreme Court relies on an existing system that combines different traditional computational methods with human experts. In this work, we enhance the existing anonymization software using a large dataset annotated with entities to be anonymized. We compared BERT-based models with models pre-trained on in-domain data. Our results show that using in-domain data to pre-train the models further improves the F1-score by more than 5\% compared to existing models. Our work demonstrates that combining existing anonymization methods, such as regular expressions, with machine learning can further reduce manual labor and enhance automatic suggestions.
</details>
<details>
<summary>摘要</summary>
发布法院判决到公众需要采用正确的匿名化方法保护所有参与者，如果必要。瑞士联邦最高法院使用现有系统，这个系统组合了不同的传统计算方法和人工专家。在这项工作中，我们提高了现有的匿名化软件使用大量标注需要匿名化的数据集。我们比较了基于BERT的模型和基于域内数据预处理的模型。我们的结果表明，使用域内数据预处理模型可以提高F1分数超过5% compare to现有模型。我们的工作表明，将现有的匿名化方法，如常见表达式，与机器学习结合可以进一步减少人工劳动并提高自动建议。
</details></li>
</ul>
<hr>
<h2 id="SERA-Sample-Efficient-Reward-Augmentation-in-offline-to-online-Reinforcement-Learning"><a href="#SERA-Sample-Efficient-Reward-Augmentation-in-offline-to-online-Reinforcement-Learning" class="headerlink" title="SERA:Sample Efficient Reward Augmentation in offline-to-online Reinforcement Learning"></a>SERA:Sample Efficient Reward Augmentation in offline-to-online Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19805">http://arxiv.org/abs/2310.19805</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziqi Zhang, Xiao Xiong, Zifeng Zhuang, Jinxin Liu, Donglin Wang</li>
<li>for: The paper aims to improve the performance of online fine-tuning in reinforcement learning (RL) by addressing the issue of diminished exploration in direct fine-tuning of offline pre-trained policies.</li>
<li>methods: The proposed method, called Sample Efficient Reward Augmentation (SERA), uses a generalized reward augmentation framework to improve exploration during online fine-tuning. SERA includes two components: State Marginal Matching (SMM) and penalization of out-of-distribution (OOD) state actions.</li>
<li>results: The paper demonstrates that SERA consistently and effectively enhances the performance of various offline algorithms in offline-to-online problems, achieving better online fine-tuning results. Additionally, SERA is versatile and can be effortlessly plugged into various RL algorithms to improve online fine-tuning and ensure sustained asymptotic improvement.<details>
<summary>Abstract</summary>
A prospective application of offline reinforcement learning (RL) involves initializing a pre-trained policy using existing static datasets for subsequent online fine-tuning. However, direct fine-tuning of the offline pre-trained policy often results in sub-optimal performance. A primary reason is that offline conservative methods diminish the agent's capability of exploration, thereby impacting online fine-tuning performance. To enhance exploration during online fine-tuning and thus enhance the overall online fine-tuning performance, we introduce a generalized reward augmentation framework called Sample Efficient Reward Augmentation (SERA). SERA aims to improve the performance of online fine-tuning by designing intrinsic rewards that encourage the agent to explore. Specifically, it implicitly implements State Marginal Matching (SMM) and penalizes out-of-distribution (OOD) state actions, thus encouraging agents to cover the target state density, and achieving better online fine-tuning results. Additionally, SERA can be effortlessly plugged into various RL algorithms to improve online fine-tuning and ensure sustained asymptotic improvement, showing the versatility as well as the effectiveness of SERA. Moreover, extensive experimental results will demonstrate that when conducting offline-to-online problems, SERA consistently and effectively enhances the performance of various offline algorithms.
</details>
<details>
<summary>摘要</summary>
可能的应用是将预训练的策略初始化使用现有的静止数据集进行后续的线上精练。然而，直接精练预训练的预训策略通常会导致下线精练性能不佳。一个主要的原因是预训保守的方法会对机器人的探索能力有限制，因此影响线上精练性能。为了增强线上精练中的探索和总体性能，我们介绍一个通用的奖励增强框架，即样本优化奖励（SERA）。SERA的目标是通过设计内在奖励来提高线上精练的表现。具体而言，它隐式实现了状态范围匹配（SMM）和外部状态动作的惩罚，因此鼓励机器人覆盖目标状态密度，并获得更好的线上精练结果。此外，SERA可以轻松地插入到不同的RL算法中，以提高线上精练并确保持长期上升，显示了SERA的多样性和有效性。此外，广泛的实验结果显示，当进行阶段性问题时，SERA可靠地和有效地提高不同的预训算法的表现。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/07/cs.AI_2023_10_07/" data-id="clp869trn0055k5887b60an21" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/31/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/30/">30</a><a class="page-number" href="/page/31/">31</a><span class="page-number current">32</span><a class="page-number" href="/page/33/">33</a><a class="page-number" href="/page/34/">34</a><span class="space">&hellip;</span><a class="page-number" href="/page/97/">97</a><a class="extend next" rel="next" href="/page/33/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">66</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">81</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">140</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
