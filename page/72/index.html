
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/72/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.CV_2023_07_25" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/25/cs.CV_2023_07_25/" class="article-date">
  <time datetime="2023-07-25T13:00:00.000Z" itemprop="datePublished">2023-07-25</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/25/cs.CV_2023_07_25/">cs.CV - 2023-07-25</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Mystique-Deconstructing-SVG-Charts-for-Layout-Reuse"><a href="#Mystique-Deconstructing-SVG-Charts-for-Layout-Reuse" class="headerlink" title="Mystique: Deconstructing SVG Charts for Layout Reuse"></a>Mystique: Deconstructing SVG Charts for Layout Reuse</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13567">http://arxiv.org/abs/2307.13567</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Chen, Bongshin Lee, Yunhai Wang, Yunjeong Chang, Zhicheng Liu</li>
<li>for: 这篇论文的目的是提出一种能够解剖 rectangle-based 图表的方法，以便在新的数据上重用现有的图表。</li>
<li>methods: 这篇论文使用了一种混合式 iniciative approach，通过提取坐标轴和标签，将图表的布局 decomposing into four semantic component： mark groups、 spatial relationships、数据编码和图形约束。</li>
<li>results: 在 150 个 rectangle-based SVG 图表上，这种方法可以达到 above 85% 的准确率 для坐标和标签提取，以及 96% 的布局 decomposing 精度。在一个图表重用研究中，参与者可以轻松地将现有的图表应用到新的数据上。<details>
<summary>Abstract</summary>
To facilitate the reuse of existing charts, previous research has examined how to obtain a semantic understanding of a chart by deconstructing its visual representation into reusable components, such as encodings. However, existing deconstruction approaches primarily focus on chart styles, handling only basic layouts. In this paper, we investigate how to deconstruct chart layouts, focusing on rectangle-based ones, as they cover not only 17 chart types but also advanced layouts (e.g., small multiples, nested layouts). We develop an interactive tool, called Mystique, adopting a mixed-initiative approach to extract the axes and legend, and deconstruct a chart's layout into four semantic components: mark groups, spatial relationships, data encodings, and graphical constraints. Mystique employs a wizard interface that guides chart authors through a series of steps to specify how the deconstructed components map to their own data. On 150 rectangle-based SVG charts, Mystique achieves above 85% accuracy for axis and legend extraction and 96% accuracy for layout deconstruction. In a chart reproduction study, participants could easily reuse existing charts on new datasets. We discuss the current limitations of Mystique and future research directions.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传送给我的文本翻译成简化中文。</SYS>>以便重用现有的图表，前期研究已经研究了如何从图表的视觉表示中提取SemanticComponents，例如编码。然而，现有的分解方法主要集中在图表风格上，只处理基本布局。在这篇论文中，我们调查了如何从图表布局中提取SemanticComponents，特点Rectangle-based布局，因为它们不仅覆盖了17种图表类型，还包括高级布局（例如小多个、嵌套布局）。我们开发了一个交互工具，名为Mystique，采用杂合主义方法来提取轴和标签，并将图表布局分解成四个SemanticComponents：标记组、空间关系、数据编码和图形约束。Mystique使用了一个帮助chart作者通过一系列步骤指定分解后的组件与他们的数据之间的映射。在150个Rectangle-based SVG图表上，Mystique实现了轴和标签提取的准确率高于85%，布局分解的准确率达96%。在一次图表重制实验中，参与者可以轻松地将现有图表应用到新的数据集上。我们讨论了Mystique的当前限制和未来研究方向。
</details></li>
</ul>
<hr>
<h2 id="Model-Calibration-in-Dense-Classification-with-Adaptive-Label-Perturbation"><a href="#Model-Calibration-in-Dense-Classification-with-Adaptive-Label-Perturbation" class="headerlink" title="Model Calibration in Dense Classification with Adaptive Label Perturbation"></a>Model Calibration in Dense Classification with Adaptive Label Perturbation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13539">http://arxiv.org/abs/2307.13539</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/carlisle-liu/aslp">https://github.com/carlisle-liu/aslp</a></li>
<li>paper_authors: Jiawei Liu, Changkun Ye, Shan Wang, Ruikai Cui, Jing Zhang, Kaihao Zhang, Nick Barnes</li>
<li>for: 这个研究旨在提高深度神经网络的准确性和信任度，以便在安全相关应用中使用。</li>
<li>methods: 本研究提出了一种名为 Adaptive Stochastic Label Perturbation (ASLP) 的方法，它可以学习每个训练图像的唯一标签变化水平。ASLP 使用的是我们所提出的 Self-Calibrating Binary Cross Entropy (SC-BCE) 损失函数，它可以统一标签变化程序，包括随机方法（如 DisturbLabel）和标签平滑，以更正均化。</li>
<li>results: 实验结果显示，ASLP 可以对封闭的 binary 分类模型进行重大的均化，并且可以保持知道的标签准确率。在 both in-distribution 和 out-of-distribution 数据上，ASLP 可以提高模型的准确性和信任度。<details>
<summary>Abstract</summary>
For safety-related applications, it is crucial to produce trustworthy deep neural networks whose prediction is associated with confidence that can represent the likelihood of correctness for subsequent decision-making. Existing dense binary classification models are prone to being over-confident. To improve model calibration, we propose Adaptive Stochastic Label Perturbation (ASLP) which learns a unique label perturbation level for each training image. ASLP employs our proposed Self-Calibrating Binary Cross Entropy (SC-BCE) loss, which unifies label perturbation processes including stochastic approaches (like DisturbLabel), and label smoothing, to correct calibration while maintaining classification rates. ASLP follows Maximum Entropy Inference of classic statistical mechanics to maximise prediction entropy with respect to missing information. It performs this while: (1) preserving classification accuracy on known data as a conservative solution, or (2) specifically improves model calibration degree by minimising the gap between the prediction accuracy and expected confidence of the target training label. Extensive results demonstrate that ASLP can significantly improve calibration degrees of dense binary classification models on both in-distribution and out-of-distribution data. The code is available on https://github.com/Carlisle-Liu/ASLP.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:为安全相关应用，生成可靠的深度神经网络的预测结果需要与正确性的可信度相关，以便进行后续决策。现有的密集二分类模型容易过于自信。为了改善模型准确性，我们提议使用 Adaptive Stochastic Label Perturbation (ASLP)，它学习每个训练图像的特有标签扰动水平。ASLP使用我们提议的 Self-Calibrating Binary Cross Entropy (SC-BCE) 损失函数，它将标签扰动过程、标签平滑和随机扰动等进行统一处理，以更正准确性。ASLP采用经典统计力学中的最大 entropy 推理来最大化预测结果的不确定性，同时： (1) 保持知道数据上的分类精度作为保守解决方案，或 (2) 特定地改善模型准确性度。广泛的结果表明，ASLP可以大幅提高密集二分类模型的准确性和可靠性。代码可以在 https://github.com/Carlisle-Liu/ASLP 上获取。
</details></li>
</ul>
<hr>
<h2 id="Not-with-my-name-Inferring-artists’-names-of-input-strings-employed-by-Diffusion-Models"><a href="#Not-with-my-name-Inferring-artists’-names-of-input-strings-employed-by-Diffusion-Models" class="headerlink" title="Not with my name! Inferring artists’ names of input strings employed by Diffusion Models"></a>Not with my name! Inferring artists’ names of input strings employed by Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13527">http://arxiv.org/abs/2307.13527</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ictlab-unict/not-with-my-name">https://github.com/ictlab-unict/not-with-my-name</a></li>
<li>paper_authors: Roberto Leotta, Oliver Giudice, Luca Guarnera, Sebastiano Battiato</li>
<li>for: 本研究的目的是探讨Diffusion Models（DM）是否可以生成艺术作品，以及如果可以的话，那么DM是如何学习和复制艺术家的风格和技巧的。</li>
<li>methods: 本研究使用了一种叫做Siamese Neural Network的特殊神经网络，以便对生成的图像进行预测和分析。</li>
<li>results: 实验结果表明，我们的方法可以准确地预测图像的一部分，并且可以作为图像的输入串进行预测。这些结果表明了我们的方法是一个有用的开始，可以用于预测某个图像的完整输入串。<details>
<summary>Abstract</summary>
Diffusion Models (DM) are highly effective at generating realistic, high-quality images. However, these models lack creativity and merely compose outputs based on their training data, guided by a textual input provided at creation time. Is it acceptable to generate images reminiscent of an artist, employing his name as input? This imply that if the DM is able to replicate an artist's work then it was trained on some or all of his artworks thus violating copyright. In this paper, a preliminary study to infer the probability of use of an artist's name in the input string of a generated image is presented. To this aim we focused only on images generated by the famous DALL-E 2 and collected images (both original and generated) of five renowned artists. Finally, a dedicated Siamese Neural Network was employed to have a first kind of probability. Experimental results demonstrate that our approach is an optimal starting point and can be employed as a prior for predicting a complete input string of an investigated image. Dataset and code are available at: https://github.com/ictlab-unict/not-with-my-name .
</details>
<details>
<summary>摘要</summary>
Diffusion Models (DM) 是非常有效的生成高质量、真实的图像。然而，这些模型缺乏创造力，只是根据它们的训练数据，遵循文本输入提供于创建时，生成输出。是否可以使用艺术家的名字来生成图像？这意味着如果 DM 能够复制艺术家的作品，那么它们可能已经训练过一些或所有的艺术作品，从而违反版权。在这篇论文中，我们提出了一项初步研究，以确定使用艺术家名字在生成图像的输入串中的概率。为此，我们仅focus在 DALL-E 2 生成的图像和五位著名艺术家的原始和生成图像上。最后，我们使用专门的 Siamese Neural Network 来获得一种首个概率。实验结果表明，我们的方法是一个优秀的起点，可以作为预测完整的输入串的先天预测。数据集和代码可以在：https://github.com/ictlab-unict/not-with-my-name 中找到。
</details></li>
</ul>
<hr>
<h2 id="HeightFormer-Explicit-Height-Modeling-without-Extra-Data-for-Camera-only-3D-Object-Detection-in-Bird’s-Eye-View"><a href="#HeightFormer-Explicit-Height-Modeling-without-Extra-Data-for-Camera-only-3D-Object-Detection-in-Bird’s-Eye-View" class="headerlink" title="HeightFormer: Explicit Height Modeling without Extra Data for Camera-only 3D Object Detection in Bird’s Eye View"></a>HeightFormer: Explicit Height Modeling without Extra Data for Camera-only 3D Object Detection in Bird’s Eye View</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13510">http://arxiv.org/abs/2307.13510</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiming Wu, Ruixiang Li, Zequn Qin, Xinhai Zhao, Xi Li<br>for:height-based bird’s eye view (BEV) representation for autonomous drivingmethods:explicitly modeling heights in the BEV space using a self-recursive approachresults:achieves state-of-the-art (SOTA) performance compared to camera-only methods without using extra data like LiDAR<details>
<summary>Abstract</summary>
Vision-based Bird's Eye View (BEV) representation is an emerging perception formulation for autonomous driving. The core challenge is to construct BEV space with multi-camera features, which is a one-to-many ill-posed problem. Diving into all previous BEV representation generation methods, we found that most of them fall into two types: modeling depths in image views or modeling heights in the BEV space, mostly in an implicit way. In this work, we propose to explicitly model heights in the BEV space, which needs no extra data like LiDAR and can fit arbitrary camera rigs and types compared to modeling depths. Theoretically, we give proof of the equivalence between height-based methods and depth-based methods. Considering the equivalence and some advantages of modeling heights, we propose HeightFormer, which models heights and uncertainties in a self-recursive way. Without any extra data, the proposed HeightFormer could estimate heights in BEV accurately. Benchmark results show that the performance of HeightFormer achieves SOTA compared with those camera-only methods.
</details>
<details>
<summary>摘要</summary>
《bird's eye view（BEV）表示方式是自动驾驶视觉形态的一种emerging概念。核心挑战是在多camera视图中构建BEV空间，这是一个一对多的ILL-posed问题。我们对所有的BEV表示生成方法进行了检查，发现大多数都 fall into两类：在图像视图中模型深度或在BEV空间中模型高度，大多数是通过隐式方式来实现。在这种工作中，我们提议在BEV空间中直接模型高度，无需额外数据如LiDAR，并且可以适应任何相机装置和类型。从理论角度来看，我们证明了高度基于方法和深度基于方法之间的等价性。考虑到等价性和高度模型的一些优点，我们提议HeightFormer，它在自我循环方式中模型高度和不确定性。无需额外数据，提议的HeightFormer可以在BEV空间中估计高度的准确性。 benchmark结果表明，HeightFormer的性能与camera-only方法相比，达到了最高的SOTA水平。》Note: "SOTA" stands for "State of the Art", which means the highest level of performance currently achieved.
</details></li>
</ul>
<hr>
<h2 id="NormAUG-Normalization-guided-Augmentation-for-Domain-Generalization"><a href="#NormAUG-Normalization-guided-Augmentation-for-Domain-Generalization" class="headerlink" title="NormAUG: Normalization-guided Augmentation for Domain Generalization"></a>NormAUG: Normalization-guided Augmentation for Domain Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13492">http://arxiv.org/abs/2307.13492</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Qi, Hongpeng Yang, Yinghuan Shi, Xin Geng</li>
<li>for: 提高深度学习模型在指导学习中的性能， Addressing the challenge of domain shift between training and test sets.</li>
<li>methods: 提出了一种名为 NormAUG（Normalization-guided Augmentation）的简单 yet effective方法，通过在不同领域的批处理中进行扩充，提高模型的泛化能力。</li>
<li>results: 在多个 benchmark datasets 上进行了广泛的实验， validate the effectiveness of our proposed method, and show that it can effectively improve the performance of deep learning models in supervised learning tasks.<details>
<summary>Abstract</summary>
Deep learning has made significant advancements in supervised learning. However, models trained in this setting often face challenges due to domain shift between training and test sets, resulting in a significant drop in performance during testing. To address this issue, several domain generalization methods have been developed to learn robust and domain-invariant features from multiple training domains that can generalize well to unseen test domains. Data augmentation plays a crucial role in achieving this goal by enhancing the diversity of the training data. In this paper, inspired by the observation that normalizing an image with different statistics generated by different batches with various domains can perturb its feature, we propose a simple yet effective method called NormAUG (Normalization-guided Augmentation). Our method includes two paths: the main path and the auxiliary (augmented) path. During training, the auxiliary path includes multiple sub-paths, each corresponding to batch normalization for a single domain or a random combination of multiple domains. This introduces diverse information at the feature level and improves the generalization of the main path. Moreover, our NormAUG method effectively reduces the existing upper boundary for generalization based on theoretical perspectives. During the test stage, we leverage an ensemble strategy to combine the predictions from the auxiliary path of our model, further boosting performance. Extensive experiments are conducted on multiple benchmark datasets to validate the effectiveness of our proposed method.
</details>
<details>
<summary>摘要</summary>
根据图像不同域生成的不同统计数据的观察，我们提出了一种简单 yet 有效的方法，即 NormAUG（normalization-guided Augmentation）。我们的方法包括两个路径：主路径和辅助（扩展）路径。在训练阶段，辅助路径包括多个子路径，每个子路径对应一个域或一个随机组合多个域的批normalization。这引入了多样化的信息水平，从而提高主路径的泛化性。此外，我们的 NormAUG 方法有效地降低了基于理论上的最大界限，以提高泛化性。在测试阶段，我们利用了一种协同策略，将辅助路径的预测结果 ensemble，进一步提高表现。我们在多个标准 benchmark 数据集上进行了广泛的实验，以验证我们的提议的效果。
</details></li>
</ul>
<hr>
<h2 id="Cos-R-CNN-for-Online-Few-shot-Object-Detection"><a href="#Cos-R-CNN-for-Online-Few-shot-Object-Detection" class="headerlink" title="Cos R-CNN for Online Few-shot Object Detection"></a>Cos R-CNN for Online Few-shot Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13485">http://arxiv.org/abs/2307.13485</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gratianus Wesley Putra Data, Henry Howard-Jenkins, David Murray, Victor Prisacariu</li>
<li>for: 这篇论文旨在提出一个简单的例子基本的几何�CNN模型，用于在网络几何中进行几何物类检测。</li>
<li>methods: 这个模型使用了学习比较任务，将未见类别表示为对象的例子图像，并通过它们的相似性进行检测。</li>
<li>results: 这个模型在5-way ImageNet几何检测测试 benchmark 上取得了最佳结果，在线上1&#x2F;5&#x2F;10-shot情况下高于8&#x2F;3&#x2F;1%，并在线上20-way几何VOC中运行所有类别，在新类别上表现最好。<details>
<summary>Abstract</summary>
We propose Cos R-CNN, a simple exemplar-based R-CNN formulation that is designed for online few-shot object detection. That is, it is able to localise and classify novel object categories in images with few examples without fine-tuning. Cos R-CNN frames detection as a learning-to-compare task: unseen classes are represented as exemplar images, and objects are detected based on their similarity to these exemplars. The cosine-based classification head allows for dynamic adaptation of classification parameters to the exemplar embedding, and encourages the clustering of similar classes in embedding space without the need for manual tuning of distance-metric hyperparameters. This simple formulation achieves best results on the recently proposed 5-way ImageNet few-shot detection benchmark, beating the online 1/5/10-shot scenarios by more than 8/3/1%, as well as performing up to 20% better in online 20-way few-shot VOC across all shots on novel classes.
</details>
<details>
<summary>摘要</summary>
我们提出了Cos R-CNN，一种简单的示例基于的R-CNN形式，用于在线少量示例Object检测。即可以在图像中检测到未经调整的新类别 объек。Cos R-CNN将检测视为学习比较任务，未seen类是用示例图像表示，并基于这些示例图像来检测对象。cosine类型的分类头允许在示例嵌入空间进行动态适应分类参数，并促进类别的嵌入空间减少，不需要手动调整距离度量参数。这种简单的形式在最新的5种ImageNet几shot检测benchmark上达到了最佳结果，在在线1/5/10-shot场景中超过8/3/1%，并在在线20-way几shotVOC中所有陌生类上达到了20%的提升。
</details></li>
</ul>
<hr>
<h2 id="Weakly-supervised-3D-Pose-Transfer-with-Keypoints"><a href="#Weakly-supervised-3D-Pose-Transfer-with-Keypoints" class="headerlink" title="Weakly-supervised 3D Pose Transfer with Keypoints"></a>Weakly-supervised 3D Pose Transfer with Keypoints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13459">http://arxiv.org/abs/2307.13459</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jinnan-chen/3d-pose-transfer">https://github.com/jinnan-chen/3d-pose-transfer</a></li>
<li>paper_authors: Jinnan Chen, Chen Li, Gim Hee Lee</li>
<li>for: 本研究旨在解决3D姿态质量转移中的三大挑战：缺乏不同人物表演同一个姿态的对应数据集，分离姿态信息和形状信息，将姿态转移应用到不同骨架上。</li>
<li>methods: 我们提出了一种新的弱监督键点基于框架，使用不同骨架的逆 kinematics 计算源和目标骨架之间的变换。我们的方法仅需要键点监督，可以应用到不同骨架上，并且具有形状不变的特点，允许提取目标骨架中的姿态信息而不是形状信息。我们还设计了一种自我超频重建来实现自监督的姿态转移，不需要与target和source骨架具有同样的姿态和形状。</li>
<li>results: 我们在人体和动物数据集上进行了评估，与状态静的无监督方法相比，我们的方法具有更高的性能，甚至与完全监督方法相当。在更复杂的 Mixamo 数据集上进行测试，我们的方法能够正确地处理具有不同骨架和衣物的骨架。跨数据集评估表明了我们的方法具有强大的总体化能力。<details>
<summary>Abstract</summary>
The main challenges of 3D pose transfer are: 1) Lack of paired training data with different characters performing the same pose; 2) Disentangling pose and shape information from the target mesh; 3) Difficulty in applying to meshes with different topologies. We thus propose a novel weakly-supervised keypoint-based framework to overcome these difficulties. Specifically, we use a topology-agnostic keypoint detector with inverse kinematics to compute transformations between the source and target meshes. Our method only requires supervision on the keypoints, can be applied to meshes with different topologies and is shape-invariant for the target which allows extraction of pose-only information from the target meshes without transferring shape information. We further design a cycle reconstruction to perform self-supervised pose transfer without the need for ground truth deformed mesh with the same pose and shape as the target and source, respectively. We evaluate our approach on benchmark human and animal datasets, where we achieve superior performance compared to the state-of-the-art unsupervised approaches and even comparable performance with the fully supervised approaches. We test on the more challenging Mixamo dataset to verify our approach's ability in handling meshes with different topologies and complex clothes. Cross-dataset evaluation further shows the strong generalization ability of our approach.
</details>
<details>
<summary>摘要</summary>
主要3D姿态传输挑战包括：1）缺乏不同人物表演同一姿态的对称训练数据；2）分离姿态信息和形状信息于目标网格；3）应用于不同顶点数的网格上。我们因此提出了一种新的弱型监督基点方法来解决这些挑战。我们使用不同顶点数的网格上的 topology-agnostic 基点检测器，并使用 inverse kinematics 计算源和目标网格之间的变换。我们的方法只需要监督基点，可以应用于不同顶点数的网格上，并且具有形状不变的特性，允许从目标网格中提取姿态信息而不是形状信息。我们进一步设计了一种自我监督的循环重建来实现无监督 pose transfer，不需要与target和source网格具有同样的姿态和形状的ground truth扭曲网格。我们在人类和动物数据集上评估了我们的方法，与无监督方法相比，我们达到了更高的性能，甚至与完全监督方法相比具有相似的性能。我们在更加具有挑战性的 Mixamo 数据集上进行了测试，以验证我们的方法可以处理不同顶点数的网格和复杂的衣服。cross-dataset评估还表明了我们的方法具有强大的泛化能力。
</details></li>
</ul>
<hr>
<h2 id="An-Explainable-Model-Agnostic-Algorithm-for-CNN-based-Biometrics-Verification"><a href="#An-Explainable-Model-Agnostic-Algorithm-for-CNN-based-Biometrics-Verification" class="headerlink" title="An Explainable Model-Agnostic Algorithm for CNN-based Biometrics Verification"></a>An Explainable Model-Agnostic Algorithm for CNN-based Biometrics Verification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13428">http://arxiv.org/abs/2307.13428</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fernando Alonso-Fernandez, Kevin Hernandez-Diaz, Jose M. Buades, Prayag Tiwari, Josef Bigun</li>
<li>for: 这 paper 描述了对生物ometric验证设定下进行 Local Interpretable Model-Agnostic Explanations (LIME) AI 方法的适应。</li>
<li>methods: 这 paper 使用了对生物ometric验证设定下的两个 CNN 模型（基于 MobileNetv2 和 ResNet50），通过对输入图像的干扰版本的特征向量之间的高 Cosine 相似性来实现解释性。</li>
<li>results: 这 paper 实现了对 face biometrics 中的两个 CNN 模型（基于 MobileNetv2 和 ResNet50）的解释性。<details>
<summary>Abstract</summary>
This paper describes an adaptation of the Local Interpretable Model-Agnostic Explanations (LIME) AI method to operate under a biometric verification setting. LIME was initially proposed for networks with the same output classes used for training, and it employs the softmax probability to determine which regions of the image contribute the most to classification. However, in a verification setting, the classes to be recognized have not been seen during training. In addition, instead of using the softmax output, face descriptors are usually obtained from a layer before the classification layer. The model is adapted to achieve explainability via cosine similarity between feature vectors of perturbated versions of the input image. The method is showcased for face biometrics with two CNN models based on MobileNetv2 and ResNet50.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-signal-processing-interpretation-of-noise-reduction-convolutional-neural-networks"><a href="#A-signal-processing-interpretation-of-noise-reduction-convolutional-neural-networks" class="headerlink" title="A signal processing interpretation of noise-reduction convolutional neural networks"></a>A signal processing interpretation of noise-reduction convolutional neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13425">http://arxiv.org/abs/2307.13425</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luis A. Zavala-Mondragón, Peter H. N. de With, Fons van der Sommen</li>
<li>for: 这篇论文旨在提供一种统一的理论框架，用于解释深度卷积神经网络（Encoding-Decoding CNNs）的内部工作机制，并且可以帮助设计更加有效率的新型神经网络架构。</li>
<li>methods: 这篇论文使用了深度卷积神经网络的基本原理，以及信号处理领域的基本概念，将其与深度学习领域的研究相结合，以建立一个自 contenido的理论框架。</li>
<li>results: 这篇论文通过这种新的理论框架，可以帮助理解深度卷积神经网络的内部工作机制，并且可以用于设计更加有效率的新型神经网络架构。<details>
<summary>Abstract</summary>
Encoding-decoding CNNs play a central role in data-driven noise reduction and can be found within numerous deep-learning algorithms. However, the development of these CNN architectures is often done in ad-hoc fashion and theoretical underpinnings for important design choices is generally lacking. Up to this moment there are different existing relevant works that strive to explain the internal operation of these CNNs. Still, these ideas are either scattered and/or may require significant expertise to be accessible for a bigger audience. In order to open up this exciting field, this article builds intuition on the theory of deep convolutional framelets and explains diverse ED CNN architectures in a unified theoretical framework. By connecting basic principles from signal processing to the field of deep learning, this self-contained material offers significant guidance for designing robust and efficient novel CNN architectures.
</details>
<details>
<summary>摘要</summary>
encoding-decoding CNNs 在数据驱动噪声reduction中扮演中心角色，可以在多种深度学习算法中找到。然而， develop these CNN architectures 通常是done in ad-hoc fashion，lacking theoretical underpinnings for important design choices。 Until now, there are different existing relevant works that strive to explain the internal operation of these CNNs，but these ideas are either scattered and/or may require significant expertise to be accessible for a bigger audience。 In order to open up this exciting field, this article builds intuition on the theory of deep convolutional framelets and explains diverse ED CNN architectures in a unified theoretical framework。By connecting basic principles from signal processing to the field of deep learning，this self-contained material offers significant guidance for designing robust and efficient novel CNN architectures。Here's the text with the traditional Chinese characters:Encoding-Decoding CNNs 在数据驱动噪音reduction中扮演中心角色，可以在多种深度学习算法中找到。然而，开发这些CNN架构通常是done in ad-hoc fashion，lacking theoretical underpinnings for important design choices。 Until now, there are different existing relevant works that strive to explain the internal operation of these CNNs，but these ideas are either scattered and/or may require significant expertise to be accessible for a bigger audience。 In order to open up this exciting field, this article builds intuition on the theory of deep convolutional framelets and explains diverse ED CNN architectures in a unified theoretical framework。By connecting basic principles from signal processing to the field of deep learning，this self-contained material offers significant guidance for designing robust and efficient novel CNN architectures。
</details></li>
</ul>
<hr>
<h2 id="Mitigating-Memory-Wall-Effects-in-CNN-Engines-with-On-the-Fly-Weights-Generation"><a href="#Mitigating-Memory-Wall-Effects-in-CNN-Engines-with-On-the-Fly-Weights-Generation" class="headerlink" title="Mitigating Memory Wall Effects in CNN Engines with On-the-Fly Weights Generation"></a>Mitigating Memory Wall Effects in CNN Engines with On-the-Fly Weights Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13412">http://arxiv.org/abs/2307.13412</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stylianos I. Venieris, Javier Fernandez-Marques, Nicholas D. Lane</li>
<li>for: 这个研究是为了提高FPGA上的深度学习运算效率和能效性。</li>
<li>methods: 本研究使用了一种叫做”on-the-fly”的方法，通过在 runtime 中预先将矩阵扩展为较大的矩阵，以提高内存系结的效率。此外，研究者还提出了一个自动对应硬件-软件架构的方法，以提高精度和效率的平衡。</li>
<li>results: 研究结果显示，提案的框架可以实现2.57倍的效率提升比高性能的GPU设计，并且可以实现3.94倍的高效率数据频谱比较一般的FPGA-based CNN加速器。<details>
<summary>Abstract</summary>
The unprecedented accuracy of convolutional neural networks (CNNs) across a broad range of AI tasks has led to their widespread deployment in mobile and embedded settings. In a pursuit for high-performance and energy-efficient inference, significant research effort has been invested in the design of FPGA-based CNN accelerators. In this context, single computation engines constitute a popular approach to support diverse CNN modes without the overhead of fabric reconfiguration. Nevertheless, this flexibility often comes with significantly degraded performance on memory-bound layers and resource underutilisation due to the suboptimal mapping of certain layers on the engine's fixed configuration. In this work, we investigate the implications in terms of CNN engine design for a class of models that introduce a pre-convolution stage to decompress the weights at run time. We refer to these approaches as on-the-fly. This paper presents unzipFPGA, a novel CNN inference system that counteracts the limitations of existing CNN engines. The proposed framework comprises a novel CNN hardware architecture that introduces a weights generator module that enables the on-chip on-the-fly generation of weights, alleviating the negative impact of limited bandwidth on memory-bound layers. We further enhance unzipFPGA with an automated hardware-aware methodology that tailors the weights generation mechanism to the target CNN-device pair, leading to an improved accuracy-performance balance. Finally, we introduce an input selective processing element (PE) design that balances the load between PEs in suboptimally mapped layers. The proposed framework yields hardware designs that achieve an average of 2.57x performance efficiency gain over highly optimised GPU designs for the same power constraints and up to 3.94x higher performance density over a diverse range of state-of-the-art FPGA-based CNN accelerators.
</details>
<details>
<summary>摘要</summary>
“ convolutional neural networks (CNNs) 在各种人工智能任务中的无前例精度，使得它们在 mobil 和嵌入式设定中广泛应用。为了实现高性能和能效的推察，研究人员对 FPGA 基于 CNN 加速器的设计进行了很大的投入。在这个上下文中，单一 computation engine 成为了广泛使用的方法，以支持多种 CNN 模式，而不需要组织预设的组件重新配置。然而，这种灵活性通常会带来内存维护层的性能下降和资源处理不当用，从而导致某些层的对应不佳。在这个研究中，我们调查了这种问题的影响，并提出了一个 novel CNN 推察系统，称为 unzipFPGA。这个架构包括一个新的 CNN 硬件架构，其中包括一个可以在 run time 中实现 weights 的生成 module，以解决由限制的带宽所导致的负面影响。我们还将 unzipFPGA 扩展到一个自动化的硬件感知方法，以适应目标 CNN-device 组合，从而获得更好的精度-性能平衡。最后，我们引入了一个输入选择处理元素 (PE) 设计，以对不同的 PE 进行负载均衡。提案的架构可以实现与高度优化的 GPU 设计相同的性能效率，同时具有更高的性能密度。”
</details></li>
</ul>
<hr>
<h2 id="Scoring-Cycling-Environments-Perceived-Safety-using-Pairwise-Image-Comparisons"><a href="#Scoring-Cycling-Environments-Perceived-Safety-using-Pairwise-Image-Comparisons" class="headerlink" title="Scoring Cycling Environments Perceived Safety using Pairwise Image Comparisons"></a>Scoring Cycling Environments Perceived Safety using Pairwise Image Comparisons</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13397">http://arxiv.org/abs/2307.13397</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mncosta/scoring_pairwise">https://github.com/mncosta/scoring_pairwise</a></li>
<li>paper_authors: Miguel Costa, Manuel Marques, Felix Wilhelm Siebert, Carlos Lima Azevedo, Filipe Moura</li>
<li>for: 这个研究旨在探讨如何分析和理解人们对自行车安全性的感受，以及城市环境和自行车情况对这种感受的影响。</li>
<li>methods: 这个研究使用了对实际图像的评估，让受试者选择他们认为更安全的自行车环境。研究还使用了多种对比方法来评估自行车环境的安全性。</li>
<li>results: 研究发现，城市环境和自行车情况对人们对自行车安全性的感受产生了重要影响。这种方法可以帮助城市规划师设计更有效的措施，以促进自行车模式的普及。此外，这种方法可以continuously评估自行车环境的改进，并快速评估措施的效果。<details>
<summary>Abstract</summary>
Today, many cities seek to transition to more sustainable transportation systems. Cycling is critical in this transition for shorter trips, including first-and-last-mile links to transit. Yet, if individuals perceive cycling as unsafe, they will not cycle and choose other transportation modes. This study presents a novel approach to identifying how the perception of cycling safety can be analyzed and understood and the impact of the built environment and cycling contexts on such perceptions. We base our work on other perception studies and pairwise comparisons, using real-world images to survey respondents. We repeatedly show respondents two road environments and ask them to select the one they perceive as safer for cycling. We compare several methods capable of rating cycling environments from pairwise comparisons and classify cycling environments perceived as safe or unsafe. Urban planning can use this score to improve interventions' effectiveness and improve cycling promotion campaigns. Furthermore, this approach facilitates the continuous assessment of changing cycling environments, allows for a short-term evaluation of measures, and is efficiently deployed in different locations or contexts.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Towards-Unifying-Anatomy-Segmentation-Automated-Generation-of-a-Full-body-CT-Dataset-via-Knowledge-Aggregation-and-Anatomical-Guidelines"><a href="#Towards-Unifying-Anatomy-Segmentation-Automated-Generation-of-a-Full-body-CT-Dataset-via-Knowledge-Aggregation-and-Anatomical-Guidelines" class="headerlink" title="Towards Unifying Anatomy Segmentation: Automated Generation of a Full-body CT Dataset via Knowledge Aggregation and Anatomical Guidelines"></a>Towards Unifying Anatomy Segmentation: Automated Generation of a Full-body CT Dataset via Knowledge Aggregation and Anatomical Guidelines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13375">http://arxiv.org/abs/2307.13375</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alexanderjaus/atlasdataset">https://github.com/alexanderjaus/atlasdataset</a></li>
<li>paper_authors: Alexander Jaus, Constantin Seibold, Kelsey Hermann, Alexandra Walter, Kristina Giske, Johannes Haubold, Jens Kleesiek, Rainer Stiefelhagen</li>
<li>for: 本研究开发了一种自动生成医学影像分割数据集的方法，使用nnU-Net基于pseudo标签和医学指导pseudo标签纠正。</li>
<li>methods: 本方法首先使用nnU-Net进行pseudo标签生成，然后通过结合多个碎片化知识库，生成了一个涵盖整个人体CT扫描图的142个小块级标签数据集，并得到了专家认可。</li>
<li>results: 我们的方法不需要手动标注数据，并在BTCV数据集上达到85%的dice分数。此外，我们还对数据集进行了可扩展的自动检查和高质量专家检查，以确保数据集的可靠性和医学有效性。<details>
<summary>Abstract</summary>
In this study, we present a method for generating automated anatomy segmentation datasets using a sequential process that involves nnU-Net-based pseudo-labeling and anatomy-guided pseudo-label refinement. By combining various fragmented knowledge bases, we generate a dataset of whole-body CT scans with $142$ voxel-level labels for 533 volumes providing comprehensive anatomical coverage which experts have approved. Our proposed procedure does not rely on manual annotation during the label aggregation stage. We examine its plausibility and usefulness using three complementary checks: Human expert evaluation which approved the dataset, a Deep Learning usefulness benchmark on the BTCV dataset in which we achieve 85% dice score without using its training dataset, and medical validity checks. This evaluation procedure combines scalable automated checks with labor-intensive high-quality expert checks. Besides the dataset, we release our trained unified anatomical segmentation model capable of predicting $142$ anatomical structures on CT data.
</details>
<details>
<summary>摘要</summary>
在这个研究中，我们提出了一种方法用于自动生成医学影像分割数据集，该方法包括基于nnU-Net的假标注和骨化假标注纠正。通过将各种碎片化知识库集成起来，我们生成了一个整体CT扫描图像的数据集，包含142个块级标签，对533个卷积提供了全面的解剖学覆盖。我们的提posed方法不需要手动标注 durante el etiquetado de etiquetas stage。我们使用三种 complementary checks to evaluate the plausibility and usefulness of our method: expert evaluation by human, deep learning usefulness benchmark on the BTCV dataset, and medical validity checks. This evaluation procedure combines scalable automated checks with labor-intensive high-quality expert checks. In addition to the dataset, we release our trained unified anatomical segmentation model, capable of predicting 142 anatomical structures on CT data.
</details></li>
</ul>
<hr>
<h2 id="Kefa-A-Knowledge-Enhanced-and-Fine-grained-Aligned-Speaker-for-Navigation-Instruction-Generation"><a href="#Kefa-A-Knowledge-Enhanced-and-Fine-grained-Aligned-Speaker-for-Navigation-Instruction-Generation" class="headerlink" title="Kefa: A Knowledge Enhanced and Fine-grained Aligned Speaker for Navigation Instruction Generation"></a>Kefa: A Knowledge Enhanced and Fine-grained Aligned Speaker for Navigation Instruction Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13368">http://arxiv.org/abs/2307.13368</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/haitianzeng/KEFA">https://github.com/haitianzeng/KEFA</a></li>
<li>paper_authors: Haitian Zeng, Xiaohan Wang, Wenguan Wang, Yi Yang</li>
<li>for: 提高视力语言导航中的导航指令生成性能</li>
<li>methods: 提出了知识更新模块和适应时间Alignment方法，以强化特征表示和精细对齐生成的指令和观察序列</li>
<li>results: 在R2R和UrbanWalk数据集上实现了视力语言导航中的导航指令生成性能的状态作者<details>
<summary>Abstract</summary>
We introduce a novel speaker model \textsc{Kefa} for navigation instruction generation. The existing speaker models in Vision-and-Language Navigation suffer from the large domain gap of vision features between different environments and insufficient temporal grounding capability. To address the challenges, we propose a Knowledge Refinement Module to enhance the feature representation with external knowledge facts, and an Adaptive Temporal Alignment method to enforce fine-grained alignment between the generated instructions and the observation sequences. Moreover, we propose a new metric SPICE-D for navigation instruction evaluation, which is aware of the correctness of direction phrases. The experimental results on R2R and UrbanWalk datasets show that the proposed KEFA speaker achieves state-of-the-art instruction generation performance for both indoor and outdoor scenes.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种新的说话模型，即KEFA模型，用于生成导航指令。现有的视语导航中的说话模型受到不同环境中视觉特征的域外差和时间固定不足的挑战。为解决这些挑战，我们提出了知识精化模块，用于增强特征表示，以及适应时间对齐方法，用于确保生成的指令和观察序列之间的细腻对齐。此外，我们提出了一个新的评价指标SPICE-D，用于评价导航指令的正确性。实验结果表明，我们提出的KEFA说话模型在R2R和UrbanWalk数据集上实现了导航指令生成性能的州际之最。
</details></li>
</ul>
<hr>
<h2 id="3DRP-Net-3D-Relative-Position-aware-Network-for-3D-Visual-Grounding"><a href="#3DRP-Net-3D-Relative-Position-aware-Network-for-3D-Visual-Grounding" class="headerlink" title="3DRP-Net: 3D Relative Position-aware Network for 3D Visual Grounding"></a>3DRP-Net: 3D Relative Position-aware Network for 3D Visual Grounding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13363">http://arxiv.org/abs/2307.13363</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zehan Wang, Haifeng Huang, Yang Zhao, Linjun Li, Xize Cheng, Yichen Zhu, Aoxiong Yin, Zhou Zhao</li>
<li>for: 本研究旨在使用自由语言描述将3D点云中的目标对象Localize。</li>
<li>methods: 我们提出了一种关注关系的一stage框架，名为3D相对位置感知网络（3DRP-Net），可以有效地捕捉对象之间的相对空间关系并增强对象特征。</li>
<li>results: 我们的方法在三个 benchmark（ScanRefer和Nr3D&#x2F;Sr3D）中的总体性能比所有现状最佳方法高。<details>
<summary>Abstract</summary>
3D visual grounding aims to localize the target object in a 3D point cloud by a free-form language description. Typically, the sentences describing the target object tend to provide information about its relative relation between other objects and its position within the whole scene. In this work, we propose a relation-aware one-stage framework, named 3D Relative Position-aware Network (3DRP-Net), which can effectively capture the relative spatial relationships between objects and enhance object attributes. Specifically, 1) we propose a 3D Relative Position Multi-head Attention (3DRP-MA) module to analyze relative relations from different directions in the context of object pairs, which helps the model to focus on the specific object relations mentioned in the sentence. 2) We designed a soft-labeling strategy to alleviate the spatial ambiguity caused by redundant points, which further stabilizes and enhances the learning process through a constant and discriminative distribution. Extensive experiments conducted on three benchmarks (i.e., ScanRefer and Nr3D/Sr3D) demonstrate that our method outperforms all the state-of-the-art methods in general. The source code will be released on GitHub.
</details>
<details>
<summary>摘要</summary>
三维视觉定位是目标对象在三维点云中的地址定位，通常通过自由形式的语言描述。这些句子通常提供对目标对象的相对关系和场景中的位置信息。在这种工作中，我们提议一种关注相对关系的一stage框架，名为三维相对位置感知网络（3DRP-Net），可以有效地捕捉对象之间的相对空间关系并增强对象特征。Specifically，我们提出了一个三维相对位置多头注意模块（3DRP-MA）来分析对象之间的相对关系，从不同方向上在对象对中分析这些关系，以帮助模型专注于文本中提到的特定对象关系。另外，我们设计了一种软标注策略，以解决由重复点所引起的空间歧义，从而使模型更加稳定和精准。我们在三个 benchmark（即ScanRefer和Nr3D/Sr3D）进行了广泛的实验，结果显示，我们的方法在总体上超过了所有现有的方法。我们将代码发布到 GitHub。
</details></li>
</ul>
<hr>
<h2 id="Of-Mice-and-Pose-2D-Mouse-Pose-Estimation-from-Unlabelled-Data-and-Synthetic-Prior"><a href="#Of-Mice-and-Pose-2D-Mouse-Pose-Estimation-from-Unlabelled-Data-and-Synthetic-Prior" class="headerlink" title="Of Mice and Pose: 2D Mouse Pose Estimation from Unlabelled Data and Synthetic Prior"></a>Of Mice and Pose: 2D Mouse Pose Estimation from Unlabelled Data and Synthetic Prior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13361">http://arxiv.org/abs/2307.13361</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jose Sosa, Sharn Perry, Jane Alty, David Hogg</li>
<li>for: 这paper是为了解决动物Behavior tracking和量度的问题，尤其是在ecology, biology和neuroscience等领域，因为大量的动物记录数据已经被生成，但是一些计算机视觉技术无法利用这些数据，因为缺乏标注。</li>
<li>methods: 我们提出了一种使用无标注图像来估算鼠体姿 pose的方法，基于最近的自动学习人体 pose估算方法，使用单张图像和一组无对应的2Dpose图像，通过GAN框架来生成empirical prior of 2Dpose。我们适应了这种方法到鼠的肢体结构，并生成了synthetic 3D鼠模型来生成empirical prior。</li>
<li>results: 我们在一个新的鼠视频数据集上进行了实验，并与手动获取的ground truth进行比较。我们还与一种已有的supervised state-of-the-art方法进行比较，并显示了Promising results，即使没有paired training data。此外，我们还使用了一个马图像集来展示这种设置的潜在应用性。<details>
<summary>Abstract</summary>
Numerous fields, such as ecology, biology, and neuroscience, use animal recordings to track and measure animal behaviour. Over time, a significant volume of such data has been produced, but some computer vision techniques cannot explore it due to the lack of annotations. To address this, we propose an approach for estimating 2D mouse body pose from unlabelled images using a synthetically generated empirical pose prior. Our proposal is based on a recent self-supervised method for estimating 2D human pose that uses single images and a set of unpaired typical 2D poses within a GAN framework. We adapt this method to the limb structure of the mouse and generate the empirical prior of 2D poses from a synthetic 3D mouse model, thereby avoiding manual annotation. In experiments on a new mouse video dataset, we evaluate the performance of the approach by comparing pose predictions to a manually obtained ground truth. We also compare predictions with those from a supervised state-of-the-art method for animal pose estimation. The latter evaluation indicates promising results despite the lack of paired training data. Finally, qualitative results using a dataset of horse images show the potential of the setting to adapt to other animal species.
</details>
<details>
<summary>摘要</summary>
许多领域，如生态学、生物学和神经科学，通过动物记录跟踪和测量动物行为。随着时间的推移，这些数据的量已经很大，但一些计算机视觉技术无法探索它们，因为缺乏注释。为解决这个问题，我们提出了一种方法，通过使用生成的经验性姿势先验来估算无注释图像中的2D鼠体姿势。我们的提议基于最近的自动适应人体 pose 估算方法，该方法使用单张图像和一组无对应的2D姿势集来生成一个GAN框架中的经验性姿势先验。我们适应了鼠的四肢结构，并生成了一个synthetic 3D鼠模型中的经验性姿势先验，从而避免手动注释。在一个新的鼠视频数据集上进行了实验，我们评估了方法的性能，并与一种已有的supervised状态的动物pose估算方法进行比较。后者的评估结果表明，我们的方法在缺乏对应数据的情况下可以获得承诺的结果。此外，使用一个马图像集来表征其他动物种类的可能性的质性结果也提供了。
</details></li>
</ul>
<hr>
<h2 id="Prior-Based-Online-Lane-Graph-Extraction-from-Single-Onboard-Camera-Image"><a href="#Prior-Based-Online-Lane-Graph-Extraction-from-Single-Onboard-Camera-Image" class="headerlink" title="Prior Based Online Lane Graph Extraction from Single Onboard Camera Image"></a>Prior Based Online Lane Graph Extraction from Single Onboard Camera Image</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13344">http://arxiv.org/abs/2307.13344</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ybarancan/lanewae">https://github.com/ybarancan/lanewae</a></li>
<li>paper_authors: Yigit Baran Can, Alexander Liniger, Danda Pani Paudel, Luc Van Gool</li>
<li>for: 本研究旨在提供一种在线 Bird’s-Eye-View 车道图生成方法，以便普遍和可靠地自动驾驶。</li>
<li>methods: 该方法使用优先信息提高估计质量。优先信息通过一种基于 transformer 的 Wasserstein 自编码器从数据集提取。然后，自编码器用于提高初始车道图估计。这是通过 latent 空间向量优化来实现的，该优化劝说车道图估计与优先信息保持一致。</li>
<li>results: 对 NuScenes 和 Argoverse 两个标准数据集进行测试，结果显示提议方法与现有方法相比有显著改善。<details>
<summary>Abstract</summary>
The local road network information is essential for autonomous navigation. This information is commonly obtained from offline HD-Maps in terms of lane graphs. However, the local road network at a given moment can be drastically different than the one given in the offline maps; due to construction works, accidents etc. Moreover, the autonomous vehicle might be at a location not covered in the offline HD-Map. Thus, online estimation of the lane graph is crucial for widespread and reliable autonomous navigation. In this work, we tackle online Bird's-Eye-View lane graph extraction from a single onboard camera image. We propose to use prior information to increase quality of the estimations. The prior is extracted from the dataset through a transformer based Wasserstein Autoencoder. The autoencoder is then used to enhance the initial lane graph estimates. This is done through optimization of the latent space vector. The optimization encourages the lane graph estimation to be logical by discouraging it to diverge from the prior distribution. We test the method on two benchmark datasets, NuScenes and Argoverse. The results show that the proposed method significantly improves the performance compared to state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
地方路网信息是自动导航的关键。这些信息通常来自于离线高级地图，表示为几何图。然而，当地方路网在给定时刻发生重大变化，比如建筑工程或意外等，那么离线地图中提供的信息可能不准确。此外，自动汽车可能位于离线地图中没有覆盖的位置。因此，在线计算路网图是自动导航的关键。在这种情况下，我们解决了在单个摄像头图像上进行在线鸟瞰视图lane图Estimation。我们提议使用先前信息来提高估计质量。这些先前信息通过一种基于trasnformer的 Wasserstein Autoencoder提取于dataset中。然后，这种autoencoder用于提高初始lane图估计。这是通过对潜在空间向量进行优化来实现的，这种优化抑制了lane图估计与先前分布的偏离。我们对NuScenes和Argoverse两个标准数据集进行测试，结果表明，我们提posed方法与当前最佳方法相比，有 significan improvement。
</details></li>
</ul>
<hr>
<h2 id="Overcoming-Distribution-Mismatch-in-Quantizing-Image-Super-Resolution-Networks"><a href="#Overcoming-Distribution-Mismatch-in-Quantizing-Image-Super-Resolution-Networks" class="headerlink" title="Overcoming Distribution Mismatch in Quantizing Image Super-Resolution Networks"></a>Overcoming Distribution Mismatch in Quantizing Image Super-Resolution Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13337">http://arxiv.org/abs/2307.13337</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheeun Hong, Kyoung Mu Lee<br>for:* This paper aims to address the distribution mismatch problem in image super-resolution (SR) networks, which can lead to severe accuracy loss when using low-bit quantization.methods:* The proposed method, called ODM, uses a new quantization-aware training framework that regularizes the variance in features during training to reduce the distribution mismatch problem.* ODM also introduces distribution offsets to layers with a significant mismatch, which either scales or shifts channel-wise features.results:* ODM effectively outperforms existing SR quantization approaches with similar or fewer computations, demonstrating the importance of reducing the distribution mismatch problem.<details>
<summary>Abstract</summary>
Quantization is a promising approach to reduce the high computational complexity of image super-resolution (SR) networks. However, compared to high-level tasks like image classification, low-bit quantization leads to severe accuracy loss in SR networks. This is because feature distributions of SR networks are significantly divergent for each channel or input image, and is thus difficult to determine a quantization range. Existing SR quantization works approach this distribution mismatch problem by dynamically adapting quantization ranges to the variant distributions during test time. However, such dynamic adaptation incurs additional computational costs that limit the benefits of quantization. Instead, we propose a new quantization-aware training framework that effectively Overcomes the Distribution Mismatch problem in SR networks without the need for dynamic adaptation. Intuitively, the mismatch can be reduced by directly regularizing the variance in features during training. However, we observe that variance regularization can collide with the reconstruction loss during training and adversely impact SR accuracy. Thus, we avoid the conflict between two losses by regularizing the variance only when the gradients of variance regularization are cooperative with that of reconstruction. Additionally, to further reduce the distribution mismatch, we introduce distribution offsets to layers with a significant mismatch, which either scales or shifts channel-wise features. Our proposed algorithm, called ODM, effectively reduces the mismatch in distributions with minimal computational overhead. Experimental results show that ODM effectively outperforms existing SR quantization approaches with similar or fewer computations, demonstrating the importance of reducing the distribution mismatch problem. Our code is available at https://github.com/Cheeun/ODM.
</details>
<details>
<summary>摘要</summary>
量化是一种有前途的方法，可以降低图像超分辨率网络的计算复杂性。然而，相比高级任务如图像分类，低位数量化会导致SR网络的准确性丢失。这是因为SR网络的特征分布非常分散，难以确定量化范围。现有的SR量化方法会在测试时动态适应量化范围，以适应变化的特征分布。然而，这种动态适应带来额外的计算成本，限制了量化的优点。相反，我们提出了一个新的量化意识训练框架，可以有效地超越分布匹配问题。我们发现，可以在训练时直接规范特征变量，以减少分布匹配问题。然而，我们发现，变量规范可能会与重建损失冲突，影响SR准确性。因此，我们避免了这两个损失之间的冲突，通过在重建损失的梯度下规范变量。此外，为了进一步减少分布匹配问题，我们引入了分布偏移，以调整不同特征的分布。我们提出的ODM算法，可以有效地减少分布匹配问题，而且计算成本很低。实验结果表明，ODM可以有效地超越现有的SR量化方法，并且需要相同或更少的计算资源，这说明了分布匹配问题的重要性。我们的代码可以在https://github.com/Cheeun/ODM上获取。
</details></li>
</ul>
<hr>
<h2 id="Unmasking-Anomalies-in-Road-Scene-Segmentation"><a href="#Unmasking-Anomalies-in-Road-Scene-Segmentation" class="headerlink" title="Unmasking Anomalies in Road-Scene Segmentation"></a>Unmasking Anomalies in Road-Scene Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13316">http://arxiv.org/abs/2307.13316</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shyam671/Mask2Anomaly-Unmasking-Anomalies-in-Road-Scene-Segmentation">https://github.com/shyam671/Mask2Anomaly-Unmasking-Anomalies-in-Road-Scene-Segmentation</a></li>
<li>paper_authors: Shyam Nandan Rai, Fabio Cermelli, Dario Fontanel, Carlo Masone, Barbara Caputo</li>
<li>for: 这篇论文主要目标是提高道路Scene anomaly detection的精度。</li>
<li>methods: 该方法基于mask classification的思想，并包括全球封装注意力模块、mask contrastive learning和面精度提高等技术新特性。</li>
<li>results: 该方法在多个benchmark上达到了新的状态方法，特别是在每个像素和组件级别评估中减少了false positives率60%。<details>
<summary>Abstract</summary>
Anomaly segmentation is a critical task for driving applications, and it is approached traditionally as a per-pixel classification problem. However, reasoning individually about each pixel without considering their contextual semantics results in high uncertainty around the objects' boundaries and numerous false positives. We propose a paradigm change by shifting from a per-pixel classification to a mask classification. Our mask-based method, Mask2Anomaly, demonstrates the feasibility of integrating an anomaly detection method in a mask-classification architecture. Mask2Anomaly includes several technical novelties that are designed to improve the detection of anomalies in masks: i) a global masked attention module to focus individually on the foreground and background regions; ii) a mask contrastive learning that maximizes the margin between an anomaly and known classes; and iii) a mask refinement solution to reduce false positives. Mask2Anomaly achieves new state-of-the-art results across a range of benchmarks, both in the per-pixel and component-level evaluations. In particular, Mask2Anomaly reduces the average false positives rate by 60% wrt the previous state-of-the-art. Github page: https://github.com/shyam671/Mask2Anomaly-Unmasking-Anomalies-in-Road-Scene-Segmentation.
</details>
<details>
<summary>摘要</summary>
traditional driving application中的异常分割问题是一个关键任务，通常是以每个像素为单位进行分类的。然而，不考虑每个像素的语义上下文会导致对象边界的高度不确定性和多个假阳性。我们提议一种思路转变，即从每个像素分类转变为Mask分类。我们的Mask2异常方法在Mask分类架构中实现了异常检测方法的集成。Mask2异常包括了一些技术创新，用于改进异常检测在Mask中的精度：1. 全局掩码注意力模块，用于对前景和背景区域进行各自焦点处理。2. 掩码对比学习，以提高异常和已知类之间的边界差距。3. 掩码修正解决方案，以减少假阳性。Mask2异常实现了新的状态anner-of-the-art结果，并在像素级和组件级评估中都达到了新的高度。具体来说，Mask2异常相比前一个状态anner-of-the-art，减少了平均假阳性率60%。GitHub页面：https://github.com/shyam671/Mask2Anomaly-Unmasking-Anomalies-in-Road-Scene-Segmentation。
</details></li>
</ul>
<hr>
<h2 id="Mitigating-Cross-client-GANs-based-Attack-in-Federated-Learning"><a href="#Mitigating-Cross-client-GANs-based-Attack-in-Federated-Learning" class="headerlink" title="Mitigating Cross-client GANs-based Attack in Federated Learning"></a>Mitigating Cross-client GANs-based Attack in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13314">http://arxiv.org/abs/2307.13314</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hong Huang, Xinyu Lei, Tao Xiang</li>
<li>For: The paper aims to improve the security of federated learning (FL) schemes by mitigating the cross-client generative adversarial networks (GANs) attack, which can reconstruct samples from other clients.* Methods: The paper proposes a technique called Federated Ensemble Data-free Knowledge Distillation (Fed-EDKD) to resist the C-GANs attack. Fed-EDKD involves each client submitting a local model to the server for obtaining an ensemble global model, and then using data-free knowledge distillation techniques to transfer knowledge from the ensemble global model to a compressed model.* Results: The experimental results demonstrate that Fed-EDKD significantly mitigates the C-GANs attack while only incurring a slight accuracy degradation of FL.Here are the three key points in Simplified Chinese text:* For: 该论文目标是提高联合学习（FL）方案的安全性，防止跨客户端生成对抗网络（GANs）攻击，该攻击可以从其他客户端中重建样本。* Methods: 该论文提出了联合集成数据自由知识传播技术（Fed-EDKD）来防止GANs攻击。Fed-EDKD方法是每个客户端将本地模型提交到服务器，从服务器获取ensemble全球模型，然后使用数据自由知识传播技术将ensemble全球模型中的知识传播到压缩模型。* Results: 实验结果表明，Fed-EDKD有效防止GANs攻击，同时仅带来FL的精度下降。<details>
<summary>Abstract</summary>
Machine learning makes multimedia data (e.g., images) more attractive, however, multimedia data is usually distributed and privacy sensitive. Multiple distributed multimedia clients can resort to federated learning (FL) to jointly learn a global shared model without requiring to share their private samples with any third-party entities. In this paper, we show that FL suffers from the cross-client generative adversarial networks (GANs)-based (C-GANs) attack, in which a malicious client (i.e., adversary) can reconstruct samples with the same distribution as the training samples from other clients (i.e., victims). Since a benign client's data can be leaked to the adversary, this attack brings the risk of local data leakage for clients in many security-critical FL applications. Thus, we propose Fed-EDKD (i.e., Federated Ensemble Data-free Knowledge Distillation) technique to improve the current popular FL schemes to resist C-GANs attack. In Fed-EDKD, each client submits a local model to the server for obtaining an ensemble global model. Then, to avoid model expansion, Fed-EDKD adopts data-free knowledge distillation techniques to transfer knowledge from the ensemble global model to a compressed model. By this way, Fed-EDKD reduces the adversary's control capability over the global model, so Fed-EDKD can effectively mitigate C-GANs attack. Finally, the experimental results demonstrate that Fed-EDKD significantly mitigates C-GANs attack while only incurring a slight accuracy degradation of FL.
</details>
<details>
<summary>摘要</summary>
机器学习使 multimedia 数据更加吸引人，然而 multimedia 数据通常是分布式并且敏感。多个分布式 multimedia 客户可以使用联邦学习（FL）来共同学习全局共享模型，而不需要将私人样本分享给任何第三方机构。在这篇论文中，我们表明了 FL 受到跨客户生成 adversarial networks（GANs） Attack，在这种攻击中，一个邪恶客户（即敌对者）可以从其他客户（即受害者）中重建样本的分布。由于benign客户的数据可以被敌对者泄露，这种攻击可能导致客户端的本地数据泄露。因此，我们提出了 Fed-EDKD（即联邦ensemble数据free知识distillation）技术，以提高当前流行的 FL 方案，抵御 C-GANs 攻击。在 Fed-EDKD 中，每个客户提交本地模型到服务器，以获取ensemble全局模型。然后，为了避免模型扩展，Fed-EDKD 采用数据free知识distillation技术，将知识从ensemble全局模型传递到压缩模型。通过这种方式，Fed-EDKD 降低了敌对者对全局模型的控制能力，因此可以有效抵御 C-GANs 攻击。最后，实验结果表明，Fed-EDKD 可以有效抵御 C-GANs 攻击，仅受到轻度的 FL 减少。
</details></li>
</ul>
<hr>
<h2 id="CT-Net-Arbitrary-Shaped-Text-Detection-via-Contour-Transformer"><a href="#CT-Net-Arbitrary-Shaped-Text-Detection-via-Contour-Transformer" class="headerlink" title="CT-Net: Arbitrary-Shaped Text Detection via Contour Transformer"></a>CT-Net: Arbitrary-Shaped Text Detection via Contour Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13310">http://arxiv.org/abs/2307.13310</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiwen Shao, Yuchen Su, Yong Zhou, Fanrong Meng, Hancheng Zhu, Bing Liu, Rui Yao</li>
<li>for: 提出一种基于Contour Transformer的自然语言场景文本检测方法，以提高文本检测精度和效率。</li>
<li>methods: 使用Contour Initialization Module生成初始文本轮廓，并采用Contour Refinement Module进行反复调整文本轮廓，以 capture contextual information和进行进度性轮廓变换。还采用Adaptive Training Strategy和Re-score Mechanism等技术来提高模型的性能。</li>
<li>results: 经过EXTensive experiments on four challenging datasets，CT-Net表现出了较高的准确率和效率，比如CT-Net在CTW1500和Total-Text datasets上的F-measure分别达到了86.1和87.8。<details>
<summary>Abstract</summary>
Contour based scene text detection methods have rapidly developed recently, but still suffer from inaccurate frontend contour initialization, multi-stage error accumulation, or deficient local information aggregation. To tackle these limitations, we propose a novel arbitrary-shaped scene text detection framework named CT-Net by progressive contour regression with contour transformers. Specifically, we first employ a contour initialization module that generates coarse text contours without any post-processing. Then, we adopt contour refinement modules to adaptively refine text contours in an iterative manner, which are beneficial for context information capturing and progressive global contour deformation. Besides, we propose an adaptive training strategy to enable the contour transformers to learn more potential deformation paths, and introduce a re-score mechanism that can effectively suppress false positives. Extensive experiments are conducted on four challenging datasets, which demonstrate the accuracy and efficiency of our CT-Net over state-of-the-art methods. Particularly, CT-Net achieves F-measure of 86.1 at 11.2 frames per second (FPS) and F-measure of 87.8 at 10.1 FPS for CTW1500 and Total-Text datasets, respectively.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>近期，基于 kontour 的Scene文本检测方法有很大的发展，但仍然受到初始 kontour 的不准确、多Stage 的错误积累以及地方信息的不足等限制。为了解决这些局限性，我们提出了一种新的arbitrary-shaped Scene文本检测框架，即CT-Net，通过进行进度ive contour regression with contour transformers。具体来说，我们首先采用一种contour initialization module，该模块可以生成不需要任何后处理的粗糙文本 kontour。然后，我们采用 contour refinement module，该模块可以在循环方式下进行文本 kontour 的细化，以捕捉更多的上下文信息并进行进度ive global kontour 的变换。此外，我们还提出了一种适应性训练策略，使得 kontour transformers 可以学习更多的可能的变换路径，并引入了一种重新分配机制，可以有效地降低假阳性。我们在四个挑战性 datasets 上进行了广泛的实验，结果表明 CT-Net 的准确率和效率比现有方法高。特别是，CT-Net 在 CTW1500 和 Total-Text  datasets 上 achieved F-measure of 86.1 at 11.2 frames per second (FPS) and F-measure of 87.8 at 10.1 FPS, respectively.
</details></li>
</ul>
<hr>
<h2 id="Mini-PointNetPlus-a-local-feature-descriptor-in-deep-learning-model-for-3d-environment-perception"><a href="#Mini-PointNetPlus-a-local-feature-descriptor-in-deep-learning-model-for-3d-environment-perception" class="headerlink" title="Mini-PointNetPlus: a local feature descriptor in deep learning model for 3d environment perception"></a>Mini-PointNetPlus: a local feature descriptor in deep learning model for 3d environment perception</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13300">http://arxiv.org/abs/2307.13300</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chuanyu Luo, Nuo Cheng, Sikun Ma, Jun Xiang, Xiaohan Li, Shengguang Lei, Pu Li</li>
<li>for: The paper is written for improving the performance of deep learning models for 3D environment perception, specifically by proposing a novel local feature descriptor called mini-PointNetPlus.</li>
<li>methods: The paper uses pillarization&#x2F;voxelization methods to convert point cloud data into pillars&#x2F;voxels, and then applies a 2D&#x2F;3D convolutional neural network (CNN) to process the data. The proposed descriptor, mini-PointNetPlus, separately projects the data points to individual features, leading to a permutation invariant and fully utilizing the features.</li>
<li>results: The proposed descriptor demonstrates a considerable performance improvement for 3D perception compared to the pioneer work PointNet, as proven in experiments.<details>
<summary>Abstract</summary>
Common deep learning models for 3D environment perception often use pillarization/voxelization methods to convert point cloud data into pillars/voxels and then process it with a 2D/3D convolutional neural network (CNN). The pioneer work PointNet has been widely applied as a local feature descriptor, a fundamental component in deep learning models for 3D perception, to extract features of a point cloud. This is achieved by using a symmetric max-pooling operator which provides unique pillar/voxel features. However, by ignoring most of the points, the max-pooling operator causes an information loss, which reduces the model performance. To address this issue, we propose a novel local feature descriptor, mini-PointNetPlus, as an alternative for plug-and-play to PointNet. Our basic idea is to separately project the data points to the individual features considered, each leading to a permutation invariant. Thus, the proposed descriptor transforms an unordered point cloud to a stable order. The vanilla PointNet is proved to be a special case of our mini-PointNetPlus. Due to fully utilizing the features by the proposed descriptor, we demonstrate in experiment a considerable performance improvement for 3D perception.
</details>
<details>
<summary>摘要</summary>
常用的深度学习模型 для 3D 环境识别常使用柱化/体积化方法将点云数据转换为柱/体积，然后使用 2D/3D 卷积神经网络（CNN）进行处理。点云网络（PointNet）是深度学习模型中的一个开创性的工作，广泛应用于当地特征描述器，用于提取点云特征。这是通过使用对称的最大汇聚操作来实现的，该操作提供了唯一的柱/体积特征。然而，由于忽略大多数点，最大汇聚操作会导致信息损失，从而降低模型性能。为解决这个问题，我们提出了一种新的本地特征描述器，mini-PointNetPlus，作为PointNet的替换。我们的基本想法是分别将数据点 proyect 到各自的特征上，每个特征带来一种排序不变的变换。因此，我们的描述器将无序点云转换为稳定的排序。vanilla PointNet 被证明是 mini-PointNetPlus 的特殊情况。由于完全利用特征，我们在实验中证明了使用我们的描述器可以获得3D 识别的显著性能提升。
</details></li>
</ul>
<hr>
<h2 id="High-Resolution-Volumetric-Reconstruction-for-Clothed-Humans"><a href="#High-Resolution-Volumetric-Reconstruction-for-Clothed-Humans" class="headerlink" title="High-Resolution Volumetric Reconstruction for Clothed Humans"></a>High-Resolution Volumetric Reconstruction for Clothed Humans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13282">http://arxiv.org/abs/2307.13282</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sicong Tang, Guangyuan Wang, Qing Ran, Lingzhi Li, Li Shen, Ping Tan</li>
<li>for: 重建人体形象 from 少量RGB图像</li>
<li>methods: 使用volume representation，3D convolution，coarse-to-fine strategy，voxel culling，subspace sparse convolution</li>
<li>results: 比state-of-the-art方法减少mean point-to-surface（P2S）精度 more than 50%，实现约2mm的准确性，并且图像从我们的文本模型中得到更高的PSNR值<details>
<summary>Abstract</summary>
We present a novel method for reconstructing clothed humans from a sparse set of, e.g., 1 to 6 RGB images. Despite impressive results from recent works employing deep implicit representation, we revisit the volumetric approach and demonstrate that better performance can be achieved with proper system design. The volumetric representation offers significant advantages in leveraging 3D spatial context through 3D convolutions, and the notorious quantization error is largely negligible with a reasonably large yet affordable volume resolution, e.g., 512. To handle memory and computation costs, we propose a sophisticated coarse-to-fine strategy with voxel culling and subspace sparse convolution. Our method starts with a discretized visual hull to compute a coarse shape and then focuses on a narrow band nearby the coarse shape for refinement. Once the shape is reconstructed, we adopt an image-based rendering approach, which computes the colors of surface points by blending input images with learned weights. Extensive experimental results show that our method significantly reduces the mean point-to-surface (P2S) precision of state-of-the-art methods by more than 50% to achieve approximately 2mm accuracy with a 512 volume resolution. Additionally, images rendered from our textured model achieve a higher peak signal-to-noise ratio (PSNR) compared to state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法，用于从稀疏的RGB图像集（例如1-6张）中重建披露人体。尽管最近的研究已经取得了很好的成果，我们还是返回到了Volume representation的方法，并证明了更好的性能可以通过合适的系统设计实现。Volume representation具有利用3D空间上下文的3D卷积的优势，而且量化误差在合理的卷积分辨率（例如512）下是极其忽略不起的。为了处理内存和计算成本，我们提议了一种复杂的粗化-细化策略，包括voxel culling和子空间稀疏卷积。我们的方法首先使用离散的视觉封顶来计算粗略的形状，然后将注意力集中在粗略形状附近进行细化。一旦形状重建完成，我们采用了基于图像的渲染方法，该方法通过权重混合输入图像来计算表面点的颜色。我们的实验结果表明，我们的方法可以在512卷积分辨率下将平均点到表面精度（P2S）降低至少于50%，并且图像从我们的纹理模型中获得的PSNR值高于状态艺术方法。
</details></li>
</ul>
<hr>
<h2 id="GaitFormer-Revisiting-Intrinsic-Periodicity-for-Gait-Recognition"><a href="#GaitFormer-Revisiting-Intrinsic-Periodicity-for-Gait-Recognition" class="headerlink" title="GaitFormer: Revisiting Intrinsic Periodicity for Gait Recognition"></a>GaitFormer: Revisiting Intrinsic Periodicity for Gait Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13259">http://arxiv.org/abs/2307.13259</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qian Wu, Ruixuan Xiao, Kaixin Xu, Jingcheng Ni, Boxun Li, Ziyao Xu</li>
<li>for: 本研究旨在提高人体步态识别精度，通过分析视频水平人体阴影，而不是仅仅依靠外观信息。</li>
<li>methods: 本研究提出了一种插件化策略，名为时间周期对齐策略（TPA），该策略利用人体步态序列中的周期性和细致的时间相关性，以提高识别性能。TPA策略包括两个关键组件：一是适应 Fourier-transform位编码（AFPE），该组件可适应地将特征和整数时钟信号转换为敏感于周期步态的嵌入。二是时间聚合模块（TAM），该组件可分解嵌入为趋势和季节性组分，并提取有用的时间相关性，以识别主要组分，而排除杂音噪声。</li>
<li>results: 我们基于TPA策略提出了一种简单有效的基eline方法，并在三个常用的公共数据集（CASIA-B、OU-MVLP、GREW）上进行了广泛的实验。结果表明，我们的提议方法在多个benchmark测试中达到了当前最佳性能。<details>
<summary>Abstract</summary>
Gait recognition aims to distinguish different walking patterns by analyzing video-level human silhouettes, rather than relying on appearance information. Previous research on gait recognition has primarily focused on extracting local or global spatial-temporal representations, while overlooking the intrinsic periodic features of gait sequences, which, when fully utilized, can significantly enhance performance. In this work, we propose a plug-and-play strategy, called Temporal Periodic Alignment (TPA), which leverages the periodic nature and fine-grained temporal dependencies of gait patterns. The TPA strategy comprises two key components. The first component is Adaptive Fourier-transform Position Encoding (AFPE), which adaptively converts features and discrete-time signals into embeddings that are sensitive to periodic walking patterns. The second component is the Temporal Aggregation Module (TAM), which separates embeddings into trend and seasonal components, and extracts meaningful temporal correlations to identify primary components, while filtering out random noise. We present a simple and effective baseline method for gait recognition, based on the TPA strategy. Extensive experiments conducted on three popular public datasets (CASIA-B, OU-MVLP, and GREW) demonstrate that our proposed method achieves state-of-the-art performance on multiple benchmark tests.
</details>
<details>
<summary>摘要</summary>
走姿识别目标是通过分析视频级别的人体擦抹图来分辨不同的步态模式，而不是仅仅依靠外观信息。过去的研究中，大多数关于走姿识别的研究都是提取局部或全局的时空特征，而忽略了走姿序列中的自然 périodic 特征，这些特征可以在完全利用时，可以帮助提高性能。在这种工作中，我们提出了一种插件式策略，即时间周期对齐策略（TPA），该策略利用走姿序列中的自然时间周期特征，以及步态模式中的细致时间相关性，从而增强识别性。TPA策略包括两个关键组件。首先是适应 Fourier 变换位置编码（AFPE），该组件可以将特征和离散时间信号转换成敏感于走姿序列时间周期特征的嵌入。其次是时间聚合模块（TAM），该模块可以将嵌入分解成趋势和季节性组件，并提取有用的时间相关性，以识别主要组件，同时滤除随机噪声。我们提出了一种简单而有效的基线方法，基于 TPA 策略，并在三个流行的公共数据集（CASIA-B、OU-MVLP 和 GREW）上进行了广泛的实验，结果表明，我们的提出的方法在多个benchmark测试中达到了当前领域的状态的最佳性能。
</details></li>
</ul>
<hr>
<h2 id="Conditional-Cross-Attention-Network-for-Multi-Space-Embedding-without-Entanglement-in-Only-a-SINGLE-Network"><a href="#Conditional-Cross-Attention-Network-for-Multi-Space-Embedding-without-Entanglement-in-Only-a-SINGLE-Network" class="headerlink" title="Conditional Cross Attention Network for Multi-Space Embedding without Entanglement in Only a SINGLE Network"></a>Conditional Cross Attention Network for Multi-Space Embedding without Entanglement in Only a SINGLE Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13254">http://arxiv.org/abs/2307.13254</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chull Hwan Song, Taebaek Hwang, Jooyoung Yoon, Shunghyun Choi, Yeong Hyeon Gu</li>
<li>for: 这个研究的目的是创建一个高效的单一标签物预测模型，以应对实际情况中的多个具体特征（如形状、颜色、长度等）。</li>
<li>methods: 我们提出了一个名为 Conditional Cross-Attention Network 的方法，它可以将多个具体特征转换为分开的多个空间表示，只需一个基础模型。我们使用了交叉注意机制来融合和转换条件（具体特征）的信息。</li>
<li>results: 我们的方法在多个标准 benchmark dataset上取得了稳定的state-of-the-art表现，包括 FashionAI、DARN、DeepFashion 和 Zappos50K。与之前的方法不同，我们的方法不受 benchmark dataset 的影响，表现一直优良。<details>
<summary>Abstract</summary>
Many studies in vision tasks have aimed to create effective embedding spaces for single-label object prediction within an image. However, in reality, most objects possess multiple specific attributes, such as shape, color, and length, with each attribute composed of various classes. To apply models in real-world scenarios, it is essential to be able to distinguish between the granular components of an object. Conventional approaches to embedding multiple specific attributes into a single network often result in entanglement, where fine-grained features of each attribute cannot be identified separately. To address this problem, we propose a Conditional Cross-Attention Network that induces disentangled multi-space embeddings for various specific attributes with only a single backbone. Firstly, we employ a cross-attention mechanism to fuse and switch the information of conditions (specific attributes), and we demonstrate its effectiveness through a diverse visualization example. Secondly, we leverage the vision transformer for the first time to a fine-grained image retrieval task and present a simple yet effective framework compared to existing methods. Unlike previous studies where performance varied depending on the benchmark dataset, our proposed method achieved consistent state-of-the-art performance on the FashionAI, DARN, DeepFashion, and Zappos50K benchmark datasets.
</details>
<details>
<summary>摘要</summary>
很多研究在视觉任务中尝试创建有效的嵌入空间，以便在图像中预测单个对象。然而，在实际情况下，大多数对象具有多个特定属性，如形状、颜色和长度，每个属性包含多个类。要将模型应用到实际场景中，需要能够分别识别对象的细腻特征。传统的嵌入多个特定属性到单个网络中的方法经常导致杂化，无法分别识别每个属性的细腻特征。为解决这个问题，我们提议一个名为Conditional Cross-Attention Network的方法，它可以生成独立的多个空间嵌入，用于不同的特定属性。首先，我们使用交叉注意机制将条件（特定属性）的信息融合和转换。我们通过多种视觉化示例展示了其效果。其次，我们是第一次应用视Transformer于细化图像检索任务，并提出了一个简单而有效的框架，与现有方法相比。与过去的研究不同，我们的提议方法在FashionAI、DARN、DeepFashion和Zappos50K benchmark dataset上实现了一致的状态空间性表现。
</details></li>
</ul>
<hr>
<h2 id="Keyword-Aware-Relative-Spatio-Temporal-Graph-Networks-for-Video-Question-Answering"><a href="#Keyword-Aware-Relative-Spatio-Temporal-Graph-Networks-for-Video-Question-Answering" class="headerlink" title="Keyword-Aware Relative Spatio-Temporal Graph Networks for Video Question Answering"></a>Keyword-Aware Relative Spatio-Temporal Graph Networks for Video Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13250">http://arxiv.org/abs/2307.13250</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi Cheng, Hehe Fan, Dongyun Lin, Ying Sun, Mohan Kankanhalli, Joo-Hwee Lim</li>
<li>for: 提高视频问答（VideoQA）中的复杂空间和时间关系捕捉和理解，以及使用关键词更好地捕捉问题特点。</li>
<li>methods: 提出了一种关键词意识Relative Spatio-Temporal（KRST）图网络，包括使用注意力机制将关键词纳入问题编码，以及将关键词意识导入视频图构建。同时，通过 integrating relative relation modeling来更好地捕捉视频中对象之间的空间和时间关系。</li>
<li>results: 在TGIF-QA、MSVD-QA和MSRVTT-QA等多个数据集上进行了广泛的实验，证明了KRST的超过多种状态艺术方法的优越性。<details>
<summary>Abstract</summary>
The main challenge in video question answering (VideoQA) is to capture and understand the complex spatial and temporal relations between objects based on given questions. Existing graph-based methods for VideoQA usually ignore keywords in questions and employ a simple graph to aggregate features without considering relative relations between objects, which may lead to inferior performance. In this paper, we propose a Keyword-aware Relative Spatio-Temporal (KRST) graph network for VideoQA. First, to make question features aware of keywords, we employ an attention mechanism to assign high weights to keywords during question encoding. The keyword-aware question features are then used to guide video graph construction. Second, because relations are relative, we integrate the relative relation modeling to better capture the spatio-temporal dynamics among object nodes. Moreover, we disentangle the spatio-temporal reasoning into an object-level spatial graph and a frame-level temporal graph, which reduces the impact of spatial and temporal relation reasoning on each other. Extensive experiments on the TGIF-QA, MSVD-QA and MSRVTT-QA datasets demonstrate the superiority of our KRST over multiple state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
主要挑战在视频问答（VideoQA）是捕捉和理解问题中的复杂空间和时间关系。现有的图学方法通常忽略问题中的关键词并使用简单的图来汇聚特征，这可能会导致性能下降。在这篇论文中，我们提出了关键词意识的相对空间时间（KRST）图网络来解决这个问题。首先，为了让问题特征意识到关键词，我们使用注意力机制来在问题编码中分配高权重到关键词。关键词意识的问题特征然后用来导引视频图建构。其次，因为关系是相对的，我们将相对关系模型纳入更好地捕捉视频中对象节点之间的空间时间动态。此外，我们将空间时间理解分解成对象级别的空间图和帧级别的时间图，这会减少空间和时间关系的相互影响。我们在TGIF-QA、MSVD-QA和MSRVTT-QA datasets上进行了广泛的实验，并证明了我们的KRST方法在多个现状顶峰方法之上。
</details></li>
</ul>
<hr>
<h2 id="Multi-Granularity-Prediction-with-Learnable-Fusion-for-Scene-Text-Recognition"><a href="#Multi-Granularity-Prediction-with-Learnable-Fusion-for-Scene-Text-Recognition" class="headerlink" title="Multi-Granularity Prediction with Learnable Fusion for Scene Text Recognition"></a>Multi-Granularity Prediction with Learnable Fusion for Scene Text Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13244">http://arxiv.org/abs/2307.13244</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alibabaresearch/advancedliteratemachinery">https://github.com/alibabaresearch/advancedliteratemachinery</a></li>
<li>paper_authors: Cheng Da, Peng Wang, Cong Yao</li>
<li>For: The paper is written for scene text recognition (STR), which is an active research topic in computer vision. The authors aim to tackle the challenging problem of STR by incorporating linguistic knowledge into the model.* Methods: The authors use a vision STR model built upon the Vision Transformer (ViT) and a tailored Adaptive Addressing and Aggregation (A$^3$) module. They also propose a Multi-Granularity Prediction strategy to inject information from the language modality into the model, using subword representations (BPE and WordPiece) in addition to the conventional character level representation.* Results: The proposed MGP-STR algorithm achieves an average recognition accuracy of 94% on standard benchmarks for scene text recognition, and also achieves state-of-the-art results on widely-used handwritten benchmarks and more challenging scene text datasets.Here are the three key points in Simplified Chinese text:* For: 这篇论文是为了Scene Text Recognition（STR）做出一种新的方法。* Methods: 作者使用了基于Vision Transformer（ViT）的视觉STR模型，并提出了一种适应性地址和聚合（A$^3$)模块。另外，他们还提出了一种多级预测策略，以在模型中注入语言特征。* Results: 提案的MGP-STR算法可以在标准的STR测试集上达到94%的识别率，同时在手写测试集和更加具有挑战性的Scene Text测试集上也达到了状态之最的结果。<details>
<summary>Abstract</summary>
Due to the enormous technical challenges and wide range of applications, scene text recognition (STR) has been an active research topic in computer vision for years. To tackle this tough problem, numerous innovative methods have been successively proposed, and incorporating linguistic knowledge into STR models has recently become a prominent trend. In this work, we first draw inspiration from the recent progress in Vision Transformer (ViT) to construct a conceptually simple yet functionally powerful vision STR model, which is built upon ViT and a tailored Adaptive Addressing and Aggregation (A$^3$) module. It already outperforms most previous state-of-the-art models for scene text recognition, including both pure vision models and language-augmented methods. To integrate linguistic knowledge, we further propose a Multi-Granularity Prediction strategy to inject information from the language modality into the model in an implicit way, \ie, subword representations (BPE and WordPiece) widely used in NLP are introduced into the output space, in addition to the conventional character level representation, while no independent language model (LM) is adopted. To produce the final recognition results, two strategies for effectively fusing the multi-granularity predictions are devised. The resultant algorithm (termed MGP-STR) is able to push the performance envelope of STR to an even higher level. Specifically, MGP-STR achieves an average recognition accuracy of $94\%$ on standard benchmarks for scene text recognition. Moreover, it also achieves state-of-the-art results on widely-used handwritten benchmarks as well as more challenging scene text datasets, demonstrating the generality of the proposed MGP-STR algorithm. The source code and models will be available at: \url{https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/OCR/MGP-STR}.
</details>
<details>
<summary>摘要</summary>
due to the enormous technical challenges and wide range of applications, scene text recognition (STR) has been an active research topic in computer vision for years. to tackle this tough problem, numerous innovative methods have been successively proposed, and incorporating linguistic knowledge into STR models has recently become a prominent trend. in this work, we first draw inspiration from the recent progress in Vision Transformer (ViT) to construct a conceptually simple yet functionally powerful vision STR model, which is built upon ViT and a tailored Adaptive Addressing and Aggregation (A$^3$) module. it already outperforms most previous state-of-the-art models for scene text recognition, including both pure vision models and language-augmented methods. to integrate linguistic knowledge, we further propose a Multi-Granularity Prediction strategy to inject information from the language modality into the model in an implicit way, 例如，使用字符级别表示和wordpiece的subword表示，while no independent language model (LM) is adopted. to produce the final recognition results, two strategies for effectively fusing the multi-granularity predictions are devised. the resultant algorithm (termed MGP-STR) is able to push the performance envelope of STR to an even higher level. specifically, MGP-STR achieves an average recognition accuracy of 94% on standard benchmarks for scene text recognition. moreover, it also achieves state-of-the-art results on widely-used handwritten benchmarks as well as more challenging scene text datasets, demonstrating the generality of the proposed MGP-STR algorithm. the source code and models will be available at: https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/OCR/MGP-STR。
</details></li>
</ul>
<hr>
<h2 id="Fashion-Matrix-Editing-Photos-by-Just-Talking"><a href="#Fashion-Matrix-Editing-Photos-by-Just-Talking" class="headerlink" title="Fashion Matrix: Editing Photos by Just Talking"></a>Fashion Matrix: Editing Photos by Just Talking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13240">http://arxiv.org/abs/2307.13240</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zheng-chong/fashionmatric">https://github.com/zheng-chong/fashionmatric</a></li>
<li>paper_authors: Zheng Chong, Xujie Zhang, Fuwei Zhao, Zhenyu Xie, Xiaodan Liang</li>
<li>for: 这个论文旨在探讨如何使用大型自然语言模型（LLM）来构建智能系统，以便在时尚领域内进行图像编辑。</li>
<li>methods: 该论文提出了一种层次结构的AI系统，名为时尚矩阵（Fashion Matrix），可以通过语音指令来编辑图像。这个系统使用LLM作为基础支持，并在用户的指令下进行迭代交互。具体来说，该系统使用了多种Semantic Segmentation Models（如Grounded-SAM、MattingAnything等）来定义特定的编辑面罩，然后使用Visual Foundation Models（如Stable Diffusion、ControlNet等）来从文本提示和面罩中生成编辑后的图像。</li>
<li>results: 实验表明，Fashion Matrix可以充分发挥大型自然语言模型在时尚编辑领域的合作潜力。<details>
<summary>Abstract</summary>
The utilization of Large Language Models (LLMs) for the construction of AI systems has garnered significant attention across diverse fields. The extension of LLMs to the domain of fashion holds substantial commercial potential but also inherent challenges due to the intricate semantic interactions in fashion-related generation. To address this issue, we developed a hierarchical AI system called Fashion Matrix dedicated to editing photos by just talking. This system facilitates diverse prompt-driven tasks, encompassing garment or accessory replacement, recoloring, addition, and removal. Specifically, Fashion Matrix employs LLM as its foundational support and engages in iterative interactions with users. It employs a range of Semantic Segmentation Models (e.g., Grounded-SAM, MattingAnything, etc.) to delineate the specific editing masks based on user instructions. Subsequently, Visual Foundation Models (e.g., Stable Diffusion, ControlNet, etc.) are leveraged to generate edited images from text prompts and masks, thereby facilitating the automation of fashion editing processes. Experiments demonstrate the outstanding ability of Fashion Matrix to explores the collaborative potential of functionally diverse pre-trained models in the domain of fashion editing.
</details>
<details>
<summary>摘要</summary>
utilization of Large Language Models (LLMs) for the construction of AI systems has garnered significant attention across diverse fields. The extension of LLMs to the domain of fashion holds substantial commercial potential but also inherent challenges due to the intricate semantic interactions in fashion-related generation. To address this issue, we developed a hierarchical AI system called Fashion Matrix dedicated to editing photos by just talking. This system facilitates diverse prompt-driven tasks, encompassing garment or accessory replacement, recoloring, addition, and removal. Specifically, Fashion Matrix employs LLM as its foundational support and engages in iterative interactions with users. It employs a range of Semantic Segmentation Models (e.g., Grounded-SAM, MattingAnything, etc.) to delineate the specific editing masks based on user instructions. Subsequently, Visual Foundation Models (e.g., Stable Diffusion, ControlNet, etc.) are leveraged to generate edited images from text prompts and masks, thereby facilitating the automation of fashion editing processes. Experiments demonstrate the outstanding ability of Fashion Matrix to explore the collaborative potential of functionally diverse pre-trained models in the domain of fashion editing.
</details></li>
</ul>
<hr>
<h2 id="Audio-aware-Query-enhanced-Transformer-for-Audio-Visual-Segmentation"><a href="#Audio-aware-Query-enhanced-Transformer-for-Audio-Visual-Segmentation" class="headerlink" title="Audio-aware Query-enhanced Transformer for Audio-Visual Segmentation"></a>Audio-aware Query-enhanced Transformer for Audio-Visual Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13236">http://arxiv.org/abs/2307.13236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinxiang Liu, Chen Ju, Chaofan Ma, Yanfeng Wang, Yu Wang, Ya Zhang</li>
<li>for: Audio-visual segmentation (AVS) task, specifically to segment sounding objects in video frames using audio cues.</li>
<li>methods: Introduces a multimodal transformer architecture that enables deep fusion and aggregation of audio-visual features, as well as an audio-aware query-enhanced transformer decoder that explicitly focuses on the segmentation of pinpointed sounding objects based on audio signals.</li>
<li>results: Outperforms previous methods and demonstrates better generalization ability in multi-sound and open-set scenarios.<details>
<summary>Abstract</summary>
The goal of the audio-visual segmentation (AVS) task is to segment the sounding objects in the video frames using audio cues. However, current fusion-based methods have the performance limitations due to the small receptive field of convolution and inadequate fusion of audio-visual features. To overcome these issues, we propose a novel \textbf{Au}dio-aware query-enhanced \textbf{TR}ansformer (AuTR) to tackle the task. Unlike existing methods, our approach introduces a multimodal transformer architecture that enables deep fusion and aggregation of audio-visual features. Furthermore, we devise an audio-aware query-enhanced transformer decoder that explicitly helps the model focus on the segmentation of the pinpointed sounding objects based on audio signals, while disregarding silent yet salient objects. Experimental results show that our method outperforms previous methods and demonstrates better generalization ability in multi-sound and open-set scenarios.
</details>
<details>
<summary>摘要</summary>
目的是对视频帧中的听起来对象进行分割，使用听音信号作为cue。然而，现有的融合方法受到小感知区域和不足的听视特征融合的限制。为了解决这些问题，我们提出了一种新的听音意识 Query-强化 transformer（AuTR）方法。与现有方法不同，我们的方法引入了多Modal transformer架构，允许深度融合和听视特征的积累。此外，我们开发了一种听音意识Query-强化 transformer解码器，具体地帮助模型根据听音信号进行对象分割，而忽略无声却突出的对象。实验结果表明，我们的方法在多音和开放集成enario中表现出色，并且在多音和开放集成enario中具有更好的泛化能力。
</details></li>
</ul>
<hr>
<h2 id="Strivec-Sparse-Tri-Vector-Radiance-Fields"><a href="#Strivec-Sparse-Tri-Vector-Radiance-Fields" class="headerlink" title="Strivec: Sparse Tri-Vector Radiance Fields"></a>Strivec: Sparse Tri-Vector Radiance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13226">http://arxiv.org/abs/2307.13226</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zerg-overmind/strivec">https://github.com/zerg-overmind/strivec</a></li>
<li>paper_authors: Quankai Gao, Qiangeng Xu, Hao Su, Ulrich Neumann, Zexiang Xu</li>
<li>for: 该文章旨在提出一种新的神经表示方法，用于模型3D场景为辐射场的local tensor feature网格。</li>
<li>methods: 该方法利用tensor decomposition， builds upon recent work TensoRF，并使用cloud of local tensors和classic CANDECOMP&#x2F;PARAFAC（CP）归一化来分解每个tensor成三个向量，表示本地特征分布 along spatial axes，并压缩表示本地神经场的compact neural field。</li>
<li>results: 该方法可以实现更好的渲染质量，使用相对较少的参数，比如TensoRF和Instant-NGP。<details>
<summary>Abstract</summary>
We propose Strivec, a novel neural representation that models a 3D scene as a radiance field with sparsely distributed and compactly factorized local tensor feature grids. Our approach leverages tensor decomposition, following the recent work TensoRF, to model the tensor grids. In contrast to TensoRF which uses a global tensor and focuses on their vector-matrix decomposition, we propose to utilize a cloud of local tensors and apply the classic CANDECOMP/PARAFAC (CP) decomposition to factorize each tensor into triple vectors that express local feature distributions along spatial axes and compactly encode a local neural field. We also apply multi-scale tensor grids to discover the geometry and appearance commonalities and exploit spatial coherence with the tri-vector factorization at multiple local scales. The final radiance field properties are regressed by aggregating neural features from multiple local tensors across all scales. Our tri-vector tensors are sparsely distributed around the actual scene surface, discovered by a fast coarse reconstruction, leveraging the sparsity of a 3D scene. We demonstrate that our model can achieve better rendering quality while using significantly fewer parameters than previous methods, including TensoRF and Instant-NGP.
</details>
<details>
<summary>摘要</summary>
我们提出了Strivec，一种新的神经表示方法，它将三维场景视为一个辐射场，并使用稀疏分布的本地维度特征网格来模型。我们的方法利用了矩阵分解，建立在最近的TensoRF工作之上，通过对每个矩阵进行类似于CP分解（CANDECOMP/PARAFAC），将每个矩阵分解成三个向量，表示地方特征分布在空间轴上，并压缩地表示当地神经场。我们还使用多尺度矩阵网格来探索场景的几何和外观共同点，并利用多个本地尺度的空间同步来提高渲染质量。最终，我们通过将多个本地矩阵的神经特征进行汇聚来预测场景的辐射场性质。我们的三向量矩阵在实际场景表面上稀疏分布，通过快速粗略重建来发现。我们示示了我们的模型可以在使用更少参数的情况下达到更好的渲染质量，比之前的方法，包括TensoRF和Instant-NGP。
</details></li>
</ul>
<hr>
<h2 id="Image-Segmentation-Keras-Implementation-of-Segnet-FCN-UNet-PSPNet-and-other-models-in-Keras"><a href="#Image-Segmentation-Keras-Implementation-of-Segnet-FCN-UNet-PSPNet-and-other-models-in-Keras" class="headerlink" title="Image Segmentation Keras : Implementation of Segnet, FCN, UNet, PSPNet and other models in Keras"></a>Image Segmentation Keras : Implementation of Segnet, FCN, UNet, PSPNet and other models in Keras</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13215">http://arxiv.org/abs/2307.13215</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/divamgupta/image-segmentation-keras">https://github.com/divamgupta/image-segmentation-keras</a></li>
<li>paper_authors: Divam Gupta</li>
<li>for: 本文提供了一个全面的 semantic segmentation 库，包含多种流行的 segmentation 模型，如 SegNet、FCN、UNet 和 PSPNet。</li>
<li>methods: 本文提供了多种 segmentation 模型的实现，并对其进行了评估和比较，为研究人员和实践者提供了一套强大的工具集，用于解决多种分类挑战。</li>
<li>results: 本文对多个数据集进行了评估和比较，以便为研究人员和实践者提供参考结果，帮助他们更好地选择合适的 segmentation 模型。<details>
<summary>Abstract</summary>
Semantic segmentation plays a vital role in computer vision tasks, enabling precise pixel-level understanding of images. In this paper, we present a comprehensive library for semantic segmentation, which contains implementations of popular segmentation models like SegNet, FCN, UNet, and PSPNet. We also evaluate and compare these models on several datasets, offering researchers and practitioners a powerful toolset for tackling diverse segmentation challenges.
</details>
<details>
<summary>摘要</summary>
semantic segmentation 在计算机视觉任务中扮演着重要的角色，允许精确地理解图像的每个像素。在这篇论文中，我们提供了一个全面的Semantic Segmentation库，包括流行的 segmentation 模型如 SegNet、FCN、UNet 和 PSPNet。我们还对这些模型在多个数据集上进行了评估和比较，为研究人员和实践者提供了一套强大的工具集，用于解决多样化的 segmentation 挑战。
</details></li>
</ul>
<hr>
<h2 id="GeoTransformer-Fast-and-Robust-Point-Cloud-Registration-with-Geometric-Transformer"><a href="#GeoTransformer-Fast-and-Robust-Point-Cloud-Registration-with-Geometric-Transformer" class="headerlink" title="GeoTransformer: Fast and Robust Point Cloud Registration with Geometric Transformer"></a>GeoTransformer: Fast and Robust Point Cloud Registration with Geometric Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03768">http://arxiv.org/abs/2308.03768</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qinzheng93/geotransformer">https://github.com/qinzheng93/geotransformer</a></li>
<li>paper_authors: Zheng Qin, Hao Yu, Changjian Wang, Yulan Guo, Yuxing Peng, Slobodan Ilic, Dewen Hu, Kai Xu</li>
<li>for: 本文targets the problem of point cloud registration, specifically focusing on accurate correspondence extraction without relying on keypoints.</li>
<li>methods: 方法基于Geometric Transformer（GeoTransformer），利用对超点的匹配来学习 геометрических特征，使匹配方法具有对静止变换的抗干扰和低 overlap场景中的稳定性。</li>
<li>results: 实验结果表明，GeoTransformer可以达到高精度匹配，无需进行RANSAC，从而提高了匹配精度和注册精度。特别是在3DLoMatch benchmark上，我们的方法提高了匹配率18%到31%和注册精度7点以上。<details>
<summary>Abstract</summary>
We study the problem of extracting accurate correspondences for point cloud registration. Recent keypoint-free methods have shown great potential through bypassing the detection of repeatable keypoints which is difficult to do especially in low-overlap scenarios. They seek correspondences over downsampled superpoints, which are then propagated to dense points. Superpoints are matched based on whether their neighboring patches overlap. Such sparse and loose matching requires contextual features capturing the geometric structure of the point clouds. We propose Geometric Transformer, or GeoTransformer for short, to learn geometric feature for robust superpoint matching. It encodes pair-wise distances and triplet-wise angles, making it invariant to rigid transformation and robust in low-overlap cases. The simplistic design attains surprisingly high matching accuracy such that no RANSAC is required in the estimation of alignment transformation, leading to $100$ times acceleration. Extensive experiments on rich benchmarks encompassing indoor, outdoor, synthetic, multiway and non-rigid demonstrate the efficacy of GeoTransformer. Notably, our method improves the inlier ratio by $18{\sim}31$ percentage points and the registration recall by over $7$ points on the challenging 3DLoMatch benchmark. Our code and models are available at \url{https://github.com/qinzheng93/GeoTransformer}.
</details>
<details>
<summary>摘要</summary>
我们研究点云注册问题中的准确匹配问题。最近的关键点无法方法已经表现出了很大的潜力，它们通过绕过复现关键点的检测而实现了更加简单的匹配方式。它们在下采样后的超点上寻找匹配，然后将匹配推广到密集点云。超点的匹配基于他们邻近的补丁 overlap。这种稀疏和松散的匹配需要捕捉点云的几何结构。我们提出了Geometric Transformer，简称为GeoTransformer，用于学习几何特征以实现Robust superpoint匹配。它编码了对称的距离和三角形的角度，使其对于平移变换不变和低 overlap情况下具有抗锁定性。我们的简单设计听起来 surprisingly high匹配精度，无需RANSAC，从而实现了100倍的加速。我们的实验结果表明，GeoTransformer在含有室内、外部、 sintetic、多方和非RIGID的丰富benchmark上都具有remarkable的效果。其中，我们的方法提高了3DLoMatchbenchmark上的准确比例by 18-31个百分点和注册记忆by more than 7个点。我们的代码和模型可以在以下链接中找到：https://github.com/qinzheng93/GeoTransformer。
</details></li>
</ul>
<hr>
<h2 id="An-Investigation-into-Glomeruli-Detection-in-Kidney-H-E-and-PAS-Images-using-YOLO"><a href="#An-Investigation-into-Glomeruli-Detection-in-Kidney-H-E-and-PAS-Images-using-YOLO" class="headerlink" title="An Investigation into Glomeruli Detection in Kidney H&amp;E and PAS Images using YOLO"></a>An Investigation into Glomeruli Detection in Kidney H&amp;E and PAS Images using YOLO</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13199">http://arxiv.org/abs/2307.13199</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AlexeyAB/darknet">https://github.com/AlexeyAB/darknet</a></li>
<li>paper_authors: Kimia Hemmatirad, Morteza Babaie, Jeffrey Hodgin, Liron Pantanowitz, H. R. Tizhoosh<br>for:This paper aims to assist pathologists in detecting glomeruli in human kidney images using computerized solutions, specifically using the YOLO-v4 object detector.methods:The YOLO-v4 model was used to detect glomeruli in human kidney images, and the model was trained on whole slide images. The model was fine-tuned using a private dataset from the University of Michigan, and tested on the same dataset using two different stains (H&amp;E and PAS).results:The results show that the YOLO-v4 model can achieve high specificity and sensitivity in detecting glomeruli in human kidney images, with an average specificity and sensitivity for all experiments. The model’s performance was also compared to existing segmentation methods on the same datasets, and the results show that the YOLO-v4 model outperforms these methods.<details>
<summary>Abstract</summary>
Context: Analyzing digital pathology images is necessary to draw diagnostic conclusions by investigating tissue patterns and cellular morphology. However, manual evaluation can be time-consuming, expensive, and prone to inter- and intra-observer variability. Objective: To assist pathologists using computerized solutions, automated tissue structure detection and segmentation must be proposed. Furthermore, generating pixel-level object annotations for histopathology images is expensive and time-consuming. As a result, detection models with bounding box labels may be a feasible solution. Design: This paper studies. YOLO-v4 (You-Only-Look-Once), a real-time object detector for microscopic images. YOLO uses a single neural network to predict several bounding boxes and class probabilities for objects of interest. YOLO can enhance detection performance by training on whole slide images. YOLO-v4 has been used in this paper. for glomeruli detection in human kidney images. Multiple experiments have been designed and conducted based on different training data of two public datasets and a private dataset from the University of Michigan for fine-tuning the model. The model was tested on the private dataset from the University of Michigan, serving as an external validation of two different stains, namely hematoxylin and eosin (H&E) and periodic acid-Schiff (PAS). Results: Average specificity and sensitivity for all experiments, and comparison of existing segmentation methods on the same datasets are discussed. Conclusions: Automated glomeruli detection in human kidney images is possible using modern AI models. The design and validation for different stains still depends on variability of public multi-stain datasets.
</details>
<details>
<summary>摘要</summary>
Context: 分析数字 PATHOLOGY 图像是必要的，以便从 Investigate 组织趋势和细胞形态中得出诊断结论。然而，手动评估可能会占用大量时间和成本，并且可能会存在Inter-和 intra-观察者的差异。目的：通过计算机化解决方案，自动检测和分类组织结构。此外，生成 Histopathology 图像的像素级对象标注是昂贵的和时间consuming。因此，使用 bounding box 标签的检测模型可能是一个可行的解决方案。设计：本文研究了 YOLO-v4（You-Only-Look-Once），一种实时物体检测器，用于微scopic 图像。YOLO 使用单个神经网络预测多个 bounding box 和对象类概率。YOLO 可以通过训练整个扫描图像来提高检测性能。本文使用 YOLO-v4 进行人肾图像中glomeruli 检测。多个实验基于不同的训练数据，包括两个公共数据集和大学Michigan 私有数据集进行了微调。模型在大学Michigan 私有数据集上进行了测试，并作为对 H&E 和 PAS 两种染料的外部验证。结果：本文提出了一些均衡性和敏感性的平均值，并与其他分 segmentation 方法在同一个数据集上进行了比较。结论：使用现代 AI 模型，自动检测人肾图像中的glomeruli 是可能的。不同的染料设计仍然取决于多个公共多种染料数据集的变化。
</details></li>
</ul>
<hr>
<h2 id="Does-Progress-On-Object-Recognition-Benchmarks-Improve-Real-World-Generalization"><a href="#Does-Progress-On-Object-Recognition-Benchmarks-Improve-Real-World-Generalization" class="headerlink" title="Does Progress On Object Recognition Benchmarks Improve Real-World Generalization?"></a>Does Progress On Object Recognition Benchmarks Improve Real-World Generalization?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13136">http://arxiv.org/abs/2307.13136</a></li>
<li>repo_url: None</li>
<li>paper_authors: Megan Richards, Polina Kirichenko, Diane Bouchacourt, Mark Ibrahim</li>
<li>for: 这个论文是为了评估对图像网络上的基本模型进行大规模数据训练是否能够提高对真实世界的泛化能力。</li>
<li>methods: 这个论文使用了两个 datasets of objects from households across the globe，并进行了广泛的实验研究，包括对nearly 100种视觉模型进行了评估。</li>
<li>results: 研究发现，通过标准的基本模型训练方法，模型在不同地区的性能差距较大，Foundation CLIP 模型也存在大量地区性能差距。此外，论文还发现，通过简单地在最后一层添加更 represervative 的数据进行再训练，可以减少地区性能差距。<details>
<summary>Abstract</summary>
For more than a decade, researchers have measured progress in object recognition on ImageNet-based generalization benchmarks such as ImageNet-A, -C, and -R. Recent advances in foundation models, trained on orders of magnitude more data, have begun to saturate these standard benchmarks, but remain brittle in practice. This suggests standard benchmarks, which tend to focus on predefined or synthetic changes, may not be sufficient for measuring real world generalization. Consequently, we propose studying generalization across geography as a more realistic measure of progress using two datasets of objects from households across the globe. We conduct an extensive empirical evaluation of progress across nearly 100 vision models up to most recent foundation models. We first identify a progress gap between standard benchmarks and real-world, geographical shifts: progress on ImageNet results in up to 2.5x more progress on standard generalization benchmarks than real-world distribution shifts. Second, we study model generalization across geographies by measuring the disparities in performance across regions, a more fine-grained measure of real world generalization. We observe all models have large geographic disparities, even foundation CLIP models, with differences of 7-20% in accuracy between regions. Counter to modern intuition, we discover progress on standard benchmarks fails to improve geographic disparities and often exacerbates them: geographic disparities between the least performant models and today's best models have more than tripled. Our results suggest scaling alone is insufficient for consistent robustness to real-world distribution shifts. Finally, we highlight in early experiments how simple last layer retraining on more representative, curated data can complement scaling as a promising direction of future work, reducing geographic disparity on both benchmarks by over two-thirds.
</details>
<details>
<summary>摘要</summary>
To address this, we propose studying generalization across geography as a more realistic measure of progress. We evaluate nearly 100 vision models, including the most recent foundation models, on two datasets of objects from households around the world. Our results show that there is a significant gap between progress on ImageNet and real-world geographical shifts. While progress on ImageNet results in up to 2.5 times more progress on standard generalization benchmarks, it does not improve geographic disparities and often exacerbates them. In fact, the geographic disparities between the least performant models and today's best models have more than tripled.Our findings suggest that scaling alone is not sufficient for consistent robustness to real-world distribution shifts. However, we do find that simple last layer retraining on more representative, curated data can complement scaling and reduce geographic disparity on both benchmarks by over two-thirds. These results highlight the importance of considering real-world geographical variations when evaluating progress in object recognition.
</details></li>
</ul>
<hr>
<h2 id="simPLE-a-visuotactile-method-learned-in-simulation-to-precisely-pick-localize-regrasp-and-place-objects"><a href="#simPLE-a-visuotactile-method-learned-in-simulation-to-precisely-pick-localize-regrasp-and-place-objects" class="headerlink" title="simPLE: a visuotactile method learned in simulation to precisely pick, localize, regrasp, and place objects"></a>simPLE: a visuotactile method learned in simulation to precisely pick, localize, regrasp, and place objects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13133">http://arxiv.org/abs/2307.13133</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maria Bauza, Antonia Bronars, Yifan Hou, Ian Taylor, Nikhil Chavan-Dafle, Alberto Rodriguez<br>for: 这篇论文旨在解决机器人抓取和安放精度问题。methods: 本论文提出了一种基于模拟和感知的方法，称为simPLE，可以帮助机器人在不知道任务的情况下，准确地抓取和安放多种不同形状的物体。results: 在使用 dual-arm 机器人和视听感知系统的实验中，simPLE 能够成功地将 15 种不同形状的物体安放到有序排列中，成功率高达 90% 以上，并且在 6 种物体上达到 1mm 的准确性。<details>
<summary>Abstract</summary>
Existing robotic systems have a clear tension between generality and precision. Deployed solutions for robotic manipulation tend to fall into the paradigm of one robot solving a single task, lacking precise generalization, i.e., the ability to solve many tasks without compromising on precision. This paper explores solutions for precise and general pick-and-place. In precise pick-and-place, i.e. kitting, the robot transforms an unstructured arrangement of objects into an organized arrangement, which can facilitate further manipulation. We propose simPLE (simulation to Pick Localize and PLacE) as a solution to precise pick-and-place. simPLE learns to pick, regrasp and place objects precisely, given only the object CAD model and no prior experience. We develop three main components: task-aware grasping, visuotactile perception, and regrasp planning. Task-aware grasping computes affordances of grasps that are stable, observable, and favorable to placing. The visuotactile perception model relies on matching real observations against a set of simulated ones through supervised learning. Finally, we compute the desired robot motion by solving a shortest path problem on a graph of hand-to-hand regrasps. On a dual-arm robot equipped with visuotactile sensing, we demonstrate pick-and-place of 15 diverse objects with simPLE. The objects span a wide range of shapes and simPLE achieves successful placements into structured arrangements with 1mm clearance over 90% of the time for 6 objects, and over 80% of the time for 11 objects. Videos are available at http://mcube.mit.edu/research/simPLE.html .
</details>
<details>
<summary>摘要</summary>
现有的机器人系统存在明确的一致性和精度之间的矛盾。已部署的机器人 manipulate 解决方案通常处于单一任务的解决方案，缺乏精度，即能够解决多个任务而不失去精度。本文探讨精度和通用性的pick-and-place解决方案。在精度的pick-and-place中，机器人将无结构的物品变换为结构化的安排，可以促进进一步的操作。我们提出了simPLE（从 simulate 到 Pick Localize 和 PLacE）作为精度和通用性的pick-and-place解决方案。simPLE通过学习，可以准确地找到、重新抓取并将物品放置在正确的位置，只需要物品 CAD 模型，没有先前经验。我们开发了三个主要 ком成分：任务意识 grasping、视听感知和重新抓 планиuning。任务意识 grasping 计算物品的可行性，包括稳定、可见和放置的优势。视听感知模型通过对实际观察与 simulations 进行比较，通过超参数学习来学习。最后，我们解决了一个最短路径问题，以计算手动重新抓取的desired robot 动作。在配备视听感知的双手机器人上，我们通过 simPLE 成功地完成了15种不同的物品的pick-and-place。物品的形状范围广泛，simPLE 在90% 的时间内成功地将物品放置到结构化的安排中，距离1毫米。视频可以在http://mcube.mit.edu/research/simPLE.html 上查看。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Approaches-for-Data-Augmentation-in-Medical-Imaging-A-Review"><a href="#Deep-Learning-Approaches-for-Data-Augmentation-in-Medical-Imaging-A-Review" class="headerlink" title="Deep Learning Approaches for Data Augmentation in Medical Imaging: A Review"></a>Deep Learning Approaches for Data Augmentation in Medical Imaging: A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13125">http://arxiv.org/abs/2307.13125</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Arminsbss/tumor-classification">https://github.com/Arminsbss/tumor-classification</a></li>
<li>paper_authors: Aghiles Kebaili, Jérôme Lapuyade-Lahorgue, Su Ruan</li>
<li>for: 这篇论文主要关注的是如何使用深度生成模型来增强医疗影像分析，特别是对于医疗领域的训练数据有限制，并且训练数据的获得可能是成本高且受到隐私法规限制。</li>
<li>methods: 这篇论文评论了三种深度生成模型，包括Variational Autoencoders、Generative Adversarial Networks和Diffusion Models，这些模型可以生成更加真实和多样的数据，并且可以帮助提高医疗影像分析中的深度学习算法性能。</li>
<li>results: 这篇论文评论了这些模型在不同的下游任务中的表现，包括分类、分 segmentation 和 Cross-modal Translation，并且评估了这些模型的优点和缺点，并提出了未来研究的方向。<details>
<summary>Abstract</summary>
Deep learning has become a popular tool for medical image analysis, but the limited availability of training data remains a major challenge, particularly in the medical field where data acquisition can be costly and subject to privacy regulations. Data augmentation techniques offer a solution by artificially increasing the number of training samples, but these techniques often produce limited and unconvincing results. To address this issue, a growing number of studies have proposed the use of deep generative models to generate more realistic and diverse data that conform to the true distribution of the data. In this review, we focus on three types of deep generative models for medical image augmentation: variational autoencoders, generative adversarial networks, and diffusion models. We provide an overview of the current state of the art in each of these models and discuss their potential for use in different downstream tasks in medical imaging, including classification, segmentation, and cross-modal translation. We also evaluate the strengths and limitations of each model and suggest directions for future research in this field. Our goal is to provide a comprehensive review about the use of deep generative models for medical image augmentation and to highlight the potential of these models for improving the performance of deep learning algorithms in medical image analysis.
</details>
<details>
<summary>摘要</summary>
深度学习已经成为医疗图像分析中广泛使用的工具，但是培训数据的有限性仍然是一个主要挑战，特别是在医疗领域，数据收集可能是成本高昂的并且受到隐私法规限制。数据扩充技术可以人工地增加培训样本数量，但这些技术通常会生成有限和不真实的结果。为解决这个问题，一些研究在医疗图像增强中使用深度生成模型，以生成更加真实和多样的数据，这些数据遵循实际数据的分布。在本文中，我们关注了三种深度生成模型，即变量自动编码器、生成对抗网络和扩散模型，并对它们在不同的下游任务中的当前状态进行了概述。我们还评估了每种模型的优缺点，并提出了未来研究的方向。我们的目标是提供一篇全面的深度生成模型在医疗图像增强中的评review，并高亮这些模型在医疗图像分析中的潜在优势。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Infant-Respiration-Estimation-from-Video-A-Deep-Flow-based-Algorithm-and-a-Novel-Public-Benchmark"><a href="#Automatic-Infant-Respiration-Estimation-from-Video-A-Deep-Flow-based-Algorithm-and-a-Novel-Public-Benchmark" class="headerlink" title="Automatic Infant Respiration Estimation from Video: A Deep Flow-based Algorithm and a Novel Public Benchmark"></a>Automatic Infant Respiration Estimation from Video: A Deep Flow-based Algorithm and a Novel Public Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13110">http://arxiv.org/abs/2307.13110</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ostadabbas/infant-respiration-estimation">https://github.com/ostadabbas/infant-respiration-estimation</a></li>
<li>paper_authors: Sai Kumar Reddy Manne, Shaotong Zhu, Sarah Ostadabbas, Michael Wan<br>for:This paper aims to develop a deep-learning method for estimating respiratory rate and waveform from plain video footage in natural settings, with the goal of providing fully automatic, continuous, and contactless respiratory monitoring for infants.methods:The proposed method, called AIRFlowNet, combines video-extracted optical flow input and spatiotemporal convolutional processing tuned to the infant domain. The model is trained using a novel spectral bandpass loss function and a public annotated infant respiration dataset (AIR-125) with 125 videos drawn from eight infant subjects.results:Compared to other state-of-the-art methods, AIRFlowNet significantly outperforms other state-of-the-art methods in respiratory rate estimation, achieving a mean absolute error of $\sim$2.9 breaths per minute.<details>
<summary>Abstract</summary>
Respiration is a critical vital sign for infants, and continuous respiratory monitoring is particularly important for newborns. However, neonates are sensitive and contact-based sensors present challenges in comfort, hygiene, and skin health, especially for preterm babies. As a step toward fully automatic, continuous, and contactless respiratory monitoring, we develop a deep-learning method for estimating respiratory rate and waveform from plain video footage in natural settings. Our automated infant respiration flow-based network (AIRFlowNet) combines video-extracted optical flow input and spatiotemporal convolutional processing tuned to the infant domain. We support our model with the first public annotated infant respiration dataset with 125 videos (AIR-125), drawn from eight infant subjects, set varied pose, lighting, and camera conditions. We include manual respiration annotations and optimize AIRFlowNet training on them using a novel spectral bandpass loss function. When trained and tested on the AIR-125 infant data, our method significantly outperforms other state-of-the-art methods in respiratory rate estimation, achieving a mean absolute error of $\sim$2.9 breaths per minute, compared to $\sim$4.7--6.2 for other public models designed for adult subjects and more uniform environments.
</details>
<details>
<summary>摘要</summary>
呼吸是新生儿的生命指标之一，不间断的呼吸监测对新生儿 particurlary 重要。然而，新生儿强健和触感型感测器存在舒适性、卫生性和皮肤健康等问题，特别是对幼儿。为了实现完全自动、无接触、不间断的呼吸监测，我们开发了一种深度学习方法，可以从普通的视频流中提取呼吸速率和呼吸波形。我们称之为婴儿呼吸流基网络（AIRFlowNet），它将视频提取的光流输入和空间时间卷积处理结合，特制 для婴儿领域。我们为这种模型提供了首个公共标注 infant 呼吸数据集（AIR-125），包括8名婴儿的125个视频，具有多种姿势、照明和摄像头条件。我们还包括手动呼吸注释和使用新的spectral bandpass损失函数来优化AIRFlowNet 的训练。当我们在AIR-125 infant数据集上训练和测试AIRFlowNet时，它与其他公共模型相比，在呼吸速率估计方面显著超越，具有$\sim$2.9 breaths per minute的平均绝对误差，与$\sim$4.7--6.2的其他公共模型设计 для成人主题和更加均匀的环境相比。
</details></li>
</ul>
<hr>
<h2 id="General-Purpose-Multi-Modal-OOD-Detection-Framework"><a href="#General-Purpose-Multi-Modal-OOD-Detection-Framework" class="headerlink" title="General-Purpose Multi-Modal OOD Detection Framework"></a>General-Purpose Multi-Modal OOD Detection Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13069">http://arxiv.org/abs/2307.13069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Viet Duong, Qiong Wu, Zhengyi Zhou, Eric Zavesky, Jiahe Chen, Xiangzhou Liu, Wen-Ling Hsu, Huajie Shao</li>
<li>for: 本研究的目的是 simultaneously detect 多个不同的 OOD 场景，以提高 ML 系统的安全性和可靠性。</li>
<li>methods: 我们提出了一种通用的 weakly-supervised OOD detection 框架，called WOOD，它结合了一个二分类器和一个对比学习组件，以便充分利用两者的优点。我们采用了 Hinge loss 来约束 ID 和 OOD 样本的准确性。</li>
<li>results: 我们在多个实际世界数据集上测试了提出的 WOOD 模型，并得到了比现状态方法更高的 OOD 检测精度。特别是，我们的方法可以同时在三个不同的 OOD 场景中具有高准确性。<details>
<summary>Abstract</summary>
Out-of-distribution (OOD) detection identifies test samples that differ from the training data, which is critical to ensuring the safety and reliability of machine learning (ML) systems. While a plethora of methods have been developed to detect uni-modal OOD samples, only a few have focused on multi-modal OOD detection. Current contrastive learning-based methods primarily study multi-modal OOD detection in a scenario where both a given image and its corresponding textual description come from a new domain. However, real-world deployments of ML systems may face more anomaly scenarios caused by multiple factors like sensor faults, bad weather, and environmental changes. Hence, the goal of this work is to simultaneously detect from multiple different OOD scenarios in a fine-grained manner. To reach this goal, we propose a general-purpose weakly-supervised OOD detection framework, called WOOD, that combines a binary classifier and a contrastive learning component to reap the benefits of both. In order to better distinguish the latent representations of in-distribution (ID) and OOD samples, we adopt the Hinge loss to constrain their similarity. Furthermore, we develop a new scoring metric to integrate the prediction results from both the binary classifier and contrastive learning for identifying OOD samples. We evaluate the proposed WOOD model on multiple real-world datasets, and the experimental results demonstrate that the WOOD model outperforms the state-of-the-art methods for multi-modal OOD detection. Importantly, our approach is able to achieve high accuracy in OOD detection in three different OOD scenarios simultaneously. The source code will be made publicly available upon publication.
</details>
<details>
<summary>摘要</summary>
外部数据（OOD）检测可以识别测试样本与训练数据之间的差异，这是机器学习（ML）系统的安全性和可靠性的关键。虽然许多方法已经开发了用于检测单modal OOD样本，但只有一些关注了多modal OOD检测。现有的对比学习基于方法主要研究了一个给定的图像和其相应的文本描述来自新领域的多modal OOD检测场景。但实际世界中部署的ML系统可能会面临更多的异常场景，如感知器故障、坏天气和环境变化。因此，我们的目标是同时从多个不同的OOD场景中同精细地检测OOD样本。为达到这个目标，我们提出了一个通用强制监督OOD检测框架，called WOOD，它将对比学习和二分类器的优点相互融合。为了更好地分解ID和OOD样本的准确表示，我们采用了缩限损失来约束它们之间的相似性。此外，我们开发了一个新的分数指标，以集成binary分类器和对比学习的预测结果，以便更好地识别OOD样本。我们在多个实际世界数据集上测试了提议的WOOD模型，实验结果表明，WOOD模型在多modal OOD检测中超过了现有方法的性能。重要的是，我们的方法能够同时在三个不同的OOD场景中同精细地检测OOD样本。代码将在出版时公开。
</details></li>
</ul>
<hr>
<h2 id="On-the-characteristics-of-natural-hydraulic-dampers-An-image-based-approach-to-study-the-fluid-flow-behaviour-inside-the-human-meniscal-tissue"><a href="#On-the-characteristics-of-natural-hydraulic-dampers-An-image-based-approach-to-study-the-fluid-flow-behaviour-inside-the-human-meniscal-tissue" class="headerlink" title="On the characteristics of natural hydraulic dampers: An image-based approach to study the fluid flow behaviour inside the human meniscal tissue"></a>On the characteristics of natural hydraulic dampers: An image-based approach to study the fluid flow behaviour inside the human meniscal tissue</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13060">http://arxiv.org/abs/2307.13060</a></li>
<li>repo_url: None</li>
<li>paper_authors: J. Waghorne, F. P. Bonomo, A. Rabbani, D. Bell, O. Barrera</li>
<li>for: 这个研究旨在了解股骨细胞层中流体流动的行为以及其与结构的关系，以便更好地理解股骨疾病的发展、治疗方法的设计和生物材料的设计。</li>
<li>methods: 这个研究使用了计算流体动力学（CFD）和图像分析（CFD-IA）的新方法，通过高分辨率3D微计算tomography扫描来分析人类股骨内部的流体流动。</li>
<li>results: 研究发现，股骨内部的流体流动与结构参数（扭曲度、连接度、孔隙率、孔径size）存在 statistically significant 相关性。一些通道的Re值可达1400，并且在输入速度为1.6m&#x2F;s时出现了非达尔cy的 regime。location-dependent permeability ranges from 20-32 Darcy。在高输入速度下，流体速度和扭曲度之间存在强相关性，以及与通道径 diameter 的相关性。<details>
<summary>Abstract</summary>
The meniscal tissue is a layered material with varying properties influenced by collagen content and arrangement. Understanding the relationship between structure and properties is crucial for disease management, treatment development, and biomaterial design. The internal layer of the meniscus is softer and more deformable than the outer layers, thanks to interconnected collagen channels that guide fluid flow. To investigate these relationships, we propose a novel approach that combines Computational Fluid Dynamics (CFD) with Image Analysis (CFD-IA). We analyze fluid flow in the internal architecture of the human meniscus across a range of inlet velocities (0.1mm/s to 1.6m/s) using high-resolution 3D micro-computed tomography scans. Statistical correlations are observed between architectural parameters (tortuosity, connectivity, porosity, pore size) and fluid flow parameters (Re number distribution, permeability). Some channels exhibit Re values of 1400 at an inlet velocity of 1.6m/s, and a transition from Darcy's regime to a non-Darcian regime occurs around an inlet velocity of 0.02m/s. Location-dependent permeability ranges from 20-32 Darcy. Regression modelling reveals a strong correlation between fluid velocity and tortuosity at high inlet velocities, as well as with channel diameter at low inlet velocities. At higher inlet velocities, flow paths deviate more from the preferential direction, resulting in a decrease in the concentration parameter by an average of 0.4. This research provides valuable insights into the fluid flow behaviour within the meniscus and its structural influences.
</details>
<details>
<summary>摘要</summary>
人门韧带组织是一种层次结构，其特性受到含氧残基的含量和排列方式的影响。理解这些结构和性能之间的关系是疾病管理、治疗开发和生物材料设计的关键。人门韧带内部层次结构比外层更软和可变形，这是因为充满气流的彩虹涂层通道导致的。为了研究这些关系，我们提出了一种结合计算流动力学（CFD）和图像分析（CFD-IA）的新方法。我们使用高分辨率3D微型计算机断层扫描器来分析人门韧带内部的液体流动情况，并对输入速度（0.1mm/s至1.6m/s）进行了 Statistical correlations were observed between architectural parameters (tortuosity, connectivity, porosity, pore size) and fluid flow parameters (Re number distribution, permeability). Some channels exhibited Re values of 1400 at an inlet velocity of 1.6m/s, and a transition from Darcy's regime to a non-Darcian regime occurred around an inlet velocity of 0.02m/s. Location-dependent permeability ranged from 20-32 Darcy. Regression modeling revealed a strong correlation between fluid velocity and tortuosity at high inlet velocities, as well as with channel diameter at low inlet velocities. At higher inlet velocities, flow paths deviated more from the preferential direction, resulting in a decrease in the concentration parameter by an average of 0.4. This research provides valuable insights into the fluid flow behavior within the meniscus and its structural influences.  investigation of fluid flow behavior within the meniscus across a range of inlet velocities. We found that the internal architecture of the meniscus has a significant impact on fluid flow, and that there is a strong correlation between architectural parameters and fluid flow parameters. Our findings provide valuable insights into the relationship between structure and properties in the meniscus, and have important implications for disease management, treatment development, and biomaterial design.
</details></li>
</ul>
<hr>
<h2 id="A-Systematic-Survey-of-Prompt-Engineering-on-Vision-Language-Foundation-Models"><a href="#A-Systematic-Survey-of-Prompt-Engineering-on-Vision-Language-Foundation-Models" class="headerlink" title="A Systematic Survey of Prompt Engineering on Vision-Language Foundation Models"></a>A Systematic Survey of Prompt Engineering on Vision-Language Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12980">http://arxiv.org/abs/2307.12980</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/JindongGu/Awesome-Prompting-on-Vision-Language-Model">https://github.com/JindongGu/Awesome-Prompting-on-Vision-Language-Model</a></li>
<li>paper_authors: Jindong Gu, Zhen Han, Shuo Chen, Ahmad Beirami, Bailan He, Gengyuan Zhang, Ruotong Liao, Yao Qin, Volker Tresp, Philip Torr<br>for: This paper provides a comprehensive survey of cutting-edge research in prompt engineering on three types of vision-language models, including multimodal-to-text generation models, image-text matching models, and text-to-image generation models.methods: The paper discusses various prompting methods for vision-language models, including manually created natural language instructions and automatically generated prompts as natural language instructions or vector representations.results: The paper summarizes and discusses the results of prompt engineering on vision-language models, including the ability to perform predictions based solely on prompts without updating model parameters, and the easier application of large pre-trained models in real-world tasks. The paper also discusses the commonalities and differences between prompting on vision-language models, language models, and vision models, as well as the challenges, future directions, and research opportunities in this field.Here is the information in Simplified Chinese text:for: 这篇论文提供了三种视觉语言模型的前沿研究报告，包括多模态文本生成模型、图像文本匹配模型以及文本图像生成模型。methods: 论文讨论了不同类型的提示方法，包括手动创建的自然语言指令以及自动生成的提示。results: 论文总结并讨论了视觉语言模型上的提示工程结果，包括基于提示进行预测而无需更新模型参数，以及使用大型预训练模型在实际任务中更加容易应用。论文还讨论了视觉语言模型、语言模型和视模型之间的相似性和不同点，以及这一领域的挑战、未来发展和研究机遇。<details>
<summary>Abstract</summary>
Prompt engineering is a technique that involves augmenting a large pre-trained model with task-specific hints, known as prompts, to adapt the model to new tasks. Prompts can be created manually as natural language instructions or generated automatically as either natural language instructions or vector representations. Prompt engineering enables the ability to perform predictions based solely on prompts without updating model parameters, and the easier application of large pre-trained models in real-world tasks. In past years, Prompt engineering has been well-studied in natural language processing. Recently, it has also been intensively studied in vision-language modeling. However, there is currently a lack of a systematic overview of prompt engineering on pre-trained vision-language models. This paper aims to provide a comprehensive survey of cutting-edge research in prompt engineering on three types of vision-language models: multimodal-to-text generation models (e.g. Flamingo), image-text matching models (e.g. CLIP), and text-to-image generation models (e.g. Stable Diffusion). For each type of model, a brief model summary, prompting methods, prompting-based applications, and the corresponding responsibility and integrity issues are summarized and discussed. Furthermore, the commonalities and differences between prompting on vision-language models, language models, and vision models are also discussed. The challenges, future directions, and research opportunities are summarized to foster future research on this topic.
</details>
<details>
<summary>摘要</summary>
广泛应用工程技术是一种方法，它利用大型预训练模型，通过添加任务特定的提示（即提示），以适应新任务。提示可以手动创建为自然语言指令，或者生成自然语言指令或者向量表示。广泛应用工程技术允许基于提示进行预测，而不需要更新模型参数，并且可以轻松地应用大型预训练模型在实际任务中。在过去几年中，广泛应用工程技术在自然语言处理领域得到了广泛的研究。在最近几年中，它也在视觉语言模型中得到了广泛的研究。然而，目前没有一篇系统的概述了广泛应用工程技术在预训练视觉语言模型上的研究。这篇论文旨在提供了广泛应用工程技术在三种类型的预训练视觉语言模型上的全面概述：多模态到文本生成模型（例如FLAMINGO）、图像文本匹配模型（例如CLIP）和文本到图像生成模型（例如稳定扩散）。对于每种模型，我们将 briefly描述模型的概述、提示方法、基于提示的应用和相应的责任和道德问题。此外，我们还将讨论广泛应用工程技术在视觉语言模型、语言模型和视觉模型之间的相似和不同。 finally,我们将 SUMMARIZE 未来的挑战、未来方向和研究机会，以促进未来在这个领域的研究。
</details></li>
</ul>
<hr>
<h2 id="DFA3D-3D-Deformable-Attention-For-2D-to-3D-Feature-Lifting"><a href="#DFA3D-3D-Deformable-Attention-For-2D-to-3D-Feature-Lifting" class="headerlink" title="DFA3D: 3D Deformable Attention For 2D-to-3D Feature Lifting"></a>DFA3D: 3D Deformable Attention For 2D-to-3D Feature Lifting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12972">http://arxiv.org/abs/2307.12972</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/IDEA-Research/3D-deformable-attention">https://github.com/IDEA-Research/3D-deformable-attention</a></li>
<li>paper_authors: Hongyang Li, Hao Zhang, Zhaoyang Zeng, Shilong Liu, Feng Li, Tianhe Ren, Lei Zhang</li>
<li>for: 提高2D图像特征的3D检测精度，通过将多视图2D图像特征映射到一个统一的3D空间中。</li>
<li>methods: 提出了一种新的操作符，即3D DeFormable Attention (DFA3D)，用于2D-to-3D特征提升，该操作符可以帮助解决depth ambiguity问题，并且可以逐层进行特征细化。</li>
<li>results: 实验结果表明，DFA3D可以提高nuScenes数据集上的平均精度+1.41%，并且在高质量深度信息可用时可以达到+15.1%的提高。<details>
<summary>Abstract</summary>
In this paper, we propose a new operator, called 3D DeFormable Attention (DFA3D), for 2D-to-3D feature lifting, which transforms multi-view 2D image features into a unified 3D space for 3D object detection. Existing feature lifting approaches, such as Lift-Splat-based and 2D attention-based, either use estimated depth to get pseudo LiDAR features and then splat them to a 3D space, which is a one-pass operation without feature refinement, or ignore depth and lift features by 2D attention mechanisms, which achieve finer semantics while suffering from a depth ambiguity problem. In contrast, our DFA3D-based method first leverages the estimated depth to expand each view's 2D feature map to 3D and then utilizes DFA3D to aggregate features from the expanded 3D feature maps. With the help of DFA3D, the depth ambiguity problem can be effectively alleviated from the root, and the lifted features can be progressively refined layer by layer, thanks to the Transformer-like architecture. In addition, we propose a mathematically equivalent implementation of DFA3D which can significantly improve its memory efficiency and computational speed. We integrate DFA3D into several methods that use 2D attention-based feature lifting with only a few modifications in code and evaluate on the nuScenes dataset. The experiment results show a consistent improvement of +1.41\% mAP on average, and up to +15.1\% mAP improvement when high-quality depth information is available, demonstrating the superiority, applicability, and huge potential of DFA3D. The code is available at https://github.com/IDEA-Research/3D-deformable-attention.git.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一新的操作符，即3D DeFormable Attention（DFA3D），用于2D-to-3D特征提升，该操作将多视图2D图像特征转换到一个统一的3D空间中，用于3D对象检测。现有的特征提升方法，如Lift-Splat-based和2D attention-based，可以通过利用估计的深度来获得pseudo LiDAR特征，然后将其扩展到3D空间，这是一个一旦性操作而不包含特征细化，或者忽略深度并通过2D attention机制提升特征，这可以达到更细的 semantics，但是受到深度抽象问题困扰。相比之下，我们的DFA3D-based方法首先利用估计的深度来扩展每个视图的2D特征图到3D，然后通过DFA3D机制来聚合来自扩展的3D特征图中的特征。通过DFA3D的帮助，可以有效解决深度抽象问题，并且可以逐层进行特征细化， благо于Transformer-like架构。此外，我们还提出了DFA3D的数学等效实现方式，可以显著提高内存利用率和计算速度。我们将DFA3D integrate到了使用2D attention-based特征提升的一些方法中，只需要在代码中做一些微调，并对nuScenes数据集进行评估。实验结果表明，DFA3D可以提供+1.41\% mAP的平均提升，并且在高质量深度信息可用时可以达到+15.1\% mAP的最大提升，这说明DFA3D的超越、可应用性和巨大的潜力。代码可以在https://github.com/IDEA-Research/3D-deformable-attention.git中找到。
</details></li>
</ul>
<hr>
<h2 id="Volcanic-ash-delimitation-using-Artificial-Intelligence-based-on-Pix2Pix"><a href="#Volcanic-ash-delimitation-using-Artificial-Intelligence-based-on-Pix2Pix" class="headerlink" title="Volcanic ash delimitation using Artificial Intelligence based on Pix2Pix"></a>Volcanic ash delimitation using Artificial Intelligence based on Pix2Pix</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12970">http://arxiv.org/abs/2307.12970</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christian Carrillo, Gissela Torres, Christian Mejia-Escobar</li>
<li>for: 这项研究的目的是提出一种基于深度学习的 ash 云定义方法，以帮助预测和 mitigate 火山喷发的影响。</li>
<li>methods: 该方法使用 Pix2Pix 模型，一种基于生成对抗网络的技术，将多spectral 卫星图像转换为黑白 ash 云图像。</li>
<li>results: 试验结果表明，该方法可以准确地定义 ash 云，并且可以在任何地区应用。这种方法可以帮助预测和 mitigate 火山喷发的影响，成为一种有用的工具。<details>
<summary>Abstract</summary>
Volcanic eruptions emit ash that can be harmful to human health and cause damage to infrastructure, economic activities and the environment. The delimitation of ash clouds allows to know their behavior and dispersion, which helps in the prevention and mitigation of this phenomenon. Traditional methods take advantage of specialized software programs to process the bands or channels that compose the satellite images. However, their use is limited to experts and demands a lot of time and significant computational resources. In recent years, Artificial Intelligence has been a milestone in the computational treatment of complex problems in different areas. In particular, Deep Learning techniques allow automatic, fast and accurate processing of digital images. The present work proposes the use of the Pix2Pix model, a type of generative adversarial network that, once trained, learns the mapping of input images to output images. The architecture of such a network consisting of a generator and a discriminator provides the versatility needed to produce black and white ash cloud images from multispectral satellite images. The evaluation of the model, based on loss and accuracy plots, a confusion matrix, and visual inspection, indicates a satisfactory solution for accurate ash cloud delineation, applicable in any area of the world and becomes a useful tool in risk management.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Learning-Dense-Correspondences-between-Photos-and-Sketches"><a href="#Learning-Dense-Correspondences-between-Photos-and-Sketches" class="headerlink" title="Learning Dense Correspondences between Photos and Sketches"></a>Learning Dense Correspondences between Photos and Sketches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12967">http://arxiv.org/abs/2307.12967</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cogtoolslab/photo-sketch-correspondence">https://github.com/cogtoolslab/photo-sketch-correspondence</a></li>
<li>paper_authors: Xuanchen Lu, Xiaolong Wang, Judith E Fan</li>
<li>For: The paper aims to support the ability of artificial systems to understand visual images at different levels of abstraction, with a focus on sketch-photo correspondence.* Methods: The paper introduces a new sketch-photo correspondence benchmark called $\textit{PSC6k}$, which contains 150K annotations of 6250 sketch-photo pairs across 125 object categories. The authors also propose a self-supervised method for learning dense correspondences between sketch-photo pairs, using a spatial transformer network to estimate the warp flow between latent representations of a sketch and photo.* Results: The authors found that their approach outperformed several strong baselines and produced predictions that were quantitatively consistent with other warp-based methods. However, their benchmark also revealed systematic differences between predictions of the suite of models they tested and those of humans.<details>
<summary>Abstract</summary>
Humans effortlessly grasp the connection between sketches and real-world objects, even when these sketches are far from realistic. Moreover, human sketch understanding goes beyond categorization -- critically, it also entails understanding how individual elements within a sketch correspond to parts of the physical world it represents. What are the computational ingredients needed to support this ability? Towards answering this question, we make two contributions: first, we introduce a new sketch-photo correspondence benchmark, $\textit{PSC6k}$, containing 150K annotations of 6250 sketch-photo pairs across 125 object categories, augmenting the existing Sketchy dataset with fine-grained correspondence metadata. Second, we propose a self-supervised method for learning dense correspondences between sketch-photo pairs, building upon recent advances in correspondence learning for pairs of photos. Our model uses a spatial transformer network to estimate the warp flow between latent representations of a sketch and photo extracted by a contrastive learning-based ConvNet backbone. We found that this approach outperformed several strong baselines and produced predictions that were quantitatively consistent with other warp-based methods. However, our benchmark also revealed systematic differences between predictions of the suite of models we tested and those of humans. Taken together, our work suggests a promising path towards developing artificial systems that achieve more human-like understanding of visual images at different levels of abstraction. Project page: https://photo-sketch-correspondence.github.io
</details>
<details>
<summary>摘要</summary>
人类可以轻松地理解绘图和实际世界之间的连接，即使绘图不够真实。此外，人类绘图理解不仅是分类，而且还包括理解绘图中的个体元素与物理世界中的部件之间的对应关系。为解答这个问题，我们提出了两个贡献：首先，我们 introduce a new sketch-photo correspondence benchmark， $\textit{PSC6k}$，包含150,000个绘图-照片对的125个物品类别中的6,250个对。我们将现有的Sketchy数据集补充了细化的对应 metadata。其次，我们提出了一种自动学习的方法，用于学习绘图-照片对的密集对应关系。我们基于现有的对应学习方法，使用一个空间变换网络来估计绘图和照片中latent表示的截面流。我们发现这种方法在多个强基elines上表现出色，并且生成的预测与其他截面基eline相比具有较高的准确性。然而，我们的benchmark还发现系统性的差异 между模型的预测和人类的预测。总之，我们的工作表明了在不同层次的视觉图像理解方面可以开发出更人类化的人工系统。我们的研究可能会为视觉计算机科学和机器学习领域的发展提供新的思路和方法。项目页面：https://photo-sketch-correspondence.github.io
</details></li>
</ul>
<hr>
<h2 id="Audio-Enhanced-Text-to-Video-Retrieval-using-Text-Conditioned-Feature-Alignment"><a href="#Audio-Enhanced-Text-to-Video-Retrieval-using-Text-Conditioned-Feature-Alignment" class="headerlink" title="Audio-Enhanced Text-to-Video Retrieval using Text-Conditioned Feature Alignment"></a>Audio-Enhanced Text-to-Video Retrieval using Text-Conditioned Feature Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12964">http://arxiv.org/abs/2307.12964</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sarah Ibrahimi, Xiaohang Sun, Pichao Wang, Amanmeet Garg, Ashutosh Sanan, Mohamed Omar</li>
<li>for: This paper focuses on the task of text-to-video retrieval, specifically addressing the issue of neglecting audio information in previous methods.</li>
<li>methods: The proposed method, TEFAL, uses two independent cross-modal attention blocks to enable the text to attend to the audio and video representations separately, producing both audio and video representations conditioned on the text query.</li>
<li>results: The proposed method achieves better than state-of-the-art performance consistently across four benchmark datasets, including MSR-VTT, LSMDC, VATEX, and Charades, demonstrating its efficacy in capturing complementary audio and video information pertinent to the text query.Here’s the simplified Chinese version of the three key points:</li>
<li>for: 这篇论文关注了文本到视频回归任务，特别是之前的方法忽略了音频信息的问题。</li>
<li>methods: 提议的方法TEFAL使用了两个独立的跨模态注意力块，使文本能够独立地对音频和视频表示进行注意力调整，生成了基于文本查询的音频和视频表示。</li>
<li>results: 提议的方法在四个标准测试集MSR-VTT、LSMDC、VATEX和Charades上取得了比前STATE-OF-THE-ART性能更好的结果， demonstarting its efficacy in capturing相关的音频和视频信息。<details>
<summary>Abstract</summary>
Text-to-video retrieval systems have recently made significant progress by utilizing pre-trained models trained on large-scale image-text pairs. However, most of the latest methods primarily focus on the video modality while disregarding the audio signal for this task. Nevertheless, a recent advancement by ECLIPSE has improved long-range text-to-video retrieval by developing an audiovisual video representation. Nonetheless, the objective of the text-to-video retrieval task is to capture the complementary audio and video information that is pertinent to the text query rather than simply achieving better audio and video alignment. To address this issue, we introduce TEFAL, a TExt-conditioned Feature ALignment method that produces both audio and video representations conditioned on the text query. Instead of using only an audiovisual attention block, which could suppress the audio information relevant to the text query, our approach employs two independent cross-modal attention blocks that enable the text to attend to the audio and video representations separately. Our proposed method's efficacy is demonstrated on four benchmark datasets that include audio: MSR-VTT, LSMDC, VATEX, and Charades, and achieves better than state-of-the-art performance consistently across the four datasets. This is attributed to the additional text-query-conditioned audio representation and the complementary information it adds to the text-query-conditioned video representation.
</details>
<details>
<summary>摘要</summary>
Text-to-video遥感系统在最近几年内已经取得了重要进步，通过使用预训练模型，这些模型在大规模的图像-文本对中训练。然而，大多数最新的方法主要关注视频模式，而忽略了声音信号。然而，ECLIPSE的最新进展已经改进了长距离文本-视频遥感。不过，文本-视频遥感任务的目标是捕捉文本查询中相关的声音和视频信息，而不仅仅是实现更好的声音和视频对齐。为解决这个问题，我们提出了TEFAL方法，它是一种基于文本查询的特征对齐方法，它生成了基于文本查询的声音和视频表示。相比使用仅仅的 audiovisual注意块，我们的方法使用两个独立的跨模态注意块，这些注意块使得文本能够独立地对声音和视频表示进行注意。我们的提议方法在四个标准数据集上进行了评估，这些数据集包括声音：MSR-VTT、LSMDC、VATEX和Charades，并在这些数据集上达到了比前方的性能。这是因为我们的方法添加了基于文本查询的声音表示，这个表示提供了与文本查询conditioned的视频表示相 complementary的信息。
</details></li>
</ul>
<hr>
<h2 id="HOOD-Real-Time-Robust-Human-Presence-and-Out-of-Distribution-Detection-with-Low-Cost-FMCW-Radar"><a href="#HOOD-Real-Time-Robust-Human-Presence-and-Out-of-Distribution-Detection-with-Low-Cost-FMCW-Radar" class="headerlink" title="HOOD: Real-Time Robust Human Presence and Out-of-Distribution Detection with Low-Cost FMCW Radar"></a>HOOD: Real-Time Robust Human Presence and Out-of-Distribution Detection with Low-Cost FMCW Radar</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02396">http://arxiv.org/abs/2308.02396</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sabri Mustafa Kahya, Muhammet Sami Yavuz, Eckehard Steinbach</li>
<li>for: 这个论文的目的是提出一种实时稳定的人员存在检测方法，以解决在室内环境中 millimeter-wave频率调制连续扫描（FMCW）雷达中人员存在检测的挑战。</li>
<li>methods: 该方法基于60GHz短距离FMCW雷达，并利用干扰图像（RDI）来实现实时人员存在和异常检测。方法根据存在或缺失人员的情况，将检测问题转化为异常检测问题，并通过一种重建性架构来实现。</li>
<li>results: 在使用60GHz短距离FMCW雷达进行数据收集后，该方法在HOOD测试数据集上 achieve an average AUROC of 94.36%。此外，对比之前的State-of-the-art（SOTA）异常检测方法，HOOD方法在常见的异常检测指标上表现更高。实时实验结果可以在<a target="_blank" rel="noopener" href="https://muskahya.github.io/HOOD%E4%B8%AD%E6%9F%A5%E7%9C%8B%E3%80%82">https://muskahya.github.io/HOOD中查看。</a><details>
<summary>Abstract</summary>
Human presence detection in indoor environments using millimeter-wave frequency-modulated continuous-wave (FMCW) radar is challenging due to the presence of moving and stationary clutters in indoor places. This work proposes "HOOD" as a real-time robust human presence and out-of-distribution (OOD) detection method by exploiting 60 GHz short-range FMCW radar. We approach the presence detection application as an OOD detection problem and solve the two problems simultaneously using a single pipeline. Our solution relies on a reconstruction-based architecture and works with radar macro and micro range-Doppler images (RDIs). HOOD aims to accurately detect the "presence" of humans in the presence or absence of moving and stationary disturbers. Since it is also an OOD detector, it aims to detect moving or stationary clutters as OOD in humans' absence and predicts the current scene's output as "no presence." HOOD is an activity-free approach that performs well in different human scenarios. On our dataset collected with a 60 GHz short-range FMCW Radar, we achieve an average AUROC of 94.36%. Additionally, our extensive evaluations and experiments demonstrate that HOOD outperforms state-of-the-art (SOTA) OOD detection methods in terms of common OOD detection metrics. Our real-time experiments are available at: https://muskahya.github.io/HOOD
</details>
<details>
<summary>摘要</summary>
人体存在检测在室内环境中使用毫米波频率调制连续波（FMCW）雷达是具有挑战性，因为室内存在移动和静止干扰物。这项工作提出了“HOOD”实时可靠人体存在和异常检测方法，通过利用60GHz短距离FMCW雷达。我们将存在检测应用作为异常检测问题，并将两个问题同时解决在单一管道中。我们的解决方案基于重建 architecture，并与雷达宽 macro和微范围Doppler图像（RDI）结合。HOOD hopes to accurately detect the "presence" of humans in the presence or absence of moving and stationary disturbers。此外，它还 hopes to检测移动或静止干扰物作为异常，并预测当前场景的输出为“无存”。HOOD是一种活动无关的方法，在不同的人类场景中表现良好。根据我们收集的60GHz短距离FMCW雷达数据集，我们实现了平均AUROC为94.36%。此外，我们的广泛评估和实验表明，HOOD在常见异常检测指标上表现出色，超过了现状顶峰（SOTA）异常检测方法。实时实验可以在：https://muskahya.github.io/HOOD
</details></li>
</ul>
<hr>
<h2 id="Dyn-E-Local-Appearance-Editing-of-Dynamic-Neural-Radiance-Fields"><a href="#Dyn-E-Local-Appearance-Editing-of-Dynamic-Neural-Radiance-Fields" class="headerlink" title="Dyn-E: Local Appearance Editing of Dynamic Neural Radiance Fields"></a>Dyn-E: Local Appearance Editing of Dynamic Neural Radiance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12909">http://arxiv.org/abs/2307.12909</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shangzhan Zhang, Sida Peng, Yinji ShenTu, Qing Shuai, Tianrun Chen, Kaicheng Yu, Hujun Bao, Xiaowei Zhou</li>
<li>for: 本文提出了一种新的方法，用于编辑动态场景中的NeRF的本地外观。</li>
<li>methods: 本方法使用了一种新的表示方法，将编辑区域的表示插入到原始NeRF和旋转学习网络中，以便在不同的帧数据上进行渲染和插值。</li>
<li>results: 经过广泛的评估，本方法可以准确地编辑动态场景中的NeRF外观，并且可以保持空间和时间上的一致性。<details>
<summary>Abstract</summary>
Recently, the editing of neural radiance fields (NeRFs) has gained considerable attention, but most prior works focus on static scenes while research on the appearance editing of dynamic scenes is relatively lacking. In this paper, we propose a novel framework to edit the local appearance of dynamic NeRFs by manipulating pixels in a single frame of training video. Specifically, to locally edit the appearance of dynamic NeRFs while preserving unedited regions, we introduce a local surface representation of the edited region, which can be inserted into and rendered along with the original NeRF and warped to arbitrary other frames through a learned invertible motion representation network. By employing our method, users without professional expertise can easily add desired content to the appearance of a dynamic scene. We extensively evaluate our approach on various scenes and show that our approach achieves spatially and temporally consistent editing results. Notably, our approach is versatile and applicable to different variants of dynamic NeRF representations.
</details>
<details>
<summary>摘要</summary>
近些时候，神经辐射场（NeRF）的编辑技术已经吸引了广泛的关注，但大多数前一些作品都是静止场景的，关于动态场景的外观编辑研究相对较少。在这篇论文中，我们提出了一种新的框架，用于编辑动态NeRF的本地外观。具体来说，我们引入了一种修改区域的本地表面表示，可以在训练视频帧中插入并与原始NeRF和扭曲学习的运动表示网络一起渲染。通过我们的方法，用户无需专业技能就可以轻松地添加愿望的内容到动态场景的外观中。我们对多个场景进行了广泛的评估，并证明了我们的方法可以在空间和时间上实现一致的编辑结果。值得一提的是，我们的方法可以应用于不同的动态NeRF表示方式。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/25/cs.CV_2023_07_25/" data-id="cloqtaeq600fjgh88ddku8vvk" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_07_25" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/25/cs.AI_2023_07_25/" class="article-date">
  <time datetime="2023-07-25T12:00:00.000Z" itemprop="datePublished">2023-07-25</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/25/cs.AI_2023_07_25/">cs.AI - 2023-07-25</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Argument-Attribution-Explanations-in-Quantitative-Bipolar-Argumentation-Frameworks-Technical-Report"><a href="#Argument-Attribution-Explanations-in-Quantitative-Bipolar-Argumentation-Frameworks-Technical-Report" class="headerlink" title="Argument Attribution Explanations in Quantitative Bipolar Argumentation Frameworks (Technical Report)"></a>Argument Attribution Explanations in Quantitative Bipolar Argumentation Frameworks (Technical Report)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13582">http://arxiv.org/abs/2307.13582</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Yin, Nico Potyka, Francesca Toni</li>
<li>for: 这篇论文旨在解释Argumentation Frameworks（AFs）的量化结果，具体来说是解释Quantitative Bipolar Argumentation Frameworks（QBAFs）中的话题论点。</li>
<li>methods: 该论文提出了一种新的Argument Attribution Explanations（AAEs）理论，兼用机器学习中的特征归因来解释AFs中的论点。</li>
<li>results: 论文通过两个实践案例（即假新闻检测和电影推荐系统）来示例AAEs的应用性。<details>
<summary>Abstract</summary>
Argumentative explainable AI has been advocated by several in recent years, with an increasing interest on explaining the reasoning outcomes of Argumentation Frameworks (AFs). While there is a considerable body of research on qualitatively explaining the reasoning outcomes of AFs with debates/disputes/dialogues in the spirit of extension-based semantics, explaining the quantitative reasoning outcomes of AFs under gradual semantics has not received much attention, despite widespread use in applications. In this paper, we contribute to filling this gap by proposing a novel theory of Argument Attribution Explanations (AAEs) by incorporating the spirit of feature attribution from machine learning in the context of Quantitative Bipolar Argumentation Frameworks (QBAFs): whereas feature attribution is used to determine the influence of features towards outputs of machine learning models, AAEs are used to determine the influence of arguments towards topic arguments of interest. We study desirable properties of AAEs, including some new ones and some partially adapted from the literature to our setting. To demonstrate the applicability of our AAEs in practice, we conclude by carrying out two case studies in the scenarios of fake news detection and movie recommender systems.
</details>
<details>
<summary>摘要</summary>
争议解释AI在最近几年来得到了许多人的支持，感兴趣的是解释Argumentation Frameworks（AFs）的结果的逻辑过程。虽然有许多关于使用辩论/争议/对话来解释AFs的质量的研究，但是对于使用加权 semantics来解释AFs的量化逻辑结果没有很多关注，尽管这在应用中广泛使用。在这篇论文中，我们减轻这一点的空白，我们提出了一种新的Argument Attribution Explanations（AAEs）理论，该理论基于机器学习中的特征归因，用于解释QBAFs中的话题Arguments。而特征归因用于确定机器学习模型输出的特征对输出产生的影响，AAEs则用于确定话题Arguments对QBAFs中的话题Arguments的影响。我们研究了AAEs的愉悦性质，包括一些新的和一些从文献中部分适应我们的设置。为了证明AAEs在实践中的可行性，我们在假新闻检测和电影推荐系统两个场景中进行了两个案例研究。
</details></li>
</ul>
<hr>
<h2 id="Reinterpreting-survival-analysis-in-the-universal-approximator-age"><a href="#Reinterpreting-survival-analysis-in-the-universal-approximator-age" class="headerlink" title="Reinterpreting survival analysis in the universal approximator age"></a>Reinterpreting survival analysis in the universal approximator age</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13579">http://arxiv.org/abs/2307.13579</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sdittmer/survival_analysis_sumo_plus_plus">https://github.com/sdittmer/survival_analysis_sumo_plus_plus</a></li>
<li>paper_authors: Sören Dittmer, Michael Roberts, Jacobus Preller, AIX COVNET, James H. F. Rudd, John A. D. Aston, Carola-Bibiane Schönlieb</li>
<li>for: 本研究旨在提供用于深度学习中survival分析的工具，以便充分发挥survival分析的潜在力量。</li>
<li>methods: 本研究使用的方法包括新的损失函数、评价指标和首个universal approximating网络，这些工具可以无需数值integation生成survival曲线。</li>
<li>results: 研究表明，新的损失函数和模型在大规模的数据研究中表现出色，超过其他方法的表现。<details>
<summary>Abstract</summary>
Survival analysis is an integral part of the statistical toolbox. However, while most domains of classical statistics have embraced deep learning, survival analysis only recently gained some minor attention from the deep learning community. This recent development is likely in part motivated by the COVID-19 pandemic. We aim to provide the tools needed to fully harness the potential of survival analysis in deep learning. On the one hand, we discuss how survival analysis connects to classification and regression. On the other hand, we provide technical tools. We provide a new loss function, evaluation metrics, and the first universal approximating network that provably produces survival curves without numeric integration. We show that the loss function and model outperform other approaches using a large numerical study.
</details>
<details>
<summary>摘要</summary>
生存分析是统计工具箱中的一个重要组成部分。然而，在经典统计领域中，深度学习已经广泛应用，而生存分析则只是在深度学习社区中最近才得到了一些微的注意。这种最近的发展可能与COVID-19大流行有关。我们的目标是为生存分析在深度学习中充分发挥作用提供工具。一方面，我们讨论了生存分析与分类和回归之间的联系。另一方面，我们提供了技术工具。我们提出了一个新的损失函数、评估指标和首个可靠地生成Survival Curve的网络。我们通过大规模的数值研究表明，我们的损失函数和模型在其他方法的比较中表现出色。
</details></li>
</ul>
<hr>
<h2 id="A-Dual-mode-Local-Search-Algorithm-for-Solving-the-Minimum-Dominating-Set-Problem"><a href="#A-Dual-mode-Local-Search-Algorithm-for-Solving-the-Minimum-Dominating-Set-Problem" class="headerlink" title="A Dual-mode Local Search Algorithm for Solving the Minimum Dominating Set Problem"></a>A Dual-mode Local Search Algorithm for Solving the Minimum Dominating Set Problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16815">http://arxiv.org/abs/2307.16815</a></li>
<li>repo_url: None</li>
<li>paper_authors: Enqiang Zhu, Yu Zhang, Shengzhi Wang, Darren Strash, Chanjuan Liu</li>
<li>for: 解决图形中最小控制集（MinDS）问题，即找到一个最小的集合 $D$，使得每个不在 $D$ 中的顶点都与至少一个 $D$ 中的顶点相邻。</li>
<li>methods: 我们提出了一种有效的本地搜索算法（DmDS），它采用了两种不同的顶点交换方案来解决MinDS问题。此外，我们还提出了一种基于频率的顶点选择 criterion，以解决其他算法中的各种绑定情况，以及一种新的Initial Solution质量提高策略，基于批处理和扰动。</li>
<li>results: 我们对 seven 个数据集进行了比较，包括 346 个实例（或家族），最多有十亿个顶点。实验结果表明，DmDS 在大多数实例中具有最高的准确率，并在广泛的实际图形上发现了许多更好的解决方案。<details>
<summary>Abstract</summary>
Given a graph, the minimum dominating set (MinDS) problem is to identify a smallest set $D$ of vertices such that every vertex not in $D$ is adjacent to at least one vertex in $D$. The MinDS problem is a classic $\mathcal{NP}$-hard problem and has been extensively studied because of its many disparate applications in network analysis. To solve this problem efficiently, many heuristic approaches have been proposed to obtain a good solution within an acceptable time limit. However, existing MinDS heuristic algorithms are always limited by various tie-breaking cases when selecting vertices, which slows down the effectiveness of the algorithms. In this paper, we design an efficient local search algorithm for the MinDS problem, named DmDS -- a dual-mode local search framework that probabilistically chooses between two distinct vertex-swapping schemes. We further address limitations of other algorithms by introducing vertex selection criterion based on the frequency of vertices added to solutions to address tie-breaking cases, and a new strategy to improve the quality of the initial solution via a greedy-based strategy integrated with perturbation. We evaluate DmDS against the state-of-the-art algorithms on seven datasets, consisting of 346 instances (or families) with up to tens of millions of vertices. Experimental results show that DmDS obtains the best performance in accuracy for almost all instances and finds much better solutions than state-of-the-art MinDS algorithms on a broad range of large real-world graphs.
</details>
<details>
<summary>摘要</summary>
Existing MinDS heuristic algorithms are limited by various tie-breaking cases when selecting vertices, which slows down their effectiveness. In this paper, we propose an efficient local search algorithm for the MinDS problem, called DmDS, which uses a dual-mode local search framework that probabilistically chooses between two distinct vertex-swapping schemes.To address limitations of other algorithms, we introduce a vertex selection criterion based on the frequency of vertices added to solutions to address tie-breaking cases, and a new strategy to improve the quality of the initial solution via a greedy-based strategy integrated with perturbation.We evaluate DmDS against state-of-the-art algorithms on seven datasets, consisting of 346 instances (or families) with up to tens of millions of vertices. Experimental results show that DmDS obtains the best performance in accuracy for almost all instances and finds much better solutions than state-of-the-art MinDS algorithms on a broad range of large real-world graphs.Here is the text in Simplified Chinese:给定一个图，最小控制集（MinDS）问题是找到最小的集合 $D$ 的 vertices，使得每个不在 $D$ 中的 vertex 都与至少一个在 $D$ 中的 vertex 相邻。这是一个 класси的 $\mathcal{NP}$-hard 问题，广泛的研究了因为它在网络分析中的许多实际应用。现有的 MinDS 规则算法都受到不同的选择情况的限制，这会使得它们的效iveness降低。在这篇论文中，我们提出一种高效的本地搜索算法 для MinDS 问题，名为 DmDS，它使用了一种 dual-mode 本地搜索框架， probabilistically 选择两种不同的 vertex-swapping 策略。为了解决其他算法的限制，我们引入一个基于频率的 vertex 选择标准，以 Address 选择情况中的僵尸性，并 introducing a new strategy to improve the quality of the initial solution via a greedy-based strategy integrated with perturbation。我们对 seven 个 datasets 进行了对比，这些 datasets 包括 346 个实例（或家族），最多达到了 tens of millions 的 vertices。实验结果显示，DmDS 在大多数实例中具有最高的准确性，并在许多实际世界图上发现了 much better 的解决方案，远超现有的 MinDS 算法。
</details></li>
</ul>
<hr>
<h2 id="The-Impact-of-Imperfect-XAI-on-Human-AI-Decision-Making"><a href="#The-Impact-of-Imperfect-XAI-on-Human-AI-Decision-Making" class="headerlink" title="The Impact of Imperfect XAI on Human-AI Decision-Making"></a>The Impact of Imperfect XAI on Human-AI Decision-Making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13566">http://arxiv.org/abs/2307.13566</a></li>
<li>repo_url: None</li>
<li>paper_authors: Katelyn Morrison, Philipp Spitzer, Violet Turri, Michelle Feng, Niklas Kühl, Adam Perer</li>
<li>for: 这研究旨在探讨人类和AI协作中如何处理不准确的解释，以提高人类和AI协作的效果。</li>
<li>methods: 本研究采用了混合方法，包括人类参与者136人的混合研究，以评估人类在鸟种识别任务中对不准确解释的影响。</li>
<li>results: 研究发现，不准确解释会影响人类对AI的依赖度和人类-AI团队性能。此外，解释的强度也影响人类的决策行为。这些发现有助于理解人类和AI协作中的不准确解释的影响，并提供设计人类-AI协作系统的指南。<details>
<summary>Abstract</summary>
Explainability techniques are rapidly being developed to improve human-AI decision-making across various cooperative work settings. Consequently, previous research has evaluated how decision-makers collaborate with imperfect AI by investigating appropriate reliance and task performance with the aim of designing more human-centered computer-supported collaborative tools. Several human-centered explainable AI (XAI) techniques have been proposed in hopes of improving decision-makers' collaboration with AI; however, these techniques are grounded in findings from previous studies that primarily focus on the impact of incorrect AI advice. Few studies acknowledge the possibility for the explanations to be incorrect even if the AI advice is correct. Thus, it is crucial to understand how imperfect XAI affects human-AI decision-making. In this work, we contribute a robust, mixed-methods user study with 136 participants to evaluate how incorrect explanations influence humans' decision-making behavior in a bird species identification task taking into account their level of expertise and an explanation's level of assertiveness. Our findings reveal the influence of imperfect XAI and humans' level of expertise on their reliance on AI and human-AI team performance. We also discuss how explanations can deceive decision-makers during human-AI collaboration. Hence, we shed light on the impacts of imperfect XAI in the field of computer-supported cooperative work and provide guidelines for designers of human-AI collaboration systems.
</details>
<details>
<summary>摘要</summary>
<<SYS>>人工智能技术在协作工作场景中快速发展，以提高人机协作决策。先前的研究已经评估了人与不完美AI的协作方式，并设计了更人类中心的计算机支持协作工具。然而，这些技术多数基于先前研究中关注 incorrect AI 建议的影响。很少的研究承认可能存在 incorrect 的解释，即使 AI 建议正确。因此，理解 incorrect XAI 如何影响人机协作决策是关键。在这种情况下，我们通过一项强大的混合方法用户研究，卷入 136 名参与者，评估 incorrect 解释如何影响人们决策行为，包括他们的专业水平和解释的强硬程度。我们的发现表明 incorrect XAI 和参与者的专业水平对人机协作的可靠性和性能产生了影响。我们还讨论了解释如何在人机协作中欺骗决策者。因此，我们为计算机支持协作系统的设计提供了指导，并着重于人机协作中 incorrect XAI 的影响。
</details></li>
</ul>
<hr>
<h2 id="Decision-Focused-Learning-Foundations-State-of-the-Art-Benchmark-and-Future-Opportunities"><a href="#Decision-Focused-Learning-Foundations-State-of-the-Art-Benchmark-and-Future-Opportunities" class="headerlink" title="Decision-Focused Learning: Foundations, State of the Art, Benchmark and Future Opportunities"></a>Decision-Focused Learning: Foundations, State of the Art, Benchmark and Future Opportunities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13565">http://arxiv.org/abs/2307.13565</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/predopt/predopt-benchmarks">https://github.com/predopt/predopt-benchmarks</a></li>
<li>paper_authors: Jayanta Mandi, James Kotary, Senne Berden, Maxime Mulamba, Victor Bucarey, Tias Guns, Ferdinando Fioretto</li>
<li>for: 这篇论文主要是为了介绍决策关注学习（DFL）这一新兴机器学习 paradigma，它将预测和优化结合在一个端到端系统中，以便在不确定环境下做出优化决策。</li>
<li>methods: 论文介绍了各种将机器学习和优化模型集成的技术，并提出了一种分类DFL方法的 Taxonomy，以及一些适用于DFL的测试数据集和任务。</li>
<li>results: 论文进行了广泛的实验评估，对DFL方法进行了valuable的探索和评估，并提供了有价值的Future research direction。<details>
<summary>Abstract</summary>
Decision-focused learning (DFL) is an emerging paradigm in machine learning which trains a model to optimize decisions, integrating prediction and optimization in an end-to-end system. This paradigm holds the promise to revolutionize decision-making in many real-world applications which operate under uncertainty, where the estimation of unknown parameters within these decision models often becomes a substantial roadblock. This paper presents a comprehensive review of DFL. It provides an in-depth analysis of the various techniques devised to integrate machine learning and optimization models, introduces a taxonomy of DFL methods distinguished by their unique characteristics, and conducts an extensive empirical evaluation of these methods proposing suitable benchmark dataset and tasks for DFL. Finally, the study provides valuable insights into current and potential future avenues in DFL research.
</details>
<details>
<summary>摘要</summary>
决策关注学习（DFL）是一种emerging paradigm在机器学习领域，它允许模型通过端到端系统来优化决策，并将预测和优化结合在一起。这种方法在不确定环境下进行决策，对决策模型中未知参数的估计成为了一个重要的障碍。本文提供了DFL的全面回顾，包括不同方法的集成、机器学习和优化模型的分类、以及对这些方法的广泛实验评估。最后，研究还提供了DFL研究的当前和未来可能的方向。Here's the translation of the text in Traditional Chinese:决策关注学习（DFL）是一种emerging paradigm在机器学习领域，它允许模型透过端到端系统来优化决策，并将预测和优化结合在一起。这种方法在不确定环境下进行决策，对决策模型中未知参数的估计成为了一个重要的障碍。本文提供了DFL的全面回顾，包括不同方法的集成、机器学习和优化模型的分类、以及对这些方法的广泛实验评估。最后，研究还提供了DFL研究的现在和未来可能的方向。
</details></li>
</ul>
<hr>
<h2 id="On-Solving-the-Rubik’s-Cube-with-Domain-Independent-Planners-Using-Standard-Representations"><a href="#On-Solving-the-Rubik’s-Cube-with-Domain-Independent-Planners-Using-Standard-Representations" class="headerlink" title="On Solving the Rubik’s Cube with Domain-Independent Planners Using Standard Representations"></a>On Solving the Rubik’s Cube with Domain-Independent Planners Using Standard Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13552">http://arxiv.org/abs/2307.13552</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bharath Muppasani, Vishal Pallagani, Biplav Srivastava, Forest Agostinelli<br>for:这篇论文的目的是将 Rubik’s Cube  puzzle 表示为 PDDL 语言，以便更好地访问 PDDL  планировщики、竞赛和知识工程工具，并使其更易于人类阅读。methods:该论文使用了 PDDL 语言表示 Rubik’s Cube  puzzle，并与现有的方法进行比较。其中包括使用 DeepCubeA 搜索算法和 Scorpion  планировщиker 的 State-Action-Space+ 表示方法，以及 FastDownward 搜索算法和 FF 规则的组合。results:该论文的实验结果显示，使用 PDDL 语言表示 Rubik’s Cube  puzzle可以提高 solve 率，但是不同的表示方法和搜索算法之间存在负荷和优化的贸易offs。Specifically, DeepCubeA 搜索算法可以解决所有问题，但只有78.5%是优化的计划；Scorpion  планировщиker 可以解决61.50%的问题，其中79.64%是优化的计划。<details>
<summary>Abstract</summary>
Rubik's Cube (RC) is a well-known and computationally challenging puzzle that has motivated AI researchers to explore efficient alternative representations and problem-solving methods. The ideal situation for planning here is that a problem be solved optimally and efficiently represented in a standard notation using a general-purpose solver and heuristics. The fastest solver today for RC is DeepCubeA with a custom representation, and another approach is with Scorpion planner with State-Action-Space+ (SAS+) representation. In this paper, we present the first RC representation in the popular PDDL language so that the domain becomes more accessible to PDDL planners, competitions, and knowledge engineering tools, and is more human-readable. We then bridge across existing approaches and compare performance. We find that in one comparable experiment, DeepCubeA (trained with 12 RC actions) solves all problems with varying complexities, albeit only 78.5% are optimal plans. For the same problem set, Scorpion with SAS+ representation and pattern database heuristics solves 61.50% problems optimally, while FastDownward with PDDL representation and FF heuristic solves 56.50% problems, out of which 79.64% of the plans generated were optimal. Our study provides valuable insights into the trade-offs between representational choice and plan optimality that can help researchers design future strategies for challenging domains combining general-purpose solving methods (planning, reinforcement learning), heuristics, and representations (standard or custom).
</details>
<details>
<summary>摘要</summary>
瑞比克立方体（RC）是一个知名且 computationally challenging 的游戏，它激发了人工智能研究者们开发高效的代表法和解决方法。理想情况是将问题解决得最优化地，使用标准notation representation 和通用的解决器和规则。目前最快的解决器是 DeepCubeA  WITH custom representation，另一种方法是使用 Scorpion  плаanner WITH State-Action-Space+（SAS+) representation。在这篇论文中，我们将 RC 的第一个 representation 在流行的 PDDL 语言中提供，使得Domain 变得更加可达性和可读性更高，并且可以用于 PDDL  плаanner、竞赛和知识工程工具。然后，我们将现有的方法相互连接，并比较性能。我们发现在一个相同的实验中，DeepCubeA（已经训练有 12 RC 动作）可以解决具有不同复杂性的所有问题，但只有 78.5% 是优化的方案。对于同一个问题集，Scorpion  WITH SAS+ representation 和模式数据库规则可以解决 61.50% 问题，而 FastDownward  WITH PDDL representation 和 FF 规则可以解决 56.50% 问题，其中 79.64% 的方案是优化的。我们的研究提供了有价值的对于表示选择和方案优化的交易所，可以帮助研究人员设计未来在复杂的 Domain 中结合通用解决方法（规划、强化学习）、规则和表示（标准或自定义）的策略。
</details></li>
</ul>
<hr>
<h2 id="A-Planning-Ontology-to-Represent-and-Exploit-Planning-Knowledge-for-Performance-Efficiency"><a href="#A-Planning-Ontology-to-Represent-and-Exploit-Planning-Knowledge-for-Performance-Efficiency" class="headerlink" title="A Planning Ontology to Represent and Exploit Planning Knowledge for Performance Efficiency"></a>A Planning Ontology to Represent and Exploit Planning Knowledge for Performance Efficiency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13549">http://arxiv.org/abs/2307.13549</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bharath Muppasani, Vishal Pallagani, Biplav Srivastava, Raghava Mutharaju, Michael N. Huhns, Vignesh Narayanan</li>
<li>for: 本研究旨在解决自动规划问题，即找到将智能机器人从初始状态转移到目标状态的有效动作序列。</li>
<li>methods: 本研究使用国际规划竞赛（IPC）数据 construct了规划ontology，并通过实验在两个用例中示出ontology可以选择有potential的规划器并提高其性能using macros。</li>
<li>results: 实验结果表明，使用规划ontology可以选择适合域的规划器并提高其性能。同时，研究者还为社区提供了规划ontology和相关资源，以便进一步研究。<details>
<summary>Abstract</summary>
Ontologies are known for their ability to organize rich metadata, support the identification of novel insights via semantic queries, and promote reuse. In this paper, we consider the problem of automated planning, where the objective is to find a sequence of actions that will move an agent from an initial state of the world to a desired goal state. We hypothesize that given a large number of available planners and diverse planning domains; they carry essential information that can be leveraged to identify suitable planners and improve their performance for a domain. We use data on planning domains and planners from the International Planning Competition (IPC) to construct a planning ontology and demonstrate via experiments in two use cases that the ontology can lead to the selection of promising planners and improving their performance using macros - a form of action ordering constraints extracted from planning ontology. We also make the planning ontology and associated resources available to the community to promote further research.
</details>
<details>
<summary>摘要</summary>
Ontologies 知道如何组织富有 metadata，支持通过semantic queries提取新的发现，并促进重用。在这篇论文中，我们考虑自动规划问题， objective 是找到一个将智能机器从初始状态转移到目标状态的 sequences of actions。我们假设， given 大量可用的 плаanner 和多样化的规划领域; 它们携带着重要信息，可以用来选择适合的 плаanner 并提高其性能。我们使用国际规划竞赛（IPC）的数据construct 规划ontology，并通过实验示例二进行了证明，规划ontology 可以选择promising planners 并使其性能提高。我们还将规划ontology 和相关资源公开发布，以便进一步的研究。
</details></li>
</ul>
<hr>
<h2 id="Group-Activity-Recognition-in-Computer-Vision-A-Comprehensive-Review-Challenges-and-Future-Perspectives"><a href="#Group-Activity-Recognition-in-Computer-Vision-A-Comprehensive-Review-Challenges-and-Future-Perspectives" class="headerlink" title="Group Activity Recognition in Computer Vision: A Comprehensive Review, Challenges, and Future Perspectives"></a>Group Activity Recognition in Computer Vision: A Comprehensive Review, Challenges, and Future Perspectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13541">http://arxiv.org/abs/2307.13541</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chuanchuan Wang, Ahmad Sufril Azlan Mohamed</li>
<li>For: 这篇论文主要研究目标是为了提高群体活动识别技术，具体来说是通过Global interactivity和活动的方式进行识别。* Methods: 这篇论文使用了多种方法，包括传统方法、基于空间结构的方法、描述符、非深度学习方法、层次回归神经网络（HRNN）、关系模型和注意机制等。* Results: 这篇论文对群体活动识别方法进行了全面的审视和比较，并提出了一种基于关系网络的模块化方法，并进行了实验验证。<details>
<summary>Abstract</summary>
Group activity recognition is a hot topic in computer vision. Recognizing activities through group relationships plays a vital role in group activity recognition. It holds practical implications in various scenarios, such as video analysis, surveillance, automatic driving, and understanding social activities. The model's key capabilities encompass efficiently modeling hierarchical relationships within a scene and accurately extracting distinctive spatiotemporal features from groups. Given this technology's extensive applicability, identifying group activities has garnered significant research attention. This work examines the current progress in technology for recognizing group activities, with a specific focus on global interactivity and activities. Firstly, we comprehensively review the pertinent literature and various group activity recognition approaches, from traditional methodologies to the latest methods based on spatial structure, descriptors, non-deep learning, hierarchical recurrent neural networks (HRNN), relationship models, and attention mechanisms. Subsequently, we present the relational network and relational architectures for each module. Thirdly, we investigate methods for recognizing group activity and compare their performance with state-of-the-art technologies. We summarize the existing challenges and provide comprehensive guidance for newcomers to understand group activity recognition. Furthermore, we review emerging perspectives in group activity recognition to explore new directions and possibilities.
</details>
<details>
<summary>摘要</summary>
There has been significant research attention on identifying group activities, and this work aims to provide a comprehensive review of the current progress in this field. We will focus on global interactivity and activities, and our approach will include the following steps:1. Literature review: We will review the relevant literature and various group activity recognition approaches, from traditional methodologies to the latest methods based on spatial structure, descriptors, non-deep learning, hierarchical recurrent neural networks (HRNN), relationship models, and attention mechanisms.2. Relational network and architectures: We will present the relational network and relational architectures for each module.3. Methods for recognizing group activity: We will investigate methods for recognizing group activity and compare their performance with state-of-the-art technologies.4. Challenges and future directions: We will summarize the existing challenges and provide comprehensive guidance for newcomers to understand group activity recognition. Additionally, we will review emerging perspectives in group activity recognition to explore new directions and possibilities.Overall, this work aims to provide a comprehensive overview of the current state of group activity recognition technology and its applications, as well as to explore new directions and possibilities for future research.
</details></li>
</ul>
<hr>
<h2 id="Spectrum-guided-Multi-granularity-Referring-Video-Object-Segmentation"><a href="#Spectrum-guided-Multi-granularity-Referring-Video-Object-Segmentation" class="headerlink" title="Spectrum-guided Multi-granularity Referring Video Object Segmentation"></a>Spectrum-guided Multi-granularity Referring Video Object Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13537">http://arxiv.org/abs/2307.13537</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bo-miao/sgmg">https://github.com/bo-miao/sgmg</a></li>
<li>paper_authors: Bo Miao, Mohammed Bennamoun, Yongsheng Gao, Ajmal Mian</li>
<li>for: 这个论文是为了解决现有的视频对象 segmentation (R-VOS) 技术中的 feature drift 问题，以提高 segmentation 效果。</li>
<li>methods: 该论文提出了一种 Spectrum-guided Multi-granularity (SgMg) 方法， Direct segmentation 在编码特征上进行，并使用视觉细节进行优化mask。同时，提出了 Spectrum-guided Cross-modal Fusion (SCF) 方法，在 spectral 频谱上进行了跨模态拟合。</li>
<li>results: 实验表明，SgMg 方法在四个视频测试集上达到了当前最佳性能，与 nearest competitor 相比，提高了2.8% 点的 Ref-YouTube-VOS 性能。同时，通过扩展 SgMg，实现了多对象 R-VOS，不仅快速响应，还可以保持满意的性能。<details>
<summary>Abstract</summary>
Current referring video object segmentation (R-VOS) techniques extract conditional kernels from encoded (low-resolution) vision-language features to segment the decoded high-resolution features. We discovered that this causes significant feature drift, which the segmentation kernels struggle to perceive during the forward computation. This negatively affects the ability of segmentation kernels. To address the drift problem, we propose a Spectrum-guided Multi-granularity (SgMg) approach, which performs direct segmentation on the encoded features and employs visual details to further optimize the masks. In addition, we propose Spectrum-guided Cross-modal Fusion (SCF) to perform intra-frame global interactions in the spectral domain for effective multimodal representation. Finally, we extend SgMg to perform multi-object R-VOS, a new paradigm that enables simultaneous segmentation of multiple referred objects in a video. This not only makes R-VOS faster, but also more practical. Extensive experiments show that SgMg achieves state-of-the-art performance on four video benchmark datasets, outperforming the nearest competitor by 2.8% points on Ref-YouTube-VOS. Our extended SgMg enables multi-object R-VOS, runs about 3 times faster while maintaining satisfactory performance. Code is available at https://github.com/bo-miao/SgMg.
</details>
<details>
<summary>摘要</summary>
当前的视频对象 segmentation (R-VOS) 技术从编码的低分辨率视Language特征中提取条件kernels来 segment decode高分辨率特征。我们发现这会导致重要的特征漂移，使segmentation kernels在前向计算中困难以感知。这 negatively affects the ability of segmentation kernels。为解决这个问题，我们提出了spectrum-guided Multi-granularity (SgMg)方法，它直接在编码特征上进行分 segmentation和使用视觉细节进一步优化Mask。此外，我们提出了spectrum-guided Cross-modal Fusion (SCF)，它在spectral domain中进行了intra-frame global interactions，以实现有效的 Multimodal Representation。 finally，我们扩展了SgMg，以实现多对象R-VOS，一种新的 paradigm，可以同时 segment multiple referred objects in a video。这不仅使R-VOS更快，而且更实用。我们的扩展SgMg在四个视频 benchmark dataset上进行了广泛的实验，并达到了状态的art Performance，比 nearest competitor高2.8%点。我们的扩展SgMg可以在3倍的速度下维持满意的性能。代码可以在https://github.com/bo-miao/SgMg 中找到。
</details></li>
</ul>
<hr>
<h2 id="Re-mine-Learn-and-Reason-Exploring-the-Cross-modal-Semantic-Correlations-for-Language-guided-HOI-detection"><a href="#Re-mine-Learn-and-Reason-Exploring-the-Cross-modal-Semantic-Correlations-for-Language-guided-HOI-detection" class="headerlink" title="Re-mine, Learn and Reason: Exploring the Cross-modal Semantic Correlations for Language-guided HOI detection"></a>Re-mine, Learn and Reason: Exploring the Cross-modal Semantic Correlations for Language-guided HOI detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13529">http://arxiv.org/abs/2307.13529</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yichao Cao, Xiu Su, Qingfei Tang, Feng Yang, Shan You, Xiaobo Lu, Chang Xu</li>
<li>for: 提高人员对象互动（HOI）检测的精度，使用视觉模型解决人员对象互动的复杂关系。</li>
<li>methods: 提出了一个系统atic和统一的框架（RmLR），通过结构化文本知识来增强HOI检测。首先，分析了两阶段HOI检测器中的交互信息损失，并提出了重新挖掘策略来生成更全面的视觉表示。其次，设计了更细化的句子和单词级别对齐和知识传递策略，以有效地解决多个交互和多个文本之间的多对多匹配问题。这些策略可以减轻在多个交互同时发生时出现的匹配混乱问题，从而提高对齐过程的有效性。</li>
<li>results: 实验结果表明，我们的方法可以减轻HOI检测的困难，并在公共测试集上达到状态 искусственный智能性能的最高水平。我们进一步分析了不同组成部分的影响，以便更好地理解我们的方法的作用。<details>
<summary>Abstract</summary>
Human-Object Interaction (HOI) detection is a challenging computer vision task that requires visual models to address the complex interactive relationship between humans and objects and predict HOI triplets. Despite the challenges posed by the numerous interaction combinations, they also offer opportunities for multimodal learning of visual texts. In this paper, we present a systematic and unified framework (RmLR) that enhances HOI detection by incorporating structured text knowledge. Firstly, we qualitatively and quantitatively analyze the loss of interaction information in the two-stage HOI detector and propose a re-mining strategy to generate more comprehensive visual representation.Secondly, we design more fine-grained sentence- and word-level alignment and knowledge transfer strategies to effectively address the many-to-many matching problem between multiple interactions and multiple texts.These strategies alleviate the matching confusion problem that arises when multiple interactions occur simultaneously, thereby improving the effectiveness of the alignment process. Finally, HOI reasoning by visual features augmented with textual knowledge substantially improves the understanding of interactions. Experimental results illustrate the effectiveness of our approach, where state-of-the-art performance is achieved on public benchmarks. We further analyze the effects of different components of our approach to provide insights into its efficacy.
</details>
<details>
<summary>摘要</summary>
人机物交互（HOI）检测是一个复杂的计算机视觉任务，需要视觉模型处理人与物之间的复杂交互关系，并预测HOI triplets。尽管交互组合多样化，但它们也提供了多模式学习视觉文本的机会。在这篇论文中，我们提出了一个系统性和统一的框架（RmLR），增强HOI检测的能力，并包括结构化文本知识。首先，我们质量和量上分析了两stage HOI检测器中的交互信息损失，并提出了重新挖掘策略，以生成更全面的视觉表示。其次，我们设计了更细grained的句子和单词级别对齐和知识传递策略，以有效地Address多对多匹配问题。这些策略可以减少同时发生多个交互时的匹配混乱问题，从而改善对齐过程的效果。最后，通过视觉特征加上文本知识来进行HOI理解，可以大幅提高交互的理解能力。实验结果表明，我们的方法可以在公共 benchMark上达到领先的性能。我们进一步分析了不同组成部分的效果，以提供对其效果的深入分析。
</details></li>
</ul>
<hr>
<h2 id="FacTool-Factuality-Detection-in-Generative-AI-–-A-Tool-Augmented-Framework-for-Multi-Task-and-Multi-Domain-Scenarios"><a href="#FacTool-Factuality-Detection-in-Generative-AI-–-A-Tool-Augmented-Framework-for-Multi-Task-and-Multi-Domain-Scenarios" class="headerlink" title="FacTool: Factuality Detection in Generative AI – A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios"></a>FacTool: Factuality Detection in Generative AI – A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13528">http://arxiv.org/abs/2307.13528</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gair-nlp/factool">https://github.com/gair-nlp/factool</a></li>
<li>paper_authors: I-Chun Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham Neubig, Pengfei Liu</li>
<li>for: 检测生成模型中的错误信息</li>
<li>methods: 提出了一个任务和领域无关的框架，用于检测由大语言模型生成的文本中的错误信息</li>
<li>results: 在四个不同的任务（知识基础问答、代码生成、数学逻辑和科学文献评论）中，实验结果表明提出的方法有效。<details>
<summary>Abstract</summary>
The emergence of generative pre-trained models has facilitated the synthesis of high-quality text, but it has also posed challenges in identifying factual errors in the generated text. In particular: (1) A wider range of tasks now face an increasing risk of containing factual errors when handled by generative models. (2) Generated texts tend to be lengthy and lack a clearly defined granularity for individual facts. (3) There is a scarcity of explicit evidence available during the process of fact checking. With the above challenges in mind, in this paper, we propose FacTool, a task and domain agnostic framework for detecting factual errors of texts generated by large language models (e.g., ChatGPT). Experiments on four different tasks (knowledge-based QA, code generation, mathematical reasoning, and scientific literature review) show the efficacy of the proposed method. We release the code of FacTool associated with ChatGPT plugin interface at https://github.com/GAIR-NLP/factool .
</details>
<details>
<summary>摘要</summary>
<<SYS>>请将以下文本翻译成简化中文。<</SYS>>生成模型的出现使得高质量文本的合成变得更加容易，但也使得检测生成文本中的事实错误变得更加困难。特别是：（1）更多的任务现在面临着增加的事实错误风险。（2）生成的文本往往很长，缺乏明确的粒度来分割个别的事实。（3）在 фактиче检查过程中没有明确的证据。为了解决这些挑战，在这篇论文中，我们提出了 FacTool，一种任务和领域无关的检测文本生成模型中的事实错误框架。实验在四个不同的任务（知识基础问答、代码生成、数学推理和科学文献综述）中展示了提案的效果。我们将 FacTool 相关的 ChatGPT 插件接口的代码发布在 GitHub 上，请参考 <https://github.com/GAIR-NLP/factool>。
</details></li>
</ul>
<hr>
<h2 id="An-Empirical-Study-on-Fairness-Improvement-with-Multiple-Protected-Attributes"><a href="#An-Empirical-Study-on-Fairness-Improvement-with-Multiple-Protected-Attributes" class="headerlink" title="An Empirical Study on Fairness Improvement with Multiple Protected Attributes"></a>An Empirical Study on Fairness Improvement with Multiple Protected Attributes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01923">http://arxiv.org/abs/2308.01923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenpeng Chen, Jie M. Zhang, Federica Sarro, Mark Harman</li>
<li>for: 该论文主要针对多个保护特征的公平性提升，而现有研究多数只是针对单个保护特征进行公平性提升。</li>
<li>methods: 该论文对11种现状顶尖公平性提升方法进行了广泛的研究，并分析了不同的数据集、度量和机器学习模型在考虑多个保护特征时的效果。</li>
<li>results: 研究发现，只考虑单一保护特征进行公平性提升可能会导致其他保护特征的不公平性增加，这种增加的比例可达88.3%（57.5%的平均值）。此外，对多个保护特征进行公平性提升不会带来减少准确性的代价，但是处理多个保护特征时的精度和回归率增加约5倍和8倍。这些结果有重要的意义，将只报告准确性作为机器学习性能指标是不充分的。<details>
<summary>Abstract</summary>
Existing research mostly improves the fairness of Machine Learning (ML) software regarding a single protected attribute at a time, but this is unrealistic given that many users have multiple protected attributes. This paper conducts an extensive study of fairness improvement regarding multiple protected attributes, covering 11 state-of-the-art fairness improvement methods. We analyze the effectiveness of these methods with different datasets, metrics, and ML models when considering multiple protected attributes. The results reveal that improving fairness for a single protected attribute can largely decrease fairness regarding unconsidered protected attributes. This decrease is observed in up to 88.3% of scenarios (57.5% on average). More surprisingly, we find little difference in accuracy loss when considering single and multiple protected attributes, indicating that accuracy can be maintained in the multiple-attribute paradigm. However, the effect on precision and recall when handling multiple protected attributes is about 5 times and 8 times that of a single attribute. This has important implications for future fairness research: reporting only accuracy as the ML performance metric, which is currently common in the literature, is inadequate.
</details>
<details>
<summary>摘要</summary>
现有研究主要是在受保护特征单个方面提高机器学习软件的公平性，但这并不是现实中的情况，用户通常有多个受保护特征。这篇论文进行了多个受保护特征公平性改进的广泛研究，涵盖了11种现状最佳实践方法。我们对不同的数据集、度量和机器学习模型进行了这些方法的分析，并发现了以下结论：在考虑多个受保护特征时，改进公平性对单个受保护特征的改进可以导致其他受保护特征的公平性下降，这种下降的比例在88.3%的情况下（57.5%的平均值）。而且，我们发现在考虑单个和多个受保护特征时，精度的影响几乎没有变化，这意味着在多个受保护特征的情况下，精度可以保持在相同的水平。然而，处理多个受保护特征时，精度和准确率的影响是单个受保护特征的8倍和5倍。这有重要的实践意义：现在流行的 literatura 中报道精度作为机器学习性能指标是不充分的。
</details></li>
</ul>
<hr>
<h2 id="Zshot-An-Open-source-Framework-for-Zero-Shot-Named-Entity-Recognition-and-Relation-Extraction"><a href="#Zshot-An-Open-source-Framework-for-Zero-Shot-Named-Entity-Recognition-and-Relation-Extraction" class="headerlink" title="Zshot: An Open-source Framework for Zero-Shot Named Entity Recognition and Relation Extraction"></a>Zshot: An Open-source Framework for Zero-Shot Named Entity Recognition and Relation Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13497">http://arxiv.org/abs/2307.13497</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriele Picco, Marcos Martínez Galindo, Alberto Purpura, Leopold Fuchs, Vanessa López, Hoang Thanh Lam</li>
<li>for: 这项研究的目的是提供一个可比较多种现代Zero-Shot Learning（ZSL）方法的框架，以便研究人员可以通过对标准benchmark数据集进行比较。</li>
<li>methods: 该框架使用了大量预训练语言模型，并提出了许多新的方法，从而导致了ZSL性能的明显提高。</li>
<li>results: 该研究提出了一个名为Zshot的新的ZSL框架，该框架包含了可extendible和可评估的API，以及多种优化技术，如管道 ensemble和可视化工具，以提高ZSL性能。<details>
<summary>Abstract</summary>
The Zero-Shot Learning (ZSL) task pertains to the identification of entities or relations in texts that were not seen during training. ZSL has emerged as a critical research area due to the scarcity of labeled data in specific domains, and its applications have grown significantly in recent years. With the advent of large pretrained language models, several novel methods have been proposed, resulting in substantial improvements in ZSL performance. There is a growing demand, both in the research community and industry, for a comprehensive ZSL framework that facilitates the development and accessibility of the latest methods and pretrained models.In this study, we propose a novel ZSL framework called Zshot that aims to address the aforementioned challenges. Our primary objective is to provide a platform that allows researchers to compare different state-of-the-art ZSL methods with standard benchmark datasets. Additionally, we have designed our framework to support the industry with readily available APIs for production under the standard SpaCy NLP pipeline. Our API is extendible and evaluable, moreover, we include numerous enhancements such as boosting the accuracy with pipeline ensembling and visualization utilities available as a SpaCy extension.
</details>
<details>
<summary>摘要</summary>
zero-shot learning (ZSL) 任务是指在训练中没有看到的文本中预测实体或关系。 ZSL 已成为一个重要的研究领域，因为特定领域的标注数据稀缺，其应用也在过去几年内不断增长。随着大型预训言语模型的出现，一些新的方法被提出，从而导致了 ZSL 性能的明显提升。现在，研究社区和业界均有强烈的需求，一个涵盖最新的 ZSL 方法和预训言语模型的全面框架。在这个研究中，我们提出了一个名为 Zshot 的新的 ZSL 框架，旨在解决以下问题。我们的主要目标是提供一个平台， allowing researchers 可以比较不同的状态对 ZSL 方法的标准 benchmark 数据集。此外，我们设计了我们的框架可以支持产业，通过在 SpaCy NLP 管道中提供可靠的 API。我们的 API 可扩展和评估，其中包括将 pipeline 结合使用以提高准确性，以及可用的 SpaCy 扩展包中的可视化工具。
</details></li>
</ul>
<hr>
<h2 id="Duet-efficient-and-scalable-hybriD-neUral-rElation-undersTanding"><a href="#Duet-efficient-and-scalable-hybriD-neUral-rElation-undersTanding" class="headerlink" title="Duet: efficient and scalable hybriD neUral rElation undersTanding"></a>Duet: efficient and scalable hybriD neUral rElation undersTanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13494">http://arxiv.org/abs/2307.13494</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/GIS-PuppetMaster/Duet">https://github.com/GIS-PuppetMaster/Duet</a></li>
<li>paper_authors: Kaixin Zhang, Hongzhi Wang, Yabin Lu, Ziqi Li, Chang Shu, Yu Yan, Donghua Yang</li>
<li>for: 估算卡尔达ности（cardinality estimation）问题，尤其是在高卡尔达ности和高维度表上，以提高learned cardinality estimator的实际应用。</li>
<li>methods: 引入 predicate information into autoregressive model，并提出了一种稳定、高效、可扩展的混合方法（Duet），可以直接估算卡尔达ности而不需要采样或非 differentiable process，从而降低推理复杂度从 O(n) 降至 O(1)，并在高卡尔达ности和高维度表上达到更高的准确性。</li>
<li>results: 实验结果表明，Duet 可以实现所有设计目标，并在 CPU 上实现更低的推理成本，而且在 GPU 上的大多数学习方法上实现更高的准确性。<details>
<summary>Abstract</summary>
Learned cardinality estimation methods have achieved high precision compared to traditional methods. Among learned methods, query-driven approaches face the data and workload drift problem for a long time. Although both query-driven and hybrid methods are proposed to avoid this problem, even the state-of-the-art of them suffer from high training and estimation costs, limited scalability, instability, and long-tailed distribution problem on high cardinality and high-dimensional tables, which seriously affects the practical application of learned cardinality estimators. In this paper, we prove that most of these problems are directly caused by the widely used progressive sampling. We solve this problem by introducing predicates information into the autoregressive model and propose Duet, a stable, efficient, and scalable hybrid method to estimate cardinality directly without sampling or any non-differentiable process, which can not only reduces the inference complexity from O(n) to O(1) compared to Naru and UAE but also achieve higher accuracy on high cardinality and high-dimensional tables. Experimental results show that Duet can achieve all the design goals above and be much more practical and even has a lower inference cost on CPU than that of most learned methods on GPU.
</details>
<details>
<summary>摘要</summary>
现代学习 cardinality 估计方法已经达到了高精度，比传统方法更高。 among 学习方法中， Query-driven 方法面临着数据和工作负载漂移问题，持续时间很长。 although  Query-driven 和混合方法都是为了避免这个问题，即使是当前最佳的它们也受到高训练和估计成本、有限扩展性、不稳定性和高维度高 cardinality 表的长板块分布问题的影响，这些问题对实际应用 cardinality 估计器产生了严重的影响。在本文中，我们证明了大多数这些问题是由广泛使用进度 sampling 所引起的。我们解决这个问题，通过将 predicate 信息添加到权重 autoregressive 模型，并提出了 Duet，一种稳定、高效和可扩展的混合方法，可以直接无需采样或任何不可微分过程，对高 cardinality 和高维度表进行 cardinality 估计，可以将推理复杂度从 O(n) 降低至 O(1)，比 Naru 和 UAE 更高。实验结果表明，Duet 可以实现所有设计目标，并且在 CPU 上比大多数学习方法在 GPU 上更具实际性，甚至在推理成本方面也更低。
</details></li>
</ul>
<hr>
<h2 id="Integrating-processed-based-models-and-machine-learning-for-crop-yield-prediction"><a href="#Integrating-processed-based-models-and-machine-learning-for-crop-yield-prediction" class="headerlink" title="Integrating processed-based models and machine learning for crop yield prediction"></a>Integrating processed-based models and machine learning for crop yield prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13466">http://arxiv.org/abs/2307.13466</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michiel G. J. Kallenberg, Bernardo Maestrini, Ron van Bree, Paul Ravensbergen, Christos Pylianidis, Frits van Evert, Ioannis N. Athanasiadis</li>
<li>for: 预测哈密瓜产量</li>
<li>methods: 使用混合元模型方法</li>
<li>results: 比基eline方法更好，但需更多实际数据 validate its practical effectiveness。Here’s the full translation of the abstract in Simplified Chinese:预测哈密瓜产量通常 involve theory-driven process-based 植物生长模型，它们在地方条件下困难准确化，或者数据驱动机器学习方法，它们需要大量数据。在这项工作中，我们调查了使用混合元模型方法进行哈密瓜产量预测。我们使用植物生长模型生成了一个数据集，并对其进行(预)训练一个卷积神经网络，然后使用观察数据进行精度调整。在Silico中，我们的元模型方法比基eline方法更好。在实际试验中，我们的方法与植物生长模型相比，在77个商业场景中表现相当，但是在303个试验场景中，两者都比一个简单的直线回归方法和专门设计的预处理方法差一些。我们的发现表明元模型方法在准确预测哈密瓜产量方面有潜力，但是需要更多的实际数据 validate its practical effectiveness。<details>
<summary>Abstract</summary>
Crop yield prediction typically involves the utilization of either theory-driven process-based crop growth models, which have proven to be difficult to calibrate for local conditions, or data-driven machine learning methods, which are known to require large datasets. In this work we investigate potato yield prediction using a hybrid meta-modeling approach. A crop growth model is employed to generate synthetic data for (pre)training a convolutional neural net, which is then fine-tuned with observational data. When applied in silico, our meta-modeling approach yields better predictions than a baseline comprising a purely data-driven approach. When tested on real-world data from field trials (n=303) and commercial fields (n=77), the meta-modeling approach yields competitive results with respect to the crop growth model. In the latter set, however, both models perform worse than a simple linear regression with a hand-picked feature set and dedicated preprocessing designed by domain experts. Our findings indicate the potential of meta-modeling for accurate crop yield prediction; however, further advancements and validation using extensive real-world datasets is recommended to solidify its practical effectiveness.
</details>
<details>
<summary>摘要</summary>
卷积预测通常使用理论驱动的生物物理型或数据驱动机器学方法。前者具有难以调整本地条件的缺点，而后者需要大量数据。在这种工作中，我们调查了混合元模型方法用于预测食用产量。我们使用生长模型生成人工数据，并将其用于（预）训练卷积神经网络，然后精度地调整 Observational data。在虚拟环境中，我们的元模型方法比基准组的数据驱动方法更好。在实际数据集（n=303）和商业场景（n=77）中，元模型方法与生长模型具有相似的性能，但是在这两个场景中，所有模型都比一个简单的直线回归和专门为域专家设计的特定预处理方法更差。我们的发现表明元模型方法在准确预测卷积产量方面有潜力，但是进一步的进展和验证使用广泛的实际数据集是建议的，以固定其实际效果。
</details></li>
</ul>
<hr>
<h2 id="Unlocking-the-Emotional-World-of-Visual-Media-An-Overview-of-the-Science-Research-and-Impact-of-Understanding-Emotion"><a href="#Unlocking-the-Emotional-World-of-Visual-Media-An-Overview-of-the-Science-Research-and-Impact-of-Understanding-Emotion" class="headerlink" title="Unlocking the Emotional World of Visual Media: An Overview of the Science, Research, and Impact of Understanding Emotion"></a>Unlocking the Emotional World of Visual Media: An Overview of the Science, Research, and Impact of Understanding Emotion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13463">http://arxiv.org/abs/2307.13463</a></li>
<li>repo_url: None</li>
<li>paper_authors: James Z. Wang, Sicheng Zhao, Chenyan Wu, Reginald B. Adams, Michelle G. Newman, Tal Shafir, Rachelle Tsachor</li>
<li>for: 这篇论文旨在探讨计算机和机器人领域中人工情感智能技术的发展，以及这种技术如何改变计算机视觉领域的研究。</li>
<li>methods: 这篇论文使用了多种方法，包括心理学、工程学和艺术等多个领域的研究成果，以提供一个全面的、多元的视听媒体情感分析领域的概述。</li>
<li>results: 这篇论文提出了计算机视觉领域中自动理解表达或诱发情感的技术存在一些挑战和限制，并提出了未来研究的重要方向和途径。<details>
<summary>Abstract</summary>
The emergence of artificial emotional intelligence technology is revolutionizing the fields of computers and robotics, allowing for a new level of communication and understanding of human behavior that was once thought impossible. While recent advancements in deep learning have transformed the field of computer vision, automated understanding of evoked or expressed emotions in visual media remains in its infancy. This foundering stems from the absence of a universally accepted definition of "emotion", coupled with the inherently subjective nature of emotions and their intricate nuances. In this article, we provide a comprehensive, multidisciplinary overview of the field of emotion analysis in visual media, drawing on insights from psychology, engineering, and the arts. We begin by exploring the psychological foundations of emotion and the computational principles that underpin the understanding of emotions from images and videos. We then review the latest research and systems within the field, accentuating the most promising approaches. We also discuss the current technological challenges and limitations of emotion analysis, underscoring the necessity for continued investigation and innovation. We contend that this represents a "Holy Grail" research problem in computing and delineate pivotal directions for future inquiry. Finally, we examine the ethical ramifications of emotion-understanding technologies and contemplate their potential societal impacts. Overall, this article endeavors to equip readers with a deeper understanding of the domain of emotion analysis in visual media and to inspire further research and development in this captivating and rapidly evolving field.
</details>
<details>
<summary>摘要</summary>
人工情感智能技术的出现正在改变计算机和机器人领域，allowing for a new level of communication and understanding of human behavior that was once thought impossible.  However, recent advancements in deep learning have transformed the field of computer vision, automated understanding of evoked or expressed emotions in visual media remains in its infancy. This foundering stems from the absence of a universally accepted definition of "emotion", coupled with the inherently subjective nature of emotions and their intricate nuances.In this article, we provide a comprehensive, multidisciplinary overview of the field of emotion analysis in visual media, drawing on insights from psychology, engineering, and the arts. We begin by exploring the psychological foundations of emotion and the computational principles that underpin the understanding of emotions from images and videos. We then review the latest research and systems within the field, accentuating the most promising approaches. We also discuss the current technological challenges and limitations of emotion analysis, underscoring the necessity for continued investigation and innovation. We contend that this represents a "Holy Grail" research problem in computing and delineate pivotal directions for future inquiry.Finally, we examine the ethical ramifications of emotion-understanding technologies and contemplate their potential societal impacts. Overall, this article endeavors to equip readers with a deeper understanding of the domain of emotion analysis in visual media and to inspire further research and development in this captivating and rapidly evolving field.
</details></li>
</ul>
<hr>
<h2 id="Fundamental-causal-bounds-of-quantum-random-access-memories"><a href="#Fundamental-causal-bounds-of-quantum-random-access-memories" class="headerlink" title="Fundamental causal bounds of quantum random access memories"></a>Fundamental causal bounds of quantum random access memories</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13460">http://arxiv.org/abs/2307.13460</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunfei Wang, Yuri Alexeev, Liang Jiang, Frederic T. Chong, Junyu Liu</li>
<li>for: 本研究旨在探讨量子Random Access Memory（QRAM）在量子物理原理下的限制，以确定量子计算应用程序在数据科学领域的长期表现。</li>
<li>methods: 本研究使用相对论和量子多体系统的 Lieb-Robinson  bounds来探讨量子快速记忆器的内在约束。</li>
<li>results: 研究发现，使用量子声学系统的硬件设计，QRAM 可以处理 $\mathcal{O}(10^7)$ 逻辑量子 bits 在一维结构中，而在二维和三维结构中可以处理 $\mathcal{O}(10^{15})$ 到 $\mathcal{O}(10^{20})$ 和 $\mathcal{O}(10^{24})$ 量子 bits  соответpectively。这些约束适用于其他量子硬件系统。研究结果表明，量子物理原理的限制对量子计算应用程序的长期表现有重要的影响，并且提出了可能提高性能的量子记忆器设计。<details>
<summary>Abstract</summary>
Quantum devices should operate in adherence to quantum physics principles. Quantum random access memory (QRAM), a fundamental component of many essential quantum algorithms for tasks such as linear algebra, data search, and machine learning, is often proposed to offer $\mathcal{O}(\log N)$ circuit depth for $\mathcal{O}(N)$ data size, given $N$ qubits. However, this claim appears to breach the principle of relativity when dealing with a large number of qubits in quantum materials interacting locally. In our study we critically explore the intrinsic bounds of rapid quantum memories based on causality, employing the relativistic quantum field theory and Lieb-Robinson bounds in quantum many-body systems. In this paper, we consider a hardware-efficient QRAM design in hybrid quantum acoustic systems. Assuming clock cycle times of approximately $10^{-3}$ seconds and a lattice spacing of about 1 micrometer, we show that QRAM can accommodate up to $\mathcal{O}(10^7)$ logical qubits in 1 dimension, $\mathcal{O}(10^{15})$ to $\mathcal{O}(10^{20})$ in various 2D architectures, and $\mathcal{O}(10^{24})$ in 3 dimensions. We contend that this causality bound broadly applies to other quantum hardware systems. Our findings highlight the impact of fundamental quantum physics constraints on the long-term performance of quantum computing applications in data science and suggest potential quantum memory designs for performance enhancement.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Monte-Carlo-Tree-Search-for-Multi-Agent-Pathfinding-Preliminary-Results"><a href="#Monte-Carlo-Tree-Search-for-Multi-Agent-Pathfinding-Preliminary-Results" class="headerlink" title="Monte-Carlo Tree Search for Multi-Agent Pathfinding: Preliminary Results"></a>Monte-Carlo Tree Search for Multi-Agent Pathfinding: Preliminary Results</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13453">http://arxiv.org/abs/2307.13453</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yelisey Pitanov, Alexey Skrynnik, Anton Andreychuk, Konstantin Yakovlev, Aleksandr Panov</li>
<li>for: 这个论文研究了多 Agent Pathfinding 问题，即在图形结构下，每个代理都有唯一的起点和目标点，需要找到一个不受碰撞的多个路径，使每个代理都能够达到其目标点。</li>
<li>methods: 我们使用 Monte-Carlo Tree Search (MCTS) 来解决这个问题。MCTS 在各种问题中表现出色，如游戏等，但在多 Agent Pathfinding 中并未得到广泛研究。我们提出了一种专门为多 Agent Pathfinding 设计的 MCTS 变体。我们在 Compute 奖励的方法中使用了特定的路径来帮助代理人员达到目标点，同时保留了代理人员可以离开路径以避免碰撞的能力。</li>
<li>results: 我们对基eline  планинг算法，例如 A*，进行比较，并证明了我们的方法在多 Agent Pathfinding 中表现出色，超过了基eline 方法。<details>
<summary>Abstract</summary>
In this work we study a well-known and challenging problem of Multi-agent Pathfinding, when a set of agents is confined to a graph, each agent is assigned a unique start and goal vertices and the task is to find a set of collision-free paths (one for each agent) such that each agent reaches its respective goal. We investigate how to utilize Monte-Carlo Tree Search (MCTS) to solve the problem. Although MCTS was shown to demonstrate superior performance in a wide range of problems like playing antagonistic games (e.g. Go, Chess etc.), discovering faster matrix multiplication algorithms etc., its application to the problem at hand was not well studied before. To this end we introduce an original variant of MCTS, tailored to multi-agent pathfinding. The crux of our approach is how the reward, that guides MCTS, is computed. Specifically, we use individual paths to assist the agents with the the goal-reaching behavior, while leaving them freedom to get off the track if it is needed to avoid collisions. We also use a dedicated decomposition technique to reduce the branching factor of the tree search procedure. Empirically we show that the suggested method outperforms the baseline planning algorithm that invokes heuristic search, e.g. A*, at each re-planning step.
</details>
<details>
<summary>摘要</summary>
To address this, we introduce an original variant of MCTS tailored to multi-agent pathfinding. The key aspect of our approach is how the reward, which guides the MCTS, is computed. Specifically, we use individual paths to assist the agents in reaching their goals while allowing them to deviate from the planned path if necessary to avoid collisions. We also employ a dedicated decomposition technique to reduce the branching factor of the tree search procedure.Empirically, we show that our suggested method outperforms a baseline planning algorithm that invokes heuristic search, such as A\*, at each re-planning step.
</details></li>
</ul>
<hr>
<h2 id="A-behavioural-transformer-for-effective-collaboration-between-a-robot-and-a-non-stationary-human"><a href="#A-behavioural-transformer-for-effective-collaboration-between-a-robot-and-a-non-stationary-human" class="headerlink" title="A behavioural transformer for effective collaboration between a robot and a non-stationary human"></a>A behavioural transformer for effective collaboration between a robot and a non-stationary human</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13447">http://arxiv.org/abs/2307.13447</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruaridh Mon-Williams, Theodoros Stouraitis, Sethu Vijayakumar</li>
<li>for:  This paper aims to address the challenges of human-robot collaboration in non-stationary environments, where human behavior changes over time.</li>
<li>methods:  The authors propose a principled meta-learning framework and develop a conditional transformer called Behaviour-Transform (BeTrans) to adapt to new human agents with non-stationary behaviors.</li>
<li>results:  BeTrans effectively collaborates with simulated human agents and adapts faster to non-stationary simulated human agents than state-of-the-art techniques.Here’s the full text in Simplified Chinese:</li>
<li>for: 本研究旨在解决人机合作中的非站点环境问题，其中人类行为随着时间的变化。</li>
<li>methods: 作者提出了一种原理化的元学习框架，并开发了一种名为行为变换（BeTrans）的条件变换器，以适应新的人类代理者具有非站点行为的情况。</li>
<li>results: BeTrans在模拟人类代理者中的原创自定义环境中显示了与非站点模拟人类代理者的更好的协作和更快的适应速度，比STATE-OF-THE-ART技术更高。<details>
<summary>Abstract</summary>
A key challenge in human-robot collaboration is the non-stationarity created by humans due to changes in their behaviour. This alters environmental transitions and hinders human-robot collaboration. We propose a principled meta-learning framework to explore how robots could better predict human behaviour, and thereby deal with issues of non-stationarity. On the basis of this framework, we developed Behaviour-Transform (BeTrans). BeTrans is a conditional transformer that enables a robot agent to adapt quickly to new human agents with non-stationary behaviours, due to its notable performance with sequential data. We trained BeTrans on simulated human agents with different systematic biases in collaborative settings. We used an original customisable environment to show that BeTrans effectively collaborates with simulated human agents and adapts faster to non-stationary simulated human agents than SOTA techniques.
</details>
<details>
<summary>摘要</summary>
人机合作中的一大挑战是由人类行为引起的非站点性，这会导致环境转移和人机合作困难。我们提出了一种原则性的元学习框架，以便让机器人更好地预测人类行为，从而解决非站点性问题。基于这个框架，我们开发了行为变换（BeTrans）。BeTrans 是一种 Conditional Transformer，它允许机器人代理人类快速适应新的非站点人类行为，因为它在序列数据上表现出了显著的性能。我们在模拟人类代理人中进行了训练，并在合作 Setting 中验证了 BeTrans 的效果。我们使用了一个自定义的环境，以示 BeTrans 可以快速适应非站点人类行为，并且比标准技术更快。
</details></li>
</ul>
<hr>
<h2 id="On-the-Learning-Dynamics-of-Attention-Networks"><a href="#On-the-Learning-Dynamics-of-Attention-Networks" class="headerlink" title="On the Learning Dynamics of Attention Networks"></a>On the Learning Dynamics of Attention Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13421">http://arxiv.org/abs/2307.13421</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vashisht-rahul/on-the-learning-dynamics-of-attention-networks">https://github.com/vashisht-rahul/on-the-learning-dynamics-of-attention-networks</a></li>
<li>paper_authors: Rahul Vashisht, Harish G. Ramaswamy</li>
<li>for: 本研究的目的是探讨Attention模型的不同损失函数（soft attention、hard attention和latent variable marginal likelihood（LVML））在模型学习中的影响。</li>
<li>methods: 本研究使用了三种不同的损失函数来训练Attention模型，包括soft attention、hard attention和LVML。</li>
<li>results: 研究发现不同的损失函数会导致Attention模型的不同行为和结果。在训练过程中，使用soft attention损失函数可以让注意力模型在初始化阶段快速改进，但后续会降低。相反，使用hard attention损失函数可以使注意力模型在训练过程中保持稳定。此外，研究还提出了一种简单的混合方法，该方法结合了不同损失函数的优点，并在一些半人工和实际数据集上进行了测试。<details>
<summary>Abstract</summary>
Attention models are typically learned by optimizing one of three standard loss functions that are variously called -- soft attention, hard attention, and latent variable marginal likelihood (LVML) attention. All three paradigms are motivated by the same goal of finding two models -- a `focus' model that `selects' the right \textit{segment} of the input and a `classification' model that processes the selected segment into the target label. However, they differ significantly in the way the selected segments are aggregated, resulting in distinct dynamics and final results. We observe a unique signature of models learned using these paradigms and explain this as a consequence of the evolution of the classification model under gradient descent when the focus model is fixed. We also analyze these paradigms in a simple setting and derive closed-form expressions for the parameter trajectory under gradient flow. With the soft attention loss, the focus model improves quickly at initialization and splutters later on. On the other hand, hard attention loss behaves in the opposite fashion. Based on our observations, we propose a simple hybrid approach that combines the advantages of the different loss functions and demonstrates it on a collection of semi-synthetic and real-world datasets
</details>
<details>
<summary>摘要</summary>
注意模型通常通过优化三种标准损失函数来学习，它们分别被称为软注意力、硬注意力和隐变量概率 marginal likelihood（LVML）注意力。这三种方法都是为了找到两个模型---一个`焦点'模型可以选择正确的输入段落，以及一个`分类'模型可以处理选择的段落并生成目标标签。然而，它们在选取段落的方式不同，从而导致了不同的动力学和最终结果。我们观察到每种模型学习的独特签名，并解释这是因为分类模型在梯度下降过程中的演化。我们还对这些方法进行了简单的分析，并 deriv了关于参数轨迹的关闭式表达式。在软注意力损失函数下，焦点模型在初始化时快速提升，然后后来受阻。相反，硬注意力损失函数 behave in the opposite fashion。基于我们的观察，我们提出了一种简单的混合方法，将不同的损失函数的优点相互融合，并在一些半Synthetic和实际世界数据集上进行了证明。
</details></li>
</ul>
<hr>
<h2 id="Synthesis-of-Procedural-Models-for-Deterministic-Transition-Systems"><a href="#Synthesis-of-Procedural-Models-for-Deterministic-Transition-Systems" class="headerlink" title="Synthesis of Procedural Models for Deterministic Transition Systems"></a>Synthesis of Procedural Models for Deterministic Transition Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14368">http://arxiv.org/abs/2307.14368</a></li>
<li>repo_url: None</li>
<li>paper_authors: Javier Segovia-Aguas, Jonathan Ferrer-Mestres, Sergio Jiménez</li>
<li>for: 这篇论文旨在提出一种总体方法，用于生成某种逻辑系统的状态转移模型。</li>
<li>methods: 该方法采用抽象搜索，在Random-Access Machine（RAM）上使用有限Memory和简单的指令集来生成结构化程序。</li>
<li>results: 该方法可以生成符合给定输入集的状态转移模型，并且可以针对不同的目标语言进行模型化。<details>
<summary>Abstract</summary>
This paper introduces a general approach for synthesizing procedural models of the state-transitions of a given discrete system. The approach is general in that it accepts different target languages for modeling the state-transitions of a discrete system; different model acquisition tasks with different target languages, such as the synthesis of STRIPS action models, or the update rule of a cellular automaton, fit as particular instances of our general approach. We follow an inductive approach to synthesis meaning that a set of examples of state-transitions, represented as (pre-state, action, post-state) tuples, are given as input. The goal is to synthesize a structured program that, when executed on a given pre-state, outputs its associated post-state. Our synthesis method implements a combinatorial search in the space of well-structured terminating programs that can be built using a Random-Access Machine (RAM), with a minimalist instruction set, and a finite amount of memory. The combinatorial search is guided with functions that asses the complexity of the candidate programs, as well as their fitness to the given input set of examples.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-short-review-of-the-main-concerns-in-A-I-development-and-application-within-the-public-sector-supported-by-NLP-and-TM"><a href="#A-short-review-of-the-main-concerns-in-A-I-development-and-application-within-the-public-sector-supported-by-NLP-and-TM" class="headerlink" title="A short review of the main concerns in A.I. development and application within the public sector supported by NLP and TM"></a>A short review of the main concerns in A.I. development and application within the public sector supported by NLP and TM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02042">http://arxiv.org/abs/2308.02042</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carlos Ferreira</li>
<li>for: 这个研究旨在捕捉公共领域中AI应用的数据隐私、伦理、可解释性、信任性和公平性问题的研究趋势。</li>
<li>methods: 该研究使用了NLP和TM基础概念，对ACMDigital Library和IEEE Xplore会议论文进行了两年内的查询和分析，以捕捉相关信息。</li>
<li>results: 研究结果显示，公平性是最常见的关注点，而数据隐私则是最少的关注点（即使它在大多数文章中都是embedded），而信任性则是最为显著的关注点。<details>
<summary>Abstract</summary>
Artificial Intelligence is not a new subject, and business, industry and public sectors have used it in different ways and contexts and considering multiple concerns. This work reviewed research papers published in ACM Digital Library and IEEE Xplore conference proceedings in the last two years supported by fundamental concepts of Natural Language Processing (NLP) and Text Mining (TM). The objective was to capture insights regarding data privacy, ethics, interpretability, explainability, trustworthiness, and fairness in the public sector. The methodology has saved analysis time and could retrieve papers containing relevant information. The results showed that fairness was the most frequent concern. The least prominent topic was data privacy (although embedded in most articles), while the most prominent was trustworthiness. Finally, gathering helpful insights about those concerns regarding A.I. applications in the public sector was also possible.
</details>
<details>
<summary>摘要</summary>
人工智能不是新的话题，商业、工业和公共部门在不同的方式和上下文中使用它，并考虑多种关注。这项工作查询了过去两年ACM数字图书馆和IEEE Xplore会议论文，基于自然语言处理（NLP）和文本挖掘（TM）的基本概念。目标是捕捉公共部门中关于数据隐私、伦理、可解性、可信度和公平性的视角。方法包括文献分析，减少分析时间，检索包含相关信息的论文。结果表明，公平性是最常见的关注点，而数据隐私即使在大多数文章中隐藏，也是最少提到的话题。最后，对于人工智能在公共部门中的应用中有所获得有用的洞察。
</details></li>
</ul>
<hr>
<h2 id="Towards-Bridging-the-Digital-Language-Divide"><a href="#Towards-Bridging-the-Digital-Language-Divide" class="headerlink" title="Towards Bridging the Digital Language Divide"></a>Towards Bridging the Digital Language Divide</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13405">http://arxiv.org/abs/2307.13405</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gábor Bella, Paula Helm, Gertraud Koch, Fausto Giunchiglia</li>
<li>for: 帮助各种语言技术扩展到受欠发达语言领域</li>
<li>methods: 通过对语言技术的研发方法进行修改，以减少语言偏见</li>
<li>results: 通过与本地社区的合作，提高语言技术的多样性和准确性<details>
<summary>Abstract</summary>
It is a well-known fact that current AI-based language technology -- language models, machine translation systems, multilingual dictionaries and corpora -- focuses on the world's 2-3% most widely spoken languages. Recent research efforts have attempted to expand the coverage of AI technology to `under-resourced languages.' The goal of our paper is to bring attention to a phenomenon that we call linguistic bias: multilingual language processing systems often exhibit a hardwired, yet usually involuntary and hidden representational preference towards certain languages. Linguistic bias is manifested in uneven per-language performance even in the case of similar test conditions. We show that biased technology is often the result of research and development methodologies that do not do justice to the complexity of the languages being represented, and that can even become ethically problematic as they disregard valuable aspects of diversity as well as the needs of the language communities themselves. As our attempt at building diversity-aware language resources, we present a new initiative that aims at reducing linguistic bias through both technological design and methodology, based on an eye-level collaboration with local communities.
</details>
<details>
<summary>摘要</summary>
现在的人工智能语言技术，包括语言模型、机器翻译系统、多语言词典和语料库，它们主要集中在世界上2-3%最广泛使用的语言上。 latest research efforts have attempted to expand the coverage of AI technology to "under-resourced languages." However, we have noticed a phenomenon that we call "linguistic bias" in multilingual language processing systems, which exhibits a hardwired yet involuntary and hidden representational preference towards certain languages. This bias is manifested in uneven per-language performance, even under similar test conditions. We argue that biased technology is often the result of research and development methodologies that do not fully consider the complexity of the languages being represented, and can even become ethically problematic as they disregard valuable aspects of diversity and the needs of language communities themselves. To address this issue, we present a new initiative that aims to reduce linguistic bias through both technological design and methodology, based on eye-level collaboration with local communities.
</details></li>
</ul>
<hr>
<h2 id="Predicting-Code-Coverage-without-Execution"><a href="#Predicting-Code-Coverage-without-Execution" class="headerlink" title="Predicting Code Coverage without Execution"></a>Predicting Code Coverage without Execution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13383">http://arxiv.org/abs/2307.13383</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/coverage-eval">https://github.com/microsoft/coverage-eval</a></li>
<li>paper_authors: Michele Tufano, Shubham Chandel, Anisha Agarwal, Neel Sundaresan, Colin Clement</li>
<li>for: 这篇论文的目的是为了评估大语言模型（LLM）对代码执行的理解程度，并提出了一个新的任务——代码覆盖率预测任务。</li>
<li>methods: 该论文使用了机器学习算法来减少计算代码覆盖率的成本，并且使用了人工生成的代码和测试用例来评估模型的性能。</li>
<li>results: 研究发现，OpenAI的GPT-4和GPT-3.5-Turbo、Google的BARD和Anthropic的Claude等四种state-of-the-art LLM在代码覆盖率预测任务中表现出色，并且 argue that code coverage as a metric and pre-training data source are valuable for overall LLM performance on software engineering tasks。<details>
<summary>Abstract</summary>
Code coverage is a widely used metric for quantifying the extent to which program elements, such as statements or branches, are executed during testing. Calculating code coverage is resource-intensive, requiring code building and execution with additional overhead for the instrumentation. Furthermore, computing coverage of any snippet of code requires the whole program context. Using Machine Learning to amortize this expensive process could lower the cost of code coverage by requiring only the source code context, and the task of code coverage prediction can be a novel benchmark for judging the ability of models to understand code. We propose a novel benchmark task called Code Coverage Prediction for Large Language Models (LLMs). We formalize this task to evaluate the capability of LLMs in understanding code execution by determining which lines of a method are executed by a given test case and inputs. We curate and release a dataset we call COVERAGEEVAL by executing tests and code from the HumanEval dataset and collecting code coverage information. We report the performance of four state-of-the-art LLMs used for code-related tasks, including OpenAI's GPT-4 and GPT-3.5-Turbo, Google's BARD, and Anthropic's Claude, on the Code Coverage Prediction task. Finally, we argue that code coverage as a metric and pre-training data source are valuable for overall LLM performance on software engineering tasks.
</details>
<details>
<summary>摘要</summary>
“代码覆盖率”是一个广泛使用的度量来量化程式码中不同元素的执行情况。计算代码覆盖率需要费时consumption，需要将代码建立和执行，并且需要额外的实现工具。此外，计算任何一段代码的覆盖率都需要整个程式码上下文。使用机器学习来优化这个费时的过程可以降低代码覆盖率的成本，只需要提供代码上下文，而不需要整个程式码。我们提出一个新的benchmark任务，名为代码覆盖预测（Code Coverage Prediction），用于评估大型自然语言模型（LLMs）的能力。我们正式定义这个任务，以评估LLMs对代码执行的理解度。我们组织了一个名为COVERAGEEVAL的数据集，通过执行HumanEval测试数据并收集代码覆盖信息。我们报告了四个现代LLMs的表现，包括OpenAI的GPT-4和GPT-3.5-Turbo、Google的BARD、以及Anthropic的Claude，在代码覆盖预测任务上的表现。最后，我们认为代码覆盖率作为度量和预训数据来源是LLM在软件工程任务上的重要因素。”
</details></li>
</ul>
<hr>
<h2 id="Empower-Your-Model-with-Longer-and-Better-Context-Comprehension"><a href="#Empower-Your-Model-with-Longer-and-Better-Context-Comprehension" class="headerlink" title="Empower Your Model with Longer and Better Context Comprehension"></a>Empower Your Model with Longer and Better Context Comprehension</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13365">http://arxiv.org/abs/2307.13365</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yileijin/attention-transition">https://github.com/yileijin/attention-transition</a></li>
<li>paper_authors: Yifei Gao, Lei Wang, Jun Fang, Longhua Hu, Jun Cheng</li>
<li>for: 提高 LL 模型在较长和复杂的上下文中的理解能力，以便更好地应用在实际场景中。</li>
<li>methods: 提出了一种新的技术 called Attention Transition，通过增强模型内部信息传递的能力，使模型能够更好地理解较长的上下文，无需额外训练或影响生成流畅性。</li>
<li>results: 在 XSum 数据集上进行了实验，与 GPT4 进行比较，得到了显著的改善，证明了 Attention Transition 的有效性。<details>
<summary>Abstract</summary>
Recently, with the emergence of numerous Large Language Models (LLMs), the implementation of AI has entered a new era. Irrespective of these models' own capacity and structure, there is a growing demand for LLMs to possess enhanced comprehension of longer and more complex contexts with relatively smaller sizes. Models often encounter an upper limit when processing sequences of sentences that extend beyond their comprehension capacity and result in off-topic or even chaotic responses. While several recent works attempt to address this issue in various ways, they rarely focus on "why models are unable to compensate or strengthen their capabilities on their own". In this paper, we thoroughly investigate the nature of information transfer within LLMs and propose a novel technique called Attention Transition. This technique empowers models to achieve longer and better context comprehension with minimal additional training or impact on generation fluency. Our experiments are conducted on the challenging XSum dataset using LLaMa-7b model with context token length ranging from 800 to 1900. Results demonstrate that we achieve substantial improvements compared with the original generation results evaluated by GPT4.
</details>
<details>
<summary>摘要</summary>
现在，许多大语言模型（LLM）的出现，AI的实现进入了新的时代。无论这些模型的本身能力和结构，有越来越多的需求要LLM具备更好的长文本理解能力，即使文本长度较短。 modeloften encounter an upper limit when processing sequences of sentences that extend beyond their comprehension capacity and result in off-topic or even chaotic responses。 although several recent works attempt to address this issue in various ways, they rarely focus on "why models are unable to compensate or strengthen their capabilities on their own".在这篇论文中，我们全面调查LLM中信息传递的本质，并提出一种新的技术 called Attention Transition。这种技术使得模型可以在不需要额外训练或影响生成流畅性的情况下，实现更长更好的文本理解。我们对XSum数据集使用LLaMa-7b模型，Context Token length在800到1900之间进行了实验。结果显示，我们在评估于GPT4的原始生成结果的基础上获得了显著提高。
</details></li>
</ul>
<hr>
<h2 id="Do-humans-and-Convolutional-Neural-Networks-attend-to-similar-areas-during-scene-classification-Effects-of-task-and-image-type"><a href="#Do-humans-and-Convolutional-Neural-Networks-attend-to-similar-areas-during-scene-classification-Effects-of-task-and-image-type" class="headerlink" title="Do humans and Convolutional Neural Networks attend to similar areas during scene classification: Effects of task and image type"></a>Do humans and Convolutional Neural Networks attend to similar areas during scene classification: Effects of task and image type</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13345">http://arxiv.org/abs/2307.13345</a></li>
<li>repo_url: None</li>
<li>paper_authors: Romy Müller, Marcel Duerschmidt, Julian Ullrich, Carsten Knoll, Sascha Weber, Steffen Seitz</li>
<li>for: 本研究旨在探讨深度学习模型如 convolutional neural networks (CNN) 是如何决定是否与人类注意力相似的因素？而前一代研究主要关注技术因素，很少关注人类注意力的因素。</li>
<li>methods: 我们在 presente 研究中采用了多种任务来诱导人类注意力地图，包括自发的视线探索、意图的视线指向以及手动选择区域。此外，我们还使用了不同类型的图像，包括单一的醒目对象、室内场景和无明确对象定义的类别。</li>
<li>results: 我们发现，人类任务对于图像类型有很大的影响。对于对象，人类手动选择生成的地图和 CNN 的注意力地图最为相似，而自发视线任务干得影响相对较小。对于室内场景，自发视线任务生成的地图和 CNN 的注意力地图最为不同，而手动选择任务生成的地图和 CNN 的注意力地图相似度较高。这些结果表明，在比较人类和 CNN 的注意力时，需要考虑人类因素。<details>
<summary>Abstract</summary>
Deep Learning models like Convolutional Neural Networks (CNN) are powerful image classifiers, but what factors determine whether they attend to similar image areas as humans do? While previous studies have focused on technological factors, little is known about the role of factors that affect human attention. In the present study, we investigated how the tasks used to elicit human attention maps interact with image characteristics in modulating the similarity between humans and CNN. We varied the intentionality of human tasks, ranging from spontaneous gaze during categorization over intentional gaze-pointing up to manual area selection. Moreover, we varied the type of image to be categorized, using either singular, salient objects, indoor scenes consisting of object arrangements, or landscapes without distinct objects defining the category. The human attention maps generated in this way were compared to the CNN attention maps revealed by explainable artificial intelligence (Grad-CAM). The influence of human tasks strongly depended on image type: For objects, human manual selection produced maps that were most similar to CNN, while the specific eye movement task has little impact. For indoor scenes, spontaneous gaze produced the least similarity, while for landscapes, similarity was equally low across all human tasks. To better understand these results, we also compared the different human attention maps to each other. Our results highlight the importance of taking human factors into account when comparing the attention of humans and CNN.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Deep Learning models" is translated as "深度学习模型" (shēn dào xué xí mó del)* "Convolutional Neural Networks" is translated as "卷积神经网络" (jué shū shēn xīn wǎng luò)* "human attention maps" is translated as "人类注意地图" (rén xìng zhù yì dì tu)* "CNN attention maps" is translated as "CNN注意地图" (CNN zhù yì dì tu)* "explainable artificial intelligence" is translated as "可解释人工智能" (kě jiě jiě rén xīn zhī neng)* "Grad-CAM" is translated as "Grad-CAM" (Grad-CAM)* "human tasks" is translated as "人类任务" (rén xìng zhī yè)* "image characteristics" is translated as "图像特点" (tú xiàng tè qǐ)* "singular objects" is translated as "单一物体" (dan yī wù tǐ)* "indoor scenes" is translated as "室内场景" (shì nérie jīng jì)* "landscapes" is translated as "风景" (fēng jǐng)* "intentionality of human tasks" is translated as "人类任务的意图性" (rén xìng zhī yè de yì tú xìng)* "specific eye movement task" is translated as "特定眼动任务" (tè dìng jǐng yù zhí zhì yè)* "manual area selection" is translated as "手动区域选择" (shǒu dòng qū yù zhì yè)Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China.
</details></li>
</ul>
<hr>
<h2 id="Analyzing-Chain-of-Thought-Prompting-in-Large-Language-Models-via-Gradient-based-Feature-Attributions"><a href="#Analyzing-Chain-of-Thought-Prompting-in-Large-Language-Models-via-Gradient-based-Feature-Attributions" class="headerlink" title="Analyzing Chain-of-Thought Prompting in Large Language Models via Gradient-based Feature Attributions"></a>Analyzing Chain-of-Thought Prompting in Large Language Models via Gradient-based Feature Attributions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13339">http://arxiv.org/abs/2307.13339</a></li>
<li>repo_url: None</li>
<li>paper_authors: Skyler Wu, Eric Meng Shen, Charumathi Badrinath, Jiaqi Ma, Himabindu Lakkaraju</li>
<li>for: 这个论文的目的是解释为何 chain-of-thought (CoT) 提示能使大型自然语言模型（LLM）在各种问答任务上具有更高的准确率。</li>
<li>methods: 这篇论文使用了 gradient-based feature attribution 方法，以生成输入字符串对模型输出的影响度量。</li>
<li>results: 研究发现，CoT 提示不会使输入字符串中相关的 Token 的重要性增加，但可以增加提问和模型输出变化时 Token 的稳定性。<details>
<summary>Abstract</summary>
Chain-of-thought (CoT) prompting has been shown to empirically improve the accuracy of large language models (LLMs) on various question answering tasks. While understanding why CoT prompting is effective is crucial to ensuring that this phenomenon is a consequence of desired model behavior, little work has addressed this; nonetheless, such an understanding is a critical prerequisite for responsible model deployment. We address this question by leveraging gradient-based feature attribution methods which produce saliency scores that capture the influence of input tokens on model output. Specifically, we probe several open-source LLMs to investigate whether CoT prompting affects the relative importances they assign to particular input tokens. Our results indicate that while CoT prompting does not increase the magnitude of saliency scores attributed to semantically relevant tokens in the prompt compared to standard few-shot prompting, it increases the robustness of saliency scores to question perturbations and variations in model output.
</details>
<details>
<summary>摘要</summary>
<<SYS>>chain-of-thought（CoT）提示有效地提高了大型语言模型（LLM）在各种问题回答任务上的准确率。 although understanding why CoT prompting is effective is crucial to ensuring that this phenomenon is a consequence of desired model behavior, little work has addressed this; nonetheless, such an understanding is a critical prerequisite for responsible model deployment. we address this question by leveraging gradient-based feature attribution methods, which produce saliency scores that capture the influence of input tokens on model output. specifically, we probe several open-source LLMs to investigate whether CoT prompting affects the relative importances they assign to particular input tokens. our results indicate that while CoT prompting does not increase the magnitude of saliency scores attributed to semantically relevant tokens in the prompt compared to standard few-shot prompting, it increases the robustness of saliency scores to question perturbations and variations in model output.Note: Please note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="The-Optimal-Approximation-Factors-in-Misspecified-Off-Policy-Value-Function-Estimation"><a href="#The-Optimal-Approximation-Factors-in-Misspecified-Off-Policy-Value-Function-Estimation" class="headerlink" title="The Optimal Approximation Factors in Misspecified Off-Policy Value Function Estimation"></a>The Optimal Approximation Factors in Misspecified Off-Policy Value Function Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13332">http://arxiv.org/abs/2307.13332</a></li>
<li>repo_url: None</li>
<li>paper_authors: Philip Amortila, Nan Jiang, Csaba Szepesvári</li>
<li>for: 这篇论文是关于 reinforcement learning（RL）中的函数估计精度的研究。特别是研究函数估计精度如何受到函数错误的影响。</li>
<li>methods: 这篇论文使用了 linear off-policy value function estimation 方法，并在不同的设定下（如 weighted $L_2$-norm、$L_\infty$ norm、状态别名和状态空间覆盖率）研究了函数估计精度的优化因子。</li>
<li>results: 研究发现，在不同的设定下，函数估计精度受到多个因素的影响，其中包括函数错误和状态别名等。这些因素的优化因子可以用来评估函数估计精度的困难程度。<details>
<summary>Abstract</summary>
Theoretical guarantees in reinforcement learning (RL) are known to suffer multiplicative blow-up factors with respect to the misspecification error of function approximation. Yet, the nature of such \emph{approximation factors} -- especially their optimal form in a given learning problem -- is poorly understood. In this paper we study this question in linear off-policy value function estimation, where many open questions remain. We study the approximation factor in a broad spectrum of settings, such as with the weighted $L_2$-norm (where the weighting is the offline state distribution), the $L_\infty$ norm, the presence vs. absence of state aliasing, and full vs. partial coverage of the state space. We establish the optimal asymptotic approximation factors (up to constants) for all of these settings. In particular, our bounds identify two instance-dependent factors for the $L_2(\mu)$ norm and only one for the $L_\infty$ norm, which are shown to dictate the hardness of off-policy evaluation under misspecification.
</details>
<details>
<summary>摘要</summary>
theoretical guarantees in reinforcement learning (RL) 知道 suffer 多个 multiplication blow-up factors with respect to the misspecification error of function approximation. yet, the nature of such approximation factors -- especially their optimal form in a given learning problem -- is poorly understood. In this paper, we study this question in linear off-policy value function estimation, where many open questions remain. We study the approximation factor in a broad spectrum of settings, such as with the weighted $L_2$-norm (where the weighting is the offline state distribution), the $L_\infty$ norm, the presence vs. absence of state aliasing, and full vs. partial coverage of the state space. We establish the optimal asymptotic approximation factors (up to constants) for all of these settings. In particular, our bounds identify two instance-dependent factors for the $L_2(\mu)$ norm and only one for the $L_\infty$ norm, which are shown to dictate the hardness of off-policy evaluation under misspecification.Here's the Chinese translation of the text:理论保证在强化学习（RL）中知道会受到函数近似错误的多个多项式增长因素的影响。然而，这些近似因素的最佳形式在给定的学习问题中仍然不够了解。在这篇论文中，我们研究了这个问题在线性偏离策略估值函数估计中，这里有许多未解之处。我们在各种设置下研究了近似因素，包括使用权重$L_2$-norm（其权重是在线状态分布上）、$L_\infty$ norm、状态别名和状态空间的完整性 vs. 部分覆盖。我们确定了所有设置的优化的极限增长因素（即常数），并且发现了这些因素在不正确的函数近似下的评估难度。 Specifically, our bounds identify two instance-dependent factors for the $L_2(\mu)$ norm and only one for the $L_\infty$ norm, which are shown to dictate the hardness of off-policy evaluation under misspecification.这里的 bounds 发现了 $L_2(\mu)$ norm 下的两个实例依赖的因素，以及 $L_\infty$ norm 下的一个因素，这些因素在函数近似错误下的评估难度。
</details></li>
</ul>
<hr>
<h2 id="2-Level-Reinforcement-Learning-for-Ships-on-Inland-Waterways"><a href="#2-Level-Reinforcement-Learning-for-Ships-on-Inland-Waterways" class="headerlink" title="2-Level Reinforcement Learning for Ships on Inland Waterways"></a>2-Level Reinforcement Learning for Ships on Inland Waterways</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16769">http://arxiv.org/abs/2307.16769</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/marwaltz/tud_rl">https://github.com/marwaltz/tud_rl</a></li>
<li>paper_authors: Martin Waltz, Niklas Paulig, Ostap Okhrin</li>
<li>for: 这个论文目的是控制自主水面车（ASV）在内陆水道（IW）上，基于深度强化学习（DRL）。</li>
<li>methods: 该框架包括两级：一级是高级本地路径规划（LPP）单元，另一级是低级路径跟踪（PF）单元，每个单元都包含一个DRL代理。LPP代理负责考虑附近船只、交通规则和水道的几何，而PF代理负责低级杆控制，并考虑水下船只的杆控制、环境力量（风、浪、涨潮）的影响。</li>
<li>results: 在模拟环境中，两个代理都进行了广泛验证，使用德国北部的下落河为例子，并使用实际的AIS轨迹来模拟其他船只的行为。<details>
<summary>Abstract</summary>
This paper proposes a realistic modularized framework for controlling autonomous surface vehicles (ASVs) on inland waterways (IWs) based on deep reinforcement learning (DRL). The framework comprises two levels: a high-level local path planning (LPP) unit and a low-level path following (PF) unit, each consisting of a DRL agent. The LPP agent is responsible for planning a path under consideration of nearby vessels, traffic rules, and the geometry of the waterway. We thereby leverage a recently proposed spatial-temporal recurrent neural network architecture, which is transferred to continuous action spaces. The PF agent is responsible for low-level actuator control while accounting for shallow water influences on the marine craft and the environmental forces winds, waves, and currents. Both agents are thoroughly validated in simulation, employing the lower Elbe in northern Germany as an example case and using real AIS trajectories to model the behavior of other ships.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Learning-Autonomous-Ultrasound-via-Latent-Task-Representation-and-Robotic-Skills-Adaptation"><a href="#Learning-Autonomous-Ultrasound-via-Latent-Task-Representation-and-Robotic-Skills-Adaptation" class="headerlink" title="Learning Autonomous Ultrasound via Latent Task Representation and Robotic Skills Adaptation"></a>Learning Autonomous Ultrasound via Latent Task Representation and Robotic Skills Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13323">http://arxiv.org/abs/2307.13323</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xutian Deng, Junnan Jiang, Wen Cheng, Miao Li</li>
<li>for: 提高机器人超声扫描的自动化精度和效率</li>
<li>methods: 使用多Modal ultrasound技术和自动适应学习方法</li>
<li>results: 实验结果显示，提议方法可以生成适应不同人群的复杂超声策略，并实现了对比较好的量化结果<details>
<summary>Abstract</summary>
As medical ultrasound is becoming a prevailing examination approach nowadays, robotic ultrasound systems can facilitate the scanning process and prevent professional sonographers from repetitive and tedious work. Despite the recent progress, it is still a challenge to enable robots to autonomously accomplish the ultrasound examination, which is largely due to the lack of a proper task representation method, and also an adaptation approach to generalize learned skills across different patients. To solve these problems, we propose the latent task representation and the robotic skills adaptation for autonomous ultrasound in this paper. During the offline stage, the multimodal ultrasound skills are merged and encapsulated into a low-dimensional probability model through a fully self-supervised framework, which takes clinically demonstrated ultrasound images, probe orientations, and contact forces into account. During the online stage, the probability model will select and evaluate the optimal prediction. For unstable singularities, the adaptive optimizer fine-tunes them to near and stable predictions in high-confidence regions. Experimental results show that the proposed approach can generate complex ultrasound strategies for diverse populations and achieve significantly better quantitative results than our previous method.
</details>
<details>
<summary>摘要</summary>
现在医疗超声成为主流检查方法，Robotic超声系统可以帮助扫描过程，避免专业医疗人员的重复和厌烦工作。尽管最近做出了一些进步，但是还是面临着自动完成超声检查的挑战，主要原因是缺乏适当的任务表示方法，以及将学习到的技能通用化到不同的病人身上。为解决这些问题，我们在这篇论文中提出了缺失任务表示和机器人技能适应。在线阶段，我们使用了完全自我超vised框架，将多modal超声技能集成到低维度概率模型中，考虑了临床证明的超声图像、探针 orientations和触摸力。在线阶段，概率模型会选择和评估最佳预测。对于不稳定的孤点，适应优化器进行了微调，使其在高信任区域靠近和稳定预测。实验结果表明，我们的方法可以生成适应不同人口的复杂超声策略，并取得了significantly更好的量化结果，比我们之前的方法更好。
</details></li>
</ul>
<hr>
<h2 id="Towards-Integrated-Traffic-Control-with-Operating-Decentralized-Autonomous-Organization"><a href="#Towards-Integrated-Traffic-Control-with-Operating-Decentralized-Autonomous-Organization" class="headerlink" title="Towards Integrated Traffic Control with Operating Decentralized Autonomous Organization"></a>Towards Integrated Traffic Control with Operating Decentralized Autonomous Organization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03769">http://arxiv.org/abs/2308.03769</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shengyue Yao, Jingru Yu, Yi Yu, Jia Xu, Xingyuan Dai, Honghai Li, Fei-Yue Wang, Yilun Lin</li>
<li>for: 提高智能交通系统（ITS）的集成控制能力，考虑多种多样智能代理的优化和扩展。</li>
<li>methods: 基于分布式自治组织（DAO）框架，实现全局能源消耗效率（ECE）的全球协商，并通过奖励机制优化本地目标。另外，对DAO结构硬直性问题进行了解决方案。</li>
<li>results: 通过 numerics 实验，提出的方法可以在各种情况下更快达成全局目标，并且可以提高本地目标。这表明该方法在智能交通系统集成控制中具有潜在的应用前景。<details>
<summary>Abstract</summary>
With a growing complexity of the intelligent traffic system (ITS), an integrated control of ITS that is capable of considering plentiful heterogeneous intelligent agents is desired. However, existing control methods based on the centralized or the decentralized scheme have not presented their competencies in considering the optimality and the scalability simultaneously. To address this issue, we propose an integrated control method based on the framework of Decentralized Autonomous Organization (DAO). The proposed method achieves a global consensus on energy consumption efficiency (ECE), meanwhile to optimize the local objectives of all involved intelligent agents, through a consensus and incentive mechanism. Furthermore, an operation algorithm is proposed regarding the issue of structural rigidity in DAO. Specifically, the proposed operation approach identifies critical agents to execute the smart contract in DAO, which ultimately extends the capability of DAO-based control. In addition, a numerical experiment is designed to examine the performance of the proposed method. The experiment results indicate that the controlled agents can achieve a consensus faster on the global objective with improved local objectives by the proposed method, compare to existing decentralized control methods. In general, the proposed method shows a great potential in developing an integrated control system in the ITS
</details>
<details>
<summary>摘要</summary>
随着智能交通系统（ITS）的复杂度的增加，一种能够考虑丰富多种智能代理人的集中化控制方法是感到需要。然而，现有的中央化或分布式控制方法未能同时考虑优化和可扩展性。为解决这个问题，我们提议一种基于分布式自治组织（DAO）的集中化控制方法。该方法可以在全球范围内达成能源消耗效率（ECE）的全球协议，同时通过协议和激励机制来优化所有参与的智能代理人的本地目标。此外，我们还提出了一种对 DAO 的结构硬直性问题的操作算法。具体来说，该算法可以在 DAO 中标识关键代理人执行智能合同，从而扩展 DAO 基础的能力。此外，我们还设计了一个数值实验，以评估提议方法的性能。实验结果表明，由于提议方法，控制代理人可以更快达成全球目标，并且提高本地目标。总之，我们的方法在智能交通系统中集中化控制方法具有很大的潜力。
</details></li>
</ul>
<hr>
<h2 id="Word-Sense-Disambiguation-as-a-Game-of-Neurosymbolic-Darts"><a href="#Word-Sense-Disambiguation-as-a-Game-of-Neurosymbolic-Darts" class="headerlink" title="Word Sense Disambiguation as a Game of Neurosymbolic Darts"></a>Word Sense Disambiguation as a Game of Neurosymbolic Darts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16663">http://arxiv.org/abs/2307.16663</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tiansi Dong, Rafet Sifa</li>
<li>for: 本研究旨在提出一种新的神经符号方法来解决自然语言理解和知识工程中的词意划分问题。</li>
<li>methods: 该方法基于一种嵌入式的 Configuration of Nested Balls (CNB) 模型，其中每个词 embedding 的中心点具有一定的稳定性，并且可以准确地表示词义的含义。而 inclusion 关系 между球体可以准确地表示符号 гиперonym 关系 между词义，从而实现了简单的逻辑推理。</li>
<li>results: 在使用预训练 n-ball 嵌入后，我们在 WSD 数据集上进行了一系列实验，并取得了 F1 分数在 90.1% 到 100.0% 之间的结果。这表明了我们的方法可以超越深度学习方法的楼层效果。<details>
<summary>Abstract</summary>
Word Sense Disambiguation (WSD) is one of the hardest tasks in natural language understanding and knowledge engineering. The glass ceiling of 80% F1 score is recently achieved through supervised deep-learning, enriched by a variety of knowledge graphs. Here, we propose a novel neurosymbolic methodology that is able to push the F1 score above 90%. The core of our methodology is a neurosymbolic sense embedding, in terms of a configuration of nested balls in n-dimensional space. The centre point of a ball well-preserves word embedding, which partially fix the locations of balls. Inclusion relations among balls precisely encode symbolic hypernym relations among senses, and enable simple logic deduction among sense embeddings, which cannot be realised before. We trained a Transformer to learn the mapping from a contextualized word embedding to its sense ball embedding, just like playing the game of darts (a game of shooting darts into a dartboard). A series of experiments are conducted by utilizing pre-training n-ball embeddings, which have the coverage of around 70% training data and 75% testing data in the benchmark WSD corpus. The F1 scores in experiments range from 90.1% to 100.0% in all six groups of test data-sets (each group has 4 testing data with different sizes of n-ball embeddings). Our novel neurosymbolic methodology has the potential to break the ceiling of deep-learning approaches for WSD. Limitations and extensions of our current works are listed.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Imperceptible-Physical-Attack-against-Face-Recognition-Systems-via-LED-Illumination-Modulation"><a href="#Imperceptible-Physical-Attack-against-Face-Recognition-Systems-via-LED-Illumination-Modulation" class="headerlink" title="Imperceptible Physical Attack against Face Recognition Systems via LED Illumination Modulation"></a>Imperceptible Physical Attack against Face Recognition Systems via LED Illumination Modulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13294">http://arxiv.org/abs/2307.13294</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junbin Fang, Canjian Jiang, You Jiang, Puxi Lin, Zhaojie Chen, Yujing Sun, Siu-Ming Yiu, Zoe L. Jiang</li>
<li>for: 本研究旨在提出一种实用、执行、不显而又低计算量的LED照明模拓ersion adversarial攻击，以攻击数据驱动的面Recognition视系统。</li>
<li>methods: 该攻击方法基于LED照明模拓ersion，通过快速幅度调整场景LED照明的强度来生成不可见的明暗变化，并利用CMOS图像感知器的滚动闸效果，将明暗信息加入到捕捉到的脸像中。</li>
<li>results: 对于Well-known的面检测模型Dlib、MTCNN和RetinaFace，DoS攻击达成率分别为97.67%、100%和100%，而对于面验证模型Dlib、FaceNet和ArcFace，掩饰攻击达成率均为100%。<details>
<summary>Abstract</summary>
Although face recognition starts to play an important role in our daily life, we need to pay attention that data-driven face recognition vision systems are vulnerable to adversarial attacks. However, the current two categories of adversarial attacks, namely digital attacks and physical attacks both have drawbacks, with the former ones impractical and the latter one conspicuous, high-computational and inexecutable. To address the issues, we propose a practical, executable, inconspicuous and low computational adversarial attack based on LED illumination modulation. To fool the systems, the proposed attack generates imperceptible luminance changes to human eyes through fast intensity modulation of scene LED illumination and uses the rolling shutter effect of CMOS image sensors in face recognition systems to implant luminance information perturbation to the captured face images. In summary,we present a denial-of-service (DoS) attack for face detection and a dodging attack for face verification. We also evaluate their effectiveness against well-known face detection models, Dlib, MTCNN and RetinaFace , and face verification models, Dlib, FaceNet,and ArcFace.The extensive experiments show that the success rates of DoS attacks against face detection models reach 97.67%, 100%, and 100%, respectively, and the success rates of dodging attacks against all face verification models reach 100%.
</details>
<details>
<summary>摘要</summary>
尽管人脸识别开始在我们日常生活中扮演重要角色，但是我们需要注意到数据驱动的人脸识别视觉系统容易受到反对攻击。然而，当前两种反对攻击方法，即数字攻击和物理攻击都有缺点，前者不实用，后者突出、计算高、不执行。为了解决这些问题，我们提议一种实用、执行、不露出来、计算低的反对攻击方法，基于LED照明模拟。通过快速强度模拟场景LED照明的快速强度变化，并使用CMOS图像感知器中的滚动闸效果，我们生成不可见的明暗变化，让人类眼睛无法感受到。总之，我们提出了一种人脸检测系统的拒绝服务（DoS）攻击和躲避攻击，并对知名的人脸检测模型Dlib、MTCNN和RetinaFace，以及人脸验证模型Dlib、FaceNet和ArcFace进行了广泛的测试，结果显示，对人脸检测模型的DoS攻击成功率为97.67%、100%和100%，对所有人脸验证模型的躲避攻击成功率均为100%。
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-Learning-based-Adaptation-and-Scheduling-Methods-for-Multi-source-DASH"><a href="#Reinforcement-Learning-based-Adaptation-and-Scheduling-Methods-for-Multi-source-DASH" class="headerlink" title="Reinforcement Learning -based Adaptation and Scheduling Methods for Multi-source DASH"></a>Reinforcement Learning -based Adaptation and Scheduling Methods for Multi-source DASH</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11621">http://arxiv.org/abs/2308.11621</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ntnghia1908/Master_Thesis">https://github.com/ntnghia1908/Master_Thesis</a></li>
<li>paper_authors: Nghia T. Nguyen, Long Luu, Phuong L. Vo, Thi Thanh Sang Nguyen, Cuong T. Do, Ngoc-thanh Nguyen</li>
<li>for: 这个论文主要研究多源视频流ING的高质量体验（QoE）优化。</li>
<li>methods: 该论文提出了两种RL算法来优化多源视频流的QoE：RL-based adaptation with greedy scheduling（RLAGS）和RL-based adaptation and scheduling（RLAS）。</li>
<li>results: 经过广泛的 simulations  validate 了提出的算法的效率。<details>
<summary>Abstract</summary>
Dynamic adaptive streaming over HTTP (DASH) has been widely used in video streaming recently. In DASH, the client downloads video chunks in order from a server. The rate adaptation function at the video client enhances the user's quality-of-experience (QoE) by choosing a suitable quality level for each video chunk to download based on the network condition. Today networks such as content delivery networks, edge caching networks, content-centric networks,... usually replicate video contents on multiple cache nodes. We study video streaming from multiple sources in this work. In multi-source streaming, video chunks may arrive out of order due to different conditions of the network paths. Hence, to guarantee a high QoE, the video client needs not only rate adaptation but also chunk scheduling. Reinforcement learning (RL) has emerged as the state-of-the-art control method in various fields in recent years. This paper proposes two algorithms for streaming from multiple sources: RL-based adaptation with greedy scheduling (RLAGS) and RL-based adaptation and scheduling (RLAS). We also build a simulation environment for training and evaluating. The efficiency of the proposed algorithms is proved via extensive simulations with real-trace data.
</details>
<details>
<summary>摘要</summary>
“对于多源串流，由于不同的网络路径，可能会有弹性的播放顺序。因此，确保高质量体验（QoE）需要不仅进行率适应，还需要进行块调度。对于多源串流，本文提出了两个算法：基于强化学习（RL）的适应调度（RLAGS）和基于RL的适应调度和调度（RLAS）。我们还建立了一个实验环境，用于训练和评估。经过广泛的实验，我们证明了提案的算法的效率。”Note: Simplified Chinese is used here, as it is more commonly used in mainland China and is the standard for most online content. Traditional Chinese is used in Taiwan and Hong Kong, and is a more complex and nuanced version of the language.
</details></li>
</ul>
<hr>
<h2 id="Curvature-based-Transformer-for-Molecular-Property-Prediction"><a href="#Curvature-based-Transformer-for-Molecular-Property-Prediction" class="headerlink" title="Curvature-based Transformer for Molecular Property Prediction"></a>Curvature-based Transformer for Molecular Property Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13275">http://arxiv.org/abs/2307.13275</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yili Chen, Zhengyu Li, Zheng Wan, Hui Yu, Xian Wei</li>
<li>for: 提高基于人工智能的药物设计中分子属性预测的能力</li>
<li>methods: 引入Discretization of Ricci Curvature来提高图像神经网络模型对分子图数据的结构信息抽取能力</li>
<li>results: 在PCQM4M-LST、MoleculeNet等化学分子数据集上进行了实验，与Uni-Mol、Graphormer等模型进行比较，结果表明该方法可以达到状态艺术的结果，并且证明了Discretized Ricci curvature可以反映分子结构和功能关系。<details>
<summary>Abstract</summary>
The prediction of molecular properties is one of the most important and challenging tasks in the field of artificial intelligence-based drug design. Among the current mainstream methods, the most commonly used feature representation for training DNN models is based on SMILES and molecular graphs, although these methods are concise and effective, they also limit the ability to capture spatial information. In this work, we propose Curvature-based Transformer to improve the ability of Graph Transformer neural network models to extract structural information on molecular graph data by introducing Discretization of Ricci Curvature. To embed the curvature in the model, we add the curvature information of the graph as positional Encoding to the node features during the attention-score calculation. This method can introduce curvature information from graph data without changing the original network architecture, and it has the potential to be extended to other models. We performed experiments on chemical molecular datasets including PCQM4M-LST, MoleculeNet and compared with models such as Uni-Mol, Graphormer, and the results show that this method can achieve the state-of-the-art results. It is proved that the discretized Ricci curvature also reflects the structural and functional relationship while describing the local geometry of the graph molecular data.
</details>
<details>
<summary>摘要</summary>
预测分子性质是人工智能基于药物设计的一个最重要和挑战性任务。现有主流方法中，最常用的特征表示方法是基于SMILES和分子图，尽管这些方法简洁有效，但它们也限制了捕捉空间信息的能力。在这种工作中，我们提出了几何基于变换器的Curvature-based Transformer，以提高分子图数据中的结构信息提取能力。为了嵌入曲率信息，我们在节点特征计算时将拟合分数加入节点特征中，从而将曲率信息作为位置编码。这种方法可以在原始网络结构不变的情况下，将曲率信息引入模型，并且具有扩展性。我们在PCQM4M-LST、MoleculeNet等化学分子数据集上进行了实验，并与Uni-Mol、Graphormer等模型进行比较，结果表明，这种方法可以实现领先的结果。此外，我们还发现，积分 Ricci 曲率也可以反映分子结构和功能关系，并描述分子图数据的地方几何结构。
</details></li>
</ul>
<hr>
<h2 id="Unbiased-Weight-Maximization"><a href="#Unbiased-Weight-Maximization" class="headerlink" title="Unbiased Weight Maximization"></a>Unbiased Weight Maximization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13270">http://arxiv.org/abs/2307.13270</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephen Chung</li>
<li>for: 本研究旨在提出一种生物学可能性的人工神经网络（ANN）训练方法，即将每个单元视为一个随机强化学习（RL）代理，从而将网络视为一群代理。这种方法可以更好地模仿生物系统中观察到的 synaptic plasticity 的形式。</li>
<li>methods: 本研究使用的方法包括 REINFORCE 本地学习规则，以及一种名为 Weight Maximization 的新方法。Weight Maximization 将每个隐藏单元的奖励信号替换为其发射量的 нор，从而让每个隐藏单元可以最大化其发射量的 norm 而不是全局奖励信号。</li>
<li>results: 研究人员通过分析Weight Maximization的理论性质和提出一种变体 Unbiased Weight Maximization，发现这种新的学习规则可以提高学习速度和最终性能。特别是，在我们所知道的情况下，这是第一种不偏不倚于网络单元数量的学习规则，可以快速地学习一个 Bernoulli-logistic 网络。<details>
<summary>Abstract</summary>
A biologically plausible method for training an Artificial Neural Network (ANN) involves treating each unit as a stochastic Reinforcement Learning (RL) agent, thereby considering the network as a team of agents. Consequently, all units can learn via REINFORCE, a local learning rule modulated by a global reward signal, which aligns more closely with biologically observed forms of synaptic plasticity. Nevertheless, this learning method is often slow and scales poorly with network size due to inefficient structural credit assignment, since a single reward signal is broadcast to all units without considering individual contributions. Weight Maximization, a proposed solution, replaces a unit's reward signal with the norm of its outgoing weight, thereby allowing each hidden unit to maximize the norm of the outgoing weight instead of the global reward signal. In this research report, we analyze the theoretical properties of Weight Maximization and propose a variant, Unbiased Weight Maximization. This new approach provides an unbiased learning rule that increases learning speed and improves asymptotic performance. Notably, to our knowledge, this is the first learning rule for a network of Bernoulli-logistic units that is unbiased and scales well with the number of network's units in terms of learning speed.
</details>
<details>
<summary>摘要</summary>
一种生物学可能性的方法 для训练人工神经网络（ANN）是将每个单元视为一个随机强化学习（RL）代理，从而考虑网络为一群代理。因此，所有单元都可以通过REINFORCE本地学习规则，该规则由全局奖励信号修饰，更加接近生物观察到的 synaptic plasticity 形式。然而，这种学习方法通常慢速并且与网络大小成比例差化学分，因为不充分考虑单元各自的贡献。Weight Maximization 是一种提议的解决方案，它将每个隐藏单元的奖励信号替换为单元的出口权重的 нор，从而让每个隐藏单元可以最大化出口权重的 norm 而不是全局奖励信号。在这份研究报告中，我们分析了Weight Maximization 的理论性质和一种变体，即偏函数Weight Maximization。这种新的学习规则提供了一种不偏学习规则，可以提高学习速度和最终性能。值得注意的是，到我们所知，这是一种可以快速学习和与网络单元数量成比例增长的学习规则，对于一个由 Bernoulli-logistic 单元组成的网络来说。
</details></li>
</ul>
<hr>
<h2 id="LoraHub-Efficient-Cross-Task-Generalization-via-Dynamic-LoRA-Composition"><a href="#LoraHub-Efficient-Cross-Task-Generalization-via-Dynamic-LoRA-Composition" class="headerlink" title="LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition"></a>LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13269">http://arxiv.org/abs/2307.13269</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sail-sg/lorahub">https://github.com/sail-sg/lorahub</a></li>
<li>paper_authors: Chengsong Huang, Qian Liu, Bill Yuchen Lin, Tianyu Pang, Chao Du, Min Lin</li>
<li>for: 这篇论文旨在研究LoRA（低级别适应）的可组合性，以实现新任务的适应性。</li>
<li>methods: 该论文提出了LoraHub框架，可以策略性地组合多个LoRA模块，从多个任务中学习各种不同的技能。</li>
<li>results: 实验结果表明，LoraHub可以在几个shot数据量的情况下，模拟内在学习的表现，而不需要具体的例子。此外，LoraHub的组合不需要新的参数或梯度。<details>
<summary>Abstract</summary>
Low-rank adaptations (LoRA) are often employed to fine-tune large language models (LLMs) for new tasks. This paper investigates LoRA composability for cross-task generalization and introduces LoraHub, a strategic framework devised for the purposive assembly of LoRA modules trained on diverse given tasks, with the objective of achieving adaptable performance on unseen tasks. With just a few examples from a novel task, LoraHub enables the fluid combination of multiple LoRA modules, eradicating the need for human expertise. Notably, the composition requires neither additional model parameters nor gradients. Our empirical results, derived from the Big-Bench Hard (BBH) benchmark, suggest that LoraHub can effectively mimic the performance of in-context learning in few-shot scenarios, excluding the necessity of in-context examples alongside each inference input. A significant contribution of our research is the fostering of a community for LoRA, where users can share their trained LoRA modules, thereby facilitating their application to new tasks. We anticipate this resource will widen access to and spur advancements in general intelligence as well as LLMs in production. Code will be available at https://github.com/sail-sg/lorahub.
</details>
<details>
<summary>摘要</summary>
低阶 adaptations（LoRA）常用于细化大语言模型（LLM）以适应新任务。这篇论文研究LoRA的可组合性，并提出了LoraHub，一个战略性框架，用于策略性将LoRA模块训练在多种任务上，以实现对未看过任务的适应性。只需几个例子，LoraHub可以快速组合多个LoRA模块，不需要人工专业知识。更重要的是，组合不需要额外参数或梯度。我们的实验结果，基于Big-Bench Hard（BBH）benchmark，表明LoraHub可以有效模拟少数例大学习的表现，排除需要在每个推理输入 alongside的具体例子。我们的研究的一个重要贡献是推动LoRA社区，用户可以共享自己训练好的LoRA模块，从而使其应用于新任务。我们预计这种资源将扩大LLM的应用范围和推动生产环境中的普通智能。代码将在https://github.com/sail-sg/lorahub上提供。
</details></li>
</ul>
<hr>
<h2 id="Federated-Split-Learning-with-Only-Positive-Labels-for-resource-constrained-IoT-environment"><a href="#Federated-Split-Learning-with-Only-Positive-Labels-for-resource-constrained-IoT-environment" class="headerlink" title="Federated Split Learning with Only Positive Labels for resource-constrained IoT environment"></a>Federated Split Learning with Only Positive Labels for resource-constrained IoT environment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13266">http://arxiv.org/abs/2307.13266</a></li>
<li>repo_url: None</li>
<li>paper_authors: Praveen Joshi, Chandra Thapa, Mohammed Hasanuzzaman, Ted Scully, Haithem Afli</li>
<li>for: 提高 IoT 设备数据隐私和提高模型训练效率</li>
<li>methods: 使用 federated split learning (SFPL) 技术，包括随机洗涤数据和本地批量正则化</li>
<li>results: SFPL 比 SFL 提高了模型训练效率和精度，具体比例为：	+ CIFAR-100 数据集上 ResNet-56 和 ResNet-32 模型的比例分别为 51.54 和 32.57	+ CIFAR-10 数据集上 ResNet-32 和 ResNet-8 模型的比例分别为 9.23 和 8.52<details>
<summary>Abstract</summary>
Distributed collaborative machine learning (DCML) is a promising method in the Internet of Things (IoT) domain for training deep learning models, as data is distributed across multiple devices. A key advantage of this approach is that it improves data privacy by removing the necessity for the centralized aggregation of raw data but also empowers IoT devices with low computational power. Among various techniques in a DCML framework, federated split learning, known as splitfed learning (SFL), is the most suitable for efficient training and testing when devices have limited computational capabilities. Nevertheless, when resource-constrained IoT devices have only positive labeled data, multiclass classification deep learning models in SFL fail to converge or provide suboptimal results. To overcome these challenges, we propose splitfed learning with positive labels (SFPL). SFPL applies a random shuffling function to the smashed data received from clients before supplying it to the server for model training. Additionally, SFPL incorporates the local batch normalization for the client-side model portion during the inference phase. Our results demonstrate that SFPL outperforms SFL: (i) by factors of 51.54 and 32.57 for ResNet-56 and ResNet-32, respectively, with the CIFAR-100 dataset, and (ii) by factors of 9.23 and 8.52 for ResNet-32 and ResNet-8, respectively, with CIFAR-10 dataset. Overall, this investigation underscores the efficacy of the proposed SFPL framework in DCML.
</details>
<details>
<summary>摘要</summary>
“分布式合作机器学习（DCML）是互联网东西（IoT）领域的一种有前途的方法，用于训练深度学习模型，因为数据分布在多个设备上。这种方法的优点在于，它提高了数据隐私，因为不需要将原始数据集中化，同时也使得 IoT 设备 WITH 较低的计算能力得到启发。在 DCML 框架中， federated split learning（SFL）是最适合高效地训练和测试，因为设备具有有限的计算能力。然而，当 IoT 设备具有只有正例数据时，SFL 中的多类分类深度学习模型无法实现或提供低效果。为了解决这些挑战，我们提出了 splitfed learning with positive labels（SFPL）。SFPL 使用随机排序函数将客户端上收到的数据进行销毁，然后将其提供给服务器进行模型训练。此外，SFPL 还在推理阶段添加了本地批处理正则化。我们的结果表明，SFPL 在 CIFAR-100 和 CIFAR-10  datasets 上分别比 SFL 提高了51.54 和 32.57 倍，并且在 CIFAR-10  datasets 上比 SFL 提高了9.23 和 8.52 倍。总的来说，这种研究证明了我们提出的 SFPL 框架在 DCML 中的效果。”Note: Please note that the translation is in Simplified Chinese, and the words and phrases in bold are the ones that are translated.
</details></li>
</ul>
<hr>
<h2 id="Structural-Credit-Assignment-with-Coordinated-Exploration"><a href="#Structural-Credit-Assignment-with-Coordinated-Exploration" class="headerlink" title="Structural Credit Assignment with Coordinated Exploration"></a>Structural Credit Assignment with Coordinated Exploration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13256">http://arxiv.org/abs/2307.13256</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephen Chung</li>
<li>for: 训练人工神经网络（ANN），使用生物学可能性的方法。</li>
<li>methods: 每个单元 treated as 随机强化学习（RL）代理，使用REINFORCE本地学习规则，且受到全局奖励信号的调整，更加符合生物观察到的 synaptic plasticity 形式。</li>
<li>results: 协调探索可以大幅提高训练速度，并且可以超过 straight-through estimator（STE）反propagation。<details>
<summary>Abstract</summary>
A biologically plausible method for training an Artificial Neural Network (ANN) involves treating each unit as a stochastic Reinforcement Learning (RL) agent, thereby considering the network as a team of agents. Consequently, all units can learn via REINFORCE, a local learning rule modulated by a global reward signal, which aligns more closely with biologically observed forms of synaptic plasticity. However, this learning method tends to be slow and does not scale well with the size of the network. This inefficiency arises from two factors impeding effective structural credit assignment: (i) all units independently explore the network, and (ii) a single reward is used to evaluate the actions of all units. Accordingly, methods aimed at improving structural credit assignment can generally be classified into two categories. The first category includes algorithms that enable coordinated exploration among units, such as MAP propagation. The second category encompasses algorithms that compute a more specific reward signal for each unit within the network, like Weight Maximization and its variants. In this research report, our focus is on the first category. We propose the use of Boltzmann machines or a recurrent network for coordinated exploration. We show that the negative phase, which is typically necessary to train Boltzmann machines, can be removed. The resulting learning rules are similar to the reward-modulated Hebbian learning rule. Experimental results demonstrate that coordinated exploration significantly exceeds independent exploration in training speed for multiple stochastic and discrete units based on REINFORCE, even surpassing straight-through estimator (STE) backpropagation.
</details>
<details>
<summary>摘要</summary>
一种生物学可能性的人工神经网络（ANN）训练方法是将每个单元视为一个随机奖励学习（RL）代理，从而考虑网络为一个团队。因此，所有单元都可以通过REINFORCE本地学习规则，该规则由全局奖励信号调整，更加接近生物观察到的 synaptic plasticity 形式。然而，这种学习方法通常慢并不能Scalable 到网络的大小。这种缺效果来自两个因素：（i）所有单元独立探索网络，（ii）全网络使用单一奖励评价所有单元的行为。因此，可以将方法分为两类：第一类包括使用MAP卷积算法进行协调探索的算法，第二类包括计算网络内每个单元的具体奖励信号的算法，如Weight Maximization 和其变种。在这份研究报告中，我们注重第一类。我们提议使用 Boltzmann 机或回归网络进行协调探索。我们发现，通常需要训练 Boltzmann 机的负阶可以被除去，其结果的学习规则与奖励调整的 Hebbian 学习规则类似。实验结果表明，协调探索在多个随机和离散单元基于 REINFORCE 训练速度上明显超过独立探索，甚至超过 straight-through estimator（STE）反propagation。
</details></li>
</ul>
<hr>
<h2 id="GaPro-Box-Supervised-3D-Point-Cloud-Instance-Segmentation-Using-Gaussian-Processes-as-Pseudo-Labelers"><a href="#GaPro-Box-Supervised-3D-Point-Cloud-Instance-Segmentation-Using-Gaussian-Processes-as-Pseudo-Labelers" class="headerlink" title="GaPro: Box-Supervised 3D Point Cloud Instance Segmentation Using Gaussian Processes as Pseudo Labelers"></a>GaPro: Box-Supervised 3D Point Cloud Instance Segmentation Using Gaussian Processes as Pseudo Labelers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13251">http://arxiv.org/abs/2307.13251</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vinairesearch/gapro">https://github.com/vinairesearch/gapro</a></li>
<li>paper_authors: Tuan Duc Ngo, Binh-Son Hua, Khoi Nguyen</li>
<li>for: 这篇论文主要针对的是3D点云实例分割的问题，即使使用软指导下进行解决。</li>
<li>methods: 我们提出了一种新的实例分割方法，即使使用软指导下进行解决。我们的方法包括从矩形框签到实例分割网络的训练。此外，我们还使用了自适应策略来进一步提高方法的性能。</li>
<li>results: 我们的实验表明，我们的方法可以比前一代软指导下的实例分割方法表现更好，并且与现有的全指导方法具有相似的性能。此外，我们还证明了我们的方法可以适应不同的全指导方法，只需使用我们生成的 Pseudo 标签进行训练即可。<details>
<summary>Abstract</summary>
Instance segmentation on 3D point clouds (3DIS) is a longstanding challenge in computer vision, where state-of-the-art methods are mainly based on full supervision. As annotating ground truth dense instance masks is tedious and expensive, solving 3DIS with weak supervision has become more practical. In this paper, we propose GaPro, a new instance segmentation for 3D point clouds using axis-aligned 3D bounding box supervision. Our two-step approach involves generating pseudo labels from box annotations and training a 3DIS network with the resulting labels. Additionally, we employ the self-training strategy to improve the performance of our method further. We devise an effective Gaussian Process to generate pseudo instance masks from the bounding boxes and resolve ambiguities when they overlap, resulting in pseudo instance masks with their uncertainty values. Our experiments show that GaPro outperforms previous weakly supervised 3D instance segmentation methods and has competitive performance compared to state-of-the-art fully supervised ones. Furthermore, we demonstrate the robustness of our approach, where we can adapt various state-of-the-art fully supervised methods to the weak supervision task by using our pseudo labels for training. The source code and trained models are available at https://github.com/VinAIResearch/GaPro.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="RoSAS-Deep-Semi-Supervised-Anomaly-Detection-with-Contamination-Resilient-Continuous-Supervision"><a href="#RoSAS-Deep-Semi-Supervised-Anomaly-Detection-with-Contamination-Resilient-Continuous-Supervision" class="headerlink" title="RoSAS: Deep Semi-Supervised Anomaly Detection with Contamination-Resilient Continuous Supervision"></a>RoSAS: Deep Semi-Supervised Anomaly Detection with Contamination-Resilient Continuous Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13239">http://arxiv.org/abs/2307.13239</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xuhongzuo/rosas">https://github.com/xuhongzuo/rosas</a></li>
<li>paper_authors: Hongzuo Xu, Yijie Wang, Guansong Pang, Songlei Jian, Ning Liu, Yongjun Wang<br>for: 这篇论文是为了解决半有向式异常检测方法中的两个限制而撰写的。这两个限制分别是：1）无法处理没有标签的异常（即异常污染），这可能导致学习过程中的混乱；2）仅使用类别型标签（例如二进制或排序标签），这会导致异常分析 scores 的学习得到极其连续的分布。methods: 这篇论文提出了一种新的半有向式异常检测方法，其中提案了一种称为“污染抑制连续超级指导”的新方法。这种方法利用标签的混合来创建新的标签数据，以减少异常污染的影响。同时，这种方法还加入了一个对应于特征学习的目标，以增强网络的弹性和适应力。results: 根据11个真实世界数据集的实验结果，这篇论文的方法与现有的竞争者相比，能够提高20%-30%的AUC-PR表现，并且在不同的异常污染水平和标签数量中具有更好的适应能力和更高的稳定性。<details>
<summary>Abstract</summary>
Semi-supervised anomaly detection methods leverage a few anomaly examples to yield drastically improved performance compared to unsupervised models. However, they still suffer from two limitations: 1) unlabeled anomalies (i.e., anomaly contamination) may mislead the learning process when all the unlabeled data are employed as inliers for model training; 2) only discrete supervision information (such as binary or ordinal data labels) is exploited, which leads to suboptimal learning of anomaly scores that essentially take on a continuous distribution. Therefore, this paper proposes a novel semi-supervised anomaly detection method, which devises \textit{contamination-resilient continuous supervisory signals}. Specifically, we propose a mass interpolation method to diffuse the abnormality of labeled anomalies, thereby creating new data samples labeled with continuous abnormal degrees. Meanwhile, the contaminated area can be covered by new data samples generated via combinations of data with correct labels. A feature learning-based objective is added to serve as an optimization constraint to regularize the network and further enhance the robustness w.r.t. anomaly contamination. Extensive experiments on 11 real-world datasets show that our approach significantly outperforms state-of-the-art competitors by 20%-30% in AUC-PR and obtains more robust and superior performance in settings with different anomaly contamination levels and varying numbers of labeled anomalies. The source code is available at https://github.com/xuhongzuo/rosas/.
</details>
<details>
<summary>摘要</summary>
semi-supervised异常检测方法可以利用一些异常示例来提高性能，但它们仍然受到两种限制：1）无标签异常（即异常污染）可能会导致学习过程中的干扰，当所有无标签数据被用作模型训练时；2）只利用精确的数据标签（如二进制或排序数据标签），这会导致异常分数的学习被强制为精确的连续分布。因此，本文提出了一种新的 semi-supervised异常检测方法，即使用“污染 resistant 连续指导信号”。具体来说，我们提出了一种质量 interpolating 方法，以填充标记为异常的数据中的异常性，并创建新的数据样本，其标签为连续的异常度。同时，污染区域可以被新生成的数据样本覆盖，这些样本由正确标签的数据组合生成。此外，我们还添加了一个基于特征学习的目标函数，以便为抗污染regular化网络，进一步提高对异常污染的Robustness。我们在11个实际世界数据集上进行了广泛的实验，结果表明，我们的方法在AUC-PR方面比状态艺术竞争者提高20%-30%，并在不同的异常污染水平和变量数量的情况下具有更加稳定和优秀的性能。代码可以在https://github.com/xuhongzuo/rosas/获取。
</details></li>
</ul>
<hr>
<h2 id="Multilevel-Large-Language-Models-for-Everyone"><a href="#Multilevel-Large-Language-Models-for-Everyone" class="headerlink" title="Multilevel Large Language Models for Everyone"></a>Multilevel Large Language Models for Everyone</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13221">http://arxiv.org/abs/2307.13221</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanhao Gong</li>
<li>for: 将大语言模型连接到一起，实现更高级别的功能，基于用户个人输入和互联网信息。</li>
<li>methods: 利用人脑蓝图的多层次结构，连接通用和专业型大语言模型，以实现更高效的自然语言处理、计算机视觉任务、专业助手、商业和医疗应用。</li>
<li>results: 提出了一种基于用户个人输入和互联网信息的多层次大语言模型，可以减少冗余并提高性能，适用于多种应用场景。<details>
<summary>Abstract</summary>
Large language models have made significant progress in the past few years. However, they are either generic {\it or} field specific, splitting the community into different groups. In this paper, we unify these large language models into a larger map, where the generic {\it and} specific models are linked together and can improve each other, based on the user personal input and information from the internet. The idea of linking several large language models together is inspired by the functionality of human brain. The specific regions on the brain cortex are specific for certain low level functionality. And these regions can jointly work together to achieve more complex high level functionality. Such behavior on human brain cortex sheds the light to design the multilevel large language models that contain global level, field level and user level models. The user level models run on local machines to achieve efficient response and protect the user's privacy. Such multilevel models reduce some redundancy and perform better than the single level models. The proposed multilevel idea can be applied in various applications, such as natural language processing, computer vision tasks, professional assistant, business and healthcare.
</details>
<details>
<summary>摘要</summary>
Our multilevel approach includes global, field, and user levels, with user-level models running on local machines to ensure efficient response and protect user privacy. This approach reduces redundancy and performs better than single-level models, and it can be applied to various applications such as natural language processing, computer vision tasks, professional assistance, business, and healthcare.
</details></li>
</ul>
<hr>
<h2 id="One-for-Multiple-Physics-informed-Synthetic-Data-Boosts-Generalizable-Deep-Learning-for-Fast-MRI-Reconstruction"><a href="#One-for-Multiple-Physics-informed-Synthetic-Data-Boosts-Generalizable-Deep-Learning-for-Fast-MRI-Reconstruction" class="headerlink" title="One for Multiple: Physics-informed Synthetic Data Boosts Generalizable Deep Learning for Fast MRI Reconstruction"></a>One for Multiple: Physics-informed Synthetic Data Boosts Generalizable Deep Learning for Fast MRI Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13220">http://arxiv.org/abs/2307.13220</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wangziblake/pisf">https://github.com/wangziblake/pisf</a></li>
<li>paper_authors: Zi Wang, Xiaotong Yu, Chengyan Wang, Weibo Chen, Jiazheng Wang, Ying-Hua Chu, Hongwei Sun, Rushuai Li, Peiyong Li, Fan Yang, Haiwei Han, Taishan Kang, Jianzhong Lin, Chen Yang, Shufu Chang, Zhang Shi, Sha Hua, Yan Li, Juan Hu, Liuhong Zhu, Jianjun Zhou, Meijing Lin, Jiefeng Guo, Congbo Cai, Zhong Chen, Di Guo, Xiaobo Qu<br>for:这个研究旨在提高快速磁共振成像（MRI）的扫描时间，并使用深度学习（DL）来进行图像重建。methods:本研究使用了一个名为Physics-Informed Synthetic data learning framework（PISF），这是一个可以在多个实验设计下进行测试和训练的框架。PISF使用了一个单一的训练模型，可以在多个实验设计下进行图像重建。results:研究发现，使用PISF可以实现对多种实验设计的图像重建，并且可以在实验设计之间进行一致性的重建。此外，PISF还可以在不同的显示器和中心之间进行一致性的重建。对10名医生进行评价后，PISF的优秀适应性得到了证明。<details>
<summary>Abstract</summary>
Magnetic resonance imaging (MRI) is a principal radiological modality that provides radiation-free, abundant, and diverse information about the whole human body for medical diagnosis, but suffers from prolonged scan time. The scan time can be significantly reduced through k-space undersampling but the introduced artifacts need to be removed in image reconstruction. Although deep learning (DL) has emerged as a powerful tool for image reconstruction in fast MRI, its potential in multiple imaging scenarios remains largely untapped. This is because not only collecting large-scale and diverse realistic training data is generally costly and privacy-restricted, but also existing DL methods are hard to handle the practically inevitable mismatch between training and target data. Here, we present a Physics-Informed Synthetic data learning framework for Fast MRI, called PISF, which is the first to enable generalizable DL for multi-scenario MRI reconstruction using solely one trained model. For a 2D image, the reconstruction is separated into many 1D basic problems and starts with the 1D data synthesis, to facilitate generalization. We demonstrate that training DL models on synthetic data, integrated with enhanced learning techniques, can achieve comparable or even better in vivo MRI reconstruction compared to models trained on a matched realistic dataset, reducing the demand for real-world MRI data by up to 96%. Moreover, our PISF shows impressive generalizability in multi-vendor multi-center imaging. Its excellent adaptability to patients has been verified through 10 experienced doctors' evaluations. PISF provides a feasible and cost-effective way to markedly boost the widespread usage of DL in various fast MRI applications, while freeing from the intractable ethical and practical considerations of in vivo human data acquisitions.
</details>
<details>
<summary>摘要</summary>
To address these challenges, we present a Physics-Informed Synthetic data learning framework for Fast MRI, called PISF. This framework enables generalizable DL for multi-scenario MRI reconstruction using solely one trained model. For a 2D image, the reconstruction is separated into many 1D basic problems, starting with 1D data synthesis to facilitate generalization. We demonstrate that training DL models on synthetic data, integrated with enhanced learning techniques, can achieve comparable or even better in vivo MRI reconstruction compared to models trained on a matched realistic dataset, reducing the demand for real-world MRI data by up to 96%. Moreover, our PISF shows impressive generalizability in multi-vendor multi-center imaging, and its excellent adaptability to patients has been verified through 10 experienced doctors' evaluations.PISF provides a feasible and cost-effective way to markedly boost the widespread usage of DL in various fast MRI applications, while freeing from the intractable ethical and practical considerations of in vivo human data acquisitions.
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Deep-Hedging-Learning-to-Hedge-without-Price-Process-Modeling"><a href="#Adversarial-Deep-Hedging-Learning-to-Hedge-without-Price-Process-Modeling" class="headerlink" title="Adversarial Deep Hedging: Learning to Hedge without Price Process Modeling"></a>Adversarial Deep Hedging: Learning to Hedge without Price Process Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13217">http://arxiv.org/abs/2307.13217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Masanori Hirano, Kentaro Minami, Kentaro Imajo</li>
<li>for: 这个论文是为了探讨deep hedging框架在不完全市场中的应用，以及如何使用机器学习来 Addressing Market frictions和其他实际市场条件。</li>
<li>methods: 这个论文提出了一种新的框架，即对抗深度减值（Adversarial Deep Hedging），它是基于对抗学习的。在这个框架中，一个农家和一个生成器，分别模拟了基础资产过程和基础资产过程，在对抗的情况下被训练。这种方法可以不Explicitly model the underlying asset process，并且可以学习一个Robust hedger。</li>
<li>results: 通过numerical experiments，我们示示了我们的提议方法在实际市场数据上的竞争性表现。<details>
<summary>Abstract</summary>
Deep hedging is a deep-learning-based framework for derivative hedging in incomplete markets. The advantage of deep hedging lies in its ability to handle various realistic market conditions, such as market frictions, which are challenging to address within the traditional mathematical finance framework. Since deep hedging relies on market simulation, the underlying asset price process model is crucial. However, existing literature on deep hedging often relies on traditional mathematical finance models, e.g., Brownian motion and stochastic volatility models, and discovering effective underlying asset models for deep hedging learning has been a challenge. In this study, we propose a new framework called adversarial deep hedging, inspired by adversarial learning. In this framework, a hedger and a generator, which respectively model the underlying asset process and the underlying asset process, are trained in an adversarial manner. The proposed method enables to learn a robust hedger without explicitly modeling the underlying asset process. Through numerical experiments, we demonstrate that our proposed method achieves competitive performance to models that assume explicit underlying asset processes across various real market data.
</details>
<details>
<summary>摘要</summary>
深度投资是一种基于深度学习的derivative投资框架，可以在不完全市场中实现效果性的补偿。深度投资的优点在于它可以处理不同的实际市场条件，如市场阻力，这些条件在传统的数学金融框架中很难处理。深度投资基于市场模拟，因此下面资产价值过程模型是关键。然而，现有的文献中的深度投资经常采用传统的数学金融模型，如 Браунов运动和随机振荡模型，找到有效的下面资产模型 для深度投资学习是一个挑战。在这项研究中，我们提出了一种新的框架，即反对抗深度投资， inspirited by adversarial learning。在这个框架中，一个投资者和一个生成器，分别模拟下面资产过程和下面资产过程，在对抗的方式下进行训练。我们的提议的方法可以不Explicitly模型下面资产过程，却可以学习一个有效的投资者。通过数值实验，我们示出了我们的提议方法可以与假设下面资产过程的模型相比，在各种实际市场数据上达到竞争性的性能。
</details></li>
</ul>
<hr>
<h2 id="FedMEKT-Distillation-based-Embedding-Knowledge-Transfer-for-Multimodal-Federated-Learning"><a href="#FedMEKT-Distillation-based-Embedding-Knowledge-Transfer-for-Multimodal-Federated-Learning" class="headerlink" title="FedMEKT: Distillation-based Embedding Knowledge Transfer for Multimodal Federated Learning"></a>FedMEKT: Distillation-based Embedding Knowledge Transfer for Multimodal Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13214">http://arxiv.org/abs/2307.13214</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huy Q. Le, Minh N. H. Nguyen, Chu Myaet Thwal, Yu Qiao, Chaoning Zhang, Choong Seon Hong</li>
<li>for: 提出了一种基于多模态学习的联合学习框架，以便在多个客户端协同训练一个通用全球模型，而不需要分享私人数据。</li>
<li>methods: 提出了一种 semi-supervised learning 方法，使得客户端可以从不同的模式中提取表示，并将其交换到服务器和客户端中。同时，我们还提出了一种基于液化的多模态嵌入知识传输机制，以便在客户端和服务器之间共享知识。</li>
<li>results: 经过广泛的实验，我们发现 FedMEKT 可以在多modal human activity recognition 任务中提高全球编码器性能，同时保护用户隐私和个人数据，并且需要更少的通信成本。<details>
<summary>Abstract</summary>
Federated learning (FL) enables a decentralized machine learning paradigm for multiple clients to collaboratively train a generalized global model without sharing their private data. Most existing works simply propose typical FL systems for single-modal data, thus limiting its potential on exploiting valuable multimodal data for future personalized applications. Furthermore, the majority of FL approaches still rely on the labeled data at the client side, which is limited in real-world applications due to the inability of self-annotation from users. In light of these limitations, we propose a novel multimodal FL framework that employs a semi-supervised learning approach to leverage the representations from different modalities. Bringing this concept into a system, we develop a distillation-based multimodal embedding knowledge transfer mechanism, namely FedMEKT, which allows the server and clients to exchange the joint knowledge of their learning models extracted from a small multimodal proxy dataset. Our FedMEKT iteratively updates the generalized global encoders with the joint embedding knowledge from the participating clients. Thereby, to address the modality discrepancy and labeled data constraint in existing FL systems, our proposed FedMEKT comprises local multimodal autoencoder learning, generalized multimodal autoencoder construction, and generalized classifier learning. Through extensive experiments on three multimodal human activity recognition datasets, we demonstrate that FedMEKT achieves superior global encoder performance on linear evaluation and guarantees user privacy for personal data and model parameters while demanding less communication cost than other baselines.
</details>
<details>
<summary>摘要</summary>
联合学习（FL）提供了一个分散式机器学习模式，让多个客户端合作训练一个通用的全球模型，无需分享私人数据。现有大部分研究仅提出传统的FL系统，仅适用于单一模式的数据，因此限制了其在未来个性化应用中的潜力。另外，大多数FL方法仍然依赖客户端上的标签数据，实际上在实际应用中因为用户无法自动标注数据而受限。为了解决这些限制，我们提出了一个新的多modal FL框架，它使用了 semi-supervised 学习方法，以利用不同模式之间的表示。我们发展了一个炼制基于的多modal嵌入知识传递机制，即 FedMEKT，让服务器和客户端可以将它们的学习模型中的通用知识交换。我们的 FedMEKT 逐步更新通用全球嵌入器，使用参与客户端的共同知识。这样可以解决现有 FL 系统中的模式差异和标签数据限制，我们的提案包括本地多modal自适应器学习、通用多modal自适应器建构和通用分类学习。经过广泛的实验，我们在三个多modal人类活动识别数据集上证明了 FedMEKT 可以实现更好的全球嵌入器性能，并保证用户隐私和个人数据，同时需要更少的通信成本。
</details></li>
</ul>
<hr>
<h2 id="Gait-Cycle-Inspired-Learning-Strategy-for-Continuous-Prediction-of-Knee-Joint-Trajectory-from-sEMG"><a href="#Gait-Cycle-Inspired-Learning-Strategy-for-Continuous-Prediction-of-Knee-Joint-Trajectory-from-sEMG" class="headerlink" title="Gait Cycle-Inspired Learning Strategy for Continuous Prediction of Knee Joint Trajectory from sEMG"></a>Gait Cycle-Inspired Learning Strategy for Continuous Prediction of Knee Joint Trajectory from sEMG</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13209">http://arxiv.org/abs/2307.13209</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xueming Fu, Hao Zheng, Luyan Liu, Wenjuan Zhong, Haowen Liu, Wenxuan Xiong, Yuyang Zhang, Yifeng Chen, Dong Wei, Mingjie Dong, Yefeng Zheng, Mingming Zhang</li>
<li>for: 预测下肢运动意图是控制机器人外科手臂和 prosthetic 臂的关键。</li>
<li>methods: 本文提出了一种结合两种步征学习策略来减少人类股关节轨迹预测性能的问题。</li>
<li>results: 实验结果显示，我们的模型可以预测股关节角度的平均Root Mean Square Error（RMSE）为3.03（0.49）度和50ms之前。这是相关文献中已知的最佳性能，与其他文献相比，减少RMSE至少9.5%。<details>
<summary>Abstract</summary>
Predicting lower limb motion intent is vital for controlling exoskeleton robots and prosthetic limbs. Surface electromyography (sEMG) attracts increasing attention in recent years as it enables ahead-of-time prediction of motion intentions before actual movement. However, the estimation performance of human joint trajectory remains a challenging problem due to the inter- and intra-subject variations. The former is related to physiological differences (such as height and weight) and preferred walking patterns of individuals, while the latter is mainly caused by irregular and gait-irrelevant muscle activity. This paper proposes a model integrating two gait cycle-inspired learning strategies to mitigate the challenge for predicting human knee joint trajectory. The first strategy is to decouple knee joint angles into motion patterns and amplitudes former exhibit low variability while latter show high variability among individuals. By learning through separate network entities, the model manages to capture both the common and personalized gait features. In the second, muscle principal activation masks are extracted from gait cycles in a prolonged walk. These masks are used to filter out components unrelated to walking from raw sEMG and provide auxiliary guidance to capture more gait-related features. Experimental results indicate that our model could predict knee angles with the average root mean square error (RMSE) of 3.03(0.49) degrees and 50ms ahead of time. To our knowledge this is the best performance in relevant literatures that has been reported, with reduced RMSE by at least 9.5%.
</details>
<details>
<summary>摘要</summary>
预测下肢运动意图是控制外骨骼机器人和人工肢的关键。表面电 MYography (sEMG) 在最近几年来引起了越来越多的关注，因为它可以在实际运动之前预测人体运动意图。然而，人体 JOINT 轨迹的预测性能仍然是一个挑战，这是因为人体之间和个体之间存在差异。前者是由生物学特征（如身高和体重）和个人偏好的步态所致，而后者是由不规则的肌肉活动所引起的。本文提出了一种将两种步征学习策略 integrate 到模型中，以减少预测人体 knee Joint 轨迹的挑战。首先，我们决定将 knee Joint 角度分解成运动模式和振荡强度两个部分。前者在各个个体中表现出低变异性，而后者则表现出高变异性。通过分解这两个部分，我们可以通过不同的网络实体学习两者。这种方法可以捕捉到各个个体的共同和个性化步态特征。其次，我们从步征征ycle中提取了肌肉主动活动面。这些面用于过滤 raw sEMG 中不相关于步行的组分，并提供辅助指导以捕捉更多的步行特征。实验结果表明，我们的模型可以预测 knee Joint 角度的平均根据 Mean Square Error (RMSE) 为 3.03（0.49）度，并在50毫秒前预测。根据我们所知，这是相关文献中最佳的性能，相比前一个最佳性能减少了至少9.5%。
</details></li>
</ul>
<hr>
<h2 id="Federated-Distributionally-Robust-Optimization-with-Non-Convex-Objectives-Algorithm-and-Analysis"><a href="#Federated-Distributionally-Robust-Optimization-with-Non-Convex-Objectives-Algorithm-and-Analysis" class="headerlink" title="Federated Distributionally Robust Optimization with Non-Convex Objectives: Algorithm and Analysis"></a>Federated Distributionally Robust Optimization with Non-Convex Objectives: Algorithm and Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14364">http://arxiv.org/abs/2307.14364</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Jiao, Kai Yang, Dongjin Song</li>
<li>for: 解决分布式环境中 asynchronous updating 问题，以及如何有效地利用 prior distribution 和适度地调整 robustness 水平。</li>
<li>methods: 提出了 asynchronous distributed algorithm ASPIRE algorithm with EASE method，并开发了新的 uncertainty set - constrained D-norm uncertainty set，以便有效地利用 prior distribution 和控制 robustness 水平。</li>
<li>results: 理论分析表明提出的算法可靠地 converge，并且 iteration complexity 也得到了分析。 empirical studies 表明该方法可以快速 converge，对数据不同性和 malicious attacks 具有抗锋性，并且可以控制 robustness 水平和性能之间的负荷。<details>
<summary>Abstract</summary>
Distributionally Robust Optimization (DRO), which aims to find an optimal decision that minimizes the worst case cost over the ambiguity set of probability distribution, has been widely applied in diverse applications, e.g., network behavior analysis, risk management, etc. However, existing DRO techniques face three key challenges: 1) how to deal with the asynchronous updating in a distributed environment; 2) how to leverage the prior distribution effectively; 3) how to properly adjust the degree of robustness according to different scenarios. To this end, we propose an asynchronous distributed algorithm, named Asynchronous Single-looP alternatIve gRadient projEction (ASPIRE) algorithm with the itErative Active SEt method (EASE) to tackle the federated distributionally robust optimization (FDRO) problem. Furthermore, a new uncertainty set, i.e., constrained D-norm uncertainty set, is developed to effectively leverage the prior distribution and flexibly control the degree of robustness. Finally, our theoretical analysis elucidates that the proposed algorithm is guaranteed to converge and the iteration complexity is also analyzed. Extensive empirical studies on real-world datasets demonstrate that the proposed method can not only achieve fast convergence, and remain robust against data heterogeneity as well as malicious attacks, but also tradeoff robustness with performance.
</details>
<details>
<summary>摘要</summary>
Distributionally Robust Optimization (DRO)，targeting at finding an optimal decision that minimizes the worst-case cost over the ambiguity set of probability distribution, has been widely applied in various fields, such as network behavior analysis and risk management. However, existing DRO techniques face three key challenges:1. How to deal with asynchronous updating in a distributed environment;2. How to effectively leverage the prior distribution;3. How to properly adjust the degree of robustness according to different scenarios.To address these challenges, we propose an asynchronous distributed algorithm, named Asynchronous Single-loop Alternating Gradient Projection (ASPIRE) algorithm with the Iterative Active Set method (EASE) to solve the Federated Distributionally Robust Optimization (FDRO) problem. Furthermore, a new uncertainty set, i.e., constrained D-norm uncertainty set, is developed to effectively leverage the prior distribution and flexibly control the degree of robustness.Our theoretical analysis shows that the proposed algorithm is guaranteed to converge, and the iteration complexity is also analyzed. Extensive empirical studies on real-world datasets demonstrate that the proposed method can not only achieve fast convergence, remain robust against data heterogeneity as well as malicious attacks, but also trade off robustness with performance.
</details></li>
</ul>
<hr>
<h2 id="Blockchain-based-Optimized-Client-Selection-and-Privacy-Preserved-Framework-for-Federated-Learning"><a href="#Blockchain-based-Optimized-Client-Selection-and-Privacy-Preserved-Framework-for-Federated-Learning" class="headerlink" title="Blockchain-based Optimized Client Selection and Privacy Preserved Framework for Federated Learning"></a>Blockchain-based Optimized Client Selection and Privacy Preserved Framework for Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04442">http://arxiv.org/abs/2308.04442</a></li>
<li>repo_url: None</li>
<li>paper_authors: Attia Qammar, Abdenacer Naouri, Jianguo Ding, Huansheng Ning</li>
<li>for: 这个研究旨在提出一个基于区块链的优化客户端选择和隐私保证的联边学习框架，以解决传统联边学习结构中的单点失灵攻击和随机选择客户端对模型训练的影响。</li>
<li>methods: 我们提出了三种智能合约：1）客户端注册合约、2）前向拍卖合约来选择优化客户端进行联边学习模型训练、3）支付和赔偿合约。另外，我们还实现了完全几何加密（CKKS）方法，以保证在传输本地模型更新时，资料的隐私不会被泄露。</li>
<li>results: 我们在 benchmark 数据集上评估了我们的提案，并与现有的研究进行比较。结果显示，我们的方法可以实现高精度率和隐私保证的联边学习框架，并且具有分散的自然 caracteristics。<details>
<summary>Abstract</summary>
Federated learning is a distributed mechanism that trained large-scale neural network models with the participation of multiple clients and data remains on their devices, only sharing the local model updates. With this feature, federated learning is considered a secure solution for data privacy issues. However, the typical FL structure relies on the client-server, which leads to the single-point-of-failure (SPoF) attack, and the random selection of clients for model training compromised the model accuracy. Furthermore, adversaries try for inference attacks i.e., attack on privacy leads to gradient leakage attacks. We proposed the blockchain-based optimized client selection and privacy-preserved framework in this context. We designed the three kinds of smart contracts such as 1) registration of clients 2) forward bidding to select optimized clients for FL model training 3) payment settlement and reward smart contracts. Moreover, fully homomorphic encryption with Cheon, Kim, Kim, and Song (CKKS) method is implemented before transmitting the local model updates to the server. Finally, we evaluated our proposed method on the benchmark dataset and compared it with state-of-the-art studies. Consequently, we achieved a higher accuracy rate and privacy-preserved FL framework with decentralized nature.
</details>
<details>
<summary>摘要</summary>
federated learning 是一种分布式机制，通过多个客户端参与训练大规模神经网络模型，保留数据在客户端上，只将本地模型更新共享。由于这种特点， federated learning 被视为一种保障数据隐私的解决方案。然而， Typical FL 结构依赖于客户端-服务器模型，导致单点失败攻击（SPoF）和随机选择客户端进行模型训练，从而影响模型精度。此外，敌方会尝试进行推理攻击，即袭击隐私导致梯度泄露攻击。我们在这种情况下提出了基于区块链的优化客户端选择和隐私保护框架。我们设计了三种种智能合约，包括1）客户端注册 2）向服务器进行前置拍卖选择优化客户端进行 FL 模型训练 3）支付和奖励智能合约。此外，我们实现了使用 Cheon、Kim、Kim 和 Song（CKKS）方法的完全同质加密，以便在向服务器传输本地模型更新之前对其进行加密。最后，我们对标准数据集进行评估，并与现有研究进行比较。因此，我们实现了高精度率和隐私保护的 FL 框架，并具有分布式的自然Characteristics。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-enhanced-Neuro-Symbolic-AI-for-Cybersecurity-and-Privacy"><a href="#Knowledge-enhanced-Neuro-Symbolic-AI-for-Cybersecurity-and-Privacy" class="headerlink" title="Knowledge-enhanced Neuro-Symbolic AI for Cybersecurity and Privacy"></a>Knowledge-enhanced Neuro-Symbolic AI for Cybersecurity and Privacy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02031">http://arxiv.org/abs/2308.02031</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aritran Piplai, Anantaa Kotal, Seyedreza Mohseni, Manas Gaur, Sudip Mittal, Anupam Joshi</li>
<li>for: 该论文旨在探讨如何使用神经网络和符号知识图来提高人类可理解性和安全性在人工智能系统中。</li>
<li>methods: 该论文使用了神经网络和符号知识图的组合方法，以提高对复杂数据空间的探索和学习，同时保持可理解性和安全性。</li>
<li>results: 该论文表明，通过神经网络和符号知识图的组合，可以在Cybersecurity和隐私等高度需要人工智能可解释性的领域中提高AI系统的准确性和安全性。<details>
<summary>Abstract</summary>
Neuro-Symbolic Artificial Intelligence (AI) is an emerging and quickly advancing field that combines the subsymbolic strengths of (deep) neural networks and explicit, symbolic knowledge contained in knowledge graphs to enhance explainability and safety in AI systems. This approach addresses a key criticism of current generation systems, namely their inability to generate human-understandable explanations for their outcomes and ensure safe behaviors, especially in scenarios with \textit{unknown unknowns} (e.g. cybersecurity, privacy). The integration of neural networks, which excel at exploring complex data spaces, and symbolic knowledge graphs, which represent domain knowledge, allows AI systems to reason, learn, and generalize in a manner understandable to experts. This article describes how applications in cybersecurity and privacy, two most demanding domains in terms of the need for AI to be explainable while being highly accurate in complex environments, can benefit from Neuro-Symbolic AI.
</details>
<details>
<summary>摘要</summary>
neural network 和 symbolic knowledge graph 的结合，即 Neuro-Symbolic AI，是一个快速发展的领域，它可以提高 AI 系统的解释性和安全性。这种方法可以解决现有系统的一个批评，即无法生成人类理解的解释，特别是在“未知未知”（如隐私、安全）的场景下。 neural network 可以很好地探索复杂数据空间，而 symbolic knowledge graph 可以表示领域知识，这使得 AI 系统可以由专家理解的方式进行推理、学习和泛化。本文介绍了如何通过 Neuro-Symbolic AI 应用于隐私和安全领域，这两个领域对 AI 系统的解释性和高精度性有特别高的需求。
</details></li>
</ul>
<hr>
<h2 id="Counterfactual-Explanation-Policies-in-RL"><a href="#Counterfactual-Explanation-Policies-in-RL" class="headerlink" title="Counterfactual Explanation Policies in RL"></a>Counterfactual Explanation Policies in RL</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13192">http://arxiv.org/abs/2307.13192</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shripad V. Deshmukh, Srivatsan R, Supriti Vijay, Jayakumar Subramanian, Chirag Agarwal</li>
<li>for: 这个论文的目的是解释RL策略的可解释性，并提供一种基于对比的策略分析方法。</li>
<li>methods: 该论文使用了对比方法，将策略视为一种可变的对比，并通过对比来分析策略的改进。</li>
<li>results: 实验结果表明，COUNTERPOL可以生成有用的对比解释，帮助分析RL策略的性能改进。 论文在五个不同的RL环境中进行了广泛的实验，并证明了对比解释的实用性。<details>
<summary>Abstract</summary>
As Reinforcement Learning (RL) agents are increasingly employed in diverse decision-making problems using reward preferences, it becomes important to ensure that policies learned by these frameworks in mapping observations to a probability distribution of the possible actions are explainable. However, there is little to no work in the systematic understanding of these complex policies in a contrastive manner, i.e., what minimal changes to the policy would improve/worsen its performance to a desired level. In this work, we present COUNTERPOL, the first framework to analyze RL policies using counterfactual explanations in the form of minimal changes to the policy that lead to the desired outcome. We do so by incorporating counterfactuals in supervised learning in RL with the target outcome regulated using desired return. We establish a theoretical connection between Counterpol and widely used trust region-based policy optimization methods in RL. Extensive empirical analysis shows the efficacy of COUNTERPOL in generating explanations for (un)learning skills while keeping close to the original policy. Our results on five different RL environments with diverse state and action spaces demonstrate the utility of counterfactual explanations, paving the way for new frontiers in designing and developing counterfactual policies.
</details>
<details>
<summary>摘要</summary>
为了使机器学习（RL）代理人在各种决策问题中使用奖励偏好，正在使得RL政策的可追踪性变得越来越重要。然而，现有的工作几乎没有系统地理解这些复杂的政策，尤其是在对比方式下进行分析。在这项工作中，我们提出了Counterpol，第一个使用对比解释来分析RL政策的框架。我们通过在RL中 incorporating counterfactuals into supervised learning，使得政策更容易理解。我们还证明了Counterpol与常用的信任区间基本策略优化方法在RL中的理论联系。我们的实验结果表明，Counterpol可以快速生成对应于不同奖励目标的解释，同时保持着原始政策的相似性。我们在五种不同的RL环境中进行了extensive empirical analysis，并证明了对于不同的状态和动作空间，Counterpol可以提供有用的对比解释，开启了新的前ier征学习和开发对比政策的可能性。
</details></li>
</ul>
<hr>
<h2 id="Digital-Emotion-Regulation-on-Social-Media"><a href="#Digital-Emotion-Regulation-on-Social-Media" class="headerlink" title="Digital Emotion Regulation on Social Media"></a>Digital Emotion Regulation on Social Media</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13187">http://arxiv.org/abs/2307.13187</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akriti Verma, Shama Islam, Valeh Moghaddam, Adnan Anwar</li>
<li>for: 这篇论文主要是关于如何利用数字技术来调节情绪 state，以支持伦理的技术设计、开发和部署。</li>
<li>methods: 论文使用了社交媒体应用程序的不同特性和功能来描述不同阶段的情绪调节过程。</li>
<li>results: 研究发现了不同社交媒体应用程序在不同阶段的情绪调节过程中的应用，以及最近的研究对社交媒体应用程序的情绪调节 intervenciones。<details>
<summary>Abstract</summary>
Emotion regulation is the process of consciously altering one's affective state, that is the underlying emotional state such as happiness, confidence, guilt, anger etc. The ability to effectively regulate emotions is necessary for functioning efficiently in everyday life. Today, the pervasiveness of digital technology is being purposefully employed to modify our affective states, a process known as digital emotion regulation. Understanding digital emotion regulation can help support the rise of ethical technology design, development, and deployment. This article presents an overview of digital emotion regulation in social media applications, as well as a synthesis of recent research on emotion regulation interventions for social media. We share our findings from analysing state-of-the-art literature on how different social media applications are utilised at different stages in the process of emotion regulation.
</details>
<details>
<summary>摘要</summary>
情绪调节是指意识地改变自己的情绪状态，包括内在的情绪状态如快乐、自信、罪愧、愤怒等。有效地调节情绪是日常生活中必要的。随着数字技术的普及，人们正在意识地利用这些技术来修改自己的情绪状态，这被称为数字情绪调节。了解数字情绪调节可以帮助促进伦理技术的设计、开发和投入。本文提供了社交媒体应用程序中数字情绪调节的概述，以及最新的研究成果表明在社交媒体上进行情绪调节的干预措施。我们分析了最新的文献，描述了不同的社交媒体应用程序在不同阶段的情绪调节过程中的使用。
</details></li>
</ul>
<hr>
<h2 id="Opinion-Mining-Using-Population-tuned-Generative-Language-Models"><a href="#Opinion-Mining-Using-Population-tuned-Generative-Language-Models" class="headerlink" title="Opinion Mining Using Population-tuned Generative Language Models"></a>Opinion Mining Using Population-tuned Generative Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13173">http://arxiv.org/abs/2307.13173</a></li>
<li>repo_url: None</li>
<li>paper_authors: Allmin Susaiyah, Abhinay Pandya, Aki Härmä</li>
<li>for: 用于挖掘文本收集中的意见</li>
<li>methods: 使用生成语言模型，通过特定的方法和数据集进行训练</li>
<li>results: 可以学习和传递意见到semantic类，保持极性分布<details>
<summary>Abstract</summary>
We present a novel method for mining opinions from text collections using generative language models trained on data collected from different populations. We describe the basic definitions, methodology and a generic algorithm for opinion insight mining. We demonstrate the performance of our method in an experiment where a pre-trained generative model is fine-tuned using specifically tailored content with unnatural and fully annotated opinions. We show that our approach can learn and transfer the opinions to the semantic classes while maintaining the proportion of polarisation. Finally, we demonstrate the usage of an insight mining system to scale up the discovery of opinion insights from a real text corpus.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法，用于从文本集中挖掘意见使用生成语言模型，这些模型在不同的人口数据上进行训练。我们描述了基本定义、方法和一个通用的算法 для意见洞察挖掘。我们在实验中使用预训练的生成模型进行微调，使用特定的内容和假备注意意见。我们显示了我们的方法可以学习并传递意见到Semantic类中，同时保持极性分布。最后，我们示出了一个洞察挖掘系统可以扩大从真实文本集中发现意见洞察的能力。Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Investigating-the-Robustness-of-Sequential-Recommender-Systems-Against-Training-Data-Perturbations-an-Empirical-Study"><a href="#Investigating-the-Robustness-of-Sequential-Recommender-Systems-Against-Training-Data-Perturbations-an-Empirical-Study" class="headerlink" title="Investigating the Robustness of Sequential Recommender Systems Against Training Data Perturbations: an Empirical Study"></a>Investigating the Robustness of Sequential Recommender Systems Against Training Data Perturbations: an Empirical Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13165">http://arxiv.org/abs/2307.13165</a></li>
<li>repo_url: None</li>
<li>paper_authors: Filippo Betello, Federico Siciliano, Pushkar Mishra, Fabrizio Silvestri</li>
<li>for: 这个论文旨在研究Sequential Recommender Systems（SRSs）在训练数据中的稳定性，具体来说是研究在 temporally ordered sequence 中移除items的影响。</li>
<li>methods: 这个论文使用了两种不同的SRS模型，在多个数据集上进行了评估，使用了Normalized Discounted Cumulative Gain（NDCG）和Rank Sensitivity List metric来衡量性能。</li>
<li>results: 研究发现，在序列中移除items的末端位置会导致性能下降，NDCG下降可达60%，而从开头或中间位置移除items没有显著影响。这些发现表明考虑训练数据中items的位置是重要的，这将有助于设计更加稳定的SRSs。<details>
<summary>Abstract</summary>
Sequential Recommender Systems (SRSs) have been widely used to model user behavior over time, but their robustness in the face of perturbations to training data is a critical issue. In this paper, we conduct an empirical study to investigate the effects of removing items at different positions within a temporally ordered sequence. We evaluate two different SRS models on multiple datasets, measuring their performance using Normalized Discounted Cumulative Gain (NDCG) and Rank Sensitivity List metrics. Our results demonstrate that removing items at the end of the sequence significantly impacts performance, with NDCG decreasing up to 60\%, while removing items from the beginning or middle has no significant effect. These findings highlight the importance of considering the position of the perturbed items in the training data and shall inform the design of more robust SRSs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Improving-Primary-Healthcare-Workflow-Using-Extreme-Summarization-of-Scientific-Literature-Based-on-Generative-AI"><a href="#Improving-Primary-Healthcare-Workflow-Using-Extreme-Summarization-of-Scientific-Literature-Based-on-Generative-AI" class="headerlink" title="Improving Primary Healthcare Workflow Using Extreme Summarization of Scientific Literature Based on Generative AI"></a>Improving Primary Healthcare Workflow Using Extreme Summarization of Scientific Literature Based on Generative AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15715">http://arxiv.org/abs/2307.15715</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gregor Stiglic, Leon Kopitar, Lucija Gosak, Primoz Kocbek, Zhe He, Prithwish Chakraborty, Pablo Meyer, Jiang Bian<br>for: 这个研究的目的是探究生成人工智能在减轻医疗专业人员的认知压力方面的潜力，以便更好地帮助他们保持最新的科学文献知识。methods: 这个研究使用了生成型人工智能技术，基于大规模语言模型，对科学报告摘要进行概括。results: 研究结果表明，使用生成型人工智能 для文献综述是高效的，可以减少医疗专业人员查找科学文献所需的时间。然而，研究还发现，当全文摘要不可用时，EXTRACTED知识的准确性会下降。这种破坏性技术有potential可以大幅减少医疗专业人员保持最新科学文献知识的时间，但是进一步的发展可能需要帮助他们更好地理解摘要中的知识。<details>
<summary>Abstract</summary>
Primary care professionals struggle to keep up to date with the latest scientific literature critical in guiding evidence-based practice related to their daily work. To help solve the above-mentioned problem, we employed generative artificial intelligence techniques based on large-scale language models to summarize abstracts of scientific papers. Our objective is to investigate the potential of generative artificial intelligence in diminishing the cognitive load experienced by practitioners, thus exploring its ability to alleviate mental effort and burden. The study participants were provided with two use cases related to preventive care and behavior change, simulating a search for new scientific literature. The study included 113 university students from Slovenia and the United States randomized into three distinct study groups. The first group was assigned to the full abstracts. The second group was assigned to the short abstracts generated by AI. The third group had the option to select a full abstract in addition to the AI-generated short summary. Each use case study included ten retrieved abstracts. Our research demonstrates that the use of generative AI for literature review is efficient and effective. The time needed to answer questions related to the content of abstracts was significantly lower in groups two and three compared to the first group using full abstracts. The results, however, also show significantly lower accuracy in extracted knowledge in cases where full abstract was not available. Such a disruptive technology could significantly reduce the time required for healthcare professionals to keep up with the most recent scientific literature; nevertheless, further developments are needed to help them comprehend the knowledge accurately.
</details>
<details>
<summary>摘要</summary>
We conducted a study with 113 university students from Slovenia and the United States, randomly assigned to three groups. The first group was given full abstracts, the second group was given AI-generated short summaries, and the third group had the option to choose a full abstract or the AI-generated summary. Each use case study included ten retrieved abstracts.Our findings show that using generative AI for literature review is efficient and effective. The time needed to answer questions related to the content of abstracts was significantly lower in groups two and three compared to the first group using full abstracts. However, the results also showed that accuracy in extracted knowledge was significantly lower when full abstracts were not available.This disruptive technology has the potential to significantly reduce the time required for healthcare professionals to keep up with the most recent scientific literature. However, further developments are needed to help them comprehend the knowledge accurately.
</details></li>
</ul>
<hr>
<h2 id="Why-Don’t-You-Clean-Your-Glasses-Perception-Attacks-with-Dynamic-Optical-Perturbations"><a href="#Why-Don’t-You-Clean-Your-Glasses-Perception-Attacks-with-Dynamic-Optical-Perturbations" class="headerlink" title="Why Don’t You Clean Your Glasses? Perception Attacks with Dynamic Optical Perturbations"></a>Why Don’t You Clean Your Glasses? Perception Attacks with Dynamic Optical Perturbations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13131">http://arxiv.org/abs/2307.13131</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi Han, Matthew Chan, Eric Wengrowski, Zhuohuan Li, Nils Ole Tippenhauer, Mani Srivastava, Saman Zonouz, Luis Garcia</li>
<li>for: 这篇论文的目的是研究攻击自适应系统中的机器学习模型，以及这些模型在物理世界中的攻击。</li>
<li>methods: 这篇论文使用了一种名为“EvilEye”的人在中渠攻击，利用透明屏幕生成动态物理攻击示例。这种攻击利用了相机的光学特性，在不同的照明条件下引起识别错误。</li>
<li>results: 实验表明，EvilEye生成的攻击示例在环境噪声和自适应系统的动态变化下表现得非常稳定，可以高效绕过当前物理世界攻击检测框架。此外，EvilEye可以针对不同的物体实现高度的攻击成功率。<details>
<summary>Abstract</summary>
Camera-based autonomous systems that emulate human perception are increasingly being integrated into safety-critical platforms. Consequently, an established body of literature has emerged that explores adversarial attacks targeting the underlying machine learning models. Adapting adversarial attacks to the physical world is desirable for the attacker, as this removes the need to compromise digital systems. However, the real world poses challenges related to the "survivability" of adversarial manipulations given environmental noise in perception pipelines and the dynamicity of autonomous systems. In this paper, we take a sensor-first approach. We present EvilEye, a man-in-the-middle perception attack that leverages transparent displays to generate dynamic physical adversarial examples. EvilEye exploits the camera's optics to induce misclassifications under a variety of illumination conditions. To generate dynamic perturbations, we formalize the projection of a digital attack into the physical domain by modeling the transformation function of the captured image through the optical pipeline. Our extensive experiments show that EvilEye's generated adversarial perturbations are much more robust across varying environmental light conditions relative to existing physical perturbation frameworks, achieving a high attack success rate (ASR) while bypassing state-of-the-art physical adversarial detection frameworks. We demonstrate that the dynamic nature of EvilEye enables attackers to adapt adversarial examples across a variety of objects with a significantly higher ASR compared to state-of-the-art physical world attack frameworks. Finally, we discuss mitigation strategies against the EvilEye attack.
</details>
<details>
<summary>摘要</summary>
摄像头基于自动化系统，模拟人类感知，在安全关键平台中得到普遍应用。因此，一个已经形成的文献出现，探讨机器学习模型的攻击。对于攻击者来说，在物理世界中进行攻击是有利的，因为这 eliminates the need to compromise digital systems。然而，物理世界具有对攻击修改的"生存性"问题，即环境噪声和自动化系统的动态性。在这篇论文中，我们采用了感知先采集的方法。我们介绍了一种基于透明显示器的人在中间攻击，称为EvilEye。EvilEye利用摄像头的光学来导致分类错误，并在不同的照明条件下实现高度的攻击成功率（ASR），并 circumvent state-of-the-art physical adversarial detection frameworks。我们的广泛实验表明，EvilEye生成的physical perturbations是对环境光度条件的变化具有更高的Robustness，相比之下，现有的物理扰动框架。我们还证明了EvilEye的动态性可以在不同的物体上实现更高的ASR，比之前的物理世界攻击框架。最后，我们讨论了对EvilEye攻击的防御策略。
</details></li>
</ul>
<hr>
<h2 id="A-Hybrid-Machine-Learning-Model-for-Classifying-Gene-Mutations-in-Cancer-using-LSTM-BiLSTM-CNN-GRU-and-GloVe"><a href="#A-Hybrid-Machine-Learning-Model-for-Classifying-Gene-Mutations-in-Cancer-using-LSTM-BiLSTM-CNN-GRU-and-GloVe" class="headerlink" title="A Hybrid Machine Learning Model for Classifying Gene Mutations in Cancer using LSTM, BiLSTM, CNN, GRU, and GloVe"></a>A Hybrid Machine Learning Model for Classifying Gene Mutations in Cancer using LSTM, BiLSTM, CNN, GRU, and GloVe</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14361">http://arxiv.org/abs/2307.14361</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanad Aburass, Osama Dorgham, Jamil Al Shaqsi</li>
<li>for: 这种研究是为了使用Kaggle的个性化医疗：再定义癌症治疗数据集来分类基因突变。</li>
<li>methods: 这个模型使用了LSTM、BiLSTM、CNN、GRU和GloVe ensemble模型来实现这一目标。</li>
<li>results: 这个模型的准确率、精度、准确率、F1分数和平均平方误差都高于所有其他模型，并且需要更少的训练时间，因此是性能和效率的完美结合。<details>
<summary>Abstract</summary>
This study presents an ensemble model combining LSTM, BiLSTM, CNN, GRU, and GloVe to classify gene mutations using Kaggle's Personalized Medicine: Redefining Cancer Treatment dataset. The results were compared against well-known transformers like as BERT, Electra, Roberta, XLNet, Distilbert, and their LSTM ensembles. Our model outperformed all other models in terms of accuracy, precision, recall, F1 score, and Mean Squared Error. Surprisingly, it also needed less training time, resulting in a perfect combination of performance and efficiency. This study demonstrates the utility of ensemble models for difficult tasks such as gene mutation classification.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "LSTM" and "BiLSTM" were translated as "长ShortTermMemory" (CHángshòu Dàimengyī) and "双向LongShortTermMemory" (Shuāngxiàng CHángshòu Dàimengyī) respectively.* "CNN" was translated as "卷积神经网络" (Jiànpán Jīngxīn Wǎngwǎng)* "GRU" was translated as "幂等循环神经网络" (Jìdé Xiàngxīng Jīngxīn Wǎngwǎng)* "GloVe" was translated as "全球最佳 embeddings" (Quánqīu Zuìjiā Embeddings)* "BERT" was translated as "Bidirectional Encoder Representations from Transformers" (Bìxiàngdìng Jīngxīn Fāngyìng)* "Electra" was translated as "Electra: A Method for Estimating the Representation of a Set of Words" (Électra: A Method for Estimating the Representation of a Set of Words)* "Roberta" was translated as "Roberta: A Simple and Efficient Transformer for Language Understanding" (Roberta: A Simple and Efficient Transformer for Language Understanding)* "XLNet" was translated as "XLNet: Generalized Autoencoders for Language Understanding" (XLNet: Generalized Autoencoders for Language Understanding)* "Distilbert" was translated as "DistilBERT: Distilled BERT for Efficient and Compact Language Models" (DistilBERT: Distilled BERT for Efficient and Compact Language Models)* "ensemble" was translated as "组合" (Zǔzhōng)Please note that the translation is in Simplified Chinese, and the translation may vary depending on the context and the specific dialect.
</details></li>
</ul>
<hr>
<h2 id="Deep-Bradley-Terry-Rating-Quantifying-Properties-from-Comparisons"><a href="#Deep-Bradley-Terry-Rating-Quantifying-Properties-from-Comparisons" class="headerlink" title="Deep Bradley-Terry Rating: Quantifying Properties from Comparisons"></a>Deep Bradley-Terry Rating: Quantifying Properties from Comparisons</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13709">http://arxiv.org/abs/2307.13709</a></li>
<li>repo_url: None</li>
<li>paper_authors: Satoru Fujii</li>
<li>for: 该论文旨在解决实际世界中不直接可观察的许多属性的难题，通过使用grade human scores作为目标标签进行训练。</li>
<li>methods: 该论文提出了一种名为深度布莱德利-泰勒评分（DBTR）的新机器学习框架，该框架将布莱德利-泰勒模型 integrates into neural network structure，并在不平等环境下进行扩展。</li>
<li>results: 经过实验分析，DBTR成功地学习和估计所需的属性。<details>
<summary>Abstract</summary>
Many properties in the real world can't be directly observed, making them difficult to learn. To deal with this challenging problem, prior works have primarily focused on estimating those properties by using graded human scores as the target label in the training. Meanwhile, rating algorithms based on the Bradley-Terry model are extensively studied to evaluate the competitiveness of players based on their match history. In this paper, we introduce the Deep Bradley-Terry Rating (DBTR), a novel machine learning framework designed to quantify and evaluate properties of unknown items. Our method seamlessly integrates the Bradley-Terry model into the neural network structure. Moreover, we generalize this architecture further to asymmetric environments with unfairness, a condition more commonly encountered in real-world settings. Through experimental analysis, we demonstrate that DBTR successfully learns to quantify and estimate desired properties.
</details>
<details>
<summary>摘要</summary>
很多现实世界中的属性是直接观察不到的，使得学习变得困难。以前的工作主要是通过使用排名为目标标签进行训练来估算这些属性。而BRADLEY-TERRY模型的评分算法在评估玩家的竞技水平上广泛研究。在这篇论文中，我们介绍了深度BRADLEY-TERRY评分（DBTR），一种新的机器学习框架，用于评估和评价未知的物品属性。我们将BRADLEY-TERRY模型集成到神经网络结构中，并将其扩展到不平等环境下，更加符合实际世界中的情况。我们通过实验分析，证明DBTR可以成功地评估和估算所需的属性。
</details></li>
</ul>
<hr>
<h2 id="Getting-pwn’d-by-AI-Penetration-Testing-with-Large-Language-Models"><a href="#Getting-pwn’d-by-AI-Penetration-Testing-with-Large-Language-Models" class="headerlink" title="Getting pwn’d by AI: Penetration Testing with Large Language Models"></a>Getting pwn’d by AI: Penetration Testing with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00121">http://arxiv.org/abs/2308.00121</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ipa-lab/hackingBuddyGPT">https://github.com/ipa-lab/hackingBuddyGPT</a></li>
<li>paper_authors: Andreas Happe, Jürgen Cito</li>
<li>for: 该论文探讨了使用大语言模型（如GPT3.5）来补充安全测试人员，以增强安全测试的效率和质量。</li>
<li>methods: 论文采用了高级语言模型进行具体的任务规划和低级漏洞搜索两种使用场景，并实现了在虚拟机上实现了封闭反馈循环，让LLM分析机器状态并提供攻击方式。</li>
<li>results: 论文初步结果显示，使用大语言模型可以帮助提高安全测试的效率和质量，并且可以帮助找到一些潜在的漏洞。但是，论文还需要进一步的改进和优化。<details>
<summary>Abstract</summary>
The field of software security testing, more specifically penetration testing, is an activity that requires high levels of expertise and involves many manual testing and analysis steps. This paper explores the potential usage of large-language models, such as GPT3.5, to augment penetration testers with AI sparring partners. We explore the feasibility of supplementing penetration testers with AI models for two distinct use cases: high-level task planning for security testing assignments and low-level vulnerability hunting within a vulnerable virtual machine. For the latter, we implemented a closed-feedback loop between LLM-generated low-level actions with a vulnerable virtual machine (connected through SSH) and allowed the LLM to analyze the machine state for vulnerabilities and suggest concrete attack vectors which were automatically executed within the virtual machine. We discuss promising initial results, detail avenues for improvement, and close deliberating on the ethics of providing AI-based sparring partners.
</details>
<details>
<summary>摘要</summary>
field 软件安全测试，具体来说是渗透测试，需要高水平的专业知识和多种手动测试和分析步骤。这篇论文探讨使用大型自然语言模型，如GPT3.5，来补充渗透测试员的人工智能对手。我们探讨在两个不同的用例中使用AI模型：一是高级任务规划 для安全测试任务，二是低级漏洞搜寻在易于攻击的虚拟机中。对于后一个，我们实现了关闭反馈循环，使用SSH连接到易于攻击的虚拟机，让LLM分析机器状态以找到漏洞并建议具体的攻击方式，然后自动在虚拟机中执行。我们讨论了初步的结果，详细描述改进的方向，并关于提供AI基本对手的伦理问题。
</details></li>
</ul>
<hr>
<h2 id="An-Explainable-Geometric-Weighted-Graph-Attention-Network-for-Identifying-Functional-Networks-Associated-with-Gait-Impairment"><a href="#An-Explainable-Geometric-Weighted-Graph-Attention-Network-for-Identifying-Functional-Networks-Associated-with-Gait-Impairment" class="headerlink" title="An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment"></a>An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13108">http://arxiv.org/abs/2307.13108</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/favour-nerrise/xgw-gat">https://github.com/favour-nerrise/xgw-gat</a></li>
<li>paper_authors: Favour Nerrise, Qingyu Zhao, Kathleen L. Poston, Kilian M. Pohl, Ehsan Adeli</li>
<li>for: 这个研究的目的是为了更好地理解parkinson病的motor进程，以开发更有效和个性化的治疗方法。</li>
<li>methods: 这个研究使用了一种可解释的、几何的、weighted-graph注意力神经网络（xGW-GAT），用于预测parkinson病患者的跑步困难程度。</li>
<li>results: xGW-GAT模型可以从resting-state功能MRI数据中提取出跑步困难相关的功能连接图，并且可以提供可解释的功能子网络，对于parkinson病患者的motor困难提供了解释。<details>
<summary>Abstract</summary>
One of the hallmark symptoms of Parkinson's Disease (PD) is the progressive loss of postural reflexes, which eventually leads to gait difficulties and balance problems. Identifying disruptions in brain function associated with gait impairment could be crucial in better understanding PD motor progression, thus advancing the development of more effective and personalized therapeutics. In this work, we present an explainable, geometric, weighted-graph attention neural network (xGW-GAT) to identify functional networks predictive of the progression of gait difficulties in individuals with PD. xGW-GAT predicts the multi-class gait impairment on the MDS Unified PD Rating Scale (MDS-UPDRS). Our computational- and data-efficient model represents functional connectomes as symmetric positive definite (SPD) matrices on a Riemannian manifold to explicitly encode pairwise interactions of entire connectomes, based on which we learn an attention mask yielding individual- and group-level explainability. Applied to our resting-state functional MRI (rs-fMRI) dataset of individuals with PD, xGW-GAT identifies functional connectivity patterns associated with gait impairment in PD and offers interpretable explanations of functional subnetworks associated with motor impairment. Our model successfully outperforms several existing methods while simultaneously revealing clinically-relevant connectivity patterns. The source code is available at https://github.com/favour-nerrise/xGW-GAT .
</details>
<details>
<summary>摘要</summary>
Our model represents functional connectomes as symmetric positive definite (SPD) matrices on a Riemannian manifold to explicitly encode pairwise interactions of entire connectomes. Based on this, we learn an attention mask that yields individual- and group-level explainability. Applied to our resting-state functional MRI (rs-fMRI) dataset of individuals with PD, xGW-GAT identifies functional connectivity patterns associated with gait impairment in PD and provides interpretable explanations of functional subnetworks associated with motor impairment. Our model outperforms several existing methods while providing clinically relevant connectivity patterns. The source code is available at https://github.com/favour-nerrise/xGW-GAT.
</details></li>
</ul>
<hr>
<h2 id="How-to-use-LLMs-for-Text-Analysis"><a href="#How-to-use-LLMs-for-Text-Analysis" class="headerlink" title="How to use LLMs for Text Analysis"></a>How to use LLMs for Text Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13106">http://arxiv.org/abs/2307.13106</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cssmodels/howtousellms">https://github.com/cssmodels/howtousellms</a></li>
<li>paper_authors: Petter Törnberg</li>
<li>for: 这篇论文是用于介绍大语言模型（LLM）在社会科学中的应用。</li>
<li>methods: 论文使用Python语言和API进行文本分析，包括文本标注和分类、情感分析和批判话语分析等多种任务。</li>
<li>results: 论文通过使用LLM来分析政治文本，并成功地超越了现有的状态。<details>
<summary>Abstract</summary>
This guide introduces Large Language Models (LLM) as a highly versatile text analysis method within the social sciences. As LLMs are easy-to-use, cheap, fast, and applicable on a broad range of text analysis tasks, ranging from text annotation and classification to sentiment analysis and critical discourse analysis, many scholars believe that LLMs will transform how we do text analysis. This how-to guide is aimed at students and researchers with limited programming experience, and offers a simple introduction to how LLMs can be used for text analysis in your own research project, as well as advice on best practices. We will go through each of the steps of analyzing textual data with LLMs using Python: installing the software, setting up the API, loading the data, developing an analysis prompt, analyzing the text, and validating the results. As an illustrative example, we will use the challenging task of identifying populism in political texts, and show how LLMs move beyond the existing state-of-the-art.
</details>
<details>
<summary>摘要</summary>
这个指南介绍大语言模型（LLM）作为社会科学中高度灵活的文本分析方法。由于LLM是容易使用、便宜、快速并可应用于广泛的文本分析任务，从文本标注和分类到情感分析和批判性文本分析，许多学者认为LLM会改变我们如何进行文本分析。这本引导是向没有programming经验的学生和研究人员的，提供了使用Python来进行文本分析的简单入门，以及最佳实践的建议。我们将通过每个步骤来分析文本数据使用LLM，包括安装软件、设置API、加载数据、开发分析提示、分析文本和验证结果。作为一个示例，我们使用政治文本中的 populism 识别任务，并示出如何使用LLM超越现有的状态。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Example-Based-Control"><a href="#Contrastive-Example-Based-Control" class="headerlink" title="Contrastive Example-Based Control"></a>Contrastive Example-Based Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13101">http://arxiv.org/abs/2307.13101</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/khatch31/laeo">https://github.com/khatch31/laeo</a></li>
<li>paper_authors: Kyle Hatch, Benjamin Eysenbach, Rafael Rafailov, Tianhe Yu, Ruslan Salakhutdinov, Sergey Levine, Chelsea Finn</li>
<li>for: 这篇论文的目的是提出一种基于实例的控制方法，可以在无线务动态环境中学习Q值函数。</li>
<li>methods: 这种方法使用数据驱动的方法，从转移动力和高返回状态中学习一个隐式模型，而不是直接学习奖励函数。</li>
<li>results: 对比基线方法，这种方法在多种状态基于和图像基于的离线控制任务中表现出色，并且在数据集大小增加时显示了更好的稳定性和扩展性。<details>
<summary>Abstract</summary>
While many real-world problems that might benefit from reinforcement learning, these problems rarely fit into the MDP mold: interacting with the environment is often expensive and specifying reward functions is challenging. Motivated by these challenges, prior work has developed data-driven approaches that learn entirely from samples from the transition dynamics and examples of high-return states. These methods typically learn a reward function from high-return states, use that reward function to label the transitions, and then apply an offline RL algorithm to these transitions. While these methods can achieve good results on many tasks, they can be complex, often requiring regularization and temporal difference updates. In this paper, we propose a method for offline, example-based control that learns an implicit model of multi-step transitions, rather than a reward function. We show that this implicit model can represent the Q-values for the example-based control problem. Across a range of state-based and image-based offline control tasks, our method outperforms baselines that use learned reward functions; additional experiments demonstrate improved robustness and scaling with dataset size.
</details>
<details>
<summary>摘要</summary>
虽然许多实际问题可以借助强化学习解决，但这些问题很少遵循MDP模型：与环境交互往往是昂贵的，并且指定奖励函数是困难的。受这些挑战的推动，先前的工作已经开发出了基于数据的方法，这些方法通过从转移动力学中采样而学习，并使用高奖状态的示例来标记转移。这些方法可以在许多任务上达到良好的结果，但它们可能复杂，需要减少和时间差更新。在这篇论文中，我们提出了一种没有奖励函数的离线控制方法，这种方法学习了多步转移的隐藏模型，而不是奖励函数。我们证明这种隐藏模型可以表示离线控制问题中的Q值。在一系列基于状态和图像的离线控制任务上，我们的方法超过了基准值，并且进行了附加的robustness和数据集大小的扩展试验。
</details></li>
</ul>
<hr>
<h2 id="Comparative-Analysis-of-Drug-GPT-and-ChatGPT-LLMs-for-Healthcare-Insights-Evaluating-Accuracy-and-Relevance-in-Patient-and-HCP-Contexts"><a href="#Comparative-Analysis-of-Drug-GPT-and-ChatGPT-LLMs-for-Healthcare-Insights-Evaluating-Accuracy-and-Relevance-in-Patient-and-HCP-Contexts" class="headerlink" title="Comparative Analysis of Drug-GPT and ChatGPT LLMs for Healthcare Insights: Evaluating Accuracy and Relevance in Patient and HCP Contexts"></a>Comparative Analysis of Drug-GPT and ChatGPT LLMs for Healthcare Insights: Evaluating Accuracy and Relevance in Patient and HCP Contexts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16850">http://arxiv.org/abs/2307.16850</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giorgos Lysandrou, Roma English Owen, Kirsty Mursec, Grant Le Brun, Elizabeth A. L. Fairley</li>
<li>for: 这个研究旨在比较三个生成预训练变换器（GPT）解决方案在问答（Q&amp;A） Setting中的表现：Drug-GPT 3、Drug-GPT 4 和 ChatGPT，以医疗应用场景为背景。研究的目的是确定哪一个模型可以在涉及到患有过敏性皮肤炎（AD）患者经验和医疗专业人员（HCP）关于糖尿病讨论中提供最准确和有 relevance 的答案。</li>
<li>methods: 这个研究使用了三个GPT模型：Drug-GPT 3、Drug-GPT 4 和 ChatGPT，通过精心编辑的患者和医疗专业人员社交媒体和讨论区域的数据来支持这三个模型。</li>
<li>results: 研究结果表明，三个模型都能生成有 relevance 和准确的答案，但Drug-GPT 3 和 Drug-GPT 4 通过使用专门编辑的患者和医疗专业人员社交媒体和讨论区域数据，为患者和医疗专业人员提供了更加有target 和深入的报告。ChatGPT 是一个更通用的模型，可以为读者提供高度概括的了解这些主题，但可能缺乏Drug-GPT模型所具备的深度和个人经验。<details>
<summary>Abstract</summary>
This study presents a comparative analysis of three Generative Pre-trained Transformer (GPT) solutions in a question and answer (Q&A) setting: Drug-GPT 3, Drug-GPT 4, and ChatGPT, in the context of healthcare applications. The objective is to determine which model delivers the most accurate and relevant information in response to prompts related to patient experiences with atopic dermatitis (AD) and healthcare professional (HCP) discussions about diabetes. The results demonstrate that while all three models are capable of generating relevant and accurate responses, Drug-GPT 3 and Drug-GPT 4, which are supported by curated datasets of patient and HCP social media and message board posts, provide more targeted and in-depth insights. ChatGPT, a more general-purpose model, generates broader and more general responses, which may be valuable for readers seeking a high-level understanding of the topics but may lack the depth and personal insights found in the answers generated by the specialized Drug-GPT models. This comparative analysis highlights the importance of considering the language model's perspective, depth of knowledge, and currency when evaluating the usefulness of generated information in healthcare applications.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Atopic dermatitis" (AD) is translated as "恶性皮肤炎" (éviation skin rash)* "Healthcare professional" (HCP) is translated as "医疗专业人员" (yījīu zhōngyè rényuè)* "Curated datasets" is translated as "精选数据集" (jīngxuǎn numérique)* "Social media and message board posts" is translated as "社交媒体和讨论版块" (shèjiāo tiēdī yǔ tǎo luó bǎ)* "General-purpose model" is translated as "通用模型" (tōngyòng módeli)* "Specialized models" is translated as "专业模型" (zhuāngyè módeli)
</details></li>
</ul>
<hr>
<h2 id="Making-Metadata-More-FAIR-Using-Large-Language-Models"><a href="#Making-Metadata-More-FAIR-Using-Large-Language-Models" class="headerlink" title="Making Metadata More FAIR Using Large Language Models"></a>Making Metadata More FAIR Using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13085">http://arxiv.org/abs/2307.13085</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sowmya S. Sundaram, Mark A. Musen</li>
<li>for: 这篇论文是为了解决实验数据中的metadata问题，尤其是对于不同的metadata数据进行比较和分组。</li>
<li>methods: 这篇论文使用自然语言处理（NLP）技术，开发了一个名为FAIRMetaText的应用程序，可以比较metadata中的自然语言描述，并提供一个数学性相似度的衡量方法，以便对metadata进行分组或找到相似的替代词。</li>
<li>results: 这篇论文透过对公开 available的研究artifacts进行详细的研究，证明了FAIRMetaText的算法可以大幅提高metadata相关的任务，包括搜寻、分组和替代词等。<details>
<summary>Abstract</summary>
With the global increase in experimental data artifacts, harnessing them in a unified fashion leads to a major stumbling block - bad metadata. To bridge this gap, this work presents a Natural Language Processing (NLP) informed application, called FAIRMetaText, that compares metadata. Specifically, FAIRMetaText analyzes the natural language descriptions of metadata and provides a mathematical similarity measure between two terms. This measure can then be utilized for analyzing varied metadata, by suggesting terms for compliance or grouping similar terms for identification of replaceable terms. The efficacy of the algorithm is presented qualitatively and quantitatively on publicly available research artifacts and demonstrates large gains across metadata related tasks through an in-depth study of a wide variety of Large Language Models (LLMs). This software can drastically reduce the human effort in sifting through various natural language metadata while employing several experimental datasets on the same topic.
</details>
<details>
<summary>摘要</summary>
global 实验数据的增加，将它们集成一起是一个主要障碍 - 坏的metadata。为了bridging这个差距，这个工作提出了一个基于自然语言处理（NLP）的应用程序，called FAIRMetaText，它比较metadata的自然语言描述。具体来说，FAIRMetaText使用自然语言描述来提供两个条件之间的数学相似度测量。这个测量可以用来分析不同的metadata，提供符合性检查或组织相似的条件。这个软件可以帮助大幅提高人工过滤不同主题的自然语言metadata的时间和努力。Here's a breakdown of the translation:* global 实验数据 (global experimental data) becomes 实验数据的增加 (increase in experimental data)* 将它们集成一起 (harnessing them in a unified fashion) becomes 将它们集成一起 (collecting them together)* 坏的metadata (bad metadata) becomes 坏的metadata (incorrect or incomplete metadata)*  bridging 这个差距 (bridging the gap) becomes 帮助大幅提高 (helping to greatly improve)* 这个工作 (this work) becomes 这个软件 (this software)* called FAIRMetaText (called FAIRMetaText) becomes 叫做 FAIRMetaText (called FAIRMetaText)* 比较metadata (compare metadata) becomes 比较metadata (compare metadata)* 自然语言描述 (natural language description) becomes 自然语言描述 (natural language description)* 提供两个条件之间的数学相似度测量 (provide a mathematical similarity measure between two terms) becomes 提供两个条件之间的数学相似度测量 (provide a mathematical similarity measure between two terms)* 这个测量可以用来 (this measurement can be used to) becomes 这个测量可以用来 (this measurement can be used to)* 分析不同的metadata (analyze varied metadata) becomes 分析不同的metadata (analyze different metadata)* 提供符合性检查 (provide compliance checks) becomes 提供符合性检查 (provide compliance checks)* 组织相似的条件 (group similar terms) becomes 组织相似的条件 (group similar terms)I hope this helps! Let me know if you have any further questions or if you'd like me to translate anything else.
</details></li>
</ul>
<hr>
<h2 id="Fairness-Under-Demographic-Scarce-Regime"><a href="#Fairness-Under-Demographic-Scarce-Regime" class="headerlink" title="Fairness Under Demographic Scarce Regime"></a>Fairness Under Demographic Scarce Regime</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13081">http://arxiv.org/abs/2307.13081</a></li>
<li>repo_url: None</li>
<li>paper_authors: Patrik Joslin Kenfack, Samira Ebrahimi Kahou, Ulrich Aïvodji</li>
<li>for: 提高模型的公平性和准确性之间的贸易offs</li>
<li>methods: 引入不确定性认识，并在具有最低不确定性的样本上遵循公平性约束</li>
<li>results: 比 класси型的属性分类器更好地平衡公平性和准确性，并且在一些实际场景下超过使用真实敏感属性的模型。<details>
<summary>Abstract</summary>
Most existing works on fairness assume the model has full access to demographic information. However, there exist scenarios where demographic information is partially available because a record was not maintained throughout data collection or due to privacy reasons. This setting is known as demographic scarce regime. Prior research have shown that training an attribute classifier to replace the missing sensitive attributes (proxy) can still improve fairness. However, the use of proxy-sensitive attributes worsens fairness-accuracy trade-offs compared to true sensitive attributes. To address this limitation, we propose a framework to build attribute classifiers that achieve better fairness-accuracy trade-offs. Our method introduces uncertainty awareness in the attribute classifier and enforces fairness on samples with demographic information inferred with the lowest uncertainty. We show empirically that enforcing fairness constraints on samples with uncertain sensitive attributes is detrimental to fairness and accuracy. Our experiments on two datasets showed that the proposed framework yields models with significantly better fairness-accuracy trade-offs compared to classic attribute classifiers. Surprisingly, our framework outperforms models trained with constraints on the true sensitive attributes.
</details>
<details>
<summary>摘要</summary>
Our method introduces uncertainty awareness in the attribute classifier and enforces fairness on samples with demographic information inferred with the lowest uncertainty. We show empirically that enforcing fairness constraints on samples with uncertain sensitive attributes is detrimental to fairness and accuracy. Our experiments on two datasets showed that the proposed framework yields models with significantly better fairness-accuracy trade-offs compared to classic attribute classifiers. Surprisingly, our framework outperforms models trained with constraints on the true sensitive attributes.In simplified Chinese:大多数现有的公平研究假设模型拥有完整的人口信息。然而，有些场景中人口信息只有部分可用，如数据采集或隐私问题，这种情况被称为人口缺乏 режим。先前的研究表明，使用代理敏感特征来取代缺失的人口信息可以提高公平性。然而，使用代理敏感特征会对公平精度负面影响。为解决这些限制，我们提出了一个框架，用于建立具有更好的公平精度负面的属性分类器。我们的方法会在属性分类器中引入不确定性意识，并在拥有最低不确定性的人口信息上遵循公平约束。我们的实验表明，对不确定的敏感特征进行公平约束是对公平和准确性的负面影响。我们的方案在两个 datasets 上进行了实验，结果表明，我们的框架可以在公平精度负面上取得显著更好的性能，并且超过使用真实敏感特征进行约束的模型。这种情况下，我们的框架可以更好地处理人口缺失的问题。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Certified-Training-Towards-Better-Accuracy-Robustness-Tradeoffs"><a href="#Adaptive-Certified-Training-Towards-Better-Accuracy-Robustness-Tradeoffs" class="headerlink" title="Adaptive Certified Training: Towards Better Accuracy-Robustness Tradeoffs"></a>Adaptive Certified Training: Towards Better Accuracy-Robustness Tradeoffs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13078">http://arxiv.org/abs/2307.13078</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhakshylyk Nurlanov, Frank R. Schmidt, Florian Bernard</li>
<li>for: 本研究旨在提高深度学习模型的可靠性，尤其是在实际应用中。</li>
<li>methods: 我们提出了一种基于适应证明半径的新训练方法，以提高模型的标准准确率和鲁棒性。</li>
<li>results: 我们在MNIST、CIFAR-10和TinyImageNet datasets上进行了实验，并证明了我们的方法可以提高模型的鲁棒性，并且在标准准确率保持不变的情况下提高模型的鲁棒性。特别是在CIFAR-10和TinyImageNet上，我们的方法可以提高模型的鲁棒性至多两倍，并且在同等标准准确率水平下。<details>
<summary>Abstract</summary>
As deep learning models continue to advance and are increasingly utilized in real-world systems, the issue of robustness remains a major challenge. Existing certified training methods produce models that achieve high provable robustness guarantees at certain perturbation levels. However, the main problem of such models is a dramatically low standard accuracy, i.e. accuracy on clean unperturbed data, that makes them impractical. In this work, we consider a more realistic perspective of maximizing the robustness of a model at certain levels of (high) standard accuracy. To this end, we propose a novel certified training method based on a key insight that training with adaptive certified radii helps to improve both the accuracy and robustness of the model, advancing state-of-the-art accuracy-robustness tradeoffs. We demonstrate the effectiveness of the proposed method on MNIST, CIFAR-10, and TinyImageNet datasets. Particularly, on CIFAR-10 and TinyImageNet, our method yields models with up to two times higher robustness, measured as an average certified radius of a test set, at the same levels of standard accuracy compared to baseline approaches.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="LLM-Rec-Personalized-Recommendation-via-Prompting-Large-Language-Models"><a href="#LLM-Rec-Personalized-Recommendation-via-Prompting-Large-Language-Models" class="headerlink" title="LLM-Rec: Personalized Recommendation via Prompting Large Language Models"></a>LLM-Rec: Personalized Recommendation via Prompting Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15780">http://arxiv.org/abs/2307.15780</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanjia Lyu, Song Jiang, Hanqing Zeng, Qifan Wang, Si Zhang, Ren Chen, Chris Leung, Jiajie Tang, Yinglong Xia, Jiebo Luo</li>
<li>for: 提高个性化推荐性能</li>
<li>methods: 使用大语言模型（LLM）输入增强strategies，包括基本提示、推荐驱动提示、参与度引导提示和推荐驱动+参与度引导提示</li>
<li>results: 结合LLM生成的增强输入文本后，个性化推荐性能得到提高，推荐驱动和参与度引导提示策略可以启动LLM理解全球和本地项目特点。<details>
<summary>Abstract</summary>
We investigate various prompting strategies for enhancing personalized recommendation performance with large language models (LLMs) through input augmentation. Our proposed approach, termed LLM-Rec, encompasses four distinct prompting strategies: (1) basic prompting, (2) recommendation-driven prompting, (3) engagement-guided prompting, and (4) recommendation-driven + engagement-guided prompting. Our empirical experiments show that incorporating the augmented input text generated by LLM leads to improved recommendation performance. Recommendation-driven and engagement-guided prompting strategies are found to elicit LLM's understanding of global and local item characteristics. This finding highlights the importance of leveraging diverse prompts and input augmentation techniques to enhance the recommendation capabilities with LLMs.
</details>
<details>
<summary>摘要</summary>
我们研究了多种提示策略，以提高大语言模型（LLM）个性化推荐性能。我们提出的方法，称之为LLM-Rec，包括四种不同的提示策略：（1）基础提示，（2）推荐驱动提示，（3）参与指导提示，和（4）推荐驱动+参与指导提示。我们的实验表明，通过将LLM生成的增强输入文本 integrating 到推荐系统中，可以提高推荐性能。推荐驱动和参与指导提示策略可以引导LLM理解全球和本地项目特征。这一发现强调了利用多种提示和输入增强技术，以提高LLM推荐能力。
</details></li>
</ul>
<hr>
<h2 id="Personalized-Category-Frequency-prediction-for-Buy-It-Again-recommendations"><a href="#Personalized-Category-Frequency-prediction-for-Buy-It-Again-recommendations" class="headerlink" title="Personalized Category Frequency prediction for Buy It Again recommendations"></a>Personalized Category Frequency prediction for Buy It Again recommendations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01195">http://arxiv.org/abs/2308.01195</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amit Pande, Kunal Ghosh, Rankyung Park</li>
<li>for: 这个论文的目的是提出一种基于个性化类别的推荐系统，以帮助零售商提高用户体验和网站参与度。</li>
<li>methods: 该论文提出了一种叫做层次PCIC模型，它包括个性化类别模型（PC模型）和个性化类别下的项模型（IC模型）。PC模型生成了个性化的类别列表，IC模型排名类别下的项目。这个模型使用生存分布模型和时间序列模型来捕捉产品的通用消耗率和趋势。</li>
<li>results: 相比十二个基准模型，PCIC提高了NDCG达16%，同时提高了回归率约2%。PCIC可以在大规模数据集上进行批量训练（耗时8个小时），并在一家大型零售商的官方网站上进行AB测试，导致用户参与度得到了显著提高。<details>
<summary>Abstract</summary>
Buy It Again (BIA) recommendations are crucial to retailers to help improve user experience and site engagement by suggesting items that customers are likely to buy again based on their own repeat purchasing patterns. Most existing BIA studies analyze guests personalized behavior at item granularity. A category-based model may be more appropriate in such scenarios. We propose a recommendation system called a hierarchical PCIC model that consists of a personalized category model (PC model) and a personalized item model within categories (IC model). PC model generates a personalized list of categories that customers are likely to purchase again. IC model ranks items within categories that guests are likely to consume within a category. The hierarchical PCIC model captures the general consumption rate of products using survival models. Trends in consumption are captured using time series models. Features derived from these models are used in training a category-grained neural network. We compare PCIC to twelve existing baselines on four standard open datasets. PCIC improves NDCG up to 16 percent while improving recall by around 2 percent. We were able to scale and train (over 8 hours) PCIC on a large dataset of 100M guests and 3M items where repeat categories of a guest out number repeat items. PCIC was deployed and AB tested on the site of a major retailer, leading to significant gains in guest engagement.
</details>
<details>
<summary>摘要</summary>
Buy It Again（BIA）建议对零售商非常重要，可以帮助提高用户体验和网站参与度，通过建议客户可能会再次购买的商品，基于客户的重复购买模式。大多数现有的BIA研究分析客人个性化行为的项目粒度。我们提出了一种推荐系统，即层次PCIC模型，它包括个性化类别模型（PC模型）和个性化类别内项模型（IC模型）。PC模型生成了客户可能会购买的个性化类别列表。IC模型在类别内排名客户可能会消耗的项目。层次PCIC模型捕捉了产品的总消耗率，使用生存模型记录时间序列模型。这些模型中的特征被用于训练类别粒度的神经网络。我们与12个基准模型进行比较，PCIC提高了NDCG达16%，同时提高了回归率约2%。我们可以在8小时内扩展和训练PCIC模型（100万客户和300万项目），并在一家大型零售商的官方网站上部署PCIC模型，导致用户参与度显著增长。
</details></li>
</ul>
<hr>
<h2 id="Parallel-Q-Learning-Scaling-Off-policy-Reinforcement-Learning-under-Massively-Parallel-Simulation"><a href="#Parallel-Q-Learning-Scaling-Off-policy-Reinforcement-Learning-under-Massively-Parallel-Simulation" class="headerlink" title="Parallel $Q$-Learning: Scaling Off-policy Reinforcement Learning under Massively Parallel Simulation"></a>Parallel $Q$-Learning: Scaling Off-policy Reinforcement Learning under Massively Parallel Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12983">http://arxiv.org/abs/2307.12983</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Improbable-AI/pql">https://github.com/Improbable-AI/pql</a></li>
<li>paper_authors: Zechu Li, Tao Chen, Zhang-Wei Hong, Anurag Ajay, Pulkit Agrawal</li>
<li>for: 这个论文是为了提高复杂任务的强化学习效率，特别是利用高性能的GPU加速器进行数据采集和训练。</li>
<li>methods: 这个论文使用了并行$Q$-学习（PQL）算法，该算法可以并行采集数据、学习策略和价值函数，从而提高强化学习的效率。</li>
<li>results: 该论文通过实验表明，使用PQL算法可以在短短的wall-clock时间内完成复杂任务的强化学习训练，并且能够保持偏离策略的数据效率。此外，论文还 investigate了强化学习学习速度的关键因素。<details>
<summary>Abstract</summary>
Reinforcement learning is time-consuming for complex tasks due to the need for large amounts of training data. Recent advances in GPU-based simulation, such as Isaac Gym, have sped up data collection thousands of times on a commodity GPU. Most prior works used on-policy methods like PPO due to their simplicity and ease of scaling. Off-policy methods are more data efficient but challenging to scale, resulting in a longer wall-clock training time. This paper presents a Parallel $Q$-Learning (PQL) scheme that outperforms PPO in wall-clock time while maintaining superior sample efficiency of off-policy learning. PQL achieves this by parallelizing data collection, policy learning, and value learning. Different from prior works on distributed off-policy learning, such as Apex, our scheme is designed specifically for massively parallel GPU-based simulation and optimized to work on a single workstation. In experiments, we demonstrate that $Q$-learning can be scaled to \textit{tens of thousands of parallel environments} and investigate important factors affecting learning speed. The code is available at https://github.com/Improbable-AI/pql.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>因为复杂任务需要大量训练数据，因此强化学习需要较长的时间。 latest advances in GPU-based simulation, such as Isaac Gym, have sped up data collection by thousands of times on a commodity GPU. Most prior works used on-policy methods like PPO due to their simplicity and ease of scaling. Off-policy methods are more data efficient but challenging to scale, resulting in longer wall-clock training time. This paper presents a Parallel $Q$-Learning (PQL) scheme that outperforms PPO in wall-clock time while maintaining the superior sample efficiency of off-policy learning. PQL achieves this by parallelizing data collection, policy learning, and value learning. Unlike prior works on distributed off-policy learning, such as Apex, our scheme is designed specifically for massively parallel GPU-based simulation and optimized to work on a single workstation. In experiments, we demonstrate that $Q$-learning can be scaled to tens of thousands of parallel environments and investigate important factors affecting learning speed. The code is available at https://github.com/Improbable-AI/pql.
</details></li>
</ul>
<hr>
<h2 id="3D-LLM-Injecting-the-3D-World-into-Large-Language-Models"><a href="#3D-LLM-Injecting-the-3D-World-into-Large-Language-Models" class="headerlink" title="3D-LLM: Injecting the 3D World into Large Language Models"></a>3D-LLM: Injecting the 3D World into Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12981">http://arxiv.org/abs/2307.12981</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/UMass-Foundation-Model/3D-LLM">https://github.com/UMass-Foundation-Model/3D-LLM</a></li>
<li>paper_authors: Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, Chuang Gan</li>
<li>for: This paper is written for proposing a new family of 3D language models (3D-LLMs) that can take 3D point clouds and their features as input and perform a diverse set of 3D-related tasks.</li>
<li>methods: The paper uses three types of prompting mechanisms to collect over 300k 3D-language data covering tasks such as captioning, dense captioning, 3D question answering, task decomposition, 3D grounding, 3D-assisted dialog, navigation, and more. The paper also utilizes a 3D feature extractor to obtain 3D features from rendered multi-view images, and uses 2D VLMs as the backbone to train the 3D-LLMs. Additionally, the paper introduces a 3D localization mechanism to better capture 3D spatial information.</li>
<li>results: The paper shows that the proposed 3D-LLMs outperform state-of-the-art baselines on the ScanQA dataset, with a BLEU-1 score that surpasses the state-of-the-art score by 9%. Additionally, the paper shows that the 3D-LLMs outperform 2D VLMs on held-in datasets for 3D captioning, task composition, and 3D-assisted dialogue, and provides qualitative examples of the model’s ability to perform tasks beyond the scope of existing LLMs and VLMs.Here’s the format you requested:</li>
<li>for: 这篇论文是为了提出一种新的3D语言模型（3D-LLMs），可以将3D点云和其特征作为输入，并执行多种3D相关任务。</li>
<li>methods: 论文使用三种提问机制来收集超过300k个3D语言数据，覆盖包括captioning、dense captioning、3D问答、任务分解、3D静止、3D-assisted dialog、导航等任务。论文还利用了一种3D特征提取器来从渲染多视图图像中提取3D特征。在训练3D-LLMs时，论文使用2D VLMs作为基础。此外，论文还引入了3D地址机制，使3D-LLMs更好地捕捉3D空间信息。</li>
<li>results: 论文显示，提出的3D-LLMs在ScanQA数据集上超过了状态艺术基线，BLEU-1分数高于状态艺术分数 by 9%。此外，论文还显示，3D-LLMs在固定数据集上超过了2D VLMs，并提供了质量例子，表明模型可以完成超出现有LLMs和VLMs的任务。<details>
<summary>Abstract</summary>
Large language models (LLMs) and Vision-Language Models (VLMs) have been proven to excel at multiple tasks, such as commonsense reasoning. Powerful as these models can be, they are not grounded in the 3D physical world, which involves richer concepts such as spatial relationships, affordances, physics, layout, and so on. In this work, we propose to inject the 3D world into large language models and introduce a whole new family of 3D-LLMs. Specifically, 3D-LLMs can take 3D point clouds and their features as input and perform a diverse set of 3D-related tasks, including captioning, dense captioning, 3D question answering, task decomposition, 3D grounding, 3D-assisted dialog, navigation, and so on. Using three types of prompting mechanisms that we design, we are able to collect over 300k 3D-language data covering these tasks. To efficiently train 3D-LLMs, we first utilize a 3D feature extractor that obtains 3D features from rendered multi- view images. Then, we use 2D VLMs as our backbones to train our 3D-LLMs. By introducing a 3D localization mechanism, 3D-LLMs can better capture 3D spatial information. Experiments on ScanQA show that our model outperforms state-of-the-art baselines by a large margin (e.g., the BLEU-1 score surpasses state-of-the-art score by 9%). Furthermore, experiments on our held-in datasets for 3D captioning, task composition, and 3D-assisted dialogue show that our model outperforms 2D VLMs. Qualitative examples also show that our model could perform more tasks beyond the scope of existing LLMs and VLMs. Project Page: : https://vis-www.cs.umass.edu/3dllm/.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）和视力语言模型（VLM）已经证明可以在多个任务上表现出色，如常识理解。尽管这些模型强大，但它们不是基于3D物理世界的，这个世界包括更加复杂的概念，如空间关系、可用性、物理、布局等。在这项工作中，我们提议将3D世界注入到大型语言模型中，并 introduce a whole new family of 3D-LLMs。Specifically, 3D-LLMs can take 3D点云和其特征作为输入，并执行一系列3D相关任务，包括captioning、dense captioning、3D问答、任务分解、3D定位、3D辅助对话、导航等。通过我们设计的三种提示机制，我们能够收集超过300k的3D语言数据覆盖这些任务。为有效地训练3D-LLMs，我们首先利用3D特征EXTRACTOR提取3D特征从渲染多视图图像中。然后，我们使用2D VLMs作为我们的背部来训练我们的3D-LLMs。通过引入3D本地化机制，3D-LLMs可以更好地捕捉3D空间信息。ScanQA实验结果显示，我们的模型超过了状态机的基准值（例如BLEU-1分数超过了状态机的分数 by 9%）。此外，我们在我们保留的数据集上进行3D captioning、任务组合和3D辅助对话的实验，我们的模型超过了2D VLMs。质量例子也表明我们的模型可以完成更多的任务，超出现有LLMs和VLMs的范围。项目页面：https://vis-www.cs.umass.edu/3dllm/。
</details></li>
</ul>
<hr>
<h2 id="A-Connection-between-One-Step-Regularization-and-Critic-Regularization-in-Reinforcement-Learning"><a href="#A-Connection-between-One-Step-Regularization-and-Critic-Regularization-in-Reinforcement-Learning" class="headerlink" title="A Connection between One-Step Regularization and Critic Regularization in Reinforcement Learning"></a>A Connection between One-Step Regularization and Critic Regularization in Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12968">http://arxiv.org/abs/2307.12968</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ben-eysenbach/ac-connection">https://github.com/ben-eysenbach/ac-connection</a></li>
<li>paper_authors: Benjamin Eysenbach, Matthieu Geist, Sergey Levine, Ruslan Salakhutdinov</li>
<li>for: 这个论文的目的是解释一些离线RL算法的正则化方法，以提高其性能。</li>
<li>methods: 这个论文使用了一些常用的离线RL算法，如CQL和一步RL，并对它们进行了正则化。</li>
<li>results: 研究发现，使用一步RL可以得到类似于critic正则化的性能，但是需要更多的计算资源。而在实际应用中，使用一步RL可以实现稳定和简单的RL方法，但是其性能可能不及critic正则化。<details>
<summary>Abstract</summary>
As with any machine learning problem with limited data, effective offline RL algorithms require careful regularization to avoid overfitting. One-step methods perform regularization by doing just a single step of policy improvement, while critic regularization methods do many steps of policy improvement with a regularized objective. These methods appear distinct. One-step methods, such as advantage-weighted regression and conditional behavioral cloning, truncate policy iteration after just one step. This ``early stopping'' makes one-step RL simple and stable, but can limit its asymptotic performance. Critic regularization typically requires more compute but has appealing lower-bound guarantees. In this paper, we draw a close connection between these methods: applying a multi-step critic regularization method with a regularization coefficient of 1 yields the same policy as one-step RL. While practical implementations violate our assumptions and critic regularization is typically applied with smaller regularization coefficients, our experiments nevertheless show that our analysis makes accurate, testable predictions about practical offline RL methods (CQL and one-step RL) with commonly-used hyperparameters. Our results that every problem can be solved with a single step of policy improvement, but rather that one-step RL might be competitive with critic regularization on RL problems that demand strong regularization.
</details>
<details>
<summary>摘要</summary>
“与有限数据的机器学习问题相似，有效的离线RL算法需要仔细的规则化以避免过拟合。一步方法在做出一步策略改进后就结束，而批处规则化方法则在多个步骤策略改进中使用规则化目标。这些方法看起来很不同。一步方法，如偏好权重回归和 conditional behavioral cloning，在做出一步策略改进后就结束。这种``早期停止''使得一步RL简单和稳定，但可能限制其极限性能。批处规则化通常需要更多的计算资源，但它具有吸引人的下界保证。在这篇论文中，我们将一步和批处规则化方法之间 Draw a close connection：在应用多步批处规则化方法时，使用规则化系数为1就等于一步RL。虽然实践中的假设不符合我们的假设，但我们的实验表明，我们的分析对实际的离线RL方法（CQL和一步RL）的实现进行了准确和可靠的预测。我们的结果表明，每个问题都可以通过一步策略改进来解决，但是一步RL可能与批处规则化在RL问题上具有强规则化的情况下竞争。”
</details></li>
</ul>
<hr>
<h2 id="Enhancing-image-captioning-with-depth-information-using-a-Transformer-based-framework"><a href="#Enhancing-image-captioning-with-depth-information-using-a-Transformer-based-framework" class="headerlink" title="Enhancing image captioning with depth information using a Transformer-based framework"></a>Enhancing image captioning with depth information using a Transformer-based framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03767">http://arxiv.org/abs/2308.03767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aya Mahmoud Ahmed, Mohamed Yousef, Khaled F. Hussain, Yousef Bassyouni Mahdy</li>
<li>for: 提高图像描述性能</li>
<li>methods: 使用 transformer 架构，RGB 图像和其对应的深度图像进行共同描述</li>
<li>results: 在 NYU-v2 和 Stanford 图像段落描述数据集上实现了提高描述性能，并提出了一个更正版的 NYU-v2 数据集。Here’s the full Chinese text:</li>
<li>for: 本文 investigate  Whether integrating depth information with RGB images can enhance the captioning task and generate better descriptions.</li>
<li>methods: 我们提出了一个基于 transformer 架构的 RGB 图像和其对应的深度图像共同描述框架。</li>
<li>results: 我们在 NYU-v2 和 Stanford 图像段落描述数据集上实现了提高描述性能，并提出了一个更正版的 NYU-v2 数据集。<details>
<summary>Abstract</summary>
Captioning images is a challenging scene-understanding task that connects computer vision and natural language processing. While image captioning models have been successful in producing excellent descriptions, the field has primarily focused on generating a single sentence for 2D images. This paper investigates whether integrating depth information with RGB images can enhance the captioning task and generate better descriptions. For this purpose, we propose a Transformer-based encoder-decoder framework for generating a multi-sentence description of a 3D scene. The RGB image and its corresponding depth map are provided as inputs to our framework, which combines them to produce a better understanding of the input scene. Depth maps could be ground truth or estimated, which makes our framework widely applicable to any RGB captioning dataset. We explored different fusion approaches to fuse RGB and depth images. The experiments are performed on the NYU-v2 dataset and the Stanford image paragraph captioning dataset. During our work with the NYU-v2 dataset, we found inconsistent labeling that prevents the benefit of using depth information to enhance the captioning task. The results were even worse than using RGB images only. As a result, we propose a cleaned version of the NYU-v2 dataset that is more consistent and informative. Our results on both datasets demonstrate that the proposed framework effectively benefits from depth information, whether it is ground truth or estimated, and generates better captions. Code, pre-trained models, and the cleaned version of the NYU-v2 dataset will be made publically available.
</details>
<details>
<summary>摘要</summary>
captioning图像是一个复杂的Scene理解任务，搭配计算机视觉和自然语言处理。虽然图像描述模型已经在生成出excelente描述，但这个领域主要集中在生成2D图像的单个句子。这篇论文 investigates whether integrating depth信息withRGB图像可以提高描述任务并生成更好的描述。为此，我们提出了一个基于Transformer架构的encoder-decoder框架，用于生成3D场景的多句子描述。RGB图像和其相应的深度图被提供为我们框架的输入，我们将它们结合以生成更好的对输入场景的理解。深度图可以是真实的或估计的，使我们的框架适用于任何RGB描述数据集。我们实现了不同的融合方法来融合RGB和深度图像。实验在NYU-v2数据集和Stanford图像段落描述数据集进行。在我们的NYU-v2数据集工作中，我们发现了不一致的标签，这阻碍了使用深度信息提高描述任务的好处。结果甚至比使用RGB图像alone更差。因此，我们提出了一个更正版的NYU-v2数据集，其标签更加一致和有用。我们的结果在两个数据集上表明，我们的提案的框架可以受益于深度信息，无论是真实的或估计的，并生成更好的描述。代码、预训练模型和更正版的NYU-v2数据集将公开发布。
</details></li>
</ul>
<hr>
<h2 id="RLCD-Reinforcement-Learning-from-Contrast-Distillation-for-Language-Model-Alignment"><a href="#RLCD-Reinforcement-Learning-from-Contrast-Distillation-for-Language-Model-Alignment" class="headerlink" title="RLCD: Reinforcement Learning from Contrast Distillation for Language Model Alignment"></a>RLCD: Reinforcement Learning from Contrast Distillation for Language Model Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12950">http://arxiv.org/abs/2307.12950</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/rlcd">https://github.com/facebookresearch/rlcd</a></li>
<li>paper_authors: Kevin Yang, Dan Klein, Asli Celikyilmaz, Nanyun Peng, Yuandong Tian</li>
<li>for: 本研究旨在开发一种不使用人类反馈的自然语言原则Alignment方法，以提高语言模型的表现。</li>
<li>methods: 本方法使用模拟的偏好对，包括高质量和低质量示例，通过对比正向和负向提示来训练偏好模型。然后，使用奖励学习来改进基础不aligned语言模型。</li>
<li>results: 实验表明，RLCD方法在三种多样化的对齐任务中（无害性、有益性、故事简 outline生成）均表现出色，并在7B和30B模型缩放下表现出超过RLAIF（Bai et al., 2022b）和上下文混合（Huang et al., 2022）基eline的result。<details>
<summary>Abstract</summary>
We propose Reinforcement Learning from Contrast Distillation (RLCD), a method for aligning language models to follow natural language principles without using human feedback. RLCD trains a preference model using simulated preference pairs that contain both a high-quality and low-quality example, generated using contrasting positive and negative prompts. The preference model is then used to improve a base unaligned language model via reinforcement learning. Empirically, RLCD outperforms RLAIF (Bai et al., 2022b) and context distillation (Huang et al., 2022) baselines across three diverse alignment tasks--harmlessness, helpfulness, and story outline generation--and on both 7B and 30B model scales for preference data simulation.
</details>
<details>
<summary>摘要</summary>
我们提议一种基于强化学习的自然语言原则对齐方法，称为强化学习自然语言原则（RLCD）。RLCD使用模拟的偏好对使用了对比正反例的高质量和低质量示例，通过偏好模型进行改进。我们在三种多样化的对齐任务中（无害、有用和故事笔记生成） empirically 证明RLCD超过RLAIF（Bai et al., 2022b）和语音维度（Huang et al., 2022）基elines，并在7B和30B模型缩放下进行偏好数据模拟。
</details></li>
</ul>
<hr>
<h2 id="On-Privileged-and-Convergent-Bases-in-Neural-Network-Representations"><a href="#On-Privileged-and-Convergent-Bases-in-Neural-Network-Representations" class="headerlink" title="On Privileged and Convergent Bases in Neural Network Representations"></a>On Privileged and Convergent Bases in Neural Network Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12941">http://arxiv.org/abs/2307.12941</a></li>
<li>repo_url: None</li>
<li>paper_authors: Davis Brown, Nikhil Vyas, Yamini Bansal</li>
<li>for: 本研究探究 neural network 学习的表示方式是否具有特权和共同基准。</li>
<li>methods: 研究使用各个神经元表示的特征方向的重要性。</li>
<li>results: 发现 neural network 表示不具有完全旋转不变性，并且在不同初始化的情况下，多个基准可以实现相同的性能。<details>
<summary>Abstract</summary>
In this study, we investigate whether the representations learned by neural networks possess a privileged and convergent basis. Specifically, we examine the significance of feature directions represented by individual neurons. First, we establish that arbitrary rotations of neural representations cannot be inverted (unlike linear networks), indicating that they do not exhibit complete rotational invariance. Subsequently, we explore the possibility of multiple bases achieving identical performance. To do this, we compare the bases of networks trained with the same parameters but with varying random initializations. Our study reveals two findings: (1) Even in wide networks such as WideResNets, neural networks do not converge to a unique basis; (2) Basis correlation increases significantly when a few early layers of the network are frozen identically.   Furthermore, we analyze Linear Mode Connectivity, which has been studied as a measure of basis correlation. Our findings give evidence that while Linear Mode Connectivity improves with increased network width, this improvement is not due to an increase in basis correlation.
</details>
<details>
<summary>摘要</summary>
在这个研究中，我们研究神经网络学习的表示方式是否具有特权和共同基准。 Specifically，我们研究神经元每个个体的特征方向的重要性。首先，我们证明神经网络中的表示不能逆转（不同于线性网络），这表明它们不具备完全旋转不变性。接着，我们探索多个基准是否可以实现相同的性能。为此，我们比较由同样的参数训练而成的不同随机初始化的网络的基准。我们的研究发现了两点：1.  même dans les réseaux larges tels que les WideResNets, les réseaux neuronaux ne convergent pas vers une base unique;2. La corrélation de la base augmente significativement lorsque les premières couches du réseau sont gelées de manière identique. En outre, nous analysons la connectivité linéaire des modes, qui a été étudiée comme une mesure de la corrélation de la base. Nos résultats montrent que si la largeur du réseau augmente, la connectivité linéaire des modes s'améliore, mais cet amélioration n'est pas due à une augmentation de la corrélation de la base.
</details></li>
</ul>
<hr>
<h2 id="Rule-By-Example-Harnessing-Logical-Rules-for-Explainable-Hate-Speech-Detection"><a href="#Rule-By-Example-Harnessing-Logical-Rules-for-Explainable-Hate-Speech-Detection" class="headerlink" title="Rule By Example: Harnessing Logical Rules for Explainable Hate Speech Detection"></a>Rule By Example: Harnessing Logical Rules for Explainable Hate Speech Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12935">http://arxiv.org/abs/2307.12935</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chrisisking/rule-by-example">https://github.com/chrisisking/rule-by-example</a></li>
<li>paper_authors: Christopher Clarke, Matthew Hall, Gaurav Mittal, Ye Yu, Sandra Sajeev, Jason Mars, Mei Chen</li>
<li>for: 这个论文旨在解决现代在线内容审核中的挑战，即使用深度学习模型来取代规则驱动的方法，以提高内容审核的可靠性和可 explainer。</li>
<li>methods: 这个论文提出了一种新的示例基于对比学习方法，称为规则示例学习（Rule By Example，RBE），可以从逻辑规则中学习rich embedding表示。</li>
<li>results: 实验结果表明，RBE可以在3个popular hate speech classification dataset上超越现状的深度学习分类器，以及使用规则和无监督学习方法，同时提供可 explainer的模型预测结果via规则基准。<details>
<summary>Abstract</summary>
Classic approaches to content moderation typically apply a rule-based heuristic approach to flag content. While rules are easily customizable and intuitive for humans to interpret, they are inherently fragile and lack the flexibility or robustness needed to moderate the vast amount of undesirable content found online today. Recent advances in deep learning have demonstrated the promise of using highly effective deep neural models to overcome these challenges. However, despite the improved performance, these data-driven models lack transparency and explainability, often leading to mistrust from everyday users and a lack of adoption by many platforms. In this paper, we present Rule By Example (RBE): a novel exemplar-based contrastive learning approach for learning from logical rules for the task of textual content moderation. RBE is capable of providing rule-grounded predictions, allowing for more explainable and customizable predictions compared to typical deep learning-based approaches. We demonstrate that our approach is capable of learning rich rule embedding representations using only a few data examples. Experimental results on 3 popular hate speech classification datasets show that RBE is able to outperform state-of-the-art deep learning classifiers as well as the use of rules in both supervised and unsupervised settings while providing explainable model predictions via rule-grounding.
</details>
<details>
<summary>摘要</summary>
传统的内容审核方法通常采用规则基于的冒泡法来标识内容。虽然规则容易自定义和人类易于理解，但它们具有脆弱性和缺乏在当今互联网上巨量undesirable content的适应性。近年来，深度学习的进步有力地解决了这些挑战。然而，尽管表现得到改善，这些数据驱动模型仍然缺乏透明性和可解释性，导致用户和多个平台的不信任。在这篇论文中，我们提出了 Rule By Example (RBE)：一种基于例子的对比学习方法，用于从逻辑规则中学习文本内容审核任务。RBE可以提供规则基于的预测，使得模型预测更加可解释和自定义。我们示示了我们的方法可以使用只有几个数据示例来学习丰富的规则嵌入表示。实验结果表明，RBE在3个流行的仇恨言语分类数据集上能够超越当前的深度学习分类器和规则在指导下的情况下，同时提供可解释的模型预测 via 规则嵌入。
</details></li>
</ul>
<hr>
<h2 id="Theoretically-Guaranteed-Policy-Improvement-Distilled-from-Model-Based-Planning"><a href="#Theoretically-Guaranteed-Policy-Improvement-Distilled-from-Model-Based-Planning" class="headerlink" title="Theoretically Guaranteed Policy Improvement Distilled from Model-Based Planning"></a>Theoretically Guaranteed Policy Improvement Distilled from Model-Based Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12933">http://arxiv.org/abs/2307.12933</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chuming Li, Ruonan Jia, Jie Liu, Yinmin Zhang, Yazhe Niu, Yaodong Yang, Yu Liu, Wanli Ouyang</li>
<li>for: 这篇论文是为了提出一种基于模型的 reinforcement learning 算法，以提高控制任务的效率。</li>
<li>methods: 该论文使用了模型改进阶段来储存优化的动作序列，并通过Policy ImprovementStep进行了优化。</li>
<li>results: 实验表明，MPDP算法在六个 MuJoCo 连续控制任务上实现了更高的样本效率和极限性性能，比较model-free和基于模型的 плани法。<details>
<summary>Abstract</summary>
Model-based reinforcement learning (RL) has demonstrated remarkable successes on a range of continuous control tasks due to its high sample efficiency. To save the computation cost of conducting planning online, recent practices tend to distill optimized action sequences into an RL policy during the training phase. Although the distillation can incorporate both the foresight of planning and the exploration ability of RL policies, the theoretical understanding of these methods is yet unclear. In this paper, we extend the policy improvement step of Soft Actor-Critic (SAC) by developing an approach to distill from model-based planning to the policy. We then demonstrate that such an approach of policy improvement has a theoretical guarantee of monotonic improvement and convergence to the maximum value defined in SAC. We discuss effective design choices and implement our theory as a practical algorithm -- Model-based Planning Distilled to Policy (MPDP) -- that updates the policy jointly over multiple future time steps. Extensive experiments show that MPDP achieves better sample efficiency and asymptotic performance than both model-free and model-based planning algorithms on six continuous control benchmark tasks in MuJoCo.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Contextual-Bandits-and-Imitation-Learning-via-Preference-Based-Active-Queries"><a href="#Contextual-Bandits-and-Imitation-Learning-via-Preference-Based-Active-Queries" class="headerlink" title="Contextual Bandits and Imitation Learning via Preference-Based Active Queries"></a>Contextual Bandits and Imitation Learning via Preference-Based Active Queries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12926">http://arxiv.org/abs/2307.12926</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ayush Sekhari, Karthik Sridharan, Wen Sun, Runzhe Wu</li>
<li>for: 本研究考虑了上下文搬狮和模仿学习问题，learner 缺乏直接行动的奖励信息，而是可以在每个回合中aktive查询专家以获取不准确的偏好反馈。learner 的目标是同时减少执行行动的 regret 和对专家进行比较查询的次数。</li>
<li>methods: 本研究提出了一种算法，该算法利用在函数类型上的在线回归 oracle，以选择行动和决定何时进行查询。对于上下文搬狮 Setting，我们的算法实现了一个 regret  bound，其中 regret 的极限为 $O(\min{\sqrt{T}, d&#x2F;\Delta})$，其中 $T$ 表示互动次数，$d$ 表示函数类型的拓扑维度，$\Delta$ 表示最佳行动对所有上下文下的最小偏好。我们的算法不需要知道 $\Delta$，并且与标准上下文搬狮 Setting 中获得的 regret bound相当。此外，我们的算法只需要对专家进行 $O(\min{T, d^2&#x2F;\Delta^2})$ 次查询。</li>
<li>results: 我们的算法可以在上下文搬狮和模仿学习 Setting 中实现 regret  bound，同时减少对专家的查询次数。在模仿学习 Setting 中，我们的算法甚至可以超过专家的性能，这 highlights 一个实际的应用优点，即在不熟悉环境中，可以通过偏好反馈来学习并超越专家。<details>
<summary>Abstract</summary>
We consider the problem of contextual bandits and imitation learning, where the learner lacks direct knowledge of the executed action's reward. Instead, the learner can actively query an expert at each round to compare two actions and receive noisy preference feedback. The learner's objective is two-fold: to minimize the regret associated with the executed actions, while simultaneously, minimizing the number of comparison queries made to the expert. In this paper, we assume that the learner has access to a function class that can represent the expert's preference model under appropriate link functions, and provide an algorithm that leverages an online regression oracle with respect to this function class for choosing its actions and deciding when to query. For the contextual bandit setting, our algorithm achieves a regret bound that combines the best of both worlds, scaling as $O(\min\{\sqrt{T}, d/\Delta\})$, where $T$ represents the number of interactions, $d$ represents the eluder dimension of the function class, and $\Delta$ represents the minimum preference of the optimal action over any suboptimal action under all contexts. Our algorithm does not require the knowledge of $\Delta$, and the obtained regret bound is comparable to what can be achieved in the standard contextual bandits setting where the learner observes reward signals at each round. Additionally, our algorithm makes only $O(\min\{T, d^2/\Delta^2\})$ queries to the expert. We then extend our algorithm to the imitation learning setting, where the learning agent engages with an unknown environment in episodes of length $H$ each, and provide similar guarantees for regret and query complexity. Interestingly, our algorithm for imitation learning can even learn to outperform the underlying expert, when it is suboptimal, highlighting a practical benefit of preference-based feedback in imitation learning.
</details>
<details>
<summary>摘要</summary>
我们考虑了上下文带强盗捕鱼和模仿学习问题，learner缺乏直接行动的奖励知识。而是可以在每个回合中活动地询问专家， comparison two actions，并 receive noisy preference feedback。学习者的目标是两fold：一是最小化执行的行动奖励相关的 regret，二是最小化向专家提问的数量。在这篇文章中，我们假设学习者可以访问一个函数类，该函数类可以表示专家的偏好模型，并提供一个在线回归 oracle，以便选择行动和决定何时向专家提问。对于上下文带强盗捕鱼设置，我们的算法可以达到 $O(\min\{\sqrt{T}, d/\Delta\})$ 的 regret bound，其中 $T$ 表示互动次数， $d$ 表示函数类的吸引力维度， $\Delta$ 表示最佳行动在所有上下文中的最小偏好。我们的算法不需要了解 $\Delta$，并且与标准上下文带强盗捕鱼设置的 regret bound相比，我们的 regret bound相对较高。此外，我们的算法只需要 $O(\min\{T, d^2/\Delta^2\})$ 次向专家提问。然后，我们将我们的算法扩展到模仿学习设置，learner在每个 episodes 中与未知环境互动，并提供了类似的 regret 和查询复杂度保证。有趣的是，我们的算法可以在专家下不佳的情况下，learn to outperform 专家，这 highlights 实用上的 benefit 。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Skeleton-Meta-Prototype-Contrastive-Learning-with-Hard-Skeleton-Mining-for-Unsupervised-Person-Re-Identification"><a href="#Hierarchical-Skeleton-Meta-Prototype-Contrastive-Learning-with-Hard-Skeleton-Mining-for-Unsupervised-Person-Re-Identification" class="headerlink" title="Hierarchical Skeleton Meta-Prototype Contrastive Learning with Hard Skeleton Mining for Unsupervised Person Re-Identification"></a>Hierarchical Skeleton Meta-Prototype Contrastive Learning with Hard Skeleton Mining for Unsupervised Person Re-Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12917">http://arxiv.org/abs/2307.12917</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kali-hac/hi-mpc">https://github.com/kali-hac/hi-mpc</a></li>
<li>paper_authors: Haocong Rao, Cyril Leung, Chunyan Miao</li>
<li>for: 本研究旨在提出一种基于深度感知器和深度学习的人重识别方法，使用无监督的 Hierarchical skeleton Meta-Prototype Contrastive learning (Hi-MPC) 方法，以提高人重识别的精度。</li>
<li>methods: 本方法首先构建了 hierarchical 表示，以模型人体的坐标系和运动特征，从 JOINTS 、component 和 limb 等多个水平。然后，提出了一种 hierarchical meta-prototype contrastive learning 模型，通过对不同水平的skeleton features进行 clustering和对比，以学习更有效的人体特征。此外，还提出了一种硬件骨挖掘机制，以适应ively 挖掘出更有用的骨骼特征。</li>
<li>results: 在五个数据集上进行了广泛的评估，显示了我们的方法可以与现有的state-of-the-art方法进行比较，并且在cross-view人重识别和 RGB 环境下也表现出色。<details>
<summary>Abstract</summary>
With rapid advancements in depth sensors and deep learning, skeleton-based person re-identification (re-ID) models have recently achieved remarkable progress with many advantages. Most existing solutions learn single-level skeleton features from body joints with the assumption of equal skeleton importance, while they typically lack the ability to exploit more informative skeleton features from various levels such as limb level with more global body patterns. The label dependency of these methods also limits their flexibility in learning more general skeleton representations. This paper proposes a generic unsupervised Hierarchical skeleton Meta-Prototype Contrastive learning (Hi-MPC) approach with Hard Skeleton Mining (HSM) for person re-ID with unlabeled 3D skeletons. Firstly, we construct hierarchical representations of skeletons to model coarse-to-fine body and motion features from the levels of body joints, components, and limbs. Then a hierarchical meta-prototype contrastive learning model is proposed to cluster and contrast the most typical skeleton features ("prototypes") from different-level skeletons. By converting original prototypes into meta-prototypes with multiple homogeneous transformations, we induce the model to learn the inherent consistency of prototypes to capture more effective skeleton features for person re-ID. Furthermore, we devise a hard skeleton mining mechanism to adaptively infer the informative importance of each skeleton, so as to focus on harder skeletons to learn more discriminative skeleton representations. Extensive evaluations on five datasets demonstrate that our approach outperforms a wide variety of state-of-the-art skeleton-based methods. We further show the general applicability of our method to cross-view person re-ID and RGB-based scenarios with estimated skeletons.
</details>
<details>
<summary>摘要</summary>
With the rapid advancements in depth sensors and deep learning, skeleton-based person re-identification (re-ID) models have recently made significant progress with many advantages. Most existing solutions learn single-level skeleton features from body joints, assuming equal skeleton importance, while they typically lack the ability to exploit more informative skeleton features from various levels such as limb level with more global body patterns. The label dependency of these methods also limits their flexibility in learning more general skeleton representations. This paper proposes a generic unsupervised Hierarchical skeleton Meta-Prototype Contrastive learning (Hi-MPC) approach with Hard Skeleton Mining (HSM) for person re-ID with unlabeled 3D skeletons. Firstly, we construct hierarchical representations of skeletons to model coarse-to-fine body and motion features from the levels of body joints, components, and limbs. Then, we propose a hierarchical meta-prototype contrastive learning model to cluster and contrast the most typical skeleton features ("prototypes") from different-level skeletons. By converting original prototypes into meta-prototypes with multiple homogeneous transformations, we induce the model to learn the inherent consistency of prototypes to capture more effective skeleton features for person re-ID. Furthermore, we devise a hard skeleton mining mechanism to adaptively infer the informative importance of each skeleton, so as to focus on harder skeletons to learn more discriminative skeleton representations. Extensive evaluations on five datasets demonstrate that our approach outperforms a wide variety of state-of-the-art skeleton-based methods. We further show the general applicability of our method to cross-view person re-ID and RGB-based scenarios with estimated skeletons.
</details></li>
</ul>
<hr>
<h2 id="Consensus-based-Participatory-Budgeting-for-Legitimacy-Decision-Support-via-Multi-agent-Reinforcement-Learning"><a href="#Consensus-based-Participatory-Budgeting-for-Legitimacy-Decision-Support-via-Multi-agent-Reinforcement-Learning" class="headerlink" title="Consensus-based Participatory Budgeting for Legitimacy: Decision Support via Multi-agent Reinforcement Learning"></a>Consensus-based Participatory Budgeting for Legitimacy: Decision Support via Multi-agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12915">http://arxiv.org/abs/2307.12915</a></li>
<li>repo_url: None</li>
<li>paper_authors: Srijoni Majumdar, Evangelos Pournaras</li>
<li>for: 这篇论文是关于如何使用协商来改善参与预算的法定程序的，以提高公共基金的分配的公正性和包容性。</li>
<li>methods: 这篇论文提出了一种新的协商方法，使用多代理人强化学习技术来支持决策，并帮助选民互动以达成可持续的妥协。</li>
<li>results: 实验结果表明，这种协商方法可以达成妥协，效率高并稳定，而且与现有的投票聚合方法相比，它可以提高公平性和包容性。<details>
<summary>Abstract</summary>
The legitimacy of bottom-up democratic processes for the distribution of public funds by policy-makers is challenging and complex. Participatory budgeting is such a process, where voting outcomes may not always be fair or inclusive. Deliberation for which project ideas to put for voting and choose for implementation lack systematization and do not scale. This paper addresses these grand challenges by introducing a novel and legitimate iterative consensus-based participatory budgeting process. Consensus is designed to be a result of decision support via an innovative multi-agent reinforcement learning approach. Voters are assisted to interact with each other to make viable compromises. Extensive experimental evaluation with real-world participatory budgeting data from Poland reveal striking findings: Consensus is reachable, efficient and robust. Compromise is required, which is though comparable to the one of existing voting aggregation methods that promote fairness and inclusion without though attaining consensus.
</details>
<details>
<summary>摘要</summary>
政策制定者的底层民主过程对公共资金的分配存在挑战和复杂性。参与预算是这种过程之一，其投票结果可能不一定公平和包容。协商选择要投票的项目意见和实施的方法缺乏系统化和扩展性。这篇论文解决这些总统困难，提出了一种新的合法的迭代共识参与预算过程。这种共识是通过创新的多代理增强学习方法支持决策的结果。选民被助け到互动相互，制定可行的妥协。实际在波兰的实验证明了 striking 的发现：共识是可以达成的，高效和稳定。妥协是必要的，与现有的投票集成方法相比，它不一定实现共识，但能够保证公平和包容。
</details></li>
</ul>
<hr>
<h2 id="Graph-Neural-Networks-For-Mapping-Variables-Between-Programs-–-Extended-Version"><a href="#Graph-Neural-Networks-For-Mapping-Variables-Between-Programs-–-Extended-Version" class="headerlink" title="Graph Neural Networks For Mapping Variables Between Programs – Extended Version"></a>Graph Neural Networks For Mapping Variables Between Programs – Extended Version</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13014">http://arxiv.org/abs/2307.13014</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pmorvalho/ecai23-gnns-for-mapping-variables-between-programs">https://github.com/pmorvalho/ecai23-gnns-for-mapping-variables-between-programs</a></li>
<li>paper_authors: Pedro Orvalho, Jelle Piepenbrock, Mikoláš Janota, Vasco Manquinho</li>
<li>for: 本研究旨在提高程序相似性比较的精度和效率，通过使用图神经网络（GNN）将两个程序中变量的集合映射到一起。</li>
<li>methods: 本研究使用了图神经网络（GNN）来映射两个程序的抽象树（AST）中的变量集合。</li>
<li>results: 实验结果显示，我们的方法可以正确地映射83%的评估数据集，而当前状态的程序修复方法（主要基于程序结构）只能修复约72%的错误程序。而我们的方法， solely based on variable mappings，可以修复约88.5%的错误程序。<details>
<summary>Abstract</summary>
Automated program analysis is a pivotal research domain in many areas of Computer Science -- Formal Methods and Artificial Intelligence, in particular. Due to the undecidability of the problem of program equivalence, comparing two programs is highly challenging. Typically, in order to compare two programs, a relation between both programs' sets of variables is required. Thus, mapping variables between two programs is useful for a panoply of tasks such as program equivalence, program analysis, program repair, and clone detection. In this work, we propose using graph neural networks (GNNs) to map the set of variables between two programs based on both programs' abstract syntax trees (ASTs). To demonstrate the strength of variable mappings, we present three use-cases of these mappings on the task of program repair to fix well-studied and recurrent bugs among novice programmers in introductory programming assignments (IPAs). Experimental results on a dataset of 4166 pairs of incorrect/correct programs show that our approach correctly maps 83% of the evaluation dataset. Moreover, our experiments show that the current state-of-the-art on program repair, greatly dependent on the programs' structure, can only repair about 72% of the incorrect programs. In contrast, our approach, which is solely based on variable mappings, can repair around 88.5%.
</details>
<details>
<summary>摘要</summary>
自动化程序分析是计算机科学多个领域的重要研究领域，特别是正式方法和人工智能。由于程序相等性问题是不可解决的，因此比较两个程序是非常困难的。通常，以便比较两个程序，需要两个程序变量集的关系。因此，将变量 между两个程序映射到相同的空间是非常有用的，这有助于许多任务，如程序相等性、程序分析、程序修复和假象检测。在这个工作中，我们提议使用图神经网络（GNNs）将两个程序的变量集映射到相同的空间，基于这两个程序的抽象语法树（ASTs）。为了证明变量映射的强大性，我们在程序修复任务上提供了三个使用情况，用于修复 novice 程序员在入门编程作业（IPAs）中经常出现的常见bug。实验结果表明，我们的方法可以在4166对错误/正确程序的评估集中正确地映射83%的评估集。此外，我们的实验还表明，现有的程序修复方法，强调程序结构，只能修复约72%的错误程序。相比之下，我们的方法，solely基于变量映射，可以修复约88.5%的错误程序。
</details></li>
</ul>
<hr>
<h2 id="Towards-a-Visual-Language-Foundation-Model-for-Computational-Pathology"><a href="#Towards-a-Visual-Language-Foundation-Model-for-Computational-Pathology" class="headerlink" title="Towards a Visual-Language Foundation Model for Computational Pathology"></a>Towards a Visual-Language Foundation Model for Computational Pathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12914">http://arxiv.org/abs/2307.12914</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ming Y. Lu, Bowen Chen, Drew F. K. Williamson, Richard J. Chen, Ivy Liang, Tong Ding, Guillaume Jaume, Igor Odintsov, Andrew Zhang, Long Phi Le, Georg Gerber, Anil V Parwani, Faisal Mahmood</li>
<li>for: 这篇研究旨在提出一个基于对比学习的类别学习模型，以推广 Histopathology 领域中的诊断和鉴别 tasks。</li>
<li>methods: 这篇研究使用了多种来源的 Histopathology 图像和生医文本，并运用了1,170万个图像-描述对的 pairs 进行task-agnostic pretraining。</li>
<li>results: 研究发现，这个 CONCH 模型可以在13个不同的benchmark上进行 transferred learning，并在 histology 图像分类、分类、描述、文本-图像和图像-文本撷取等下测试得到了顶尖性能。<details>
<summary>Abstract</summary>
The accelerated adoption of digital pathology and advances in deep learning have enabled the development of powerful models for various pathology tasks across a diverse array of diseases and patient cohorts. However, model training is often difficult due to label scarcity in the medical domain and the model's usage is limited by the specific task and disease for which it is trained. Additionally, most models in histopathology leverage only image data, a stark contrast to how humans teach each other and reason about histopathologic entities. We introduce CONtrastive learning from Captions for Histopathology (CONCH), a visual-language foundation model developed using diverse sources of histopathology images, biomedical text, and notably over 1.17 million image-caption pairs via task-agnostic pretraining. Evaluated on a suite of 13 diverse benchmarks, CONCH can be transferred to a wide range of downstream tasks involving either or both histopathology images and text, achieving state-of-the-art performance on histology image classification, segmentation, captioning, text-to-image and image-to-text retrieval. CONCH represents a substantial leap over concurrent visual-language pretrained systems for histopathology, with the potential to directly facilitate a wide array of machine learning-based workflows requiring minimal or no further supervised fine-tuning.
</details>
<details>
<summary>摘要</summary>
随着数字 PATHOLOGY 的加速采用和深度学习的进步，已经开发出了许多强大的模型用于各种疾病和患者群体中的 PATHOLOGY 任务。然而，模型训练往往困难，因为医疗领域中标签的缺乏和模型的使用受到特定任务和疾病的限制。此外，大多数 histopathology 模型仅利用图像数据，与人类教育和理解 histopathologic 实体不符。我们介绍了 CONtrastive learning from Captions for Histopathology (CONCH)，一种基于多种 histopathology 图像、生物医学文本和着重于 1.17 万个图像-caption 对的视觉语言基础模型。在 13 种多样化的标准测试集上评估，CONCH 可以转移到覆盖图像和文本下游任务，实现 histology 图像分类、 segmentation、captioning、text-to-image 和 image-to-text 检索的状态计算机科学中的最佳性能。CONCH 代表了对于 histopathology 的较大的进步，具有直接促进许多机器学习基于工作流程，需要 minimal 或无需进一步的监督微调的潜在。
</details></li>
</ul>
<hr>
<h2 id="GridMM-Grid-Memory-Map-for-Vision-and-Language-Navigation"><a href="#GridMM-Grid-Memory-Map-for-Vision-and-Language-Navigation" class="headerlink" title="GridMM: Grid Memory Map for Vision-and-Language Navigation"></a>GridMM: Grid Memory Map for Vision-and-Language Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12907">http://arxiv.org/abs/2307.12907</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mrzihan/gridmm">https://github.com/mrzihan/gridmm</a></li>
<li>paper_authors: Zihan Wang, Xiangyang Li, Jiahao Yang, Yeqi Liu, Shuqiang Jiang</li>
<li>for: 本研究旨在提出一种新的视觉语言导航（VLN）方法，以便在3D环境中根据自然语言指令进行导航。</li>
<li>methods: 我们提出了一种名为Grid Memory Map（GridMM）的新方法，它使用了顺序状态、 topological maps 或 top-down semantic maps来表示已经游览过的环境。我们还提出了一种 instruction relevance aggregation 方法，用于在每个格子区域中捕捉细腻的视觉提示。</li>
<li>results: 我们在REVERIE、R2R、SOON数据集上进行了广泛的实验，并在R2R-CE数据集上进行了连续环境的实验，结果显示了我们的提posed方法的优越性。<details>
<summary>Abstract</summary>
Vision-and-language navigation (VLN) enables the agent to navigate to a remote location following the natural language instruction in 3D environments. To represent the previously visited environment, most approaches for VLN implement memory using recurrent states, topological maps, or top-down semantic maps. In contrast to these approaches, we build the top-down egocentric and dynamically growing Grid Memory Map (i.e., GridMM) to structure the visited environment. From a global perspective, historical observations are projected into a unified grid map in a top-down view, which can better represent the spatial relations of the environment. From a local perspective, we further propose an instruction relevance aggregation method to capture fine-grained visual clues in each grid region. Extensive experiments are conducted on both the REVERIE, R2R, SOON datasets in the discrete environments, and the R2R-CE dataset in the continuous environments, showing the superiority of our proposed method.
</details>
<details>
<summary>摘要</summary>
视觉语言导航（VLN）允许代理人在三维环境中根据自然语言指令进行导航。以前的环境表示方法中，大多数方法使用循环状态、 topological map 或 top-down semantic map 来实现记忆。在这些方法中，我们构建了从上而下的 egocentric 和动态增长的 Grid Memory Map（i.e., GridMM），以 структуриze 已经探索的环境。从全球视角来看，历史观察被投影到一个统一的格子地图上，可以更好地表示环境的空间关系。从本地视角来看，我们还提出了一种指令相关积累方法，以捕捉每个格子区域中的细腻视觉准确。我们在 discrete 环境中的 REVERIE、R2R 和 SOON 数据集上，以及 continuous 环境中的 R2R-CE 数据集上进行了广泛的实验，显示了我们提出的方法的优越性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/25/cs.AI_2023_07_25/" data-id="cloqtaelu0019gh882ma3ghm5" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_07_25" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/25/cs.CL_2023_07_25/" class="article-date">
  <time datetime="2023-07-25T11:00:00.000Z" itemprop="datePublished">2023-07-25</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/25/cs.CL_2023_07_25/">cs.CL - 2023-07-25</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="XDLM-Cross-lingual-Diffusion-Language-Model-for-Machine-Translation"><a href="#XDLM-Cross-lingual-Diffusion-Language-Model-for-Machine-Translation" class="headerlink" title="XDLM: Cross-lingual Diffusion Language Model for Machine Translation"></a>XDLM: Cross-lingual Diffusion Language Model for Machine Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13560">http://arxiv.org/abs/2307.13560</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linyao Chen, Aosong Feng, Boming Yang, Zihui Li</li>
<li>for: 这个研究是为了探讨 cross-lingual 的 diffusion model，以提高机器翻译的效果。</li>
<li>methods: 本研究使用了一种新的训练目标—TLDM，并在 fine-tuning 阶段使用了一个基于这个模型的翻译系统。</li>
<li>results: 研究发现，使用 XDLM 可以在机器翻译 benchmark 上超越 diffusion 和 Transformer 基于模型的基eline。<details>
<summary>Abstract</summary>
Recently, diffusion models have excelled in image generation tasks and have also been applied to neural language processing (NLP) for controllable text generation. However, the application of diffusion models in a cross-lingual setting is less unexplored. Additionally, while pretraining with diffusion models has been studied within a single language, the potential of cross-lingual pretraining remains understudied. To address these gaps, we propose XDLM, a novel Cross-lingual diffusion model for machine translation, consisting of pretraining and fine-tuning stages. In the pretraining stage, we propose TLDM, a new training objective for mastering the mapping between different languages; in the fine-tuning stage, we build up the translation system based on the pretrained model. We evaluate the result on several machine translation benchmarks and outperformed both diffusion and Transformer baselines.
</details>
<details>
<summary>摘要</summary>
近些时间，扩散模型在图像生成任务中表现出色，同时也应用于神经语言处理（NLP）中控制文本生成。然而，扩散模型在跨语言设置下的应用仍然未得到充分研究。此外，在单一语言预训练下的扩散模型预训练还未得到充分研究。为了解决这些漏洞，我们提出了 XDLM，一种新的跨语言扩散模型 для机器翻译，包括预训练和精度调整两个阶段。在预训练阶段，我们提出了 TLDM，一个新的训练目标，用于掌握不同语言之间的映射关系；在精度调整阶段，我们建立了基于预训练模型的翻译系统。我们对多个机器翻译标准 benchmark 进行评估，并在 diffusion 和 Transformer 基elines 上出perform。
</details></li>
</ul>
<hr>
<h2 id="Holistic-Exploration-on-Universal-Decompositional-Semantic-Parsing-Architecture-Data-Augmentation-and-LLM-Paradigm"><a href="#Holistic-Exploration-on-Universal-Decompositional-Semantic-Parsing-Architecture-Data-Augmentation-and-LLM-Paradigm" class="headerlink" title="Holistic Exploration on Universal Decompositional Semantic Parsing: Architecture, Data Augmentation, and LLM Paradigm"></a>Holistic Exploration on Universal Decompositional Semantic Parsing: Architecture, Data Augmentation, and LLM Paradigm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13424">http://arxiv.org/abs/2307.13424</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hexuandeng/hexp4uds">https://github.com/hexuandeng/hexp4uds</a></li>
<li>paper_authors: Hexuan Deng, Xin Zhang, Meishan Zhang, Xuebo Liu, Min Zhang</li>
<li>for: 这个论文主要是为了探讨Universal Decompositional Semantic（UDS）分析的全面性。</li>
<li>methods: 这个论文提出了一种卷积模型来实现UDS分析，将复杂的分析任务分解成semantically appropriate的子任务。此外，论文还 incorporates了 syntax information和进一步优化了architecture。</li>
<li>results: 论文的实验结果显示， compared to prior models, our approach significantly reduces inference time while maintaining performance. 在不同的数据增强方法下，我们还进行了实验调查，发现ChatGPT在Attribute parsing方面表现出色，但在Relation parsing方面表现不佳，而使用ChatGPT进行数据增强则得不到优秀的结果。<details>
<summary>Abstract</summary>
In this paper, we conduct a holistic exploration of the Universal Decompositional Semantic (UDS) Parsing. We first introduce a cascade model for UDS parsing that decomposes the complex parsing task into semantically appropriate subtasks. Our approach outperforms the prior models, while significantly reducing inference time. We also incorporate syntactic information and further optimized the architecture. Besides, different ways for data augmentation are explored, which further improve the UDS Parsing. Lastly, we conduct experiments to investigate the efficacy of ChatGPT in handling the UDS task, revealing that it excels in attribute parsing but struggles in relation parsing, and using ChatGPT for data augmentation yields suboptimal results. Our code is available at https://github.com/hexuandeng/HExp4UDS.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们进行了整体的 Universal Decompositional Semantic（UDS）解析探索。我们首先介绍了一种卷积模型为UDS解析任务进行分解，将复杂的解析任务分解成Semantically相应的子任务。我们的方法在优化后比之前的模型表现更优，同时减少了推理时间。我们还将 sintactic information  incorporated 到架构中，进一步优化了 architecture。此外，我们还 explore 了不同的数据增强方法，进一步提高了 UDS解析。最后，我们对 ChatGPT 在 UDS 任务中的处理进行了实验，发现它在 attribute 解析方面表现出色，而在 relation 解析方面却遇到了困难，并且使用 ChatGPT 进行数据增强后的结果不佳。我们的代码可以在 GitHub 上找到：https://github.com/hexuandeng/HExp4UDS。
</details></li>
</ul>
<hr>
<h2 id="Towards-Resolving-Word-Ambiguity-with-Word-Embeddings"><a href="#Towards-Resolving-Word-Ambiguity-with-Word-Embeddings" class="headerlink" title="Towards Resolving Word Ambiguity with Word Embeddings"></a>Towards Resolving Word Ambiguity with Word Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13417">http://arxiv.org/abs/2307.13417</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthias Thurnbauer, Johannes Reisinger, Christoph Goller, Andreas Fischer</li>
<li>for: This paper aims to address the problem of ambiguity in natural language processing, specifically in the context of word embeddings and information retrieval tasks.</li>
<li>methods: The authors propose using DBSCAN clustering to identify ambiguous words and evaluate their level of ambiguity in the latent space. They also propose an automatic parameter selection method for DBSCAN to ensure high-quality clusters.</li>
<li>results: The authors show that their approach can identify ambiguous words and evaluate their level of ambiguity, and that the resulting clusters are semantically coherent and correspond well to the perceived meanings of the words.<details>
<summary>Abstract</summary>
Ambiguity is ubiquitous in natural language. Resolving ambiguous meanings is especially important in information retrieval tasks. While word embeddings carry semantic information, they fail to handle ambiguity well. Transformer models have been shown to handle word ambiguity for complex queries, but they cannot be used to identify ambiguous words, e.g. for a 1-word query. Furthermore, training these models is costly in terms of time, hardware resources, and training data, prohibiting their use in specialized environments with sensitive data. Word embeddings can be trained using moderate hardware resources. This paper shows that applying DBSCAN clustering to the latent space can identify ambiguous words and evaluate their level of ambiguity. An automatic DBSCAN parameter selection leads to high-quality clusters, which are semantically coherent and correspond well to the perceived meanings of a given word.
</details>
<details>
<summary>摘要</summary>
<<SYS>>自然语言中的歧义是 ubique 存在的。在信息检索任务中，解决歧义的含义特别重要。虽然词嵌入带有含义信息，但它们不能好地处理歧义。 transformer 模型可以处理复杂的查询中的词歧义，但它们无法识别歧义的单个词，例如一个单词查询。此外，使用这些模型进行训练需要大量的时间、硬件资源和训练数据，这限制了它们在特殊环境中使用。 word embedding 可以通过中等级别的硬件资源进行训练。这篇论文显示，将 DBSCAN 归一化算法应用到封闭空间可以识别歧义的单个词，并评估它们的歧义水平。自动选择 DBSCAN 参数可以获得高质量的归一化结果，这些结果是semantically coherent 的，与给定词的感知含义相吻合。
</details></li>
</ul>
<hr>
<h2 id="Embedding-Models-for-Supervised-Automatic-Extraction-and-Classification-of-Named-Entities-in-Scientific-Acknowledgements"><a href="#Embedding-Models-for-Supervised-Automatic-Extraction-and-Classification-of-Named-Entities-in-Scientific-Acknowledgements" class="headerlink" title="Embedding Models for Supervised Automatic Extraction and Classification of Named Entities in Scientific Acknowledgements"></a>Embedding Models for Supervised Automatic Extraction and Classification of Named Entities in Scientific Acknowledgements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13377">http://arxiv.org/abs/2307.13377</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kalawinka/season">https://github.com/kalawinka/season</a></li>
<li>paper_authors: Nina Smirnova, Philipp Mayr</li>
<li>for: 本研究旨在评估不同嵌入模型在科学论文感谢部分自动提取和分类承认实体的性能。</li>
<li>methods: 我们使用Flair NLP框架进行命名实体识别（NER）任务，训练 employed three default Flair NER模型，使用四个不同的训练集和不同版本的Flair NLP框架。</li>
<li>results: 我们发现，使用Flair Embeddings模型在中等训练集和最新版本Flair NLP框架下，性能最高，准确率为0.79。训练集的大小从非常小到中等大幅提高了所有训练算法的准确率，但是进一步扩大训练集不再提高性能。模型可以识别六种实体类型：资金机构、奖励编号、个人、大学、公司和其他。模型在一些实体类型上具有较高的F1分数，如个人和奖励编号，它们的F1分数都高于0.9。<details>
<summary>Abstract</summary>
Acknowledgments in scientific papers may give an insight into aspects of the scientific community, such as reward systems, collaboration patterns, and hidden research trends. The aim of the paper is to evaluate the performance of different embedding models for the task of automatic extraction and classification of acknowledged entities from the acknowledgment text in scientific papers. We trained and implemented a named entity recognition (NER) task using the Flair NLP framework. The training was conducted using three default Flair NER models with four differently-sized corpora and different versions of the Flair NLP framework. The Flair Embeddings model trained on the medium corpus with the latest FLAIR version showed the best accuracy of 0.79. Expanding the size of a training corpus from very small to medium size massively increased the accuracy of all training algorithms, but further expansion of the training corpus did not bring further improvement. Moreover, the performance of the model slightly deteriorated. Our model is able to recognize six entity types: funding agency, grant number, individuals, university, corporation, and miscellaneous. The model works more precisely for some entity types than for others; thus, individuals and grant numbers showed a very good F1-Score over 0.9. Most of the previous works on acknowledgment analysis were limited by the manual evaluation of data and therefore by the amount of processed data. This model can be applied for the comprehensive analysis of acknowledgment texts and may potentially make a great contribution to the field of automated acknowledgment analysis.
</details>
<details>
<summary>摘要</summary>
科学论文的感谢部分可以提供科学社区的一些方面的信息，如奖励系统、合作模式和隐藏的研究趋势。本文的目标是评估不同的嵌入模型在自动抽取和分类感谢 Entity 的任务中的表现。我们使用 Flair NLP 框架进行命名实体识别（NER）任务，并对三个默认的 Flair NER 模型进行训练。训练使用了不同的四个 corpus 和不同的 Flair NLP 框架版本。Flair Embeddings 模型使用 medium corpus 和最新的 FLAIR 版本显示最好的准确率为 0.79。将训练 corpus 的大小从非常小到中型大小会大幅提高所有训练算法的准确率，但是进一步扩大训练 corpus 不会再得到更好的改善。此外，模型的性能略有下降。我们的模型能够识别六种实体类型：资金机构、奖学金号、个人、大学、公司和其他。模型在一些实体类型上更加精准，例如个人和奖学金号的 F1 分数都高于 0.9。大多数前一些 acknowledgment 分析的研究都是通过手动评估数据来限制的，因此只能处理有限量的数据。这种模型可以应用于全面的 acknowledgment 文本分析，并可能对自动 acknowledgment 分析领域产生很大的贡献。
</details></li>
</ul>
<hr>
<h2 id="Prot2Text-Multimodal-Protein’s-Function-Generation-with-GNNs-and-Transformers"><a href="#Prot2Text-Multimodal-Protein’s-Function-Generation-with-GNNs-and-Transformers" class="headerlink" title="Prot2Text: Multimodal Protein’s Function Generation with GNNs and Transformers"></a>Prot2Text: Multimodal Protein’s Function Generation with GNNs and Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14367">http://arxiv.org/abs/2307.14367</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hadi Abdine, Michail Chatzianastasis, Costas Bouyioukos, Michalis Vazirgiannis</li>
<li>for: 这 paper 的目的是提出一种新的蛋白质功能预测方法，即 Prot2Text，可以在文本化的方式下预测蛋白质的功能。</li>
<li>methods: 这 paper 使用了 Graph Neural Networks(GNNs) 和 Large Language Models(LLMs) 组合在一起，在encoder-decoder框架下实现蛋白质功能的文本化预测。</li>
<li>results: 该 paper 的实验结果表明，Prot2Text 可以准确地预测蛋白质的功能，并且可以生成详细的文本描述。这些结果表明了 multimodal 模型的转变性，特别是 GNNs 和 LLMs 的融合，为蛋白质功能预测提供了 poderful 工具。<details>
<summary>Abstract</summary>
The complex nature of big biological systems pushed some scientists to classify its understanding under the inconceivable missions. Different leveled challenges complicated this task, one of is the prediction of a protein's function. In recent years, significant progress has been made in this field through the development of various machine learning approaches. However, most existing methods formulate the task as a multi-classification problem, i.e assigning predefined labels to proteins. In this work, we propose a novel approach, \textbf{Prot2Text}, which predicts a protein function's in a free text style, moving beyond the conventional binary or categorical classifications. By combining Graph Neural Networks(GNNs) and Large Language Models(LLMs), in an encoder-decoder framework, our model effectively integrates diverse data types including proteins' sequences, structures, and textual annotations. This multimodal approach allows for a holistic representation of proteins' functions, enabling the generation of detailed and accurate descriptions. To evaluate our model, we extracted a multimodal protein dataset from SwissProt, and demonstrate empirically the effectiveness of Prot2Text. These results highlight the transformative impact of multimodal models, specifically the fusion of GNNs and LLMs, empowering researchers with powerful tools for more accurate prediction of proteins' functions. The code, the models and a demo will be publicly released.
</details>
<details>
<summary>摘要</summary>
大型生物系统的复杂性让一些科学家将其理解归类为不可思议任务。不同的层次挑战使得这项任务更加复杂，其中之一是蛋白质功能预测。在最近几年，通过开发不同的机器学习方法，有 significante进步在这一领域。然而，大多数现有方法将任务 формули化为多类别问题，即将蛋白质分配预定的标签。在这项工作中，我们提出了一种新的方法——Prot2Text，它预测蛋白质功能的方式与传统的多类别分类法不同，而是通过组合图 neural network和大型自然语言模型，在Encoder-Decoder框架中进行融合。这种多Modal的方法允许对蛋白质功能进行全面表示，从而生成详细和准确的描述。为评估我们的模型，我们从SwissProt中提取了一个多Modal蛋白质数据集，并通过实验证明了Prot2Text的效果。这些结果显示了多Modal模型的转变性，尤其是GNNs和LLMs的融合，为研究人员提供了更准确的蛋白质功能预测工具。代码、模型和 demo 将公开发布。
</details></li>
</ul>
<hr>
<h2 id="Improving-the-Generalization-Ability-in-Essay-Coherence-Evaluation-through-Monotonic-Constraints"><a href="#Improving-the-Generalization-Ability-in-Essay-Coherence-Evaluation-through-Monotonic-Constraints" class="headerlink" title="Improving the Generalization Ability in Essay Coherence Evaluation through Monotonic Constraints"></a>Improving the Generalization Ability in Essay Coherence Evaluation through Monotonic Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02506">http://arxiv.org/abs/2308.02506</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Zheng, Huan Zhang, Yan Zhao, Yuxuan Lai</li>
<li>for: 评估文本可读性的一个重要方面是 coherence，可以通过两个主要因素评估一篇文章的 coherence：第一个因素是逻辑连接的正确使用，第二个因素是句子结构的适用性。</li>
<li>methods: 为了解决这些问题，我们提出了一种 coherence 评估模型，包括一个回归模型和两个特征提取器：地方逻辑连接推论模型和句子结构修正模型。我们使用Gradient Boosting 回归树作为回归模型，并对输入特征做出假设约束。</li>
<li>results: 我们的提出的模型能够更好地泛化未经见过的数据。模型在 NLPCC 2023 年度共同任务7的第三名上进行了比赛，并 briefly 介绍了我们的解决方案的剩下的 tracks，其中在第二名上进行了第二名，并在第三名和第四名上进行了第一名。<details>
<summary>Abstract</summary>
Coherence is a crucial aspect of evaluating text readability and can be assessed through two primary factors when evaluating an essay in a scoring scenario. The first factor is logical coherence, characterized by the appropriate use of discourse connectives and the establishment of logical relationships between sentences. The second factor is the appropriateness of punctuation, as inappropriate punctuation can lead to confused sentence structure. To address these concerns, we propose a coherence scoring model consisting of a regression model with two feature extractors: a local coherence discriminative model and a punctuation correction model. We employ gradient-boosting regression trees as the regression model and impose monotonicity constraints on the input features. The results show that our proposed model better generalizes unseen data. The model achieved third place in track 1 of NLPCC 2023 shared task 7. Additionally, we briefly introduce our solution for the remaining tracks, which achieves second place for track 2 and first place for both track 3 and track 4.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Coherence is a crucial aspect of evaluating text readability and can be assessed through two primary factors when evaluating an essay in a scoring scenario. The first factor is logical coherence, characterized by the appropriate use of discourse connectives and the establishment of logical relationships between sentences. The second factor is the appropriateness of punctuation, as inappropriate punctuation can lead to confused sentence structure. To address these concerns, we propose a coherence scoring model consisting of a regression model with two feature extractors: a local coherence discriminative model and a punctuation correction model. We employ gradient-boosting regression trees as the regression model and impose monotonicity constraints on the input features. The results show that our proposed model better generalizes unseen data. The model achieved third place in track 1 of NLPCC 2023 shared task 7. Additionally, we briefly introduce our solution for the remaining tracks, which achieves second place for track 2 and first place for both track 3 and track 4." into Simplified Chinese. coherence 是文本可读性评估中的一个关键因素，可以通过两个主要因素进行评估：一是逻辑连贯性，即使用演示连接词和建立句子之间的逻辑关系；二是句子结构的括号正确性，因为不当的括号可能导致句子结构混乱。为解决这些问题，我们提出了一种减量模型，包括两个特征提取器：本地连贯性推论模型和括号修正模型。我们使用梯度提升回归树作为回归模型，并对输入特征受到约束。结果表明，我们的提出的模型在未seen数据上更好地泛化。这个模型在 NLPCC 2023 共享任务 7 的第三名。此外，我们简要介绍我们对其余轨迹的解决方案，其中在第二轨迹上获得第二名，并在第三轨迹和第四轨迹上均获得第一名。Note: "NLPCC" stands for "Natural Language Processing and Chinese Computing" conference.
</details></li>
</ul>
<hr>
<h2 id="QuIP-2-Bit-Quantization-of-Large-Language-Models-With-Guarantees"><a href="#QuIP-2-Bit-Quantization-of-Large-Language-Models-With-Guarantees" class="headerlink" title="QuIP: 2-Bit Quantization of Large Language Models With Guarantees"></a>QuIP: 2-Bit Quantization of Large Language Models With Guarantees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13304">http://arxiv.org/abs/2307.13304</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jerry-chee/quip">https://github.com/jerry-chee/quip</a></li>
<li>paper_authors: Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, Christopher De Sa</li>
<li>for: 本文研究大语言模型（LLM）后期参数量化。</li>
<li>methods: 我们提出了一种基于 weights和Hessian矩阵偏移的新方法，称为量化偏移处理（QuIP）。QuIP包括两个步骤：（1）一种适应减法程序，使得量化后的模型仍然能够保持准确性；（2）高效的预处理和后处理，使得模型的权重和Hessian矩阵偏移。</li>
<li>results: 我们通过实验发现，我们的偏移预处理可以提高一些现有的量化算法，并且使用只有两个位数的量化方法实现了LLM模型的可行结果。我们的代码可以在<a target="_blank" rel="noopener" href="https://github.com/jerry-chee/QuIP%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/jerry-chee/QuIP上找到。</a><details>
<summary>Abstract</summary>
This work studies post-training parameter quantization in large language models (LLMs). We introduce quantization with incoherence processing (QuIP), a new method based on the insight that quantization benefits from incoherent weight and Hessian matrices, i.e., from the weights and the directions in which it is important to round them accurately being unaligned with the coordinate axes. QuIP consists of two steps: (1) an adaptive rounding procedure minimizing a quadratic proxy objective; (2) efficient pre- and post-processing that ensures weight and Hessian incoherence via multiplication by random orthogonal matrices. We complement QuIP with the first theoretical analysis for an LLM-scale quantization algorithm, and show that our theory also applies to an existing method, OPTQ. Empirically, we find that our incoherence preprocessing improves several existing quantization algorithms and yields the first LLM quantization methods that produce viable results using only two bits per weight. Our code can be found at https://github.com/jerry-chee/QuIP .
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>An adaptive rounding procedure that minimizes a quadratic proxy objective.2. Efficient pre- and post-processing that ensures weight and Hessian incoherence through multiplication by random orthogonal matrices.We also provide the first theoretical analysis for an LLM-scale quantization algorithm and show that our theory applies to an existing method, OPTQ. Empirically, we find that our incoherence preprocessing improves several existing quantization algorithms and yields the first LLM quantization methods that produce viable results using only two bits per weight. Our code can be found at <a target="_blank" rel="noopener" href="https://github.com/jerry-chee/QuIP">https://github.com/jerry-chee/QuIP</a>.</details></li>
</ol>
<hr>
<h2 id="An-Intent-Taxonomy-of-Legal-Case-Retrieval"><a href="#An-Intent-Taxonomy-of-Legal-Case-Retrieval" class="headerlink" title="An Intent Taxonomy of Legal Case Retrieval"></a>An Intent Taxonomy of Legal Case Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13298">http://arxiv.org/abs/2307.13298</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunqiu Shao, Haitao Li, Yueyue Wu, Yiqun Liu, Qingyao Ai, Jiaxin Mao, Yixiao Ma, Shaoping Ma</li>
<li>for: 本研究旨在理解法律案例检索用户的搜寻意图，并开发一个新的法律案例检索Intent taxonomy。</li>
<li>methods: 本研究使用了面试、编辑用户研究和查询日志分析等方法构建了一个法律案例检索Intent taxonomy。</li>
<li>results: 研究发现在不同的搜寻意图下，用户的搜寻行为和满意度有显著差异。此外，该taxonomy可以应用于多个下游法律检索任务，如结果排名和满意度预测。<details>
<summary>Abstract</summary>
Legal case retrieval is a special Information Retrieval~(IR) task focusing on legal case documents. Depending on the downstream tasks of the retrieved case documents, users' information needs in legal case retrieval could be significantly different from those in Web search and traditional ad-hoc retrieval tasks. While there are several studies that retrieve legal cases based on text similarity, the underlying search intents of legal retrieval users, as shown in this paper, are more complicated than that yet mostly unexplored. To this end, we present a novel hierarchical intent taxonomy of legal case retrieval. It consists of five intent types categorized by three criteria, i.e., search for Particular Case(s), Characterization, Penalty, Procedure, and Interest. The taxonomy was constructed transparently and evaluated extensively through interviews, editorial user studies, and query log analysis. Through a laboratory user study, we reveal significant differences in user behavior and satisfaction under different search intents in legal case retrieval. Furthermore, we apply the proposed taxonomy to various downstream legal retrieval tasks, e.g., result ranking and satisfaction prediction, and demonstrate its effectiveness. Our work provides important insights into the understanding of user intents in legal case retrieval and potentially leads to better retrieval techniques in the legal domain, such as intent-aware ranking strategies and evaluation methodologies.
</details>
<details>
<summary>摘要</summary>
法律案例检索是一种特殊的信息检索任务，专注于法律案例文档。根据下游任务中返回的案例文档的用户信息需求，用户在法律检索任务中的搜索意图可能与传统的Web搜索和特殊检索任务存在很大差异。虽然有几篇研究文章通过文本相似性来检索法律案例，但用户在法律检索中的搜索意图还未得到了充分的研究。为此，我们提出了一个新的层次意图分类法，它包括五种意图类别，分为三个标准：寻找特定案例（Search for Particular Case）、特征化（Characterization）、裁罚（Penalty）、程序（Procedure）和利益（Interest）。这个分类体系由 transparent construction 和广泛的用户研究进行验证，并通过实验研究表明了用户在不同搜索意图下的行为和满意度之间存在显著差异。此外，我们还应用该分类法到不同的下游法律检索任务中，如结果排名和满意度预测，并证明其效果。我们的工作为法律检索领域的理解用户意图提供了重要的洞察，并可能导致更好的检索技术的发展，如意向检索策略和评价方法。
</details></li>
</ul>
<hr>
<h2 id="Schema-Driven-Actionable-Insight-Generation-and-Smart-Recommendation"><a href="#Schema-Driven-Actionable-Insight-Generation-and-Smart-Recommendation" class="headerlink" title="Schema-Driven Actionable Insight Generation and Smart Recommendation"></a>Schema-Driven Actionable Insight Generation and Smart Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13176">http://arxiv.org/abs/2307.13176</a></li>
<li>repo_url: None</li>
<li>paper_authors: Allmin Susaiyah, Aki Härmä, Milan Petković</li>
<li>for: 这篇论文是用于生成可行的洞察结论，以促进增长和变革。</li>
<li>methods: 该方法使用了Schema-driven的方法，通过挖掘数据中有趣的模式，生成可读的洞察结论。</li>
<li>results: 该方法可以根据用户反馈进行排序，以适应用户的兴趣。我们已经展示了这种方法可以生成的先验结果。<details>
<summary>Abstract</summary>
In natural language generation (NLG), insight mining is seen as a data-to-text task, where data is mined for interesting patterns and verbalised into 'insight' statements. An 'over-generate and rank' paradigm is intuitively used to generate such insights. The multidimensionality and subjectivity of this process make it challenging. This paper introduces a schema-driven method to generate actionable insights from data to drive growth and change. It also introduces a technique to rank the insights to align with user interests based on their feedback. We show preliminary qualitative results of the insights generated using our technique and demonstrate its ability to adapt to feedback.
</details>
<details>
<summary>摘要</summary>
natural language generation (NLG) 中，启示挖掘被看作是一个数据到文本任务，通过挖掘数据中有趣的模式，并将其转化为“启示”声明。一种“过度生成并排序”的思路是INTUITIVELY用于生成这些启示。由于这个过程的多维度和主观性，使其具有挑战性。这篇论文介绍了一种基于Schema驱动的方法，用于从数据中生成可行的启示，以驱动增长和变革。它还介绍了一种基于用户反馈的技术，用于对启示进行排序，以符合用户的兴趣。我们展示了先期的Qualitative结果，证明了我们的方法能够适应反馈。
</details></li>
</ul>
<hr>
<h2 id="Explaining-Math-Word-Problem-Solvers"><a href="#Explaining-Math-Word-Problem-Solvers" class="headerlink" title="Explaining Math Word Problem Solvers"></a>Explaining Math Word Problem Solvers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13128">http://arxiv.org/abs/2307.13128</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abby Newcomb, Jugal Kalita</li>
<li>for: 本研究旨在探讨自动解 math word problem 的方法是否遵循语言表达的 semantic logic。</li>
<li>methods: 本研究使用了 removing parts of the input 来测试模型的性能，以确定模型是否仅仅匹配语言表达的特征。</li>
<li>results: 结果表明，模型对输入中的多个字词的删除不会影响其解题能力，而且可以从 nonsense 问题中提取正确的解答。这表明自动解 math word problem 的模型可能会匹配语言表达的表现，而不是遵循语言表达的 semantic logic。<details>
<summary>Abstract</summary>
Automated math word problem solvers based on neural networks have successfully managed to obtain 70-80\% accuracy in solving arithmetic word problems. However, it has been shown that these solvers may rely on superficial patterns to obtain their equations. In order to determine what information math word problem solvers use to generate solutions, we remove parts of the input and measure the model's performance on the perturbed dataset. Our results show that the model is not sensitive to the removal of many words from the input and can still manage to find a correct answer when given a nonsense question. This indicates that automatic solvers do not follow the semantic logic of math word problems, and may be overfitting to the presence of specific words.
</details>
<details>
<summary>摘要</summary>
自动化的数学问题解决程式基于神经网络已经成功地在解决算数问题上获得70-80%的准确率。然而，研究人员发现这些解决方案可能会从 superficier 的特征中获得其方程。为了决定这些解决方案所使用的信息，我们将输入中的部分 removed 并评估模型在这些干扰dataset上的性能。我们的结果显示，模型对输入中的许多字都不敏感，仍然可以从非现实问题中获得正确的解答。这表明自动解决方案不会跟着数学问题的semantic逻辑，可能是过拟合特定的字汇。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-the-Ripple-Effects-of-Knowledge-Editing-in-Language-Models"><a href="#Evaluating-the-Ripple-Effects-of-Knowledge-Editing-in-Language-Models" class="headerlink" title="Evaluating the Ripple Effects of Knowledge Editing in Language Models"></a>Evaluating the Ripple Effects of Knowledge Editing in Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12976">http://arxiv.org/abs/2307.12976</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/edenbiran/rippleedits">https://github.com/edenbiran/rippleedits</a></li>
<li>paper_authors: Roi Cohen, Eden Biran, Ori Yoran, Amir Globerson, Mor Geva</li>
<li>for: 这篇论文主要针对现代语言模型中的知识更新问题。</li>
<li>methods: 论文提出了一种新的评估标准，用于评估编辑方法对模型知识的影响。</li>
<li>results: 研究发现，目前的编辑方法通常无法在模型知识中引入一致的变化，而一种简单的上下文编辑基线得到了最佳成绩。<details>
<summary>Abstract</summary>
Modern language models capture a large body of factual knowledge. However, some facts can be incorrectly induced or become obsolete over time, resulting in factually incorrect generations. This has led to the development of various editing methods that allow updating facts encoded by the model. Evaluation of these methods has primarily focused on testing whether an individual fact has been successfully injected, and if similar predictions for other subjects have not changed. Here we argue that such evaluation is limited, since injecting one fact (e.g. ``Jack Depp is the son of Johnny Depp'') introduces a ``ripple effect'' in the form of additional facts that the model needs to update (e.g.``Jack Depp is the sibling of Lily-Rose Depp''). To address this issue, we propose a novel set of evaluation criteria that consider the implications of an edit on related facts. Using these criteria, we then construct \ripple{}, a diagnostic benchmark of 5K factual edits, capturing a variety of types of ripple effects. We evaluate prominent editing methods on \ripple{}, showing that current methods fail to introduce consistent changes in the model's knowledge. In addition, we find that a simple in-context editing baseline obtains the best scores on our benchmark, suggesting a promising research direction for model editing.
</details>
<details>
<summary>摘要</summary>
现代语言模型可以捕捉大量的事实知识。然而，一些事实可能会在时间的推移中变得过时或者错误地被推导出来，导致模型生成的结果不准确。为了解决这个问题，人们开发了多种修改方法，以更新模型中的事实。然而，评估这些方法的主要方法是测试模型中的一个特定事实是否已经成功地更新，并且其他主题的预测没有变化。在这篇文章中，我们 argue这种评估方法是有限的，因为更新一个事实（例如，“杰克·德普是Johnny Depp的儿子”）会导致模型需要更新其他相关的事实（例如，“杰克·德普是LILY-ROSE DEPP的姐妹”）。为解决这个问题，我们提出了一组新的评估标准，考虑修改的影响于相关的事实。使用这些标准，我们然后构建了一个名为\ripple{}的诊断 benchmark，包含5000个事实修改。我们对这些修改进行评估，发现当前的修改方法无法在模型中引入一致的改变。此外，我们发现一个简单的 Context-sensitive editing baseline 在我们的benchmark上获得了最好的分数， suggesting a promising research direction for model editing。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Label-Variation-in-Large-Language-Models-for-Zero-Shot-Text-Classification"><a href="#Leveraging-Label-Variation-in-Large-Language-Models-for-Zero-Shot-Text-Classification" class="headerlink" title="Leveraging Label Variation in Large Language Models for Zero-Shot Text Classification"></a>Leveraging Label Variation in Large Language Models for Zero-Shot Text Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12973">http://arxiv.org/abs/2307.12973</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zeniSoida/pl1">https://github.com/zeniSoida/pl1</a></li>
<li>paper_authors: Flor Miriam Plaza-del-Arco, Debora Nozza, Dirk Hovy</li>
<li>for: 这个论文旨在探讨大语言模型（LLM）在无监督学习下的文本分类能力，以及这些模型在不同任务、数据和语言下的表现。</li>
<li>methods: 这个论文使用了5种当今最佳的LLM作为”注解器”，在5个不同的任务（年龄、性别、主题、情感预测和仇恨言语检测）中进行了4种不同的语言（英语、法语、德语和西班牙语）的测试。</li>
<li>results: 论文发现，虽然不同的任务、数据和语言下的模型表现不同，但使用人工注解者的汇集技术可以substantially better than任何一个个体模型。然而，LLMs仍然不能与人工注解者相比，因此它们并不可以完全取代人工注解。<details>
<summary>Abstract</summary>
The zero-shot learning capabilities of large language models (LLMs) make them ideal for text classification without annotation or supervised training. Many studies have shown impressive results across multiple tasks. While tasks, data, and results differ widely, their similarities to human annotation can aid us in tackling new tasks with minimal expenses. We evaluate using 5 state-of-the-art LLMs as "annotators" on 5 different tasks (age, gender, topic, sentiment prediction, and hate speech detection), across 4 languages: English, French, German, and Spanish. No single model excels at all tasks, across languages, or across all labels within a task. However, aggregation techniques designed for human annotators perform substantially better than any one individual model. Overall, though, LLMs do not rival even simple supervised models, so they do not (yet) replace the need for human annotation. We also discuss the tradeoffs between speed, accuracy, cost, and bias when it comes to aggregated model labeling versus human annotation.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）的零shot学习能力使其成为文本分类无需注释或指导式训练的理想选择。许多研究表明在多个任务上获得了吸引人的结果。虽然任务、数据和结果之间存在差异，但它们在人类注释上的相似性可以帮助我们解决新的任务，降低成本。我们使用5种 state-of-the-art LLM作为“注释员”进行5个任务（年龄、性别、主题、情感预测和词语攻击检测），在英语、法语、德语和西班牙语四种语言上进行评估。没有任何模型在所有任务和语言上表现出色，但是为human annotator的汇集技术表现出了明显的改善。总的来说，LLMs现在没有超过简单的指导式模型，因此它们还没有取代人类注释。我们还讨论了在汇集模型标签与人类注释之间的速度、准确率、成本和偏见的贸易。
</details></li>
</ul>
<hr>
<h2 id="Aligning-Large-Language-Models-with-Human-A-Survey"><a href="#Aligning-Large-Language-Models-with-Human-A-Survey" class="headerlink" title="Aligning Large Language Models with Human: A Survey"></a>Aligning Large Language Models with Human: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12966">http://arxiv.org/abs/2307.12966</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/garyyufei/alignllmhumansurvey">https://github.com/garyyufei/alignllmhumansurvey</a></li>
<li>paper_authors: Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, Qun Liu</li>
<li>For: This paper provides a comprehensive overview of alignment technologies for large language models (LLMs) to better suit human-oriented tasks and expectations.* Methods: The paper reviews various training methodologies for LLM alignment, including supervised fine-tuning, online and offline human preference training, and parameter-efficient training mechanisms.* Results: The paper evaluates the effectiveness of human-aligned LLMs using a multifaceted approach and highlights several promising future research avenues in the field.Here is the same information in Simplified Chinese text:* For: 这篇论文提供了大语言模型（LLM）的启用技术的全面回顾，以便更好地适应人类需求。* Methods: 论文回顾了各种用于LLM启用的训练方法，包括监督精度调整、在线和离线人类偏好训练以及参数高效训练机制。* Results: 论文使用多方面的评估方法评估了人类启用LLM的效果，并提出了许多可能的未来研究方向。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) trained on extensive textual corpora have emerged as leading solutions for a broad array of Natural Language Processing (NLP) tasks. Despite their notable performance, these models are prone to certain limitations such as misunderstanding human instructions, generating potentially biased content, or factually incorrect (hallucinated) information. Hence, aligning LLMs with human expectations has become an active area of interest within the research community. This survey presents a comprehensive overview of these alignment technologies, including the following aspects. (1) Data collection: the methods for effectively collecting high-quality instructions for LLM alignment, including the use of NLP benchmarks, human annotations, and leveraging strong LLMs. (2) Training methodologies: a detailed review of the prevailing training methods employed for LLM alignment. Our exploration encompasses Supervised Fine-tuning, both Online and Offline human preference training, along with parameter-efficient training mechanisms. (3) Model Evaluation: the methods for evaluating the effectiveness of these human-aligned LLMs, presenting a multifaceted approach towards their assessment. In conclusion, we collate and distill our findings, shedding light on several promising future research avenues in the field. This survey, therefore, serves as a valuable resource for anyone invested in understanding and advancing the alignment of LLMs to better suit human-oriented tasks and expectations. An associated GitHub link collecting the latest papers is available at https://github.com/GaryYufei/AlignLLMHumanSurvey.
</details>
<details>
<summary>摘要</summary>
庞大语言模型（LLM）在各种自然语言处理（NLP）任务中表现出色，但它们也存在一些局限性，如不理解人类指令、生成可能偏见的内容或者 factually incorrect（hallucinated）信息。因此，与人类期望的 aligning LLM 已成为研究领域的热点。这篇评论文章提供了一个全面的对这些对齐技术的评论，包括以下方面：1. 数据采集：如何有效地采集高质量的人类指令，包括使用 NLP 标准准则、人工标注和利用强大的 LLM。2. 训练方法：详细介绍了在 LLM 对齐中广泛使用的训练方法，包括监督精细调整、在线人类喜好训练和效率的参数训练机制。3. 模型评价：如何评价这些人类对齐的 LLM，提出了多方面的评价方法，以提供全面的评价方式。总之，这篇评论文章为各种投入 NLP 领域的人提供了一个有价值的资源，帮助他们更好地理解和提高 LLM 的对齐性，以更好地适应人类中心的任务和期望。关于这些研究的最新论文，可以通过以下 GitHub 链接获取：https://github.com/GaryYufei/AlignLLMHumanSurvey.
</details></li>
</ul>
<hr>
<h2 id="Boosting-Punctuation-Restoration-with-Data-Generation-and-Reinforcement-Learning"><a href="#Boosting-Punctuation-Restoration-with-Data-Generation-and-Reinforcement-Learning" class="headerlink" title="Boosting Punctuation Restoration with Data Generation and Reinforcement Learning"></a>Boosting Punctuation Restoration with Data Generation and Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12949">http://arxiv.org/abs/2307.12949</a></li>
<li>repo_url: None</li>
<li>paper_authors: Viet Dac Lai, Abel Salinas, Hao Tan, Trung Bui, Quan Tran, Seunghyun Yoon, Hanieh Deilamsalehy, Franck Dernoncourt, Thien Huu Nguyen</li>
<li>for: 提高自动语音识别（ASR）系统的可读性，通过使用生成语言模型和强化学习来修复ASR文本中的 sintactic structure。</li>
<li>methods: 使用强化学习方法，利用topic中的文本和大型预训练的生成语言模型，将written文本和ASR文本之间的差异 bridge。</li>
<li>results: 实验表明，我们的方法在ASR测试集上的两个标准 datasets上达到了状态的最佳性能。<details>
<summary>Abstract</summary>
Punctuation restoration is an important task in automatic speech recognition (ASR) which aim to restore the syntactic structure of generated ASR texts to improve readability. While punctuated texts are abundant from written documents, the discrepancy between written punctuated texts and ASR texts limits the usability of written texts in training punctuation restoration systems for ASR texts. This paper proposes a reinforcement learning method to exploit in-topic written texts and recent advances in large pre-trained generative language models to bridge this gap. The experiments show that our method achieves state-of-the-art performance on the ASR test set on two benchmark datasets for punctuation restoration.
</details>
<details>
<summary>摘要</summary>
“短语结构修复是自动语音识别（ASR）中的一项重要任务，旨在提高ASR文本的可读性。written文本充沛，但是ASR文本与written文本之间存在差异，这限制了使用written文本来训练ASR文本的短语结构修复系统。本文提出了一种利用topic内文本和大型预训练生成语言模型的强化学习方法，bridge这个差异。实验结果显示，我们的方法在ASR测试集上两个benchmark dataset上达到了状态对抗性的性能。”Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know and I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="The-potential-of-LLMs-for-coding-with-low-resource-and-domain-specific-programming-languages"><a href="#The-potential-of-LLMs-for-coding-with-low-resource-and-domain-specific-programming-languages" class="headerlink" title="The potential of LLMs for coding with low-resource and domain-specific programming languages"></a>The potential of LLMs for coding with low-resource and domain-specific programming languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13018">http://arxiv.org/abs/2307.13018</a></li>
<li>repo_url: None</li>
<li>paper_authors: Artur Tarassow</li>
<li>for: 这项研究旨在探讨使用大语言模型（LLM）进行低资源和域语言编程语言的编程，以便更好地利用LLM处理技术。</li>
<li>methods: 本研究使用了一种基于GPT-3.5的专有LLM，并采用了经济cripting语言名为hansl的开源软件gretl进行实践。</li>
<li>results: 研究发现，LLM可以用于写作、理解、改进和文档gretl代码，包括生成函数的描述文档和提供 econometric代码的准确解释。但是，LLM还有一些局限性，如无法改进某些代码部分和写入正确的单元测试代码。<details>
<summary>Abstract</summary>
This paper presents a study on the feasibility of using large language models (LLM) for coding with low-resource and domain-specific programming languages that typically lack the amount of data required for effective LLM processing techniques. This study focuses on the econometric scripting language named hansl of the open-source software gretl and employs a proprietary LLM based on GPT-3.5. Our findings suggest that LLMs can be a useful tool for writing, understanding, improving, and documenting gretl code, which includes generating descriptive docstrings for functions and providing precise explanations for abstract and poorly documented econometric code. While the LLM showcased promoting docstring-to-code translation capability, we also identify some limitations, such as its inability to improve certain sections of code and to write accurate unit tests. This study is a step towards leveraging the power of LLMs to facilitate software development in low-resource programming languages and ultimately to lower barriers to entry for their adoption.
</details>
<details>
<summary>摘要</summary>
Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and widely used in other countries as well.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/25/cs.CL_2023_07_25/" data-id="cloqtaenw008kgh888ruk7x5p" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_07_25" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/25/cs.LG_2023_07_25/" class="article-date">
  <time datetime="2023-07-25T10:00:00.000Z" itemprop="datePublished">2023-07-25</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/25/cs.LG_2023_07_25/">cs.LG - 2023-07-25</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Multi-GPU-Approach-for-Training-of-Graph-ML-Models-on-large-CFD-Meshes"><a href="#Multi-GPU-Approach-for-Training-of-Graph-ML-Models-on-large-CFD-Meshes" class="headerlink" title="Multi-GPU Approach for Training of Graph ML Models on large CFD Meshes"></a>Multi-GPU Approach for Training of Graph ML Models on large CFD Meshes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13592">http://arxiv.org/abs/2307.13592</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sebastian Strönisch, Maximilian Sander, Andreas Knüpfer, Marcus Meyer</li>
<li>For: 这 paper 的目的是开发一种基于机器学习的拟合模型，用于加速计算流体力学 simulate 的过程。* Methods: 这 paper 使用了图 neural network (GNN) 作为拟合模型，并在多个 GPU 上分区和分配流体流体Domain。* Results:  Comparing 该 paper 的拟合模型与传统的分布式模型，后者 производи了更好的预测结果，并且超越了该拟合模型。<details>
<summary>Abstract</summary>
Mesh-based numerical solvers are an important part in many design tool chains. However, accurate simulations like computational fluid dynamics are time and resource consuming which is why surrogate models are employed to speed-up the solution process. Machine Learning based surrogate models on the other hand are fast in predicting approximate solutions but often lack accuracy. Thus, the development of the predictor in a predictor-corrector approach is the focus here, where the surrogate model predicts a flow field and the numerical solver corrects it. This paper scales a state-of-the-art surrogate model from the domain of graph-based machine learning to industry-relevant mesh sizes of a numerical flow simulation. The approach partitions and distributes the flow domain to multiple GPUs and provides halo exchange between these partitions during training. The utilized graph neural network operates directly on the numerical mesh and is able to preserve complex geometries as well as all other properties of the mesh. The proposed surrogate model is evaluated with an application on a three dimensional turbomachinery setup and compared to a traditionally trained distributed model. The results show that the traditional approach produces superior predictions and outperforms the proposed surrogate model. Possible explanations, improvements and future directions are outlined.
</details>
<details>
<summary>摘要</summary>
mesh-based numerical solvers 是设计工具链中的一个重要部分。然而，精确的 simulate like computational fluid dynamics 需要时间和资源，这是 why 使用 surrogate models 来快速解决方案。机器学习基于 surrogate models 则很快速预测 approximate solutions，但frequently lack accuracy。因此，在这里的发展问题是predictor-corrector方法中的开发预测器，这里的 surrogate model 预测了流场，而numerical solver 则更正。这篇文章 scales 一个 state-of-the-art surrogate model 从 domain of graph-based machine learning 到 industry-relevant mesh sizes 的 numerical flow simulation。该方法将流体Domain  partitioned 和分配到多个 GPUs，并在训练中提供了 halos exchange  между这些分区。使用的 graph neural network 直接操作 numerical mesh，能够保留复杂的几何和所有其他 mesh 的属性。提案的 surrogate model 与一个 three-dimensional turbomachinery 应用中进行比较，与传统的分布式训练模型相比，传统方法产生了更好的预测，超越了提案的 surrogate model。 possible explanations, improvements 和 future directions 也被详细描述。
</details></li>
</ul>
<hr>
<h2 id="Settling-the-Sample-Complexity-of-Online-Reinforcement-Learning"><a href="#Settling-the-Sample-Complexity-of-Online-Reinforcement-Learning" class="headerlink" title="Settling the Sample Complexity of Online Reinforcement Learning"></a>Settling the Sample Complexity of Online Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13586">http://arxiv.org/abs/2307.13586</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihan Zhang, Yuxin Chen, Jason D. Lee, Simon S. Du</li>
<li>for: The paper is written to address the issue of data efficiency in online reinforcement learning, specifically the problem of achieving minimax-optimal regret without incurring any burn-in cost.</li>
<li>methods: The paper proposes a modified version of Monotonic Value Propagation (MVP), a model-based algorithm, and develops a new regret decomposition strategy and analysis paradigm to decouple complicated statistical dependency.</li>
<li>results: The paper achieves a regret on the order of $(SAH^3K)&#x2F;\sqrt{\log K}$, which matches the minimax lower bound for the entire range of sample size $K\geq 1$, and translates to a PAC sample complexity of $\frac{SAH^3}{\varepsilon^2}$ up to log factor, which is minimax-optimal for the full $\varepsilon$-range.<details>
<summary>Abstract</summary>
A central issue lying at the heart of online reinforcement learning (RL) is data efficiency. While a number of recent works achieved asymptotically minimal regret in online RL, the optimality of these results is only guaranteed in a ``large-sample'' regime, imposing enormous burn-in cost in order for their algorithms to operate optimally. How to achieve minimax-optimal regret without incurring any burn-in cost has been an open problem in RL theory.   We settle this problem for the context of finite-horizon inhomogeneous Markov decision processes. Specifically, we prove that a modified version of Monotonic Value Propagation (MVP), a model-based algorithm proposed by \cite{zhang2020reinforcement}, achieves a regret on the order of (modulo log factors) \begin{equation*}   \min\big\{ \sqrt{SAH^3K}, \,HK \big\}, \end{equation*} where $S$ is the number of states, $A$ is the number of actions, $H$ is the planning horizon, and $K$ is the total number of episodes. This regret matches the minimax lower bound for the entire range of sample size $K\geq 1$, essentially eliminating any burn-in requirement. It also translates to a PAC sample complexity (i.e., the number of episodes needed to yield $\varepsilon$-accuracy) of $\frac{SAH^3}{\varepsilon^2}$ up to log factor, which is minimax-optimal for the full $\varepsilon$-range.   Further, we extend our theory to unveil the influences of problem-dependent quantities like the optimal value/cost and certain variances. The key technical innovation lies in the development of a new regret decomposition strategy and a novel analysis paradigm to decouple complicated statistical dependency -- a long-standing challenge facing the analysis of online RL in the sample-hungry regime.
</details>
<details>
<summary>摘要</summary>
在在线强化学习中，数据效率是中心问题。虽然一些最近的研究已达到了几何上的最小误差，但这些结果的可行性只在大样本 regime 中保证，这意味着在使用这些算法时需要支付巨大的烧毁成本。如何在不支付任何烧毁成本的情况下实现最优误差响应是在RL理论中的开放问题。我们在具有finite-horizon不规则 Markov决策过程的上下文中解决了这个问题。我们证明了一种修改后的升权宣传（MVP）算法可以在不支付任何烧毁成本的情况下实现误差的最小化。具体来说，我们证明了MVP算法在SAH^3K sample size中的误差是(modulo log factor)最多为\begin{equation*}   \min\big\{ \sqrt{SAH^3K}, \,HK \big\}, \end{equation*}  where $S$ is the number of states, $A$ is the number of actions, $H$ is the planning horizon, and $K$ is the total number of episodes.这个误差与整个样本大小$K\geq 1$的最小误差响应相同，实际上消除了任何烧毁要求。它还翻译到一个PAC样本复杂度（即要求episode数来实现 $\varepsilon$-精度）为$\frac{SAH^3}{\varepsilon^2}$，这是最优的PAC样本复杂度。此外，我们还扩展了我们的理论，探讨了问题依赖于问题特定的量，如优值/成本和某些方差。我们的关键技术创新在于开发了一种新的误差分解策略和一种新的分析方法，用于解耦在线RL在样本匮乏 regime 中的复杂的统计依赖关系。
</details></li>
</ul>
<hr>
<h2 id="Piecewise-Linear-Functions-Representable-with-Infinite-Width-Shallow-ReLU-Neural-Networks"><a href="#Piecewise-Linear-Functions-Representable-with-Infinite-Width-Shallow-ReLU-Neural-Networks" class="headerlink" title="Piecewise Linear Functions Representable with Infinite Width Shallow ReLU Neural Networks"></a>Piecewise Linear Functions Representable with Infinite Width Shallow ReLU Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14373">http://arxiv.org/abs/2307.14373</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sarah McCarty</li>
<li>for: 这个论文研究了连续piecewise线性函数的无限宽深度学习模型，使用Rectified Linear Unit（ReLU）作为活化函数。</li>
<li>methods: 通过积分表示，这种深度学习模型可以被视为一种finite cost shallow neural network，并且可以被相应的signed,finite measure表示在适当的参数空间中。</li>
<li>results: 论文证明了 ONgie et al.的 conjecture，即任何连续piecewise线性函数都可以通过这种无限宽深度学习模型表示，而且这种表示可以被finite width shallow ReLU neural network来实现。<details>
<summary>Abstract</summary>
This paper analyzes representations of continuous piecewise linear functions with infinite width, finite cost shallow neural networks using the rectified linear unit (ReLU) as an activation function. Through its integral representation, a shallow neural network can be identified by the corresponding signed, finite measure on an appropriate parameter space. We map these measures on the parameter space to measures on the projective $n$-sphere cross $\mathbb{R}$, allowing points in the parameter space to be bijectively mapped to hyperplanes in the domain of the function. We prove a conjecture of Ongie et al. that every continuous piecewise linear function expressible with this kind of infinite width neural network is expressible as a finite width shallow ReLU neural network.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese translation:这篇论文研究了无限宽 continuous piecewise linear function的表示，使用 finite cost shallow neural network 和 rectified linear unit (ReLU)  activation function。通过积分表示，我们可以将 shallow neural network 与signed, finite measure on 相应的参数空间进行对应。然后，我们将这些度量映射到 projective $n$-sphere cross $\mathbb{R}$ 上，使得参数空间中的点可以一一映射到函数的域中的hyperplane。我们证明了 Ongie et al. 的 conjecture，即任何可表示为 infinite width neural network 的 continuous piecewise linear function都可以表示为 finite width shallow ReLU neural network。
</details></li>
</ul>
<hr>
<h2 id="Comparing-Forward-and-Inverse-Design-Paradigms-A-Case-Study-on-Refractory-High-Entropy-Alloys"><a href="#Comparing-Forward-and-Inverse-Design-Paradigms-A-Case-Study-on-Refractory-High-Entropy-Alloys" class="headerlink" title="Comparing Forward and Inverse Design Paradigms: A Case Study on Refractory High-Entropy Alloys"></a>Comparing Forward and Inverse Design Paradigms: A Case Study on Refractory High-Entropy Alloys</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13581">http://arxiv.org/abs/2307.13581</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arindam Debnath, Lavanya Raman, Wenjie Li, Adam M. Krajewski, Marcia Ahn, Shuang Lin, Shunli Shang, Allison M. Beese, Zi-Kui Liu, Wesley F. Reinhart</li>
<li>for: 本研究的目的是比较前向和反向设计模型范文在实际应用中的性能。</li>
<li>methods: 本研究使用了反向设计方法，并对其进行了对比，以评估其在不同的目标和约束下的性能。</li>
<li>results: 研究发现，反向设计方法在refractory高 entropy合金设计中表现出色，能够更好地满足不同的目标和约束。<details>
<summary>Abstract</summary>
The rapid design of advanced materials is a topic of great scientific interest. The conventional, ``forward'' paradigm of materials design involves evaluating multiple candidates to determine the best candidate that matches the target properties. However, recent advances in the field of deep learning have given rise to the possibility of an ``inverse'' design paradigm for advanced materials, wherein a model provided with the target properties is able to find the best candidate. Being a relatively new concept, there remains a need to systematically evaluate how these two paradigms perform in practical applications. Therefore, the objective of this study is to directly, quantitatively compare the forward and inverse design modeling paradigms. We do so by considering two case studies of refractory high-entropy alloy design with different objectives and constraints and comparing the inverse design method to other forward schemes like localized forward search, high throughput screening, and multi objective optimization.
</details>
<details>
<summary>摘要</summary>
rapid design of advanced materials 是科学领域中很受欢迎的话题。传统的，“前进”的材料设计方法是评估多个候选者，以确定最佳符合目标性能的候选者。然而，近年，深度学习的发展使得“反向”的材料设计方法变得可能，其中一个模型提供目标性能后，能够找到最佳候选者。由于是一个新的概念，因此还需要系统地评估这两种方法在实际应用中的性能。因此，本研究的目标是直接、量化地比较前进和反向设计模型方法。我们通过考虑高熔环境高级合金设计的两个案例研究，并与其他前进方案如本地前进搜索、高通过率检测和多目标优化进行比较。
</details></li>
</ul>
<hr>
<h2 id="Reinterpreting-survival-analysis-in-the-universal-approximator-age"><a href="#Reinterpreting-survival-analysis-in-the-universal-approximator-age" class="headerlink" title="Reinterpreting survival analysis in the universal approximator age"></a>Reinterpreting survival analysis in the universal approximator age</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13579">http://arxiv.org/abs/2307.13579</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sdittmer/survival_analysis_sumo_plus_plus">https://github.com/sdittmer/survival_analysis_sumo_plus_plus</a></li>
<li>paper_authors: Sören Dittmer, Michael Roberts, Jacobus Preller, AIX COVNET, James H. F. Rudd, John A. D. Aston, Carola-Bibiane Schönlieb</li>
<li>for: 本文旨在提供用于深度学习中survival分析的工具，以便全面利用survival分析的潜在力量。</li>
<li>methods: 本文提出了一种新的损失函数、评价指标和首个可提供survival曲线的universalapproximation网络。</li>
<li>results: 对比其他方法，该损失函数和模型在大规模数字实验中表现出色。<details>
<summary>Abstract</summary>
Survival analysis is an integral part of the statistical toolbox. However, while most domains of classical statistics have embraced deep learning, survival analysis only recently gained some minor attention from the deep learning community. This recent development is likely in part motivated by the COVID-19 pandemic. We aim to provide the tools needed to fully harness the potential of survival analysis in deep learning. On the one hand, we discuss how survival analysis connects to classification and regression. On the other hand, we provide technical tools. We provide a new loss function, evaluation metrics, and the first universal approximating network that provably produces survival curves without numeric integration. We show that the loss function and model outperform other approaches using a large numerical study.
</details>
<details>
<summary>摘要</summary>
生存分析是统计工具箱中的一个重要组成部分。然而，在классиical统计领域中，大多数领域都已经欢迎了深度学习，而生存分析则只是最近才从深度学习社区得到了一些微的关注。这种最近的发展可能受到了COVID-19大流行的影响。我们的目标是为生存分析在深度学习中充分发挥作用的工具。一方面，我们讨论了生存分析与分类和回归之间的连接。另一方面，我们提供技术工具。我们提出了一个新的损失函数、评估指标和首个可提供生存曲线的全面拟合网络，无需数值积分。我们通过大量的数据分析表明，我们的损失函数和模型在其他方法上出现较好的表现。
</details></li>
</ul>
<hr>
<h2 id="PT-mathrm-L-p-Partial-Transport-mathrm-L-p-Distances"><a href="#PT-mathrm-L-p-Partial-Transport-mathrm-L-p-Distances" class="headerlink" title="PT$\mathrm{L}^{p}$: Partial Transport $\mathrm{L}^{p}$ Distances"></a>PT$\mathrm{L}^{p}$: Partial Transport $\mathrm{L}^{p}$ Distances</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13571">http://arxiv.org/abs/2307.13571</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinran Liu, Yikun Bai, Huy Tran, Zhanqi Zhu, Matthew Thorpe, Soheil Kolouri</li>
<li>for: 本文提出了一种新的策略来比较普通的信号，即基于优化运输的partial transport $\mathrm{L}^{p}$距离。</li>
<li>methods: 本文使用了优化运输框架，并提出了partial transport $\mathrm{L}^{p}$距离作为一种新的策略来比较普通的信号。</li>
<li>results: 本文提供了partial transport $\mathrm{L}^{p}$距离的理论背景，包括优化计划的存在和距离在不同的限制下的行为。此外，本文还介绍了对这种距离的剖分变化，以及它在信号分类和最近邻近分类中的应用。<details>
<summary>Abstract</summary>
Optimal transport and its related problems, including optimal partial transport, have proven to be valuable tools in machine learning for computing meaningful distances between probability or positive measures. This success has led to a growing interest in defining transport-based distances that allow for comparing signed measures and, more generally, multi-channeled signals. Transport $\mathrm{L}^{p}$ distances are notable extensions of the optimal transport framework to signed and possibly multi-channeled signals. In this paper, we introduce partial transport $\mathrm{L}^{p}$ distances as a new family of metrics for comparing generic signals, benefiting from the robustness of partial transport distances. We provide theoretical background such as the existence of optimal plans and the behavior of the distance in various limits. Furthermore, we introduce the sliced variation of these distances, which allows for rapid comparison of generic signals. Finally, we demonstrate the application of the proposed distances in signal class separability and nearest neighbor classification.
</details>
<details>
<summary>摘要</summary>
优化交通和其相关问题，包括优化部分交通，在机器学习中证明是有用的工具来计算概率或正式推论中的有意义距离。这种成功引起了比较Transport基于距离的定义，以便比较签名的推论和更一般的多通道信号。TransportLP distances是优化交通框架中的可扩展，用于比较签名或多通道信号。在这篇论文中，我们介绍partial transportLP distances作为比较通用信号的新家族度量，受到部分交通距离的稳定性的启发。我们还提供了有关最佳计划的存在和距离的不同限制下的行为。此外，我们还介绍了这些距离的割裂变种，可以快速比较通用信号。最后，我们示出了提案的距离在信号分类和最近邻居分类中的应用。
</details></li>
</ul>
<hr>
<h2 id="Introducing-Hybrid-Modeling-with-Time-series-Transformers-A-Comparative-Study-of-Series-and-Parallel-Approach-in-Batch-Crystallization"><a href="#Introducing-Hybrid-Modeling-with-Time-series-Transformers-A-Comparative-Study-of-Series-and-Parallel-Approach-in-Batch-Crystallization" class="headerlink" title="Introducing Hybrid Modeling with Time-series-Transformers: A Comparative Study of Series and Parallel Approach in Batch Crystallization"></a>Introducing Hybrid Modeling with Time-series-Transformers: A Comparative Study of Series and Parallel Approach in Batch Crystallization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05749">http://arxiv.org/abs/2308.05749</a></li>
<li>repo_url: None</li>
<li>paper_authors: Niranjan Sitapure, Joseph S Kwon</li>
<li>For: The paper is written for the development of a first-of-a-kind, attention-based time-series transformer (TST) hybrid framework for batch crystallization, which aims to improve the accuracy and interpretability of digital twins in chemical manufacturing.* Methods: The paper uses a hybrid approach that combines first-principles physics-based dynamics with machine learning (ML) models, specifically attention-based TSTs, to capture long-term and short-term changes in process states. The authors compare two different configurations of TST-based hybrid models and evaluate their performance using normalized-mean-square-error (NMSE) and $R^2$ values.* Results: The paper reports improved accuracy and interpretability of the TST-based hybrid models compared to traditional black-box models, with NMSE values in the range of $[10, 50]\times10^{-4}$ and $R^2$ values over 0.99. The authors also demonstrate the effectiveness of the hybrid models in predicting batch crystallization processes.<details>
<summary>Abstract</summary>
Most existing digital twins rely on data-driven black-box models, predominantly using deep neural recurrent, and convolutional neural networks (DNNs, RNNs, and CNNs) to capture the dynamics of chemical systems. However, these models have not seen the light of day, given the hesitance of directly deploying a black-box tool in practice due to safety and operational issues. To tackle this conundrum, hybrid models combining first-principles physics-based dynamics with machine learning (ML) models have increased in popularity as they are considered a 'best of both worlds' approach. That said, existing simple DNN models are not adept at long-term time-series predictions and utilizing contextual information on the trajectory of the process dynamics. Recently, attention-based time-series transformers (TSTs) that leverage multi-headed attention mechanism and positional encoding to capture long-term and short-term changes in process states have shown high predictive performance. Thus, a first-of-a-kind, TST-based hybrid framework has been developed for batch crystallization, demonstrating improved accuracy and interpretability compared to traditional black-box models. Specifically, two different configurations (i.e., series and parallel) of TST-based hybrid models are constructed and compared, which show a normalized-mean-square-error (NMSE) in the range of $[10, 50]\times10^{-4}$ and an $R^2$ value over 0.99. Given the growing adoption of digital twins, next-generation attention-based hybrid models are expected to play a crucial role in shaping the future of chemical manufacturing.
</details>
<details>
<summary>摘要</summary>
现有的数字双胞虫大多采用数据驱动黑盒模型，主要使用深度循环神经网络（RNN）和卷积神经网络（CNN）来捕捉化学系统的动态。然而，这些模型尚未得到实际应用，因为直接部署黑盒工具会带来安全和运营问题。为解决这个悖论，Hybrid模型，结合物理基础知识和机器学习（ML）模型，在化学制造中得到了广泛的应用。然而，现有的简单的DNN模型不具备长期时间序列预测和利用过程动态轨迹上的 контекст信息。最近，听力基于时间序列变换器（TST），利用多头听力机制和位置编码，能够捕捉长期和短期变化的过程状态，已经显示出高预测性能。因此，一种首次实现的TST基于混合框架，在批晶凝聚过程中得到了改进的准确性和可解释性，相比传统黑盒模型。具体来说，我们构建了两种不同的配置（即串行和平行）的TST基于混合模型，其NMSE在 $[10, 50]\times10^{-4}$ 之间，$R^2$ 值超过 0.99。随着数字双胞虫的普及，未来的听力基于混合模型将在化学制造中扮演关键的角色。
</details></li>
</ul>
<hr>
<h2 id="Decision-Focused-Learning-Foundations-State-of-the-Art-Benchmark-and-Future-Opportunities"><a href="#Decision-Focused-Learning-Foundations-State-of-the-Art-Benchmark-and-Future-Opportunities" class="headerlink" title="Decision-Focused Learning: Foundations, State of the Art, Benchmark and Future Opportunities"></a>Decision-Focused Learning: Foundations, State of the Art, Benchmark and Future Opportunities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13565">http://arxiv.org/abs/2307.13565</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/predopt/predopt-benchmarks">https://github.com/predopt/predopt-benchmarks</a></li>
<li>paper_authors: Jayanta Mandi, James Kotary, Senne Berden, Maxime Mulamba, Victor Bucarey, Tias Guns, Ferdinando Fioretto</li>
<li>for: 本研究は、机器学习中的决策专注式学习（DFL）的实现方法についての综观的评估を提供。</li>
<li>methods: 本研究使用了多种integraging machine learning和优化模型的技术，包括内置式学习、迭代式学习、迭代式优化、等。</li>
<li>results: 本研究提出了一个综合性的DFL方法分类系统，并进行了广泛的实验评估。结果显示，DFL方法可以在许多不确定性下的决策任务中实现更好的性能。<details>
<summary>Abstract</summary>
Decision-focused learning (DFL) is an emerging paradigm in machine learning which trains a model to optimize decisions, integrating prediction and optimization in an end-to-end system. This paradigm holds the promise to revolutionize decision-making in many real-world applications which operate under uncertainty, where the estimation of unknown parameters within these decision models often becomes a substantial roadblock. This paper presents a comprehensive review of DFL. It provides an in-depth analysis of the various techniques devised to integrate machine learning and optimization models, introduces a taxonomy of DFL methods distinguished by their unique characteristics, and conducts an extensive empirical evaluation of these methods proposing suitable benchmark dataset and tasks for DFL. Finally, the study provides valuable insights into current and potential future avenues in DFL research.
</details>
<details>
<summary>摘要</summary>
决策驱动学习（DFL）是一种emerging paradigm在机器学习领域，它将机器学习模型训练为优化决策，并将预测和优化 integrate into an end-to-end system。这种 paradigm 承诺可以 revolutionize 决策making 在uncertainty 环境中的应用，因为在这些决策模型中 estimate unknown parameters 时常常成为一个substantial roadblock。这篇文章提供了 DFL 的全面 Review，包括了不同技术的 integrate machine learning 和优化模型的分析，并提出了 DFL 方法的分类，以及适用于 DFL 的 Benchmark 数据集和任务。最后，文章还提供了有价值的 Insight 到当前和未来 DFL 研究的方向。
</details></li>
</ul>
<hr>
<h2 id="Node-Injection-Link-Stealing-Attack"><a href="#Node-Injection-Link-Stealing-Attack" class="headerlink" title="Node Injection Link Stealing Attack"></a>Node Injection Link Stealing Attack</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13548">http://arxiv.org/abs/2307.13548</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oualid Zari, Javier Parra-Arnau, Ayşe Ünsal, Melek Önen</li>
<li>for: 攻击Graph Neural Networks（GNNs）中的隐私漏洞，泄露图形数据中的私有链接信息。</li>
<li>methods: 利用新节点加入图和API查询预测来研究隐私链接信息泄露的可能性，并提出防止隐私泄露的方法以保持模型实用性。</li>
<li>results: 对比现有方法，我们的攻击方法在推断链接信息方面表现出色，同时我们还分析了应用 differential privacy（DP）机制来mitigate攻击的影响，并研究了隐私保护和模型实用性之间的质量衡量。<details>
<summary>Abstract</summary>
In this paper, we present a stealthy and effective attack that exposes privacy vulnerabilities in Graph Neural Networks (GNNs) by inferring private links within graph-structured data. Focusing on the inductive setting where new nodes join the graph and an API is used to query predictions, we investigate the potential leakage of private edge information. We also propose methods to preserve privacy while maintaining model utility. Our attack demonstrates superior performance in inferring the links compared to the state of the art. Furthermore, we examine the application of differential privacy (DP) mechanisms to mitigate the impact of our proposed attack, we analyze the trade-off between privacy preservation and model utility. Our work highlights the privacy vulnerabilities inherent in GNNs, underscoring the importance of developing robust privacy-preserving mechanisms for their application.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种隐蔽和有效的攻击，暴露了图神经网络（GNN）中的隐私漏洞，通过推断图结构数据中的私人链接。我们在新节点加入图时的招呼设定下进行研究，并使用 API 来查询预测结果。我们发现了私人边信息泄露的可能性，并提出了保持隐私的方法，以保持模型的有用性。我们的攻击性能superior于现有的状态。此外，我们还研究了在应用 differential privacy（DP）机制时，如何减轻我们所提出的攻击的影响。我们分析了隐私保护和模型有用性之间的负担，我们的工作抛光了 GNN 中的隐私漏洞，强调了在其应用中发展Robust隐私保护机制的重要性。
</details></li>
</ul>
<hr>
<h2 id="Transfer-Learning-for-Portfolio-Optimization"><a href="#Transfer-Learning-for-Portfolio-Optimization" class="headerlink" title="Transfer Learning for Portfolio Optimization"></a>Transfer Learning for Portfolio Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13546">http://arxiv.org/abs/2307.13546</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoyang Cao, Haotian Gu, Xin Guo, Mathieu Rosenbaum</li>
<li>for: 本文探讨了通过传输学习技术解决金融 portefolio优化问题的可能性。</li>
<li>methods: 本文引入了一种新的概念 called “传输风险”，用于优化传输学习方法的优化框架。</li>
<li>results: numerical experiments 表明，传输风险与传输学习方法的总性表现之间存在强相关关系，表明传输风险为”可传输性”的可靠指标。<details>
<summary>Abstract</summary>
In this work, we explore the possibility of utilizing transfer learning techniques to address the financial portfolio optimization problem. We introduce a novel concept called "transfer risk", within the optimization framework of transfer learning. A series of numerical experiments are conducted from three categories: cross-continent transfer, cross-sector transfer, and cross-frequency transfer. In particular, 1. a strong correlation between the transfer risk and the overall performance of transfer learning methods is established, underscoring the significance of transfer risk as a viable indicator of "transferability"; 2. transfer risk is shown to provide a computationally efficient way to identify appropriate source tasks in transfer learning, enhancing the efficiency and effectiveness of the transfer learning approach; 3. additionally, the numerical experiments offer valuable new insights for portfolio management across these different settings.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们探讨了使用传输学习技术来解决金融股票优化问题的可能性。我们介绍了一种新的概念 called "传输风险"，这个概念在传输学习优化框架中被引入。我们从三类实验中进行了数据分析：跨大陆传输、跨业传输和跨频传输。具体来说，我们发现：1. 传输风险和传输学习方法的总性性能之间存在强正相关关系，这确立了传输风险作为"传输可用性"的可靠指标的重要性。2. 传输风险可以提供一种计算效率高的方法来确定合适的源任务，从而提高传输学习方法的效率和效果。3. 数据分析还提供了有价值的新视角 для股票管理在不同设置下。Here's the translation in Traditional Chinese:在这个工作中，我们探讨了使用传递学习技术来解决金融股票优化问题的可能性。我们介绍了一个新的概念 called "传递风险"，这个概念在传递学习优化框架中被引入。我们从三种类型的实验中进行了数据分析：跨大陆传递、跨业传递和跨频传递。具体来说，我们发现：1. 传递风险和传递学习方法的总性性能之间存在强正相关关系，这确立了传递风险作为"传递可用性"的可靠指标的重要性。2. 传递风险可以提供一种计算效率高的方法来决定合适的源任务，从而提高传递学习方法的效率和效果。3. 数据分析还提供了有价值的新视角 для股票管理在不同设置下。
</details></li>
</ul>
<hr>
<h2 id="A-model-for-efficient-dynamical-ranking-in-networks"><a href="#A-model-for-efficient-dynamical-ranking-in-networks" class="headerlink" title="A model for efficient dynamical ranking in networks"></a>A model for efficient dynamical ranking in networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13544">http://arxiv.org/abs/2307.13544</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrea Della Vecchia, Kibidi Neocosmos, Daniel B. Larremore, Cristopher Moore, Caterina De Bacco</li>
<li>for: 这篇论文旨在提出一种物理启发的方法，用于在指定的时间网络中INFER动态排名。</li>
<li>methods: 该方法是解一个线性方程系统，只需要一个参数调整。</li>
<li>results: 经测试，该方法可以更好地预测交互（边的存在）和其结果（边的方向），在许多情况下表现比较出色。<details>
<summary>Abstract</summary>
We present a physics-inspired method for inferring dynamic rankings in directed temporal networks - networks in which each directed and timestamped edge reflects the outcome and timing of a pairwise interaction. The inferred ranking of each node is real-valued and varies in time as each new edge, encoding an outcome like a win or loss, raises or lowers the node's estimated strength or prestige, as is often observed in real scenarios including sequences of games, tournaments, or interactions in animal hierarchies. Our method works by solving a linear system of equations and requires only one parameter to be tuned. As a result, the corresponding algorithm is scalable and efficient. We test our method by evaluating its ability to predict interactions (edges' existence) and their outcomes (edges' directions) in a variety of applications, including both synthetic and real data. Our analysis shows that in many cases our method's performance is better than existing methods for predicting dynamic rankings and interaction outcomes.
</details>
<details>
<summary>摘要</summary>
我们提出一种物理启发的方法估算直接时间网络中的动态排名 - 直接时间网络中每个Directed和时间戳的边都表示对话的结果和时间，例如赢利或失败。我们的方法会解决一个线性方程系统，只需要一个参数调整，因此算法可扩展和高效。我们对多种应用进行测试，包括 sintetic 数据和实际数据，并发现我们的方法在许多情况下的性能比现有方法更高。
</details></li>
</ul>
<hr>
<h2 id="Model-Calibration-in-Dense-Classification-with-Adaptive-Label-Perturbation"><a href="#Model-Calibration-in-Dense-Classification-with-Adaptive-Label-Perturbation" class="headerlink" title="Model Calibration in Dense Classification with Adaptive Label Perturbation"></a>Model Calibration in Dense Classification with Adaptive Label Perturbation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13539">http://arxiv.org/abs/2307.13539</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/carlisle-liu/aslp">https://github.com/carlisle-liu/aslp</a></li>
<li>paper_authors: Jiawei Liu, Changkun Ye, Shan Wang, Ruikai Cui, Jing Zhang, Kaihao Zhang, Nick Barnes</li>
<li>for: 本研究旨在提高深度神经网络的准确率和信任度，以便在安全应用中使用。</li>
<li>methods: 本文提出了一种名为 Adaptive Stochastic Label Perturbation（ASLP）的方法，它可以学习每个训练图像的唯一标签杂化水平。ASLP使用的是我们提出的 Self-Calibrating Binary Cross Entropy（SC-BCE）损失函数，它将杂化过程、标签杂化和标签平滑等进程集成到一起，以达到更好的准确率和信任度。</li>
<li>results: 对比于传统的 dense binary classification 模型，ASLP 可以显著提高模型的准确率和信任度。在 known data 上保持 классификация精度作为保守解决方案，或者特定地改进模型的准确率和信任度。<details>
<summary>Abstract</summary>
For safety-related applications, it is crucial to produce trustworthy deep neural networks whose prediction is associated with confidence that can represent the likelihood of correctness for subsequent decision-making. Existing dense binary classification models are prone to being over-confident. To improve model calibration, we propose Adaptive Stochastic Label Perturbation (ASLP) which learns a unique label perturbation level for each training image. ASLP employs our proposed Self-Calibrating Binary Cross Entropy (SC-BCE) loss, which unifies label perturbation processes including stochastic approaches (like DisturbLabel), and label smoothing, to correct calibration while maintaining classification rates. ASLP follows Maximum Entropy Inference of classic statistical mechanics to maximise prediction entropy with respect to missing information. It performs this while: (1) preserving classification accuracy on known data as a conservative solution, or (2) specifically improves model calibration degree by minimising the gap between the prediction accuracy and expected confidence of the target training label. Extensive results demonstrate that ASLP can significantly improve calibration degrees of dense binary classification models on both in-distribution and out-of-distribution data. The code is available on https://github.com/Carlisle-Liu/ASLP.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>保持知道数据上的分类精度作为保守解决方案，或2. 特别是提高模型准确率和预期确信度之间的差异，以iminimize the gap between the prediction accuracy and expected confidence of the target training label.Extensive results demonstrate that ASLP can significantly improve calibration degrees of dense binary classification models on both in-distribution and out-of-distribution data. The code is available on <a target="_blank" rel="noopener" href="https://github.com/Carlisle-Liu/ASLP">https://github.com/Carlisle-Liu/ASLP</a>.</details></li>
</ol>
<hr>
<h2 id="INFINITY-Neural-Field-Modeling-for-Reynolds-Averaged-Navier-Stokes-Equations"><a href="#INFINITY-Neural-Field-Modeling-for-Reynolds-Averaged-Navier-Stokes-Equations" class="headerlink" title="INFINITY: Neural Field Modeling for Reynolds-Averaged Navier-Stokes Equations"></a>INFINITY: Neural Field Modeling for Reynolds-Averaged Navier-Stokes Equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13538">http://arxiv.org/abs/2307.13538</a></li>
<li>repo_url: None</li>
<li>paper_authors: Louis Serrano, Leon Migus, Yuan Yin, Jocelyn Ahmed Mazari, Patrick Gallinari</li>
<li>for: 这篇论文的目的是提出一种基于深度学习的精准仿真模型，用于简化物理现象的计算。</li>
<li>methods: 该方法使用含义 Neil 表示（INRs）来地址这个挑战，将物理场景的几何信息和物理场景编码成紧凑的表示，然后学习这些表示之间的映射，以便从物理场景中推断物理场景。</li>
<li>results: 在一个空气foil设计优化问题中，该方法达到了当前最佳性能，准确地预测了物理场景中的场景和表面上的物理场景。此外，该方法还可以在设计探索和形状优化等应用中使用，并能够正确预测拖拽和升力系数。<details>
<summary>Abstract</summary>
For numerical design, the development of efficient and accurate surrogate models is paramount. They allow us to approximate complex physical phenomena, thereby reducing the computational burden of direct numerical simulations. We propose INFINITY, a deep learning model that utilizes implicit neural representations (INRs) to address this challenge. Our framework encodes geometric information and physical fields into compact representations and learns a mapping between them to infer the physical fields. We use an airfoil design optimization problem as an example task and we evaluate our approach on the challenging AirfRANS dataset, which closely resembles real-world industrial use-cases. The experimental results demonstrate that our framework achieves state-of-the-art performance by accurately inferring physical fields throughout the volume and surface. Additionally we demonstrate its applicability in contexts such as design exploration and shape optimization: our model can correctly predict drag and lift coefficients while adhering to the equations.
</details>
<details>
<summary>摘要</summary>
für numerische Entwurfsdesign ist die Entwicklung effizienter und genauer surrogatmodelle von entscheidender Bedeutung. Sie ermöglichen uns, komplexe physikalische Phänomene zu approximiieren, somit die computermonierte Last von direkten numerischen Simulationen zu reduzieren. Wir schlagen INFINITY vor, ein tiefes lernendes Modell, das impliciten neuronalen Darstellungen (INRs) nutzt, um diese Herausforderung zu meistern. Unser Framework kodiert geometrische Informationen und physikalische Felder in compacten Darstellungen und lernt eine Abbildung zwischen ihnen, um die physikalischen Felder zu infolgen. Wir verwenden ein airfoil-Design-Optimierungstask als Beispielaufgabe und bewerten unsere Methode am schwierigen AirfRANS-Datensatz, der eng mit realen industriellen Anwendungen verwandt ist. Die experimentellen Ergebnisse zeigen, dass unsere Methode einen neuen Standardsatz erreicht, indem sie physikalische Felder in Volumen und Oberfläche genau approximiiert. Wir demonstrieren ferner ihre Anwendbarkeit in Kontexten wie Design-Exploration und Form-Optimierung: unser Modell kann richtig druck- und liftkoeffizienten vorhersagen, ohne die Gleichungen zu verletzen.
</details></li>
</ul>
<hr>
<h2 id="Do-algorithms-and-barriers-for-sparse-principal-component-analysis-extend-to-other-structured-settings"><a href="#Do-algorithms-and-barriers-for-sparse-principal-component-analysis-extend-to-other-structured-settings" class="headerlink" title="Do algorithms and barriers for sparse principal component analysis extend to other structured settings?"></a>Do algorithms and barriers for sparse principal component analysis extend to other structured settings?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13535">http://arxiv.org/abs/2307.13535</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guanyi Wang, Mengqi Lou, Ashwin Pananjady</li>
<li>for: 本研究探讨了带有杂质模型的主成分分析问题，使用 union-of-subspace 模型捕捉信号结构。</li>
<li>methods: 本研究使用了统计和计算方法，并设立了基本的限制，证明了一种自然的投影力方法在特定情况下展现了局部 converges 性。</li>
<li>results: 研究发现，一些对于普通稀缺 PCA 的现象也适用于其结构化版本。<details>
<summary>Abstract</summary>
We study a principal component analysis problem under the spiked Wishart model in which the structure in the signal is captured by a class of union-of-subspace models. This general class includes vanilla sparse PCA as well as its variants with graph sparsity. With the goal of studying these problems under a unified statistical and computational lens, we establish fundamental limits that depend on the geometry of the problem instance, and show that a natural projected power method exhibits local convergence to the statistically near-optimal neighborhood of the solution. We complement these results with end-to-end analyses of two important special cases given by path and tree sparsity in a general basis, showing initialization methods and matching evidence of computational hardness. Overall, our results indicate that several of the phenomena observed for vanilla sparse PCA extend in a natural fashion to its structured counterparts.
</details>
<details>
<summary>摘要</summary>
我们研究一个主成分分析问题，其中信号结构是由一类联合子空间模型捕捉的。这个总类包括普通稀畴PCA以及其变体具有图稀畴。为了在统一的统计和计算镜头下研究这些问题，我们确立了基本的限制，并证明自然的投影力方法在统计上准确的邻居解决方案附近进行本地准确。我们补充了一些重要的特殊情况分析，包括路径和树稀畴在一般基础上，并提供了初始化方法和匹配证明的计算困难。总之，我们的结果表明，许多vanilla sparse PCA的现象在其结构化对应中也有自然的扩展。
</details></li>
</ul>
<hr>
<h2 id="Differentiable-Turbulence-II"><a href="#Differentiable-Turbulence-II" class="headerlink" title="Differentiable Turbulence II"></a>Differentiable Turbulence II</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13533">http://arxiv.org/abs/2307.13533</a></li>
<li>repo_url: None</li>
<li>paper_authors: Varun Shankar, Romit Maulik, Venkatasubramanian Viswanathan</li>
<li>for:  This paper is written for developing data-driven models in computational fluid dynamics (CFD) using differentiable fluid simulators and machine learning (ML) methods.</li>
<li>methods: The paper proposes a framework for integrating deep learning models into a generic finite element numerical scheme for solving the Navier-Stokes equations, and applies the technique to learn a sub-grid scale closure using a multi-scale graph neural network.</li>
<li>results: The learned closure can achieve accuracy comparable to traditional large eddy simulation on a finer grid, resulting in an equivalent speedup of 10x. The method has been demonstrated on several realizations of flow over a backwards-facing step, testing on both unseen Reynolds numbers and new geometry.<details>
<summary>Abstract</summary>
Differentiable fluid simulators are increasingly demonstrating value as useful tools for developing data-driven models in computational fluid dynamics (CFD). Differentiable turbulence, or the end-to-end training of machine learning (ML) models embedded in CFD solution algorithms, captures both the generalization power and limited upfront cost of physics-based simulations, and the flexibility and automated training of deep learning methods. We develop a framework for integrating deep learning models into a generic finite element numerical scheme for solving the Navier-Stokes equations, applying the technique to learn a sub-grid scale closure using a multi-scale graph neural network. We demonstrate the method on several realizations of flow over a backwards-facing step, testing on both unseen Reynolds numbers and new geometry. We show that the learned closure can achieve accuracy comparable to traditional large eddy simulation on a finer grid that amounts to an equivalent speedup of 10x. As the desire and need for cheaper CFD simulations grows, we see hybrid physics-ML methods as a path forward to be exploited in the near future.
</details>
<details>
<summary>摘要</summary>
diferenciable 流体 simulator 在 Computational Fluid Dynamics (CFD) 中展示了越来越多的价值，作为数据驱动模型的有用工具。 diferenciable turbulence，也就是在 CFD 解决方案算法中嵌入机器学习 (ML) 模型的终端训练，捕捉了物理基础的通用力和初始成本的限制，以及深度学习方法的自动训练和灵活性。我们开发了一个抽象 Finite Element 数学模型的框架，将深度学习模型集成到 Navier-Stokes 方程中，并使用多尺度图 neural network 来学习子网格抑制。我们在不同的 Reynolds 数和新geometry 上进行了多个实现，并示出了学习 closure 可以与传统大 Eddy simulation 相当的精度相比，在一个等效的加速10倍的粗网上。随着 CFD  simulation 的成本下降的需求和需求，我们认为 hybrid physics-ML 方法将在未来被利用。
</details></li>
</ul>
<hr>
<h2 id="Towards-Long-Term-predictions-of-Turbulence-using-Neural-Operators"><a href="#Towards-Long-Term-predictions-of-Turbulence-using-Neural-Operators" class="headerlink" title="Towards Long-Term predictions of Turbulence using Neural Operators"></a>Towards Long-Term predictions of Turbulence using Neural Operators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13517">http://arxiv.org/abs/2307.13517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fernando Gonzalez, François-Xavier Demoulin, Simon Bernard</li>
<li>for: 这 paper 探讨了使用神经操作符来预测湍流，主要关注于欧姆 neural 操作符（FNO）模型。它的目的是通过机器学习来开发减少的order&#x2F;代理模型，以便为湍流计算 simulate 提供更好的估算。</li>
<li>methods: 这 paper 使用了不同的模型配置，包括 U-NET 结构（UNO 和 U-FNET），并发现这些结构在准确性和稳定性方面表现更好于标准 FNO。U-FNET 在更高的 Reynolds 数下预测湍流的能力更高。使用梯度和稳定性损失来保证模型的稳定和准确预测。</li>
<li>results: 这 paper 发现，使用不同的模型配置和梯度损失可以获得更好的预测结果。特别是，U-FNET 在更高的 Reynolds 数下预测湍流的能力更高。然而，为了更好地评估深度学习模型在液流预测中的性能，还需要开发更好的评价指标。<details>
<summary>Abstract</summary>
This paper explores Neural Operators to predict turbulent flows, focusing on the Fourier Neural Operator (FNO) model. It aims to develop reduced-order/surrogate models for turbulent flow simulations using Machine Learning. Different model configurations are analyzed, with U-NET structures (UNO and U-FNET) performing better than the standard FNO in accuracy and stability. U-FNET excels in predicting turbulence at higher Reynolds numbers. Regularization terms, like gradient and stability losses, are essential for stable and accurate predictions. The study emphasizes the need for improved metrics for deep learning models in fluid flow prediction. Further research should focus on models handling complex flows and practical benchmarking metrics.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="An-Empirical-Study-on-Fairness-Improvement-with-Multiple-Protected-Attributes"><a href="#An-Empirical-Study-on-Fairness-Improvement-with-Multiple-Protected-Attributes" class="headerlink" title="An Empirical Study on Fairness Improvement with Multiple Protected Attributes"></a>An Empirical Study on Fairness Improvement with Multiple Protected Attributes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01923">http://arxiv.org/abs/2308.01923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenpeng Chen, Jie M. Zhang, Federica Sarro, Mark Harman</li>
<li>for: 本研究旨在探讨多个保护特征的公平改进策略的效果，以帮助更好地理解多特征公平改进策略的性能。</li>
<li>methods: 本研究使用了11种最新的公平改进方法，包括对多个保护特征进行公平改进。我们使用不同的数据集、度量和机器学习模型来分析这些方法的效果。</li>
<li>results: 研究发现，为一个保护特征进行公平改进可能会导致其他保护特征的公平性下降，这种下降的比例可达88.3%（平均为57.5%）。同时，我们发现在考虑多个保护特征时，精度和准确率的影响相对较小，但是 recall 的影响相对较大。这些结论有重要意义，因为现有的研究通常只报告精度作为机器学习性能指标，这并不充分。<details>
<summary>Abstract</summary>
Existing research mostly improves the fairness of Machine Learning (ML) software regarding a single protected attribute at a time, but this is unrealistic given that many users have multiple protected attributes. This paper conducts an extensive study of fairness improvement regarding multiple protected attributes, covering 11 state-of-the-art fairness improvement methods. We analyze the effectiveness of these methods with different datasets, metrics, and ML models when considering multiple protected attributes. The results reveal that improving fairness for a single protected attribute can largely decrease fairness regarding unconsidered protected attributes. This decrease is observed in up to 88.3% of scenarios (57.5% on average). More surprisingly, we find little difference in accuracy loss when considering single and multiple protected attributes, indicating that accuracy can be maintained in the multiple-attribute paradigm. However, the effect on precision and recall when handling multiple protected attributes is about 5 times and 8 times that of a single attribute. This has important implications for future fairness research: reporting only accuracy as the ML performance metric, which is currently common in the literature, is inadequate.
</details>
<details>
<summary>摘要</summary>
现有研究主要是在单个保护属性上提高机器学习软件的公平性，但这是不切实际的，因为用户通常有多个保护属性。这篇论文进行了对多个保护属性的公平性提高方法的广泛研究，涵盖了11种现状最佳实践。我们对不同的数据集、 метри和机器学习模型进行了这些方法的分析，并评估了它们在考虑多个保护属性时的效果。结果表明，只考虑一个保护属性进行公平性提高可能会导致其他保护属性的不公平性减少，这种减少率在88.3%的场景中（57.5%的平均值）被观察到。更有趣的是，考虑单个和多个保护属性时，准确性损失的差异很小，这表示在多属性情况下，准确性可以维持。然而，处理多个保护属性时的精度和回归的影响相对较大，大约是单个属性的5倍和8倍。这有重要的意义，未来公平性研究应该不再仅仅是通过准确性来评估机器学习软件的性能。
</details></li>
</ul>
<hr>
<h2 id="Continuous-Time-Evidential-Distributions-for-Irregular-Time-Series"><a href="#Continuous-Time-Evidential-Distributions-for-Irregular-Time-Series" class="headerlink" title="Continuous Time Evidential Distributions for Irregular Time Series"></a>Continuous Time Evidential Distributions for Irregular Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13503">http://arxiv.org/abs/2307.13503</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/twkillian/edict">https://github.com/twkillian/edict</a></li>
<li>paper_authors: Taylor W. Killian, Haoran Zhang, Thomas Hartvigsen, Ava P. Amini</li>
<li>for: 这篇论文是用于描述一种为健康照顾等实际应用场景中的不规则时间序列进行预测的方法。</li>
<li>methods: 这篇论文使用了一种名为EDICT的策略，它 learns一个证据分布来描述不规则时间序列。这个分布可以在不同的时间点进行不同的推论，并且可以在缺乏观察的情况下提供更好的uncertainty estimation。</li>
<li>results: 这篇论文的结果显示，EDICT可以在具有挑战性的时间序列分类任务中实现竞争性的表现，并且可以在缺乏观察的情况下提供更好的uncertainty-guided推论。<details>
<summary>Abstract</summary>
Prevalent in many real-world settings such as healthcare, irregular time series are challenging to formulate predictions from. It is difficult to infer the value of a feature at any given time when observations are sporadic, as it could take on a range of values depending on when it was last observed. To characterize this uncertainty we present EDICT, a strategy that learns an evidential distribution over irregular time series in continuous time. This distribution enables well-calibrated and flexible inference of partially observed features at any time of interest, while expanding uncertainty temporally for sparse, irregular observations. We demonstrate that EDICT attains competitive performance on challenging time series classification tasks and enabling uncertainty-guided inference when encountering noisy data.
</details>
<details>
<summary>摘要</summary>
广泛存在在现实世界中的应用场景，如医疗、财经等，不规则时间序列是预测的挑战。因为观察是间歇的，特征值的推测是具有uncertainty的，可能在不同的时间点 prendre on a range of values。为了捕捉这种uncertainty，我们提出了EDICT策略，它在连续时间中学习不规则时间序列的证据分布。这种分布允许在任何时间点进行高度抽象和灵活的特征值推测，同时在笼统观察中扩展uncertainty。我们示例了EDICT在具有噪声数据的时间序列分类任务中的竞争性表现和uncertainty导航能力。
</details></li>
</ul>
<hr>
<h2 id="Deep-Reinforcement-Learning-for-Robust-Goal-Based-Wealth-Management"><a href="#Deep-Reinforcement-Learning-for-Robust-Goal-Based-Wealth-Management" class="headerlink" title="Deep Reinforcement Learning for Robust Goal-Based Wealth Management"></a>Deep Reinforcement Learning for Robust Goal-Based Wealth Management</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13501">http://arxiv.org/abs/2307.13501</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tessa Bauman, Bruno Gašperov, Stjepan Begušić, Zvonko Kostanjčar</li>
<li>for: 这个研究旨在提出一种基于深度强化学习的 Robust Goal-Based Wealth Management 方法，以便实现特定金融目标。</li>
<li>methods: 本研究使用了深度强化学习技术来估算投资选择，并通过训练一个对应投资策略的神经网络来实现目标。</li>
<li>results: 实验结果显示，该方法比较多的目标评估和投资策略选择方法来得到更好的效果，并在实际市场数据上显示出优越性。<details>
<summary>Abstract</summary>
Goal-based investing is an approach to wealth management that prioritizes achieving specific financial goals. It is naturally formulated as a sequential decision-making problem as it requires choosing the appropriate investment until a goal is achieved. Consequently, reinforcement learning, a machine learning technique appropriate for sequential decision-making, offers a promising path for optimizing these investment strategies. In this paper, a novel approach for robust goal-based wealth management based on deep reinforcement learning is proposed. The experimental results indicate its superiority over several goal-based wealth management benchmarks on both simulated and historical market data.
</details>
<details>
<summary>摘要</summary>
目的基本投资是一种财务管理方法，强调达到特定的金融目标。这是一个顺序决策问题，因为需要选择适当的投资直到达到目标。因此，深度回归学习，一种适合顺序决策的机器学习技术，对于优化这些投资策略表现出了承诺。在这篇论文中，一种基于深度回归学习的新方法 дляrobust目的基本财务管理被提议。实验结果表明，这种方法在模拟和历史市场数据上都超过了多个目的基本财务管理参考标准。
</details></li>
</ul>
<hr>
<h2 id="Finding-Money-Launderers-Using-Heterogeneous-Graph-Neural-Networks"><a href="#Finding-Money-Launderers-Using-Heterogeneous-Graph-Neural-Networks" class="headerlink" title="Finding Money Launderers Using Heterogeneous Graph Neural Networks"></a>Finding Money Launderers Using Heterogeneous Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13499">http://arxiv.org/abs/2307.13499</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fredjo89/heterogeneous-mpnn">https://github.com/fredjo89/heterogeneous-mpnn</a></li>
<li>paper_authors: Fredrik Johannessen, Martin Jullum</li>
<li>for: 本研究旨在提高银行电子监测系统的检测犯罪吸收能力，使用机器学习方法对大规模不同类型数据图进行分析。</li>
<li>methods: 本研究使用图神经网络（GNN）方法，对实际世界银行交易和企业角色数据构建的大型不同类型数据图进行分析。特别是，我们对现有的同质GNN方法（Message Passing Neural Network，MPNN）进行扩展，使其在不同类型数据图上有效运行。我们还提出了一种新的消息汇聚方法，以便在不同边的消息之间进行效果地汇聚。</li>
<li>results: 我们的模型实现了在大规模不同类型数据图上对犯罪吸收进行有效检测，提高了银行电子监测系统的检测精度。这是首次将GNN应用于实际世界大规模不同类型数据图进行反洗钱检测，我们的研究成果具有广泛的应用前景。<details>
<summary>Abstract</summary>
Current anti-money laundering (AML) systems, predominantly rule-based, exhibit notable shortcomings in efficiently and precisely detecting instances of money laundering. As a result, there has been a recent surge toward exploring alternative approaches, particularly those utilizing machine learning. Since criminals often collaborate in their money laundering endeavors, accounting for diverse types of customer relations and links becomes crucial. In line with this, the present paper introduces a graph neural network (GNN) approach to identify money laundering activities within a large heterogeneous network constructed from real-world bank transactions and business role data belonging to DNB, Norway's largest bank. Specifically, we extend the homogeneous GNN method known as the Message Passing Neural Network (MPNN) to operate effectively on a heterogeneous graph. As part of this procedure, we propose a novel method for aggregating messages across different edges of the graph. Our findings highlight the importance of using an appropriate GNN architecture when combining information in heterogeneous graphs. The performance results of our model demonstrate great potential in enhancing the quality of electronic surveillance systems employed by banks to detect instances of money laundering. To the best of our knowledge, this is the first published work applying GNN on a large real-world heterogeneous network for anti-money laundering purposes.
</details>
<details>
<summary>摘要</summary>
现有的反贩卖财 (AML) 系统，主要基于规则，显示出明显的缺陷，无法有效地和精准地检测贩卖财活动。因此，有一些最新的研究尝试使用机器学习方法。由于别派犯罪分子通常会合作，在检测贩卖财活动时，考虑到不同类型的客户关系和链接变得非常重要。针对这一点，本文提出了一种基于图神经网络 (GNN) 的方法，用于在大规模不同类型图中检测贩卖财活动。具体来说，我们将已知的同类GNN方法——消息传递神经网络 (MPNN)——修改以适应不同类型图。在这个过程中，我们提出了一种新的消息汇聚方法，用于在不同的图边缘上进行消息汇聚。我们的发现表明，在组合不同类型图的情况下，使用适当的 GNN 建筑可以提高电子监测系统的检测贩卖财活动质量。我们知道，这是首次在实际世界上大规模不同类型图上应用 GNN 的反贩卖财研究。
</details></li>
</ul>
<hr>
<h2 id="Zshot-An-Open-source-Framework-for-Zero-Shot-Named-Entity-Recognition-and-Relation-Extraction"><a href="#Zshot-An-Open-source-Framework-for-Zero-Shot-Named-Entity-Recognition-and-Relation-Extraction" class="headerlink" title="Zshot: An Open-source Framework for Zero-Shot Named Entity Recognition and Relation Extraction"></a>Zshot: An Open-source Framework for Zero-Shot Named Entity Recognition and Relation Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13497">http://arxiv.org/abs/2307.13497</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriele Picco, Marcos Martínez Galindo, Alberto Purpura, Leopold Fuchs, Vanessa López, Hoang Thanh Lam</li>
<li>For: The paper is written for researchers and industry professionals who are interested in zero-shot learning (ZSL) and its applications in natural language processing (NLP).* Methods: The paper proposes a novel ZSL framework called Zshot, which aims to address the challenges of ZSL by providing a platform for comparing different state-of-the-art ZSL methods with standard benchmark datasets. The framework also includes readily available APIs for production under the standard SpaCy NLP pipeline, and it is designed to be extendible and evaluable.* Results: The paper does not provide specific results, but it aims to provide a platform for comparing different ZSL methods and evaluating their performance on standard benchmark datasets. The authors also include numerous enhancements such as pipeline ensembling and visualization utilities available as a SpaCy extension.<details>
<summary>Abstract</summary>
The Zero-Shot Learning (ZSL) task pertains to the identification of entities or relations in texts that were not seen during training. ZSL has emerged as a critical research area due to the scarcity of labeled data in specific domains, and its applications have grown significantly in recent years. With the advent of large pretrained language models, several novel methods have been proposed, resulting in substantial improvements in ZSL performance. There is a growing demand, both in the research community and industry, for a comprehensive ZSL framework that facilitates the development and accessibility of the latest methods and pretrained models.In this study, we propose a novel ZSL framework called Zshot that aims to address the aforementioned challenges. Our primary objective is to provide a platform that allows researchers to compare different state-of-the-art ZSL methods with standard benchmark datasets. Additionally, we have designed our framework to support the industry with readily available APIs for production under the standard SpaCy NLP pipeline. Our API is extendible and evaluable, moreover, we include numerous enhancements such as boosting the accuracy with pipeline ensembling and visualization utilities available as a SpaCy extension.
</details>
<details>
<summary>摘要</summary>
zero-shot learning (ZSL) 任务是指在训练过程中未看过的文本中预测实体或关系。 ZSL 已成为一个重要的研究领域，因为特定领域的标注数据稀缺，而其应用也在过去几年内有所增长。随着大型预训语言模型的出现，许多新的方法被提出，导致 ZSL 性能得到了显著提高。在研究 сообществе和industry 中，有一个增长的需求，即开发一个通用的 ZSL 框架，以便开发和访问最新的方法和预训模型。在这项研究中，我们提出了一个新的 ZSL 框架，称为 Zshot。我们的主要目标是提供一个平台，允许研究人员比较不同的状态艺术 ZSL 方法，并使用标准的 benchmark 数据集进行比较。此外，我们设计了我们的框架，以支持产业，并提供了可靠的 SpaCy NLP 管道中的 API。我们的 API 可扩展和评估，并且包括了多种改进，例如将管道 ensemble 提高准确性，以及可用于 SpaCy 扩展的可视化工具。
</details></li>
</ul>
<hr>
<h2 id="Duet-efficient-and-scalable-hybriD-neUral-rElation-undersTanding"><a href="#Duet-efficient-and-scalable-hybriD-neUral-rElation-undersTanding" class="headerlink" title="Duet: efficient and scalable hybriD neUral rElation undersTanding"></a>Duet: efficient and scalable hybriD neUral rElation undersTanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13494">http://arxiv.org/abs/2307.13494</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/GIS-PuppetMaster/Duet">https://github.com/GIS-PuppetMaster/Duet</a></li>
<li>paper_authors: Kaixin Zhang, Hongzhi Wang, Yabin Lu, Ziqi Li, Chang Shu, Yu Yan, Donghua Yang</li>
<li>for: 本研究旨在解决learned cardinality estimation方法中的数据和工作负荷飘移问题，以及高纬度和高维度表中Cardinality estimator的应用问题。</li>
<li>methods: 本文提出了一种基于预测模型的hybrid方法，named Duet，它可以直接估计 cardinality  без采样或任何非�ifferentiable过程，并且可以提高高纬度和高维度表中Cardinality estimator的准确性。</li>
<li>results: 实验结果表明，Duet可以实现所有设计目标，并且在CPU上比较多学方法更加实用，甚至在GPU上也具有较低的推理成本。<details>
<summary>Abstract</summary>
Learned cardinality estimation methods have achieved high precision compared to traditional methods. Among learned methods, query-driven approaches face the data and workload drift problem for a long time. Although both query-driven and hybrid methods are proposed to avoid this problem, even the state-of-the-art of them suffer from high training and estimation costs, limited scalability, instability, and long-tailed distribution problem on high cardinality and high-dimensional tables, which seriously affects the practical application of learned cardinality estimators. In this paper, we prove that most of these problems are directly caused by the widely used progressive sampling. We solve this problem by introducing predicates information into the autoregressive model and propose Duet, a stable, efficient, and scalable hybrid method to estimate cardinality directly without sampling or any non-differentiable process, which can not only reduces the inference complexity from O(n) to O(1) compared to Naru and UAE but also achieve higher accuracy on high cardinality and high-dimensional tables. Experimental results show that Duet can achieve all the design goals above and be much more practical and even has a lower inference cost on CPU than that of most learned methods on GPU.
</details>
<details>
<summary>摘要</summary>
现有学习Cardinality estimation方法已经实现了高精度比 tradicional方法。 Among these learned methods, query-driven approaches have faced the data and workload drift problem for a long time. Although both query-driven and hybrid methods have been proposed to avoid this problem, even the state-of-the-art of them suffer from high training and estimation costs, limited scalability, instability, and long-tailed distribution problem on high cardinality and high-dimensional tables, which seriously affects the practical application of learned cardinality estimators.在这篇论文中，我们证明了大多数这些问题是由广泛使用进度 sampling 所导致的。 We solve this problem by introducing predicates information into the autoregressive model and propose Duet, a stable, efficient, and scalable hybrid method to estimate cardinality directly without sampling or any non-differentiable process, which can not only reduce the inference complexity from O(n) to O(1) compared to Naru and UAE but also achieve higher accuracy on high cardinality and high-dimensional tables. Experimental results show that Duet can achieve all the design goals above and be much more practical and even has a lower inference cost on CPU than that of most learned methods on GPU.
</details></li>
</ul>
<hr>
<h2 id="ECG-classification-using-Deep-CNN-and-Gramian-Angular-Field"><a href="#ECG-classification-using-Deep-CNN-and-Gramian-Angular-Field" class="headerlink" title="ECG classification using Deep CNN and Gramian Angular Field"></a>ECG classification using Deep CNN and Gramian Angular Field</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02395">http://arxiv.org/abs/2308.02395</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youssef Elmir, Yassine Himeur, Abbes Amira</li>
<li>for: 这个研究提供了一种新的ECG信号分析方法，用于心血管疾病诊断和异常检测。</li>
<li>methods: 该方法基于将时域1D вектор转换为2D图像使用 Gramian Angular Field transform，并使用卷积神经网络（CNN）进行分类。</li>
<li>results: 实验结果显示，提出的方法可以达到97.47%和98.65%的分类精度，并且可以辨别和可视化ECG信号中的时间特征，如心率、心音和信号形态变化，这些变化可能不可见于原始信号中。<details>
<summary>Abstract</summary>
This paper study provides a novel contribution to the field of signal processing and DL for ECG signal analysis by introducing a new feature representation method for ECG signals. The proposed method is based on transforming time frequency 1D vectors into 2D images using Gramian Angular Field transform. Moving on, the classification of the transformed ECG signals is performed using Convolutional Neural Networks (CNN). The obtained results show a classification accuracy of 97.47% and 98.65% for anomaly detection. Accordingly, in addition to improving the classification performance compared to the state-of-the-art, the feature representation helps identify and visualize temporal patterns in the ECG signal, such as changes in heart rate, rhythm, and morphology, which may not be apparent in the original signal. This has significant implications in the diagnosis and treatment of cardiovascular diseases and detection of anomalies.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Rational-kernel-based-interpolation-for-complex-valued-frequency-response-functions"><a href="#Rational-kernel-based-interpolation-for-complex-valued-frequency-response-functions" class="headerlink" title="Rational kernel-based interpolation for complex-valued frequency response functions"></a>Rational kernel-based interpolation for complex-valued frequency response functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13484">http://arxiv.org/abs/2307.13484</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stk-kriging/complex-rational-interpolation">https://github.com/stk-kriging/complex-rational-interpolation</a></li>
<li>paper_authors: Julien Bect, Niklas Georg, Ulrich Römer, Sebastian Schöps</li>
<li>for: 这个论文关注了使用kernel方法估计复杂数值函数的问题，特别是在频域中的频谱响应函数。</li>
<li>methods: 这篇论文使用了kernel方法，但标准kernel并不perform well。作者引入了新的复杂数值函数抽象空间，并将问题转化为最小二乘问题在这些空间中。此外，作者还结合了一个低阶racional函数，其阶数由一个新的模型选择 criterion 来动态选择。</li>
<li>results: 作者的方法在不同领域的实例中进行了数值实验，包括电磁学和声学实例。比较 Result 与可用的racionalapproximation方法，这种方法的性能很高。<details>
<summary>Abstract</summary>
This work is concerned with the kernel-based approximation of a complex-valued function from data, where the frequency response function of a partial differential equation in the frequency domain is of particular interest. In this setting, kernel methods are employed more and more frequently, however, standard kernels do not perform well. Moreover, the role and mathematical implications of the underlying pair of kernels, which arises naturally in the complex-valued case, remain to be addressed. We introduce new reproducing kernel Hilbert spaces of complex-valued functions, and formulate the problem of complex-valued interpolation with a kernel pair as minimum norm interpolation in these spaces. Moreover, we combine the interpolant with a low-order rational function, where the order is adaptively selected based on a new model selection criterion. Numerical results on examples from different fields, including electromagnetics and acoustic examples, illustrate the performance of the method, also in comparison to available rational approximation methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Combinatorial-Auctions-and-Graph-Neural-Networks-for-Local-Energy-Flexibility-Markets"><a href="#Combinatorial-Auctions-and-Graph-Neural-Networks-for-Local-Energy-Flexibility-Markets" class="headerlink" title="Combinatorial Auctions and Graph Neural Networks for Local Energy Flexibility Markets"></a>Combinatorial Auctions and Graph Neural Networks for Local Energy Flexibility Markets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13470">http://arxiv.org/abs/2307.13470</a></li>
<li>repo_url: None</li>
<li>paper_authors: Awadelrahman M. A. Ahmed, Frank Eliassen, Yan Zhang</li>
<li>for: 本研究提出了一个新的 combinatorial 拍卖框架，用于地方能源灵活性市场，以解决潜在参与者无法组合多个灵活时间间隔的问题。</li>
<li>methods: 本研究使用了简单 yet powerful 三元图表示法和图生物学网络模型来解决背景NP-完备的胜出决定问题。</li>
<li>results: 模型实现了与商业解决方案相对的优化值差不 біль于5%，并且显示了线性推论时间复杂性，与商业解决方案的指数复杂性相比。<details>
<summary>Abstract</summary>
This paper proposes a new combinatorial auction framework for local energy flexibility markets, which addresses the issue of prosumers' inability to bundle multiple flexibility time intervals. To solve the underlying NP-complete winner determination problems, we present a simple yet powerful heterogeneous tri-partite graph representation and design graph neural network-based models. Our models achieve an average optimal value deviation of less than 5\% from an off-the-shelf optimization tool and show linear inference time complexity compared to the exponential complexity of the commercial solver. Contributions and results demonstrate the potential of using machine learning to efficiently allocate energy flexibility resources in local markets and solving optimization problems in general.
</details>
<details>
<summary>摘要</summary>
translate to Simplified Chinese as follows:这篇论文提出了一种新的 combinatorial 拍卖框架，用于本地能源灵活性市场，解决了潜在用户无法组合多个灵活时间间隔的问题。为解决这个下面NP完备的赢家决定问题，我们提出了一种简单强大的三元 Graph 表示和图 neural network 模型。我们的模型在与 commercial 优化工具的比较中，实现了 Less than 5% 的最佳值偏差，并且显示了与商业解决方案的线性推理时间复杂度。研讨和结果表明，使用机器学习可以有效地分配本地能源灵活性资源，并在总体上解决优化问题。
</details></li>
</ul>
<hr>
<h2 id="Gaussian-Graph-with-Prototypical-Contrastive-Learning-in-E-Commerce-Bundle-Recommendation"><a href="#Gaussian-Graph-with-Prototypical-Contrastive-Learning-in-E-Commerce-Bundle-Recommendation" class="headerlink" title="Gaussian Graph with Prototypical Contrastive Learning in E-Commerce Bundle Recommendation"></a>Gaussian Graph with Prototypical Contrastive Learning in E-Commerce Bundle Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13468">http://arxiv.org/abs/2307.13468</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhao-Yang Liu, Liucheng Sun, Chenwei Weng, Qijin Chen, Chengfu Huo</li>
<li>for: 提高电商平台上的bundle推荐的精度和效果，解决实际推荐场景中的uncertainty问题。</li>
<li>methods: 提出了一种新的 Gaussian Graph with Prototypical Contrastive Learning (GPCL)框架，使用Gaussian分布代替固定向量，并设计了一种prototypical contrastive learning模块来捕捉 Contextual信息，缓解采样偏见问题。</li>
<li>results: 经验表明，GPCL在多个公共数据集上达到了新的州OF-the-art性能水平，并在真实的电商平台上实现了显著的提升。<details>
<summary>Abstract</summary>
Bundle recommendation aims to provide a bundle of items to satisfy the user preference on e-commerce platform. Existing successful solutions are based on the contrastive graph learning paradigm where graph neural networks (GNNs) are employed to learn representations from user-level and bundle-level graph views with a contrastive learning module to enhance the cooperative association between different views. Nevertheless, they ignore the uncertainty issue which has a significant impact in real bundle recommendation scenarios due to the lack of discriminative information caused by highly sparsity or diversity. We further suggest that their instancewise contrastive learning fails to distinguish the semantically similar negatives (i.e., sampling bias issue), resulting in performance degradation. In this paper, we propose a novel Gaussian Graph with Prototypical Contrastive Learning (GPCL) framework to overcome these challenges. In particular, GPCL embeds each user/bundle/item as a Gaussian distribution rather than a fixed vector. We further design a prototypical contrastive learning module to capture the contextual information and mitigate the sampling bias issue. Extensive experiments demonstrate that benefiting from the proposed components, we achieve new state-of-the-art performance compared to previous methods on several public datasets. Moreover, GPCL has been deployed on real-world e-commerce platform and achieved substantial improvements.
</details>
<details>
<summary>摘要</summary>
<izin>电商平台上的Bundle推荐 aimsto提供一个Bundle的item来满足用户的首选。现有的成功解决方案基于对冲raph学习 paradigm，使用граф neural networks (GNNs)来学习用户和Bundle的表示，并通过对冲学习模块来增强不同视图之间的合作关系。然而，它们忽略了uncertainty问题，这有着很大的影响在实际的Bundle推荐场景中，因为缺乏特征信息引起的高稀疏或多样性。我们还指出，它们的Instancewise对冲学习无法分辨semantic Similarity的负样本（即采样偏见问题），导致性能下降。在这篇论文中，我们提出了一种novel Gaussian Graph with Prototypical Contrastive Learning (GPCL)框架，以解决这些挑战。具体来说，GPCL将每个用户/Bundle/itemembed为 Gaussian Distribution而不是固定的 вектор。我们还设计了一个prototypical对冲学习模块，以捕捉上下文信息并缓解采样偏见问题。我们的实验证明，由于我们提出的组件，我们在多个公共数据集上达到了新的状态态performanced比之前的方法。此外，GPCL已经部署在实际的电商平台上，并实现了显著的改善。</izin>Note: "izin" is a marker in Simplified Chinese to indicate that the following text is a translation of a foreign language text.
</details></li>
</ul>
<hr>
<h2 id="Integrating-processed-based-models-and-machine-learning-for-crop-yield-prediction"><a href="#Integrating-processed-based-models-and-machine-learning-for-crop-yield-prediction" class="headerlink" title="Integrating processed-based models and machine learning for crop yield prediction"></a>Integrating processed-based models and machine learning for crop yield prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13466">http://arxiv.org/abs/2307.13466</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michiel G. J. Kallenberg, Bernardo Maestrini, Ron van Bree, Paul Ravensbergen, Christos Pylianidis, Frits van Evert, Ioannis N. Athanasiadis</li>
<li>for: 预测哈比甜菜的产量</li>
<li>methods: 使用гибрид元模型方法，结合理论驱动的植物生长模型和数据驱动的神经网络</li>
<li>results: 在silico和实际场景中，元模型方法比基eline方法更好地预测哈比甜菜的产量，但需要进一步的 validate和优化以确定实际效果。<details>
<summary>Abstract</summary>
Crop yield prediction typically involves the utilization of either theory-driven process-based crop growth models, which have proven to be difficult to calibrate for local conditions, or data-driven machine learning methods, which are known to require large datasets. In this work we investigate potato yield prediction using a hybrid meta-modeling approach. A crop growth model is employed to generate synthetic data for (pre)training a convolutional neural net, which is then fine-tuned with observational data. When applied in silico, our meta-modeling approach yields better predictions than a baseline comprising a purely data-driven approach. When tested on real-world data from field trials (n=303) and commercial fields (n=77), the meta-modeling approach yields competitive results with respect to the crop growth model. In the latter set, however, both models perform worse than a simple linear regression with a hand-picked feature set and dedicated preprocessing designed by domain experts. Our findings indicate the potential of meta-modeling for accurate crop yield prediction; however, further advancements and validation using extensive real-world datasets is recommended to solidify its practical effectiveness.
</details>
<details>
<summary>摘要</summary>
通常，耐作预测通过使用理论驱动的过程基于植物生长模型或数据驱动的机器学习方法进行实现。这两种方法都有其缺点，其中一种是难以适应当地区条件，另一种是需要大量数据。在这种情况下，我们研究了使用半结构化模型的hybrid meta-modeling方法来预测芋头收获。我们使用植物生长模型生成了合成数据，然后使用卷积神经网络进行预测，并且通过观察数据进行精度调整。在silico中应用的meta-modeling方法比基准情况下的纯数据驱动方法更好。在实际场景中，我们对303个试验场和77个商业场的数据进行测试，并发现meta-modeling方法和植物生长模型在这些场景中具有竞争力。然而，在这些场景中，一个简单的直线回归模型和专业人员设计的特定预处理和特征集合得到了更好的表现。我们的发现表明meta-modeling方法在准确预测耐作收获方面存在潜力，但是进一步的发展和验证使用广泛的实际数据是必要的，以固化其实际效果。
</details></li>
</ul>
<hr>
<h2 id="Fundamental-causal-bounds-of-quantum-random-access-memories"><a href="#Fundamental-causal-bounds-of-quantum-random-access-memories" class="headerlink" title="Fundamental causal bounds of quantum random access memories"></a>Fundamental causal bounds of quantum random access memories</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13460">http://arxiv.org/abs/2307.13460</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunfei Wang, Yuri Alexeev, Liang Jiang, Frederic T. Chong, Junyu Liu</li>
<li>for: This paper explores the fundamental limits of rapid quantum memories in quantum computing applications, particularly in the context of hybrid quantum acoustic systems.</li>
<li>methods: The paper employs relativistic quantum field theory and Lieb-Robinson bounds to critically examine the causality constraints of quantum memories and their impact on quantum computing performance.</li>
<li>results: The paper shows that the number of logical qubits that can be accommodated in a QRAM design can be scaled up to $\mathcal{O}(10^7)$ in 1 dimension, $\mathcal{O}(10^{15})$ to $\mathcal{O}(10^{20})$ in various 2D architectures, and $\mathcal{O}(10^{24})$ in 3 dimensions, subject to the causality bound. These findings have important implications for the long-term performance of quantum computing applications in data science.<details>
<summary>Abstract</summary>
Quantum devices should operate in adherence to quantum physics principles. Quantum random access memory (QRAM), a fundamental component of many essential quantum algorithms for tasks such as linear algebra, data search, and machine learning, is often proposed to offer $\mathcal{O}(\log N)$ circuit depth for $\mathcal{O}(N)$ data size, given $N$ qubits. However, this claim appears to breach the principle of relativity when dealing with a large number of qubits in quantum materials interacting locally. In our study we critically explore the intrinsic bounds of rapid quantum memories based on causality, employing the relativistic quantum field theory and Lieb-Robinson bounds in quantum many-body systems. In this paper, we consider a hardware-efficient QRAM design in hybrid quantum acoustic systems. Assuming clock cycle times of approximately $10^{-3}$ seconds and a lattice spacing of about 1 micrometer, we show that QRAM can accommodate up to $\mathcal{O}(10^7)$ logical qubits in 1 dimension, $\mathcal{O}(10^{15})$ to $\mathcal{O}(10^{20})$ in various 2D architectures, and $\mathcal{O}(10^{24})$ in 3 dimensions. We contend that this causality bound broadly applies to other quantum hardware systems. Our findings highlight the impact of fundamental quantum physics constraints on the long-term performance of quantum computing applications in data science and suggest potential quantum memory designs for performance enhancement.
</details>
<details>
<summary>摘要</summary>
量子设备应遵循量子物理原理运行。量子随机访问存储器（QRAM），许多关键量子算法中的基本组件，通常被提议可以提供 $\mathcal{O}(\log N)$ 圈深度，对于 $\mathcal{O}(N)$ 数据大小， givent $N$  qubits。然而，这个宣称似乎违反了 relativity 原理，当处理大量的 qubits 在量子材料中互动时。在我们的研究中，我们 kritisch 探讨了快速量子存储器的内在 bound，基于 causality，使用量子场论和 Lieb-Robinson bound 在量子多体系统中。在这篇文章中，我们考虑了硬件高效的 QRAM 设计，在半导体量子声学系统中。假设clock cycle 时间约为 $10^{-3}$ 秒，格子间距约为 1 微米，我们显示了 QRAM 可以容纳 $\mathcal{O}(10^7)$ 逻辑 qubits 在1维度，$\mathcal{O}(10^{15})$ 到 $\mathcal{O}(10^{20})$ 在不同的 2D 架构中，以及 $\mathcal{O}(10^{24})$ 在 3 维度。我们认为这种 causality bound 广泛适用于其他量子硬件系统。我们的发现指出了量子计算应用中长期表现的基本量子物理约束，并提出了可能的量子存储器设计来提高性能。
</details></li>
</ul>
<hr>
<h2 id="A-behavioural-transformer-for-effective-collaboration-between-a-robot-and-a-non-stationary-human"><a href="#A-behavioural-transformer-for-effective-collaboration-between-a-robot-and-a-non-stationary-human" class="headerlink" title="A behavioural transformer for effective collaboration between a robot and a non-stationary human"></a>A behavioural transformer for effective collaboration between a robot and a non-stationary human</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13447">http://arxiv.org/abs/2307.13447</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruaridh Mon-Williams, Theodoros Stouraitis, Sethu Vijayakumar</li>
<li>for: 本研究旨在解决人机合作中人类行为变化带来的非站立性问题，提高机器人的预测能力以适应新的人类代理人。</li>
<li>methods: 本研究提出了一种原则式的meta学框架，并基于这个框架开发了Behavior-Transform（BeTrans）。BeTrans是一种可适应新人类代理人的 conditional transformer，可以快速适应新的人类行为变化。</li>
<li>results: 通过在 simulated human agents 上进行训练，我们发现BeTrans在合作设置下与不同系统偏见的人类代理人协作得非常好，并且比SOTA技术更快地适应新的人类行为变化。<details>
<summary>Abstract</summary>
A key challenge in human-robot collaboration is the non-stationarity created by humans due to changes in their behaviour. This alters environmental transitions and hinders human-robot collaboration. We propose a principled meta-learning framework to explore how robots could better predict human behaviour, and thereby deal with issues of non-stationarity. On the basis of this framework, we developed Behaviour-Transform (BeTrans). BeTrans is a conditional transformer that enables a robot agent to adapt quickly to new human agents with non-stationary behaviours, due to its notable performance with sequential data. We trained BeTrans on simulated human agents with different systematic biases in collaborative settings. We used an original customisable environment to show that BeTrans effectively collaborates with simulated human agents and adapts faster to non-stationary simulated human agents than SOTA techniques.
</details>
<details>
<summary>摘要</summary>
人机合作中的一大挑战是由人类行为引起的非站点性，这会导致环境变化和人机合作困难。我们提出了一种原则正的meta学框架，以便机器人更好地预测人类行为，从而更好地处理非站点性问题。基于这个框架，我们开发了Behaviour-Transform（BeTrans）。BeTrans是一种 Conditional Transformer，它允许机器人代理人类快速适应新的人类行为，并且在序列数据上表现出色。我们在 simulate human agents with different systematic biases in collaborative settings 中训练了 BeTrans，并用自定义环境示出了它在与 simulate human agents 合作中的有效性，并且更快地适应非站点性 simulate human agents than SOTA技术。
</details></li>
</ul>
<hr>
<h2 id="Network-Traffic-Classification-based-on-Single-Flow-Time-Series-Analysis"><a href="#Network-Traffic-Classification-based-on-Single-Flow-Time-Series-Analysis" class="headerlink" title="Network Traffic Classification based on Single Flow Time Series Analysis"></a>Network Traffic Classification based on Single Flow Time Series Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13434">http://arxiv.org/abs/2307.13434</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/koumajos/classificationbasedonsfts">https://github.com/koumajos/classificationbasedonsfts</a></li>
<li>paper_authors: Josef Koumar, Karel Hynek, Tomáš Čejka</li>
<li>for: 用于分析加密网络通信的现场挑战</li>
<li>methods: 基于时间序列分析单流时间序列（包括每个 packet 的字节数和时间戳），提出69种统一特征</li>
<li>results: 在15种公共可用的数据集上进行了多种网络流量分类任务的评估，表明提议的特征向量可以达到与相关工作相当或更好的分类性能，在超过一半的评估任务中，分类性能提高了5%以上。<details>
<summary>Abstract</summary>
Network traffic monitoring using IP flows is used to handle the current challenge of analyzing encrypted network communication. Nevertheless, the packet aggregation into flow records naturally causes information loss; therefore, this paper proposes a novel flow extension for traffic features based on the time series analysis of the Single Flow Time series, i.e., a time series created by the number of bytes in each packet and its timestamp. We propose 69 universal features based on the statistical analysis of data points, time domain analysis, packet distribution within the flow timespan, time series behavior, and frequency domain analysis. We have demonstrated the usability and universality of the proposed feature vector for various network traffic classification tasks using 15 well-known publicly available datasets. Our evaluation shows that the novel feature vector achieves classification performance similar or better than related works on both binary and multiclass classification tasks. In more than half of the evaluated tasks, the classification performance increased by up to 5\%.
</details>
<details>
<summary>摘要</summary>
网络流量监测使用流量记录来处理现在的挑战，即分析加密网络通信。然而，流量聚合到记录中自然导致信息损失，因此这篇论文提议一种新的流量扩展 для交通特征基于单流时间序列分析，即每个包的字节数和时间戳创建的时间序列。我们提出69个统一特征，包括数据点统计分析、时间域分析、流时间范围内包分布、时间序列行为和频域分析。我们通过使用15个公共可用的数据集来证明提议的特征向量的可用性和通用性，并在 binary 和多类分类任务中达到类似或更好的性能。在评估中，在超过半个评估任务中，分类性能提高5%以上。
</details></li>
</ul>
<hr>
<h2 id="Achieving-Linear-Speedup-in-Decentralized-Stochastic-Compositional-Minimax-Optimization"><a href="#Achieving-Linear-Speedup-in-Decentralized-Stochastic-Compositional-Minimax-Optimization" class="headerlink" title="Achieving Linear Speedup in Decentralized Stochastic Compositional Minimax Optimization"></a>Achieving Linear Speedup in Decentralized Stochastic Compositional Minimax Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13430">http://arxiv.org/abs/2307.13430</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongchang Gao</li>
<li>for: 本研究的目的是提出一种基于分布式数据的各自练习作业的分布式compositional minimax问题的解决方案，以便在分布式设置下优化这类问题。</li>
<li>methods: 我们提出了一种基于均匀采样和动量的分布式Stochastic Compositional Gradient Descent Ascent算法，用于降低内层函数的共识错误。</li>
<li>results: 我们的 teoría results 表明，该算法可以实现线性增速，即与工作者数量 linearly 相关。  Additionally, we applied our method to the imbalanced classification problem and obtained extensive experimental results, which demonstrate the effectiveness of our algorithm.<details>
<summary>Abstract</summary>
The stochastic compositional minimax problem has attracted a surge of attention in recent years since it covers many emerging machine learning models. Meanwhile, due to the emergence of distributed data, optimizing this kind of problem under the decentralized setting becomes badly needed. However, the compositional structure in the loss function brings unique challenges to designing efficient decentralized optimization algorithms. In particular, our study shows that the standard gossip communication strategy cannot achieve linear speedup for decentralized compositional minimax problems due to the large consensus error about the inner-level function. To address this issue, we developed a novel decentralized stochastic compositional gradient descent ascent with momentum algorithm to reduce the consensus error in the inner-level function. As such, our theoretical results demonstrate that it is able to achieve linear speedup with respect to the number of workers. We believe this novel algorithmic design could benefit the development of decentralized compositional optimization. Finally, we applied our methods to the imbalanced classification problem. The extensive experimental results provide evidence for the effectiveness of our algorithm.
</details>
<details>
<summary>摘要</summary>
“ Stochastic compositional minimax problem 在 recent years 已经吸引了许多关注，因为它涵盖了许多emerging machine learning models。然而，由于分布式数据的出现，对这种问题的分布式优化成为了非常需要。然而，compositional structure 在损失函数中带来了独特的挑战，对于设计高效的分布式优化算法。具体来说，我们的研究显示，标准的gossip Communication Strategy 不能实现linear speedup  для分布式compositional minimax problem，因为内部函数的大调和错误。为了解决这个问题，我们开发了一个新的分布式随机compositional gradient descent ascent with momentum algorithm，以减少内部函数的调和错误。因此，我们的理论结果显示，它能够实现linear speedup 与 respect to the number of workers。我们认为这个新的算法设计可以帮助分布式compositional optimization的发展。 finally，我们将我们的方法应用到不对称类别问题。广泛的实验结果为我们的算法的有效性提供了证据。”
</details></li>
</ul>
<hr>
<h2 id="A-signal-processing-interpretation-of-noise-reduction-convolutional-neural-networks"><a href="#A-signal-processing-interpretation-of-noise-reduction-convolutional-neural-networks" class="headerlink" title="A signal processing interpretation of noise-reduction convolutional neural networks"></a>A signal processing interpretation of noise-reduction convolutional neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13425">http://arxiv.org/abs/2307.13425</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luis A. Zavala-Mondragón, Peter H. N. de With, Fons van der Sommen</li>
<li>for: 本文旨在为数据驱动降噪和深度学习算法中的Encoding-decoding CNNs提供理论基础，以便更好地理解这些架构的内部工作机制。</li>
<li>methods: 本文使用了深度卷积架构，并提出了一种基于深度学习和信号处理的理论框架，用于解释Encoding-decoding CNNs的内部工作机制。</li>
<li>results: 本文通过 connecting basic principles from signal processing to the field of deep learning, 提供了一种可以用于设计robust和高效的新型Encoding-decoding CNNs架构的有效指导。<details>
<summary>Abstract</summary>
Encoding-decoding CNNs play a central role in data-driven noise reduction and can be found within numerous deep-learning algorithms. However, the development of these CNN architectures is often done in ad-hoc fashion and theoretical underpinnings for important design choices is generally lacking. Up to this moment there are different existing relevant works that strive to explain the internal operation of these CNNs. Still, these ideas are either scattered and/or may require significant expertise to be accessible for a bigger audience. In order to open up this exciting field, this article builds intuition on the theory of deep convolutional framelets and explains diverse ED CNN architectures in a unified theoretical framework. By connecting basic principles from signal processing to the field of deep learning, this self-contained material offers significant guidance for designing robust and efficient novel CNN architectures.
</details>
<details>
<summary>摘要</summary>
Encoding-decoding CNNs 在数据驱动的噪声缓解中扮演中心角色，可以在多种深度学习算法中找到。然而，这些 CNN 架构的开发通常是靠悄悄话的，lacking 理论基础。到目前为止，有很多相关的工作努力来解释这些 CNN 的内部运作。然而，这些想法是分散的，或者需要一定的专业知识才能访问。为了开放这个激动人心的领域，这篇文章建立了深度卷积框架的理论基础，并将多种 ED CNN 架构集成到一个统一的理论框架中。通过将信号处理的基本原理与深度学习相连接，这篇自包含的材料提供了设计robust和高效的新的 CNN 架构的重要指南。
</details></li>
</ul>
<hr>
<h2 id="Non-Intrusive-Intelligibility-Predictor-for-Hearing-Impaired-Individuals-using-Self-Supervised-Speech-Representations"><a href="#Non-Intrusive-Intelligibility-Predictor-for-Hearing-Impaired-Individuals-using-Self-Supervised-Speech-Representations" class="headerlink" title="Non Intrusive Intelligibility Predictor for Hearing Impaired Individuals using Self Supervised Speech Representations"></a>Non Intrusive Intelligibility Predictor for Hearing Impaired Individuals using Self Supervised Speech Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13423">http://arxiv.org/abs/2307.13423</a></li>
<li>repo_url: None</li>
<li>paper_authors: George Close, Thomas Hain, Stefan Goetze</li>
<li>for: 这个论文旨在扩展自我监督抽象语音表示法（SSSR），以便非侵入式方式预测听力障碍用户的语音质量评分。</li>
<li>methods: 本文使用了SSSR作为输入特征，通过非侵入式预测模型来预测听力障碍用户的语音理解度。</li>
<li>results: 研究发现，SSSR可以作为输入特征，实现与更复杂的系统相当的竞争性性能。分析表明，更多的数据可能需要用于将预测模型 generalized to unknown systems and individuals。<details>
<summary>Abstract</summary>
Self-supervised speech representations (SSSRs) have been successfully applied to a number of speech-processing tasks, e.g. as feature extractor for speech quality (SQ) prediction, which is, in turn, relevant for assessment and training speech enhancement systems for users with normal or impaired hearing. However, exact knowledge of why and how quality-related information is encoded well in such representations remains poorly understood. In this work, techniques for non-intrusive prediction of SQ ratings are extended to the prediction of intelligibility for hearing-impaired users. It is found that self-supervised representations are useful as input features to non-intrusive prediction models, achieving competitive performance to more complex systems. A detailed analysis of the performance depending on Clarity Prediction Challenge 1 listeners and enhancement systems indicates that more data might be needed to allow generalisation to unknown systems and (hearing-impaired) individuals
</details>
<details>
<summary>摘要</summary>
自我监督的语音表示 (SSSR) 已成功应用于多个语音处理任务中，例如作为语音质量预测的特征提取器，这同时对于评估和培训语音增强系统的用户进行评估和培训具有重要性。然而，关于为什么和如何在这些表示中编码质量相关信息的准确知识仍然不够了解。在这种情况下，非侵入式预测模型的扩展被应用于预测听力障碍用户的语音明白度。结果表明，自我监督表示可以作为输入特征进行非侵入式预测模型，实现与更复杂的系统相当的性能。一个细化的性能分析，具体分析了根据Clearity Prediction Challenge 1的听众和增强系统，表明更多的数据可能需要以Allow generalization to unknown systems and (hearing-impaired) individuals。
</details></li>
</ul>
<hr>
<h2 id="On-the-Learning-Dynamics-of-Attention-Networks"><a href="#On-the-Learning-Dynamics-of-Attention-Networks" class="headerlink" title="On the Learning Dynamics of Attention Networks"></a>On the Learning Dynamics of Attention Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13421">http://arxiv.org/abs/2307.13421</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vashisht-rahul/on-the-learning-dynamics-of-attention-networks">https://github.com/vashisht-rahul/on-the-learning-dynamics-of-attention-networks</a></li>
<li>paper_authors: Rahul Vashisht, Harish G. Ramaswamy</li>
<li>for: 本文探讨了三种常见的注意力模型优化方法，即软注意力、硬注意力和隐变量 marginal likelihood（LVML）注意力。这三种方法都是为了找到一个 <code>ocus&#39; 模型，可以选择输入中的正确段落，以及一个 </code>classification’ 模型，可以处理选择的段落并生成目标标签。但是它们在选取段落的方式不同，导致了不同的动态和最终结果。</li>
<li>methods: 本文使用了不同的注意力优化方法，包括软注意力损失、硬注意力损失和隐变量 marginal likelihood（LVML）注意力损失。这些方法的选择对于模型的性能有很大的影响。</li>
<li>results: 本文通过对一系列半 sintetic和实际世界数据集进行实验，发现了不同的注意力优化方法在模型性能上的不同表现。软注意力损失在初始化时能够快速改进 focus 模型，但是后续会降低。与此相反，硬注意力损失在初始化时会降低 focus 模型，但是后续会快速改进。基于这些观察，文章提出了一种简单的混合方法，可以结合不同的注意力优化方法的优点，并在实验中得到了良好的性能。<details>
<summary>Abstract</summary>
Attention models are typically learned by optimizing one of three standard loss functions that are variously called -- soft attention, hard attention, and latent variable marginal likelihood (LVML) attention. All three paradigms are motivated by the same goal of finding two models -- a `focus' model that `selects' the right \textit{segment} of the input and a `classification' model that processes the selected segment into the target label. However, they differ significantly in the way the selected segments are aggregated, resulting in distinct dynamics and final results. We observe a unique signature of models learned using these paradigms and explain this as a consequence of the evolution of the classification model under gradient descent when the focus model is fixed. We also analyze these paradigms in a simple setting and derive closed-form expressions for the parameter trajectory under gradient flow. With the soft attention loss, the focus model improves quickly at initialization and splutters later on. On the other hand, hard attention loss behaves in the opposite fashion. Based on our observations, we propose a simple hybrid approach that combines the advantages of the different loss functions and demonstrates it on a collection of semi-synthetic and real-world datasets
</details>
<details>
<summary>摘要</summary>
听力模型通常通过优化三种标准损失函数来学习：软注意力、硬注意力和隐变量极值 probabilistic（LVML）注意力。这三种方法均由同一目标而导向：找到一个`焦点'模型，可以选择输入中正确的`段'，以及一个`分类'模型，可以处理选择的段来生成目标标签。然而，它们在选择段的方式不同，从而导致了不同的动态和最终结果。我们观察到这些模型学习使用这些方法的独特签名，并解释这为梯度下降在 fixes 焦点模型下的演化。我们还分析了这些方法在简单的设置下，并 deriv 出closed-form 表达式用于参数轨迹的梯度流。与软注意力损失函数相比，硬注意力损失函数在初始化时快速改进，然后后来停滞不前进。相反，硬注意力损失函数在另一方面表现出opposite的特点。基于我们的观察，我们提出了一种简单的混合方法，将不同损失函数的优点结合起来，并在一些半Synthetic 和实际数据集上进行了证明。
</details></li>
</ul>
<hr>
<h2 id="Co-Design-of-Out-of-Distribution-Detectors-for-Autonomous-Emergency-Braking-Systems"><a href="#Co-Design-of-Out-of-Distribution-Detectors-for-Autonomous-Emergency-Braking-Systems" class="headerlink" title="Co-Design of Out-of-Distribution Detectors for Autonomous Emergency Braking Systems"></a>Co-Design of Out-of-Distribution Detectors for Autonomous Emergency Braking Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13419">http://arxiv.org/abs/2307.13419</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Yuhas, Arvind Easwaran</li>
<li>For: The paper aims to improve the safety of autonomous vehicles (AVs) by co-designing an out-of-distribution (OOD) detector and a learning-enabled component (LEC) to detect and mitigate potential failures in the LEC.* Methods: The paper uses a risk model to analyze the impact of design parameters on both the OOD detector and the LEC, and co-designs the two components to minimize the risk of failure.* Results: The paper demonstrates a 42.3% risk reduction in the system while maintaining equivalent resource utilization, indicating the effectiveness of the co-design methodology in improving the safety of AVs.<details>
<summary>Abstract</summary>
Learning enabled components (LECs), while critical for decision making in autonomous vehicles (AVs), are likely to make incorrect decisions when presented with samples outside of their training distributions. Out-of-distribution (OOD) detectors have been proposed to detect such samples, thereby acting as a safety monitor, however, both OOD detectors and LECs require heavy utilization of embedded hardware typically found in AVs. For both components, there is a tradeoff between non-functional and functional performance, and both impact a vehicle's safety. For instance, giving an OOD detector a longer response time can increase its accuracy at the expense of the LEC. We consider an LEC with binary output like an autonomous emergency braking system (AEBS) and use risk, the combination of severity and occurrence of a failure, to model the effect of both components' design parameters on each other's functional and non-functional performance, as well as their impact on system safety. We formulate a co-design methodology that uses this risk model to find the design parameters for an OOD detector and LEC that decrease risk below that of the baseline system and demonstrate it on a vision based AEBS. Using our methodology, we achieve a 42.3% risk reduction while maintaining equivalent resource utilization.
</details>
<details>
<summary>摘要</summary>
We use risk, which is the combination of the severity and occurrence of a failure, to model the effect of both components' design parameters on each other's functional and non-functional performance, as well as their impact on system safety. We develop a co-design methodology that uses this risk model to find the design parameters for an OOD detector and LEC that minimize risk. We demonstrate the effectiveness of our methodology on a vision-based autonomous emergency braking system (AEBS).By using our co-design methodology, we achieve a 42.3% risk reduction while maintaining equivalent resource utilization. This demonstrates the potential of our approach to improve the safety of AVs by optimizing the design parameters of both OOD detectors and LECs.
</details></li>
</ul>
<hr>
<h2 id="Communication-Efficient-Orchestrations-for-URLLC-Service-via-Hierarchical-Reinforcement-Learning"><a href="#Communication-Efficient-Orchestrations-for-URLLC-Service-via-Hierarchical-Reinforcement-Learning" class="headerlink" title="Communication-Efficient Orchestrations for URLLC Service via Hierarchical Reinforcement Learning"></a>Communication-Efficient Orchestrations for URLLC Service via Hierarchical Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13415">http://arxiv.org/abs/2307.13415</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Shi, Milad Ganjalizadeh, Hossein Shokri Ghadikolaei, Marina Petrova</li>
<li>for: 此研究旨在提高5G中的可靠低延迟通信服务（URLLC）的可靠性和响应速度。</li>
<li>methods: 本研究使用多代理 Hierarchical Reinforcement Learning（HRL）框架，实现多级政策的实现，并且通过不同控制循环时间的调整，提高控制循环的响应速度和灵活性。</li>
<li>results: 在一个先前的实验中，使用HRL框架优化工业设备的最大重传数和传输功率，并获得了较好的性能，比基eline单代理RL方法更好，同时具有较少的信号传输 overhead和延迟。<details>
<summary>Abstract</summary>
Ultra-reliable low latency communications (URLLC) service is envisioned to enable use cases with strict reliability and latency requirements in 5G. One approach for enabling URLLC services is to leverage Reinforcement Learning (RL) to efficiently allocate wireless resources. However, with conventional RL methods, the decision variables (though being deployed at various network layers) are typically optimized in the same control loop, leading to significant practical limitations on the control loop's delay as well as excessive signaling and energy consumption. In this paper, we propose a multi-agent Hierarchical RL (HRL) framework that enables the implementation of multi-level policies with different control loop timescales. Agents with faster control loops are deployed closer to the base station, while the ones with slower control loops are at the edge or closer to the core network providing high-level guidelines for low-level actions. On a use case from the prior art, with our HRL framework, we optimized the maximum number of retransmissions and transmission power of industrial devices. Our extensive simulation results on the factory automation scenario show that the HRL framework achieves better performance as the baseline single-agent RL method, with significantly less overhead of signal transmissions and delay compared to the one-agent RL methods.
</details>
<details>
<summary>摘要</summary>
超可靠低延迟通信服务（URLLC）在5G中被视为实现严格可靠性和延迟要求的应用场景。一种实现URLLC服务的方法是通过强化学习（RL）有效地分配无线资源。然而，传统RL方法中的决策变量（即在不同网络层部署）通常在同一控制循环中优化，这会导致控制循环延迟的限制以及过分的信号传输和能耗。在这篇论文中，我们提出了一种多代理层RL（HRL）框架，该框架允许实现多级策略，并且在不同层次上有不同的控制循环时间尺度。靠近基站的代理在更快的控制循环中部署，而Edge或更接近核心网络的代理则提供高级指导 для低级动作。在一个优化 industrial device 的Factory automation scenario中，我们使用HRL框架进行优化，并取得了较好的性能，与基准单代理RL方法相比，减少了信号传输的 overhead和延迟。Note: The translation is in Simplified Chinese, which is the standardized form of Chinese used in mainland China and widely used in informal writing. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Mitigating-Memory-Wall-Effects-in-CNN-Engines-with-On-the-Fly-Weights-Generation"><a href="#Mitigating-Memory-Wall-Effects-in-CNN-Engines-with-On-the-Fly-Weights-Generation" class="headerlink" title="Mitigating Memory Wall Effects in CNN Engines with On-the-Fly Weights Generation"></a>Mitigating Memory Wall Effects in CNN Engines with On-the-Fly Weights Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13412">http://arxiv.org/abs/2307.13412</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stylianos I. Venieris, Javier Fernandez-Marques, Nicholas D. Lane</li>
<li>for: 本研究旨在提高FPGA基于Convolutional Neural Networks（CNN）加速器的性能和能效性。</li>
<li>methods: 本文提出了一种新的CNN推理系统，称为unzipFPGA，它使用了一种新的硬件架构，包括一个 weights生成模块，可以在运行时生成权重，以解决受限制的带宽的问题。此外，文章还提出了一种自动硬件快照方法，可以根据目标CNN设备对硬件进行优化，从而提高精度和性能的平衡。</li>
<li>results: 根据结果表明，unzipFPGA可以实现2.57倍的性能效率提升相比高优化的GPU设计，并且可以达到3.94倍的性能密度，超过了一系列state-of-the-art FPGA基于CNN加速器。<details>
<summary>Abstract</summary>
The unprecedented accuracy of convolutional neural networks (CNNs) across a broad range of AI tasks has led to their widespread deployment in mobile and embedded settings. In a pursuit for high-performance and energy-efficient inference, significant research effort has been invested in the design of FPGA-based CNN accelerators. In this context, single computation engines constitute a popular approach to support diverse CNN modes without the overhead of fabric reconfiguration. Nevertheless, this flexibility often comes with significantly degraded performance on memory-bound layers and resource underutilisation due to the suboptimal mapping of certain layers on the engine's fixed configuration. In this work, we investigate the implications in terms of CNN engine design for a class of models that introduce a pre-convolution stage to decompress the weights at run time. We refer to these approaches as on-the-fly. This paper presents unzipFPGA, a novel CNN inference system that counteracts the limitations of existing CNN engines. The proposed framework comprises a novel CNN hardware architecture that introduces a weights generator module that enables the on-chip on-the-fly generation of weights, alleviating the negative impact of limited bandwidth on memory-bound layers. We further enhance unzipFPGA with an automated hardware-aware methodology that tailors the weights generation mechanism to the target CNN-device pair, leading to an improved accuracy-performance balance. Finally, we introduce an input selective processing element (PE) design that balances the load between PEs in suboptimally mapped layers. The proposed framework yields hardware designs that achieve an average of 2.57x performance efficiency gain over highly optimised GPU designs for the same power constraints and up to 3.94x higher performance density over a diverse range of state-of-the-art FPGA-based CNN accelerators.
</details>
<details>
<summary>摘要</summary>
“ convolutional neural networks (CNNs) 在许多人工智能任务中表现无 precedent 的准确率，导致它们在移动和嵌入设备上广泛应用。为了实现高性能且能效的推理，大量的研究精力被投入到基于 FPGA 的 CNN 加速器的设计中。在这个上下文中，单 computation 引擎是一种广泛使用的方法，以支持多种 CNN 模式，而无需 fabric 重新配置的开销。然而，这种灵活性通常会导致在占用内存层的执行中表现下降和资源利用率下降，因为在固定配置的引擎上对某些层的映射是不优化的。在这种情况下，我们 investigate 了在 CNN 引擎设计方面的影响，特别是那些引入预处理阶段来压缩 веса的模型。我们称这些方法为“在 fly”。本文提出了 unzipFPGA，一种新的 CNN 推理系统，该系统通过引入 weights 生成器模块来解决限制现有 CNN 引擎的缺点。我们还提供了一种自适应硬件方法，该方法根据目标 CNN-设备对可以自动调整 weights 生成机制，从而提高精度-性能平衡。 finally，我们引入了一种输入选择处理元件（PE）的设计，以平衡在不优化的层中的负载。提出的框架可以在同等功耗和能源约束下实现2.57倍的性能效率提升，相比高优化的 GPU 设计，以及3.94倍的性能密度提升，超过了多种现有的 FPGA 基于 CNN 加速器。”
</details></li>
</ul>
<hr>
<h2 id="The-Double-Edged-Sword-of-Big-Data-and-Information-Technology-for-the-Disadvantaged-A-Cautionary-Tale-from-Open-Banking"><a href="#The-Double-Edged-Sword-of-Big-Data-and-Information-Technology-for-the-Disadvantaged-A-Cautionary-Tale-from-Open-Banking" class="headerlink" title="The Double-Edged Sword of Big Data and Information Technology for the Disadvantaged: A Cautionary Tale from Open Banking"></a>The Double-Edged Sword of Big Data and Information Technology for the Disadvantaged: A Cautionary Tale from Open Banking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13408">http://arxiv.org/abs/2307.13408</a></li>
<li>repo_url: None</li>
<li>paper_authors: Savina Dine Kim, Galina Andreeva, Michael Rovatsos</li>
<li>for: 本研究探讨了开放银行技术的隐含不平等风险，通过使用 machine learning（ML）技术和 UK FinTech 银行数据集来示例。</li>
<li>methods: 本研究使用了三种 ML 分类器来预测 финан参与者的可能性，并通过集成特征分析groups exhibiting不同的大小和形式的 Financial Vulnerability（FV）。</li>
<li>results: 研究发现，工程化的金融行为特征可以预测排除个人信息的 omitted 个人特征，特别是敏感或保护特征，这解释了开放银行数据的隐藏危险。<details>
<summary>Abstract</summary>
This research article analyses and demonstrates the hidden implications for fairness of seemingly neutral data coupled with powerful technology, such as machine learning (ML), using Open Banking as an example. Open Banking has ignited a revolution in financial services, opening new opportunities for customer acquisition, management, retention, and risk assessment. However, the granularity of transaction data holds potential for harm where unnoticed proxies for sensitive and prohibited characteristics may lead to indirect discrimination. Against this backdrop, we investigate the dimensions of financial vulnerability (FV), a global concern resulting from COVID-19 and rising inflation. Specifically, we look to understand the behavioral elements leading up to FV and its impact on at-risk, disadvantaged groups through the lens of fair interpretation. Using a unique dataset from a UK FinTech lender, we demonstrate the power of fine-grained transaction data while simultaneously cautioning its safe usage. Three ML classifiers are compared in predicting the likelihood of FV, and groups exhibiting different magnitudes and forms of FV are identified via clustering to highlight the effects of feature combination. Our results indicate that engineered features of financial behavior can be predictive of omitted personal information, particularly sensitive or protected characteristics, shedding light on the hidden dangers of Open Banking data. We discuss the implications and conclude fairness via unawareness is ineffective in this new technological environment.
</details>
<details>
<summary>摘要</summary>
Using a unique dataset from a UK FinTech lender, the article demonstrates the power of fine-grained transaction data while cautioning its safe usage. Three machine learning (ML) classifiers are compared in predicting the likelihood of FV, and groups exhibiting different magnitudes and forms of FV are identified through clustering. The results show that engineered features of financial behavior can be predictive of omitted personal information, particularly sensitive or protected characteristics, highlighting the hidden dangers of Open Banking data.The article concludes that fairness via unawareness is ineffective in this new technological environment and discusses the implications for ensuring fairness in the use of Open Banking data. The findings have important implications for the financial industry, policymakers, and consumers, highlighting the need for careful consideration of the potential risks and benefits of Open Banking data and the importance of ensuring fairness in its use.
</details></li>
</ul>
<hr>
<h2 id="Counterfactual-Explanation-via-Search-in-Gaussian-Mixture-Distributed-Latent-Space"><a href="#Counterfactual-Explanation-via-Search-in-Gaussian-Mixture-Distributed-Latent-Space" class="headerlink" title="Counterfactual Explanation via Search in Gaussian Mixture Distributed Latent Space"></a>Counterfactual Explanation via Search in Gaussian Mixture Distributed Latent Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13390">http://arxiv.org/abs/2307.13390</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuan Zhao, Klaus Broelemann, Gjergji Kasneci</li>
<li>for: 本研究旨在提供一种新的方法来生成Counterfactual Explanations（CE），以帮助用户更好地理解AI系统的决策过程和改进其结果。</li>
<li>methods: 本研究使用了一种基于自适应卷积神经网络的方法，首先将矩阵空间转换成一个 mixture of Gaussian distributions 的形式，然后通过线性 interpolate 生成 CE。</li>
<li>results: 对于各种图像和表格数据集，我们的方法能够具有比例尺度和数据抽象的优势，并且能够高效地返回更加真实的结果，相比三种现有的方法。<details>
<summary>Abstract</summary>
Counterfactual Explanations (CEs) are an important tool in Algorithmic Recourse for addressing two questions: 1. What are the crucial factors that led to an automated prediction/decision? 2. How can these factors be changed to achieve a more favorable outcome from a user's perspective? Thus, guiding the user's interaction with AI systems by proposing easy-to-understand explanations and easy-to-attain feasible changes is essential for the trustworthy adoption and long-term acceptance of AI systems. In the literature, various methods have been proposed to generate CEs, and different quality measures have been suggested to evaluate these methods. However, the generation of CEs is usually computationally expensive, and the resulting suggestions are unrealistic and thus non-actionable. In this paper, we introduce a new method to generate CEs for a pre-trained binary classifier by first shaping the latent space of an autoencoder to be a mixture of Gaussian distributions. CEs are then generated in latent space by linear interpolation between the query sample and the centroid of the target class. We show that our method maintains the characteristics of the input sample during the counterfactual search. In various experiments, we show that the proposed method is competitive based on different quality measures on image and tabular datasets -- efficiently returns results that are closer to the original data manifold compared to three state-of-the-art methods, which are essential for realistic high-dimensional machine learning applications.
</details>
<details>
<summary>摘要</summary>
“ counterfactual 解释 (CEs) 是 Algorithmic Recourse 中一种重要的工具，用于回答以下两个问题：1. 自动预测/决策中的关键因素为何？2. 如何变化这些因素以获得更有利的结果从用户的角度？因此，为AI系统的使用者提供易于理解的解释和可行的改变建议是Algorithmic Recourse 的重要 Component。在文献中，多种方法已经被提出供Counterfactual 解释，并且不同的质量指标已经被建议来评估这些方法。然而，Counterfactual 解释的生成通常是 computationally expensive 的，并且生成的建议通常是不现实的，因此无法使用。在本文中，我们创新了一种用于预训binary classifier的Counterfactual 解释方法，通过首先将 autoencoder 的latent space 变成一个 mixture of Gaussian distributions。Counterfactual 解释在latent space中generated by linear interpolation between the query sample and the centroid of the target class。我们显示了我们的方法可以维持输入样本的特性。在多个实验中，我们显示了我们的方法与三种state-of-the-art方法相比，能够实现更高的质量指标。”
</details></li>
</ul>
<hr>
<h2 id="BotHawk-An-Approach-for-Bots-Detection-in-Open-Source-Software-Projects"><a href="#BotHawk-An-Approach-for-Bots-Detection-in-Open-Source-Software-Projects" class="headerlink" title="BotHawk: An Approach for Bots Detection in Open Source Software Projects"></a>BotHawk: An Approach for Bots Detection in Open Source Software Projects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13386">http://arxiv.org/abs/2307.13386</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bifenglin/bothawk">https://github.com/bifenglin/bothawk</a></li>
<li>paper_authors: Fenglin Bi, Zhiwei Zhu, Wei Wang, Xiaoya Xia, Hassan Ali Khan, Peng Pu</li>
<li>for: 这个研究旨在调查开源软件项目中的机器人账户，并尝试准确地识别机器人账户。</li>
<li>methods: 该研究使用了一种严格的数据采集工作流程，以确保收集到的数据准确、可重复、可扩展和有效。研究人员还提出了一种名为BotHawk的机器人检测模型，可以高效地检测开源软件项目中的机器人账户。</li>
<li>results: 研究人员通过分析17个特征在5个维度中，确定了开源软件项目中机器人账户的四种类型。此外，研究人员发现，跟踪者数、仓库数和标签含义最有用于识别账户类型。BotHawk模型在检测开源软件项目中的机器人账户方面表现出色，其AUC为0.947，F1分数为0.89。<details>
<summary>Abstract</summary>
Social coding platforms have revolutionized collaboration in software development, leading to using software bots for streamlining operations. However, The presence of open-source software (OSS) bots gives rise to problems including impersonation, spamming, bias, and security risks. Identifying bot accounts and behavior is a challenging task in the OSS project. This research aims to investigate bots' behavior in open-source software projects and identify bot accounts with maximum possible accuracy. Our team gathered a dataset of 19,779 accounts that meet standardized criteria to enable future research on bots in open-source projects. We follow a rigorous workflow to ensure that the data we collect is accurate, generalizable, scalable, and up-to-date. We've identified four types of bot accounts in open-source software projects by analyzing their behavior across 17 features in 5 dimensions. Our team created BotHawk, a highly effective model for detecting bots in open-source software projects. It outperforms other models, achieving an AUC of 0.947 and an F1-score of 0.89. BotHawk can detect a wider variety of bots, including CI/CD and scanning bots. Furthermore, we find that the number of followers, number of repositories, and tags contain the most relevant features to identify the account type.
</details>
<details>
<summary>摘要</summary>
社交代码平台已经革命化软件开发合作，使用软件机器人来简化操作。然而，开源软件（OSS）机器人的存在导致了多种问题，包括伪造、诈骗、偏见和安全风险。标识机器人帐户和行为是开源项目中的挑战。本研究目的是调查开源软件项目中的机器人行为，并尽可能准确地识别机器人帐户。我们的团队收集了19,779个符合标准化riteria的帐户，以便未来对开源项目中的机器人进行研究。我们采用了严格的工作流程，以确保收集的数据准确、可重复、可扩展和时尚。我们通过分析17个特征在5个维度来Identify four types of bot accounts in open-source software projects。我们创建了BotHawk模型，可以高效地检测开源软件项目中的机器人。它比其他模型高效，AUC为0.947，F1分数为0.89。BotHawk可以检测更多的机器人，包括CI/CD和扫描机器人。此外，我们发现帐户类型的最有用特征是粉丝数、仓库数和标签。
</details></li>
</ul>
<hr>
<h2 id="Scaff-PD-Communication-Efficient-Fair-and-Robust-Federated-Learning"><a href="#Scaff-PD-Communication-Efficient-Fair-and-Robust-Federated-Learning" class="headerlink" title="Scaff-PD: Communication Efficient Fair and Robust Federated Learning"></a>Scaff-PD: Communication Efficient Fair and Robust Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13381">http://arxiv.org/abs/2307.13381</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yaodong Yu, Sai Praneeth Karimireddy, Yi Ma, Michael I. Jordan</li>
<li>for: 提高 Federated Learning 中的公平性和鲁棒性，适用于资源受限和多样化环境。</li>
<li>methods: 使用 acceleration primal dual (APD) 算法，利用偏好 corrected local steps (as in Scaffold) 实现更高效的通信协调和更快的收敛速度。</li>
<li>results: 在多个 benchmark 数据集上测试，Scaff-PD 能够提高公平性和鲁棒性，同时保持竞争性的准确率。<details>
<summary>Abstract</summary>
We present Scaff-PD, a fast and communication-efficient algorithm for distributionally robust federated learning. Our approach improves fairness by optimizing a family of distributionally robust objectives tailored to heterogeneous clients. We leverage the special structure of these objectives, and design an accelerated primal dual (APD) algorithm which uses bias corrected local steps (as in Scaffold) to achieve significant gains in communication efficiency and convergence speed. We evaluate Scaff-PD on several benchmark datasets and demonstrate its effectiveness in improving fairness and robustness while maintaining competitive accuracy. Our results suggest that Scaff-PD is a promising approach for federated learning in resource-constrained and heterogeneous settings.
</details>
<details>
<summary>摘要</summary>
我团队现请Scaff-PD，一种快速并通信效率高的分布robust federated learning算法。我们的方法通过优化适应于异构客户端的分布robust目标函数来提高公平性。我们利用这些目标函数的特殊结构，并设计了一种加速的 principales dual（APD）算法，使用偏好修正的本地步骤（如Scaffold）来实现重要的通信效率和速度增加。我们在多个 Referenced datasets上评估Scaff-PD，并证明其在保持竞争性准确性的情况下提高公平性和鲁棒性。我们的结果表明Scaff-PD是一种有前途的approach federated learning中的资源受限和异构环境中。
</details></li>
</ul>
<hr>
<h2 id="Submodular-Reinforcement-Learning"><a href="#Submodular-Reinforcement-Learning" class="headerlink" title="Submodular Reinforcement Learning"></a>Submodular Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13372">http://arxiv.org/abs/2307.13372</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/manish-pra/non-additive-rl">https://github.com/manish-pra/non-additive-rl</a></li>
<li>paper_authors: Manish Prajapat, Mojmír Mutný, Melanie N. Zeilinger, Andreas Krause</li>
<li>for: 这个论文是为了解决强迫学习（RL）中的奖励问题，奖励通常是加法的，但在许多重要应用中，奖励具有减少返回的特点，例如覆盖控制、实验设计和信息 PATH 规划。</li>
<li>methods: 作者提出了一种新的概念——submodular RL（SubRL），它寻找更一般、非加法（历史相互作用）的奖励模型，使用 submodular 集合函数来捕捉减少返回的特点。然而，在总的来说，即使在表格设定中，这种优化问题是Difficult to approximate。</li>
<li>results: 作者提出了一种简单的policy gradient算法——SubPO，它可以处理非加法奖励。SubPO 可以在一些假设下 recuperate 优化的常数因子应用，并且在大 state-和 action- 空间下可以进行本地优化。作者通过应用 SubPO 到不同的应用中，如生物多样性监测、抽象实验设计、信息 PATH 规划和覆盖最大化，来展示其效果。结果表明 SubPO 具有高效的样本使用和可扩展性。<details>
<summary>Abstract</summary>
In reinforcement learning (RL), rewards of states are typically considered additive, and following the Markov assumption, they are $\textit{independent}$ of states visited previously. In many important applications, such as coverage control, experiment design and informative path planning, rewards naturally have diminishing returns, i.e., their value decreases in light of similar states visited previously. To tackle this, we propose $\textit{submodular RL}$ (SubRL), a paradigm which seeks to optimize more general, non-additive (and history-dependent) rewards modelled via submodular set functions which capture diminishing returns. Unfortunately, in general, even in tabular settings, we show that the resulting optimization problem is hard to approximate. On the other hand, motivated by the success of greedy algorithms in classical submodular optimization, we propose SubPO, a simple policy gradient-based algorithm for SubRL that handles non-additive rewards by greedily maximizing marginal gains. Indeed, under some assumptions on the underlying Markov Decision Process (MDP), SubPO recovers optimal constant factor approximations of submodular bandits. Moreover, we derive a natural policy gradient approach for locally optimizing SubRL instances even in large state- and action- spaces. We showcase the versatility of our approach by applying SubPO to several applications, such as biodiversity monitoring, Bayesian experiment design, informative path planning, and coverage maximization. Our results demonstrate sample efficiency, as well as scalability to high-dimensional state-action spaces.
</details>
<details>
<summary>摘要</summary>
在增强学习（RL）中，状态奖励通常是加法的，并且根据马可夫假设，它们是独立的。在许多重要应用中，如覆盖控制、实验设计和有益路径规划，奖励自然地具有减少的返回，即在相似的状态前后访问的情况下，奖励的价值逐渐减少。为解决这个问题，我们提议了“增强RL”（SubRL），一种潜在优化更一般、非加法（历史相依）奖励的 paradigma。然而，在总的来说，即使在表格设置中，我们显示出的优化问题是难以估算的。在这种情况下，我们提出了一种简单的政策梯度方法，即SubPO，用于解决SubRL问题。SubPO通过积极地最大化权重的增加来处理非加法奖励。在某些假设下，SubPO可以在MDP中获得优化的常量因子approximation。此外，我们还 deriv了一种自然的政策梯度方法，用于本地优化SubRL实例，即使在大的状态和动作空间中。我们在各种应用中应用SubPO，如生物多样性监测、推论实验设计、有益路径规划和覆盖最大化。我们的结果显示了样本效率，以及可扩展性。
</details></li>
</ul>
<hr>
<h2 id="Learning-Regions-of-Interest-for-Bayesian-Optimization-with-Adaptive-Level-Set-Estimation"><a href="#Learning-Regions-of-Interest-for-Bayesian-Optimization-with-Adaptive-Level-Set-Estimation" class="headerlink" title="Learning Regions of Interest for Bayesian Optimization with Adaptive Level-Set Estimation"></a>Learning Regions of Interest for Bayesian Optimization with Adaptive Level-Set Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13371">http://arxiv.org/abs/2307.13371</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fengxue Zhang, Jialin Song, James Bowden, Alexander Ladd, Yisong Yue, Thomas A. Desautels, Yuxin Chen</li>
<li>for: 这篇论文是关于 Bayesian 优化 (BO) 在高维和非站ARY enario 中的研究。</li>
<li>methods: 论文提出了一个框架，called BALLET，它可以在高维和非站ARY enario 中实现 Bayesian 优化。BALLET 使用了两个 probabilistic 模型：一个粗糙的 Gaussian 过程 (GP) 来识别高信度区域 (ROI)，以及一个本地化的 GP 来优化在 ROI 中。</li>
<li>results: 论文证明了 BALLET 可以有效缩小搜索空间，并且可以比标准 BO 方法更紧的对应 regret bound。论文还进行了实验证明，证明了 BALLET 在实际应用中的效果。<details>
<summary>Abstract</summary>
We study Bayesian optimization (BO) in high-dimensional and non-stationary scenarios. Existing algorithms for such scenarios typically require extensive hyperparameter tuning, which limits their practical effectiveness. We propose a framework, called BALLET, which adaptively filters for a high-confidence region of interest (ROI) as a superlevel-set of a nonparametric probabilistic model such as a Gaussian process (GP). Our approach is easy to tune, and is able to focus on local region of the optimization space that can be tackled by existing BO methods. The key idea is to use two probabilistic models: a coarse GP to identify the ROI, and a localized GP for optimization within the ROI. We show theoretically that BALLET can efficiently shrink the search space, and can exhibit a tighter regret bound than standard BO without ROI filtering. We demonstrate empirically the effectiveness of BALLET on both synthetic and real-world optimization tasks.
</details>
<details>
<summary>摘要</summary>
我们研究 bayesian 优化（BO）在高维和非站点场景下。现有的算法通常需要广泛的 гипер参数调整，这限制了它们的实际效果。我们提出了一个框架，叫做 BALLET，它可以动态筛选出高信息域的兴趣点（ROI），作为非参数型 probabilistic 模型，如 Gaussian process（GP）的超级集。我们的方法容易调整，可以将关注点放在可以由现有的 BO 方法解决的本地优化空间上。关键思想是使用两种 probabilistic 模型：一个粗细的 GP 来识别 ROI，并一个局部化的 GP 进行优化在 ROI 中。我们证明了 BALLET 可以有效缩小搜索空间，并可以比标准 BO 无 ROI 筛选更紧的 regret  bound。我们在 sintetic 和实际优化任务上证明了 BALLET 的实际效果。
</details></li>
</ul>
<hr>
<h2 id="Computational-Guarantees-for-Doubly-Entropic-Wasserstein-Barycenters-via-Damped-Sinkhorn-Iterations"><a href="#Computational-Guarantees-for-Doubly-Entropic-Wasserstein-Barycenters-via-Damped-Sinkhorn-Iterations" class="headerlink" title="Computational Guarantees for Doubly Entropic Wasserstein Barycenters via Damped Sinkhorn Iterations"></a>Computational Guarantees for Doubly Entropic Wasserstein Barycenters via Damped Sinkhorn Iterations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13370">http://arxiv.org/abs/2307.13370</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lénaïc Chizat, Tomas Vaškevičius</li>
<li>for:  Computation of doubly regularized Wasserstein barycenters</li>
<li>methods:  Damped Sinkhorn iterations followed by exact maximization&#x2F;minimization steps</li>
<li>results:  Convergence guarantees for any choice of regularization parameters, and non-asymptotic convergence guarantees for approximating Wasserstein barycenters between discrete point clouds in the free-support&#x2F;grid-free setting.Here’s the format you requested:</li>
<li>for: &lt;what are the paper written for?&gt;</li>
<li>methods: &lt;what methods the paper use?&gt;</li>
<li>results: &lt;what results the paper get?&gt;I hope that helps!<details>
<summary>Abstract</summary>
We study the computation of doubly regularized Wasserstein barycenters, a recently introduced family of entropic barycenters governed by inner and outer regularization strengths. Previous research has demonstrated that various regularization parameter choices unify several notions of entropy-penalized barycenters while also revealing new ones, including a special case of debiased barycenters. In this paper, we propose and analyze an algorithm for computing doubly regularized Wasserstein barycenters. Our procedure builds on damped Sinkhorn iterations followed by exact maximization/minimization steps and guarantees convergence for any choice of regularization parameters. An inexact variant of our algorithm, implementable using approximate Monte Carlo sampling, offers the first non-asymptotic convergence guarantees for approximating Wasserstein barycenters between discrete point clouds in the free-support/grid-free setting.
</details>
<details>
<summary>摘要</summary>
我们研究双正则化 Wasserstein 质心的计算，这是最近引入的一种内外正则化强度控制的泛化积分质心。先前的研究表明，不同的正则化参数选择可以统一各种束缚积分质心，同时还可以揭示新的质心，包括特殊情况下的减偏积分质心。在这篇文章中，我们提出了一种计算双正则化 Wasserstein 质心的算法，该算法基于抑制式谱散数据进行融合，然后使用精确的最大化/最小化步骤，可以保证任何正则化参数选择的收敛。在实际应用中，我们还提出了一种使用伪随机抽样来实现准确的质心计算的不精准变体，这是自由支持/网格自由设置下的第一个不对称收敛保证的方法。
</details></li>
</ul>
<hr>
<h2 id="Prot2Text-Multimodal-Protein’s-Function-Generation-with-GNNs-and-Transformers"><a href="#Prot2Text-Multimodal-Protein’s-Function-Generation-with-GNNs-and-Transformers" class="headerlink" title="Prot2Text: Multimodal Protein’s Function Generation with GNNs and Transformers"></a>Prot2Text: Multimodal Protein’s Function Generation with GNNs and Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14367">http://arxiv.org/abs/2307.14367</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hadi Abdine, Michail Chatzianastasis, Costas Bouyioukos, Michalis Vazirgiannis</li>
<li>for: 这篇论文的目的是提出一个新的方法来预测蛋白质的功能，这个方法使用 Graph Neural Networks(GNNs) 和 Large Language Models(LLMs) 在encoder-decoder架构中结合，以生成蛋白质功能的详细描述。</li>
<li>methods: 这篇论文使用的方法是一种 multimodal 方法，结合蛋白质的序列、结构和文本描述，使用 GNNs 和 LLMs 进行融合，实现蛋白质功能的全面表示。</li>
<li>results: 这篇论文的实验结果显示，这个新的方法可以实现更高的预测精度，并且可以生成蛋白质功能的详细描述。<details>
<summary>Abstract</summary>
The complex nature of big biological systems pushed some scientists to classify its understanding under the inconceivable missions. Different leveled challenges complicated this task, one of is the prediction of a protein's function. In recent years, significant progress has been made in this field through the development of various machine learning approaches. However, most existing methods formulate the task as a multi-classification problem, i.e assigning predefined labels to proteins. In this work, we propose a novel approach, \textbf{Prot2Text}, which predicts a protein function's in a free text style, moving beyond the conventional binary or categorical classifications. By combining Graph Neural Networks(GNNs) and Large Language Models(LLMs), in an encoder-decoder framework, our model effectively integrates diverse data types including proteins' sequences, structures, and textual annotations. This multimodal approach allows for a holistic representation of proteins' functions, enabling the generation of detailed and accurate descriptions. To evaluate our model, we extracted a multimodal protein dataset from SwissProt, and demonstrate empirically the effectiveness of Prot2Text. These results highlight the transformative impact of multimodal models, specifically the fusion of GNNs and LLMs, empowering researchers with powerful tools for more accurate prediction of proteins' functions. The code, the models and a demo will be publicly released.
</details>
<details>
<summary>摘要</summary>
大生物系统的复杂性让一些科学家将其理解列入不可思议任务之列。不同的等级挑战困扰了这个任务，其中之一是蛋白质功能预测。在过去几年，我们在这一领域进行了重要的进步，通过开发多种机器学习方法。然而，大多数现有方法将任务定义为多类别问题，即将蛋白质分配预先定义的标签。在这种情况下，我们提出了一种新的方法——Prot2Text，它预测蛋白质功能在自由文本格式下，超越传统的二分或分类化预测。我们通过将图 neural network（GNN）和大型自然语言模型（LLM）组合在encoder-decoder框架中，能够集成多种蛋白质数据类型，包括序列、结构和文本注释。这种多模式方法允许我们对蛋白质功能进行整体表示，使得生成详细和准确的描述。为评估我们的模型，我们从SwissProt中提取了多模式蛋白质数据集，并通过实验证明Prot2Text的效果。这些结果显示了多模式模型的融合，特别是GNN和LLM的融合，为研究人员提供了更加准确的蛋白质功能预测工具。代码、模型和demo将公共发布。
</details></li>
</ul>
<hr>
<h2 id="High-Dimensional-Distributed-Gradient-Descent-with-Arbitrary-Number-of-Byzantine-Attackers"><a href="#High-Dimensional-Distributed-Gradient-Descent-with-Arbitrary-Number-of-Byzantine-Attackers" class="headerlink" title="High Dimensional Distributed Gradient Descent with Arbitrary Number of Byzantine Attackers"></a>High Dimensional Distributed Gradient Descent with Arbitrary Number of Byzantine Attackers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13352">http://arxiv.org/abs/2307.13352</a></li>
<li>repo_url: None</li>
<li>paper_authors: Puning Zhao, Zhiguo Wan</li>
<li>for: 这篇论文是关于robust分布式学习，拥有Byzantine失败的情况下的研究。</li>
<li>methods: 该方法采用了直方正方向的半验证方法，可以在高维问题上进行解决，并且可以适应arbitrary数量的Byzantine攻击者。</li>
<li>results: 我们的研究表明，该方法可以在高维问题上实现最佳的统计效果，并且与前一些研究相比，它在维度上具有更好的性能。<details>
<summary>Abstract</summary>
Robust distributed learning with Byzantine failures has attracted extensive research interests in recent years. However, most of existing methods suffer from curse of dimensionality, which is increasingly serious with the growing complexity of modern machine learning models. In this paper, we design a new method that is suitable for high dimensional problems, under arbitrary number of Byzantine attackers. The core of our design is a direct high dimensional semi-verified mean estimation method. Our idea is to identify a subspace first. The components of mean value perpendicular to this subspace can be estimated via gradient vectors uploaded from worker machines, while the components within this subspace are estimated using auxiliary dataset. We then use our new method as the aggregator of distributed learning problems. Our theoretical analysis shows that the new method has minimax optimal statistical rates. In particular, the dependence on dimensionality is significantly improved compared with previous works.
</details>
<details>
<summary>摘要</summary>
robust 分布式学习受到了最近几年的广泛研究兴趣。然而，大多数现有方法受到了维度灾难的拥挤，这在现代机器学习模型的复杂度逐渐增加时变得越来越严重。在这篇论文中，我们设计了适用于高维问题的新方法，可以抗 resist 任意数量的拜占庭攻击者。我们的设计核心在于直接使用高维半验证平均值计算方法。我们的想法是先Identify一个子空间，然后通过上传 worker 机器的梯度向量来计算沿着这个子空间的方向的部分，而在这个子空间内部使用 auxillary 数据来计算剩余的部分。我们然后使用我们的新方法来汇集分布式学习问题。我们的理论分析表明，我们的新方法具有最优的最小最大统计率。具体来说，与前一些工作相比，我们的方法在维度方面具有显著的改进。
</details></li>
</ul>
<hr>
<h2 id="Explainable-Disparity-Compensation-for-Efficient-Fair-Ranking"><a href="#Explainable-Disparity-Compensation-for-Efficient-Fair-Ranking" class="headerlink" title="Explainable Disparity Compensation for Efficient Fair Ranking"></a>Explainable Disparity Compensation for Efficient Fair Ranking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14366">http://arxiv.org/abs/2307.14366</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abraham Gale, Amélie Marian</li>
<li>for: This paper aims to address the issue of disparate outcomes in decision systems, specifically in ranking functions, and proposes data-driven compensatory measures to improve fairness.</li>
<li>methods: The proposed measures rely on generating bonus points for members of underrepresented groups to address disparity in the ranking function. Efficient sampling-based algorithms are used to calculate the number of bonus points to minimize disparity.</li>
<li>results: The authors validate their algorithms using real-world school admissions and recidivism datasets, and compare their results with those of existing fair ranking algorithms. The results show that their proposed measures can effectively improve fairness in the ranking function.Here’s the full text in Simplified Chinese:</li>
<li>for: 这篇论文目标是解决决策系统中的不平等结果问题，具体来说是对排名函数中的不平等进行补偿。</li>
<li>methods: 提议的补偿措施基于为受排除群体成员分配加分点，以解决排名函数中的不平等。 authors使用高效的采样算法来计算加分点的数量，以最小化不平等。</li>
<li>results: authors使用实际的学校招生和重犯罪数据集来验证他们的算法，并与现有的公平排名算法进行比较。结果表明，提议的补偿措施可以有效地提高排名函数的公平性。<details>
<summary>Abstract</summary>
Ranking functions that are used in decision systems often produce disparate results for different populations because of bias in the underlying data. Addressing, and compensating for, these disparate outcomes is a critical problem for fair decision-making. Recent compensatory measures have mostly focused on opaque transformations of the ranking functions to satisfy fairness guarantees or on the use of quotas or set-asides to guarantee a minimum number of positive outcomes to members of underrepresented groups. In this paper we propose easily explainable data-driven compensatory measures for ranking functions. Our measures rely on the generation of bonus points given to members of underrepresented groups to address disparity in the ranking function. The bonus points can be set in advance, and can be combined, allowing for considering the intersections of representations and giving better transparency to stakeholders. We propose efficient sampling-based algorithms to calculate the number of bonus points to minimize disparity. We validate our algorithms using real-world school admissions and recidivism datasets, and compare our results with that of existing fair ranking algorithms.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Feature-Importance-Measurement-based-on-Decision-Tree-Sampling"><a href="#Feature-Importance-Measurement-based-on-Decision-Tree-Sampling" class="headerlink" title="Feature Importance Measurement based on Decision Tree Sampling"></a>Feature Importance Measurement based on Decision Tree Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13333">http://arxiv.org/abs/2307.13333</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tsudalab/dt-sampler">https://github.com/tsudalab/dt-sampler</a></li>
<li>paper_authors: Chao Huang, Diptesh Das, Koji Tsuda</li>
<li>for: 用于提高树基型模型中FeatureImportance的可 interpretability和稳定性。</li>
<li>methods: 使用SAT理论来测试FeatureImportance，具有 fewer parameters 和更高的可 interpretability，适用于实际问题。</li>
<li>results: 在实际问题中，DT-Sampler可以提供更高的可 interpretability和稳定性，并且比Random Forest具有更少的参数。Translation:</li>
<li>for: Used to improve the interpretability and stability of feature importance in tree-based models.</li>
<li>methods: Uses SAT theory to test feature importance, with fewer parameters and higher interpretability, applicable to real-world problems.</li>
<li>results: In practical problems, DT-Sampler can provide higher interpretability and stability, and has fewer parameters than Random Forest.<details>
<summary>Abstract</summary>
Random forest is effective for prediction tasks but the randomness of tree generation hinders interpretability in feature importance analysis. To address this, we proposed DT-Sampler, a SAT-based method for measuring feature importance in tree-based model. Our method has fewer parameters than random forest and provides higher interpretability and stability for the analysis in real-world problems. An implementation of DT-Sampler is available at https://github.com/tsudalab/DT-sampler.
</details>
<details>
<summary>摘要</summary>
随机森林可以很有效地进行预测任务，但随机生成树的randomness会降低特征重要性的解释性。为解决这个问题，我们提出了DT-Sampler，一种基于SAT的方法来测量树型模型中特征的重要性。我们的方法有 fewer parameters than random forest，并且在实际问题中提供了更高的解释性和稳定性。DT-Sampler的实现可以在https://github.com/tsudalab/DT-sampler中找到。
</details></li>
</ul>
<hr>
<h2 id="The-Optimal-Approximation-Factors-in-Misspecified-Off-Policy-Value-Function-Estimation"><a href="#The-Optimal-Approximation-Factors-in-Misspecified-Off-Policy-Value-Function-Estimation" class="headerlink" title="The Optimal Approximation Factors in Misspecified Off-Policy Value Function Estimation"></a>The Optimal Approximation Factors in Misspecified Off-Policy Value Function Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13332">http://arxiv.org/abs/2307.13332</a></li>
<li>repo_url: None</li>
<li>paper_authors: Philip Amortila, Nan Jiang, Csaba Szepesvári</li>
<li>for: 这篇论文主要针对 linear off-policy value function estimation 问题进行研究，具体来说是研究函数approximation factor在不同设置下的优化形式。</li>
<li>methods: 论文使用了各种方法来研究函数approximation factor，包括使用权重$L_2$ norm、$L_\infty$ norm、状态别名和完整&#x2F;半 coverage的state space。</li>
<li>results: 论文的结果显示，在不同的设置下，函数approximation factor的优化形式是不同的，并且可以确定具体的常数因素。特别是，$L_2(\mu)$ norm 下的两个实例特定因素和 $L_\infty$ norm 下的一个常数因素被证明为决定了偏离策略评估的困难程度。<details>
<summary>Abstract</summary>
Theoretical guarantees in reinforcement learning (RL) are known to suffer multiplicative blow-up factors with respect to the misspecification error of function approximation. Yet, the nature of such \emph{approximation factors} -- especially their optimal form in a given learning problem -- is poorly understood. In this paper we study this question in linear off-policy value function estimation, where many open questions remain. We study the approximation factor in a broad spectrum of settings, such as with the weighted $L_2$-norm (where the weighting is the offline state distribution), the $L_\infty$ norm, the presence vs. absence of state aliasing, and full vs. partial coverage of the state space. We establish the optimal asymptotic approximation factors (up to constants) for all of these settings. In particular, our bounds identify two instance-dependent factors for the $L_2(\mu)$ norm and only one for the $L_\infty$ norm, which are shown to dictate the hardness of off-policy evaluation under misspecification.
</details>
<details>
<summary>摘要</summary>
theoretically guarantees in reinforcement learning (RL) are known to suffer multiplicative blow-up factors with respect to the misspecification error of function approximation. yet, the nature of such \emph{approximation factors} -- especially their optimal form in a given learning problem -- is poorly understood. in this paper, we study this question in linear off-policy value function estimation, where many open questions remain. we study the approximation factor in a broad spectrum of settings, such as with the weighted $L_2$-norm (where the weighting is the offline state distribution), the $L_\infty$ norm, the presence vs. absence of state aliasing, and full vs. partial coverage of the state space. we establish the optimal asymptotic approximation factors (up to constants) for all of these settings. in particular, our bounds identify two instance-dependent factors for the $L_2(\mu)$ norm and only one for the $L_\infty$ norm, which are shown to dictate the hardness of off-policy evaluation under misspecification.Note that Simplified Chinese is a written language, and the translation is based on the standardized grammar and vocabulary of Simplified Chinese. However, the actual translation may vary depending on the specific context and register used in the original text.
</details></li>
</ul>
<hr>
<h2 id="Unleash-the-Power-of-Context-Enhancing-Large-Scale-Recommender-Systems-with-Context-Based-Prediction-Models"><a href="#Unleash-the-Power-of-Context-Enhancing-Large-Scale-Recommender-Systems-with-Context-Based-Prediction-Models" class="headerlink" title="Unleash the Power of Context: Enhancing Large-Scale Recommender Systems with Context-Based Prediction Models"></a>Unleash the Power of Context: Enhancing Large-Scale Recommender Systems with Context-Based Prediction Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01231">http://arxiv.org/abs/2308.01231</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan Hartman, Assaf Klein, Davorin Kopič, Natalia Silberstein</li>
<li>for: 提高大规模商业推荐系统的性能，具有广泛的个性化推荐应用场景。</li>
<li>methods: 基于用户和上下文特征的预测模型，不考虑物品特征，可以减少服务成本。</li>
<li>results: 实验表明，这种方法可以在线上和离线上的商业指标中带来显著改善，而且对服务成本的影响很小。<details>
<summary>Abstract</summary>
In this work, we introduce the notion of Context-Based Prediction Models. A Context-Based Prediction Model determines the probability of a user's action (such as a click or a conversion) solely by relying on user and contextual features, without considering any specific features of the item itself. We have identified numerous valuable applications for this modeling approach, including training an auxiliary context-based model to estimate click probability and incorporating its prediction as a feature in CTR prediction models. Our experiments indicate that this enhancement brings significant improvements in offline and online business metrics while having minimal impact on the cost of serving. Overall, our work offers a simple and scalable, yet powerful approach for enhancing the performance of large-scale commercial recommender systems, with broad implications for the field of personalized recommendations.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们介绍了基于上下文的预测模型。这种预测模型根据用户和上下文特征决定用户行为（如点击或购买）的概率，不考虑物品本身的特征。我们已经认为这种模型途径具有很多有价值的应用，包括培养一个辅助上下文基于模型来估计点击概率，并将其预测作为ctr预测模型中的一个特征。我们的实验表明，这种增强可以在线上和Offline商业指标方面带来显著改善，而无需增加服务成本。总之，我们的工作提供了一种简单、可扩展、 yet 具有强大能力的方法来提高大规模的商业推荐系统的性能，对个人化推荐领域产生广泛的影响。
</details></li>
</ul>
<hr>
<h2 id="QuIP-2-Bit-Quantization-of-Large-Language-Models-With-Guarantees"><a href="#QuIP-2-Bit-Quantization-of-Large-Language-Models-With-Guarantees" class="headerlink" title="QuIP: 2-Bit Quantization of Large Language Models With Guarantees"></a>QuIP: 2-Bit Quantization of Large Language Models With Guarantees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13304">http://arxiv.org/abs/2307.13304</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jerry-chee/quip">https://github.com/jerry-chee/quip</a></li>
<li>paper_authors: Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, Christopher De Sa</li>
<li>for: 本研究探讨大语言模型（LLM）后期参数归一化。</li>
<li>methods: 我们提出了一种基于干扰量和矩阵方向的归一化方法（QuIP），包括两个步骤：(1) 适应归一化过程中的二次proxy目标函数; (2) 高效的预处理和后处理，通过随机正交矩阵来保证参数和偏差矩阵的不一致。</li>
<li>results: 我们的实验表明，QuIP可以提高多种现有的归一化算法的性能，并且在只使用两个位数据时实现了首个可行的LLM归一化方法。我们的代码可以在<a target="_blank" rel="noopener" href="https://github.com/jerry-chee/QuIP%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/jerry-chee/QuIP上找到。</a><details>
<summary>Abstract</summary>
This work studies post-training parameter quantization in large language models (LLMs). We introduce quantization with incoherence processing (QuIP), a new method based on the insight that quantization benefits from incoherent weight and Hessian matrices, i.e., from the weights and the directions in which it is important to round them accurately being unaligned with the coordinate axes. QuIP consists of two steps: (1) an adaptive rounding procedure minimizing a quadratic proxy objective; (2) efficient pre- and post-processing that ensures weight and Hessian incoherence via multiplication by random orthogonal matrices. We complement QuIP with the first theoretical analysis for an LLM-scale quantization algorithm, and show that our theory also applies to an existing method, OPTQ. Empirically, we find that our incoherence preprocessing improves several existing quantization algorithms and yields the first LLM quantization methods that produce viable results using only two bits per weight. Our code can be found at https://github.com/jerry-chee/QuIP .
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>An adaptive rounding procedure that minimizes a quadratic proxy objective.2. Efficient pre- and post-processing that ensures weight and Hessian incoherence through multiplication by random orthogonal matrices.We also provide the first theoretical analysis for an LLM-scale quantization algorithm and show that our theory applies to an existing method, OPTQ. Our experiments demonstrate that our incoherence preprocessing improves several existing quantization algorithms and yields the first LLM quantization methods that produce viable results using only two bits per weight. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/jerry-chee/QuIP">https://github.com/jerry-chee/QuIP</a>.</details></li>
</ol>
<hr>
<h2 id="Word-Sense-Disambiguation-as-a-Game-of-Neurosymbolic-Darts"><a href="#Word-Sense-Disambiguation-as-a-Game-of-Neurosymbolic-Darts" class="headerlink" title="Word Sense Disambiguation as a Game of Neurosymbolic Darts"></a>Word Sense Disambiguation as a Game of Neurosymbolic Darts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16663">http://arxiv.org/abs/2307.16663</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tiansi Dong, Rafet Sifa</li>
<li>for: 提高Word Sense Disambiguation（WSD）任务的性能，突破深度学习方法的“玻璃天花”</li>
<li>methods: 提出了一种新的神经符号方法，利用嵌入式的方法，通过嵌入式的方法来实现简单的逻辑推理，并通过游戏“dart”来训练Transformer模型</li>
<li>results: 在多个测试数据集上达到了F1分数的90%以上，并且在不同的n-ball嵌入中得到了70%-75%的覆盖率，表明该方法可以超越深度学习方法的性能 bound.<details>
<summary>Abstract</summary>
Word Sense Disambiguation (WSD) is one of the hardest tasks in natural language understanding and knowledge engineering. The glass ceiling of 80% F1 score is recently achieved through supervised deep-learning, enriched by a variety of knowledge graphs. Here, we propose a novel neurosymbolic methodology that is able to push the F1 score above 90%. The core of our methodology is a neurosymbolic sense embedding, in terms of a configuration of nested balls in n-dimensional space. The centre point of a ball well-preserves word embedding, which partially fix the locations of balls. Inclusion relations among balls precisely encode symbolic hypernym relations among senses, and enable simple logic deduction among sense embeddings, which cannot be realised before. We trained a Transformer to learn the mapping from a contextualized word embedding to its sense ball embedding, just like playing the game of darts (a game of shooting darts into a dartboard). A series of experiments are conducted by utilizing pre-training n-ball embeddings, which have the coverage of around 70% training data and 75% testing data in the benchmark WSD corpus. The F1 scores in experiments range from 90.1% to 100.0% in all six groups of test data-sets (each group has 4 testing data with different sizes of n-ball embeddings). Our novel neurosymbolic methodology has the potential to break the ceiling of deep-learning approaches for WSD. Limitations and extensions of our current works are listed.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Modify-Training-Directions-in-Function-Space-to-Reduce-Generalization-Error"><a href="#Modify-Training-Directions-in-Function-Space-to-Reduce-Generalization-Error" class="headerlink" title="Modify Training Directions in Function Space to Reduce Generalization Error"></a>Modify Training Directions in Function Space to Reduce Generalization Error</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13290">http://arxiv.org/abs/2307.13290</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi Yu, Wenlian Lu, Boyu Chen</li>
<li>for: 提高神经网络模型的泛化性能</li>
<li>methods: 使用修改后的自然向导方法在神经网络函数空间进行 theoretically 分析，并利用 eigendecompositions 和统计学理论来derive 神经网络函数的泛化误差</li>
<li>results: 提出一个基于 eigendecompositions 和统计学理论的泛化误差减少方法，并通过数学示例证明该方法可以改善神经网络模型的泛化性能。此外，这种 theoretically 方法还可以解释许多现有的泛化提高方法的效果。<details>
<summary>Abstract</summary>
We propose theoretical analyses of a modified natural gradient descent method in the neural network function space based on the eigendecompositions of neural tangent kernel and Fisher information matrix. We firstly present analytical expression for the function learned by this modified natural gradient under the assumptions of Gaussian distribution and infinite width limit. Thus, we explicitly derive the generalization error of the learned neural network function using theoretical methods from eigendecomposition and statistics theory. By decomposing of the total generalization error attributed to different eigenspace of the kernel in function space, we propose a criterion for balancing the errors stemming from training set and the distribution discrepancy between the training set and the true data. Through this approach, we establish that modifying the training direction of the neural network in function space leads to a reduction in the total generalization error. Furthermore, We demonstrate that this theoretical framework is capable to explain many existing results of generalization enhancing methods. These theoretical results are also illustrated by numerical examples on synthetic data.
</details>
<details>
<summary>摘要</summary>
我们提出了一种基于神经网络函数空间的修改后自然算法的理论分析。我们首先提出了假设 Gaussian 分布和无限宽限的情况下，修改后自然算法所学习的函数的analytical表达。这使我们可以明确地 Compute 修改后神经网络函数的通用错误。我们还将Total generalization error decomposed into different eigenspace of the kernel in function space，并提出一个均衡错误的对象，以减少Total generalization error。此外，我们还证明了这个理论框架可以解释许多现有的通用提升方法的结果。这些理论结果也通过了实验示例 validate 在Synthetic data 上。Here's the translation in Traditional Chinese:我们提出了一种基于神经网络函数空间的修改后自然算法的理论分析。我们首先提出了假设 Gaussian 分布和无限宽限的情况下，修改后自然算法所学习的函数的analytical表达。这使我们可以明确地 Compute 修改后神经网络函数的通用错误。我们还将Total generalization error decomposed into different eigenspace of the kernel in function space，并提出一个均衡错误的对象，以减少Total generalization error。此外，我们还证明了这个理论框架可以解释许多现有的通用提升方法的结果。这些理论结果也通过了实验示例 validate 在Synthetic data 上。
</details></li>
</ul>
<hr>
<h2 id="Curvature-based-Transformer-for-Molecular-Property-Prediction"><a href="#Curvature-based-Transformer-for-Molecular-Property-Prediction" class="headerlink" title="Curvature-based Transformer for Molecular Property Prediction"></a>Curvature-based Transformer for Molecular Property Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13275">http://arxiv.org/abs/2307.13275</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yili Chen, Zhengyu Li, Zheng Wan, Hui Yu, Xian Wei</li>
<li>for: 提高基于人工智能的药物设计中分子属性预测的能力</li>
<li>methods: 引入Discretization of Ricci Curvature来提高图граaph神经网络模型对分子图数据的结构信息抽取能力</li>
<li>results: 在PCQM4M-LST、MoleculeNet等化学分子数据集上进行实验，与Uni-Mol、Graphormer等模型进行比较，结果表明该方法可以达到当前最佳结果。另外，Discretized Ricci curvature还能够捕捉分子结构和功能关系的信息，描述分子图数据的地方几何特征。<details>
<summary>Abstract</summary>
The prediction of molecular properties is one of the most important and challenging tasks in the field of artificial intelligence-based drug design. Among the current mainstream methods, the most commonly used feature representation for training DNN models is based on SMILES and molecular graphs, although these methods are concise and effective, they also limit the ability to capture spatial information. In this work, we propose Curvature-based Transformer to improve the ability of Graph Transformer neural network models to extract structural information on molecular graph data by introducing Discretization of Ricci Curvature. To embed the curvature in the model, we add the curvature information of the graph as positional Encoding to the node features during the attention-score calculation. This method can introduce curvature information from graph data without changing the original network architecture, and it has the potential to be extended to other models. We performed experiments on chemical molecular datasets including PCQM4M-LST, MoleculeNet and compared with models such as Uni-Mol, Graphormer, and the results show that this method can achieve the state-of-the-art results. It is proved that the discretized Ricci curvature also reflects the structural and functional relationship while describing the local geometry of the graph molecular data.
</details>
<details>
<summary>摘要</summary>
“分子质量预测是人工智能基于药物设计的一个最重要和挑战性任务。现今主流方法中，最常用的特征表示方法是基于SMILES和分子图，尽管这些方法简洁有效，但它们也限制了捕捉空间信息的能力。在这种工作中，我们提出了几何基于 transformer 的 Curvature-based Transformer，以提高对分子图数据的结构信息抽取能力。在计算注意力分布时，我们添加了图的 Ricci 曲率信息作为 pozitional Encoding，以将曲率信息 embed 到节点特征中。这种方法可以在不改变原始网络结构的情况下，将曲率信息从图数据中引入到模型中，并且具有扩展性。我们在 PCQM4M-LST、MoleculeNet 等化学分子数据集上进行了实验，与 Uni-Mol、Graphormer 等模型进行比较，结果显示，这种方法可以达到领先的成绩。这也证明了对分子图数据的几何基本特征的描述，同时还能够反映分子的结构和功能关系。”
</details></li>
</ul>
<hr>
<h2 id="Unbiased-Weight-Maximization"><a href="#Unbiased-Weight-Maximization" class="headerlink" title="Unbiased Weight Maximization"></a>Unbiased Weight Maximization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13270">http://arxiv.org/abs/2307.13270</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephen Chung</li>
<li>for: 本研究旨在提出一种生物学上有效的人工神经网络（ANN）训练方法，即对每个单元视为杂种学习（RL）代理，以视网膜为一个团队。</li>
<li>methods: 本方法使用REINFORCE算法，但是由于单元之间的信息不准确传递，会导致学习过程缓慢，难以扩展到大规模网络。为解决这个问题，提出了Weight Maximization方法，即每个隐藏单元可以通过自己的出口权重的 нор 来最大化自己的学习效果。</li>
<li>results: 研究人员分析了Weight Maximization方法的理论性质，并提出了一种变体Unbiased Weight Maximization。这种新方法可以提供一种不偏学习规则，使学习速度加快，并在网络规模增加时保持良好的性能。具体来说，这是目前所知道的第一种可以快速学习、适用于大规模网络的不偏学习规则。<details>
<summary>Abstract</summary>
A biologically plausible method for training an Artificial Neural Network (ANN) involves treating each unit as a stochastic Reinforcement Learning (RL) agent, thereby considering the network as a team of agents. Consequently, all units can learn via REINFORCE, a local learning rule modulated by a global reward signal, which aligns more closely with biologically observed forms of synaptic plasticity. Nevertheless, this learning method is often slow and scales poorly with network size due to inefficient structural credit assignment, since a single reward signal is broadcast to all units without considering individual contributions. Weight Maximization, a proposed solution, replaces a unit's reward signal with the norm of its outgoing weight, thereby allowing each hidden unit to maximize the norm of the outgoing weight instead of the global reward signal. In this research report, we analyze the theoretical properties of Weight Maximization and propose a variant, Unbiased Weight Maximization. This new approach provides an unbiased learning rule that increases learning speed and improves asymptotic performance. Notably, to our knowledge, this is the first learning rule for a network of Bernoulli-logistic units that is unbiased and scales well with the number of network's units in terms of learning speed.
</details>
<details>
<summary>摘要</summary>
一种生物学可能性的方法 для训练人工神经网络（ANN）是将每个单元视为一个随机反弹学习（RL）代理，从而考虑神经网络为一支代理队伍。因此，所有单元都可以通过REINFORCE本地学习规则，该规则由全局奖励信号修饰，与生物观察到的 synaptic plasticity更加相似。然而，这种学习方法通常慢和网络大小不好扩展，因为不效的结构归因分配，单个奖励信号 Broadcast 到所有单元而无法考虑单元之间的贡献。Weight Maximization，一种提议的解决方案，将单元的奖励信号替换为出口重量的norm， allowing each hidden unit to maximize the norm of the outgoing weight instead of the global reward signal。在这份研究报告中，我们分析Weight Maximization的理论性质和提出一种变体，即不偏学习Weight Maximization。这种新的学习规则提供了一个不偏的学习规则，提高学习速度和长期性表现。值得注意的是，我们知道，这是一种可以扩展到神经网络中的 Bernoulli-logistic 单元数量的学习规则，并且与网络大小成直线关系。
</details></li>
</ul>
<hr>
<h2 id="Federated-K-Means-Clustering-via-Dual-Decomposition-based-Distributed-Optimization"><a href="#Federated-K-Means-Clustering-via-Dual-Decomposition-based-Distributed-Optimization" class="headerlink" title="Federated K-Means Clustering via Dual Decomposition-based Distributed Optimization"></a>Federated K-Means Clustering via Dual Decomposition-based Distributed Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13267">http://arxiv.org/abs/2307.13267</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vassilios Yfantis, Achim Wagner, Martin Ruskowski</li>
<li>for: This paper is written for researchers and practitioners interested in distributed optimization for machine learning, particularly in the context of $ K $-means clustering.</li>
<li>methods: The paper uses dual decomposition to solve the distributed training of $ K $-means clustering problems. The authors propose a mixed-integer quadratically constrained programming-based formulation of the clustering training problem and evaluate the performance of three optimization algorithms (subgradient method, bundle trust method, and quasi-Newton dual ascent algorithm) on a set of benchmark problems.</li>
<li>results: The paper demonstrates the potential of using dual decomposition for distributed training of $ K $-means clustering problems, but notes that the mixed-integer programming-based formulation of the clustering problems suffers from weak integer relaxations. The authors evaluate the performance of three optimization algorithms and show that the proposed approach can potentially enable an efficient solution in the future, both in a central and distributed setting.<details>
<summary>Abstract</summary>
The use of distributed optimization in machine learning can be motivated either by the resulting preservation of privacy or the increase in computational efficiency. On the one hand, training data might be stored across multiple devices. Training a global model within a network where each node only has access to its confidential data requires the use of distributed algorithms. Even if the data is not confidential, sharing it might be prohibitive due to bandwidth limitations. On the other hand, the ever-increasing amount of available data leads to large-scale machine learning problems. By splitting the training process across multiple nodes its efficiency can be significantly increased. This paper aims to demonstrate how dual decomposition can be applied for distributed training of $ K $-means clustering problems. After an overview of distributed and federated machine learning, the mixed-integer quadratically constrained programming-based formulation of the $ K $-means clustering training problem is presented. The training can be performed in a distributed manner by splitting the data across different nodes and linking these nodes through consensus constraints. Finally, the performance of the subgradient method, the bundle trust method, and the quasi-Newton dual ascent algorithm are evaluated on a set of benchmark problems. While the mixed-integer programming-based formulation of the clustering problems suffers from weak integer relaxations, the presented approach can potentially be used to enable an efficient solution in the future, both in a central and distributed setting.
</details>
<details>
<summary>摘要</summary>
使用分布式优化在机器学习中可以受到保持隐私和提高计算效率的两种动机。一个是训练数据可能会被存储在多个设备上，而每个节点只有访问自己的敏感数据时，需要使用分布式算法来训练全球模型。另一个是由于数据量的增加，导致大规模机器学习问题的出现。通过将训练过程分布到多个节点来加速其效率。这篇论文旨在演示如何使用分布式优化解决分布式训练 $ K $-means clustering问题。文章首先介绍分布式和联邦机器学习，然后提出基于杂Integer编程的$ K $-means clustering训练问题的混合形式。通过在不同节点上分布数据并通过共识约束相连接这些节点来进行分布式训练。最后，文章评估了在一组 benchmark 问题上的贪婪法、杂Integer法和 quasi-Newton  dual ascent 算法的性能。虽然杂Integer编程基本形式的 clustering 问题受到弱的整数放宽，但是该方法可能可以在未来的中心和分布式设置中启用高效的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Federated-Split-Learning-with-Only-Positive-Labels-for-resource-constrained-IoT-environment"><a href="#Federated-Split-Learning-with-Only-Positive-Labels-for-resource-constrained-IoT-environment" class="headerlink" title="Federated Split Learning with Only Positive Labels for resource-constrained IoT environment"></a>Federated Split Learning with Only Positive Labels for resource-constrained IoT environment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13266">http://arxiv.org/abs/2307.13266</a></li>
<li>repo_url: None</li>
<li>paper_authors: Praveen Joshi, Chandra Thapa, Mohammed Hasanuzzaman, Ted Scully, Haithem Afli</li>
<li>for: 提高 IoT 设备数据隐私和提高模型训练效率</li>
<li>methods: 使用分布式合作机器学习（DCML）和分布式分裂学习（SFL）技术，并在客户端模型部分应用本地批处理和批处理随机混淆</li>
<li>results: SFPL 比 SFL 提高了模型训练效率和预测精度，具体达到了以下因素：（i）CIFAR-100 数据集上，SFPL 比 SFL 提高了 ResNet-56 和 ResNet-32 模型的训练效率，分别提高了51.54和32.57倍；（ii）CIFAR-10 数据集上，SFPL 比 SFL 提高了 ResNet-32 和 ResNet-8 模型的训练效率，分别提高了9.23和8.52倍。<details>
<summary>Abstract</summary>
Distributed collaborative machine learning (DCML) is a promising method in the Internet of Things (IoT) domain for training deep learning models, as data is distributed across multiple devices. A key advantage of this approach is that it improves data privacy by removing the necessity for the centralized aggregation of raw data but also empowers IoT devices with low computational power. Among various techniques in a DCML framework, federated split learning, known as splitfed learning (SFL), is the most suitable for efficient training and testing when devices have limited computational capabilities. Nevertheless, when resource-constrained IoT devices have only positive labeled data, multiclass classification deep learning models in SFL fail to converge or provide suboptimal results. To overcome these challenges, we propose splitfed learning with positive labels (SFPL). SFPL applies a random shuffling function to the smashed data received from clients before supplying it to the server for model training. Additionally, SFPL incorporates the local batch normalization for the client-side model portion during the inference phase. Our results demonstrate that SFPL outperforms SFL: (i) by factors of 51.54 and 32.57 for ResNet-56 and ResNet-32, respectively, with the CIFAR-100 dataset, and (ii) by factors of 9.23 and 8.52 for ResNet-32 and ResNet-8, respectively, with CIFAR-10 dataset. Overall, this investigation underscores the efficacy of the proposed SFPL framework in DCML.
</details>
<details>
<summary>摘要</summary>
分布式协同机器学习（DCML）是互联网物联网（IoT）领域的一种有前途的方法，用于训练深度学习模型，因为数据分布在多个设备上。DCML的一个优点是改善数据隐私，不需要将原始数据集中化，同时也使 IoT 设备具备较低的计算能力。在 DCML 框架中，联邦分割学习（SFL）是最适合高效地训练和测试，当设备有限的计算能力时。然而，当资源有限的 IoT 设备只有正例数据时，SFL 中的多类分类深度学习模型会无法 converge 或提供低优的结果。为了解决这些挑战，我们提议使用分布式学习Positive Label（SFPL）。SFPL 使用客户端接收到的数据进行随机混淆函数处理，然后将其提供给服务器进行模型训练。此外，SFPL 还在推理阶段在客户端上实现本地批处理标准化。我们的结果表明，SFPL 在 CIFAR-100 和 CIFAR-10 数据集上分别比 SFL 提高了51.54 和 32.57 倍，并且在 CIFAR-10 数据集上比 SFL 提高了9.23 和 8.52 倍。总的来说，这种研究证明了我们提议的 SFPL 框架在 DCML 中的效果。
</details></li>
</ul>
<hr>
<h2 id="Structural-Credit-Assignment-with-Coordinated-Exploration"><a href="#Structural-Credit-Assignment-with-Coordinated-Exploration" class="headerlink" title="Structural Credit Assignment with Coordinated Exploration"></a>Structural Credit Assignment with Coordinated Exploration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13256">http://arxiv.org/abs/2307.13256</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephen Chung<br>for: 这个论文旨在提出一种生物学上可能性的人工神经网络（ANN）训练方法，该方法是将每个单元视为随机奖励学习（RL）代理，从而将网络视为一群代理。methods: 该方法使用REINFORCE本地学习规则，该规则由全局奖励信号修饰，与生物观察到的 synaptic plasticity更加一致。然而，这种学习方法的启用缓慢，并且与网络大小不相关。这种缓慢的原因是：（i）所有单元独立探索网络，（ii）所有单元都使用同一个奖励来评估其行动。因此，可以分为两类方法来改进结构准确评估。results: 我们提出使用博尔ツ曼机或回卷网络进行协调探索。我们发现，在训练博尔ツ曼机时，可以消除负阶段，其学习规则与奖励调整 Hebbian 学习规则相似。实验结果表明，协调探索在多个随机和离散单元基于 REINFORCE 训练速度上明显超过独立探索，甚至超过 straight-through estimator（STE）反propagation。<details>
<summary>Abstract</summary>
A biologically plausible method for training an Artificial Neural Network (ANN) involves treating each unit as a stochastic Reinforcement Learning (RL) agent, thereby considering the network as a team of agents. Consequently, all units can learn via REINFORCE, a local learning rule modulated by a global reward signal, which aligns more closely with biologically observed forms of synaptic plasticity. However, this learning method tends to be slow and does not scale well with the size of the network. This inefficiency arises from two factors impeding effective structural credit assignment: (i) all units independently explore the network, and (ii) a single reward is used to evaluate the actions of all units. Accordingly, methods aimed at improving structural credit assignment can generally be classified into two categories. The first category includes algorithms that enable coordinated exploration among units, such as MAP propagation. The second category encompasses algorithms that compute a more specific reward signal for each unit within the network, like Weight Maximization and its variants. In this research report, our focus is on the first category. We propose the use of Boltzmann machines or a recurrent network for coordinated exploration. We show that the negative phase, which is typically necessary to train Boltzmann machines, can be removed. The resulting learning rules are similar to the reward-modulated Hebbian learning rule. Experimental results demonstrate that coordinated exploration significantly exceeds independent exploration in training speed for multiple stochastic and discrete units based on REINFORCE, even surpassing straight-through estimator (STE) backpropagation.
</details>
<details>
<summary>摘要</summary>
生物学上有效的人工神经网络（ANN）训练方法是将每个单元视为随机奖励学习（RL）代理，从而考虑整个网络为一支代理队伍。这样做的优点是每个单元都可以通过REINFORCE本地学习规则和全局奖励信号来学习，这更加符合生物观察到的神经元强化突变。然而，这种学习方法具有两个缺点，导致效率低下：（1）所有单元独立探索网络，（2）网络中的所有单元都接受同一个奖励。这两点导致了结构准确评价的障碍。为了改进结构准确评价，一般可以分为两类方法：第一类是使单元之间协同探索的算法，如MAP协同传播；第二类是计算网络中每个单元的更加准确的奖励信号，如质量最大化和其变种。本研究报告的焦点是第一类方法。我们提议使用博尔ツ曼机或回归网络进行协同探索。我们发现，通常需要训练博尔ツ曼机的负阶段可以除去。结果的学习规则类似于奖励调节的希质bean学习规则。实验结果表明，协同探索在多个随机离散单元基于REINFORCE训练速度上明显高于独立探索，甚至超过STE归整梯度归整。
</details></li>
</ul>
<hr>
<h2 id="RoSAS-Deep-Semi-Supervised-Anomaly-Detection-with-Contamination-Resilient-Continuous-Supervision"><a href="#RoSAS-Deep-Semi-Supervised-Anomaly-Detection-with-Contamination-Resilient-Continuous-Supervision" class="headerlink" title="RoSAS: Deep Semi-Supervised Anomaly Detection with Contamination-Resilient Continuous Supervision"></a>RoSAS: Deep Semi-Supervised Anomaly Detection with Contamination-Resilient Continuous Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13239">http://arxiv.org/abs/2307.13239</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xuhongzuo/rosas">https://github.com/xuhongzuo/rosas</a></li>
<li>paper_authors: Hongzuo Xu, Yijie Wang, Guansong Pang, Songlei Jian, Ning Liu, Yongjun Wang</li>
<li>for: 这篇论文的目的是提出一种新的半指导型异常检测方法，以提高异常检测的性能。</li>
<li>methods: 这篇论文使用了一种新的混合梯度法，将涉猎到异常的标签资料与正常标签资料混合在一起，从而创建了新的标签资料集。同时，这篇论文还使用了一个特别的目标函数来规范网络，以提高网络的Robustness。</li>
<li>results: 这篇论文的实验结果显示，该方法可以在11个真实世界数据集上取得20%-30%的提升，并且在不同的异常污染水平和不同数量的标签异常下展现出更加稳定和更好的性能。<details>
<summary>Abstract</summary>
Semi-supervised anomaly detection methods leverage a few anomaly examples to yield drastically improved performance compared to unsupervised models. However, they still suffer from two limitations: 1) unlabeled anomalies (i.e., anomaly contamination) may mislead the learning process when all the unlabeled data are employed as inliers for model training; 2) only discrete supervision information (such as binary or ordinal data labels) is exploited, which leads to suboptimal learning of anomaly scores that essentially take on a continuous distribution. Therefore, this paper proposes a novel semi-supervised anomaly detection method, which devises \textit{contamination-resilient continuous supervisory signals}. Specifically, we propose a mass interpolation method to diffuse the abnormality of labeled anomalies, thereby creating new data samples labeled with continuous abnormal degrees. Meanwhile, the contaminated area can be covered by new data samples generated via combinations of data with correct labels. A feature learning-based objective is added to serve as an optimization constraint to regularize the network and further enhance the robustness w.r.t. anomaly contamination. Extensive experiments on 11 real-world datasets show that our approach significantly outperforms state-of-the-art competitors by 20%-30% in AUC-PR and obtains more robust and superior performance in settings with different anomaly contamination levels and varying numbers of labeled anomalies. The source code is available at https://github.com/xuhongzuo/rosas/.
</details>
<details>
<summary>摘要</summary>
semi-supervised异常检测方法可以借鉴一些异常示例，以实现与不supervised模型相比的显著改善。然而，它们仍然受到两个限制：1）无标签异常（即异常杂化）可能会导致学习过程中的混乱，当所有无标签数据作为模型训练中的内liers使用时；2）只是利用简单的数据标签（如二分或ORDinal数据标签），导致异常分数的学习变得不优化。因此，本文提出了一种新的 semi-supervised异常检测方法，即使用“杂化防御”的continuous supervisory signals。具体来说，我们提出了一种杂化 interpolating方法，以帮助异常标注样本的异常程度进行灵活的销毁，并创建了新的数据样本，其中每个样本都有连续的异常度标签。此外，杂化区域可以通过组合正确标注的数据来覆盖。我们还添加了一个特征学习基于的目标函数，以便为杂化异常进行更好的规范化和强化。我们在11个真实世界数据集上进行了广泛的实验，结果显示，我们的方法在AUC-PR方面与当前竞争对手相比，提高了20%-30%，并且在不同的异常杂化水平和异常标注数量的情况下具有更加稳定和优秀的性能。代码可以在https://github.com/xuhongzuo/rosas/ obtain。
</details></li>
</ul>
<hr>
<h2 id="Audio-aware-Query-enhanced-Transformer-for-Audio-Visual-Segmentation"><a href="#Audio-aware-Query-enhanced-Transformer-for-Audio-Visual-Segmentation" class="headerlink" title="Audio-aware Query-enhanced Transformer for Audio-Visual Segmentation"></a>Audio-aware Query-enhanced Transformer for Audio-Visual Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13236">http://arxiv.org/abs/2307.13236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinxiang Liu, Chen Ju, Chaofan Ma, Yanfeng Wang, Yu Wang, Ya Zhang</li>
<li>for:  Audio-visual segmentation (AVS) task, 用于将声音cue integrate into视频帧中的物体 segmentation</li>
<li>methods: 提出了一种新的AUDIO-aware query-enhanced TRANSFORMER（AuTR）方法，通过多模态变换架构和声音注意力机制来深度融合和聚合声音-视频特征</li>
<li>results: 比前方法更高的性能和更好的泛化能力在多声音和开放集成enario中<details>
<summary>Abstract</summary>
The goal of the audio-visual segmentation (AVS) task is to segment the sounding objects in the video frames using audio cues. However, current fusion-based methods have the performance limitations due to the small receptive field of convolution and inadequate fusion of audio-visual features. To overcome these issues, we propose a novel \textbf{Au}dio-aware query-enhanced \textbf{TR}ansformer (AuTR) to tackle the task. Unlike existing methods, our approach introduces a multimodal transformer architecture that enables deep fusion and aggregation of audio-visual features. Furthermore, we devise an audio-aware query-enhanced transformer decoder that explicitly helps the model focus on the segmentation of the pinpointed sounding objects based on audio signals, while disregarding silent yet salient objects. Experimental results show that our method outperforms previous methods and demonstrates better generalization ability in multi-sound and open-set scenarios.
</details>
<details>
<summary>摘要</summary>
文本：目标是使用音频cue将视频帧中的响应物分 segment。然而，现有的混合方法受限于小感知范围和不足的音频视频特征混合。为解决这些问题，我们提出一种新的听音感知Query加强的 transformer（AuTR）来解决这个任务。与现有方法不同，我们的方法 introduce一种多模态 transformer架构，该架构允许深度融合和音频视频特征的总结。此外，我们设计了一种听音感知Query加强的 transformer解码器，该解码器会在音频信号上显式地帮助模型将注意力集中在音频cue上，而忽略沉默却重要的物体。实验结果表明，我们的方法在多音和开放集成enario中表现出色，并且比前方法更好地普适化。Note: Simplified Chinese is used in mainland China and Singapore, while Traditional Chinese is used in Taiwan, Hong Kong, and Macau.
</details></li>
</ul>
<hr>
<h2 id="Spectral-DP-Differentially-Private-Deep-Learning-through-Spectral-Perturbation-and-Filtering"><a href="#Spectral-DP-Differentially-Private-Deep-Learning-through-Spectral-Perturbation-and-Filtering" class="headerlink" title="Spectral-DP: Differentially Private Deep Learning through Spectral Perturbation and Filtering"></a>Spectral-DP: Differentially Private Deep Learning through Spectral Perturbation and Filtering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13231">http://arxiv.org/abs/2307.13231</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ce Feng, Nuo Xu, Wujie Wen, Parv Venkitasubramaniam, Caiwen Ding</li>
<li>For: This paper proposes a new approach to differentially private deep learning called Spectral-DP, which improves upon existing methods by achieving a desired privacy guarantee with a lower noise scale and thus better utility.* Methods: The paper uses a combination of gradient perturbation in the spectral domain and spectral filtering to achieve differential privacy, and develops methods for both convolutional and fully connected layers.* Results: The paper shows through comprehensive experiments that Spectral-DP has uniformly better utility performance compared to state-of-the-art DP-SGD based approaches, both in training from scratch and transfer learning settings.Here’s the simplified Chinese text:</li>
<li>for: 这篇论文提出了一种新的扩展privacy深度学习方法，叫做Spectral-DP，它可以在保持隐私性的同时提高实用性。</li>
<li>methods: 该论文使用了spectral domain中的梯度偏移和spectral filtering来实现扩展privacy，并为 convolutional和fully connected层都开发了方法。</li>
<li>results: 经过了广泛的实验，论文显示Spectral-DP在训练从头开始和传输学习设置下都有uniformly更好的实用性表现，比采用现有的DP-SGD基于方法更好。<details>
<summary>Abstract</summary>
Differential privacy is a widely accepted measure of privacy in the context of deep learning algorithms, and achieving it relies on a noisy training approach known as differentially private stochastic gradient descent (DP-SGD). DP-SGD requires direct noise addition to every gradient in a dense neural network, the privacy is achieved at a significant utility cost. In this work, we present Spectral-DP, a new differentially private learning approach which combines gradient perturbation in the spectral domain with spectral filtering to achieve a desired privacy guarantee with a lower noise scale and thus better utility. We develop differentially private deep learning methods based on Spectral-DP for architectures that contain both convolution and fully connected layers. In particular, for fully connected layers, we combine a block-circulant based spatial restructuring with Spectral-DP to achieve better utility. Through comprehensive experiments, we study and provide guidelines to implement Spectral-DP deep learning on benchmark datasets. In comparison with state-of-the-art DP-SGD based approaches, Spectral-DP is shown to have uniformly better utility performance in both training from scratch and transfer learning settings.
</details>
<details>
<summary>摘要</summary>
diffeential privacy 是深度学习算法中广泛接受的隐私标准，实现 diffeential privacy 需要使用带有直接噪声的 dense neural network 的启发式梯度下降（DP-SGD）。DP-SGD 需要在每个梯度上添加直接噪声，以实现隐私，但是这会导致较高的额外成本。在这项工作中，我们介绍 Spectral-DP，一种新的启发式隐私学习方法，它在spectral domain中添加梯度扰动，并通过spectral filtering来实现隐私保证，并且具有较低的噪声级别和更好的实用性。我们开发了基于 Spectral-DP 的启发式深度学习方法，包括具有 convolution 和 fully connected 层的架构。尤其是在 fully connected 层上，我们将 block-circulant 基于的空间重构与 Spectral-DP 结合使用，以实现更好的实用性。通过广泛的实验，我们研究了 Spectral-DP 深度学习的实现方法，并提供了实现指南。与state-of-the-art DP-SGD 基于方法相比，Spectral-DP 在训练从头开始和转移学习设置下具有更好的实用性表现。
</details></li>
</ul>
<hr>
<h2 id="A-Primer-on-the-Data-Cleaning-Pipeline"><a href="#A-Primer-on-the-Data-Cleaning-Pipeline" class="headerlink" title="A Primer on the Data Cleaning Pipeline"></a>A Primer on the Data Cleaning Pipeline</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13219">http://arxiv.org/abs/2307.13219</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rebecca C. Steorts</li>
<li>for: 这篇论文主要是为了介绍数据整理管道（data cleaning pipeline）的科学，以便在下游任务、预测分析或统计分析中使用“净化数据”。</li>
<li>methods: 论文介绍了数据整理管道的四个阶段，包括数据预处理、数据整理、数据检查和数据净化。</li>
<li>results: 论文介绍了一些常用的数据整理方法和技术，以及在实际应用中的效果。<details>
<summary>Abstract</summary>
The availability of both structured and unstructured databases, such as electronic health data, social media data, patent data, and surveys that are often updated in real time, among others, has grown rapidly over the past decade. With this expansion, the statistical and methodological questions around data integration, or rather merging multiple data sources, has also grown. Specifically, the science of the ``data cleaning pipeline'' contains four stages that allow an analyst to perform downstream tasks, predictive analyses, or statistical analyses on ``cleaned data.'' This article provides a review of this emerging field, introducing technical terminology and commonly used methods.
</details>
<details>
<summary>摘要</summary>
“过去一代，数据库的可用性，包括电子健康数据、社交媒体数据、专利数据和调查等，在快速增长。这种增长也导致了数据集成问题的统计和方法问题的增长。特别是“数据清洁管道”科学中的四个阶段，允许分析员在“净化数据”后进行下游任务、预测分析或统计分析。本文将介绍这个新兴领域，并介绍技术术语和常用方法。”Note: "数据清洁管道" (data cleaning pipeline) is a term used to describe the process of preparing data for analysis, including cleaning, transforming, and integrating data from multiple sources.
</details></li>
</ul>
<hr>
<h2 id="FedMEKT-Distillation-based-Embedding-Knowledge-Transfer-for-Multimodal-Federated-Learning"><a href="#FedMEKT-Distillation-based-Embedding-Knowledge-Transfer-for-Multimodal-Federated-Learning" class="headerlink" title="FedMEKT: Distillation-based Embedding Knowledge Transfer for Multimodal Federated Learning"></a>FedMEKT: Distillation-based Embedding Knowledge Transfer for Multimodal Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13214">http://arxiv.org/abs/2307.13214</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huy Q. Le, Minh N. H. Nguyen, Chu Myaet Thwal, Yu Qiao, Chaoning Zhang, Choong Seon Hong<br>for:这个研究旨在提出一个基于联邦学习的多 modal 机器学习架构，以便在多个客户端上实现共同训练一个通用的全球模型，而无需分享私人数据。methods:这个架构使用了一种 semi-supervised 学习方法，以利用不同modalities的表现。它还包括一个发散基于多 modal 嵌入知识转移机制，名为 FedMEKT，可以将服务器和客户端的学习模型中的共同知识转移到参与的客户端上。results:这个研究透过三个多 modal 人类活动识别数据集进行广泛的实验，展示了 FedMEKT 可以实现更好的全球嵌入器性能，并且保护使用者的隐私和模型参数，并且需要较少的通信成本。<details>
<summary>Abstract</summary>
Federated learning (FL) enables a decentralized machine learning paradigm for multiple clients to collaboratively train a generalized global model without sharing their private data. Most existing works simply propose typical FL systems for single-modal data, thus limiting its potential on exploiting valuable multimodal data for future personalized applications. Furthermore, the majority of FL approaches still rely on the labeled data at the client side, which is limited in real-world applications due to the inability of self-annotation from users. In light of these limitations, we propose a novel multimodal FL framework that employs a semi-supervised learning approach to leverage the representations from different modalities. Bringing this concept into a system, we develop a distillation-based multimodal embedding knowledge transfer mechanism, namely FedMEKT, which allows the server and clients to exchange the joint knowledge of their learning models extracted from a small multimodal proxy dataset. Our FedMEKT iteratively updates the generalized global encoders with the joint embedding knowledge from the participating clients. Thereby, to address the modality discrepancy and labeled data constraint in existing FL systems, our proposed FedMEKT comprises local multimodal autoencoder learning, generalized multimodal autoencoder construction, and generalized classifier learning. Through extensive experiments on three multimodal human activity recognition datasets, we demonstrate that FedMEKT achieves superior global encoder performance on linear evaluation and guarantees user privacy for personal data and model parameters while demanding less communication cost than other baselines.
</details>
<details>
<summary>摘要</summary>
联邦学习（FL）可以实现分布式机器学习的分布式机器学习模型，让多个客户端共同训练一个通用全球模型，而不需要分享私人数据。现有大多数工作都只是提议典型的FL系统，因此限制了其在多模态数据上的潜在应用。另外，大多数FL方法仍然依赖客户端上的标注数据，这在实际应用中是有限的，由于用户无法进行自我标注。为了解决这些限制，我们提出了一种新的多模态FL框架，该框架使用半指导学习方法，以利用不同模式的表示。将这个概念引入系统，我们开发了一种基于储革的多模态嵌入知识传递机制，即FedMEKT，该机制让服务器和客户端可以交换归一化的知识。我们的FedMEKT通过迭代更新通用全球编码器，以获取参与客户端的共同归一化知识。因此，我们的提议的FedMEKT包括本地多模态自动编码学习、通用多模态自动编码器建构和通用分类学习。通过对三个多模态人动识别数据集进行广泛的实验，我们证明了FedMEKT可以在线评估中 достичь更高的全球编码器性能，保护用户隐私和个人数据，同时减少通信成本。
</details></li>
</ul>
<hr>
<h2 id="Transferability-of-Graph-Neural-Networks-using-Graphon-and-Sampling-Theories"><a href="#Transferability-of-Graph-Neural-Networks-using-Graphon-and-Sampling-Theories" class="headerlink" title="Transferability of Graph Neural Networks using Graphon and Sampling Theories"></a>Transferability of Graph Neural Networks using Graphon and Sampling Theories</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13206">http://arxiv.org/abs/2307.13206</a></li>
<li>repo_url: None</li>
<li>paper_authors: A. Martina Neuman, Jason J. Bramburger</li>
<li>for: 本研究旨在应用graphon来提高graph neural network（GNN）的可转移性。</li>
<li>methods: 本研究使用了two-layer graphon neural network（WNN）架构，并证明了其能够高效地近似带限信号。</li>
<li>results: 研究表明，使用WNN架构可以在不同图形式的数据上保持高度的表现，而且可以在不同图大小之间进行可转移学习。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) have become powerful tools for processing graph-based information in various domains. A desirable property of GNNs is transferability, where a trained network can swap in information from a different graph without retraining and retain its accuracy. A recent method of capturing transferability of GNNs is through the use of graphons, which are symmetric, measurable functions representing the limit of large dense graphs. In this work, we contribute to the application of graphons to GNNs by presenting an explicit two-layer graphon neural network (WNN) architecture. We prove its ability to approximate bandlimited signals within a specified error tolerance using a minimal number of network weights. We then leverage this result, to establish the transferability of an explicit two-layer GNN over all sufficiently large graphs in a sequence converging to a graphon. Our work addresses transferability between both deterministic weighted graphs and simple random graphs and overcomes issues related to the curse of dimensionality that arise in other GNN results. The proposed WNN and GNN architectures offer practical solutions for handling graph data of varying sizes while maintaining performance guarantees without extensive retraining.
</details>
<details>
<summary>摘要</summary>
граф neural networks (GNNs) 已成为处理图形信息的有力工具。一个愿望的特性是转移性，其中训练好的网络可以将图形信息交换到另一个图形上，而不需要重新训练，并保持准确性。一种最近提出的捕捉GNNs的转移性的方法是通过图拟函数（graphons），它们是对大量紧凑图的极限函数。在这个工作中，我们对GNNs中的图拟函数应用了Explicit two-layer graphon neural network（WNN）架构。我们证明了它可以在给定的误差范围内近似离散信号，使用最小的网络重量。然后，我们利用这个结论，以证明Explicit two-layer GNN在所有足够大的图上的转移性。我们的工作解决了对权重图和简单随机图之间的转移性问题，并超越了其他GNN结果中的尺度繁殖问题。提出的WNN和GNN架构为处理图数据的不同大小而提供了实用的解决方案，无需进行广泛的重新训练。
</details></li>
</ul>
<hr>
<h2 id="Federated-Distributionally-Robust-Optimization-with-Non-Convex-Objectives-Algorithm-and-Analysis"><a href="#Federated-Distributionally-Robust-Optimization-with-Non-Convex-Objectives-Algorithm-and-Analysis" class="headerlink" title="Federated Distributionally Robust Optimization with Non-Convex Objectives: Algorithm and Analysis"></a>Federated Distributionally Robust Optimization with Non-Convex Objectives: Algorithm and Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14364">http://arxiv.org/abs/2307.14364</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Jiao, Kai Yang, Dongjin Song</li>
<li>For:	+ The paper aims to solve the federated distributionally robust optimization (FDRO) problem, which is to find an optimal decision that minimizes the worst-case cost over the ambiguity set of probability distributions in a distributed environment.* Methods:	+ The proposed algorithm is called Asynchronous Single-looP alternatIve gRadient projEction (ASPIRE) algorithm with the itErative Active SEt method (EASE).	+ The algorithm leverages the prior distribution using a new uncertainty set called constrained D-norm uncertainty set.* Results:	+ The proposed algorithm is guaranteed to converge and the iteration complexity is analyzed.	+ Extensive empirical studies on real-world datasets demonstrate that the proposed method can achieve fast convergence, remain robust against data heterogeneity and malicious attacks, and trade off robustness with performance.<details>
<summary>Abstract</summary>
Distributionally Robust Optimization (DRO), which aims to find an optimal decision that minimizes the worst case cost over the ambiguity set of probability distribution, has been widely applied in diverse applications, e.g., network behavior analysis, risk management, etc. However, existing DRO techniques face three key challenges: 1) how to deal with the asynchronous updating in a distributed environment; 2) how to leverage the prior distribution effectively; 3) how to properly adjust the degree of robustness according to different scenarios. To this end, we propose an asynchronous distributed algorithm, named Asynchronous Single-looP alternatIve gRadient projEction (ASPIRE) algorithm with the itErative Active SEt method (EASE) to tackle the federated distributionally robust optimization (FDRO) problem. Furthermore, a new uncertainty set, i.e., constrained D-norm uncertainty set, is developed to effectively leverage the prior distribution and flexibly control the degree of robustness. Finally, our theoretical analysis elucidates that the proposed algorithm is guaranteed to converge and the iteration complexity is also analyzed. Extensive empirical studies on real-world datasets demonstrate that the proposed method can not only achieve fast convergence, and remain robust against data heterogeneity as well as malicious attacks, but also tradeoff robustness with performance.
</details>
<details>
<summary>摘要</summary>
Distributionally Robust Optimization (DRO) 目标是找到最佳决策，以最小化不确定性集中的最差成本。这种技术在多个应用中广泛使用，如网络行为分析、风险管理等。然而，现有的 DRO 技术面临三大挑战：1）如何在分布式环境中 asynchronous 更新; 2）如何有效地利用先前分布; 3）如何适当调整不确定性中的度量。为此，我们提出了一个异步分布式算法，名为异步单脚拟合gradient projection（ASPIRE）算法，并与itErative Active SEt方法（EASE）结合以解决联邦分布式不确定优化（FDRO）问题。此外，我们还开发了一个新的不确定集，即受限的 D-norm 不确定集，以有效地利用先前分布并flexibly控制不确定性度量。最后，我们的理论分析表明，提案的算法可以保证收敛，并且迭代复杂性也进行了分析。实际实验表明，提案的方法可以不仅快速收敛，同时也能够对数据不一致和恶意攻击具有抗性。
</details></li>
</ul>
<hr>
<h2 id="An-Investigation-into-Glomeruli-Detection-in-Kidney-H-E-and-PAS-Images-using-YOLO"><a href="#An-Investigation-into-Glomeruli-Detection-in-Kidney-H-E-and-PAS-Images-using-YOLO" class="headerlink" title="An Investigation into Glomeruli Detection in Kidney H&amp;E and PAS Images using YOLO"></a>An Investigation into Glomeruli Detection in Kidney H&amp;E and PAS Images using YOLO</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13199">http://arxiv.org/abs/2307.13199</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AlexeyAB/darknet">https://github.com/AlexeyAB/darknet</a></li>
<li>paper_authors: Kimia Hemmatirad, Morteza Babaie, Jeffrey Hodgin, Liron Pantanowitz, H. R. Tizhoosh<br>for: This paper aims to assist pathologists in detecting glomeruli in human kidney images using computerized solutions, specifically by proposing an automated tissue structure detection and segmentation method using the YOLO-v4 object detector.methods: The YOLO-v4 model was trained on whole slide images and fine-tuned on a private dataset from the University of Michigan for glomeruli detection. Multiple experiments were conducted using different training data and stains.results: The model achieved average specificity and sensitivity for all experiments and outperformed existing segmentation methods on the same datasets. However, the design and validation for different stains still depend on the variability of public multi-stain datasets.<details>
<summary>Abstract</summary>
Context: Analyzing digital pathology images is necessary to draw diagnostic conclusions by investigating tissue patterns and cellular morphology. However, manual evaluation can be time-consuming, expensive, and prone to inter- and intra-observer variability. Objective: To assist pathologists using computerized solutions, automated tissue structure detection and segmentation must be proposed. Furthermore, generating pixel-level object annotations for histopathology images is expensive and time-consuming. As a result, detection models with bounding box labels may be a feasible solution. Design: This paper studies. YOLO-v4 (You-Only-Look-Once), a real-time object detector for microscopic images. YOLO uses a single neural network to predict several bounding boxes and class probabilities for objects of interest. YOLO can enhance detection performance by training on whole slide images. YOLO-v4 has been used in this paper. for glomeruli detection in human kidney images. Multiple experiments have been designed and conducted based on different training data of two public datasets and a private dataset from the University of Michigan for fine-tuning the model. The model was tested on the private dataset from the University of Michigan, serving as an external validation of two different stains, namely hematoxylin and eosin (H&E) and periodic acid-Schiff (PAS). Results: Average specificity and sensitivity for all experiments, and comparison of existing segmentation methods on the same datasets are discussed. Conclusions: Automated glomeruli detection in human kidney images is possible using modern AI models. The design and validation for different stains still depends on variability of public multi-stain datasets.
</details>
<details>
<summary>摘要</summary>
Context: 分析数字 PATHOLOGY 图像是必要的，以便从investigating 组织模式和细胞形态中提取诊断结论。然而，手动评估可能会 consume 时间和成本，并且存在 между观察者和内部观察者的差异。目标：使用计算机化解决方案，自动检测和分割组织结构。此外，为 Histopathology 图像生成像素级对象标注是昂贵和时间consuming。因此，使用 bounding box 标签的检测模型可能是一个可行的解决方案。Design：本文研究了 YOLO-v4（You-Only-Look-Once），一种实时物体检测器，用于微scopic 图像。YOLO 使用单个神经网络预测多个 bounding box 和对象类概率。YOLO 可以通过训练整个扫描图像来提高检测性能。本文使用 YOLO-v4 进行了 glomeruli 检测在人脏肾图像中。基于两个公共数据集和一个由美国密歇根大学提供的私人数据集进行了多个实验，并对模型进行了微调。模型在密歇根大学私人数据集上进行了测试，作为对两种染色物（HE和PAS）的EXternal 验证。Results：本文的结果显示，使用 YOLO-v4 可以自动检测人脏肾中的 glomeruli。average 特异性和敏感性的讨论，以及现有的分 segmentation 方法在同一个数据集上的比较。Conclusions：使用现代 AI 模型，自动检测人脏肾中的 glomeruli 是可能的。然而，设计和验证不同染色物的方法仍然виси于多个公共多染色物数据集的变化。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-enhanced-Neuro-Symbolic-AI-for-Cybersecurity-and-Privacy"><a href="#Knowledge-enhanced-Neuro-Symbolic-AI-for-Cybersecurity-and-Privacy" class="headerlink" title="Knowledge-enhanced Neuro-Symbolic AI for Cybersecurity and Privacy"></a>Knowledge-enhanced Neuro-Symbolic AI for Cybersecurity and Privacy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02031">http://arxiv.org/abs/2308.02031</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aritran Piplai, Anantaa Kotal, Seyedreza Mohseni, Manas Gaur, Sudip Mittal, Anupam Joshi</li>
<li>for: This paper is written to explore the potential of Neuro-Symbolic Artificial Intelligence (AI) in addressing the challenges of explainability and safety in AI systems, particularly in the domains of cybersecurity and privacy.</li>
<li>methods: The paper uses a combination of neural networks and symbolic knowledge graphs to integrate the strengths of both approaches, enabling AI systems to reason, learn, and generalize in a manner understandable to experts.</li>
<li>results: The paper demonstrates the potential of Neuro-Symbolic AI to improve the explainability and safety of AI systems in complex environments, specifically in the domains of cybersecurity and privacy.<details>
<summary>Abstract</summary>
Neuro-Symbolic Artificial Intelligence (AI) is an emerging and quickly advancing field that combines the subsymbolic strengths of (deep) neural networks and explicit, symbolic knowledge contained in knowledge graphs to enhance explainability and safety in AI systems. This approach addresses a key criticism of current generation systems, namely their inability to generate human-understandable explanations for their outcomes and ensure safe behaviors, especially in scenarios with \textit{unknown unknowns} (e.g. cybersecurity, privacy). The integration of neural networks, which excel at exploring complex data spaces, and symbolic knowledge graphs, which represent domain knowledge, allows AI systems to reason, learn, and generalize in a manner understandable to experts. This article describes how applications in cybersecurity and privacy, two most demanding domains in terms of the need for AI to be explainable while being highly accurate in complex environments, can benefit from Neuro-Symbolic AI.
</details>
<details>
<summary>摘要</summary>
neuromorphic artificial intelligence (AI) 是一个emerging 和rapidly advancing field，它将SUBSYMBOLIC  strengths of (deep) neural networks 和Explicit, symbolic knowledge contained in knowledge graphs 搭配，以提高AI系统的可解释性和安全性。这种方法解决了当前一代系统的一个批评，即它们无法生成人理解的解释，特别是在“Unknown unknowns”（例如Cybersecurity, privacy）的场景中。 combining neural networks, which excel at exploring complex data spaces, and symbolic knowledge graphs, which represent domain knowledge, allows AI systems to reason, learn, and generalize in a manner understandable to experts。This article describes how applications in cybersecurity and privacy, two most demanding domains in terms of the need for AI to be explainable while being highly accurate in complex environments, can benefit from Neuro-Symbolic AI。
</details></li>
</ul>
<hr>
<h2 id="Counterfactual-Explanation-Policies-in-RL"><a href="#Counterfactual-Explanation-Policies-in-RL" class="headerlink" title="Counterfactual Explanation Policies in RL"></a>Counterfactual Explanation Policies in RL</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13192">http://arxiv.org/abs/2307.13192</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shripad V. Deshmukh, Srivatsan R, Supriti Vijay, Jayakumar Subramanian, Chirag Agarwal</li>
<li>for: 这个论文旨在探讨如何使用对假设的解释来分析RL策略，以便更好地理解RL策略的含义。</li>
<li>methods: 该论文提出了一种新的Counterpol方法，通过在RL中 incorporating counterfactuals in supervised learning，以生成对策略的解释。</li>
<li>results: 实验结果表明，Counterpol可以帮助分析RL策略，并且可以在不同的状态和动作空间中工作。<details>
<summary>Abstract</summary>
As Reinforcement Learning (RL) agents are increasingly employed in diverse decision-making problems using reward preferences, it becomes important to ensure that policies learned by these frameworks in mapping observations to a probability distribution of the possible actions are explainable. However, there is little to no work in the systematic understanding of these complex policies in a contrastive manner, i.e., what minimal changes to the policy would improve/worsen its performance to a desired level. In this work, we present COUNTERPOL, the first framework to analyze RL policies using counterfactual explanations in the form of minimal changes to the policy that lead to the desired outcome. We do so by incorporating counterfactuals in supervised learning in RL with the target outcome regulated using desired return. We establish a theoretical connection between Counterpol and widely used trust region-based policy optimization methods in RL. Extensive empirical analysis shows the efficacy of COUNTERPOL in generating explanations for (un)learning skills while keeping close to the original policy. Our results on five different RL environments with diverse state and action spaces demonstrate the utility of counterfactual explanations, paving the way for new frontiers in designing and developing counterfactual policies.
</details>
<details>
<summary>摘要</summary>
“在增强学习（Reinforcement Learning，RL）Agent increasingly 应用于多元决策问题中，使得Policy learned by these frameworks in mapping observations to a probability distribution of possible actions的可读性变得越来越重要。然而，有很少关于这些复杂的策略的系统性理解，即对策略的改进或恶化的最小变化。在这种情况下，我们提出了COUNTERPOL，第一个使用对假设的批评性解释来分析RL策略的框架。我们通过在RL中 incorporating counterfactuals in supervised learning with the target outcome regulated using desired return来实现这一点。我们还证明了COUNTERPOL和常用的信任区基于RL策略优化方法之间的理论联系。我们的实验结果表明，COUNTERPOL能够生成高效的解释，并保持与原始策略几乎相同。我们在五个不同的RL环境中进行了Extensive empirical analysis，这些环境包括多种状态和动作空间。我们的结果表明，对假设的批评性解释可以帮助RL Agent 学习和改进策略，开辟出新的前iers in designing and developing counterfactual policies。”Note: Please note that the translation is in Simplified Chinese, and the word order and grammar may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="Neural-Memory-Decoding-with-EEG-Data-and-Representation-Learning"><a href="#Neural-Memory-Decoding-with-EEG-Data-and-Representation-Learning" class="headerlink" title="Neural Memory Decoding with EEG Data and Representation Learning"></a>Neural Memory Decoding with EEG Data and Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13181">http://arxiv.org/abs/2307.13181</a></li>
<li>repo_url: None</li>
<li>paper_authors: Glenn Bruns, Michael Haidar, Federico Rubino</li>
<li>for: 本研究描述了一种使用神经网络快速解oding记忆的方法，用于从EEG数据中提取记忆。</li>
<li>methods: 该方法使用深度表示学习，并使用supervised contrastive损失来将EEG记录转换到低维度空间中。</li>
<li>results: 该方法可以在EEG数据中准确地识别概念，其准确率为About 78.4%（ chance 4%）。此外，该方法还应用于信息检索问题，可以生成基于EEG数据的预测文档列表。<details>
<summary>Abstract</summary>
We describe a method for the neural decoding of memory from EEG data. Using this method, a concept being recalled can be identified from an EEG trace with an average top-1 accuracy of about 78.4% (chance 4%). The method employs deep representation learning with supervised contrastive loss to map an EEG recording of brain activity to a low-dimensional space. Because representation learning is used, concepts can be identified even if they do not appear in the training data set. However, reference EEG data must exist for each such concept. We also show an application of the method to the problem of information retrieval. In neural information retrieval, EEG data is captured while a user recalls the contents of a document, and a list of links to predicted documents is produced.
</details>
<details>
<summary>摘要</summary>
我们描述了一种使用神经网络对电enzephalogram（EEG）数据进行记忆解oding的方法。使用这种方法，在EEG追踪记录中检测到的概念可以与高达78.4%的准确率（Random 4%）进行匹配。该方法利用深度表示学习和监督对比损失来将EEG记录的脑动activty映射到低维度空间中。由于使用表示学习，即使概念没有出现在训练数据集中，也可以正确地识别出它们。然而，每个概念都需要相应的参照EEG数据。我们还展示了该方法的应用于信息检索问题。在神经信息检索中，EEG数据被记录在用户回忆文档内容时，并生成一个预测文档列表。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-the-reliability-of-automatically-generated-pedestrian-and-bicycle-crash-surrogates"><a href="#Evaluating-the-reliability-of-automatically-generated-pedestrian-and-bicycle-crash-surrogates" class="headerlink" title="Evaluating the reliability of automatically generated pedestrian and bicycle crash surrogates"></a>Evaluating the reliability of automatically generated pedestrian and bicycle crash surrogates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13178">http://arxiv.org/abs/2307.13178</a></li>
<li>repo_url: None</li>
<li>paper_authors: Agnimitra Sengupta, S. Ilgin Guler, Vikash V. Gayah, Shannon Warchol</li>
<li>For: This study aims to assess the reliability of automatically generated surrogates in predicting confirmed conflicts involving vulnerable road users (VRUs) at signalized intersections.* Methods: The study uses a video-based event monitoring system to collect data on VRU and motor vehicle interactions at 15 signalized intersections in Pennsylvania. Advanced data-driven models are used to analyze the surrogate data, including automatically collectable variables such as speeds, movements, and post-encroachment time, as well as manually collected variables like signal states, lighting, and weather conditions.* Results: The findings highlight the varying importance of specific surrogates in predicting true conflicts, with some being more informative than others. The results can assist transportation agencies in prioritizing infrastructure investments, such as bike lanes and crosswalks, and evaluating their effectiveness.<details>
<summary>Abstract</summary>
Vulnerable road users (VRUs), such as pedestrians and bicyclists, are at a higher risk of being involved in crashes with motor vehicles, and crashes involving VRUs also are more likely to result in severe injuries or fatalities. Signalized intersections are a major safety concern for VRUs due to their complex and dynamic nature, highlighting the need to understand how these road users interact with motor vehicles and deploy evidence-based countermeasures to improve safety performance. Crashes involving VRUs are relatively infrequent, making it difficult to understand the underlying contributing factors. An alternative is to identify and use conflicts between VRUs and motorized vehicles as a surrogate for safety performance. Automatically detecting these conflicts using a video-based systems is a crucial step in developing smart infrastructure to enhance VRU safety. The Pennsylvania Department of Transportation conducted a study using video-based event monitoring system to assess VRU and motor vehicle interactions at fifteen signalized intersections across Pennsylvania to improve VRU safety performance. This research builds on that study to assess the reliability of automatically generated surrogates in predicting confirmed conflicts using advanced data-driven models. The surrogate data used for analysis include automatically collectable variables such as vehicular and VRU speeds, movements, post-encroachment time, in addition to manually collected variables like signal states, lighting, and weather conditions. The findings highlight the varying importance of specific surrogates in predicting true conflicts, some being more informative than others. The findings can assist transportation agencies to collect the right types of data to help prioritize infrastructure investments, such as bike lanes and crosswalks, and evaluate their effectiveness.
</details>
<details>
<summary>摘要</summary>
易受损的路用者（VRU），如行人和自行车客，与机动车在交通冲突中更容易发生事故，事故也更可能导致严重的伤害或死亡。信号控制的交叉口是VRU安全的主要问题，因为它们的复杂性和动态性使得需要更好地了解路用者与机动车之间的互动，并采取基于证据的减少措施以提高安全性。与机动车相撞的事故 rarely occurs，这使得Difficult to understand the underlying contributing factors. An alternative is to identify and use conflicts between VRUs and motorized vehicles as a surrogate for safety performance. Using a video-based system to automatically detect these conflicts is a crucial step in developing smart infrastructure to enhance VRU safety.在美国宾夕法尼亚州交通部的一项研究中，使用视频基本监测系统评估了全州十五个信号控制交叉口中VRU和机动车之间的互动，以提高VRU安全性。本研究基于这项研究，以自动生成的代理数据来评估确定的冲突。代理数据包括自动收集的变量，如机动车和VRU的速度、移动、后续时间，以及手动收集的变量，如信号状态、灯光和天气情况。研究发现代理数据中的特定变量在预测真正的冲突中扮演着不同的重要性。这些发现可以帮助交通部门收集适当的数据，以便优先投入基础设施投资，如自行车道和横渡道，并评估其效果。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-reconstruction-of-accelerated-cardiac-cine-MRI-using-Neural-Fields"><a href="#Unsupervised-reconstruction-of-accelerated-cardiac-cine-MRI-using-Neural-Fields" class="headerlink" title="Unsupervised reconstruction of accelerated cardiac cine MRI using Neural Fields"></a>Unsupervised reconstruction of accelerated cardiac cine MRI using Neural Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14363">http://arxiv.org/abs/2307.14363</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tabita Catalán, Matías Courdurier, Axel Osses, René Botnar, Francisco Sahli Costabal, Claudia Prieto</li>
<li>for: 这项研究的目的是提出一种无监督的深度学习方法来加速卡ди亚维度MRI的重建。</li>
<li>methods: 该方法基于启发型神经场表示，并在实验中使用金字塔型多晶维度收集器进行受损样本收集。</li>
<li>results: 实验结果表明，该方法可以在实验室中实现高质量的卡ди亚维度MRI重建，并且比传统方法更具有空间和时间表示能力。<details>
<summary>Abstract</summary>
Cardiac cine MRI is the gold standard for cardiac functional assessment, but the inherently slow acquisition process creates the necessity of reconstruction approaches for accelerated undersampled acquisitions. Several regularization approaches that exploit spatial-temporal redundancy have been proposed to reconstruct undersampled cardiac cine MRI. More recently, methods based on supervised deep learning have been also proposed to further accelerate acquisition and reconstruction. However, these techniques rely on usually large dataset for training, which are not always available. In this work, we propose an unsupervised approach based on implicit neural field representations for cardiac cine MRI (so called NF-cMRI). The proposed method was evaluated in in-vivo undersampled golden-angle radial multi-coil acquisitions for undersampling factors of 26x and 52x, achieving good image quality, and comparable spatial and improved temporal depiction than a state-of-the-art reconstruction technique.
</details>
<details>
<summary>摘要</summary>
卡ди亚金枪MRI是心脏功能评估的标准，但它的自然slow acquisition process creates the necessity of reconstruction approaches for accelerated undersampled acquisitions.  Several regularization approaches that exploit spatial-temporal redundancy have been proposed to reconstruct undersampled cardiac cine MRI. More recently, methods based on supervised deep learning have been also proposed to further accelerate acquisition and reconstruction. However, these techniques rely on usually large dataset for training, which are not always available. In this work, we propose an unsupervised approach based on implicit neural field representations for cardiac cine MRI (so called NF-cMRI). The proposed method was evaluated in in-vivo undersampled golden-angle radial multi-coil acquisitions for undersampling factors of 26x and 52x, achieving good image quality, and comparable spatial and improved temporal depiction than a state-of-the-art reconstruction technique.Here's the translation in Traditional Chinese:卡迪亚金枪MRI是心脏功能评估的标准，但它的自然slow acquisition process creates the necessity of reconstruction approaches for accelerated undersampled acquisitions.  Several regularization approaches that exploit spatial-temporal redundancy have been proposed to reconstruct undersampled cardiac cine MRI. More recently, methods based on supervised deep learning have been also proposed to further accelerate acquisition and reconstruction. However, these techniques rely on usually large dataset for training, which are not always available. In this work, we propose an unsupervised approach based on implicit neural field representations for cardiac cine MRI (so called NF-cMRI). The proposed method was evaluated in in-vivo undersampled golden-angle radial multi-coil acquisitions for undersampling factors of 26x and 52x, achieving good image quality, and comparable spatial and improved temporal depiction than a state-of-the-art reconstruction technique.
</details></li>
</ul>
<hr>
<h2 id="Multi-UAV-Speed-Control-with-Collision-Avoidance-and-Handover-aware-Cell-Association-DRL-with-Action-Branching"><a href="#Multi-UAV-Speed-Control-with-Collision-Avoidance-and-Handover-aware-Cell-Association-DRL-with-Action-Branching" class="headerlink" title="Multi-UAV Speed Control with Collision Avoidance and Handover-aware Cell Association: DRL with Action Branching"></a>Multi-UAV Speed Control with Collision Avoidance and Handover-aware Cell Association: DRL with Action Branching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13158">http://arxiv.org/abs/2307.13158</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zijiang Yan, Wael Jaafar, Bassant Selim, Hina Tabassum</li>
<li>for: 提高运输和通信性能，包括碰撞避免、连接稳定和交换。</li>
<li>methods: 使用深度强化学习解决多架空交通干道上UAV协调决策和移动速度优化问题，形式化为Markov决策过程（MDP），UAV状态定义为速度和通信数据率。提议一种神经网络架构，具有共享决策模块和多个网络分支，每个分支专门处理特定的行动维度在2D交通通信空间。</li>
<li>results: 通过 simulation 结果表明，与现有 refer  bench 比较，该方法可以提高18.32%。<details>
<summary>Abstract</summary>
This paper presents a deep reinforcement learning solution for optimizing multi-UAV cell-association decisions and their moving velocity on a 3D aerial highway. The objective is to enhance transportation and communication performance, including collision avoidance, connectivity, and handovers. The problem is formulated as a Markov decision process (MDP) with UAVs' states defined by velocities and communication data rates. We propose a neural architecture with a shared decision module and multiple network branches, each dedicated to a specific action dimension in a 2D transportation-communication space. This design efficiently handles the multi-dimensional action space, allowing independence for individual action dimensions. We introduce two models, Branching Dueling Q-Network (BDQ) and Branching Dueling Double Deep Q-Network (Dueling DDQN), to demonstrate the approach. Simulation results show a significant improvement of 18.32% compared to existing benchmarks.
</details>
<details>
<summary>摘要</summary>
To address the multi-dimensional action space, the proposed solution features a neural architecture with a shared decision module and multiple network branches, each dedicated to a specific action dimension in a 2D transportation-communication space. This design enables independence for individual action dimensions.Two models, Branching Dueling Q-Network (BDQ) and Branching Dueling Double Deep Q-Network (Dueling DDQN), are introduced to demonstrate the approach. Simulation results show a significant improvement of 18.32% compared to existing benchmarks.translate to 简化中文 as follows:这篇论文提出了一种深度强化学习解决方案，用于优化多架航空器（UAV）的维度协调决策和移动速度在3D空中高速公路上。目标是提高交通和通信性能，包括避免冲突、连接和交换。问题被形式化为一个Markov决策过程（MDP），UAV的状态被定义为速度和通信数据速率。为了处理多维动作空间，提议的解决方案具有一个共享决策模块和多个网络分支，每个分支专门处理一个特定的动作维度在2D交通通信空间中。这种设计允许每个动作维度独立进行决策。提议的解决方案还 introduce了两种模型：分支决策网络（BDQ）和分支决策双深度网络（DDQN），以示解决方案。实验结果显示，与现有 referential 比较，提议的方案具有18.32%的显著提高。
</details></li>
</ul>
<hr>
<h2 id="Discovering-interpretable-elastoplasticity-models-via-the-neural-polynomial-method-enabled-symbolic-regressions"><a href="#Discovering-interpretable-elastoplasticity-models-via-the-neural-polynomial-method-enabled-symbolic-regressions" class="headerlink" title="Discovering interpretable elastoplasticity models via the neural polynomial method enabled symbolic regressions"></a>Discovering interpretable elastoplasticity models via the neural polynomial method enabled symbolic regressions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13149">http://arxiv.org/abs/2307.13149</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bahador Bahmani, Hyoung Suk Suh, WaiChing Sun</li>
<li>for: 这篇论文旨在提出一种两步机器学习方法，以帮助解释神经网络模型的含义。</li>
<li>methods: 这种方法首先使用超级vised学习获得单变量特征映射，然后使用符号回归将这些映射转化为数学模型。</li>
<li>results: 这种方法可以解决神经网络模型的解释性问题，同时提供了一些优点，如缩放问题的解决和代码的可重用性。<details>
<summary>Abstract</summary>
Conventional neural network elastoplasticity models are often perceived as lacking interpretability. This paper introduces a two-step machine-learning approach that returns mathematical models interpretable by human experts. In particular, we introduce a surrogate model where yield surfaces are expressed in terms of a set of single-variable feature mappings obtained from supervised learning. A postprocessing step is then used to re-interpret the set of single-variable neural network mapping functions into mathematical form through symbolic regression. This divide-and-conquer approach provides several important advantages. First, it enables us to overcome the scaling issue of symbolic regression algorithms. From a practical perspective, it enhances the portability of learned models for partial differential equation solvers written in different programming languages. Finally, it enables us to have a concrete understanding of the attributes of the materials, such as convexity and symmetries of models, through automated derivations and reasoning. Numerical examples have been provided, along with an open-source code to enable third-party validation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Scalability: The symbolic regression algorithms are able to handle large datasets.2. Portability: The learned models can be easily integrated into partial differential equation solvers written in different programming languages.3. Interpretability: The approach provides a concrete understanding of the attributes of the materials, such as convexity and symmetries of models, through automated derivations and reasoning.Numerical examples are provided, along with open-source code for third-party validation.</details></li>
</ol>
<hr>
<h2 id="Learnable-wavelet-neural-networks-for-cosmological-inference"><a href="#Learnable-wavelet-neural-networks-for-cosmological-inference" class="headerlink" title="Learnable wavelet neural networks for cosmological inference"></a>Learnable wavelet neural networks for cosmological inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14362">http://arxiv.org/abs/2307.14362</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chris-pedersen/learnablewavelets">https://github.com/chris-pedersen/learnablewavelets</a></li>
<li>paper_authors: Christian Pedersen, Michael Eickenberg, Shirley Ho</li>
<li>for:  cosmological inference and marginalisation over astrophysical effects</li>
<li>methods: 使用可学习散射转换，一种基于 convolutional neural network 的trainable wavelets滤波器</li>
<li>results: 比 CNN 更高效，特别是对小训练数据样本; 提供可读的干扰网络Here’s the breakdown of each point:1. for: The paper is written for the purpose of cosmological inference and marginalisation over astrophysical effects using the learnable scattering transform.2. methods: The paper uses the learnable scattering transform, which is a type of convolutional neural network that utilizes trainable wavelets as filters, to perform cosmological inference and marginalisation over astrophysical effects.3. results: The paper shows that scattering architectures can outperform a convolutional neural network (CNN) in terms of performance, especially when dealing with small training data samples. Additionally, the paper presents a lightweight scattering network that is highly interpretable.<details>
<summary>Abstract</summary>
Convolutional neural networks (CNNs) have been shown to both extract more information than the traditional two-point statistics from cosmological fields, and marginalise over astrophysical effects extremely well. However, CNNs require large amounts of training data, which is potentially problematic in the domain of expensive cosmological simulations, and it is difficult to interpret the network. In this work we apply the learnable scattering transform, a kind of convolutional neural network that uses trainable wavelets as filters, to the problem of cosmological inference and marginalisation over astrophysical effects. We present two models based on the scattering transform, one constructed for performance, and one constructed for interpretability, and perform a comparison with a CNN. We find that scattering architectures are able to outperform a CNN, significantly in the case of small training data samples. Additionally we present a lightweight scattering network that is highly interpretable.
</details>
<details>
<summary>摘要</summary>
卷积神经网络（CNN）能够提取更多的信息于 cosmological 场的传统两点统计，并且很好地把astrophysical效应卷积。但是，CNN需要大量的训练数据，这可能是costly cosmological simulations中的问题，而且很难 interpret the network。在这个工作中，我们使用可学习的散射变换，一种基于trainable wavelets的卷积神经网络，来解决 cosmological inference和astrophysical effects的卷积。我们提出了两种基于散射变换的模型，一种是 для性能，另一种是 для可读性，并与CNN进行比较。我们发现，散射架构能够超过CNN，特别是在小训练样本情况下。此外，我们还提出了一个轻量级的散射网络，具有很好的可读性。
</details></li>
</ul>
<hr>
<h2 id="Extending-Path-Dependent-NJ-ODEs-to-Noisy-Observations-and-a-Dependent-Observation-Framework"><a href="#Extending-Path-Dependent-NJ-ODEs-to-Noisy-Observations-and-a-Dependent-Observation-Framework" class="headerlink" title="Extending Path-Dependent NJ-ODEs to Noisy Observations and a Dependent Observation Framework"></a>Extending Path-Dependent NJ-ODEs to Noisy Observations and a Dependent Observation Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13147">http://arxiv.org/abs/2307.13147</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/floriankrach/pd-njode">https://github.com/floriankrach/pd-njode</a></li>
<li>paper_authors: William Andersson, Jakob Heiss, Florian Krach, Josef Teichmann</li>
<li>for: 预测连续时间随机过程，具有不规则和部分观测。</li>
<li>methods: 使用Path-Dependent Neural Jump ODE（PD-NJ-ODE）模型，学习最佳预测给 irregularly sampled时间序列的受限观测。</li>
<li>results: 提供了两种扩展，使模型能够满足不独立的时间序列和均匀观测，并且提供了理论保证和实际示例。<details>
<summary>Abstract</summary>
The Path-Dependent Neural Jump ODE (PD-NJ-ODE) is a model for predicting continuous-time stochastic processes with irregular and incomplete observations. In particular, the method learns optimal forecasts given irregularly sampled time series of incomplete past observations. So far the process itself and the coordinate-wise observation times were assumed to be independent and observations were assumed to be noiseless. In this work we discuss two extensions to lift these restrictions and provide theoretical guarantees as well as empirical examples for them.
</details>
<details>
<summary>摘要</summary>
“path-dependent neural jump ODE（PD-NJ-ODE）是一种预测连续时间随机过程的模型，特别是在含有异常和缺失观测的时间序列上。在现有的方法中，这种方法学习最佳预测， givens irregularly sampled time series of incomplete past observations。在这篇文章中，我们讨论了两种扩展，以增强这种模型的应用范围和提供理论保证。”Here's a breakdown of the translation:* "Path-Dependent Neural Jump ODE" (PD-NJ-ODE) is translated as "path-dependent neural jump ODE" (PD-NJ-ODE), which is a direct translation of the English name.* "in particular" is translated as "特别是" (tèbié shì), which is a common way to emphasize a specific aspect of the previous statement.* "the method learns optimal forecasts given irregularly sampled time series of incomplete past observations" is translated as "这种方法学习最佳预测， givens irregularly sampled time series of incomplete past observations" (zhè zhǒng fāng yào xué xí zhì yì, gěi yìn zhèng yè qián zhèng yǐ jīn zhèng yè qián zhèng). This translation maintains the structure of the original sentence, but uses more formal language and adds the word "givens" to clarify the context.* "So far the process itself and the coordinate-wise observation times were assumed to be independent" is translated as "在现有的方法中，这种过程自身和坐标点wise的观测时间被认为是独立的" (zhè zhǒng fāng yào xiàng yì, zhè zhǒng fāng yào xiàng yì zhèng yè qián zhèng yǐ jīn zhèng yè qián zhèng). This translation maintains the structure of the original sentence, but uses more formal language and adds the word " coordinate-wise" to clarify the context.* "and observations were assumed to be noiseless" is translated as "并且观测被认为是噪声的" (bèi qǐ zhèng yì, gòu yì zhèng yǐ jīn zhèng yè qián zhèng). This translation maintains the structure of the original sentence, but uses more formal language and adds the word "噪声" (noise) to clarify the context.* "In this work we discuss two extensions to lift these restrictions" is translated as "在这篇文章中，我们讨论了两种扩展，以增强这种模型的应用范围" (zhè zhǒng fāng yào xiàng yì, wǒmen tiǎo yì zhèng zhèng yīn zhèng yè qián zhèng yǐ jīn zhèng yè qián zhèng). This translation maintains the structure of the original sentence, but uses more formal language and adds the word "扩展" (extension) to clarify the context.* "and provide theoretical guarantees as well as empirical examples for them" is translated as "并提供理论保证，以及实证示例" (bèi tīng yì zhèng, yǐng qǐ zhèng yì). This translation maintains the structure of the original sentence, but uses more formal language and adds the word "理论保证" (theoretical guarantees) to clarify the context.
</details></li>
</ul>
<hr>
<h2 id="Does-Progress-On-Object-Recognition-Benchmarks-Improve-Real-World-Generalization"><a href="#Does-Progress-On-Object-Recognition-Benchmarks-Improve-Real-World-Generalization" class="headerlink" title="Does Progress On Object Recognition Benchmarks Improve Real-World Generalization?"></a>Does Progress On Object Recognition Benchmarks Improve Real-World Generalization?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13136">http://arxiv.org/abs/2307.13136</a></li>
<li>repo_url: None</li>
<li>paper_authors: Megan Richards, Polina Kirichenko, Diane Bouchacourt, Mark Ibrahim<br>for:* 这种研究旨在测试对象识别模型在不同地区的普遍性。methods:* 使用了两个 datasets of objects from households across the globe，并进行了大量的实验研究。results:* 发现标准 benchmarks 不能准确度量实际世界中的普遍性，而实际的地区差异导致模型的性能差异较大。* 发现规模alone 不能 garantate 实际世界中的一致性，而在早期实验中，简单地 retrained 最后一层的数据可以减少地区差异。<details>
<summary>Abstract</summary>
For more than a decade, researchers have measured progress in object recognition on ImageNet-based generalization benchmarks such as ImageNet-A, -C, and -R. Recent advances in foundation models, trained on orders of magnitude more data, have begun to saturate these standard benchmarks, but remain brittle in practice. This suggests standard benchmarks, which tend to focus on predefined or synthetic changes, may not be sufficient for measuring real world generalization. Consequently, we propose studying generalization across geography as a more realistic measure of progress using two datasets of objects from households across the globe. We conduct an extensive empirical evaluation of progress across nearly 100 vision models up to most recent foundation models. We first identify a progress gap between standard benchmarks and real-world, geographical shifts: progress on ImageNet results in up to 2.5x more progress on standard generalization benchmarks than real-world distribution shifts. Second, we study model generalization across geographies by measuring the disparities in performance across regions, a more fine-grained measure of real world generalization. We observe all models have large geographic disparities, even foundation CLIP models, with differences of 7-20% in accuracy between regions. Counter to modern intuition, we discover progress on standard benchmarks fails to improve geographic disparities and often exacerbates them: geographic disparities between the least performant models and today's best models have more than tripled. Our results suggest scaling alone is insufficient for consistent robustness to real-world distribution shifts. Finally, we highlight in early experiments how simple last layer retraining on more representative, curated data can complement scaling as a promising direction of future work, reducing geographic disparity on both benchmarks by over two-thirds.
</details>
<details>
<summary>摘要</summary>
We conducted an extensive empirical evaluation of progress across nearly 100 vision models, including the most recent foundation models. Our results reveal a significant gap between progress on ImageNet and real-world, geographical shifts. Specifically, we found that progress on ImageNet results in up to 2.5 times more progress on standard generalization benchmarks than real-world distribution shifts.Furthermore, we studied model generalization across geographies by measuring the disparities in performance across regions, providing a more fine-grained measure of real-world generalization. Our findings show that all models, including foundation CLIP models, exhibit large geographic disparities, with differences of 7-20% in accuracy between regions. Surprisingly, we found that progress on standard benchmarks does not improve geographic disparities and often exacerbates them. In fact, the geographic disparities between the least performant models and today's best models have more than tripled.Our results suggest that scaling alone is insufficient for achieving consistent robustness to real-world distribution shifts. However, we discovered that simple last layer retraining on more representative, curated data can complement scaling as a promising direction of future work. By doing so, we were able to reduce geographic disparity on both benchmarks by over two-thirds. Our findings highlight the importance of considering real-world distribution shifts when evaluating progress in object recognition and provide a new direction for future research.
</details></li>
</ul>
<hr>
<h2 id="simPLE-a-visuotactile-method-learned-in-simulation-to-precisely-pick-localize-regrasp-and-place-objects"><a href="#simPLE-a-visuotactile-method-learned-in-simulation-to-precisely-pick-localize-regrasp-and-place-objects" class="headerlink" title="simPLE: a visuotactile method learned in simulation to precisely pick, localize, regrasp, and place objects"></a>simPLE: a visuotactile method learned in simulation to precisely pick, localize, regrasp, and place objects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13133">http://arxiv.org/abs/2307.13133</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maria Bauza, Antonia Bronars, Yifan Hou, Ian Taylor, Nikhil Chavan-Dafle, Alberto Rodriguez</li>
<li>for: 这篇论文旨在解决 robotic manipulation 中的一般性和精度之间的矛盾。</li>
<li>methods: 该论文提出了一种叫做 simPLE 的解决方案，用于精度的 pick-and-place 任务。 simPLE 包括三个主要组成部分：任务意识 grasping，视听感知和重新抓取规划。</li>
<li>results: 在一个 dual-arm 机器人上，使用 simPLE 可以成功地完成 15 种不同的物体的 pick-and-place 任务，其中 6 种物体的成功率高于 90%，而剩下 11 种物体的成功率高于 80%。视频可以在 <a target="_blank" rel="noopener" href="http://mcube.mit.edu/research/simPLE.html">http://mcube.mit.edu/research/simPLE.html</a> 上查看。<details>
<summary>Abstract</summary>
Existing robotic systems have a clear tension between generality and precision. Deployed solutions for robotic manipulation tend to fall into the paradigm of one robot solving a single task, lacking precise generalization, i.e., the ability to solve many tasks without compromising on precision. This paper explores solutions for precise and general pick-and-place. In precise pick-and-place, i.e. kitting, the robot transforms an unstructured arrangement of objects into an organized arrangement, which can facilitate further manipulation. We propose simPLE (simulation to Pick Localize and PLacE) as a solution to precise pick-and-place. simPLE learns to pick, regrasp and place objects precisely, given only the object CAD model and no prior experience. We develop three main components: task-aware grasping, visuotactile perception, and regrasp planning. Task-aware grasping computes affordances of grasps that are stable, observable, and favorable to placing. The visuotactile perception model relies on matching real observations against a set of simulated ones through supervised learning. Finally, we compute the desired robot motion by solving a shortest path problem on a graph of hand-to-hand regrasps. On a dual-arm robot equipped with visuotactile sensing, we demonstrate pick-and-place of 15 diverse objects with simPLE. The objects span a wide range of shapes and simPLE achieves successful placements into structured arrangements with 1mm clearance over 90% of the time for 6 objects, and over 80% of the time for 11 objects. Videos are available at http://mcube.mit.edu/research/simPLE.html .
</details>
<details>
<summary>摘要</summary>
现有的 роботиче系统存在一种明显的矛盾：一个机器人可以解决一个特定任务，但缺乏高级别的总体化能力，即能够解决多个任务而不失去精度。这篇论文探讨精度和通用性的pick-and-place解决方案。在精度的pick-and-place任务中，机器人将无结构的物品排序成结构化的排序，以便进一步的操作。我们提出了simPLE（从模拟到选择和置入）解决方案，该方案可以帮助机器人准确地找到、重新抓取和置入物品，只需要物品的CAD模型，无需互联网经验。我们开发了三个主要组成部分：任务意识 grasping、视听感知和重新抓取规划。任务意识 grasping计算可行的抓取方式，以确保稳定、可见和置入的情况。视听感知模型通过对实际观察与 simulate 的对比，通过超过学习来学习实际观察。最后，我们解决了一个短路问题，以计算手中重新抓取的最佳动作。在配备视听感知的双臂机器人上，我们用simPLE实现了15种多样化的物品的pick-and-place任务，物品的形状覆盖广泛，simPLE在90%的时间内成功地将物品置入结构化的排序中，距离1毫米。视频可以在http://mcube.mit.edu/research/simPLE.html 中找到。
</details></li>
</ul>
<hr>
<h2 id="A-Differentially-Private-Weighted-Empirical-Risk-Minimization-Procedure-and-its-Application-to-Outcome-Weighted-Learning"><a href="#A-Differentially-Private-Weighted-Empirical-Risk-Minimization-Procedure-and-its-Application-to-Outcome-Weighted-Learning" class="headerlink" title="A Differentially Private Weighted Empirical Risk Minimization Procedure and its Application to Outcome Weighted Learning"></a>A Differentially Private Weighted Empirical Risk Minimization Procedure and its Application to Outcome Weighted Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13127">http://arxiv.org/abs/2307.13127</a></li>
<li>repo_url: None</li>
<li>paper_authors: Spencer Giddens, Yiwang Zhou, Kevin R. Krull, Tara M. Brinkman, Peter X. K. Song, Fang Liu</li>
<li>For: 这个论文的目的是提出一种在使用敏感数据时实现隐私保护的可靠学习方法，以解决数据隐私issue。* Methods: 该论文使用了 differential privacy（DP）框架，并提出了一种基于 weights ERM 的首个具有隐私保证的算法，以及一种对现有 DP-ERM 方法的推广。* Results: 实验研究表明，通过在 wERM 中应用 DP 保证可以train OWL 模型，而不会导致模型性能下降。这种隐私保护的 OWL 方法在实验中得到了证明，并在实际临床试验中得到了应用。<details>
<summary>Abstract</summary>
It is commonplace to use data containing personal information to build predictive models in the framework of empirical risk minimization (ERM). While these models can be highly accurate in prediction, results obtained from these models with the use of sensitive data may be susceptible to privacy attacks. Differential privacy (DP) is an appealing framework for addressing such data privacy issues by providing mathematically provable bounds on the privacy loss incurred when releasing information from sensitive data. Previous work has primarily concentrated on applying DP to unweighted ERM. We consider an important generalization to weighted ERM (wERM). In wERM, each individual's contribution to the objective function can be assigned varying weights. In this context, we propose the first differentially private wERM algorithm, backed by a rigorous theoretical proof of its DP guarantees under mild regularity conditions. Extending the existing DP-ERM procedures to wERM paves a path to deriving privacy-preserving learning methods for individualized treatment rules, including the popular outcome weighted learning (OWL). We evaluate the performance of the DP-wERM application to OWL in a simulation study and in a real clinical trial of melatonin for sleep health. All empirical results demonstrate the viability of training OWL models via wERM with DP guarantees while maintaining sufficiently useful model performance. Therefore, we recommend practitioners consider implementing the proposed privacy-preserving OWL procedure in real-world scenarios involving sensitive data.
</details>
<details>
<summary>摘要</summary>
通常使用包含个人信息的数据来构建预测模型在预测风险最小化（ERM）框架中。虽然这些模型可以在预测上具有非常高的准确性，但使用敏感数据时可能会遭受隐私攻击。不等式隐私（DP）是一个吸引人的框架，可以为隐私泄露提供数学上可证明的约束。前一个研究主要集中在应用DP于不加权的ERM。我们考虑了一个重要的扩展，即加权ERM（wERM）。在wERM中，每个个体的对象函数中的贡献可以被分配不同的权重。在这种情况下，我们提出了首个具有DP保证的wERM算法，并提供了在轻度Regularity Conditions下的理论证明。通过扩展现有的DP-ERM过程，我们开辟了一条privacy-preserving学习方法的道路，包括流行的结果权重学习（OWL）。我们在一个Simulation Study和一个实际的临床试验中评估了DP-wERM应用于OWL的性能。所有实验结果表明，可以通过wERM WITH DP保证来训练OWL模型，而不会丧失有用的模型性能。因此，我们建议实践者在涉及敏感数据的实际场景中考虑实施我们提议的隐私保护OWL过程。
</details></li>
</ul>
<hr>
<h2 id="A-Hybrid-Machine-Learning-Model-for-Classifying-Gene-Mutations-in-Cancer-using-LSTM-BiLSTM-CNN-GRU-and-GloVe"><a href="#A-Hybrid-Machine-Learning-Model-for-Classifying-Gene-Mutations-in-Cancer-using-LSTM-BiLSTM-CNN-GRU-and-GloVe" class="headerlink" title="A Hybrid Machine Learning Model for Classifying Gene Mutations in Cancer using LSTM, BiLSTM, CNN, GRU, and GloVe"></a>A Hybrid Machine Learning Model for Classifying Gene Mutations in Cancer using LSTM, BiLSTM, CNN, GRU, and GloVe</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14361">http://arxiv.org/abs/2307.14361</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanad Aburass, Osama Dorgham, Jamil Al Shaqsi</li>
<li>for: 这个研究旨在使用Kaggle的个性化医学：重新定义癌症治疗数据集来类型基因变化。</li>
<li>methods: 该模型 combinig LSTM, BiLSTM, CNN, GRU, 以及GloVe来进行基因变化分类。</li>
<li>results: 该模型在准确率、精度、回归率、F1分数和平均方差方面与其他模型进行比较，并取得了最高的表现。此外，它还需要较少的训练时间，从而实现了性能和效率的完美协同。<details>
<summary>Abstract</summary>
This study presents an ensemble model combining LSTM, BiLSTM, CNN, GRU, and GloVe to classify gene mutations using Kaggle's Personalized Medicine: Redefining Cancer Treatment dataset. The results were compared against well-known transformers like as BERT, Electra, Roberta, XLNet, Distilbert, and their LSTM ensembles. Our model outperformed all other models in terms of accuracy, precision, recall, F1 score, and Mean Squared Error. Surprisingly, it also needed less training time, resulting in a perfect combination of performance and efficiency. This study demonstrates the utility of ensemble models for difficult tasks such as gene mutation classification.
</details>
<details>
<summary>摘要</summary>
Note:* LSTM: Long Short-Term Memory* BiLSTM: Bidirectional Long Short-Term Memory* CNN: Convolutional Neural Network* GRU: Gated Recurrent Unit* GloVe: Global Vectors for Word Representation* BERT: Bidirectional Encoder Representations from Transformers* Electra: Efficient Lifelong End-to-End Text Recognition* Roberta: Robustly Optimized BERT Pretraining Approach* XLNet: Extreme Language Modeling* Distilbert: Distilled BERTPlease note that the translation is in Simplified Chinese, and the words and phrases in bold are the names of the models and techniques used in the study.
</details></li>
</ul>
<hr>
<h2 id="Deep-Bradley-Terry-Rating-Quantifying-Properties-from-Comparisons"><a href="#Deep-Bradley-Terry-Rating-Quantifying-Properties-from-Comparisons" class="headerlink" title="Deep Bradley-Terry Rating: Quantifying Properties from Comparisons"></a>Deep Bradley-Terry Rating: Quantifying Properties from Comparisons</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13709">http://arxiv.org/abs/2307.13709</a></li>
<li>repo_url: None</li>
<li>paper_authors: Satoru Fujii</li>
<li>for: 这篇论文的目的是为了量化和评估未知对象的属性。</li>
<li>methods: 这篇论文使用了深度学习框架，并将布莱德利-泰勒模型 integrate 到 neural network 结构中。此外，它还推广到不平等环境下，以便更好地应用于实际场景。</li>
<li>results: 经过实验分析，这篇论文成功地量化和估算了欲要的属性。<details>
<summary>Abstract</summary>
Many properties in the real world can't be directly observed, making them difficult to learn. To deal with this challenging problem, prior works have primarily focused on estimating those properties by using graded human scores as the target label in the training. Meanwhile, rating algorithms based on the Bradley-Terry model are extensively studied to evaluate the competitiveness of players based on their match history. In this paper, we introduce the Deep Bradley-Terry Rating (DBTR), a novel machine learning framework designed to quantify and evaluate properties of unknown items. Our method seamlessly integrates the Bradley-Terry model into the neural network structure. Moreover, we generalize this architecture further to asymmetric environments with unfairness, a condition more commonly encountered in real-world settings. Through experimental analysis, we demonstrate that DBTR successfully learns to quantify and estimate desired properties.
</details>
<details>
<summary>摘要</summary>
多种Properties在现实世界中难以直接观察，这使得它们学习变得困难。以前的工作主要通过使用排名作为目标标签进行估算这些Properties。而BRADLEY-TERRY模型的评分算法在评估玩家的竞争力方面得到了广泛的研究。在这篇论文中，我们引入了深度BRADLEY-TERRY评分（DBTR），一种新的机器学习框架，用于评估和评价未知项目的属性。我们将BRADLEY-TERRY模型与神经网络结构结合，并将其扩展到偏袋环境中，以适应实际世界中更常见的不平等条件。通过实验分析，我们证明了DBTR成功地学习和估算所需的属性。
</details></li>
</ul>
<hr>
<h2 id="Conformal-prediction-for-frequency-severity-modeling"><a href="#Conformal-prediction-for-frequency-severity-modeling" class="headerlink" title="Conformal prediction for frequency-severity modeling"></a>Conformal prediction for frequency-severity modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13124">http://arxiv.org/abs/2307.13124</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/heltongraziadei/conformal-fs">https://github.com/heltongraziadei/conformal-fs</a></li>
<li>paper_authors: Helton Graziadei, Paulo C. Marques F., Eduardo F. L. de Melo, Rodrigo S. Targino</li>
<li>for: 预测保险索赔数量</li>
<li>methods: 非参数模型无关框架、分割兼容预测、基于出袋机制的适应宽度调整</li>
<li>results: 适用于实际数据集和仿真数据集，可以生成具有finite sample statistcial guarantees的预测间隔<details>
<summary>Abstract</summary>
We present a nonparametric model-agnostic framework for building prediction intervals of insurance claims, with finite sample statistical guarantees, extending the technique of split conformal prediction to the domain of two-stage frequency-severity modeling. The effectiveness of the framework is showcased with simulated and real datasets. When the underlying severity model is a random forest, we extend the two-stage split conformal prediction procedure, showing how the out-of-bag mechanism can be leveraged to eliminate the need for a calibration set and to enable the production of prediction intervals with adaptive width.
</details>
<details>
<summary>摘要</summary>
我们提出了一种非参数化、模型无关的框架，用于构建保险索赔预测范围，具有有限样本统计保证，基于分割哲学预测技术扩展到两个阶段频率严重模型预测领域。我们通过使用 simulate 和实际数据示例，证明了该框架的效iveness。当下面际严重模型是随机森林时，我们对两个阶段分割哲学预测过程进行扩展，并示出了如何通过尝试机制来消除需要Calibration集和生成适应宽度的预测范围。
</details></li>
</ul>
<hr>
<h2 id="An-Explainable-Geometric-Weighted-Graph-Attention-Network-for-Identifying-Functional-Networks-Associated-with-Gait-Impairment"><a href="#An-Explainable-Geometric-Weighted-Graph-Attention-Network-for-Identifying-Functional-Networks-Associated-with-Gait-Impairment" class="headerlink" title="An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment"></a>An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13108">http://arxiv.org/abs/2307.13108</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/favour-nerrise/xgw-gat">https://github.com/favour-nerrise/xgw-gat</a></li>
<li>paper_authors: Favour Nerrise, Qingyu Zhao, Kathleen L. Poston, Kilian M. Pohl, Ehsan Adeli<br>for:这个研究旨在更好地理解帕金森病（PD）的运动进程，特别是步行障碍和平衡问题的发展。通过识别大脑功能失调的特征，可以更好地理解PD的运动进程，从而开发更有效和个性化的治疗方法。methods:这个研究使用了一种可解释的、几何学的、质量权重 graphs attention neural network（xGW-GAT），用于预测帕金森病患者的步行障碍级别。xGW-GAT使用了函数连接矩阵来表示整个连接网络，并使用个性化的注意力mask来提取个体和群体水平的解释。results:这个研究发现，xGW-GAT在resting-state功能磁共振成像（rs-fMRI）数据集中的帕金森病患者中出色地预测了步行障碍级别，并提供了解释性的功能子网络结构。与现有的方法相比，xGW-GAT模型成功地超越了其他方法，同时揭示了临床有关的连接模式。<details>
<summary>Abstract</summary>
One of the hallmark symptoms of Parkinson's Disease (PD) is the progressive loss of postural reflexes, which eventually leads to gait difficulties and balance problems. Identifying disruptions in brain function associated with gait impairment could be crucial in better understanding PD motor progression, thus advancing the development of more effective and personalized therapeutics. In this work, we present an explainable, geometric, weighted-graph attention neural network (xGW-GAT) to identify functional networks predictive of the progression of gait difficulties in individuals with PD. xGW-GAT predicts the multi-class gait impairment on the MDS Unified PD Rating Scale (MDS-UPDRS). Our computational- and data-efficient model represents functional connectomes as symmetric positive definite (SPD) matrices on a Riemannian manifold to explicitly encode pairwise interactions of entire connectomes, based on which we learn an attention mask yielding individual- and group-level explainability. Applied to our resting-state functional MRI (rs-fMRI) dataset of individuals with PD, xGW-GAT identifies functional connectivity patterns associated with gait impairment in PD and offers interpretable explanations of functional subnetworks associated with motor impairment. Our model successfully outperforms several existing methods while simultaneously revealing clinically-relevant connectivity patterns. The source code is available at https://github.com/favour-nerrise/xGW-GAT .
</details>
<details>
<summary>摘要</summary>
一个典型的parkinson病（PD）的表现之一是慢慢地失去姿态反射，这会导致步态困难和平衡问题。确定潜在的脑功能干预在步态困难方面可能是理解PD motor进程的关键，从而开发更有效和个性化的治疗方法。在这项工作中，我们提出了一种可解释的、几何的、质量权重的神经网络（xGW-GAT），用于预测PD患者的步态困难级别。xGW-GAT预测了MDS联合PD评估rating scale（MDS-UPDRS）中的多个步态困难类型。我们的计算和数据有效的模型将功能连接矩阵（connectome）表示为对称正定的矩阵（SPD），并在RIemannian manifold上进行Explicitly编码对整个connectome的对称对应关系，基于这些对应关系我们学习一个注意力mask，以获得个体和组级别的解释。应用于我们的resting-state功能MRI（rs-fMRI）数据集中的PD患者，xGW-GAT确定了与步态困难相关的功能连接模式，并提供了可解释的功能子网络相关于运动障碍。我们的模型成功击败了一些现有的方法，同时揭示了临床有用的连接模式。模型代码可以在https://github.com/favour-nerrise/xGW-GAT 上获取。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Example-Based-Control"><a href="#Contrastive-Example-Based-Control" class="headerlink" title="Contrastive Example-Based Control"></a>Contrastive Example-Based Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13101">http://arxiv.org/abs/2307.13101</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/khatch31/laeo">https://github.com/khatch31/laeo</a></li>
<li>paper_authors: Kyle Hatch, Benjamin Eysenbach, Rafael Rafailov, Tianhe Yu, Ruslan Salakhutdinov, Sergey Levine, Chelsea Finn</li>
<li>for: 本研究旨在提出一种基于例子的离线控制方法，可以学习多步转移的隐藏模型，而不需要 specify 奖励函数。</li>
<li>methods: 本方法使用了数据驱动的方法，通过从转移动力学中采样得到的例子来学习隐藏模型，并使用这些模型来预测转移的Q值。</li>
<li>results: 相比基elines，本方法在多个状态基于和图像基于的离线控制任务中表现出色，并且在数据集大小增加时显示了更好的稳定性和扩展性。<details>
<summary>Abstract</summary>
While many real-world problems that might benefit from reinforcement learning, these problems rarely fit into the MDP mold: interacting with the environment is often expensive and specifying reward functions is challenging. Motivated by these challenges, prior work has developed data-driven approaches that learn entirely from samples from the transition dynamics and examples of high-return states. These methods typically learn a reward function from high-return states, use that reward function to label the transitions, and then apply an offline RL algorithm to these transitions. While these methods can achieve good results on many tasks, they can be complex, often requiring regularization and temporal difference updates. In this paper, we propose a method for offline, example-based control that learns an implicit model of multi-step transitions, rather than a reward function. We show that this implicit model can represent the Q-values for the example-based control problem. Across a range of state-based and image-based offline control tasks, our method outperforms baselines that use learned reward functions; additional experiments demonstrate improved robustness and scaling with dataset size.
</details>
<details>
<summary>摘要</summary>
虽然许多实际问题可以受惠于强化学习，但这些问题很少遵循MDP模型：与环境交互通常是昂贵的，并且指定奖励函数是困难的。为了解决这些挑战，先前的工作已经开发了基于数据的方法，这些方法通过从转移动态和高返回状态中提取样本来学习。这些方法通常会学习一个奖励函数从高返回状态，使用这个奖励函数来标注转移，然后应用在这些转移上的离线RL算法。虽然这些方法可以在许多任务上达到良好的结果，但它们可能是复杂的，经常需要规则化和时间差更新。在这篇论文中，我们提出了一种离线、示例基于的控制方法，这种方法学习了多步转移的隐藏模型，而不是奖励函数。我们示示了这个隐藏模型可以表示示例基于的控制问题中的Q值。在一系列的状态基本和图像基本的离线控制任务上，我们的方法超过了基于学习的奖励函数的基eline，其他实验还证明了我们的方法具有更好的 Robustness 和数据集大小增长。
</details></li>
</ul>
<hr>
<h2 id="Label-Noise-Correcting-a-Correction"><a href="#Label-Noise-Correcting-a-Correction" class="headerlink" title="Label Noise: Correcting a Correction"></a>Label Noise: Correcting a Correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13100">http://arxiv.org/abs/2307.13100</a></li>
<li>repo_url: None</li>
<li>paper_authors: William Toner, Amos Storkey</li>
<li>for: addressing the issue of overfitting in training neural network classifiers on datasets with label noise</li>
<li>methods: proposing a more direct approach to mitigate overfitting by imposing a lower bound on the empirical risk during training</li>
<li>results: providing theoretical results with explicit, easily computable bounds on the minimum achievable noisy risk for different loss functions, and demonstrating significant enhancement in robustness with virtually no additional computational cost.Here’s the full text in Simplified Chinese:</li>
<li>for: 这个研究旨在解决对于训练神经网络分类器的标签杂读问题</li>
<li>methods: 我们提出了一种更直接的方法，通过在训练过程中对实验风险下设置下限，以遏止过拟合</li>
<li>results: 我们提供了具体、容易计算的下限 bounds，以及实验结果，显示这种方法可以帮助提高神经网络分类器的Robustness，而且无需额外的计算成本。<details>
<summary>Abstract</summary>
Training neural network classifiers on datasets with label noise poses a risk of overfitting them to the noisy labels. To address this issue, researchers have explored alternative loss functions that aim to be more robust. However, many of these alternatives are heuristic in nature and still vulnerable to overfitting or underfitting. In this work, we propose a more direct approach to tackling overfitting caused by label noise. We observe that the presence of label noise implies a lower bound on the noisy generalised risk. Building upon this observation, we propose imposing a lower bound on the empirical risk during training to mitigate overfitting. Our main contribution is providing theoretical results that yield explicit, easily computable bounds on the minimum achievable noisy risk for different loss functions. We empirically demonstrate that using these bounds significantly enhances robustness in various settings, with virtually no additional computational cost.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将神经网络分类器训练数据集中的标签噪声可能会导致模型过度适应。为解决这个问题，研究人员已经探索了一些alternative的损失函数，以增强模型的Robustness。然而，许多这些alternative都是HEURISTIC的 Natur，可能会导致过度适应或者下降。在这项工作中，我们提出了一种更直接的方法来处理标签噪声导致的过度适应。我们发现，标签噪声存在一个下界，这个下界对于不同的损失函数来说都是可能的。基于这个发现，我们提议在训练过程中尝试在Empirical risk下设置下界，以避免过度适应。我们的主要贡献是提供了理论结果，可以给出不同损失函数的明确、容易计算的下界，以降低过度适应的风险。我们的实验结果表明，使用这些下界可以在不同的设置下提高模型的Robustness，而且几乎没有额外的计算成本。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Ensemble-Learning-for-Materials-Property-Prediction-with-Classical-Interatomic-Potentials-Carbon-as-an-Example"><a href="#Interpretable-Ensemble-Learning-for-Materials-Property-Prediction-with-Classical-Interatomic-Potentials-Carbon-as-an-Example" class="headerlink" title="Interpretable Ensemble Learning for Materials Property Prediction with Classical Interatomic Potentials: Carbon as an Example"></a>Interpretable Ensemble Learning for Materials Property Prediction with Classical Interatomic Potentials: Carbon as an Example</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10818">http://arxiv.org/abs/2308.10818</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyu Jiang, Haofan Sun, Kamal Choudhary, Houlong Zhuang, Qiong Nian</li>
<li>for: 这篇论文目的是用机器学习（ML）技术预测晶体材料的性能。</li>
<li>methods: 该方法使用了回归树 ensemble learning，不需要任何描述符，直接使用分子动力学计算的物理性质作为输入。</li>
<li>results: 结果显示，对于碳杂合物的小样本数据，ensemble learning的预测结果比使用传统的分子动力学potential更准确，并且能够捕捉9种不同的分子动力学potential中的相对准确性。<details>
<summary>Abstract</summary>
Machine learning (ML) is widely used to explore crystal materials and predict their properties. However, the training is time-consuming for deep-learning models, and the regression process is a black box that is hard to interpret. Also, the preprocess to transfer a crystal structure into the input of ML, called descriptor, needs to be designed carefully. To efficiently predict important properties of materials, we propose an approach based on ensemble learning consisting of regression trees to predict formation energy and elastic constants based on small-size datasets of carbon allotropes as an example. Without using any descriptor, the inputs are the properties calculated by molecular dynamics with 9 different classical interatomic potentials. Overall, the results from ensemble learning are more accurate than those from classical interatomic potentials, and ensemble learning can capture the relatively accurate properties from the 9 classical potentials as criteria for predicting the final properties.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）广泛应用于探索晶体材料和预测其性能。然而，训练深度学习模型需时间consuming， regression 过程是一个难以解释的黑盒子。此外，将晶体结构转换为 ML 的输入，即描述符，需要仔细设计。为了有效预测材料的重要性能，我们提出了基于集成学习的方法，包括回归树来预测基于小型 datasets of carbon allotropes 的形成能gy和弹性常数。无需使用任何描述符，输入是通过分子动力学计算的物理性质。总的来说， ensemble 学习的结果比 классиical interatomic potentials 更加准确，并且 ensemble 学习可以捕捉来自 nine classical potentials 的相对准确性作为预测最终性能的标准。
</details></li>
</ul>
<hr>
<h2 id="Fairness-Under-Demographic-Scarce-Regime"><a href="#Fairness-Under-Demographic-Scarce-Regime" class="headerlink" title="Fairness Under Demographic Scarce Regime"></a>Fairness Under Demographic Scarce Regime</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13081">http://arxiv.org/abs/2307.13081</a></li>
<li>repo_url: None</li>
<li>paper_authors: Patrik Joslin Kenfack, Samira Ebrahimi Kahou, Ulrich Aïvodji</li>
<li>for: 本研究旨在 Addressing the limitation of prior works on fairness, which assume full access to demographic information, but in reality, demographic information may be partially available or unavailable due to privacy concerns.</li>
<li>methods: 本研究提出了一个 attribute classifier 建构框架，通过将不确定性写入模型中，并在有demographic信息的样本上强制施行公平性约束，以提高公平精度贡献。</li>
<li>results: 经过实验显示，提出的框架可以实现更好的公平精度贡献，并且超越了使用真实敏感特征的模型。此外，模型还能够在没有demographic信息的情况下提供更好的公平精度贡献。<details>
<summary>Abstract</summary>
Most existing works on fairness assume the model has full access to demographic information. However, there exist scenarios where demographic information is partially available because a record was not maintained throughout data collection or due to privacy reasons. This setting is known as demographic scarce regime. Prior research have shown that training an attribute classifier to replace the missing sensitive attributes (proxy) can still improve fairness. However, the use of proxy-sensitive attributes worsens fairness-accuracy trade-offs compared to true sensitive attributes. To address this limitation, we propose a framework to build attribute classifiers that achieve better fairness-accuracy trade-offs. Our method introduces uncertainty awareness in the attribute classifier and enforces fairness on samples with demographic information inferred with the lowest uncertainty. We show empirically that enforcing fairness constraints on samples with uncertain sensitive attributes is detrimental to fairness and accuracy. Our experiments on two datasets showed that the proposed framework yields models with significantly better fairness-accuracy trade-offs compared to classic attribute classifiers. Surprisingly, our framework outperforms models trained with constraints on the true sensitive attributes.
</details>
<details>
<summary>摘要</summary>
现有大多数工作假设模型拥有完整的人口信息。然而，有些场景下人口信息部分可用，例如记录不完整或因隐私原因无法获取。这种情况被称为人口缺乏 regime。先前的研究表明，使用代理敏感特征来代替缺失的敏感特征可以改善公平。然而，使用代理敏感特征会对公平精度负面影响比使用真实的敏感特征更大。为解决这些限制，我们提出了一个框架，用于建立具有更好的公平精度负面影响的特征分类器。我们的方法在特征分类器中引入了不确定性意识，并在具有最低不确定性的人口信息上遵循公平约束。我们的实验表明，对不确定的敏感特征进行公平约束是对公平和准确性的负面影响。我们的方法在两个数据集上进行了实验，并显示了与经典特征分类器相比，我们的框架可以获得显著更好的公平精度负面影响。另外，我们的方法还超过使用约束的真实敏感特征模型。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Certified-Training-Towards-Better-Accuracy-Robustness-Tradeoffs"><a href="#Adaptive-Certified-Training-Towards-Better-Accuracy-Robustness-Tradeoffs" class="headerlink" title="Adaptive Certified Training: Towards Better Accuracy-Robustness Tradeoffs"></a>Adaptive Certified Training: Towards Better Accuracy-Robustness Tradeoffs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13078">http://arxiv.org/abs/2307.13078</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhakshylyk Nurlanov, Frank R. Schmidt, Florian Bernard</li>
<li>for: 提高模型的强健性和标准准确率之间的质量衡量。</li>
<li>methods: 基于适应证明的半径的训练方法，通过提高模型的准确率和强健性来实现更好的准确率-强健性质量衡量。</li>
<li>results: 在MNIST、CIFAR-10和TinyImageNet dataset上，提出的方法可以实现更高的强健性和标准准确率之间的质量衡量，特别是在CIFAR-10和TinyImageNet dataset上，模型的强健性可以提高至两倍的水平，而且保持同等水平的标准准确率。<details>
<summary>Abstract</summary>
As deep learning models continue to advance and are increasingly utilized in real-world systems, the issue of robustness remains a major challenge. Existing certified training methods produce models that achieve high provable robustness guarantees at certain perturbation levels. However, the main problem of such models is a dramatically low standard accuracy, i.e. accuracy on clean unperturbed data, that makes them impractical. In this work, we consider a more realistic perspective of maximizing the robustness of a model at certain levels of (high) standard accuracy. To this end, we propose a novel certified training method based on a key insight that training with adaptive certified radii helps to improve both the accuracy and robustness of the model, advancing state-of-the-art accuracy-robustness tradeoffs. We demonstrate the effectiveness of the proposed method on MNIST, CIFAR-10, and TinyImageNet datasets. Particularly, on CIFAR-10 and TinyImageNet, our method yields models with up to two times higher robustness, measured as an average certified radius of a test set, at the same levels of standard accuracy compared to baseline approaches.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="General-Purpose-Multi-Modal-OOD-Detection-Framework"><a href="#General-Purpose-Multi-Modal-OOD-Detection-Framework" class="headerlink" title="General-Purpose Multi-Modal OOD Detection Framework"></a>General-Purpose Multi-Modal OOD Detection Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13069">http://arxiv.org/abs/2307.13069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Viet Duong, Qiong Wu, Zhengyi Zhou, Eric Zavesky, Jiahe Chen, Xiangzhou Liu, Wen-Ling Hsu, Huajie Shao</li>
<li>for: 这个研究的目的是为了实现多种不同的假值检测方法，以确保机器学习系统的安全性和可靠性。</li>
<li>methods: 这个研究使用了一个通用的弱监督的假值检测框架，叫做WOOD，它结合了一个二分类器和一个对照学习部分，以获得两者的好处。</li>
<li>results: 实验结果显示，WOOD模型在多个真实世界的数据集上表现出色，能够同时在三个不同的假值enario中实现高准确性。<details>
<summary>Abstract</summary>
Out-of-distribution (OOD) detection identifies test samples that differ from the training data, which is critical to ensuring the safety and reliability of machine learning (ML) systems. While a plethora of methods have been developed to detect uni-modal OOD samples, only a few have focused on multi-modal OOD detection. Current contrastive learning-based methods primarily study multi-modal OOD detection in a scenario where both a given image and its corresponding textual description come from a new domain. However, real-world deployments of ML systems may face more anomaly scenarios caused by multiple factors like sensor faults, bad weather, and environmental changes. Hence, the goal of this work is to simultaneously detect from multiple different OOD scenarios in a fine-grained manner. To reach this goal, we propose a general-purpose weakly-supervised OOD detection framework, called WOOD, that combines a binary classifier and a contrastive learning component to reap the benefits of both. In order to better distinguish the latent representations of in-distribution (ID) and OOD samples, we adopt the Hinge loss to constrain their similarity. Furthermore, we develop a new scoring metric to integrate the prediction results from both the binary classifier and contrastive learning for identifying OOD samples. We evaluate the proposed WOOD model on multiple real-world datasets, and the experimental results demonstrate that the WOOD model outperforms the state-of-the-art methods for multi-modal OOD detection. Importantly, our approach is able to achieve high accuracy in OOD detection in three different OOD scenarios simultaneously. The source code will be made publicly available upon publication.
</details>
<details>
<summary>摘要</summary>
外部数据（OOD）检测可以识别测试样本与训练数据之间的差异，这是机器学习（ML）系统的安全性和可靠性的关键。虽然大量方法已经开发出来检测uni-modal OOD样本，但只有一些关注多模态 OOD 检测。当前的对比学习基于方法主要在一个给定的图像和其相应的文本描述来自新领域的情况下进行多模态 OOD 检测。然而，实际世界中 ML 系统的部署可能会遇到更多的异常情况，如感知器故障、坏天气和环境变化。因此，本研究的目标是同时从多个不同的 OOD 场景中进行细致的检测。为达到这个目标，我们提出了一种通用的弱监督 OOD 检测框架，称为 WOOD，该框架结合了一个二分类器和一个对比学习组件，以便充分利用它们的优势。为了更好地分别 ID 和 OOD 样本的幂本表示，我们采用了缺角损失来约束它们的相似性。此外，我们开发了一个新的评分指标，以集成 binary 分类器和对比学习的预测结果，以便更好地识别 OOD 样本。我们对多个实际世界数据集进行了实验，结果显示，提出的 WOOD 模型在多modal OOD 检测中高度超越了现状的方法。特别是，我们的方法能够同时高精度地识别 OOD 样本在三个不同的 OOD 场景中。代码将在发表后公开。
</details></li>
</ul>
<hr>
<h2 id="Personalized-Category-Frequency-prediction-for-Buy-It-Again-recommendations"><a href="#Personalized-Category-Frequency-prediction-for-Buy-It-Again-recommendations" class="headerlink" title="Personalized Category Frequency prediction for Buy It Again recommendations"></a>Personalized Category Frequency prediction for Buy It Again recommendations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01195">http://arxiv.org/abs/2308.01195</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amit Pande, Kunal Ghosh, Rankyung Park</li>
<li>for: 提高用户体验和站点参与度，预测用户可能会购买的商品</li>
<li>methods: 提出一种层次PCIC模型，包括个性化类别模型（PC模型）和个性化类别内项模型（IC模型），模型使用存活模型和时间序列模型生成特征，然后使用类别划分的神经网络进行训练</li>
<li>results: 与12个基线比较，PCIC在四个标准开放数据集上提高了NDCG达16%，同时提高了回归率约2%，并在大规模数据集上进行了扩展和训练（超过8小时），并在一家大商户的官方网站上进行了AB测试，导致了用户参与度的显著提高<details>
<summary>Abstract</summary>
Buy It Again (BIA) recommendations are crucial to retailers to help improve user experience and site engagement by suggesting items that customers are likely to buy again based on their own repeat purchasing patterns. Most existing BIA studies analyze guests personalized behavior at item granularity. A category-based model may be more appropriate in such scenarios. We propose a recommendation system called a hierarchical PCIC model that consists of a personalized category model (PC model) and a personalized item model within categories (IC model). PC model generates a personalized list of categories that customers are likely to purchase again. IC model ranks items within categories that guests are likely to consume within a category. The hierarchical PCIC model captures the general consumption rate of products using survival models. Trends in consumption are captured using time series models. Features derived from these models are used in training a category-grained neural network. We compare PCIC to twelve existing baselines on four standard open datasets. PCIC improves NDCG up to 16 percent while improving recall by around 2 percent. We were able to scale and train (over 8 hours) PCIC on a large dataset of 100M guests and 3M items where repeat categories of a guest out number repeat items. PCIC was deployed and AB tested on the site of a major retailer, leading to significant gains in guest engagement.
</details>
<details>
<summary>摘要</summary>
Buy It Again (BIA) 建议是重要的 для零售商，帮助改善用户体验和网站参与度，通过建议客户可能会再次购买的商品，基于他们自己的重复购买模式。大多数现有的BIA研究分析客人个性化行为的项目粒度。我们提出一种推荐系统，即层次PCIC模型，包括个性化类别模型（PC模型）和个性化类别内容模型（IC模型）。PC模型生成个性化的类别列表，客户可能会购买的类别。IC模型将类别内容排名，客户可能会在类别内消耗的商品。层次PCIC模型捕捉产品的总消耗率，使用存生模型捕捉消耗趋势。这些特征被用于训练分类器。我们与十二个基准值进行比较，PCIC提高了NDCG达16%，同时提高了回归率约2%。我们可以在8小时内扩展和训练PCIC（超过100万客户和300万个商品），其中客户重复购买的类别大于客户重复购买的商品。PCIC在一家大型零售商的官方网站上进行了部署和AB测试，导致用户参与度显著提高。
</details></li>
</ul>
<hr>
<h2 id="Feature-Gradient-Flow-for-Interpreting-Deep-Neural-Networks-in-Head-and-Neck-Cancer-Prediction"><a href="#Feature-Gradient-Flow-for-Interpreting-Deep-Neural-Networks-in-Head-and-Neck-Cancer-Prediction" class="headerlink" title="Feature Gradient Flow for Interpreting Deep Neural Networks in Head and Neck Cancer Prediction"></a>Feature Gradient Flow for Interpreting Deep Neural Networks in Head and Neck Cancer Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13061">http://arxiv.org/abs/2307.13061</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yinzhu Jin, Jonathan C. Garneau, P. Thomas Fletcher</li>
<li>for: 这篇论文旨在介绍一种新的深度学习模型解释技术，即通过计算模型的梯度流来解释模型做出决策时使用的特征。</li>
<li>methods: 该技术使用了计算模型的梯度流来定义输入数据空间中的非线性坐标，从而解释模型做出决策时使用的信息。然后，通过比较特征的梯度流度量和基线噪声特征的梯度流度量来评估特征的重要性。</li>
<li>results: 在使用了该技术进行训练后，模型的解释性得到了提高。研究人员通过计算模型的梯度流来评估特征的重要性，并发现了一些有用的特征，例如肿瘤大小和形态等。<details>
<summary>Abstract</summary>
This paper introduces feature gradient flow, a new technique for interpreting deep learning models in terms of features that are understandable to humans. The gradient flow of a model locally defines nonlinear coordinates in the input data space representing the information the model is using to make its decisions. Our idea is to measure the agreement of interpretable features with the gradient flow of a model. To then evaluate the importance of a particular feature to the model, we compare that feature's gradient flow measure versus that of a baseline noise feature. We then develop a technique for training neural networks to be more interpretable by adding a regularization term to the loss function that encourages the model gradients to align with those of chosen interpretable features. We test our method in a convolutional neural network prediction of distant metastasis of head and neck cancer from a computed tomography dataset from the Cancer Imaging Archive.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:这篇论文介绍了一种新的技术，即特征涟流，可以使深度学习模型变得更加解释性。特征涟流定义了模型在输入数据空间中的非线性坐标，表示模型做出决策的信息。我们的方法是测量特定的可解释特征与模型涟流的一致程度，并评估特定特征对模型的重要性。我们还开发了一种方法，通过添加一个减少项到损失函数中，使模型的梯度与选择的可解释特征的梯度进行对齐。我们在计算tomography数据集中预测头颈癌 distant metastasis的 convolutional neural network中测试了我们的方法。
</details></li>
</ul>
<hr>
<h2 id="MARIO-Model-Agnostic-Recipe-for-Improving-OOD-Generalization-of-Graph-Contrastive-Learning"><a href="#MARIO-Model-Agnostic-Recipe-for-Improving-OOD-Generalization-of-Graph-Contrastive-Learning" class="headerlink" title="MARIO: Model Agnostic Recipe for Improving OOD Generalization of Graph Contrastive Learning"></a>MARIO: Model Agnostic Recipe for Improving OOD Generalization of Graph Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13055">http://arxiv.org/abs/2307.13055</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhuyun97/mario">https://github.com/zhuyun97/mario</a></li>
<li>paper_authors: Yun Zhu, Haizhou Shi, Zhenshuo Zhang, Siliang Tang</li>
<li>for: 本文研究的问题是非标注图数据上的非标注泛化（Out-of-distribution, OOD）泛化问题，特别是图神经网络（Graph Neural Network, GNN）在分布转移时的敏感性问题。</li>
<li>methods: 我们提出了一种名为MARIO的模型无关的热革命方法，用于改进非标注图谱离散学习方法的OOD泛化性能。 MARIO包括两个原则：信息瓶颈（Information Bottleneck, IB）原则以实现泛化表示，以及不变原则，通过对敏感数据进行对抗数据增强来获得不变表示。</li>
<li>results: 我们通过广泛的实验表明，我们的方法可以在OOD测试集上实现状态之最的性能，而与现有方法相比，在标注测试集上保持相似的性能。代码可以在 GitHub 上找到：<a target="_blank" rel="noopener" href="https://github.com/ZhuYun97/MARIO%E3%80%82">https://github.com/ZhuYun97/MARIO。</a><details>
<summary>Abstract</summary>
In this work, we investigate the problem of out-of-distribution (OOD) generalization for unsupervised learning methods on graph data. This scenario is particularly challenging because graph neural networks (GNNs) have been shown to be sensitive to distributional shifts, even when labels are available. To address this challenge, we propose a \underline{M}odel-\underline{A}gnostic \underline{R}ecipe for \underline{I}mproving \underline{O}OD generalizability of unsupervised graph contrastive learning methods, which we refer to as MARIO. MARIO introduces two principles aimed at developing distributional-shift-robust graph contrastive methods to overcome the limitations of existing frameworks: (i) Information Bottleneck (IB) principle for achieving generalizable representations and (ii) Invariant principle that incorporates adversarial data augmentation to obtain invariant representations. To the best of our knowledge, this is the first work that investigates the OOD generalization problem of graph contrastive learning, with a specific focus on node-level tasks. Through extensive experiments, we demonstrate that our method achieves state-of-the-art performance on the OOD test set, while maintaining comparable performance on the in-distribution test set when compared to existing approaches. The source code for our method can be found at: https://github.com/ZhuYun97/MARIO
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们研究了无监督学习方法在图数据上的 OUT-OF-DISTRIBUTION（OOD）泛化问题。这种情况特别是挑战性的，因为图神经网络（GNNs）已经被证明是 Distributional Shifts 敏感的，即使标签可用。为了解决这个挑战，我们提出了一种名为 MARIO 的模型无关的照片，用于提高无监督图对比学习方法的 OOD 泛化性。MARIO 包括两个原则：(i) 信息瓶颈（IB）原则，以实现泛化表示，以及(ii) 不变原则，通过对数据进行对抗式数据增强来获得不变表示。根据我们所知，这是首次研究 OOD 泛化问题的图对比学习方法，具体注重节点级任务。通过广泛的实验，我们证明了我们的方法在 OOD 测试集上具有最佳性能，而且与现有方法相比，在同一个测试集上保持了相似的性能。MARIO 的源代码可以在 GitHub 上找到：https://github.com/ZhuYun97/MARIO。
</details></li>
</ul>
<hr>
<h2 id="Parallel-Q-Learning-Scaling-Off-policy-Reinforcement-Learning-under-Massively-Parallel-Simulation"><a href="#Parallel-Q-Learning-Scaling-Off-policy-Reinforcement-Learning-under-Massively-Parallel-Simulation" class="headerlink" title="Parallel $Q$-Learning: Scaling Off-policy Reinforcement Learning under Massively Parallel Simulation"></a>Parallel $Q$-Learning: Scaling Off-policy Reinforcement Learning under Massively Parallel Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12983">http://arxiv.org/abs/2307.12983</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Improbable-AI/pql">https://github.com/Improbable-AI/pql</a></li>
<li>paper_authors: Zechu Li, Tao Chen, Zhang-Wei Hong, Anurag Ajay, Pulkit Agrawal</li>
<li>For: 这篇论文是关于加速复杂任务的强化学习的研究，通过使用大量的训练数据来提高模型的性能。* Methods: 这篇论文使用了Isaac Gym提供的GPU基于的 simulate系统，通过并行收集数据、策略学习和价值学习来提高强化学习的效率。* Results: 这篇论文提出了一种并行$Q$-学习（PQL）方案，可以在几个工作站上并行进行数据收集、策略学习和价值学习，从而提高强化学习的效率。在实验中，PQL方案可以扩展到数以千计的并行环境，并调查了学习速度的重要因素。<details>
<summary>Abstract</summary>
Reinforcement learning is time-consuming for complex tasks due to the need for large amounts of training data. Recent advances in GPU-based simulation, such as Isaac Gym, have sped up data collection thousands of times on a commodity GPU. Most prior works used on-policy methods like PPO due to their simplicity and ease of scaling. Off-policy methods are more data efficient but challenging to scale, resulting in a longer wall-clock training time. This paper presents a Parallel $Q$-Learning (PQL) scheme that outperforms PPO in wall-clock time while maintaining superior sample efficiency of off-policy learning. PQL achieves this by parallelizing data collection, policy learning, and value learning. Different from prior works on distributed off-policy learning, such as Apex, our scheme is designed specifically for massively parallel GPU-based simulation and optimized to work on a single workstation. In experiments, we demonstrate that $Q$-learning can be scaled to \textit{tens of thousands of parallel environments} and investigate important factors affecting learning speed. The code is available at https://github.com/Improbable-AI/pql.
</details>
<details>
<summary>摘要</summary>
强化学习因为复杂任务而需要大量训练数据，Recent advances in GPU-based simulation, such as Isaac Gym, have sped up data collection thousands of times on a commodity GPU. Most prior works used on-policy methods like PPO due to their simplicity and ease of scaling. Off-policy methods are more data efficient but challenging to scale, resulting in a longer wall-clock training time. This paper presents a Parallel $Q$-Learning (PQL) scheme that outperforms PPO in wall-clock time while maintaining superior sample efficiency of off-policy learning. PQL achieves this by parallelizing data collection, policy learning, and value learning. Different from prior works on distributed off-policy learning, such as Apex, our scheme is designed specifically for massively parallel GPU-based simulation and optimized to work on a single workstation. In experiments, we demonstrate that $Q$-learning can be scaled to 万�� nombreux parallel environments and investigate important factors affecting learning speed. The code is available at https://github.com/Improbable-AI/pql.Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and other countries. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="3D-LLM-Injecting-the-3D-World-into-Large-Language-Models"><a href="#3D-LLM-Injecting-the-3D-World-into-Large-Language-Models" class="headerlink" title="3D-LLM: Injecting the 3D World into Large Language Models"></a>3D-LLM: Injecting the 3D World into Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12981">http://arxiv.org/abs/2307.12981</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/UMass-Foundation-Model/3D-LLM">https://github.com/UMass-Foundation-Model/3D-LLM</a></li>
<li>paper_authors: Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, Chuang Gan</li>
<li>for: 这个研究想要尝试在大语言模型（LLM）和视力语言模型（VLM）上插入3D世界，以提高这些模型在多个任务上的表现，包括常识推理。</li>
<li>methods: 这个研究使用了三种提示机制，并利用3D特征提取器从渲染的多视图图像中提取3D特征来训练3D语言模型（3D-LLM）。</li>
<li>results: 研究表明，在ScanQA任务上，该模型的表现比状态正常基eline高出9%的BLEU-1分数。此外，在3D描述、任务组合和3D辅助对话等任务上，该模型也表现出优于2D VLM。 Qualitative例子也显示，该模型可以完成跨度外的任务。<details>
<summary>Abstract</summary>
Large language models (LLMs) and Vision-Language Models (VLMs) have been proven to excel at multiple tasks, such as commonsense reasoning. Powerful as these models can be, they are not grounded in the 3D physical world, which involves richer concepts such as spatial relationships, affordances, physics, layout, and so on. In this work, we propose to inject the 3D world into large language models and introduce a whole new family of 3D-LLMs. Specifically, 3D-LLMs can take 3D point clouds and their features as input and perform a diverse set of 3D-related tasks, including captioning, dense captioning, 3D question answering, task decomposition, 3D grounding, 3D-assisted dialog, navigation, and so on. Using three types of prompting mechanisms that we design, we are able to collect over 300k 3D-language data covering these tasks. To efficiently train 3D-LLMs, we first utilize a 3D feature extractor that obtains 3D features from rendered multi- view images. Then, we use 2D VLMs as our backbones to train our 3D-LLMs. By introducing a 3D localization mechanism, 3D-LLMs can better capture 3D spatial information. Experiments on ScanQA show that our model outperforms state-of-the-art baselines by a large margin (e.g., the BLEU-1 score surpasses state-of-the-art score by 9%). Furthermore, experiments on our held-in datasets for 3D captioning, task composition, and 3D-assisted dialogue show that our model outperforms 2D VLMs. Qualitative examples also show that our model could perform more tasks beyond the scope of existing LLMs and VLMs. Project Page: : https://vis-www.cs.umass.edu/3dllm/.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）和视觉语言模型（VLM）已经被证明可以在多个任务上表现出色，如常识 reasoning。强大的这些模型可以是，它们没有与3D物理世界相关的概念，包括空间关系、可用性、物理学、布局等。在这项工作中，我们提议将3D世界注入到大型语言模型中，并引入一个全新的3D-LLM家族。specifically，3D-LLM可以从3D点云和其特征输入，并完成一系列3D相关任务，包括captioning、dense captioning、3D问答、任务分解、3D定位、3D-assisted dialog、导航等。通过我们设计的三种提示机制，我们能够收集超过300k个3D语言数据覆盖这些任务。为了效率地训练3D-LLM，我们首先利用3D特征提取器从渲染多视图图像中获取3D特征。然后，我们使用2D VLM作为我们的背部来训练我们的3D-LLM。通过引入3D本地化机制，3D-LLM可以更好地捕捉3D空间信息。在ScanQA上进行实验，我们的模型比州态艺术基eline的基eline高出大幅度（例如，BLEU-1分数超过州态艺术基eline的分数 by 9%）。此外，在我们保留的数据集上进行3D captioning、任务组合和3D-assisted dialogue的实验中，我们的模型超过2D VLM。Qualitative例子也表明我们的模型可以完成现有LLM和VLM的任务之外的更多任务。项目页面：https://vis-www.cs.umass.edu/3dllm/.
</details></li>
</ul>
<hr>
<h2 id="An-Isometric-Stochastic-Optimizer"><a href="#An-Isometric-Stochastic-Optimizer" class="headerlink" title="An Isometric Stochastic Optimizer"></a>An Isometric Stochastic Optimizer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12979">http://arxiv.org/abs/2307.12979</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jacob Jackson</li>
<li>for: 这 paper 的目的是解释 Adam 优化器的成功，并基于这个原理提出一种新的优化器。</li>
<li>methods: 这 paper 使用了一种新的优化器 called Iso，它使每个参数的步长独立于其他参数的 нор。 Additionally, the paper proposes a variant of Iso called IsoAdam, which allows for the transfer of optimal hyperparameters from Adam.</li>
<li>results: 实验结果表明，IsoAdam 在训练一个小型 Transformer 时比 Adam 快速。<details>
<summary>Abstract</summary>
The Adam optimizer is the standard choice in deep learning applications. I propose a simple explanation of Adam's success: it makes each parameter's step size independent of the norms of the other parameters. Based on this principle I derive Iso, a new optimizer which makes the norm of a parameter's update invariant to the application of any linear transformation to its inputs and outputs. I develop a variant of Iso called IsoAdam that allows optimal hyperparameters to be transferred from Adam, and demonstrate that IsoAdam obtains a speedup over Adam when training a small Transformer.
</details>
<details>
<summary>摘要</summary>
《Adam优化器在深度学习应用中是标准选择。我提出了对Adam成功的简单解释：它使每个参数的步长独立于其他参数的norm。基于这个原理，我 derivated一种新的优化器叫做Iso，它使参数更新的 нор免受输入和输出的任何线性变换的影响。我还开发了一种名为IsoAdam的变体，它允许从Adam中传输优化参数，并证明IsoAdam在训练小型Transformer时比Adam快。》Note that Simplified Chinese is used here, which is one of the two standard forms of Chinese writing. Traditional Chinese is the other form, and it may be used in different regions or contexts.
</details></li>
</ul>
<hr>
<h2 id="Provable-Benefits-of-Policy-Learning-from-Human-Preferences-in-Contextual-Bandit-Problems"><a href="#Provable-Benefits-of-Policy-Learning-from-Human-Preferences-in-Contextual-Bandit-Problems" class="headerlink" title="Provable Benefits of Policy Learning from Human Preferences in Contextual Bandit Problems"></a>Provable Benefits of Policy Learning from Human Preferences in Contextual Bandit Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12975">http://arxiv.org/abs/2307.12975</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Ji, Huazheng Wang, Minshuo Chen, Tuo Zhao, Mengdi Wang</li>
<li>for: 这个论文是关于决策问题中的奖励工程。在实际应用中，常有无明显的奖励函数选择的情况，因此引入人工反馈以帮助学习奖励函数的方法得到了广泛应用。</li>
<li>methods: 这个论文使用了人工反馈来学习奖励函数，并且提出了一种基于偏好的方法，该方法在recent empirical applications such as InstructGPT中表现出色。</li>
<li>results: 论文提供了一种理论分析，证明了基于偏好的方法在offline上下文ual bandits中的优势。具体来说，论文提高了运行policy learning方法的人工分类样本的模型和下界分析，并与基于偏好的方法的下界比较，证明了基于偏好的方法具有较低的下界。<details>
<summary>Abstract</summary>
A crucial task in decision-making problems is reward engineering. It is common in practice that no obvious choice of reward function exists. Thus, a popular approach is to introduce human feedback during training and leverage such feedback to learn a reward function. Among all policy learning methods that use human feedback, preference-based methods have demonstrated substantial success in recent empirical applications such as InstructGPT. In this work, we develop a theory that provably shows the benefits of preference-based methods in offline contextual bandits. In particular, we improve the modeling and suboptimality analysis for running policy learning methods on human-scored samples directly. Then, we compare it with the suboptimality guarantees of preference-based methods and show that preference-based methods enjoy lower suboptimality.
</details>
<details>
<summary>摘要</summary>
决策问题中一项非常重要的任务是奖励工程。在实践中，不存在明显的奖励函数选择。因此，一种受欢迎的方法是在训练过程中引入人类反馈，并使用这些反馈学习一个奖励函数。在所有基于策略学习方法中使用人类反馈的方法中，偏好基于方法在最近的实际应用中，如InstructGPT，已经实现了显著的成功。在这项工作中，我们发展了一种理论，证明了偏好基于方法在线上上下文带动机中的优点。具体来说，我们改进了运行策略学习方法直接使用人类评分样本的模型和下optimality分析。然后，我们与偏好基于方法的下optimality保证进行比较，并证明偏好基于方法在下optimality方面具有更低的下optimality。
</details></li>
</ul>
<hr>
<h2 id="Big-Data-Supply-Chain-Management-Framework-for-Forecasting-Data-Preprocessing-and-Machine-Learning-Techniques"><a href="#Big-Data-Supply-Chain-Management-Framework-for-Forecasting-Data-Preprocessing-and-Machine-Learning-Techniques" class="headerlink" title="Big Data - Supply Chain Management Framework for Forecasting: Data Preprocessing and Machine Learning Techniques"></a>Big Data - Supply Chain Management Framework for Forecasting: Data Preprocessing and Machine Learning Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12971">http://arxiv.org/abs/2307.12971</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zeniSoida/pl1">https://github.com/zeniSoida/pl1</a></li>
<li>paper_authors: Md Abrar Jahin, Md Sakib Hossain Shovon, Jungpil Shin, Istiyaque Ahmed Ridoy, Yoichi Tomioka, M. F. Mridha</li>
<li>For: This paper aims to systematically identify and comparatively analyze state-of-the-art supply chain (SC) forecasting strategies and technologies, and to propose a novel framework incorporating Big Data Analytics in SC Management.* Methods: The proposed framework includes problem identification, data sources, exploratory data analysis, machine-learning model training, hyperparameter tuning, performance evaluation, and optimization. The paper discusses the need for different types of forecasting according to the period or SC objective, and recommends SC KPIs and error-measurement systems to optimize the top-performing model.* Results: The paper illustrates the adverse effects of phantom inventory on forecasting and the dependence of managerial decisions on the SC KPIs for determining model performance parameters and improving operations management, transparency, and planning efficiency. The cyclic connection within the framework introduces preprocessing optimization based on the post-process KPIs, optimizing the overall control process (inventory management, workforce determination, cost, production and capacity planning).Here are the three points in Simplified Chinese text:* For: 这篇论文目标是系统地检查和比较现有的供应链（SC）预测策略和技术，并提出一种新的框架，将大数据分析integrated into SC Management。* Methods: 该提案的框架包括问题识别、数据来源、探索数据分析、机器学习模型训练、 гипер参数调整、性能评估和优化。 paper discusses the need for different types of forecasting according to the period or SC objective, and recommends SC KPIs and error-measurement systems to optimize the top-performing model.* Results: paper illustrates the adverse effects of phantom inventory on forecasting and the dependence of managerial decisions on the SC KPIs for determining model performance parameters and improving operations management, transparency, and planning efficiency. The cyclic connection within the framework introduces preprocessing optimization based on the post-process KPIs, optimizing the overall control process (inventory management, workforce determination, cost, production and capacity planning).<details>
<summary>Abstract</summary>
This article intends to systematically identify and comparatively analyze state-of-the-art supply chain (SC) forecasting strategies and technologies. A novel framework has been proposed incorporating Big Data Analytics in SC Management (problem identification, data sources, exploratory data analysis, machine-learning model training, hyperparameter tuning, performance evaluation, and optimization), forecasting effects on human-workforce, inventory, and overall SC. Initially, the need to collect data according to SC strategy and how to collect them has been discussed. The article discusses the need for different types of forecasting according to the period or SC objective. The SC KPIs and the error-measurement systems have been recommended to optimize the top-performing model. The adverse effects of phantom inventory on forecasting and the dependence of managerial decisions on the SC KPIs for determining model performance parameters and improving operations management, transparency, and planning efficiency have been illustrated. The cyclic connection within the framework introduces preprocessing optimization based on the post-process KPIs, optimizing the overall control process (inventory management, workforce determination, cost, production and capacity planning). The contribution of this research lies in the standard SC process framework proposal, recommended forecasting data analysis, forecasting effects on SC performance, machine learning algorithms optimization followed, and in shedding light on future research.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇文章的目的是系统地找出当前最佳实践的供应链（SC）预测策略和技术，并提出一种新的框架，该框架包括在SC管理中使用大数据分析。文章讨论了SC预测的数据收集方式和不同类型的预测，以及适用于不同的时间间隔或SC目标。文章还建议了SC指标和错误度量系统，以便优化最佳模型。文章还描述了预测对人工资源、存储和整体SC的影响，以及管理决策的依赖于SC指标以确定模型性能参数和改善运营管理、透明度和规划效率。文章还提出了一种循环连接的框架，该框架包括根据后处理指标进行预处理优化，以及供应链管理、人力决策、成本、生产和容量规划。本文的贡献在于提出了标准SC过程框架、预测数据分析、预测对SC性能的影响、机器学习算法优化和未来研究方向。
</details></li>
</ul>
<hr>
<h2 id="A-Connection-between-One-Step-Regularization-and-Critic-Regularization-in-Reinforcement-Learning"><a href="#A-Connection-between-One-Step-Regularization-and-Critic-Regularization-in-Reinforcement-Learning" class="headerlink" title="A Connection between One-Step Regularization and Critic Regularization in Reinforcement Learning"></a>A Connection between One-Step Regularization and Critic Regularization in Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12968">http://arxiv.org/abs/2307.12968</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ben-eysenbach/ac-connection">https://github.com/ben-eysenbach/ac-connection</a></li>
<li>paper_authors: Benjamin Eysenbach, Matthieu Geist, Sergey Levine, Ruslan Salakhutdinov</li>
<li>for: 这 paper 的目的是解释一些关于 Offline Reinforcement Learning 的问题，包括如何使用一步或多步的策略改进来避免过拟合。</li>
<li>methods: 这 paper 使用了一些不同的方法来进行策略改进，包括 advantage-weighted regression 和 conditional behavioral cloning。它们的主要区别在于，一步方法会在一步的策略改进后停止，而多步方法会通过多个步骤来进行策略改进。</li>
<li>results: 这 paper 的实验结果表明，一步 RL 可以与多步 RL 相比肩，但是它们在不同的问题上的表现可能不同。具体来说，一步 RL 在需要强regularization的问题上可能更 Competitive。<details>
<summary>Abstract</summary>
As with any machine learning problem with limited data, effective offline RL algorithms require careful regularization to avoid overfitting. One-step methods perform regularization by doing just a single step of policy improvement, while critic regularization methods do many steps of policy improvement with a regularized objective. These methods appear distinct. One-step methods, such as advantage-weighted regression and conditional behavioral cloning, truncate policy iteration after just one step. This ``early stopping'' makes one-step RL simple and stable, but can limit its asymptotic performance. Critic regularization typically requires more compute but has appealing lower-bound guarantees. In this paper, we draw a close connection between these methods: applying a multi-step critic regularization method with a regularization coefficient of 1 yields the same policy as one-step RL. While practical implementations violate our assumptions and critic regularization is typically applied with smaller regularization coefficients, our experiments nevertheless show that our analysis makes accurate, testable predictions about practical offline RL methods (CQL and one-step RL) with commonly-used hyperparameters. Our results that every problem can be solved with a single step of policy improvement, but rather that one-step RL might be competitive with critic regularization on RL problems that demand strong regularization.
</details>
<details>
<summary>摘要</summary>
如果机器学习问题受限于数据，有效的离线RL算法需要谨慎的补偿来避免过拟合。一步方法通过做一步策略改进来实现补偿，而批评补偿方法则通过多个步骤的策略改进来实现补偿。这些方法看起来很不同。一步方法，如优点权重回归和Conditional Behavioral Cloning，在策略迭代后 truncate 策略迭代。这个``早期停止''使一步RL简单和稳定，但可能限制其极限性表现。批评补偿通常需要更多的计算，但它具有吸引人的下界保证。在这篇论文中，我们 Draw a close connection between these methods：在应用多步批评补偿方法时，使用补偿系数为1就可以获得与一步RL相同的策略。虽然实际实现可能违反我们的假设，但我们的实验表明，我们的分析可以准确预测实际的离线RL方法（CQL和一步RL）在通用的 гиперпараметры下的表现。我们的结果表明，每个问题都可以通过一步策略改进来解决，但是一步RL可能与批评补偿在RL问题中具有强补偿需求的问题竞争。
</details></li>
</ul>
<hr>
<h2 id="Learning-Dense-Correspondences-between-Photos-and-Sketches"><a href="#Learning-Dense-Correspondences-between-Photos-and-Sketches" class="headerlink" title="Learning Dense Correspondences between Photos and Sketches"></a>Learning Dense Correspondences between Photos and Sketches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12967">http://arxiv.org/abs/2307.12967</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cogtoolslab/photo-sketch-correspondence">https://github.com/cogtoolslab/photo-sketch-correspondence</a></li>
<li>paper_authors: Xuanchen Lu, Xiaolong Wang, Judith E Fan</li>
<li>for: 这个论文旨在研究计算机系统是如何模拟人类的简图理解能力？</li>
<li>methods: 作者提出了一种基于自我超vised学习的方法，使用尺寸变换网络来估计简图和照片之间的匹配关系。</li>
<li>results: 研究发现，该方法可以超过多个强化基elines，并且与其他抽象级别的方法相比，具有更高的准确率。然而，研究还发现了人类和机器系统之间的差异，提示了更多的研究空间。<details>
<summary>Abstract</summary>
Humans effortlessly grasp the connection between sketches and real-world objects, even when these sketches are far from realistic. Moreover, human sketch understanding goes beyond categorization -- critically, it also entails understanding how individual elements within a sketch correspond to parts of the physical world it represents. What are the computational ingredients needed to support this ability? Towards answering this question, we make two contributions: first, we introduce a new sketch-photo correspondence benchmark, $\textit{PSC6k}$, containing 150K annotations of 6250 sketch-photo pairs across 125 object categories, augmenting the existing Sketchy dataset with fine-grained correspondence metadata. Second, we propose a self-supervised method for learning dense correspondences between sketch-photo pairs, building upon recent advances in correspondence learning for pairs of photos. Our model uses a spatial transformer network to estimate the warp flow between latent representations of a sketch and photo extracted by a contrastive learning-based ConvNet backbone. We found that this approach outperformed several strong baselines and produced predictions that were quantitatively consistent with other warp-based methods. However, our benchmark also revealed systematic differences between predictions of the suite of models we tested and those of humans. Taken together, our work suggests a promising path towards developing artificial systems that achieve more human-like understanding of visual images at different levels of abstraction. Project page: https://photo-sketch-correspondence.github.io
</details>
<details>
<summary>摘要</summary>
人类很自然地理解绘图和现实世界之间的连接，即使绘图非常不真实。此外，人类绘图理解不仅是分类，更重要的是理解绘图中的各个元素与物理世界中的部分之间的对应关系。为解答这个问题，我们提出了两个贡献：首先，我们引入了一个新的绘图-照片对应 bencmark，称为 $\textit{PSC6k}$，包含150万个绘图-照片对应的注释，其中有6250个绘图和125种物品类别。其次，我们提出了一种自动学习的方法，用于学习绘图-照片对应的密集对应关系，基于近期对照片对应学习的进步。我们的模型使用一个空间变换网络来估算绘图和照片的卷积流，从而学习绘图中的各个元素与物理世界中的部分之间的对应关系。我们发现，这种方法比许多强大的基elines表现出色，并且生成的预测值与其他旋转基elines的预测值是量化一致的。然而，我们的benchmark还发现了模型预测值与人类预测值之间的系统性差异。总之，我们的工作建议了一种可能的方法，可以开发出更加人类化的视觉图像理解系统，以达到不同层次的抽象水平。项目页面：https://photo-sketch-correspondence.github.io
</details></li>
</ul>
<hr>
<h2 id="Synthetic-pre-training-for-neural-network-interatomic-potentials"><a href="#Synthetic-pre-training-for-neural-network-interatomic-potentials" class="headerlink" title="Synthetic pre-training for neural-network interatomic potentials"></a>Synthetic pre-training for neural-network interatomic potentials</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15714">http://arxiv.org/abs/2307.15714</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jla-gardner/nnp-pre-training">https://github.com/jla-gardner/nnp-pre-training</a></li>
<li>paper_authors: John L. A. Gardner, Kathryn T. Baker, Volker L. Deringer</li>
<li>for: 该研究旨在提高atomistic materials模型中ml基于potential的精度和稳定性，通过使用“synthetic”数据进行预训练。</li>
<li>methods: 研究使用了一种基于图 neural network的equivariant graph-neural-network potential，并通过大量生成的synthetic数据进行预训练，然后 fine-tune 到一个较小的量子力学参考数据集。</li>
<li>results: 研究发现，通过使用synthetic数据进行预训练，可以提高模型的精度和稳定性，并且可以避免一些由量子力学参考数据集的限制。<details>
<summary>Abstract</summary>
Machine learning (ML) based interatomic potentials have transformed the field of atomistic materials modelling. However, ML potentials depend critically on the quality and quantity of quantum-mechanical reference data with which they are trained, and therefore developing datasets and training pipelines is becoming an increasingly central challenge. Leveraging the idea of "synthetic" (artificial) data that is common in other areas of ML research, we here show that synthetic atomistic data, themselves obtained at scale with an existing ML potential, constitute a useful pre-training task for neural-network interatomic potential models. Once pre-trained with a large synthetic dataset, these models can be fine-tuned on a much smaller, quantum-mechanical one, improving numerical accuracy and stability in computational practice. We demonstrate feasibility for a series of equivariant graph-neural-network potentials for carbon, and we carry out initial experiments to test the limits of the approach.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Efficiently-Sampling-the-PSD-Cone-with-the-Metric-Dikin-Walk"><a href="#Efficiently-Sampling-the-PSD-Cone-with-the-Metric-Dikin-Walk" class="headerlink" title="Efficiently Sampling the PSD Cone with the Metric Dikin Walk"></a>Efficiently Sampling the PSD Cone with the Metric Dikin Walk</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12943">http://arxiv.org/abs/2307.12943</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunbum Kook, Santosh S. Vempala</li>
<li>for: 这篇论文关注于半定义程序的效率计算 frontier。</li>
<li>methods: 论文使用了Dikin walk和相关的metric的概念，并提出了一种新的metric的选择方法，以提高混合时间和每步复杂度。</li>
<li>results: 论文的结果表明，使用这种新的metric和混合方法可以大幅降低混合时间和每步复杂度，并且可以将依赖于约束的数量限制为多少。<details>
<summary>Abstract</summary>
Semi-definite programs represent a frontier of efficient computation. While there has been much progress on semi-definite optimization, with moderate-sized instances currently solvable in practice by the interior-point method, the basic problem of sampling semi-definite solutions remains a formidable challenge. The direct application of known polynomial-time algorithms for sampling general convex bodies to semi-definite sampling leads to a prohibitively high running time. In addition, known general methods require an expensive rounding phase as pre-processing. Here we analyze the Dikin walk, by first adapting it to general metrics, then devising suitable metrics for the PSD cone with affine constraints. The resulting mixing time and per-step complexity are considerably smaller, and by an appropriate choice of the metric, the dependence on the number of constraints can be made polylogarithmic. We introduce a refined notion of self-concordant matrix functions and give rules for combining different metrics. Along the way, we further develop the theory of interior-point methods for sampling.
</details>
<details>
<summary>摘要</summary>
semi-definite 计划表示一种高效计算的前沿。虽然有很多进步在半定义优化方面，但基本的半定义解析问题仍然是一项挑战。直接将通用 convex 体的算法应用到半定义 sampling 中会导致非常高的运行时间。此外，已知的通用方法需要费时的舒缩阶段作为先决条件。我们分析了 Dikin 步行，首先适应到通用度量，然后适应 PSD  cone 中的 affine 约束。得到的混合时间和每步复杂度较小，并且通过适当的度量选择，对数量约束的依赖可以被polylogarithmic。我们引入了自适应矩阵函数的更加细化的定义，并给出了不同度量的结合规则。在过程中，我们进一步发展了内点方法的 sampling 理论。
</details></li>
</ul>
<hr>
<h2 id="On-Privileged-and-Convergent-Bases-in-Neural-Network-Representations"><a href="#On-Privileged-and-Convergent-Bases-in-Neural-Network-Representations" class="headerlink" title="On Privileged and Convergent Bases in Neural Network Representations"></a>On Privileged and Convergent Bases in Neural Network Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12941">http://arxiv.org/abs/2307.12941</a></li>
<li>repo_url: None</li>
<li>paper_authors: Davis Brown, Nikhil Vyas, Yamini Bansal</li>
<li>for: 本研究探讨了神经网络学习的表示学习是否具有特权和共同基准。Specifically, we examine the significance of feature directions represented by individual neurons.</li>
<li>methods: 我们使用arbitrary rotations of neural representations来检验神经网络的特权性。我们还比较了由不同随机初始化生成的神经网络的基准。</li>
<li>results: 我们的研究发现，神经网络不 converge to a unique basis，而且基准相关性在各层神经网络中增加了 significannotly。Linear Mode Connectivity也与网络宽度相关，但这种相关性不是由基准相关性增加的。<details>
<summary>Abstract</summary>
In this study, we investigate whether the representations learned by neural networks possess a privileged and convergent basis. Specifically, we examine the significance of feature directions represented by individual neurons. First, we establish that arbitrary rotations of neural representations cannot be inverted (unlike linear networks), indicating that they do not exhibit complete rotational invariance. Subsequently, we explore the possibility of multiple bases achieving identical performance. To do this, we compare the bases of networks trained with the same parameters but with varying random initializations. Our study reveals two findings: (1) Even in wide networks such as WideResNets, neural networks do not converge to a unique basis; (2) Basis correlation increases significantly when a few early layers of the network are frozen identically.   Furthermore, we analyze Linear Mode Connectivity, which has been studied as a measure of basis correlation. Our findings give evidence that while Linear Mode Connectivity improves with increased network width, this improvement is not due to an increase in basis correlation.
</details>
<details>
<summary>摘要</summary>
在本研究中，我们研究神经网络学习的表示方式是否具有特权和归一化的基准。我们专门研究神经元个体表达的特征方向的重要性。首先，我们证明神经网络中的表示不能被逆转（不同于线性网络），这表明它们不具有完全的旋转不变性。接着，我们探索是否存在多个基准可以达到同样的性能。为此，我们比较具有相同参数但具有不同随机初始化的网络的基准。我们的研究发现了两个结论：（1）甚至在宽度较大的网络如WideResNets中，神经网络并不会 converges to a unique basis;（2）基准相关性在冻结某些早期层时明显增加。此外，我们分析了Linear Mode Connectivity，这是一种基准相关性的度量。我们的发现表明，Linear Mode Connectivity在网络宽度增加时会提高，但这并不是基准相关性的提高。
</details></li>
</ul>
<hr>
<h2 id="HOOD-Real-Time-Robust-Human-Presence-and-Out-of-Distribution-Detection-with-Low-Cost-FMCW-Radar"><a href="#HOOD-Real-Time-Robust-Human-Presence-and-Out-of-Distribution-Detection-with-Low-Cost-FMCW-Radar" class="headerlink" title="HOOD: Real-Time Robust Human Presence and Out-of-Distribution Detection with Low-Cost FMCW Radar"></a>HOOD: Real-Time Robust Human Presence and Out-of-Distribution Detection with Low-Cost FMCW Radar</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02396">http://arxiv.org/abs/2308.02396</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sabri Mustafa Kahya, Muhammet Sami Yavuz, Eckehard Steinbach</li>
<li>For: 这种研究旨在实现indoor环境中的人员存在检测，使用60GHz短距离FMCW雷达，并提供一个实时Robust人员存在和非典型检测方法（HOOD）。* Methods: 该方法基于重构建立 architecture，使用60GHz短距离FMCW雷达生成 macro和微距离Doppler图像（RDIs），并通过对RDIs的重构来实现人员存在和非典型检测。* Results: 在基于60GHz短距离FMCW雷达的数据集上，HOOD方法实现了94.36%的平均AUROC水平，并在不同的人类场景下表现良好。此外，HOOD方法也在常见的OOD检测指标上表现出excel。实际实验结果可以在以下链接中找到：<a target="_blank" rel="noopener" href="https://muskahya.github.io/HOOD">https://muskahya.github.io/HOOD</a><details>
<summary>Abstract</summary>
Human presence detection in indoor environments using millimeter-wave frequency-modulated continuous-wave (FMCW) radar is challenging due to the presence of moving and stationary clutters in indoor places. This work proposes "HOOD" as a real-time robust human presence and out-of-distribution (OOD) detection method by exploiting 60 GHz short-range FMCW radar. We approach the presence detection application as an OOD detection problem and solve the two problems simultaneously using a single pipeline. Our solution relies on a reconstruction-based architecture and works with radar macro and micro range-Doppler images (RDIs). HOOD aims to accurately detect the "presence" of humans in the presence or absence of moving and stationary disturbers. Since it is also an OOD detector, it aims to detect moving or stationary clutters as OOD in humans' absence and predicts the current scene's output as "no presence." HOOD is an activity-free approach that performs well in different human scenarios. On our dataset collected with a 60 GHz short-range FMCW Radar, we achieve an average AUROC of 94.36%. Additionally, our extensive evaluations and experiments demonstrate that HOOD outperforms state-of-the-art (SOTA) OOD detection methods in terms of common OOD detection metrics. Our real-time experiments are available at: https://muskahya.github.io/HOOD
</details>
<details>
<summary>摘要</summary>
人体存在检测在室内环境中使用毫米波频率调制连续波（FMCW）雷达是具有挑战性，原因在于室内的移动和静止干扰物。这项工作提出了“HOOD”实时可靠人体存在和非典型检测方法，通过利用60GHz短距离FMCW雷达。我们将存在检测应用作为非典型检测问题，并同时解决两个问题使用单一管道。我们的解决方案基于重建建筑，并与雷达macro和微范围Doppler图像（RDI）结合使用。HOOD hoped to accurately detect human presence in the presence or absence of moving and stationary disturbances. Since it is also an OOD detector, it aims to detect moving or stationary clutters as OOD in humans' absence and predicts the current scene's output as "no presence." HOOD is an activity-free approach that performs well in different human scenarios. On our dataset collected with a 60 GHz short-range FMCW Radar, we achieve an average AUROC of 94.36%. Additionally, our extensive evaluations and experiments demonstrate that HOOD outperforms state-of-the-art (SOTA) OOD detection methods in terms of common OOD detection metrics. Our real-time experiments are available at: https://muskahya.github.io/HOOD.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Contextual-Bandits-and-Imitation-Learning-via-Preference-Based-Active-Queries"><a href="#Contextual-Bandits-and-Imitation-Learning-via-Preference-Based-Active-Queries" class="headerlink" title="Contextual Bandits and Imitation Learning via Preference-Based Active Queries"></a>Contextual Bandits and Imitation Learning via Preference-Based Active Queries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12926">http://arxiv.org/abs/2307.12926</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ayush Sekhari, Karthik Sridharan, Wen Sun, Runzhe Wu</li>
<li>for: 本文研究了 Contextual Bandits 和模仿学习问题，learner 缺乏直接行动的奖励信息，而是可以在每个回合 queries 专家，获得偏好反馈。learner 的目标是 minimize 执行动的尴尬吗，同时 minimize  queries 的数量。</li>
<li>methods: 本文提出了一种 Algorithm ，利用在线回归 oracle 来选择行动和决定是否 queries。该 Algorithm 基于函数类型，可以在适当的链接函数下表示专家的偏好模型。</li>
<li>results: 本文证明了该 Algorithm 在 Contextual Bandits 设置下可以获得 $O(\min{\sqrt{T}, d&#x2F;\Delta})$ 的尴尬 regret bound，其中 $T$ 是互动次数，$d$ 是函数类型的 eluder 维度，$\Delta$ 是最佳动作对所有上下文的最小偏好。此外，该 Algorithm 只需要 $O(\min{T, d^2&#x2F;\Delta^2})$ 个 queries。在 imitation learning 设置下，本文也提出了一种 Algorithm，并证明了其在 regret 和 queries 上有类似的 guarantees。 interessingly，该 Algorithm 可以在专家不优秀时，even learn to outperform the underlying expert，这表明了 preference-based feedback 在 imitation learning 中的实际优势。<details>
<summary>Abstract</summary>
We consider the problem of contextual bandits and imitation learning, where the learner lacks direct knowledge of the executed action's reward. Instead, the learner can actively query an expert at each round to compare two actions and receive noisy preference feedback. The learner's objective is two-fold: to minimize the regret associated with the executed actions, while simultaneously, minimizing the number of comparison queries made to the expert. In this paper, we assume that the learner has access to a function class that can represent the expert's preference model under appropriate link functions, and provide an algorithm that leverages an online regression oracle with respect to this function class for choosing its actions and deciding when to query. For the contextual bandit setting, our algorithm achieves a regret bound that combines the best of both worlds, scaling as $O(\min\{\sqrt{T}, d/\Delta\})$, where $T$ represents the number of interactions, $d$ represents the eluder dimension of the function class, and $\Delta$ represents the minimum preference of the optimal action over any suboptimal action under all contexts. Our algorithm does not require the knowledge of $\Delta$, and the obtained regret bound is comparable to what can be achieved in the standard contextual bandits setting where the learner observes reward signals at each round. Additionally, our algorithm makes only $O(\min\{T, d^2/\Delta^2\})$ queries to the expert. We then extend our algorithm to the imitation learning setting, where the learning agent engages with an unknown environment in episodes of length $H$ each, and provide similar guarantees for regret and query complexity. Interestingly, our algorithm for imitation learning can even learn to outperform the underlying expert, when it is suboptimal, highlighting a practical benefit of preference-based feedback in imitation learning.
</details>
<details>
<summary>摘要</summary>
我们考虑了上下文强化策略和模仿学习问题，learner缺乏直接行动的奖励信息。相反，learner可以在每个回合中活动地询问专家， compare two actions 并获得受损的偏好反馈。learner的目标是二元的：一方面，避免 Executed 动作的后悔，另一方面，避免向专家提问。在这篇文章中，我们假设learner有Function class的存在，可以表示专家的偏好模型，并提供了一个给予online regression oracle 的算法，用于选择动作和决定当 query。 For 上下文强化策略设定，我们的算法可以获得 $O(\min\{\sqrt{T}, d/\Delta\})$ 的后悔 bound，where $T$ represents the number of interactions, $d$ represents the eluder dimension of the function class, and $\Delta$ represents the minimum preference of the optimal action over any suboptimal action under all contexts。我们的算法不需要知道 $\Delta$，而且的后悔 bound 与标准上下文强化策略设定，where the learner observes reward signals at each round,相同。此外，我们的算法仅需要 $O(\min\{T, d^2/\Delta^2\})$ 问题给专家。然后，我们延伸我们的算法到模仿学习设定，learner在每个回合中与未知环境进行交互，并提供了相似的后悔和问题复杂性 guarantee。有趣的是，我们的算法可以在模仿学习设定中learn to outperform the underlying expert，当专家是不良的时候，显示了偏好反馈在模仿学习中的实际优点。
</details></li>
</ul>
<hr>
<h2 id="A-new-derivative-free-optimization-method-Gaussian-Crunching-Search"><a href="#A-new-derivative-free-optimization-method-Gaussian-Crunching-Search" class="headerlink" title="A new derivative-free optimization method: Gaussian Crunching Search"></a>A new derivative-free optimization method: Gaussian Crunching Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14359">http://arxiv.org/abs/2307.14359</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benny Wong</li>
<li>for: 这个研究论文是为了探讨一种新的优化方法—— Gaussian Crunching Search（GCS），以及其在不同领域中的应用。</li>
<li>methods: 本研究使用 Gaussian Crunching Search（GCS）方法，启发自狄拉克分布中的粒子行为，旨在高效地探索解决空间并趋向于全局最优点。</li>
<li>results: 通过实验评估和与现有优化方法比较，本研究展示了 GCS 方法的优势和特点。这篇研究论文对于优化方法的研究和实践者都是一个有价值的资源。<details>
<summary>Abstract</summary>
Optimization methods are essential in solving complex problems across various domains. In this research paper, we introduce a novel optimization method called Gaussian Crunching Search (GCS). Inspired by the behaviour of particles in a Gaussian distribution, GCS aims to efficiently explore the solution space and converge towards the global optimum. We present a comprehensive analysis of GCS, including its working mechanism, and potential applications. Through experimental evaluations and comparisons with existing optimization methods, we highlight the advantages and strengths of GCS. This research paper serves as a valuable resource for researchers, practitioners, and students interested in optimization, providing insights into the development and potential of Gaussian Crunching Search as a new and promising approach.
</details>
<details>
<summary>摘要</summary>
优化方法是解决复杂问题的关键，在不同领域都有广泛应用。本研究论文介绍一种新的优化方法——高斯压缩搜索（GCS）。该方法 draws inspiration from高斯分布中粒子的行为，旨在效率地探索解决空间并趋向于全球最优点。我们对GCS进行了全面的分析，包括它的工作机制和潜在应用。通过实验评估和现有优化方法的比较，我们提出了GCS的优势和特点。这篇研究论文对优化领域的研究人员、实践者和学生都是一种有价值的资源，为他们提供了GCS的开发和潜力的深入了解。
</details></li>
</ul>
<hr>
<h2 id="Graph-Neural-Networks-For-Mapping-Variables-Between-Programs-–-Extended-Version"><a href="#Graph-Neural-Networks-For-Mapping-Variables-Between-Programs-–-Extended-Version" class="headerlink" title="Graph Neural Networks For Mapping Variables Between Programs – Extended Version"></a>Graph Neural Networks For Mapping Variables Between Programs – Extended Version</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13014">http://arxiv.org/abs/2307.13014</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pmorvalho/ecai23-gnns-for-mapping-variables-between-programs">https://github.com/pmorvalho/ecai23-gnns-for-mapping-variables-between-programs</a></li>
<li>paper_authors: Pedro Orvalho, Jelle Piepenbrock, Mikoláš Janota, Vasco Manquinho</li>
<li>for: 本研究旨在提出一种基于图神经网络（GNN）的变量映射方法，用于比较两个程序的变量集。</li>
<li>methods: 本研究使用GNN来映射两个程序的抽象 sintaxis树（AST）中的变量集。</li>
<li>results: 实验结果表明，我们的方法可以正确地映射83%的评估数据集中的变量。此外，当前领先的程序修复方法仅能修复约72%的错误程序，而我们的方法则可以修复约88.5%的错误程序。<details>
<summary>Abstract</summary>
Automated program analysis is a pivotal research domain in many areas of Computer Science -- Formal Methods and Artificial Intelligence, in particular. Due to the undecidability of the problem of program equivalence, comparing two programs is highly challenging. Typically, in order to compare two programs, a relation between both programs' sets of variables is required. Thus, mapping variables between two programs is useful for a panoply of tasks such as program equivalence, program analysis, program repair, and clone detection. In this work, we propose using graph neural networks (GNNs) to map the set of variables between two programs based on both programs' abstract syntax trees (ASTs). To demonstrate the strength of variable mappings, we present three use-cases of these mappings on the task of program repair to fix well-studied and recurrent bugs among novice programmers in introductory programming assignments (IPAs). Experimental results on a dataset of 4166 pairs of incorrect/correct programs show that our approach correctly maps 83% of the evaluation dataset. Moreover, our experiments show that the current state-of-the-art on program repair, greatly dependent on the programs' structure, can only repair about 72% of the incorrect programs. In contrast, our approach, which is solely based on variable mappings, can repair around 88.5%.
</details>
<details>
<summary>摘要</summary>
自动化程序分析是计算机科学多个领域的关键研究领域之一，尤其是形式方法和人工智能。由于程序等价问题是不可解决的，因此比较两个程序很困难。通常，为了比较两个程序，需要在两个程序中变量集的关系。因此，将变量 между两个程序映射到相同的空间是有用的，可以用于许多任务，如程序等价、程序分析、程序修复和冲击检测。在这种工作中，我们提出使用图 neural network（GNN）将两个程序的抽象语法树（AST）中的变量集映射到相同的空间。为了证明变量映射的强大性，我们提出了三种用例，用于修复 novice 程序员在入门编程作业（IPA）中常见的错误。我们的实验结果表明，我们的方法可以正确地映射83%的评估数据集。此外，我们的实验还表明，现有的程序修复方法，强调程序结构，只能修复约72%的错误程序。与此相比，我们的方法， solely 基于变量映射，可以修复约88.5%的错误程序。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/25/cs.LG_2023_07_25/" data-id="cloqtaesr00mtgh88f8oc2b76" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_25" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/25/eess.IV_2023_07_25/" class="article-date">
  <time datetime="2023-07-25T09:00:00.000Z" itemprop="datePublished">2023-07-25</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/25/eess.IV_2023_07_25/">eess.IV - 2023-07-25</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Towards-Unifying-Anatomy-Segmentation-Automated-Generation-of-a-Full-body-CT-Dataset-via-Knowledge-Aggregation-and-Anatomical-Guidelines"><a href="#Towards-Unifying-Anatomy-Segmentation-Automated-Generation-of-a-Full-body-CT-Dataset-via-Knowledge-Aggregation-and-Anatomical-Guidelines" class="headerlink" title="Towards Unifying Anatomy Segmentation: Automated Generation of a Full-body CT Dataset via Knowledge Aggregation and Anatomical Guidelines"></a>Towards Unifying Anatomy Segmentation: Automated Generation of a Full-body CT Dataset via Knowledge Aggregation and Anatomical Guidelines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13375">http://arxiv.org/abs/2307.13375</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alexanderjaus/atlasdataset">https://github.com/alexanderjaus/atlasdataset</a></li>
<li>paper_authors: Alexander Jaus, Constantin Seibold, Kelsey Hermann, Alexandra Walter, Kristina Giske, Johannes Haubold, Jens Kleesiek, Rainer Stiefelhagen<br>for: 这种方法用于生成自动生成的解剖学 segmentation 数据集，使用紧跟式的 nnU-Net 基于 pseudo-labeling 和 anatomy-guided pseudo-label 精度调整。methods: 这种方法通过结合多种分立的知识库，生成了一个整体 CT 扫描图像的 $142$ 块级标签，提供了全面的解剖学覆盖。results: 我们的方法不需要手动标注 durante 标签聚合阶段，并在 BTCV 数据集上实现了 85% dice 分数。此外，我们还进行了医学有效性检查和可扩展自动检查。<details>
<summary>Abstract</summary>
In this study, we present a method for generating automated anatomy segmentation datasets using a sequential process that involves nnU-Net-based pseudo-labeling and anatomy-guided pseudo-label refinement. By combining various fragmented knowledge bases, we generate a dataset of whole-body CT scans with $142$ voxel-level labels for 533 volumes providing comprehensive anatomical coverage which experts have approved. Our proposed procedure does not rely on manual annotation during the label aggregation stage. We examine its plausibility and usefulness using three complementary checks: Human expert evaluation which approved the dataset, a Deep Learning usefulness benchmark on the BTCV dataset in which we achieve 85% dice score without using its training dataset, and medical validity checks. This evaluation procedure combines scalable automated checks with labor-intensive high-quality expert checks. Besides the dataset, we release our trained unified anatomical segmentation model capable of predicting $142$ anatomical structures on CT data.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们提出了一种方法，用于自动生成骨科影像分割数据集，通过nnU-Net基于pseudo标签和骨科指导pseudo标签纠正的顺序 proces。通过结合多个分割知识库，我们生成了整体 CT 扫描图像的 $142$ 块级标签，对 533 幅提供了全面的解剖学覆盖，经过专家审核。我们的提议过程不依赖于手动标注 during the label aggregation stage。我们使用三种 complementary 检查来评估我们的方法：人工专家评估，btcv 数据集上的深度学习有用性测试，以及医学有效性检查。这种评估过程结合了扩展自动检查和劳动密集高质量专家检查。除了数据集之外，我们发布了我们的训练过的一体解剖学分割模型，可以在 CT 数据上预测 $142$ 种解剖学结构。
</details></li>
</ul>
<hr>
<h2 id="Overcoming-Distribution-Mismatch-in-Quantizing-Image-Super-Resolution-Networks"><a href="#Overcoming-Distribution-Mismatch-in-Quantizing-Image-Super-Resolution-Networks" class="headerlink" title="Overcoming Distribution Mismatch in Quantizing Image Super-Resolution Networks"></a>Overcoming Distribution Mismatch in Quantizing Image Super-Resolution Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13337">http://arxiv.org/abs/2307.13337</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheeun Hong, Kyoung Mu Lee</li>
<li>for: 这篇论文的目的是提出一个新的几何量化框架，以解决图像超解析网络中的分布差异问题，以提高量化后的精准度。</li>
<li>methods: 这篇论文使用了一个新的几何量化框架，named ODM，它通过在训练过程中直接调整特征分布的方式来降低分布差异问题。此外，ODM还引入了分布偏移来更好地调整各个通道的特征分布。</li>
<li>results: 实验结果显示，ODM可以对图像超解析网络进行有效的量化，并且与现有的量化方法相比，ODM可以更好地维持精准度。此外，ODM还可以降低分布差异问题的影响，使量化后的精准度得到更大的提升。<details>
<summary>Abstract</summary>
Quantization is a promising approach to reduce the high computational complexity of image super-resolution (SR) networks. However, compared to high-level tasks like image classification, low-bit quantization leads to severe accuracy loss in SR networks. This is because feature distributions of SR networks are significantly divergent for each channel or input image, and is thus difficult to determine a quantization range. Existing SR quantization works approach this distribution mismatch problem by dynamically adapting quantization ranges to the variant distributions during test time. However, such dynamic adaptation incurs additional computational costs that limit the benefits of quantization. Instead, we propose a new quantization-aware training framework that effectively Overcomes the Distribution Mismatch problem in SR networks without the need for dynamic adaptation. Intuitively, the mismatch can be reduced by directly regularizing the variance in features during training. However, we observe that variance regularization can collide with the reconstruction loss during training and adversely impact SR accuracy. Thus, we avoid the conflict between two losses by regularizing the variance only when the gradients of variance regularization are cooperative with that of reconstruction. Additionally, to further reduce the distribution mismatch, we introduce distribution offsets to layers with a significant mismatch, which either scales or shifts channel-wise features. Our proposed algorithm, called ODM, effectively reduces the mismatch in distributions with minimal computational overhead. Experimental results show that ODM effectively outperforms existing SR quantization approaches with similar or fewer computations, demonstrating the importance of reducing the distribution mismatch problem. Our code is available at https://github.com/Cheeun/ODM.
</details>
<details>
<summary>摘要</summary>
“量化是一种可能的方法来降低图像超解像网络的高度计算复杂性。然而，相比高水平任务如图像分类，低位数量化对SR网络导致严重的准确损失。这是因为SR网络的特征分布在每个通道或输入图像之间存在严重的分布不对称性。现有的SR量化工作通过在试用时适应性的方式来解决这个分布不对称问题。然而，这种动态适应带来更多的计算成本，限制了量化的利弊。相反，我们提出了一个新的量化意识训练框架，可以有效地解决SR网络中的分布不对称问题，无需动态适应。”“我们观察到，SR网络的特征分布存在严重的分布不对称性，这可以通过对特征的方差调控来缓和。然而，我们发现，在训练时对方差进行调控可能会与重建loss发生冲突，导致SR准确下降。因此，我们避免了这两个损失之间的冲突，通过对方差调控时只有在重建loss的Gradient与方差调控的Gradient之间有着合作的情况下进行调控。”“此外，为了进一步缓和分布不对称问题，我们引入了分布偏移，将通道对频率偏移或扭转。我们称之为ODM。实验结果显示，ODM可以对SR量化进行有效的缓和，并且与相同或 fewer 的计算成本下，实现SR准确的提高。”“我们的代码可以在https://github.com/Cheeun/ODM上找到。”
</details></li>
</ul>
<hr>
<h2 id="A-Visual-Quality-Assessment-Method-for-Raster-Images-in-Scanned-Document"><a href="#A-Visual-Quality-Assessment-Method-for-Raster-Images-in-Scanned-Document" class="headerlink" title="A Visual Quality Assessment Method for Raster Images in Scanned Document"></a>A Visual Quality Assessment Method for Raster Images in Scanned Document</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13241">http://arxiv.org/abs/2307.13241</a></li>
<li>repo_url: None</li>
<li>paper_authors: Justin Yang, Peter Bauer, Todd Harris, Changhyung Lee, Hyeon Seok Seo, Jan P Allebach, Fengqing Zhu</li>
<li>for: 本研究探讨了扫描文档中的图像质量，特别是针对灰度图像区域。</li>
<li>methods: 我们提出了一种基于机器学习的分类方法，以确定扫描灰度图像的视觉质量是否符合标准。</li>
<li>results: 我们通过进行心理学实验，确定了不同分辨率设定下图像质量的可接受程度，并使用这些人工标准来训练机器学习模型。 However, this dataset is unbalanced as most images were rated as visually acceptable. To address the data imbalance problem, we introduce several noise models to simulate the degradation of image quality during the scanning process. Our results show that by including augmented data in training, we can significantly improve the performance of the classifier to determine whether the visual quality of raster images in a scanned document is acceptable or not for a given resolution setting.<details>
<summary>Abstract</summary>
Image quality assessment (IQA) is an active research area in the field of image processing. Most prior works focus on visual quality of natural images captured by cameras. In this paper, we explore visual quality of scanned documents, focusing on raster image areas. Different from many existing works which aim to estimate a visual quality score, we propose a machine learning based classification method to determine whether the visual quality of a scanned raster image at a given resolution setting is acceptable. We conduct a psychophysical study to determine the acceptability at different image resolutions based on human subject ratings and use them as the ground truth to train our machine learning model. However, this dataset is unbalanced as most images were rated as visually acceptable. To address the data imbalance problem, we introduce several noise models to simulate the degradation of image quality during the scanning process. Our results show that by including augmented data in training, we can significantly improve the performance of the classifier to determine whether the visual quality of raster images in a scanned document is acceptable or not for a given resolution setting.
</details>
<details>
<summary>摘要</summary>
We conduct a psychophysical study to determine the acceptability of images at different resolutions based on human subject ratings and use them as the ground truth to train our machine learning model. However, the dataset is unbalanced as most images were rated as visually acceptable. To address this problem, we introduce several noise models to simulate the degradation of image quality during the scanning process. Our results show that by including augmented data in training, we can significantly improve the performance of the classifier to determine whether the visual quality of raster images in a scanned document is acceptable or not for a given resolution setting.
</details></li>
</ul>
<hr>
<h2 id="One-for-Multiple-Physics-informed-Synthetic-Data-Boosts-Generalizable-Deep-Learning-for-Fast-MRI-Reconstruction"><a href="#One-for-Multiple-Physics-informed-Synthetic-Data-Boosts-Generalizable-Deep-Learning-for-Fast-MRI-Reconstruction" class="headerlink" title="One for Multiple: Physics-informed Synthetic Data Boosts Generalizable Deep Learning for Fast MRI Reconstruction"></a>One for Multiple: Physics-informed Synthetic Data Boosts Generalizable Deep Learning for Fast MRI Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13220">http://arxiv.org/abs/2307.13220</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wangziblake/pisf">https://github.com/wangziblake/pisf</a></li>
<li>paper_authors: Zi Wang, Xiaotong Yu, Chengyan Wang, Weibo Chen, Jiazheng Wang, Ying-Hua Chu, Hongwei Sun, Rushuai Li, Peiyong Li, Fan Yang, Haiwei Han, Taishan Kang, Jianzhong Lin, Chen Yang, Shufu Chang, Zhang Shi, Sha Hua, Yan Li, Juan Hu, Liuhong Zhu, Jianjun Zhou, Meijing Lin, Jiefeng Guo, Congbo Cai, Zhong Chen, Di Guo, Xiaobo Qu</li>
<li>for: 这个论文旨在提高快速磁共振成像（MRI）的扫描时间，使用深度学习（DL）技术进行图像重建，但是现有的DL方法在不同的成像场景下的应用尚未得到广泛开发。</li>
<li>methods: 这个研究使用了物理学习Synthetic数据框架（PISF），该框架可以通过具有一个训练后可通用的模型来实现多种成像场景下的MRI重建。在2D图像重建中，扫描是分解成多个1D基本问题，并从1D数据生成开始，以便普适化。</li>
<li>results: 研究发现，使用PISF学习Synthetic数据，并与提高学习技术相结合，可以在实验室中比较或者更好地重建实验室中的MRI图像，比对使用匹配的真实数据训练的模型。此外，PISF还能够在多种供应商多个中心的成像中表现出优异的适应性。10名经验丰富的医生也证明了PISF在实际应用中的优秀适应性。<details>
<summary>Abstract</summary>
Magnetic resonance imaging (MRI) is a principal radiological modality that provides radiation-free, abundant, and diverse information about the whole human body for medical diagnosis, but suffers from prolonged scan time. The scan time can be significantly reduced through k-space undersampling but the introduced artifacts need to be removed in image reconstruction. Although deep learning (DL) has emerged as a powerful tool for image reconstruction in fast MRI, its potential in multiple imaging scenarios remains largely untapped. This is because not only collecting large-scale and diverse realistic training data is generally costly and privacy-restricted, but also existing DL methods are hard to handle the practically inevitable mismatch between training and target data. Here, we present a Physics-Informed Synthetic data learning framework for Fast MRI, called PISF, which is the first to enable generalizable DL for multi-scenario MRI reconstruction using solely one trained model. For a 2D image, the reconstruction is separated into many 1D basic problems and starts with the 1D data synthesis, to facilitate generalization. We demonstrate that training DL models on synthetic data, integrated with enhanced learning techniques, can achieve comparable or even better in vivo MRI reconstruction compared to models trained on a matched realistic dataset, reducing the demand for real-world MRI data by up to 96%. Moreover, our PISF shows impressive generalizability in multi-vendor multi-center imaging. Its excellent adaptability to patients has been verified through 10 experienced doctors' evaluations. PISF provides a feasible and cost-effective way to markedly boost the widespread usage of DL in various fast MRI applications, while freeing from the intractable ethical and practical considerations of in vivo human data acquisitions.
</details>
<details>
<summary>摘要</summary>
To address these challenges, we present a Physics-Informed Synthetic data learning framework for Fast MRI, called PISF. PISF enables generalizable DL for multi-scenario MRI reconstruction using solely one trained model. For a 2D image, the reconstruction is separated into many 1D basic problems, starting with the synthesis of 1D data. This approach facilitates generalization and reduces the demand for real-world MRI data by up to 96%. Additionally, PISF demonstrates impressive generalizability in multi-vendor multi-center imaging and has been evaluated by 10 experienced doctors, who have verified its excellent adaptability to patients.PISF provides a feasible and cost-effective way to markedly boost the widespread usage of DL in various fast MRI applications, while freeing from the intractable ethical and practical considerations of in vivo human data acquisitions.
</details></li>
</ul>
<hr>
<h2 id="Magnetic-Resonance-Parameter-Mapping-using-Self-supervised-Deep-Learning-with-Model-Reinforcement"><a href="#Magnetic-Resonance-Parameter-Mapping-using-Self-supervised-Deep-Learning-with-Model-Reinforcement" class="headerlink" title="Magnetic Resonance Parameter Mapping using Self-supervised Deep Learning with Model Reinforcement"></a>Magnetic Resonance Parameter Mapping using Self-supervised Deep Learning with Model Reinforcement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13211">http://arxiv.org/abs/2307.13211</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wanyu Bian, Albert Jang, Fang Liu</li>
<li>for: 本研究提出了一种新的自动学习方法，RELAX-MORE，用于生物医学影像重建（qMRI）。</li>
<li>methods: 该方法使用优化算法将模型基于qMRI重建拓展到深度学习框架中，以生成高精度和可靠的MR参数图像。</li>
<li>results: 在不同的脑、膝和phantom实验中，提出的方法能够高效地重建MR参数图像，正确地纠正影像损害、除除噪音和恢复图像特征。与其他状态前方法相比，RELAX-MORE显著提高了效率、准确性、可靠性和通用性。这种方法有很大的应用前途，可能为qMRI的临床翻译提供很大的助力。<details>
<summary>Abstract</summary>
This paper proposes a novel self-supervised learning method, RELAX-MORE, for quantitative MRI (qMRI) reconstruction. The proposed method uses an optimization algorithm to unroll a model-based qMRI reconstruction into a deep learning framework, enabling the generation of highly accurate and robust MR parameter maps at imaging acceleration. Unlike conventional deep learning methods requiring a large amount of training data, RELAX-MORE is a subject-specific method that can be trained on single-subject data through self-supervised learning, making it accessible and practically applicable to many qMRI studies. Using the quantitative $T_1$ mapping as an example at different brain, knee and phantom experiments, the proposed method demonstrates excellent performance in reconstructing MR parameters, correcting imaging artifacts, removing noises, and recovering image features at imperfect imaging conditions. Compared with other state-of-the-art conventional and deep learning methods, RELAX-MORE significantly improves efficiency, accuracy, robustness, and generalizability for rapid MR parameter mapping. This work demonstrates the feasibility of a new self-supervised learning method for rapid MR parameter mapping, with great potential to enhance the clinical translation of qMRI.
</details>
<details>
<summary>摘要</summary>
The proposed method was tested using the quantitative $T_1$ mapping as an example at different brain, knee, and phantom experiments. The results showed that RELAX-MORE demonstrated excellent performance in reconstructing MR parameters, correcting imaging artifacts, removing noise, and recovering image features at imperfect imaging conditions. Compared with other state-of-the-art conventional and deep learning methods, RELAX-MORE significantly improved efficiency, accuracy, robustness, and generalizability for rapid MR parameter mapping.This work demonstrates the feasibility of self-supervised learning for rapid MR parameter mapping, with great potential to enhance the clinical translation of qMRI.
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Approaches-for-Data-Augmentation-in-Medical-Imaging-A-Review"><a href="#Deep-Learning-Approaches-for-Data-Augmentation-in-Medical-Imaging-A-Review" class="headerlink" title="Deep Learning Approaches for Data Augmentation in Medical Imaging: A Review"></a>Deep Learning Approaches for Data Augmentation in Medical Imaging: A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13125">http://arxiv.org/abs/2307.13125</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Arminsbss/tumor-classification">https://github.com/Arminsbss/tumor-classification</a></li>
<li>paper_authors: Aghiles Kebaili, Jérôme Lapuyade-Lahorgue, Su Ruan</li>
<li>for: 这篇论文主要针对医疗影像分析领域中深度学习模型的训练数据有限制的问题，即使用深度生成模型来生成更真实和多样化的数据，以提高深度学习模型在医疗影像分析中的表现。</li>
<li>methods: 这篇论文主要介绍了三种深度生成模型，即变量自动编码器、对抗网络和扩散模型，以及它们在医疗影像分析中的应用。</li>
<li>results: 论文提供了现有深度生成模型在医疗影像分析中的最新状况，以及它们在不同下游任务中的潜在应用，包括分类、分割和cross-modal翻译。同时，论文也评估了每种模型的优缺点，并提出了未来研究的方向。<details>
<summary>Abstract</summary>
Deep learning has become a popular tool for medical image analysis, but the limited availability of training data remains a major challenge, particularly in the medical field where data acquisition can be costly and subject to privacy regulations. Data augmentation techniques offer a solution by artificially increasing the number of training samples, but these techniques often produce limited and unconvincing results. To address this issue, a growing number of studies have proposed the use of deep generative models to generate more realistic and diverse data that conform to the true distribution of the data. In this review, we focus on three types of deep generative models for medical image augmentation: variational autoencoders, generative adversarial networks, and diffusion models. We provide an overview of the current state of the art in each of these models and discuss their potential for use in different downstream tasks in medical imaging, including classification, segmentation, and cross-modal translation. We also evaluate the strengths and limitations of each model and suggest directions for future research in this field. Our goal is to provide a comprehensive review about the use of deep generative models for medical image augmentation and to highlight the potential of these models for improving the performance of deep learning algorithms in medical image analysis.
</details>
<details>
<summary>摘要</summary>
深度学习已成为医疗影像分析的流行工具，但培训数据的有限性仍然是主要的挑战，尤其在医疗领域，数据获取可能昂贵且受隐私法规限制。数据扩充技术可以人工增加培训样本数量，但这些技术通常生成有限和不置人心的结果。为解决这个问题，一些研究提出使用深度生成模型生成更真实和多样的数据，以符合实际数据的分布。在这篇评论中，我们关注了医疗影像增强中三种深度生成模型：变量自适应网络、对抗网络和扩散模型。我们提供了每种模型的当前状态之讲，并讨论它们在不同下游任务中的潜在应用，包括分类、分割和cross-modal翻译。我们还评估了每种模型的优缺点，并建议未来在这一领域的发展方向。我们的目标是提供深度生成模型在医疗影像增强中的全面评论，并高亮这些模型在医疗影像分析中的潜在优势，以及未来研究的发展方向。
</details></li>
</ul>
<hr>
<h2 id="In-Situ-Thickness-Measurement-of-Die-Silicon-Using-Voltage-Imaging-for-Hardware-Assurance"><a href="#In-Situ-Thickness-Measurement-of-Die-Silicon-Using-Voltage-Imaging-for-Hardware-Assurance" class="headerlink" title="In-Situ Thickness Measurement of Die Silicon Using Voltage Imaging for Hardware Assurance"></a>In-Situ Thickness Measurement of Die Silicon Using Voltage Imaging for Hardware Assurance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13118">http://arxiv.org/abs/2307.13118</a></li>
<li>repo_url: None</li>
<li>paper_authors: Olivia P. Dizon-Paradis, Nitin Varshney, M Tanjidur Rahman, Michael Strizich, Haoting Shen, Navid Asadizanjani</li>
<li>for: 这篇论文的目的是提出一种基于电子束电压成像、图像处理和蒙特卡洛模拟的快速减厚方法，以便在减厚过程中保证层的厚度均匀。</li>
<li>methods: 该方法使用电子束电压成像技术、图像处理和蒙特卡洛模拟来测量剩下的硅层的厚度，以便在减厚过程中实时监测和调整层的厚度。</li>
<li>results: 该方法可以快速、准确地测量硅层的厚度，并且可以在减厚过程中实时监测和调整层的厚度，以保证减厚过程的准确性和效率。<details>
<summary>Abstract</summary>
Hardware assurance of electronics is a challenging task and is of great interest to the government and the electronics industry. Physical inspection-based methods such as reverse engineering (RE) and Trojan scanning (TS) play an important role in hardware assurance. Therefore, there is a growing demand for automation in RE and TS. Many state-of-the-art physical inspection methods incorporate an iterative imaging and delayering workflow. In practice, uniform delayering can be challenging if the thickness of the initial layer of material is non-uniform. Moreover, this non-uniformity can reoccur at any stage during delayering and must be corrected. Therefore, it is critical to evaluate the thickness of the layers to be removed in a real-time fashion. Our proposed method uses electron beam voltage imaging, image processing, and Monte Carlo simulation to measure the thickness of remaining silicon to guide a uniform delayering process
</details>
<details>
<summary>摘要</summary>
硬件保证是电子设备领域的一项挑战，政府和电子行业对其具有很大的兴趣。物理检查方法如反工程（RE）和 Trojan 扫描（TS）在硬件保证中扮演着重要的角色。因此，自动化在 RE 和 TS 中的需求在增长。许多当今的物理检查方法具有迭代性的图像和层去除工艺。在实践中，均匀的层去除可以是一个挑战，特别是当初始材料厚度不均匀时。此外，这种不均匀性可能会在任何阶段重新出现，需要实时纠正。因此，我们的提议的方法使用电子束电幕摄影、图像处理和 Монте卡洛 simulate 来测量剩下的硬件厚度，以便实现均匀的层去除过程。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Infant-Respiration-Estimation-from-Video-A-Deep-Flow-based-Algorithm-and-a-Novel-Public-Benchmark"><a href="#Automatic-Infant-Respiration-Estimation-from-Video-A-Deep-Flow-based-Algorithm-and-a-Novel-Public-Benchmark" class="headerlink" title="Automatic Infant Respiration Estimation from Video: A Deep Flow-based Algorithm and a Novel Public Benchmark"></a>Automatic Infant Respiration Estimation from Video: A Deep Flow-based Algorithm and a Novel Public Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13110">http://arxiv.org/abs/2307.13110</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ostadabbas/infant-respiration-estimation">https://github.com/ostadabbas/infant-respiration-estimation</a></li>
<li>paper_authors: Sai Kumar Reddy Manne, Shaotong Zhu, Sarah Ostadabbas, Michael Wan</li>
<li>for: 这篇论文目标是为新生儿提供自动、无接触的呼吸监测。</li>
<li>methods: 该论文使用深度学习方法，使用普通的视频捕捉来估计新生儿的呼吸速率和呼吸波形。</li>
<li>results: 该论文在使用AIRFlowNet模型和AIR-125 infant数据集上进行训练后，与其他state-of-the-art方法相比，在呼吸速率估计中显著提高了精度， сред平均误差为$\sim$2.9 breaths per minute。<details>
<summary>Abstract</summary>
Respiration is a critical vital sign for infants, and continuous respiratory monitoring is particularly important for newborns. However, neonates are sensitive and contact-based sensors present challenges in comfort, hygiene, and skin health, especially for preterm babies. As a step toward fully automatic, continuous, and contactless respiratory monitoring, we develop a deep-learning method for estimating respiratory rate and waveform from plain video footage in natural settings. Our automated infant respiration flow-based network (AIRFlowNet) combines video-extracted optical flow input and spatiotemporal convolutional processing tuned to the infant domain. We support our model with the first public annotated infant respiration dataset with 125 videos (AIR-125), drawn from eight infant subjects, set varied pose, lighting, and camera conditions. We include manual respiration annotations and optimize AIRFlowNet training on them using a novel spectral bandpass loss function. When trained and tested on the AIR-125 infant data, our method significantly outperforms other state-of-the-art methods in respiratory rate estimation, achieving a mean absolute error of $\sim$2.9 breaths per minute, compared to $\sim$4.7--6.2 for other public models designed for adult subjects and more uniform environments.
</details>
<details>
<summary>摘要</summary>
呼吸是新生儿的重要生命 Parameter，连续呼吸监测特别重要。然而，新生儿脆弱， contact-based 感测器会带来舒适、卫生和皮肤健康问题，特别是 Premature 新生儿。为了实现完全自动、不间断、无接触的呼吸监测，我们开发了一种深度学习方法，可以从平面视频 Footage 中获取呼吸速率和波形。我们的自动 infant 呼吸流基本网络（AIRFlowNet）将视频提取的光学流输入和空间时间卷积处理相结合，并在婴儿领域进行了调整。我们为模型提供了首个公共标注 infant 呼吸数据集（AIR-125），包含 125 个视频，来自八个婴儿素材，有不同的姿势、照明和摄像头条件。我们还包括手动呼吸标注和使用一种新的spectral bandpass损失函数来优化 AIRFlowNet 的训练。当我们在 AIR-125 婴儿数据集上训练和测试 AIRFlowNet 时，它在呼吸速率估计方面表现出色，与其他公共模型在 adult 主题和更uniform 环境中的表现相比，表现出较低的mean absolute error（约为 2.9 呼吸/分钟）。
</details></li>
</ul>
<hr>
<h2 id="Framework-for-Automatic-PCB-Marking-Detection-and-Recognition-for-Hardware-Assurance"><a href="#Framework-for-Automatic-PCB-Marking-Detection-and-Recognition-for-Hardware-Assurance" class="headerlink" title="Framework for Automatic PCB Marking Detection and Recognition for Hardware Assurance"></a>Framework for Automatic PCB Marking Detection and Recognition for Hardware Assurance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13105">http://arxiv.org/abs/2307.13105</a></li>
<li>repo_url: None</li>
<li>paper_authors: Olivia P. Dizon-Paradis, Daniel E. Capecci, Nathan T. Jessurun, Damon L. Woodard, Mark M. Tehranipoor, Navid Asadizanjani</li>
<li>for: 这个研究的目的是提出一种自动电路板标注EXTRACTION方法，以便为政府和电子行业提供高精度的自动硬件保证。</li>
<li>methods: 该研究提出了一种收集PCB标注数据的计划，以及一种将该数据integrated到自动硬件保证过程中的框架。</li>
<li>results: 该研究提出了一种收集PCB标注数据的计划和一种将该数据integrated到自动硬件保证过程中的框架，可以提高自动硬件保证的精度。<details>
<summary>Abstract</summary>
A Bill of Materials (BoM) is a list of all components on a printed circuit board (PCB). Since BoMs are useful for hardware assurance, automatic BoM extraction (AutoBoM) is of great interest to the government and electronics industry. To achieve a high-accuracy AutoBoM process, domain knowledge of PCB text and logos must be utilized. In this study, we discuss the challenges associated with automatic PCB marking extraction and propose 1) a plan for collecting salient PCB marking data, and 2) a framework for incorporating this data for automatic PCB assurance. Given the proposed dataset plan and framework, subsequent future work, implications, and open research possibilities are detailed.
</details>
<details>
<summary>摘要</summary>
一份成本物品列表（Bill of Materials，BoM）是印刷电路板（Printed Circuit Board，PCB）上所有组件的列表。由于BoM对硬件保证有益，因此自动BoM提取（AutoBoM）对政府和电子业界来说非常有利。为实现高精度的AutoBoM过程，需要利用PCB文本和标识符的领域知识。在这篇研究中，我们介绍了自动PCB标识提取的挑战，并提出了1）PCB标识数据收集计划，2）基于这些数据的自动PCB保证框架。根据提出的数据计划和框架，我们释放了未来工作、后续研究和开放的研究可能性。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-image-captioning-with-depth-information-using-a-Transformer-based-framework"><a href="#Enhancing-image-captioning-with-depth-information-using-a-Transformer-based-framework" class="headerlink" title="Enhancing image captioning with depth information using a Transformer-based framework"></a>Enhancing image captioning with depth information using a Transformer-based framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03767">http://arxiv.org/abs/2308.03767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aya Mahmoud Ahmed, Mohamed Yousef, Khaled F. Hussain, Yousef Bassyouni Mahdy</li>
<li>for: 该论文旨在提高图像captioning任务中的场景理解，通过将RGB图像和其相应的深度图 integrate into一个Transformer-based encoder-decoder框架中，生成多句文本描述3D场景。</li>
<li>methods: 该论文提出了一种将RGB图像和深度图进行拼接的方法，并使用Transformer架构来生成多句文本描述。不同的拼接方法也被研究以实现最佳的结果。</li>
<li>results: 实验结果表明，使用RGB图像和深度图进行拼接可以提高图像captioning任务的效果，无论depth图是否为真实的或估算值。此外，该论文还提出了一个更正版的NYU-v2数据集，以解决存在问题的标注问题。<details>
<summary>Abstract</summary>
Captioning images is a challenging scene-understanding task that connects computer vision and natural language processing. While image captioning models have been successful in producing excellent descriptions, the field has primarily focused on generating a single sentence for 2D images. This paper investigates whether integrating depth information with RGB images can enhance the captioning task and generate better descriptions. For this purpose, we propose a Transformer-based encoder-decoder framework for generating a multi-sentence description of a 3D scene. The RGB image and its corresponding depth map are provided as inputs to our framework, which combines them to produce a better understanding of the input scene. Depth maps could be ground truth or estimated, which makes our framework widely applicable to any RGB captioning dataset. We explored different fusion approaches to fuse RGB and depth images. The experiments are performed on the NYU-v2 dataset and the Stanford image paragraph captioning dataset. During our work with the NYU-v2 dataset, we found inconsistent labeling that prevents the benefit of using depth information to enhance the captioning task. The results were even worse than using RGB images only. As a result, we propose a cleaned version of the NYU-v2 dataset that is more consistent and informative. Our results on both datasets demonstrate that the proposed framework effectively benefits from depth information, whether it is ground truth or estimated, and generates better captions. Code, pre-trained models, and the cleaned version of the NYU-v2 dataset will be made publically available.
</details>
<details>
<summary>摘要</summary>
标题： integrate depth information to enhance image captioning task摘要：在图像描述任务中，将RGB图像和深度图像融合在一起，可以提高描述任务的质量。我们提出了一种基于Transformer的RGB图像和深度图像融合框架，用于生成多句话描述3D场景。我们的框架可以将RGB图像和其对应的深度图像作为输入，并将它们融合在一起，以更好地理解输入场景。我们实现了不同的融合方法，并对NYU-v2数据集和Stanford图像描述数据集进行了实验。在我们的工作中，我们发现了NYU-v2数据集中的不一致标注问题，这使得使用深度信息来提高描述任务的 beneficial effects become less effective。最终，我们提出了一个更加一致的NYU-v2数据集，并对这两个数据集进行了实验。我们的结果表明，我们的提议的框架可以借助深度信息，无论是真实的深度图像还是估计的深度图像，生成更好的描述。我们将代码、预训练模型和清洁版NYU-v2数据集公开发布。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/25/eess.IV_2023_07_25/" data-id="cloqtaezn0143gh88gxvxgpjs" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_07_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/24/cs.SD_2023_07_24/" class="article-date">
  <time datetime="2023-07-24T15:00:00.000Z" itemprop="datePublished">2023-07-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/24/cs.SD_2023_07_24/">cs.SD - 2023-07-24</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="An-objective-evaluation-of-Hearing-Aids-and-DNN-based-speech-enhancement-in-complex-acoustic-scenes"><a href="#An-objective-evaluation-of-Hearing-Aids-and-DNN-based-speech-enhancement-in-complex-acoustic-scenes" class="headerlink" title="An objective evaluation of Hearing Aids and DNN-based speech enhancement in complex acoustic scenes"></a>An objective evaluation of Hearing Aids and DNN-based speech enhancement in complex acoustic scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12888">http://arxiv.org/abs/2307.12888</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/enricguso/guso_waspaa23">https://github.com/enricguso/guso_waspaa23</a></li>
<li>paper_authors: Enric Gusó, Joanna Luberadzka, Martí Baig, Umut Sayin Saraç, Xavier Serra</li>
<li>for: 评估五种高级商业听力器（HA）device的目标性能，并与基于深度神经网络（DNN）的抽象环境中的语音提高算法进行比较。</li>
<li>methods: 测量一个HAdevice的Head-Related Transfer Functions（HRTFs），用于Synthesize一个双抽象 dataset для训练两种 state-of-the-art causal和非 causal DNN增强模型。然后，通过Ambisonics loudspeaker设置生成一个评估集，并通过KU100 dummy head记录每个HA device上的语音，包括和不包括传统HA算法。</li>
<li>results: 发现DNN增强比传统HA算法在噪声抑制和对话情况中的对话智能度指标方面表现更好。<details>
<summary>Abstract</summary>
We investigate the objective performance of five high-end commercially available Hearing Aid (HA) devices compared to DNN-based speech enhancement algorithms in complex acoustic environments. To this end, we measure the HRTFs of a single HA device to synthesize a binaural dataset for training two state-of-the-art causal and non-causal DNN enhancement models. We then generate an evaluation set of realistic speech-in-noise situations using an Ambisonics loudspeaker setup and record with a KU100 dummy head wearing each of the HA devices, both with and without the conventional HA algorithms, applying the DNN enhancers to the latter. We find that the DNN-based enhancement outperforms the HA algorithms in terms of noise suppression and objective intelligibility metrics.
</details>
<details>
<summary>摘要</summary>
我们研究了五种高级商业可用的听力器（HA）device的目标性能，与基于深度学习（DNN）的语音提升算法在复杂的噪声环境中进行比较。为此，我们测量了一个单个HAdevice的Head-Related Transfer Functions（HRTFs），以生成一个双核心state-of-the-art causal和非 causal DNN增强模型的训练数据集。然后，我们使用一个Ambisonics喇叭设置生成一个评估集，记录了每个HA设备上的KU100假头和不同的传统HA算法，并应用DNN增强器。我们发现，DNN基于的增强方法在噪声抑制和对象智能度量标中超过HA算法。
</details></li>
</ul>
<hr>
<h2 id="Joint-speech-and-overlap-detection-a-benchmark-over-multiple-audio-setup-and-speech-domains"><a href="#Joint-speech-and-overlap-detection-a-benchmark-over-multiple-audio-setup-and-speech-domains" class="headerlink" title="Joint speech and overlap detection: a benchmark over multiple audio setup and speech domains"></a>Joint speech and overlap detection: a benchmark over multiple audio setup and speech domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13012">http://arxiv.org/abs/2307.13012</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martin Lebourdais, Théo Mariotte, Marie Tahon, Anthony Larcher, Antoine Laurent, Silvio Montresor, Sylvain Meignier, Jean-Hugh Thomas</li>
<li>for: 本研究旨在提供一个完整的精度评估材料，用于评估不同的语音分类器在单&#x2F;多通道和多种语音频道上的性能。</li>
<li>methods: 本研究使用了一种新的多类别分类模型，将语音分类和 overlap speech detection 融合到一起进行训练。</li>
<li>results: 研究结果显示，该模型在多种语音频道和单&#x2F;多通道上具有优秀的性能，与单独的语音分类和 overlap speech detection 系统相比，具有更高的一致性和更低的训练成本。<details>
<summary>Abstract</summary>
Voice activity and overlapped speech detection (respectively VAD and OSD) are key pre-processing tasks for speaker diarization. The final segmentation performance highly relies on the robustness of these sub-tasks. Recent studies have shown VAD and OSD can be trained jointly using a multi-class classification model. However, these works are often restricted to a specific speech domain, lacking information about the generalization capacities of the systems. This paper proposes a complete and new benchmark of different VAD and OSD models, on multiple audio setups (single/multi-channel) and speech domains (e.g. media, meeting...). Our 2/3-class systems, which combine a Temporal Convolutional Network with speech representations adapted to the setup, outperform state-of-the-art results. We show that the joint training of these two tasks offers similar performances in terms of F1-score to two dedicated VAD and OSD systems while reducing the training cost. This unique architecture can also be used for single and multichannel speech processing.
</details>
<details>
<summary>摘要</summary>
声音活动和重叠说话检测（简称VAD和OSD）是speaker分类的关键前置处理任务。最终 segmentation 性能强度取决于这两个子任务的稳定性。近年来研究表明，VAD和OSD可以通过多类型分类模型进行合作训练。然而，这些研究通常受到特定的speech域的限制，缺乏对系统总体化能力的信息。本文提出了一个完整的VAD和OSD模型 benchmark，在多个音频设置（单/多通道）和语音域（例如媒体、会议等）上进行测试。我们的2/3类系统，即将时间卷积网络与适应设置的语音表示相结合，在F1分数上超越了现有的state-of-the-artResult。我们显示，将这两个任务合作训练可以与专门的VAD和OSD系统相比，提高了training cost，同时保持了相似的F1分数性能。此特有的架构还可以用于单通道和多通道speech处理。
</details></li>
</ul>
<hr>
<h2 id="Integration-of-Frame-and-Label-synchronous-Beam-Search-for-Streaming-Encoder-decoder-Speech-Recognition"><a href="#Integration-of-Frame-and-Label-synchronous-Beam-Search-for-Streaming-Encoder-decoder-Speech-Recognition" class="headerlink" title="Integration of Frame- and Label-synchronous Beam Search for Streaming Encoder-decoder Speech Recognition"></a>Integration of Frame- and Label-synchronous Beam Search for Streaming Encoder-decoder Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12767">http://arxiv.org/abs/2307.12767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emiru Tsunoo, Hayato Futami, Yosuke Kashiwagi, Siddhant Arora, Shinji Watanabe</li>
<li>for: 这个论文是为了提高自动语音识别的精度和Robustness，尤其是在遇到过去知识不足的情况下。</li>
<li>methods: 本论文使用了几种方法，包括frame-based模型和label-based注意力Encoder-Decoder，并通过在单一搜寻算法中交替进行F-Sync和L-Sync搜寻。</li>
<li>results: 实验结果显示，提案的搜寻算法比其他搜寻方法来得更低的误差率，并且在过去知识不足的情况下保持稳定性。<details>
<summary>Abstract</summary>
Although frame-based models, such as CTC and transducers, have an affinity for streaming automatic speech recognition, their decoding uses no future knowledge, which could lead to incorrect pruning. Conversely, label-based attention encoder-decoder mitigates this issue using soft attention to the input, while it tends to overestimate labels biased towards its training domain, unlike CTC. We exploit these complementary attributes and propose to integrate the frame- and label-synchronous (F-/L-Sync) decoding alternately performed within a single beam-search scheme. F-Sync decoding leads the decoding for block-wise processing, while L-Sync decoding provides the prioritized hypotheses using look-ahead future frames within a block. We maintain the hypotheses from both decoding methods to perform effective pruning. Experiments demonstrate that the proposed search algorithm achieves lower error rates compared to the other search methods, while being robust against out-of-domain situations.
</details>
<details>
<summary>摘要</summary>
尽管框架基模型，如 CTC 和传播器，与流动自动语音识别有着很好的相互作用，但它们的解码没有使用未来知识，这可能会导致错误的剪辑。相反，标签基于注意力Encoder-Decoder可以通过软注意力来输入，但它往往对它的训练领域偏好，不同于 CTC。我们利用这些 complementary 特点，并提议将帧和标签同步（F-/L-Sync）的解码 alternately 在同一个搜索方案中进行。F-Sync 解码在块级处理中领先，而 L-Sync 解码在预测未来帧内一个块中提供了优先的 гипотезы。我们保留了两个解码方法的假设，以实现有效的剪辑。实验表明，我们提议的搜索算法可以与其他搜索方法相比，在低误差情况下实现更好的性能，而且对于不同领域的情况也具有更高的稳定性。
</details></li>
</ul>
<hr>
<h2 id="Code-Switched-Urdu-ASR-for-Noisy-Telephonic-Environment-using-Data-Centric-Approach-with-Hybrid-HMM-and-CNN-TDNN"><a href="#Code-Switched-Urdu-ASR-for-Noisy-Telephonic-Environment-using-Data-Centric-Approach-with-Hybrid-HMM-and-CNN-TDNN" class="headerlink" title="Code-Switched Urdu ASR for Noisy Telephonic Environment using Data Centric Approach with Hybrid HMM and CNN-TDNN"></a>Code-Switched Urdu ASR for Noisy Telephonic Environment using Data Centric Approach with Hybrid HMM and CNN-TDNN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12759">http://arxiv.org/abs/2307.12759</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sage-khan/code-switched-noisy-urdu-asr">https://github.com/sage-khan/code-switched-noisy-urdu-asr</a></li>
<li>paper_authors: Muhammad Danyal Khan, Raheem Ali, Arshad Aziz</li>
<li>For: The paper aims to develop a resource-efficient Automatic Speech Recognition (ASR) system for code-switched Urdu language in a noisy call-center environment.* Methods: The proposed system uses a Chain Hybrid HMM and CNN-TDNN approach, which combines the advantages of HMM and DNN models with less labelled data. The system also utilizes a noisy environment-aware CNN to improve accuracy.* Results: The proposed system achieves a Word Error Rate (WER) of 5.2% in both noisy and clean environments, outperforming other ASR systems for code-switched Urdu language. The system also shows improved performance in recognizing isolated words, numbers, and continuous spontaneous speech.<details>
<summary>Abstract</summary>
Call Centers have huge amount of audio data which can be used for achieving valuable business insights and transcription of phone calls is manually tedious task. An effective Automated Speech Recognition system can accurately transcribe these calls for easy search through call history for specific context and content allowing automatic call monitoring, improving QoS through keyword search and sentiment analysis. ASR for Call Center requires more robustness as telephonic environment are generally noisy. Moreover, there are many low-resourced languages that are on verge of extinction which can be preserved with help of Automatic Speech Recognition Technology. Urdu is the $10^{th}$ most widely spoken language in the world, with 231,295,440 worldwide still remains a resource constrained language in ASR. Regional call-center conversations operate in local language, with a mix of English numbers and technical terms generally causing a "code-switching" problem. Hence, this paper describes an implementation framework of a resource efficient Automatic Speech Recognition/ Speech to Text System in a noisy call-center environment using Chain Hybrid HMM and CNN-TDNN for Code-Switched Urdu Language. Using Hybrid HMM-DNN approach allowed us to utilize the advantages of Neural Network with less labelled data. Adding CNN with TDNN has shown to work better in noisy environment due to CNN's additional frequency dimension which captures extra information from noisy speech, thus improving accuracy. We collected data from various open sources and labelled some of the unlabelled data after analysing its general context and content from Urdu language as well as from commonly used words from other languages, primarily English and were able to achieve WER of 5.2% with noisy as well as clean environment in isolated words or numbers as well as in continuous spontaneous speech.
</details>
<details>
<summary>摘要</summary>
call centers 有庞大的音频数据，可以用于获得有价值的商业意见和电话交流的自动识别，以便搜索历史记录中的特定上下文和内容，进行自动监控、关键词搜索和情感分析。为了在电话交流中提高质量，需要一个更加可靠的自动识别系统，因为电话环境通常噪音。此外，有很多资源受限的语言正在濒临灭绝，可以通过自动识别技术来保存它们。urd 是全球第10大最流行的语言，有231295440名使用者，但它仍然是资源受限的语言。本文描述了在噪音电话交流环境中实现资源有效的自动识别/文本转语系统的实现框架，使用链式混合HMM和CNN-TDNN进行混合语言识别。通过将HMM和DNN结合使用，我们可以利用神经网络的优势，而不需要大量标注数据。另外，通过添加CNN和TDNN，我们可以在噪音环境中提高准确率，因为CNN的额外频率维度可以捕捉更多的噪音语音信息。我们从多个开源资源中收集数据，并对一些未标注的数据进行分析和标注，以获得urd语言的WER为5.2%，包括噪音和清晰环境下的隔离单词或数字以及连续自由语言。
</details></li>
</ul>
<hr>
<h2 id="IteraTTA-An-interface-for-exploring-both-text-prompts-and-audio-priors-in-generating-music-with-text-to-audio-models"><a href="#IteraTTA-An-interface-for-exploring-both-text-prompts-and-audio-priors-in-generating-music-with-text-to-audio-models" class="headerlink" title="IteraTTA: An interface for exploring both text prompts and audio priors in generating music with text-to-audio models"></a>IteraTTA: An interface for exploring both text prompts and audio priors in generating music with text-to-audio models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13005">http://arxiv.org/abs/2307.13005</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hiromu Yakura, Masataka Goto</li>
<li>for: 帮助 novice 用户自由生成音乐音频，即使他们没有音乐知识，如和声进程和乐器知识。</li>
<li>methods: 我们使用 text-to-audio 生成技术，并提供了一个特定的 interface 以帮助用户逐步调整文本提示和选择有利的音频先导。</li>
<li>results: 通过这种双重探索方式，用户可以了解不同文本提示和音频先导对生成结果的影响，并逐步实现他们的模糊定义目标。<details>
<summary>Abstract</summary>
Recent text-to-audio generation techniques have the potential to allow novice users to freely generate music audio. Even if they do not have musical knowledge, such as about chord progressions and instruments, users can try various text prompts to generate audio. However, compared to the image domain, gaining a clear understanding of the space of possible music audios is difficult because users cannot listen to the variations of the generated audios simultaneously. We therefore facilitate users in exploring not only text prompts but also audio priors that constrain the text-to-audio music generation process. This dual-sided exploration enables users to discern the impact of different text prompts and audio priors on the generation results through iterative comparison of them. Our developed interface, IteraTTA, is specifically designed to aid users in refining text prompts and selecting favorable audio priors from the generated audios. With this, users can progressively reach their loosely-specified goals while understanding and exploring the space of possible results. Our implementation and discussions highlight design considerations that are specifically required for text-to-audio models and how interaction techniques can contribute to their effectiveness.
</details>
<details>
<summary>摘要</summary>
现代文本到音频生成技术具有让新手可以自由生成音频的潜力。即使他们没有音乐知识，如和声进程和乐器，用户仍可以尝试不同的文本提示来生成音频。然而，与图像领域不同，了解生成音频的可能性空间是困难的，因为用户无法同时听到生成音频的变化。我们因此为用户提供了探索不同文本提示和音频先前的机会。这种双重探索允许用户通过比较不同的文本提示和音频先前来了解不同的生成结果的影响。我们开发的界面IteraTTA专门为用户帮助制定文本提示和选择生成音频中有利的先前。通过这种方式，用户可以逐步实现自己的抽象目标，同时了解和探索生成结果的可能性空间。我们的实现和讨论探讨了特定于文本到音频模型的设计考虑因素，以及如何通过互动技术来提高其效iveness。
</details></li>
</ul>
<hr>
<h2 id="A-Model-for-Every-User-and-Budget-Label-Free-and-Personalized-Mixed-Precision-Quantization"><a href="#A-Model-for-Every-User-and-Budget-Label-Free-and-Personalized-Mixed-Precision-Quantization" class="headerlink" title="A Model for Every User and Budget: Label-Free and Personalized Mixed-Precision Quantization"></a>A Model for Every User and Budget: Label-Free and Personalized Mixed-Precision Quantization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12659">http://arxiv.org/abs/2307.12659</a></li>
<li>repo_url: None</li>
<li>paper_authors: Edward Fish, Umberto Michieli, Mete Ozay</li>
<li>for: 这篇论文目的是提出一种可以个别化的数字化模型优化方法，以提高自动语音识别（ASR）模型在移动设备上的部署。</li>
<li>methods: 本文提出了一种混合精度量化方法（myQASR），可以根据不同的用户和目标领域，生成特化的数字化方案，并且不需要精确调整。myQASR通过分析全精度活动值，自动评估网络层的数字化敏感度，然后生成个别化的混合精度量化方案。</li>
<li>results: 本文的实验结果显示，myQASR可以对大规模ASR模型进行个别化优化，以提高特定的性别、语言和说话者的表现。<details>
<summary>Abstract</summary>
Recent advancement in Automatic Speech Recognition (ASR) has produced large AI models, which become impractical for deployment in mobile devices. Model quantization is effective to produce compressed general-purpose models, however such models may only be deployed to a restricted sub-domain of interest. We show that ASR models can be personalized during quantization while relying on just a small set of unlabelled samples from the target domain. To this end, we propose myQASR, a mixed-precision quantization method that generates tailored quantization schemes for diverse users under any memory requirement with no fine-tuning. myQASR automatically evaluates the quantization sensitivity of network layers by analysing the full-precision activation values. We are then able to generate a personalised mixed-precision quantization scheme for any pre-determined memory budget. Results for large-scale ASR models show how myQASR improves performance for specific genders, languages, and speakers.
</details>
<details>
<summary>摘要</summary>
myQASR uses mixed-precision quantization to generate personalized schemes for each user. The method evaluates the quantization sensitivity of network layers by analyzing full-precision activation values. This allows for the creation of a personalized mixed-precision quantization scheme for any pre-determined memory budget.Results for large-scale ASR models show that myQASR improves performance for specific genders, languages, and speakers. This demonstrates the effectiveness of personalized quantization for ASR models, and highlights the potential of myQASR for improving the performance of ASR systems in a wide range of applications.Here is the text in Simplified Chinese:最近的自动语音识别（ASR）技术发展，导致大型AI模型的出现，但这些模型在移动设备上部署是不实际的。模型压缩是一种解决方案，但它只能部署到一个限制的子domain中。我们提出myQASR方法，它可以在压缩过程中为不同用户personal化ASR模型，而无需细化。myQASR使用混合精度压缩生成用户化的压缩方案，并通过分析全精度活动值来评估网络层的压缩敏感度。这allow for生成任何预先确定的内存预算的个性化混合精度压缩方案。results表明，myQASR可以对大规模ASR模型进行特定性别、语言和发音人的改进。这表明个性化压缩对ASR模型的性能有积极的影响，并 highlights myQASR在各种应用场景中的潜在应用前景。
</details></li>
</ul>
<hr>
<h2 id="Robust-Automatic-Speech-Recognition-via-WavAugment-Guided-Phoneme-Adversarial-Training"><a href="#Robust-Automatic-Speech-Recognition-via-WavAugment-Guided-Phoneme-Adversarial-Training" class="headerlink" title="Robust Automatic Speech Recognition via WavAugment Guided Phoneme Adversarial Training"></a>Robust Automatic Speech Recognition via WavAugment Guided Phoneme Adversarial Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12498">http://arxiv.org/abs/2307.12498</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/WAPATASR/WAPAT">https://github.com/WAPATASR/WAPAT</a></li>
<li>paper_authors: Gege Qi, Yuefeng Chen, Xiaofeng Mao, Xiaojun Jia, Ranjie Duan, Rong Zhang, Hui Xue</li>
<li>for: 提高自动语音识别（ASR）模型在小量干扰和大域转移下的稳定性。</li>
<li>methods: 使用phoneme空间中的对抗例进行听话示例的挤压，使模型对phoneme表示具有抗衰减性，并通过使用挤压示例的phoneme表示来引导对抗例生成，以找到更稳定和多样的梯度方向，提高总体性。</li>
<li>results: 在End-to-end Speech Challenge Benchmark（ESB）上实现了6.28%的WRER降低，超过原始模型，达到新的领域最优。<details>
<summary>Abstract</summary>
Developing a practically-robust automatic speech recognition (ASR) is challenging since the model should not only maintain the original performance on clean samples, but also achieve consistent efficacy under small volume perturbations and large domain shifts. To address this problem, we propose a novel WavAugment Guided Phoneme Adversarial Training (wapat). wapat use adversarial examples in phoneme space as augmentation to make the model invariant to minor fluctuations in phoneme representation and preserve the performance on clean samples. In addition, wapat utilizes the phoneme representation of augmented samples to guide the generation of adversaries, which helps to find more stable and diverse gradient-directions, resulting in improved generalization. Extensive experiments demonstrate the effectiveness of wapat on End-to-end Speech Challenge Benchmark (ESB). Notably, SpeechLM-wapat outperforms the original model by 6.28% WER reduction on ESB, achieving the new state-of-the-art.
</details>
<details>
<summary>摘要</summary>
发展一个实用robust的自动语音识别（ASR）模型是挑战，因为模型不仅需要保持干净样本上的原始性能，还需要在小量干扰和大域变化下达到一致的效果。为解决这问题，我们提出了一种新的WavAugment导向的phoneme adversarial Training（wapat）方法。wapat使用phoneme空间中的 adversarial example作为增强素，以使模型对phoneme表示的小变化免疫，并保持干净样本上的性能。此外，wapat使用增强后的phoneme表示来导引敌对生成，以找到更稳定和多样的梯度方向，从而提高了总体的一致性。我们在End-to-end Speech Challenge Benchmark（ESB）上进行了广泛的实验，并证明了wapat的有效性。特别是，SpeechLM-wapat比原始模型减少了6.28%的WRR，达到了新的状态态-of-the-art。
</details></li>
</ul>
<hr>
<h2 id="SCRAPS-Speech-Contrastive-Representations-of-Acoustic-and-Phonetic-Spaces"><a href="#SCRAPS-Speech-Contrastive-Representations-of-Acoustic-and-Phonetic-Spaces" class="headerlink" title="SCRAPS: Speech Contrastive Representations of Acoustic and Phonetic Spaces"></a>SCRAPS: Speech Contrastive Representations of Acoustic and Phonetic Spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12445">http://arxiv.org/abs/2307.12445</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ivan Vallés-Pérez, Grzegorz Beringer, Piotr Bilinski, Gary Cook, Roberto Barra-Chicote</li>
<li>for: This paper aims to learn shared representations of phonetic and acoustic spaces in the speech domain using a CLIP-based model.</li>
<li>methods: The proposed model is trained using the CLIP framework, which enables deep learning systems to learn shared latent spaces between images and text descriptions.</li>
<li>results: The model shows sensitivity to phonetic changes and robustness against different types of noise, with a 91% score drop when replacing 20% of the phonemes at random and a 10% performance drop when mixing the audio with 75% of Gaussian noise. The resulting embeddings are also found to be useful for downstream applications such as intelligibility evaluation and speech generation.<details>
<summary>Abstract</summary>
Numerous examples in the literature proved that deep learning models have the ability to work well with multimodal data. Recently, CLIP has enabled deep learning systems to learn shared latent spaces between images and text descriptions, with outstanding zero- or few-shot results in downstream tasks. In this paper we explore the same idea proposed by CLIP but applied to the speech domain, where the phonetic and acoustic spaces usually coexist. We train a CLIP-based model with the aim to learn shared representations of phonetic and acoustic spaces. The results show that the proposed model is sensible to phonetic changes, with a 91% of score drops when replacing 20% of the phonemes at random, while providing substantial robustness against different kinds of noise, with a 10% performance drop when mixing the audio with 75% of Gaussian noise. We also provide empirical evidence showing that the resulting embeddings are useful for a variety of downstream applications, such as intelligibility evaluation and the ability to leverage rich pre-trained phonetic embeddings in speech generation task. Finally, we discuss potential applications with interesting implications for the speech generation and recognition fields.
</details>
<details>
<summary>摘要</summary>
多种例子在 literatur 中证明深度学习模型可以处理多模态数据。近期，CLIP 使得深度学习系统可以学习共同的封闭空间 между图像和文本描述， obtained outstanding zero- or few-shot results in downstream tasks。在这篇文章中，我们探索了同样的想法，但是应用到语音领域， где phonetic 和 acoustic 空间通常共存。我们使用 CLIP 基于的模型， aiming to learn shared representations of phonetic and acoustic spaces。结果显示，我们的模型对 phonetic 变化敏感，对于Randomly replacing 20% of phonemes 的情况下，得分下降了 91%，而对于不同类型的噪音混合情况下，得分下降了 10%。我们还提供了实验证明， embedding 是对下游应用场景有用，如智能识别和speech generation 任务中的质量评估。最后，我们讨论了可能的应用，具有 interessing 的潜在应用于语音生成和识别领域。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/24/cs.SD_2023_07_24/" data-id="cloqtaev500ttgh880oa4ddd9" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_07_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/24/eess.AS_2023_07_24/" class="article-date">
  <time datetime="2023-07-24T14:00:00.000Z" itemprop="datePublished">2023-07-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/24/eess.AS_2023_07_24/">eess.AS - 2023-07-24</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Adaptation-of-Whisper-models-to-child-speech-recognition"><a href="#Adaptation-of-Whisper-models-to-child-speech-recognition" class="headerlink" title="Adaptation of Whisper models to child speech recognition"></a>Adaptation of Whisper models to child speech recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13008">http://arxiv.org/abs/2307.13008</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/c3imaging/whisper_child_speech">https://github.com/c3imaging/whisper_child_speech</a></li>
<li>paper_authors: Rishabh Jain, Andrei Barcovschi, Mariam Yiwere, Peter Corcoran, Horia Cucu</li>
<li>for: 提高儿童语音识别（ASR）系统对儿童语音的识别精度。</li>
<li>methods: 利用大量的成人语音数据创建多语言ASR模型，如Whisper，并对其进行适应化以适应儿童语音。</li>
<li>results: 对Whisper模型进行finetuning后，对儿童语音识别表现出显著改善，而使用自然语言生成模型wav2vec2进行finetuning则超过了Whisper模型的表现。<details>
<summary>Abstract</summary>
Automatic Speech Recognition (ASR) systems often struggle with transcribing child speech due to the lack of large child speech datasets required to accurately train child-friendly ASR models. However, there are huge amounts of annotated adult speech datasets which were used to create multilingual ASR models, such as Whisper. Our work aims to explore whether such models can be adapted to child speech to improve ASR for children. In addition, we compare Whisper child-adaptations with finetuned self-supervised models, such as wav2vec2. We demonstrate that finetuning Whisper on child speech yields significant improvements in ASR performance on child speech, compared to non finetuned Whisper models. Additionally, utilizing self-supervised Wav2vec2 models that have been finetuned on child speech outperforms Whisper finetuning.
</details>
<details>
<summary>摘要</summary>
自动语音识别（ASR）系统经常遇到儿童语音识别问题，原因在于缺乏儿童语音数据集，以训练适合儿童的 ASR 模型。然而，有大量已注解的成人语音数据集，用于创建多语言 ASR 模型，如呐喊（Whisper）。我们的工作旨在探讨是否可以将这些模型适应儿童语音，以提高 ASR 性能。此外，我们还比较了呐喊儿童化的模型与自动学习的 wav2vec2 模型，并证明了后者在儿童语音识别中表现更优。
</details></li>
</ul>
<hr>
<h2 id="Performance-Comparison-Between-VoLTE-and-non-VoLTE-Voice-Calls-During-Mobility-in-Commercial-Deployment-A-Drive-Test-Based-Analysis"><a href="#Performance-Comparison-Between-VoLTE-and-non-VoLTE-Voice-Calls-During-Mobility-in-Commercial-Deployment-A-Drive-Test-Based-Analysis" class="headerlink" title="Performance Comparison Between VoLTE and non-VoLTE Voice Calls During Mobility in Commercial Deployment: A Drive Test-Based Analysis"></a>Performance Comparison Between VoLTE and non-VoLTE Voice Calls During Mobility in Commercial Deployment: A Drive Test-Based Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12397">http://arxiv.org/abs/2307.12397</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rashed Hasan Ratul, Muhammad Iqbal, Jen-Yi Pan, Mohammad Mahadi Al Deen, Mohammad Tawhid Kawser, Mohammad Masum Billah</li>
<li>for: 这个论文主要研究了 Voice over LTE (VoLTE) 技术对移动通信网络的性能优化的影响，尤其是 call setup delay 和用户设备 (UE) 电池寿命。</li>
<li>methods: 该研究使用了 XCAL 驱动测试工具收集实时网络参数数据，并对 VoLTE 和非 VoLTE 语音呼电中的实时网络特性进行分析。</li>
<li>results: 研究发现，使用 VoLTE 技术可以提高 call setup delay 的速度和 UE 电池寿命，并且在 VoLTE 和非 VoLTE 语音呼电中比较研究了 DRX 机制。这些结果可以帮助优化移动通信网络的质量服务 (QoS)。<details>
<summary>Abstract</summary>
The optimization of network performance is vital for the delivery of services using standard cellular technologies for mobile communications. Call setup delay and User Equipment (UE) battery savings significantly influence network performance. Improving these factors is vital for ensuring optimal service delivery. In comparison to traditional circuit-switched voice calls, VoLTE (Voice over LTE) technology offers faster call setup durations and better battery-saving performance. To validate these claims, a drive test was carried out using the XCAL drive test tool to collect real-time network parameter details in VoLTE and non-VoLTE voice calls. The findings highlight the analysis of real-time network characteristics, such as the call setup delay calculation, battery-saving performance, and DRX mechanism. The study contributes to the understanding of network optimization strategies and provides insights for enhancing the quality of service (QoS) in mobile communication networks. Examining VoLTE and non-VoLTE operations, this research highlights the substantial energy savings obtained by VoLTE. Specifically, VoLTE saves approximately 60.76% of energy before the Service Request and approximately 38.97% of energy after the Service Request. Moreover, VoLTE to VoLTE calls have a 72.6% faster call setup delay than non-VoLTE-based LTE to LTE calls, because of fewer signaling messages required. Furthermore, as compared to non-VoLTE to non-VoLTE calls, VoLTE to non-VoLTE calls offer an 18.6% faster call setup delay. These results showcase the performance advantages of VoLTE and reinforce its potential for offering better services in wireless communication networks.
</details>
<details>
<summary>摘要</summary>
网络性能优化对于通过标准无线通信技术提供服务是非常重要。启动延迟和用户设备（UE）电池寿命具有重要影响力。提高这些因素对于确保优质服务的提供是非常重要。与传统的循环连接语音电话相比，VoLTE（音声在LTE）技术提供了更快的启动延迟和更好的电池寿命性能。为了证明这些声明，我们使用了XCAL驱动测试工具来收集实时网络参数详细信息在VoLTE和非VoLTE语音电话中。研究结果显示了实时网络特性的分析，包括启动延迟计算、电池寿命性能和DRX机制。这项研究对网络优化策略的理解和服务质量（QoS）的提高做出了贡献。对VoLTE和非VoLTE操作进行比较，这项研究显示了VoLTE可以获得约60.76%的电源储存和约38.97%的电源储存。此外，VoLTE到VoLTE电话的启动延迟比非VoLTE基于LTE到LTE电话的启动延迟更快，这是因为VoLTE需要 fewer signaling messages。此外，VoLTE到非VoLTE电话的启动延迟比非VoLTE到非VoLTE电话的启动延迟更快，这是因为VoLTE需要更少的信号处理。这些结果显示了VoLTE的性能优势，并证明它在无线通信网络中可以提供更好的服务。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/24/eess.AS_2023_07_24/" data-id="cloqtaey0010bgh8873pp0z2f" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_07_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/24/cs.CV_2023_07_24/" class="article-date">
  <time datetime="2023-07-24T13:00:00.000Z" itemprop="datePublished">2023-07-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/24/cs.CV_2023_07_24/">cs.CV - 2023-07-24</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Automotive-Object-Detection-via-Learning-Sparse-Events-by-Temporal-Dynamics-of-Spiking-Neurons"><a href="#Automotive-Object-Detection-via-Learning-Sparse-Events-by-Temporal-Dynamics-of-Spiking-Neurons" class="headerlink" title="Automotive Object Detection via Learning Sparse Events by Temporal Dynamics of Spiking Neurons"></a>Automotive Object Detection via Learning Sparse Events by Temporal Dynamics of Spiking Neurons</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12900">http://arxiv.org/abs/2307.12900</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hu Zhang, Luziwei Leng, Kaiwei Che, Qian Liu, Jie Cheng, Qinghai Guo, Jiangxing Liao, Ran Cheng</li>
<li>for: 这篇论文旨在应用事件驱动神经网络（SNN）来进行事件基数据的物体探测。</li>
<li>methods: 这篇论文使用了SNN的传入层、遗传层和调节层，并使用了脉幅动力学来调节网络活动。</li>
<li>results: 这篇论文使用了SNN取得了47.7%的精确率（map50）在Gen1标准dataset上，比前一代SNN提高9.7%，并且比使用注意力机制的对照模型还要好。<details>
<summary>Abstract</summary>
Event-based sensors, with their high temporal resolution (1us) and dynamical range (120dB), have the potential to be deployed in high-speed platforms such as vehicles and drones. However, the highly sparse and fluctuating nature of events poses challenges for conventional object detection techniques based on Artificial Neural Networks (ANNs). In contrast, Spiking Neural Networks (SNNs) are well-suited for representing event-based data due to their inherent temporal dynamics. In particular, we demonstrate that the membrane potential dynamics can modulate network activity upon fluctuating events and strengthen features of sparse input. In addition, the spike-triggered adaptive threshold can stabilize training which further improves network performance. Based on this, we develop an efficient spiking feature pyramid network for event-based object detection. Our proposed SNN outperforms previous SNNs and sophisticated ANNs with attention mechanisms, achieving a mean average precision (map50) of 47.7% on the Gen1 benchmark dataset. This result significantly surpasses the previous best SNN by 9.7% and demonstrates the potential of SNNs for event-based vision. Our model has a concise architecture while maintaining high accuracy and much lower computation cost as a result of sparse computation. Our code will be publicly available.
</details>
<details>
<summary>摘要</summary>
Event-based 感测器，具有高度的时间分辨率（1us）和动态范围（120dB），有可能在高速平台 such as 车辆和无人机上进行部署。然而，事件的高度稀疏和波动性带来了传统的人工神经网络（ANNs）中的挑战。相比之下，脉冲神经网络（SNNs）因其内置的时间动力学而适用于表示事件基本数据。具体来说，我们表明了膜电位动力学可以在事件波动时调整网络活动，并强制特征的稀疏输入。此外，使用触发适应阈值可以稳定训练，从而进一步提高网络性能。基于这些，我们开发了高效的脉冲特征峰网络，用于事件基本对象检测。我们的提出的 SNN 在 Gen1 测试集上达到了47.7%的 Mean Average Precision（map50），这比前一代最佳 SNN 高出9.7%，并证明了 SNN 在事件基本视觉中的潜力。我们的模型具有简洁的架构，同时保持高度准确和较低的计算成本。我们的代码将公开。
</details></li>
</ul>
<hr>
<h2 id="Data-free-Black-box-Attack-based-on-Diffusion-Model"><a href="#Data-free-Black-box-Attack-based-on-Diffusion-Model" class="headerlink" title="Data-free Black-box Attack based on Diffusion Model"></a>Data-free Black-box Attack based on Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12872">http://arxiv.org/abs/2307.12872</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingwen Shao, Lingzhuang Meng, Yuanjian Qiao, Lixu Zhang, Wangmeng Zuo</li>
<li>for: 增强数据隐身攻击的效率和准确性，使用扩散模型生成数据来训练代理模型。</li>
<li>methods: 使用扩散模型生成数据，并提出了干扰码修饰（LCA）方法来指导扩散模型生成数据，使得生成的数据可以更好地满足目标模型的批判标准。</li>
<li>results: 通过使用干扰码修饰方法，可以使扩散模型生成的数据更加符合目标模型的批判标准，并且可以提高黑盒攻击的成功率和减少查询预算。<details>
<summary>Abstract</summary>
Since the training data for the target model in a data-free black-box attack is not available, most recent schemes utilize GANs to generate data for training substitute model. However, these GANs-based schemes suffer from low training efficiency as the generator needs to be retrained for each target model during the substitute training process, as well as low generation quality. To overcome these limitations, we consider utilizing the diffusion model to generate data, and propose a data-free black-box attack scheme based on diffusion model to improve the efficiency and accuracy of substitute training. Despite the data generated by the diffusion model exhibits high quality, it presents diverse domain distributions and contains many samples that do not meet the discriminative criteria of the target model. To further facilitate the diffusion model to generate data suitable for the target model, we propose a Latent Code Augmentation (LCA) method to guide the diffusion model in generating data. With the guidance of LCA, the data generated by the diffusion model not only meets the discriminative criteria of the target model but also exhibits high diversity. By utilizing this data, it is possible to train substitute model that closely resemble the target model more efficiently. Extensive experiments demonstrate that our LCA achieves higher attack success rates and requires fewer query budgets compared to GANs-based schemes for different target models.
</details>
<details>
<summary>摘要</summary>
由于目标模型的训练数据不可获取，最近的方案多利用GANs生成数据进行训练代理模型。然而，这些GANs基本方案受到低训练效率和低生成质量的限制。为了突破这些限制，我们考虑使用扩散模型来生成数据，并提出了基于扩散模型的数据自由黑盒攻击方案，以提高代理训练的效率和准确率。尽管扩散模型生成的数据具有高质量，但它们具有多样化的领域分布和含有大量不符合目标模型的检准标准的样本。为了更好地使用扩散模型生成数据，我们提出了秘密代码增强（LCA）方法，以导引扩散模型生成数据。通过LCA的引导，扩散模型生成的数据不仅符合目标模型的检准标准，而且具有高多样性。通过利用这些数据，我们可以更高效地训练代理模型，并且需要更少的查询预算。我们的LCA在不同的目标模型上实现了更高的攻击成功率和更低的查询预算。
</details></li>
</ul>
<hr>
<h2 id="Understanding-the-Latent-Space-of-Diffusion-Models-through-the-Lens-of-Riemannian-Geometry"><a href="#Understanding-the-Latent-Space-of-Diffusion-Models-through-the-Lens-of-Riemannian-Geometry" class="headerlink" title="Understanding the Latent Space of Diffusion Models through the Lens of Riemannian Geometry"></a>Understanding the Latent Space of Diffusion Models through the Lens of Riemannian Geometry</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12868">http://arxiv.org/abs/2307.12868</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yong-Hyun Park, Mingi Kwon, Jaewoong Choi, Junghyo Jo, Youngjung Uh</li>
<li>for: 这 paper 旨在更好地理解Diffusion Model（DM）的潜在空间，从几何学角度进行分析。</li>
<li>methods: 这 paper 使用了 pullback 度量来找到 $\mathcal{X}$ 中的本地几何基底和其对应的 $\mathcal{H}$ 中的中间特征图。</li>
<li>results: 这 paper 发现了一种不supervised image editing能力，通过 traversal 在 $\mathbf{x}$ 空间中。此外，paper 还分析了这些结构如何随 diffusion 步骤的变化，以及如何基于文本条件进行修改。<details>
<summary>Abstract</summary>
Despite the success of diffusion models (DMs), we still lack a thorough understanding of their latent space. To understand the latent space $\mathbf{x}_t \in \mathcal{X}$, we analyze them from a geometrical perspective. Specifically, we utilize the pullback metric to find the local latent basis in $\mathcal{X}$ and their corresponding local tangent basis in $\mathcal{H}$, the intermediate feature maps of DMs. The discovered latent basis enables unsupervised image editing capability through latent space traversal. We investigate the discovered structure from two perspectives. First, we examine how geometric structure evolves over diffusion timesteps. Through analysis, we show that 1) the model focuses on low-frequency components early in the generative process and attunes to high-frequency details later; 2) At early timesteps, different samples share similar tangent spaces; and 3) The simpler datasets that DMs trained on, the more consistent the tangent space for each timestep. Second, we investigate how the geometric structure changes based on text conditioning in Stable Diffusion. The results show that 1) similar prompts yield comparable tangent spaces; and 2) the model depends less on text conditions in later timesteps. To the best of our knowledge, this paper is the first to present image editing through $\mathbf{x}$-space traversal and provide thorough analyses of the latent structure of DMs.
</details>
<details>
<summary>摘要</summary>
不withstanding the success of diffusion models (DMs), we still lack a thorough understanding of their latent space. To understand the latent space $\mathbf{x}_t \in \mathcal{X}$, we analyze them from a geometrical perspective. Specifically, we utilize the pullback metric to find the local latent basis in $\mathcal{X}$ and their corresponding local tangent basis in $\mathcal{H}$, the intermediate feature maps of DMs. The discovered latent basis enables unsupervised image editing capability through latent space traversal. We investigate the discovered structure from two perspectives. First, we examine how geometric structure evolves over diffusion timesteps. Through analysis, we show that 1) the model focuses on low-frequency components early in the generative process and attunes to high-frequency details later; 2) At early timesteps, different samples share similar tangent spaces; and 3) The simpler datasets that DMs trained on, the more consistent the tangent space for each timestep. Second, we investigate how the geometric structure changes based on text conditioning in Stable Diffusion. The results show that 1) similar prompts yield comparable tangent spaces; and 2) the model depends less on text conditions in later timesteps. To the best of our knowledge, this paper is the first to present image editing through $\mathbf{x}$-space traversal and provide thorough analyses of the latent structure of DMs.Note: The translation is in Simplified Chinese, which is the standardized form of Chinese used in mainland China and Singapore. The traditional Chinese form of the translation is slightly different due to the difference in word order and character usage.
</details></li>
</ul>
<hr>
<h2 id="Treatment-Outcome-Prediction-for-Intracerebral-Hemorrhage-via-Generative-Prognostic-Model-with-Imaging-and-Tabular-Data"><a href="#Treatment-Outcome-Prediction-for-Intracerebral-Hemorrhage-via-Generative-Prognostic-Model-with-Imaging-and-Tabular-Data" class="headerlink" title="Treatment Outcome Prediction for Intracerebral Hemorrhage via Generative Prognostic Model with Imaging and Tabular Data"></a>Treatment Outcome Prediction for Intracerebral Hemorrhage via Generative Prognostic Model with Imaging and Tabular Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12858">http://arxiv.org/abs/2307.12858</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/med-air/top-gpm">https://github.com/med-air/top-gpm</a></li>
<li>paper_authors: Wenao Ma, Cheng Chen, Jill Abrigo, Calvin Hoi-Kwan Mak, Yuqi Gong, Nga Yan Chan, Chu Han, Zaiyi Liu, Qi Dou</li>
<li>For: 预测 интра脑出血(ICH) 治疗结果* Methods: 使用 imaging 和 tabular 数据建立报告模型，并使用 variational autoencoder 模型生成低维度预测分数，以Address selection bias* Results: 对实际临床数据进行了广泛的实验，并显示了substantial improvement 在治疗结果预测 compared to 现有的状态 искусственный智能方法。Here’s the breakdown of each point:* For: 预测 ICH 治疗结果 (What the paper is written for)* Methods: 使用 imaging 和 tabular 数据建立报告模型，并使用 variational autoencoder 模型生成低维度预测分数，以Address selection bias (What methods the paper uses)* Results: 对实际临床数据进行了广泛的实验，并显示了substantial improvement 在治疗结果预测 compared to 现有的状态 искусственный智能方法。 (What results the paper gets)<details>
<summary>Abstract</summary>
Intracerebral hemorrhage (ICH) is the second most common and deadliest form of stroke. Despite medical advances, predicting treat ment outcomes for ICH remains a challenge. This paper proposes a novel prognostic model that utilizes both imaging and tabular data to predict treatment outcome for ICH. Our model is trained on observational data collected from non-randomized controlled trials, providing reliable predictions of treatment success. Specifically, we propose to employ a variational autoencoder model to generate a low-dimensional prognostic score, which can effectively address the selection bias resulting from the non-randomized controlled trials. Importantly, we develop a variational distributions combination module that combines the information from imaging data, non-imaging clinical data, and treatment assignment to accurately generate the prognostic score. We conducted extensive experiments on a real-world clinical dataset of intracerebral hemorrhage. Our proposed method demonstrates a substantial improvement in treatment outcome prediction compared to existing state-of-the-art approaches. Code is available at https://github.com/med-air/TOP-GPM
</details>
<details>
<summary>摘要</summary>
中风血盖（ICH）是第二常见且最致命的 stroke 型。医学进步不withstanding，预测 IC 的治疗结果仍然是一大挑战。本文提出了一种新的 прогности 模型，该模型利用了 Both imaging 和 tabular data 预测 IC 的治疗结果。我们的模型在非随机化控制试验中收集的 observational 数据上进行训练，以提供可靠的治疗成功预测。特别是，我们提出了一种 variational autoencoder 模型，用于生成低维度的 прогности 分数，以有效地 Addressing the selection bias  resulting from non-randomized controlled trials。我们还开发了一种 variational distributions combination module，用于组合 imaging 数据、非 imaging клиниче数据和治疗分配信息，以准确生成 prognostic 分数。我们对实际患有中风血盖的临床数据进行了广泛的实验，并证明了我们的提出方法可以具有显著改善的治疗结果预测效果，比现有的状态 искусственный智能方法更好。代码可以在 https://github.com/med-air/TOP-GPM 上找到。
</details></li>
</ul>
<hr>
<h2 id="Multiscale-Video-Pretraining-for-Long-Term-Activity-Forecasting"><a href="#Multiscale-Video-Pretraining-for-Long-Term-Activity-Forecasting" class="headerlink" title="Multiscale Video Pretraining for Long-Term Activity Forecasting"></a>Multiscale Video Pretraining for Long-Term Activity Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12854">http://arxiv.org/abs/2307.12854</a></li>
<li>repo_url: None</li>
<li>paper_authors: Reuben Tan, Matthias De Lange, Michael Iuzzolino, Bryan A. Plummer, Kate Saenko, Karl Ridgeway, Lorenzo Torresani</li>
<li>for: 预测人类活动长期趋势，提高机器学习模型对未看过数据的泛化能力。</li>
<li>methods: 提出了一种新的自我监督预训练方法—多 scales video pretraining (MVP)，通过学习预测视频片断的上下文化表示来学习Robust Representation。</li>
<li>results: 与现有方法进行比较，MVP在长期动作预测和视频概要预测任务上表现出了显著的性能优势，对于视频概要预测任务，MVP的表现提高了Relative Performance Gain超过20%。<details>
<summary>Abstract</summary>
Long-term activity forecasting is an especially challenging research problem because it requires understanding the temporal relationships between observed actions, as well as the variability and complexity of human activities. Despite relying on strong supervision via expensive human annotations, state-of-the-art forecasting approaches often generalize poorly to unseen data. To alleviate this issue, we propose Multiscale Video Pretraining (MVP), a novel self-supervised pretraining approach that learns robust representations for forecasting by learning to predict contextualized representations of future video clips over multiple timescales. MVP is based on our observation that actions in videos have a multiscale nature, where atomic actions typically occur at a short timescale and more complex actions may span longer timescales. We compare MVP to state-of-the-art self-supervised video learning approaches on downstream long-term forecasting tasks including long-term action anticipation and video summary prediction. Our comprehensive experiments across the Ego4D and Epic-Kitchens-55/100 datasets demonstrate that MVP out-performs state-of-the-art methods by significant margins. Notably, MVP obtains a relative performance gain of over 20% accuracy in video summary forecasting over existing methods.
</details>
<details>
<summary>摘要</summary>
长期活动预测是一个特别困难的研究问题，因为它需要理解视频中观察到的动作之间的时间关系，以及人类活动的多样性和复杂性。尽管通过昂贵的人工纠正，使用现状的预测方法可以取得良好的性能，但这些方法在未看到的数据上 generalize 很差。为了解决这个问题，我们提出了多Scale Video Pretraining（MVP），一种新的自我监督预训练方法，该方法通过学习预测视频clip的多Scale representation来学习Robust的表示。MVP基于我们观察到的动作在视频中有多Scale性质， atomic 动作通常发生在短时间尺度，而更复杂的动作可能 span 更长的时间尺度。我们与现状的自我监督视频学习方法进行比较，在下游长期预测任务中，包括长期动作预测和视频概要预测。我们在Ego4D和Epic-Kitchens-55/100 datasets上进行了广泛的实验，结果表明MVP在性能上超过了现状的方法，具体来说，MVP在视频概要预测任务中取得了20%以上的相对性能提升。
</details></li>
</ul>
<hr>
<h2 id="Spatiotemporal-Modeling-Encounters-3D-Medical-Image-Analysis-Slice-Shift-UNet-with-Multi-View-Fusion"><a href="#Spatiotemporal-Modeling-Encounters-3D-Medical-Image-Analysis-Slice-Shift-UNet-with-Multi-View-Fusion" class="headerlink" title="Spatiotemporal Modeling Encounters 3D Medical Image Analysis: Slice-Shift UNet with Multi-View Fusion"></a>Spatiotemporal Modeling Encounters 3D Medical Image Analysis: Slice-Shift UNet with Multi-View Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12853">http://arxiv.org/abs/2307.12853</a></li>
<li>repo_url: None</li>
<li>paper_authors: C. I. Ugwu, S. Casarin, O. Lanz</li>
<li>for: 这paper的目的是提出一种基于2DConvolutional Neural Networks的多模态脏器部分 segmentation方法，以提高诊断和治疗的效率和准确性。</li>
<li>methods: 这paper使用了一种基于多视图的2DConvolutional Neural Networks方法，通过在多个方向上进行2D convolution来学习多维特征，并通过卷积缓存共享机制来重新inteporate第三维信息。</li>
<li>results: 对于Multi-Modality Abdominal Multi-Organ Segmentation (AMOS)和Multi-Atlas Labeling Beyond the Cranial Vault (BTCV)数据集，这paper的方法得到了较高的效率和相当于顶尖模型的性能。<details>
<summary>Abstract</summary>
As a fundamental part of computational healthcare, Computer Tomography (CT) and Magnetic Resonance Imaging (MRI) provide volumetric data, making the development of algorithms for 3D image analysis a necessity. Despite being computationally cheap, 2D Convolutional Neural Networks can only extract spatial information. In contrast, 3D CNNs can extract three-dimensional features, but they have higher computational costs and latency, which is a limitation for clinical practice that requires fast and efficient models. Inspired by the field of video action recognition we propose a new 2D-based model dubbed Slice SHift UNet (SSH-UNet) which encodes three-dimensional features at 2D CNN's complexity. More precisely multi-view features are collaboratively learned by performing 2D convolutions along the three orthogonal planes of a volume and imposing a weights-sharing mechanism. The third dimension, which is neglected by the 2D convolution, is reincorporated by shifting a portion of the feature maps along the slices' axis. The effectiveness of our approach is validated in Multi-Modality Abdominal Multi-Organ Segmentation (AMOS) and Multi-Atlas Labeling Beyond the Cranial Vault (BTCV) datasets, showing that SSH-UNet is more efficient while on par in performance with state-of-the-art architectures.
</details>
<details>
<summary>摘要</summary>
为computational healthcare的基本部分，计算机断层成像（CT）和核磁共振成像（MRI）提供了体积数据，因此开发3D图像分析算法是必要的。然而，使用2D卷积神经网络（CNN）只能提取空间信息，而3D CNN则可以提取三维特征，但它们的计算成本和延迟较高，这限制了临床实践中需要快速和高效的模型。 drawing inspiration from the field of video action recognition, we propose a new 2D-based model called Slice SHift UNet (SSH-UNet) that encodes three-dimensional features at the complexity of 2D CNNs. Specifically, we perform 2D convolutions along the three orthogonal planes of a volume and share weights across different planes to collaboratively learn multi-view features. The third dimension, which is neglected by the 2D convolution, is reincorporated by shifting a portion of the feature maps along the slices' axis. Our approach is validated on the Multi-Modality Abdominal Multi-Organ Segmentation (AMOS) and Multi-Atlas Labeling Beyond the Cranial Vault (BTCV) datasets, showing that SSH-UNet is more efficient while on par in performance with state-of-the-art architectures.
</details></li>
</ul>
<hr>
<h2 id="Multi-View-Vertebra-Localization-and-Identification-from-CT-Images"><a href="#Multi-View-Vertebra-Localization-and-Identification-from-CT-Images" class="headerlink" title="Multi-View Vertebra Localization and Identification from CT Images"></a>Multi-View Vertebra Localization and Identification from CT Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12845">http://arxiv.org/abs/2307.12845</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shanghaitech-impact/multi-view-vertebra-localization-and-identification-from-ct-images">https://github.com/shanghaitech-impact/multi-view-vertebra-localization-and-identification-from-ct-images</a></li>
<li>paper_authors: Han Wu, Jiadong Zhang, Yu Fang, Zhentao Liu, Nizhuan Wang, Zhiming Cui, Dinggang Shen</li>
<li>for: 本研究旨在提高CT图像中骨架的准确位置和识别率，为各种临床应用提供技术支持。</li>
<li>methods: 本研究提出了一种基于多视图的骨架localization和识别方法，将3D问题转化为2D的本地化和识别任务。而无需裁剪patch操作，我们的方法可以自然地学习多视图全局信息。此外，为更好地捕捉不同视角的骨架结构信息，我们还提出了一种多视图对比学习策略来预训练后处理网络。</li>
<li>results: 我们的方法可以在只使用两个2D网络的情况下，准确地位置和识别骨架在CT图像中。并且与状态艺术方法相比，我们的方法显著超越了其表现。代码可以在<a target="_blank" rel="noopener" href="https://github.com/ShanghaiTech-IMPACT/Multi-View-Vertebra-Localization-and-Identification-from-CT-Images%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ShanghaiTech-IMPACT/Multi-View-Vertebra-Localization-and-Identification-from-CT-Images上获取。</a><details>
<summary>Abstract</summary>
Accurately localizing and identifying vertebrae from CT images is crucial for various clinical applications. However, most existing efforts are performed on 3D with cropping patch operation, suffering from the large computation costs and limited global information. In this paper, we propose a multi-view vertebra localization and identification from CT images, converting the 3D problem into a 2D localization and identification task on different views. Without the limitation of the 3D cropped patch, our method can learn the multi-view global information naturally. Moreover, to better capture the anatomical structure information from different view perspectives, a multi-view contrastive learning strategy is developed to pre-train the backbone. Additionally, we further propose a Sequence Loss to maintain the sequential structure embedded along the vertebrae. Evaluation results demonstrate that, with only two 2D networks, our method can localize and identify vertebrae in CT images accurately, and outperforms the state-of-the-art methods consistently. Our code is available at https://github.com/ShanghaiTech-IMPACT/Multi-View-Vertebra-Localization-and-Identification-from-CT-Images.
</details>
<details>
<summary>摘要</summary>
通过CT图像进行精准的骨vertebrae的本地化和识别是许多临床应用中的关键。然而，大多数现有的尝试都是在3D中进行，通过剪辑patch操作，它们受到了大量计算成本和局部信息的限制。在这篇论文中，我们提出了基于多视图的骨vertebrae本地化和识别方法，将3D问题转化为2D本地化和识别任务。不同于剪辑patch的限制，我们的方法可以自然地学习多视图的全局信息。此外，为了更好地捕捉不同视图角度的骨 vertebrae结构信息，我们开发了一种多视图冲击学习策略来预训练干部。此外，我们还提出了一种序列损失来保持骨vertebrae中的顺序结构嵌入。评估结果表明，只有两个2D网络，我们的方法可以在CT图像中精准地本地化和识别骨vertebrae，并在状态艺术方法上一直保持领先。我们的代码可以在https://github.com/ShanghaiTech-IMPACT/Multi-View-Vertebra-Localization-and-Identification-from-CT-Images中找到。
</details></li>
</ul>
<hr>
<h2 id="Learning-Provably-Robust-Estimators-for-Inverse-Problems-via-Jittering"><a href="#Learning-Provably-Robust-Estimators-for-Inverse-Problems-via-Jittering" class="headerlink" title="Learning Provably Robust Estimators for Inverse Problems via Jittering"></a>Learning Provably Robust Estimators for Inverse Problems via Jittering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12822">http://arxiv.org/abs/2307.12822</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mli-lab/robust_reconstructors_via_jittering">https://github.com/mli-lab/robust_reconstructors_via_jittering</a></li>
<li>paper_authors: Anselm Krainovic, Mahdi Soltanolkotabi, Reinhard Heckel</li>
<li>for: 本研究 investigate whether jittering, a simple regularization technique, can be used to train efficient worst-case robust deep neural networks for inverse problems.</li>
<li>methods: 本研究使用了一种简单的正则化技术，即在训练时添加均匀的高斯噪声，来提高深度神经网络的最坏情况Robustness。</li>
<li>results: 研究发现，使用jittering可以实现最佳的 $\ell_2$-worst-case robust estimator for linear denoising,并且通过训练深度神经网络（U-net）对自然图像减 noise、deconvolution和加速Magnetic Resonance Imaging（MRI）进行了实验，结果表明，jittering可以增强最坏情况Robustness，但可能不适用于 inverse problems beyond denoising。此外，研究还发现，训练在真实数据上，通常包含一些噪声，可以提高模型的Robustness。<details>
<summary>Abstract</summary>
Deep neural networks provide excellent performance for inverse problems such as denoising. However, neural networks can be sensitive to adversarial or worst-case perturbations. This raises the question of whether such networks can be trained efficiently to be worst-case robust. In this paper, we investigate whether jittering, a simple regularization technique that adds isotropic Gaussian noise during training, is effective for learning worst-case robust estimators for inverse problems. While well studied for prediction in classification tasks, the effectiveness of jittering for inverse problems has not been systematically investigated. In this paper, we present a novel analytical characterization of the optimal $\ell_2$-worst-case robust estimator for linear denoising and show that jittering yields optimal robust denoisers. Furthermore, we examine jittering empirically via training deep neural networks (U-nets) for natural image denoising, deconvolution, and accelerated magnetic resonance imaging (MRI). The results show that jittering significantly enhances the worst-case robustness, but can be suboptimal for inverse problems beyond denoising. Moreover, our results imply that training on real data which often contains slight noise is somewhat robustness enhancing.
</details>
<details>
<summary>摘要</summary>
深度神经网络在逆问题中提供了优秀的性能，但神经网络可能会受到敌意或最坏情况的攻击。这引发了我们是否可以有效地训练神经网络以适应最坏情况的问题。在这篇论文中，我们研究了加入 ISO 型 Gaussian 噪声 durante 训练是否可以有效地学习最坏情况Robust 估计器。虽然这种技术在预测 classification 任务中已经得到了广泛的研究，但对于逆问题的研究尚未得到系统的探讨。在这篇论文中，我们提出了一种新的分析方法，用于计算最佳 $\ell_2$ 最坏情况Robust 估计器。我们发现，在线性噪声降解 задании下，加入噪声可以实现最佳的Robust 性能。此外，我们通过训练深度神经网络（U-net）进行自然图像降解、减 convolution 和加速核磁共振成像（MRI）任务，实验结果表明，加入噪声可以显著提高最坏情况Robust 性能，但在涉及到逆问题的更高级别上可能不是最佳的方法。此外，我们的结果还 imply 训练在真实数据上，常常包含一定的噪声，可能会有一定的 robustness 提升。
</details></li>
</ul>
<hr>
<h2 id="Exposing-the-Troublemakers-in-Described-Object-Detection"><a href="#Exposing-the-Troublemakers-in-Described-Object-Detection" class="headerlink" title="Exposing the Troublemakers in Described Object Detection"></a>Exposing the Troublemakers in Described Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12813">http://arxiv.org/abs/2307.12813</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shikras/d-cube">https://github.com/shikras/d-cube</a></li>
<li>paper_authors: Chi Xie, Zhao Zhang, Yixuan Wu, Feng Zhu, Rui Zhao, Shuang Liang</li>
<li>for: 本研究旨在提高Open-Vocabulary object Detection (OVD)和Referring Expression Comprehension (REC)的实际应用，通过扩展分类名称为描述语言表达来提高DOD任务的研究基础。</li>
<li>methods: 我们采用了Description Detection Dataset ($D^3$)，其特点是包含flexible language expressions和全面的描述对象标注，并评估了之前的SOTA方法在$D^3$上的性能。此外，我们还提出了一种基线方法，通过重新构建训练数据和引入二分类子任务来大幅提高REC方法的性能。</li>
<li>results: 我们发现现有REC方法在$D^3$上存在许多问题，包括信任分数、拒绝负例、多目标场景等。此外，最新的bi-functional方法也不太适用于DOD任务，因为它们在训练和测试过程中分离的程序和推理策略。我们的基线方法在$D^3$上表现出色，大幅超越现有方法。<details>
<summary>Abstract</summary>
Detecting objects based on language descriptions is a popular task that includes Open-Vocabulary object Detection (OVD) and Referring Expression Comprehension (REC). In this paper, we advance them to a more practical setting called Described Object Detection (DOD) by expanding category names to flexible language expressions for OVD and overcoming the limitation of REC to only grounding the pre-existing object. We establish the research foundation for DOD tasks by constructing a Description Detection Dataset ($D^3$), featuring flexible language expressions and annotating all described objects without omission. By evaluating previous SOTA methods on $D^3$, we find some troublemakers that fail current REC, OVD, and bi-functional methods. REC methods struggle with confidence scores, rejecting negative instances, and multi-target scenarios, while OVD methods face constraints with long and complex descriptions. Recent bi-functional methods also do not work well on DOD due to their separated training procedures and inference strategies for REC and OVD tasks. Building upon the aforementioned findings, we propose a baseline that largely improves REC methods by reconstructing the training data and introducing a binary classification sub-task, outperforming existing methods. Data and code is available at https://github.com/shikras/d-cube.
</details>
<details>
<summary>摘要</summary>
检测基于语言描述的对象是一个受欢迎的任务，包括开放词汇对象检测（OVD）和引用表达理解（REC）。在这篇论文中，我们将其推进到更实用的设定中，称为描述对象检测（DOD），通过扩展类别名称到 flexible language expression 来进一步提高 OVD 的精度。我们建立了描述检测任务的研究基础，constructing a Description Detection Dataset ($D^3$), featuring flexible language expressions and annotating all described objects without omission。通过评估先前的 SOTA 方法在 $D^3$ 上，我们发现了一些“困难者”，其中 REC 方法受到负样本、多目标场景和 confidence score 等限制，而 OVD 方法则面临长和复杂的描述句所带来的限制。最近的 bi-functional 方法也不太适合 DOD 任务，因为它们在 REC 和 OVD 任务之间具有分开的训练过程和推理策略。基于以上发现，我们提出了一个基线方案，可以大幅提高 REC 方法的性能，通过重新构建训练数据和引入 binary classification 子任务。数据和代码可以在 https://github.com/shikras/d-cube 上获取。
</details></li>
</ul>
<hr>
<h2 id="Compact-Capable-Harnessing-Graph-Neural-Networks-and-Edge-Convolution-for-Medical-Image-Classification"><a href="#Compact-Capable-Harnessing-Graph-Neural-Networks-and-Edge-Convolution-for-Medical-Image-Classification" class="headerlink" title="Compact &amp; Capable: Harnessing Graph Neural Networks and Edge Convolution for Medical Image Classification"></a>Compact &amp; Capable: Harnessing Graph Neural Networks and Edge Convolution for Medical Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12790">http://arxiv.org/abs/2307.12790</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anonrepo-keeper/gcnn-ec">https://github.com/anonrepo-keeper/gcnn-ec</a></li>
<li>paper_authors: Aryan Singh, Pepijn Van de Ven, Ciarán Eising, Patrick Denny</li>
<li>for: 这个研究探讨了图像分类 tasks 上的图形学习模型，具体来说是使用 Graph Neural Networks (GNNs) 和 edge convolution 来强化图像之间的连接。</li>
<li>methods: 本研究提出了一个新的 GNN 模型，融合了 GNNs 和 edge convolution，以利用 RGB 通道特征值之间的连接来强化图像的表现。</li>
<li>results: 比较 GNN 和 pre-trained DNNs，GNN 能够在 MedMNIST 数据集上显示出与 DNNs 相似的表现，并且仅需要1000个参数，相较于 DNNs 的训练时间和数据量。<details>
<summary>Abstract</summary>
Graph-based neural network models are gaining traction in the field of representation learning due to their ability to uncover latent topological relationships between entities that are otherwise challenging to identify. These models have been employed across a diverse range of domains, encompassing drug discovery, protein interactions, semantic segmentation, and fluid dynamics research. In this study, we investigate the potential of Graph Neural Networks (GNNs) for medical image classification. We introduce a novel model that combines GNNs and edge convolution, leveraging the interconnectedness of RGB channel feature values to strongly represent connections between crucial graph nodes. Our proposed model not only performs on par with state-of-the-art Deep Neural Networks (DNNs) but does so with 1000 times fewer parameters, resulting in reduced training time and data requirements. We compare our Graph Convolutional Neural Network (GCNN) to pre-trained DNNs for classifying MedMNIST dataset classes, revealing promising prospects for GNNs in medical image analysis. Our results also encourage further exploration of advanced graph-based models such as Graph Attention Networks (GAT) and Graph Auto-Encoders in the medical imaging domain. The proposed model yields more reliable, interpretable, and accurate outcomes for tasks like semantic segmentation and image classification compared to simpler GCNNs
</details>
<details>
<summary>摘要</summary>
“基于图的神经网络模型在 Representation Learning 领域受到广泛应用，因为它们可以捕捉到难以识别的实体之间的隐藏 topological 关系。这些模型在药物发现、蛋白结合、 semantic segmentation 和 fluid dynamics 等领域都有应用。在这项研究中，我们研究了医学图像分类中 Graph Neural Networks（GNNs）的潜力。我们提出了一种新的模型，它将 GNNs 和边 convolution 结合起来，利用 RGB 通道特征值之间的连接来强大地表示图像中关键节点之间的连接。我们的提出的模型不仅与 state-of-the-art Deep Neural Networks（DNNs）的性能相当，而且只需要1000倍 fewer 参数，从而减少了训练时间和数据需求。我们对 MedMNIST 数据集类型进行比较，发现 GNNs 在医学图像分类中有极好的前景。我们的结果还鼓励了在医学图像分类和 semantic segmentation 等任务中进一步探索更高级别的图基于模型，如 Graph Attention Networks（GAT）和 Graph Auto-Encoders。我们的模型在 semantic segmentation 和图像分类任务中比 simpler GNNs 更可靠、更加可解释、更高精度。”
</details></li>
</ul>
<hr>
<h2 id="Fast-Full-frame-Video-Stabilization-with-Iterative-Optimization"><a href="#Fast-Full-frame-Video-Stabilization-with-Iterative-Optimization" class="headerlink" title="Fast Full-frame Video Stabilization with Iterative Optimization"></a>Fast Full-frame Video Stabilization with Iterative Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12774">http://arxiv.org/abs/2307.12774</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiyue Zhao, Xin Li, Zhan Peng, Xianrui Luo, Xinyi Ye, Hao Lu, Zhiguo Cao</li>
<li>for: 提高视频稳定性和计算效率的交互优化方法</li>
<li>methods: 使用Synthetic datasets、两级（粗到细）稳定算法、可信度地图和反射推断等技术</li>
<li>results: 提供了高效且视觉质量高的视频稳定方法，并通过实验证明其在计算速度和视觉质量两个方面的优势<details>
<summary>Abstract</summary>
Video stabilization refers to the problem of transforming a shaky video into a visually pleasing one. The question of how to strike a good trade-off between visual quality and computational speed has remained one of the open challenges in video stabilization. Inspired by the analogy between wobbly frames and jigsaw puzzles, we propose an iterative optimization-based learning approach using synthetic datasets for video stabilization, which consists of two interacting submodules: motion trajectory smoothing and full-frame outpainting. First, we develop a two-level (coarse-to-fine) stabilizing algorithm based on the probabilistic flow field. The confidence map associated with the estimated optical flow is exploited to guide the search for shared regions through backpropagation. Second, we take a divide-and-conquer approach and propose a novel multiframe fusion strategy to render full-frame stabilized views. An important new insight brought about by our iterative optimization approach is that the target video can be interpreted as the fixed point of nonlinear mapping for video stabilization. We formulate video stabilization as a problem of minimizing the amount of jerkiness in motion trajectories, which guarantees convergence with the help of fixed-point theory. Extensive experimental results are reported to demonstrate the superiority of the proposed approach in terms of computational speed and visual quality. The code will be available on GitHub.
</details>
<details>
<summary>摘要</summary>
视频稳定化问题是将晃动视频转化为美观的视频。计算速度和视觉质量之间的平衡问题一直是视频稳定化领域的开放问题。我们提出了基于iterative optimization-based learning方法的视频稳定化方法，该方法包括两个互动子模块：运动轨迹缓和全帧补充。首先，我们开发了一种两级（粗细到细）稳定化算法，基于概率流场。利用估算的光学流场的信息来引导搜索共享区域，我们使用回传propagation。其次，我们提出了一种新的分解和融合策略，以生成稳定视频全帧视图。我们发现，通过iterative optimization方法，目标视频可以被视为非线性映射的稳定点，我们将视频稳定化问题解释为减少运动轨迹中的抖动量，以 garantía converge。我们通过实验报告了我们的方法的计算速度和视觉质量的超越性。代码将在GitHub上发布。
</details></li>
</ul>
<hr>
<h2 id="LiDAR-Meta-Depth-Completion"><a href="#LiDAR-Meta-Depth-Completion" class="headerlink" title="LiDAR Meta Depth Completion"></a>LiDAR Meta Depth Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12761">http://arxiv.org/abs/2307.12761</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wbkit/reslan">https://github.com/wbkit/reslan</a></li>
<li>paper_authors: Wolfgang Boettcher, Lukas Hoyer, Ozan Unal, Ke Li, Dengxin Dai</li>
<li>for: 提高移动自适应系统中的深度估计精度</li>
<li>methods: 动态适应LiDAR扫描模式的深度完成网络</li>
<li>results: 与专业LiDAR模型相比，单个模型可以在多个LiDAR扫描模式上显示更好的性能，并且可以扩展到未经训练的扫描模式<details>
<summary>Abstract</summary>
Depth estimation is one of the essential tasks to be addressed when creating mobile autonomous systems. While monocular depth estimation methods have improved in recent times, depth completion provides more accurate and reliable depth maps by additionally using sparse depth information from other sensors such as LiDAR. However, current methods are specifically trained for a single LiDAR sensor. As the scanning pattern differs between sensors, every new sensor would require re-training a specialized depth completion model, which is computationally inefficient and not flexible. Therefore, we propose to dynamically adapt the depth completion model to the used sensor type enabling LiDAR adaptive depth completion. Specifically, we propose a meta depth completion network that uses data patterns derived from the data to learn a task network to alter weights of the main depth completion network to solve a given depth completion task effectively. The method demonstrates a strong capability to work on multiple LiDAR scanning patterns and can also generalize to scanning patterns that are unseen during training. While using a single model, our method yields significantly better results than a non-adaptive baseline trained on different LiDAR patterns. It outperforms LiDAR-specific expert models for very sparse cases. These advantages allow flexible deployment of a single depth completion model on different sensors, which could also prove valuable to process the input of nascent LiDAR technology with adaptive instead of fixed scanning patterns.
</details>
<details>
<summary>摘要</summary>
深度估算是创建移动自主系统中的一项重要任务。虽然单投射深度估算方法在最近有所进步，但深度完成提供更加准确和可靠的深度地图，并使用其他感知器 such as LiDAR 上的稀疏深度信息。然而，当前的方法都是特定的 LiDAR 扫描模式进行训练。因此，我们提出了动态适应 LiDAR 类型的深度完成模型，以便在不同的 LiDAR 扫描模式上进行适应性的深度完成。具体来说，我们提出了一个元深度完成网络，通过数据模式来学习一个任务网络，以修改主深度完成网络中的权重，以解决给定的深度完成任务。该方法能够在多个 LiDAR 扫描模式上工作，并且还能够对未在训练中看到的扫描模式进行泛化。使用单一模型，我们的方法能够在不同 LiDAR 扫描模式上显著提高结果，并且在非常稀疏的情况下也能够超越特定 LiDAR 模型。这些优势使得我们的方法可以在不同感知器上进行灵活的部署，这也可以证明有用于处理新型 LiDAR 技术的适应式扫描模式。
</details></li>
</ul>
<hr>
<h2 id="ICF-SRSR-Invertible-scale-Conditional-Function-for-Self-Supervised-Real-world-Single-Image-Super-Resolution"><a href="#ICF-SRSR-Invertible-scale-Conditional-Function-for-Self-Supervised-Real-world-Single-Image-Super-Resolution" class="headerlink" title="ICF-SRSR: Invertible scale-Conditional Function for Self-Supervised Real-world Single Image Super-Resolution"></a>ICF-SRSR: Invertible scale-Conditional Function for Self-Supervised Real-world Single Image Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12751">http://arxiv.org/abs/2307.12751</a></li>
<li>repo_url: None</li>
<li>paper_authors: Reyhaneh Neshatavar, Mohsen Yavartanoo, Sanghyun Son, Kyoung Mu Lee</li>
<li>for: 这个论文是为了解决单图超解析（SISR）问题，即将低分辨率（LR）图像提升到高分辨率（HR）图像的问题。</li>
<li>methods: 该论文提出了一种新的归一化可能函数（ICF），可以将输入图像缩放到不同的比例条件下，并且可以还原原始输入图像。基于该ICF，该论文提出了一种新的无监督SISR框架（ICF-SRSR），可以在实际世界中进行SR任务无需使用任何对应&#x2F;无对应的训练数据。</li>
<li>results: 该论文的实验表明，ICF-SRSR可以在无监督情况下处理SISR问题，并且在实际世界中表现更好于使用synthesized paired图像进行训练的方法。此外，ICF-SRSR还可以生成更加真实和可行的LR-HR对，使现有的监督SISR网络更加可靠。<details>
<summary>Abstract</summary>
Single image super-resolution (SISR) is a challenging ill-posed problem that aims to up-sample a given low-resolution (LR) image to a high-resolution (HR) counterpart. Due to the difficulty in obtaining real LR-HR training pairs, recent approaches are trained on simulated LR images degraded by simplified down-sampling operators, e.g., bicubic. Such an approach can be problematic in practice because of the large gap between the synthesized and real-world LR images. To alleviate the issue, we propose a novel Invertible scale-Conditional Function (ICF), which can scale an input image and then restore the original input with different scale conditions. By leveraging the proposed ICF, we construct a novel self-supervised SISR framework (ICF-SRSR) to handle the real-world SR task without using any paired/unpaired training data. Furthermore, our ICF-SRSR can generate realistic and feasible LR-HR pairs, which can make existing supervised SISR networks more robust. Extensive experiments demonstrate the effectiveness of the proposed method in handling SISR in a fully self-supervised manner. Our ICF-SRSR demonstrates superior performance compared to the existing methods trained on synthetic paired images in real-world scenarios and exhibits comparable performance compared to state-of-the-art supervised/unsupervised methods on public benchmark datasets.
</details>
<details>
<summary>摘要</summary>
单图超分辨 (SISR) 是一个具有挑战性的不定性问题，旨在将给定的低分辨 (LR) 图像提升到高分辨 (HR) 对应的图像。由于实际获得LR-HR训练对的困难，现有的方法通常是使用简化下采样算法，如二次方程，进行模拟LR图像。这种方法在实践中可能存在大量的差异，这使得SR任务变得更加困难。为了解决这个问题，我们提议一种新的归一化可逆函数 (ICF)，可以将输入图像缩放，然后将原始输入图像还原，并且可以在不同的比例条件下进行缩放。通过利用我们提议的ICF，我们建立了一种新的自适应SISR框架 (ICF-SRSR)，可以在实际的SR任务中处理不需要使用任何配对/非配对训练数据。此外，我们的ICF-SRSR可以生成真实可行的LR-HR对，这可以使得现有的supervised SISR网络更加强大。我们进行了广泛的实验，并证明了我们的ICF-SRSR可以在不需要任何配对训练数据的情况下 Handle SISR任务。我们的ICF-SRSR在实际场景中表现出了supervised/非配对方法的相当性，并且在公共的benchmark datasets上达到了相当的性能。
</details></li>
</ul>
<hr>
<h2 id="CLIP-KD-An-Empirical-Study-of-Distilling-CLIP-Models"><a href="#CLIP-KD-An-Empirical-Study-of-Distilling-CLIP-Models" class="headerlink" title="CLIP-KD: An Empirical Study of Distilling CLIP Models"></a>CLIP-KD: An Empirical Study of Distilling CLIP Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12732">http://arxiv.org/abs/2307.12732</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/winycg/CLIP-KD">https://github.com/winycg/CLIP-KD</a></li>
<li>paper_authors: Chuanguang Yang, Zhulin An, Libo Huang, Junyu Bi, Xinqiang Yu, Han Yang, Yongjun Xu</li>
<li>for: 本文旨在静态小CLIP模型，通过与大教师CLIP模型的导师学习来提高性能。</li>
<li>methods: 本文提出了多种静态CLIP模型的精简策略，包括关系、特征、梯度和对比方式，以评估其影响CLIP精简。</li>
<li>results: 研究发现最简单的特征模仿与MSE损失最佳，并且交互对比学习和关系基于精简也对性能有益。在应用于多个学生网络，CLIP精简在零shotImageNet分类和跨模态检索benchmark上具有consistent的改进。<details>
<summary>Abstract</summary>
CLIP has become a promising language-supervised visual pre-training framework and achieves excellent performance over a wide range of tasks. This paper aims to distill small CLIP models supervised by a large teacher CLIP model. We propose several distillation strategies, including relation, feature, gradient and contrastive paradigm, to examine the impact on CLIP distillation. We show that the simplest feature mimicry with MSE loss performs best. Moreover, interactive contrastive learning and relation-based distillation are also critical in performance improvement. We apply the unified method to distill several student networks trained on 15 million (image, text) pairs. Distillation improves the student CLIP models consistently over zero-shot ImageNet classification and cross-modal retrieval benchmarks. We hope our empirical study will become an important baseline for future CLIP distillation research. The code is available at \url{https://github.com/winycg/CLIP-KD}.
</details>
<details>
<summary>摘要</summary>
CLIP 已成为一个有前途的语言监督 visual 预训练框架，在各种任务上表现出色。本文目的是将大 teacher CLIP 模型监督小 CLIP 模型进行学习减少。我们提出了多种减少策略，包括关系、特征、梯度和对比方法，以确定 CLIP 减少的影响。我们发现最简单的特征模仿方法使用 MSE 损失最佳。此外，交互式对比学习和关系基于减少也对性能进行了贡献。我们应用这种简化方法到多个学生网络，每个网络都在 15 百万（图像、文本）对的训练下进行学习。减少提高了学生 CLIP 模型在零shot ImageNet 分类和 cross-modal 检索标准准则上的表现。我们希望我们的实验研究将成为未来 CLIP 减少研究的重要基准。代码可以在 \url{https://github.com/winycg/CLIP-KD} 上获取。
</details></li>
</ul>
<hr>
<h2 id="COCO-O-A-Benchmark-for-Object-Detectors-under-Natural-Distribution-Shifts"><a href="#COCO-O-A-Benchmark-for-Object-Detectors-under-Natural-Distribution-Shifts" class="headerlink" title="COCO-O: A Benchmark for Object Detectors under Natural Distribution Shifts"></a>COCO-O: A Benchmark for Object Detectors under Natural Distribution Shifts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12730">http://arxiv.org/abs/2307.12730</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alibaba/easyrobust">https://github.com/alibaba/easyrobust</a></li>
<li>paper_authors: Xiaofeng Mao, Yuefeng Chen, Yao Zhu, Da Chen, Hang Su, Rong Zhang, Hui Xue<br>for:This paper aims to provide a comprehensive assessment of the robustness of object detection models under natural distribution shifts, and to introduce a new test dataset called COCO-O to benchmark the OOD robustness of detectors.methods:The authors use a large-scale dataset called COCO-O, which contains six types of natural distribution shifts, to evaluate the OOD robustness of more than 100 modern object detection models. They also conduct experiments to study the effect of different components of the detector, such as the backbone, detection head, and augmentation techniques, on its OOD robustness.results:The authors find that most classic detectors do not exhibit strong OOD generalization, and that the backbone is the most important part for robustness. They also find that an end-to-end detection transformer design does not bring any enhancement, and may even reduce robustness. Finally, they show that large-scale foundation models have made a great leap on robust object detection. The dataset will be available at <a target="_blank" rel="noopener" href="https://github.com/alibaba/easyrobust/tree/main/benchmarks/coco_o.Here">https://github.com/alibaba/easyrobust/tree/main/benchmarks/coco_o.Here</a> is the Chinese version of the three key points:for:这篇论文的目的是为了提供一个广泛的对象检测模型在自然分布变化下的robustness的评估，并为了 benchmarking 对象检测模型的OOD robustness而引入了COCO-O测试集。methods:作者使用了COCO-O测试集，这个测试集包含6种自然分布变化，来评估现代对象检测模型的OOD robustness。他们还进行了不同组件的检测器，如背bone、检测头和增强技术的效果的研究。results:作者发现大多数经典的检测器在自然分布变化下并没有强大的OOD普适性，背bone是检测器的robustness中最重要的部分。他们还发现，检测transformer设计并不提供任何改进，可能甚至会降低robustness。最后，他们发现大规模基础模型在robust对象检测方面做出了很大的进步。<details>
<summary>Abstract</summary>
Practical object detection application can lose its effectiveness on image inputs with natural distribution shifts. This problem leads the research community to pay more attention on the robustness of detectors under Out-Of-Distribution (OOD) inputs. Existing works construct datasets to benchmark the detector's OOD robustness for a specific application scenario, e.g., Autonomous Driving. However, these datasets lack universality and are hard to benchmark general detectors built on common tasks such as COCO. To give a more comprehensive robustness assessment, we introduce COCO-O(ut-of-distribution), a test dataset based on COCO with 6 types of natural distribution shifts. COCO-O has a large distribution gap with training data and results in a significant 55.7% relative performance drop on a Faster R-CNN detector. We leverage COCO-O to conduct experiments on more than 100 modern object detectors to investigate if their improvements are credible or just over-fitting to the COCO test set. Unfortunately, most classic detectors in early years do not exhibit strong OOD generalization. We further study the robustness effect on recent breakthroughs of detector's architecture design, augmentation and pre-training techniques. Some empirical findings are revealed: 1) Compared with detection head or neck, backbone is the most important part for robustness; 2) An end-to-end detection transformer design brings no enhancement, and may even reduce robustness; 3) Large-scale foundation models have made a great leap on robust object detection. We hope our COCO-O could provide a rich testbed for robustness study of object detection. The dataset will be available at https://github.com/alibaba/easyrobust/tree/main/benchmarks/coco_o.
</details>
<details>
<summary>摘要</summary>
实际应用中的物体检测应用可能会在自然分布变化下失效。这个问题导致研究人员更加关注检测器在不同分布下的Robustness。现有的 dataset 用于测试检测器的 OOD Robustness，如 autonomous driving 应用场景。然而，这些 dataset 缺乏通用性，难以测试基于 common task 如 COCO 的检测器。为了给出更加全面的 Robustness 评估，我们引入 COCO-O（out-of-distribution）测试集，基于 COCO 的6种自然分布变化。COCO-O 与训练数据之间存在大的分布差，导致 Faster R-CNN 检测器的Relative performance drop 为 55.7%。我们使用 COCO-O 对 более than 100 种现代 object detection 算法进行实验，以查找他们是否具有可靠的 OOD 通用性。结果显示，大多数早期的检测器不具有强的 OOD 泛化能力。我们进一步研究了检测器的architecture design，增强技术和预训练技术的Robustness效果。我们发现：1）与检测头或 neck 相比，后凹是检测器 Robustness 的关键部分; 2） end-to-end 检测转换设计不具有改善效果，可能 même 减少 Robustness; 3）大规模基础模型在 Robust object detection 方面做出了很大的进步。我们希望 COCO-O 可以为 Robustness 研究提供一个丰富的测试平台。 dataset 将在 https://github.com/alibaba/easyrobust/tree/main/benchmarks/coco_o 上提供。
</details></li>
</ul>
<hr>
<h2 id="Persistent-Transient-Duality-A-Multi-mechanism-Approach-for-Modeling-Human-Object-Interaction"><a href="#Persistent-Transient-Duality-A-Multi-mechanism-Approach-for-Modeling-Human-Object-Interaction" class="headerlink" title="Persistent-Transient Duality: A Multi-mechanism Approach for Modeling Human-Object Interaction"></a>Persistent-Transient Duality: A Multi-mechanism Approach for Modeling Human-Object Interaction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12729">http://arxiv.org/abs/2307.12729</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hung Tran, Vuong Le, Svetha Venkatesh, Truyen Tran</li>
<li>for: 这个论文旨在解释人类在人机交互（HOI）活动中的多机制性，以及如何使用Persistent-Transient Duality来模型这些机制。</li>
<li>methods: 这篇论文使用了Parent-Child neural network，其中Persistent频道和Transient频道是两个独立的神经网络，用于模型不同的机制。此外，还使用了一个神经网络模块来实现机制之间的动态切换。</li>
<li>results: 在两个丰富的数据集和多种设置下，这个模型在HOI动作预测中具有了superior的性能，证明了其适用性。<details>
<summary>Abstract</summary>
Humans are highly adaptable, swiftly switching between different modes to progressively handle different tasks, situations and contexts. In Human-object interaction (HOI) activities, these modes can be attributed to two mechanisms: (1) the large-scale consistent plan for the whole activity and (2) the small-scale children interactive actions that start and end along the timeline. While neuroscience and cognitive science have confirmed this multi-mechanism nature of human behavior, machine modeling approaches for human motion are trailing behind. While attempted to use gradually morphing structures (e.g., graph attention networks) to model the dynamic HOI patterns, they miss the expeditious and discrete mode-switching nature of the human motion. To bridge that gap, this work proposes to model two concurrent mechanisms that jointly control human motion: the Persistent process that runs continually on the global scale, and the Transient sub-processes that operate intermittently on the local context of the human while interacting with objects. These two mechanisms form an interactive Persistent-Transient Duality that synergistically governs the activity sequences. We model this conceptual duality by a parent-child neural network of Persistent and Transient channels with a dedicated neural module for dynamic mechanism switching. The framework is trialed on HOI motion forecasting. On two rich datasets and a wide variety of settings, the model consistently delivers superior performances, proving its suitability for the challenge.
</details>
<details>
<summary>摘要</summary>
人类具有高度适应性，快速 switching  между不同模式以逐渐处理不同任务、情况和上下文。在人机交互（HOI）活动中，这些模式可以归结为两种机制：（1）整体活动的大规模一致计划，和（2）在时间轴上开始和结束的小规模儿童交互动作。而神经科学和认知科学已经证实了人类行为的多机制性，但机器模型人体运动的方法尚未跟上。尝试使用渐变变换结构（如图像注意力网络）来模型动态 HOI 模式，但它们缺乏人类运动快速和精细的模式转换特点。为了bridging这个差距，本工作提出了同时控制人体运动的两个机制：持续过程，该过程在全局范围内一直运行，和间歇性子进程，该子进程在人类与对象交互时在本地上运行。这两种机制组成了互动的持续-间歇性双重性，这种双重性同时控制活动序列。我们使用父母-孩子神经网络，其中持续通道和间歇性通道各自拥有特定的神经元模块，以实现动态机制转换。这种概念双重性在 HOI 动作预测中得到证明，在两个丰富的数据集和多种设置下，模型一致地表现出优秀的表现，证明其适用性。
</details></li>
</ul>
<hr>
<h2 id="AMAE-Adaptation-of-Pre-Trained-Masked-Autoencoder-for-Dual-Distribution-Anomaly-Detection-in-Chest-X-Rays"><a href="#AMAE-Adaptation-of-Pre-Trained-Masked-Autoencoder-for-Dual-Distribution-Anomaly-Detection-in-Chest-X-Rays" class="headerlink" title="AMAE: Adaptation of Pre-Trained Masked Autoencoder for Dual-Distribution Anomaly Detection in Chest X-Rays"></a>AMAE: Adaptation of Pre-Trained Masked Autoencoder for Dual-Distribution Anomaly Detection in Chest X-Rays</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12721">http://arxiv.org/abs/2307.12721</a></li>
<li>repo_url: None</li>
<li>paper_authors: Behzad Bozorgtabar, Dwarikanath Mahapatra, Jean-Philippe Thiran</li>
<li>for: 这个研究是针对医疗影像中的无Supervised anomaly detection问题，尤其是胸部X-ray影像。</li>
<li>methods: 这个研究使用了一个名为AMAE的两阶段算法，它首先将normal类别的训练图像 sintethesize为假想的异常图像，然后使用内部的lightweight分类器来训练。接着，它利用了一个pseudo-labeling scheme来利用无标注图像中的异常。</li>
<li>results: AMAE在不同的异常比例下评估了其效果，并与其他自动标注和双分布异常检测方法进行比较。结果显示，AMAE在三个公开的胸部X-raybenchmark上设置了新的State-of-the-art。<details>
<summary>Abstract</summary>
Unsupervised anomaly detection in medical images such as chest radiographs is stepping into the spotlight as it mitigates the scarcity of the labor-intensive and costly expert annotation of anomaly data. However, nearly all existing methods are formulated as a one-class classification trained only on representations from the normal class and discard a potentially significant portion of the unlabeled data. This paper focuses on a more practical setting, dual distribution anomaly detection for chest X-rays, using the entire training data, including both normal and unlabeled images. Inspired by a modern self-supervised vision transformer model trained using partial image inputs to reconstruct missing image regions -- we propose AMAE, a two-stage algorithm for adaptation of the pre-trained masked autoencoder (MAE). Starting from MAE initialization, AMAE first creates synthetic anomalies from only normal training images and trains a lightweight classifier on frozen transformer features. Subsequently, we propose an adaptation strategy to leverage unlabeled images containing anomalies. The adaptation scheme is accomplished by assigning pseudo-labels to unlabeled images and using two separate MAE based modules to model the normative and anomalous distributions of pseudo-labeled images. The effectiveness of the proposed adaptation strategy is evaluated with different anomaly ratios in an unlabeled training set. AMAE leads to consistent performance gains over competing self-supervised and dual distribution anomaly detection methods, setting the new state-of-the-art on three public chest X-ray benchmarks: RSNA, NIH-CXR, and VinDr-CXR.
</details>
<details>
<summary>摘要</summary>
不监督异常检测在医学影像中，如胸部X射线图像，正在受到关注，因为它解决了专业人员对异常数据的繁琐和成本的批注短缺。然而，大多数现有方法都是基于一个类型的分类，只使用正常类的表示来训练。这篇论文关注了更实际的设定：对胸部X射线图像进行双分布异常检测，使用整个训练数据，包括正常和无标记图像。 Drawing inspiration from a modern self-supervised vision transformer model trained using partial image inputs to reconstruct missing image regions，我们提出了AMAE算法，一个两个阶段的算法，用于适应预先训练的封闭自动encoder（MAE）。从MAE初始化开始，AMAE首先将正常训练图像中生成假异常，并训练一个轻量级分类器。接着，我们提出了适应策略，使用无标记图像中含异常的图像进行适应。这种适应策略是通过将无标记图像 assign pseudo-labels，并使用两个基于MAE模块来模型无标记图像的正常和异常分布。我们对不同异常比例的无标记训练集进行评估，AMAE在自我超vised和双分布异常检测方法中表现出优异，创造了新的state-of-the-art在三个公共胸部X射线标准benchmark上：RSNA、NIH-CXR和VinDr-CXR。
</details></li>
</ul>
<hr>
<h2 id="CarPatch-A-Synthetic-Benchmark-for-Radiance-Field-Evaluation-on-Vehicle-Components"><a href="#CarPatch-A-Synthetic-Benchmark-for-Radiance-Field-Evaluation-on-Vehicle-Components" class="headerlink" title="CarPatch: A Synthetic Benchmark for Radiance Field Evaluation on Vehicle Components"></a>CarPatch: A Synthetic Benchmark for Radiance Field Evaluation on Vehicle Components</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12718">http://arxiv.org/abs/2307.12718</a></li>
<li>repo_url: None</li>
<li>paper_authors: Davide Di Nucci, Alessandro Simoni, Matteo Tomei, Luca Ciuffreda, Roberto Vezzani, Rita Cucchiara</li>
<li>for: 本研究旨在提供一个Synthetic benchmark dataset for vehicle inspection, 以便用于评估和比较不同技术的效果。</li>
<li>methods: 本研究使用NeRFs技术来生成3D重建图像，并提供了相应的深度地图和semantic segmentationmask。</li>
<li>results: 本研究提供了一个公共可用的Synthetic benchmark dataset，可以作为评估和比较不同技术的导向。<details>
<summary>Abstract</summary>
Neural Radiance Fields (NeRFs) have gained widespread recognition as a highly effective technique for representing 3D reconstructions of objects and scenes derived from sets of images. Despite their efficiency, NeRF models can pose challenges in certain scenarios such as vehicle inspection, where the lack of sufficient data or the presence of challenging elements (e.g. reflections) strongly impact the accuracy of the reconstruction. To this aim, we introduce CarPatch, a novel synthetic benchmark of vehicles. In addition to a set of images annotated with their intrinsic and extrinsic camera parameters, the corresponding depth maps and semantic segmentation masks have been generated for each view. Global and part-based metrics have been defined and used to evaluate, compare, and better characterize some state-of-the-art techniques. The dataset is publicly released at https://aimagelab.ing.unimore.it/go/carpatch and can be used as an evaluation guide and as a baseline for future work on this challenging topic.
</details>
<details>
<summary>摘要</summary>
neural radiance fields (NeRFs) 已经得到广泛的认可作为用于表示基于图像集的物体和场景三维重建的高效技术。尽管它们有效，但 NeRF 模型在某些情况下可能会遇到挑战，如车辆检查，因为缺乏足够的数据或存在挑战性的元素（例如反射）会强烈影响重建的准确性。为此，我们引入了 CarPatch，一个新的人工测试集。除了每个视图的图像、摄像机参数、深度地图和semantic segmentation映射之外，还包括了每个视图的全球和部分评价指标。这些指标用于评估、比较和更好地描述一些现状技术的性能。该数据集publicly released at <https://aimagelab.ing.unimore.it/go/carpatch>，可以用作评估指南和未来工作的基准。
</details></li>
</ul>
<hr>
<h2 id="Dense-Transformer-based-Enhanced-Coding-Network-for-Unsupervised-Metal-Artifact-Reduction"><a href="#Dense-Transformer-based-Enhanced-Coding-Network-for-Unsupervised-Metal-Artifact-Reduction" class="headerlink" title="Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction"></a>Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12717">http://arxiv.org/abs/2307.12717</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wangduo Xie, Matthew B. Blaschko</li>
<li>for: 这篇论文的目的是提出一种不需要组织数据的自动化阴极 artifact 除除法，以帮助 CT 影像诊断中受到阴极 artifact 的影响。</li>
<li>methods: 本论文提出了一个基于 dense transformer 的增强编码网络 (DTEC-Net)，包括一个层次分解对接oder 和 transformer 来取得高密度编码序列，以及一个第二阶分解方法来改善密度序列的解码过程。</li>
<li>results: 实验和模型讨论表明，DTEC-Net 能够优于前一代方法，对 CT 影像进行自动化阴极 artifact 除除法，同时保留 CT 影像的结构资讯。<details>
<summary>Abstract</summary>
CT images corrupted by metal artifacts have serious negative effects on clinical diagnosis. Considering the difficulty of collecting paired data with ground truth in clinical settings, unsupervised methods for metal artifact reduction are of high interest. However, it is difficult for previous unsupervised methods to retain structural information from CT images while handling the non-local characteristics of metal artifacts. To address these challenges, we proposed a novel Dense Transformer based Enhanced Coding Network (DTEC-Net) for unsupervised metal artifact reduction. Specifically, we introduce a Hierarchical Disentangling Encoder, supported by the high-order dense process, and transformer to obtain densely encoded sequences with long-range correspondence. Then, we present a second-order disentanglement method to improve the dense sequence's decoding process. Extensive experiments and model discussions illustrate DTEC-Net's effectiveness, which outperforms the previous state-of-the-art methods on a benchmark dataset, and greatly reduces metal artifacts while restoring richer texture details.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Damage-Vision-Mining-Opportunity-for-Imbalanced-Anomaly-Detection"><a href="#Damage-Vision-Mining-Opportunity-for-Imbalanced-Anomaly-Detection" class="headerlink" title="Damage Vision Mining Opportunity for Imbalanced Anomaly Detection"></a>Damage Vision Mining Opportunity for Imbalanced Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12676">http://arxiv.org/abs/2307.12676</a></li>
<li>repo_url: None</li>
<li>paper_authors: Takato Yasuno</li>
<li>for: 这篇论文是为了探讨受损测量数据的挑战和解决方案，尤其是在工业应用中使用深度学习方法进行预测维护和应急维护。</li>
<li>methods: 本研究使用了深度学习方法，包括回归、图像分类、物件检测和 semantic segmentation，以解决受损测量数据的问题。</li>
<li>results: 研究发现，在受损测量数据中，使用深度学习方法可以提高预测维护和应急维护的精度。此外，研究还发现了一个有用的正常检测应用，可以在受损测量数据中检测到异常情况。<details>
<summary>Abstract</summary>
In past decade, previous balanced datasets have been used to advance algorithms for classification, object detection, semantic segmentation, and anomaly detection in industrial applications. Specifically, for condition-based maintenance, automating visual inspection is crucial to ensure high quality. Deterioration prognostic attempts to optimize the fine decision process for predictive maintenance and proactive repair. In civil infrastructure and living environment, damage data mining cannot avoid the imbalanced data issue because of rare unseen events and high quality status by improved operations. For visual inspection, deteriorated class acquired from the surface of concrete and steel components are occasionally imbalanced. From numerous related surveys, we summarize that imbalanced data problems can be categorized into four types; 1) missing range of target and label valuables, 2) majority-minority class imbalance, 3) foreground-background of spatial imbalance, 4) long-tailed class of pixel-wise imbalance. Since 2015, there has been many imbalanced studies using deep learning approaches that includes regression, image classification, object detection, semantic segmentation. However, anomaly detection for imbalanced data is not yet well known. In the study, we highlight one-class anomaly detection application whether anomalous class or not, and demonstrate clear examples on imbalanced vision datasets: blood smear, lung infection, hazardous driving, wooden, concrete deterioration, river sludge, and disaster damage. Illustrated in Fig.1, we provide key results on damage vision mining advantage, hypothesizing that the more effective range of positive ratio, the higher accuracy gain of anomaly detection application. In our imbalanced studies, compared with the balanced case of positive ratio 1/1, we find that there is applicable positive ratio, where the accuracy are consistently high.
</details>
<details>
<summary>摘要</summary>
过去一个 décennial，以前的平衡数据集被用于提高分类、物体检测、semantic segmentation和异常检测在工业应用中的算法。具体来说，为condition-based maintenance，自动化视觉检查是关键以确保高质量。预测维护和抢修的决策进程优化。在公共基础设施和生活环境中，损害数据挖掘无法避免偏移数据问题，因为罕见的未看到事件和高质量状态的提高。对于视觉检查，坏化类型从混凝土和钢结构的表面获得的数据 occasionally imbalanced。从多个相关的调查中，我们总结出四种偏移数据问题：1）目标和标签值的缺失范围，2）多数少数类别偏移，3）前景背景的空间偏移，4）像素级偏移。自2015年以来，有很多关于偏移数据的研究使用深度学习方法，包括回归、图像分类、物体检测和semantic segmentation。然而，异常检测对偏移数据还未得到充分的研究。在本研究中，我们强调一类异常检测应用，无论异常类或不，并提供了清晰的示例，包括血癌、肺感染、危险驾驶、木材、混凝土衰老、河流淤泥和灾害损害。图1中，我们提供了危害视觉矿物优势，假设更高的正确率，异常检测应用的更高精度。在我们的偏移研究中，与平衡情况相比，我们发现了可采用的正确比例，其准确率一致高。
</details></li>
</ul>
<hr>
<h2 id="Industrial-Segment-Anything-–-a-Case-Study-in-Aircraft-Manufacturing-Intralogistics-Maintenance-Repair-and-Overhaul"><a href="#Industrial-Segment-Anything-–-a-Case-Study-in-Aircraft-Manufacturing-Intralogistics-Maintenance-Repair-and-Overhaul" class="headerlink" title="Industrial Segment Anything – a Case Study in Aircraft Manufacturing, Intralogistics, Maintenance, Repair, and Overhaul"></a>Industrial Segment Anything – a Case Study in Aircraft Manufacturing, Intralogistics, Maintenance, Repair, and Overhaul</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12674">http://arxiv.org/abs/2307.12674</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keno Moenck, Arne Wendt, Philipp Prünte, Julian Koch, Arne Sahrhage, Johann Gierecker, Ole Schmedemann, Falko Kähler, Dirk Holst, Martin Gomse, Thorsten Schüppstuhl, Daniel Schoepflin</li>
<li>for: 这篇论文旨在探讨在飞机生产专业领域中应用深度学习基于应用程序的问题。</li>
<li>methods: 该论文使用视觉基础模型（VFM）的零shot能力来解决数据、上下文和感知器多样性的问题。</li>
<li>results: 论文对飞机生产专业领域中的制造、内部物流、维护、维修和更换过程进行了应用，并考虑了领域知识的投入。<details>
<summary>Abstract</summary>
Deploying deep learning-based applications in specialized domains like the aircraft production industry typically suffers from the training data availability problem. Only a few datasets represent non-everyday objects, situations, and tasks. Recent advantages in research around Vision Foundation Models (VFM) opened a new area of tasks and models with high generalization capabilities in non-semantic and semantic predictions. As recently demonstrated by the Segment Anything Project, exploiting VFM's zero-shot capabilities is a promising direction in tackling the boundaries spanned by data, context, and sensor variety. Although, investigating its application within specific domains is subject to ongoing research. This paper contributes here by surveying applications of the SAM in aircraft production-specific use cases. We include manufacturing, intralogistics, as well as maintenance, repair, and overhaul processes, also representing a variety of other neighboring industrial domains. Besides presenting the various use cases, we further discuss the injection of domain knowledge.
</details>
<details>
<summary>摘要</summary>
通常在特殊领域 like 飞机生产 industri에서推广深度学习基于应用typically suffers from the training data availability problem。只有一些数据集表示不同的对象、情况和任务。 current Advantages in Research on Vision Foundation Models (VFM) opened a new area of tasks and models with high generalization capabilities in non-semantic and semantic predictions。 As recently demonstrated by the Segment Anything Project， exploiting VFM's zero-shot capabilities is a promising direction in tackling the boundaries spanned by data, context, and sensor variety。Although，investigating its application within specific domains is subject to ongoing research。This paper contributes here by surveying applications of the SAM in aircraft production-specific use cases。We include manufacturing, intralogistics, as well as maintenance, repair, and overhaul processes，also representing a variety of other neighboring industrial domains。Besides presenting the various use cases，we further discuss the injection of domain knowledge。
</details></li>
</ul>
<hr>
<h2 id="Global-k-Space-Interpolation-for-Dynamic-MRI-Reconstruction-using-Masked-Image-Modeling"><a href="#Global-k-Space-Interpolation-for-Dynamic-MRI-Reconstruction-using-Masked-Image-Modeling" class="headerlink" title="Global k-Space Interpolation for Dynamic MRI Reconstruction using Masked Image Modeling"></a>Global k-Space Interpolation for Dynamic MRI Reconstruction using Masked Image Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12672">http://arxiv.org/abs/2307.12672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiazhen Pan, Suprosanna Shit, Özgün Turgut, Wenqi Huang, Hongwei Bran Li, Nil Stolt-Ansó, Thomas Küstner, Kerstin Hammernik, Daniel Rueckert</li>
<li>for: This paper focuses on improving dynamic magnetic resonance imaging (MRI) reconstruction by interpolating undersampled k-space data before obtaining images with Fourier transform.</li>
<li>methods: The proposed approach uses a Transformer-based k-space Global Interpolation Network (k-GIN) to learn global dependencies among low- and high-frequency components of 2D+t k-space, and a novel k-space Iterative Refinement Module (k-IRM) to enhance high-frequency components learning.</li>
<li>results: The proposed method outperforms baseline methods in terms of both quantitative and qualitative measures, and achieves higher robustness and generalizability in highly-undersampled MR data.Here’s the Chinese translation of the three points:</li>
<li>for: 这篇论文关注改进动态磁共振成像重建，通过在傅里叶变换前 interpolate 受损的 k-空间数据。</li>
<li>methods: 提议的方法使用 Transformer 型 k-空间全球 interpolating 网络 (k-GIN) 学习 k-空间低频和高频成分之间的全球相互关系，并使用 novel k-space Iterative Refinement Module (k-IRM) 进一步提高高频成分学习。</li>
<li>results: 提议的方法相比基eline方法，在量化和质量上都有显著提高，并在高度受损的 MR 数据中具有更高的Robustness和普适性。<details>
<summary>Abstract</summary>
In dynamic Magnetic Resonance Imaging (MRI), k-space is typically undersampled due to limited scan time, resulting in aliasing artifacts in the image domain. Hence, dynamic MR reconstruction requires not only modeling spatial frequency components in the x and y directions of k-space but also considering temporal redundancy. Most previous works rely on image-domain regularizers (priors) to conduct MR reconstruction. In contrast, we focus on interpolating the undersampled k-space before obtaining images with Fourier transform. In this work, we connect masked image modeling with k-space interpolation and propose a novel Transformer-based k-space Global Interpolation Network, termed k-GIN. Our k-GIN learns global dependencies among low- and high-frequency components of 2D+t k-space and uses it to interpolate unsampled data. Further, we propose a novel k-space Iterative Refinement Module (k-IRM) to enhance the high-frequency components learning. We evaluate our approach on 92 in-house 2D+t cardiac MR subjects and compare it to MR reconstruction methods with image-domain regularizers. Experiments show that our proposed k-space interpolation method quantitatively and qualitatively outperforms baseline methods. Importantly, the proposed approach achieves substantially higher robustness and generalizability in cases of highly-undersampled MR data.
</details>
<details>
<summary>摘要</summary>
在动态磁共振成像（MRI）中，通常因为扫描时间有限，因此会出现嵌套artefacts在图像领域。因此，动态MR重建需要不仅考虑图像频谱中的空间频率组件，还需要考虑时间重复性。大多数前一些工作都是通过图像频谱约束（约束）来进行MR重建。在这种情况下，我们将掩码图像模型与嵌套空间 interpolate的方法相连接，并提出了一种新的Transformer基于的嵌套空间全球 interpolate网络，称为k-GIN。我们的k-GIN可以学习2D+t嵌套空间中低频和高频组件之间的全球依赖关系，并使用它来 interpolate不扫描的数据。此外，我们还提出了一种新的嵌套空间迭代优化模块（k-IRM），以提高高频组件的学习。我们对92个自有2D+t心脏MR数据进行了评估，并与图像频谱约束的MR重建方法进行比较。实验结果表明，我们的提posed方法在量化和质量上都超过了基eline方法。其中，我们的方法在高度压缩MR数据的情况下具有显著更高的Robustness和普适性。
</details></li>
</ul>
<hr>
<h2 id="A-Theoretically-Guaranteed-Quaternion-Weighted-Schatten-p-norm-Minimization-Method-for-Color-Image-Restoration"><a href="#A-Theoretically-Guaranteed-Quaternion-Weighted-Schatten-p-norm-Minimization-Method-for-Color-Image-Restoration" class="headerlink" title="A Theoretically Guaranteed Quaternion Weighted Schatten p-norm Minimization Method for Color Image Restoration"></a>A Theoretically Guaranteed Quaternion Weighted Schatten p-norm Minimization Method for Color Image Restoration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12656">http://arxiv.org/abs/2307.12656</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qiuxuanzhizi/qwsnm">https://github.com/qiuxuanzhizi/qwsnm</a></li>
<li>paper_authors: Qing-Hua Zhang, Liang-Tian He, Yi-Lun Wang, Liang-Jian Deng, Jun Liu</li>
<li>for: 这篇论文主要针对的是颜色图像修复（CIR）问题，具体来说是使用Weighted Nuclear Norm Minimization（WNNM）和Weighted Schatten $p$-norm Minimization（WSNM）方法来解决CIR问题。</li>
<li>methods: 这篇论文提出了一种基于四元数的WNNM方法（QWNNM），该方法可以将颜色图像 Represented as a whole in the quaternion domain，并且保持了颜色通道之间的自然协同关系。此外，该论文还提出了一种基于四元数的WSNM模型（QWSNM），用于解决CIR问题。</li>
<li>results: 该论文通过对两种CIR任务， namely color image denoising和deblurring，进行了广泛的实验，并证明了QWSNM方法在量化和质量上都有优于许多现有的方法。此外，论文还提供了一种初步的理论收敛分析，表明QWNNM和QWSNM的解决方案都具有固定点收敛保证。<details>
<summary>Abstract</summary>
Inspired by the fact that the matrix formulated by nonlocal similar patches in a natural image is of low rank, the rank approximation issue have been extensively investigated over the past decades, among which weighted nuclear norm minimization (WNNM) and weighted Schatten $p$-norm minimization (WSNM) are two prevailing methods have shown great superiority in various image restoration (IR) problems. Due to the physical characteristic of color images, color image restoration (CIR) is often a much more difficult task than its grayscale image counterpart. However, when applied to CIR, the traditional WNNM/WSNM method only processes three color channels individually and fails to consider their cross-channel correlations. Very recently, a quaternion-based WNNM approach (QWNNM) has been developed to mitigate this issue, which is capable of representing the color image as a whole in the quaternion domain and preserving the inherent correlation among the three color channels. Despite its empirical success, unfortunately, the convergence behavior of QWNNM has not been strictly studied yet. In this paper, on the one side, we extend the WSNM into quaternion domain and correspondingly propose a novel quaternion-based WSNM model (QWSNM) for tackling the CIR problems. Extensive experiments on two representative CIR tasks, including color image denoising and deblurring, demonstrate that the proposed QWSNM method performs favorably against many state-of-the-art alternatives, in both quantitative and qualitative evaluations. On the other side, more importantly, we preliminarily provide a theoretical convergence analysis, that is, by modifying the quaternion alternating direction method of multipliers (QADMM) through a simple continuation strategy, we theoretically prove that both the solution sequences generated by the QWNNM and QWSNM have fixed-point convergence guarantees.
</details>
<details>
<summary>摘要</summary>
基于自然图像中非local相似区域矩阵的低级数据结构，过去几十年内，对矩阵近似问题进行了广泛的研究，其中包括权重核函数最小化（WNNM）和权重斜率p- norm最小化（WSNM）等两种方法，在不同的图像修复（IR）问题中显示出了优异的表现。然而，由于图像的物理特性，对于颜色图像的修复（CIR）是对灰度图像修复的一个非常更加困难的任务。然而，传统的WNNM/WSNM方法只是对每个色道进行独立处理，而忽略了它们之间的相互关系。最近，一种基于四元数的WNNM方法（QWNNM）已经开发出来，可以将颜色图像作为一个整体在四元数域中表示，并保留它们之间的自然相互关系。虽然它在实际中表现了良好，但它们的减法性还没有得到严格的研究。在这篇论文中，我们首先将WSNM扩展到四元数域，并对此提出了一种新的四元数基于WNNM模型（QWSNM），用于解决CIR问题。我们在两个代表性的CIR任务上进行了广泛的实验，包括颜色图像噪声去除和颜色图像补做。结果表明，我们提出的QWSNM方法在量化和质量上的评价中表现出色，胜过许多当前的状态艺术。此外，我们还提供了一种初步的理论减法分析，即通过修改四元数alternating direction method of multipliers（QADMM）的简单继续策略，我们 theoretically proves that both the solution sequences generated by QWNNM and QWSNM have fixed-point convergence guarantees.
</details></li>
</ul>
<hr>
<h2 id="PG-RCNN-Semantic-Surface-Point-Generation-for-3D-Object-Detection"><a href="#PG-RCNN-Semantic-Surface-Point-Generation-for-3D-Object-Detection" class="headerlink" title="PG-RCNN: Semantic Surface Point Generation for 3D Object Detection"></a>PG-RCNN: Semantic Surface Point Generation for 3D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12637">http://arxiv.org/abs/2307.12637</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/quotation2520/pg-rcnn">https://github.com/quotation2520/pg-rcnn</a></li>
<li>paper_authors: Inyong Koo, Inyoung Lee, Se-Ho Kim, Hee-Seon Kim, Woo-jin Jeon, Changick Kim</li>
<li>for: 该 paper 是为了解决 LiDAR 数据中 объек的三维检测困难而写的。</li>
<li>methods: 该 paper 使用了点云补充方法，包括使用预训练网络生成 RoI 中的点云。</li>
<li>results: 该 paper 提出了 Point Generation R-CNN（PG-RCNN），一种新的端到端检测器，可以生成准确的前景对象的 semantic surface points。<details>
<summary>Abstract</summary>
One of the main challenges in LiDAR-based 3D object detection is that the sensors often fail to capture the complete spatial information about the objects due to long distance and occlusion. Two-stage detectors with point cloud completion approaches tackle this problem by adding more points to the regions of interest (RoIs) with a pre-trained network. However, these methods generate dense point clouds of objects for all region proposals, assuming that objects always exist in the RoIs. This leads to the indiscriminate point generation for incorrect proposals as well. Motivated by this, we propose Point Generation R-CNN (PG-RCNN), a novel end-to-end detector that generates semantic surface points of foreground objects for accurate detection. Our method uses a jointly trained RoI point generation module to process the contextual information of RoIs and estimate the complete shape and displacement of foreground objects. For every generated point, PG-RCNN assigns a semantic feature that indicates the estimated foreground probability. Extensive experiments show that the point clouds generated by our method provide geometrically and semantically rich information for refining false positive and misaligned proposals. PG-RCNN achieves competitive performance on the KITTI benchmark, with significantly fewer parameters than state-of-the-art models. The code is available at https://github.com/quotation2520/PG-RCNN.
</details>
<details>
<summary>摘要</summary>
Motivated by this, we propose Point Generation R-CNN (PG-RCNN), a novel end-to-end detector that generates semantic surface points of foreground objects for accurate detection. Our method uses a jointly trained RoI point generation module to process the contextual information of RoIs and estimate the complete shape and displacement of foreground objects. For every generated point, PG-RCNN assigns a semantic feature that indicates the estimated foreground probability.Extensive experiments show that the point clouds generated by our method provide geometrically and semantically rich information for refining false positive and misaligned proposals. PG-RCNN achieves competitive performance on the KITTI benchmark, with significantly fewer parameters than state-of-the-art models. The code is available at https://github.com/quotation2520/PG-RCNN.Translated into Simplified Chinese:一个主要挑战在LiDAR基于3D物体检测中是感知器通常无法捕捉物体的完整空间信息，这主要是因为距离较远和遮挡。两Stage检测器通过添加更多的点云来补充RoI中的点云，但这些方法生成的点云都是对所有的区域提案中的物体，假设物体总是存在于RoI中。这会导致无用的点云生成和预测错误的提案。我们提出了Point Generation R-CNN（PG-RCNN），一种新的端到端检测器，它可以生成对象的语义表面点，以提高检测的准确性。我们的方法使用一个同时训练的RoI点生成模块，以处理RoI中的上下文信息，并估计前景对象的完整形状和偏移。每个生成的点都被PG-RCNN分配一个语义特征，这个特征指示了对象的预测概率。广泛的实验表明，PG-RCNN生成的点云具有很好的准确性和语义特征，可以用于修正错误的提案和偏移。PG-RCNN在KITTI测试benchmark上达到了竞争性性能，并且 Parameters 比 state-of-the-art 模型少得多。代码可以在https://github.com/quotation2520/PG-RCNN中下载。
</details></li>
</ul>
<hr>
<h2 id="Automatic-lobe-segmentation-using-attentive-cross-entropy-and-end-to-end-fissure-generation"><a href="#Automatic-lobe-segmentation-using-attentive-cross-entropy-and-end-to-end-fissure-generation" class="headerlink" title="Automatic lobe segmentation using attentive cross entropy and end-to-end fissure generation"></a>Automatic lobe segmentation using attentive cross entropy and end-to-end fissure generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12634">http://arxiv.org/abs/2307.12634</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/htytewx/softcam">https://github.com/htytewx/softcam</a></li>
<li>paper_authors: Qi Su, Na Wang, Jiawen Xie, Yinan Chen, Xiaofan Zhang</li>
<li>for:  automatic lung lobe segmentation algorithm for the diagnosis and treatment of lung diseases</li>
<li>methods:  task-specific loss function to pay attention to the area around the pulmonary fissure, end-to-end pulmonary fissure generation method, registration-based loss function to alleviate convergence difficulty</li>
<li>results:  dice scores of 97.83% on private dataset STLB and 94.75% on public LUNA16 dataset<details>
<summary>Abstract</summary>
The automatic lung lobe segmentation algorithm is of great significance for the diagnosis and treatment of lung diseases, however, which has great challenges due to the incompleteness of pulmonary fissures in lung CT images and the large variability of pathological features. Therefore, we propose a new automatic lung lobe segmentation framework, in which we urge the model to pay attention to the area around the pulmonary fissure during the training process, which is realized by a task-specific loss function. In addition, we introduce an end-to-end pulmonary fissure generation method in the auxiliary pulmonary fissure segmentation task, without any additional network branch. Finally, we propose a registration-based loss function to alleviate the convergence difficulty of the Dice loss supervised pulmonary fissure segmentation task. We achieve 97.83% and 94.75% dice scores on our private dataset STLB and public LUNA16 dataset respectively.
</details>
<details>
<summary>摘要</summary>
“自动肺lobus分割算法具有诊断和治疗肺病的重要意义，但受到肺 CT 影像中肺裂的不完整性和病理特征的大幅度variability所困扰。因此，我们提出了一个新的自动肺lobus分割框架，其中我们要求模型在训练过程中对肺裂附近区域做出优化。此外，我们引入了一个独立的辅助肺裂分割任务，并在这个任务中使用了一个统一的损失函数。最后，我们提出了一个注册损失函数，以解决基于 Dice 损失函数的肺裂分割任务中的整合问题。我们在私人数据集 STLB 和公共数据集 LUNA16 上实现了97.83% 和 94.75%的 Dice 分数。”
</details></li>
</ul>
<hr>
<h2 id="Semi-Supervised-Medical-Image-Segmentation-with-Co-Distribution-Alignment"><a href="#Semi-Supervised-Medical-Image-Segmentation-with-Co-Distribution-Alignment" class="headerlink" title="Semi-Supervised Medical Image Segmentation with Co-Distribution Alignment"></a>Semi-Supervised Medical Image Segmentation with Co-Distribution Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12630">http://arxiv.org/abs/2307.12630</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tao Wang, Zhongzheng Huang, Jiawei Wu, Yuanzheng Cai, Zuoyong Li</li>
<li>for: 这篇论文主要是为了提出一种基于半指导学习的医学影像分割方法，以便在缺乏大量标注数据的情况下进行医学影像分割。</li>
<li>methods: 这篇论文提出了一种名为Co-Distribution Alignment（Co-DA）的方法，它可以在半指导学习情况下提高医学影像分割的性能。Co-DA方法包括使用两个不同初始化的模型进行类别匹配，并使用一个模型生成的pseudo-labels来监督另一个模型。此外，论文还提出了一种过期预期极限似一个权重函数来降低无效的pseudo-labels噪音。</li>
<li>results: 根据论文的实验结果，Co-DA方法在三个公共dataset上都有较好的性能，尤其是在2D CaDIS dataset和3D LGE-MRI和ACDC dataset上，它可以仅使用24%的标注数据而 достиieving an mIoU of 0.8515和Dice score of 0.8824和0.8773，即使只使用20%的数据。<details>
<summary>Abstract</summary>
Medical image segmentation has made significant progress when a large amount of labeled data are available. However, annotating medical image segmentation datasets is expensive due to the requirement of professional skills. Additionally, classes are often unevenly distributed in medical images, which severely affects the classification performance on minority classes. To address these problems, this paper proposes Co-Distribution Alignment (Co-DA) for semi-supervised medical image segmentation. Specifically, Co-DA aligns marginal predictions on unlabeled data to marginal predictions on labeled data in a class-wise manner with two differently initialized models before using the pseudo-labels generated by one model to supervise the other. Besides, we design an over-expectation cross-entropy loss for filtering the unlabeled pixels to reduce noise in their pseudo-labels. Quantitative and qualitative experiments on three public datasets demonstrate that the proposed approach outperforms existing state-of-the-art semi-supervised medical image segmentation methods on both the 2D CaDIS dataset and the 3D LGE-MRI and ACDC datasets, achieving an mIoU of 0.8515 with only 24% labeled data on CaDIS, and a Dice score of 0.8824 and 0.8773 with only 20% data on LGE-MRI and ACDC, respectively.
</details>
<details>
<summary>摘要</summary>
医疗图像分割技术在有大量标注数据时已经做出了 significiant进步。然而，为了创建医疗图像分割数据集， annotating 医疗图像分割数据集是昂贵的，主要因为需要专业技能。此外，医疗图像中的类别经常不均匀分布，这会严重地影响少数类别的分类性能。为了解决这些问题，这篇论文提出了Co-Distribution Alignment（Co-DA）方法，用于 semi-supervised 医疗图像分割。具体来说，Co-DA 方法将未标注数据中的边缘预测与已标注数据中的边缘预测进行类别匹配，使用两个不同初始化的模型来实现。此外，我们还设计了过期cross-entropy损失函数，用于筛选未标注的像素，以降低它们的 Pseudo-labels 中的噪音。我们对三个公共数据集进行了量化和质量测试，结果显示，我们的方法在 CaDIS 数据集上的 mIoU 为 0.8515，只使用 24% 的标注数据；在 LGE-MRI 和 ACDC 数据集上，我们的方法的 Dice 分数分别为 0.8824 和 0.8773，只使用 20% 的数据。
</details></li>
</ul>
<hr>
<h2 id="Phase-Matching-for-Out-of-Distribution-Generalization"><a href="#Phase-Matching-for-Out-of-Distribution-Generalization" class="headerlink" title="Phase Matching for Out-of-Distribution Generalization"></a>Phase Matching for Out-of-Distribution Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12622">http://arxiv.org/abs/2307.12622</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengming Hu, Yeqian Du, Rui Wang, Hao Chen</li>
<li>for: 本研究旨在解释卷积神经网络（CNNs）在不同分布下的泛化行为，并提出一种基于傅ри映射的频谱层次结构的方法来解决频谱预测问题。</li>
<li>methods: 本研究使用傅ри映射来分解视觉信号，并提出了一种基于频谱的相匹配方法（PhaMa）来 Address Domain Generalization（DG）问题。</li>
<li>results: 经过实验证明，提出的方法可以在多个标准准 benchmark上达到领先的性能水平，并且在不同分布下的泛化和Out-of-distribution（OOD） robustness任务中表现出色。<details>
<summary>Abstract</summary>
The Fourier transform, serving as an explicit decomposition method for visual signals, has been employed to explain the out-of-distribution generalization behaviors of Convolutional Neural Networks (CNNs). Previous studies have indicated that the amplitude spectrum is susceptible to the disturbance caused by distribution shifts. On the other hand, the phase spectrum preserves highly-structured spatial information, which is crucial for robust visual representation learning. However, the spatial relationships of phase spectrum remain unexplored in previous researches. In this paper, we aim to clarify the relationships between Domain Generalization (DG) and the frequency components, and explore the spatial relationships of the phase spectrum. Specifically, we first introduce a Fourier-based structural causal model which interprets the phase spectrum as semi-causal factors and the amplitude spectrum as non-causal factors. Then, we propose Phase Matching (PhaMa) to address DG problems. Our method introduces perturbations on the amplitude spectrum and establishes spatial relationships to match the phase components. Through experiments on multiple benchmarks, we demonstrate that our proposed method achieves state-of-the-art performance in domain generalization and out-of-distribution robustness tasks.
</details>
<details>
<summary>摘要</summary>
《傅里叶变换在视觉信号中的明确分解方法》，已经用于解释深度学习模型在不同分布下的泛化行为。先前的研究表明，振荡спектrum容易受到分布变化的影响。然而，相对于振荡спектrum，频率спектrum preserve了高度结构化的空间信息，这是重要的视觉表示学习的基础。然而，频率спектrum中的空间关系尚未在先前的研究中得到了探讨。在这篇论文中，我们想要解释频率спектrum和频率组件之间的关系，并探讨频率спектrum中的空间关系。我们首先介绍了一种基于傅里叶变换的结构 causal模型，其中 interprets频率спектrum为半 causal因素，而振荡спектrum为非 causal因素。然后，我们提出了phasematching（PhaMa）方法，用于解决频率特征泛化问题。我们的方法在振荡spectrum中引入了干扰并建立了空间关系，以匹配频率спектrum的组件。通过多个标准benchmark experiment表明，我们提出的方法可以在频率特征泛化和out-of-distribution Robustness任务中达到状态之巅的性能。
</details></li>
</ul>
<hr>
<h2 id="Sparse-annotation-strategies-for-segmentation-of-short-axis-cardiac-MRI"><a href="#Sparse-annotation-strategies-for-segmentation-of-short-axis-cardiac-MRI" class="headerlink" title="Sparse annotation strategies for segmentation of short axis cardiac MRI"></a>Sparse annotation strategies for segmentation of short axis cardiac MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12619">http://arxiv.org/abs/2307.12619</a></li>
<li>repo_url: None</li>
<li>paper_authors: Josh Stein, Maxime Di Folco, Julia Schnabel</li>
<li>for: 这个论文的目的是研究使用减少数据量和标注量来实现高精度的心脏MRI分割。</li>
<li>methods: 这个论文使用了减少数据量和标注量来降低标注量的方法，包括训练 sparse volumes 和 sparse annotations。</li>
<li>results: 研究发现，训练 sparse volumes 和 sparse annotations 可以获得高达 0.85 的 Dice 分数，并且比使用完整数据集（160 和 240 个数据集）更好。此外，中部的剖面标注对分割性能最有利，而脊梁区域的标注对分割性能最差。<details>
<summary>Abstract</summary>
Short axis cardiac MRI segmentation is a well-researched topic, with excellent results achieved by state-of-the-art models in a supervised setting. However, annotating MRI volumes is time-consuming and expensive. Many different approaches (e.g. transfer learning, data augmentation, few-shot learning, etc.) have emerged in an effort to use fewer annotated data and still achieve similar performance as a fully supervised model. Nevertheless, to the best of our knowledge, none of these works focus on which slices of MRI volumes are most important to annotate for yielding the best segmentation results. In this paper, we investigate the effects of training with sparse volumes, i.e. reducing the number of cases annotated, and sparse annotations, i.e. reducing the number of slices annotated per case. We evaluate the segmentation performance using the state-of-the-art nnU-Net model on two public datasets to identify which slices are the most important to annotate. We have shown that training on a significantly reduced dataset (48 annotated volumes) can give a Dice score greater than 0.85 and results comparable to using the full dataset (160 and 240 volumes for each dataset respectively). In general, training on more slice annotations provides more valuable information compared to training on more volumes. Further, annotating slices from the middle of volumes yields the most beneficial results in terms of segmentation performance, and the apical region the worst. When evaluating the trade-off between annotating volumes against slices, annotating as many slices as possible instead of annotating more volumes is a better strategy.
</details>
<details>
<summary>摘要</summary>
短轴心臓MRI分割是已有广泛研究的话题，现有前沿模型在监督环境下实现了出色的结果。然而，对MRI卷积的标注是时间consuming和昂贵的。多种方法（如转移学习、数据扩展、少数案例学习等）在尝试使用 fewer annotated data 并 still achieve similar performance as a fully supervised model 中出现。然而，到目前为止，这些工作没有关注在哪些MRI卷积中最重要的标注，以获得最佳分割结果。在这篇论文中，我们investigate the effects of training with sparse volumes和 sparse annotations。我们使用state-of-the-art nnU-Net模型在两个公共数据集上评估分割性能，以确定哪些卷积是最重要的标注。我们发现，使用很少的数据（48个标注卷积）可以达到 Dice 分数大于 0.85 和与全数据集（160和240卷积）的结果相当。总的来说，训练更多的卷积标注提供更多有价值的信息，而不是训练更多的卷积。此外，从中心部分标注MRI卷积可以提供最佳分割结果，而Apical区域则是最差。当评估标注卷积与卷积之间的权衡时，可以看到， annotating as many slices as possible instead of annotating more volumes 是一个更好的策略。
</details></li>
</ul>
<hr>
<h2 id="Attribute-Regularized-Soft-Introspective-VAE-Towards-Cardiac-Attribute-Regularization-Through-MRI-Domains"><a href="#Attribute-Regularized-Soft-Introspective-VAE-Towards-Cardiac-Attribute-Regularization-Through-MRI-Domains" class="headerlink" title="Attribute Regularized Soft Introspective VAE: Towards Cardiac Attribute Regularization Through MRI Domains"></a>Attribute Regularized Soft Introspective VAE: Towards Cardiac Attribute Regularization Through MRI Domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12618">http://arxiv.org/abs/2307.12618</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maxime Di Folco, Cosmin Bercea, Julia A. Schnabel</li>
<li>for: 这篇论文旨在提高深度生成模型的可控性，通过修改数据特征来控制数据生成。</li>
<li>methods: 这篇论文提出了Attribute-Regularized Soft Introspective VAE（Attri-SIVAE）模型，通过添加特征规范损失来提高VAE的可控性。</li>
<li>results: 实验表明，Attri-SIVAE模型在不同的MRI数据集上表现相当于Attributed regularized VAE，并且可以在不同的数据集上保持同等的规范水平。<details>
<summary>Abstract</summary>
Deep generative models have emerged as influential instruments for data generation and manipulation. Enhancing the controllability of these models by selectively modifying data attributes has been a recent focus. Variational Autoencoders (VAEs) have shown promise in capturing hidden attributes but often produce blurry reconstructions. Controlling these attributes through different imaging domains is difficult in medical imaging. Recently, Soft Introspective VAE leverage the benefits of both VAEs and Generative Adversarial Networks (GANs), which have demonstrated impressive image synthesis capabilities, by incorporating an adversarial loss into VAE training. In this work, we propose the Attributed Soft Introspective VAE (Attri-SIVAE) by incorporating an attribute regularized loss, into the Soft-Intro VAE framework. We evaluate experimentally the proposed method on cardiac MRI data from different domains, such as various scanner vendors and acquisition centers. The proposed method achieves similar performance in terms of reconstruction and regularization compared to the state-of-the-art Attributed regularized VAE but additionally also succeeds in keeping the same regularization level when tested on a different dataset, unlike the compared method.
</details>
<details>
<summary>摘要</summary>
深度生成模型已经成为数据生成和修改的重要工具。提高这些模型的可控性，通过选择性地修改数据特性，是最近的焦点。变量自编码器（VAEs）已经表现出捕捉隐藏特性的抑或，但它们经常生成模糊的重建。在医学成像中，控制这些特性通过不同的成像频谱是困难的。最近，软 introspective VAE 利用 VAEs 和生成对抗网络（GANs）的优点，通过在 VAE 训练中添加对抗损失来提高图像合成能力。在这项工作中，我们提议 incorporating  attribute regularized loss 到 Soft-Intro VAE 框架中，并对其进行实验评估。我们发现，提议的方法在不同的 MRI 数据集上实现了相似的重建和规范性能，与 state-of-the-art  attributed regularized VAE 相比，同时还能在不同的数据集上保持同等的规范水平。
</details></li>
</ul>
<hr>
<h2 id="ExWarp-Extrapolation-and-Warping-based-Temporal-Supersampling-for-High-frequency-Displays"><a href="#ExWarp-Extrapolation-and-Warping-based-Temporal-Supersampling-for-High-frequency-Displays" class="headerlink" title="ExWarp: Extrapolation and Warping-based Temporal Supersampling for High-frequency Displays"></a>ExWarp: Extrapolation and Warping-based Temporal Supersampling for High-frequency Displays</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12607">http://arxiv.org/abs/2307.12607</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akanksha Dixit, Yashashwee Chakrabarty, Smruti R. Sarangi</li>
<li>For: The paper aims to increase the frame rate of high-frequency displays by 4x with minimal reduction in perceived image quality.* Methods: The paper proposes using reinforcement learning (RL) to intelligently choose between slower DNN-based extrapolation and faster warping-based methods.* Results: The proposed approach, called Exwarp, achieves a 4x increase in frame rate with minimal reduction in image quality.<details>
<summary>Abstract</summary>
High-frequency displays are gaining immense popularity because of their increasing use in video games and virtual reality applications. However, the issue is that the underlying GPUs cannot continuously generate frames at this high rate -- this results in a less smooth and responsive experience. Furthermore, if the frame rate is not synchronized with the refresh rate, the user may experience screen tearing and stuttering. Previous works propose increasing the frame rate to provide a smooth experience on modern displays by predicting new frames based on past or future frames. Interpolation and extrapolation are two widely used algorithms that predict new frames. Interpolation requires waiting for the future frame to make a prediction, which adds additional latency. On the other hand, extrapolation provides a better quality of experience because it relies solely on past frames -- it does not incur any additional latency. The simplest method to extrapolate a frame is to warp the previous frame using motion vectors; however, the warped frame may contain improperly rendered visual artifacts due to dynamic objects -- this makes it very challenging to design such a scheme. Past work has used DNNs to get good accuracy, however, these approaches are slow. This paper proposes Exwarp -- an approach based on reinforcement learning (RL) to intelligently choose between the slower DNN-based extrapolation and faster warping-based methods to increase the frame rate by 4x with an almost negligible reduction in the perceived image quality.
</details>
<details>
<summary>摘要</summary>
高频显示器在游戏和虚拟现实应用中的使用越来越普遍，但是下面的 GPU 无法不断生成这高速帧，这会导致用户体验不平滑和不响应。此外，如果帧率与刷新率不同步，用户可能会经历屏扑和停顿。先前的工作建议通过预测新帧来提高现代显示器的帧率，以提供平滑的用户体验。 interpolate 和 extrapolate 是两种广泛使用的预测算法。 interpolate 需要等待未来的帧来作预测，这会添加额外的延迟。 extrapolate 提供了更好的用户体验，因为它仅基于过去的帧，无需添加额外的延迟。在 extrapolate 帧时，最简单的方法是通过运动向量来扭曲上一帧，但是扭曲后的帧可能会包含不正确渲染的视觉artefacts，这使得设计这种方案非常困难。过去的工作使用 DNN 来获得好的准确性，但这些方法比较慢。这篇论文提出了 Exwarp，一种基于 reinforcement learning（RL）的方法，可以智能地选择 slower DNN-based extrapolation 和 faster warping-based方法，以提高帧率4倍，并且几乎无法感受到图像质量的下降。
</details></li>
</ul>
<hr>
<h2 id="SwinMM-Masked-Multi-view-with-Swin-Transformers-for-3D-Medical-Image-Segmentation"><a href="#SwinMM-Masked-Multi-view-with-Swin-Transformers-for-3D-Medical-Image-Segmentation" class="headerlink" title="SwinMM: Masked Multi-view with Swin Transformers for 3D Medical Image Segmentation"></a>SwinMM: Masked Multi-view with Swin Transformers for 3D Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12591">http://arxiv.org/abs/2307.12591</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ucsc-vlaa/swinmm">https://github.com/ucsc-vlaa/swinmm</a></li>
<li>paper_authors: Yiqing Wang, Zihan Li, Jieru Mei, Zihao Wei, Li Liu, Chen Wang, Shengtian Sang, Alan Yuille, Cihang Xie, Yuyin Zhou<br>for:这篇论文主要目的是提高自主学习方法 для医疗影像分割。methods:论文使用的方法包括masked multi-view encoder和cross-view decoder，以及一种新的多视图学习方法。results:论文比前一个状态的自主学习方法Swin UNITR显示了明显的优势，能够更好地 интегрирова多视图信息，提高模型的准确率和数据效率。<details>
<summary>Abstract</summary>
Recent advancements in large-scale Vision Transformers have made significant strides in improving pre-trained models for medical image segmentation. However, these methods face a notable challenge in acquiring a substantial amount of pre-training data, particularly within the medical field. To address this limitation, we present Masked Multi-view with Swin Transformers (SwinMM), a novel multi-view pipeline for enabling accurate and data-efficient self-supervised medical image analysis. Our strategy harnesses the potential of multi-view information by incorporating two principal components. In the pre-training phase, we deploy a masked multi-view encoder devised to concurrently train masked multi-view observations through a range of diverse proxy tasks. These tasks span image reconstruction, rotation, contrastive learning, and a novel task that employs a mutual learning paradigm. This new task capitalizes on the consistency between predictions from various perspectives, enabling the extraction of hidden multi-view information from 3D medical data. In the fine-tuning stage, a cross-view decoder is developed to aggregate the multi-view information through a cross-attention block. Compared with the previous state-of-the-art self-supervised learning method Swin UNETR, SwinMM demonstrates a notable advantage on several medical image segmentation tasks. It allows for a smooth integration of multi-view information, significantly boosting both the accuracy and data-efficiency of the model. Code and models are available at https://github.com/UCSC-VLAA/SwinMM/.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="PRIOR-Prototype-Representation-Joint-Learning-from-Medical-Images-and-Reports"><a href="#PRIOR-Prototype-Representation-Joint-Learning-from-Medical-Images-and-Reports" class="headerlink" title="PRIOR: Prototype Representation Joint Learning from Medical Images and Reports"></a>PRIOR: Prototype Representation Joint Learning from Medical Images and Reports</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12577">http://arxiv.org/abs/2307.12577</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qtacierp/prior">https://github.com/qtacierp/prior</a></li>
<li>paper_authors: Pujin Cheng, Li Lin, Junyan Lyu, Yijin Huang, Wenhan Luo, Xiaoying Tang</li>
<li>for: 本文提出了一种基于对比学习的视频语言共同预训练框架，用于学习医学图像和报告之间的对应关系。</li>
<li>methods: 该方法使用了全球对对比方法，以及一种细致的本地对对比模块，以便学习高级клиниче语言特征和低级视觉特征。此外，一种跨Modalities的条件重建模块是用于在训练阶段交换modalities之间的信息。</li>
<li>results: 实验结果表明，提出的方法在五个下游任务中（包括监督分类、零扩展分类、图像到文本检索、semantic segmentation和物体检测）均表现出色，并且在不同的数据集大小设置下也具有优异性。<details>
<summary>Abstract</summary>
Contrastive learning based vision-language joint pre-training has emerged as a successful representation learning strategy. In this paper, we present a prototype representation learning framework incorporating both global and local alignment between medical images and reports. In contrast to standard global multi-modality alignment methods, we employ a local alignment module for fine-grained representation. Furthermore, a cross-modality conditional reconstruction module is designed to interchange information across modalities in the training phase by reconstructing masked images and reports. For reconstructing long reports, a sentence-wise prototype memory bank is constructed, enabling the network to focus on low-level localized visual and high-level clinical linguistic features. Additionally, a non-auto-regressive generation paradigm is proposed for reconstructing non-sequential reports. Experimental results on five downstream tasks, including supervised classification, zero-shot classification, image-to-text retrieval, semantic segmentation, and object detection, show the proposed method outperforms other state-of-the-art methods across multiple datasets and under different dataset size settings. The code is available at https://github.com/QtacierP/PRIOR.
</details>
<details>
<summary>摘要</summary>
医疗图像和报告的共同预训练基于对比学习已经成为一种成功的表示学习策略。在这篇论文中，我们提出了一种原型学习框架，其中包括医疗图像和报告之间的全局和局部对齐。与标准的全局多Modalities对齐方法不同，我们使用了局部对齐模块，以获得细化的表示。此外，我们还设计了跨Modalities的Conditional重建模块，用于在训练阶段交换modalities之间的信息，通过重建遮盖的图像和报告来进行交换。为恢复长报告，我们构建了句子级prototype记忆银行，使得网络能够关注低级的本地视觉和高级的医学语言特征。此外，我们还提出了一种非自动生成 paradigma，用于恢复非顺序报告。实验结果表明，我们的方法在五个下游任务中，包括supervised分类、零shot分类、图像到文本检索、semantic segmentation和物体检测中，都超过了其他当前state-of-the-art方法。代码可以在https://github.com/QtacierP/PRIOR上获取。
</details></li>
</ul>
<hr>
<h2 id="A-Good-Student-is-Cooperative-and-Reliable-CNN-Transformer-Collaborative-Learning-for-Semantic-Segmentation"><a href="#A-Good-Student-is-Cooperative-and-Reliable-CNN-Transformer-Collaborative-Learning-for-Semantic-Segmentation" class="headerlink" title="A Good Student is Cooperative and Reliable: CNN-Transformer Collaborative Learning for Semantic Segmentation"></a>A Good Student is Cooperative and Reliable: CNN-Transformer Collaborative Learning for Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12574">http://arxiv.org/abs/2307.12574</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinjing Zhu, Yunhao Luo, Xu Zheng, Hao Wang, Lin Wang</li>
<li>for: 本研究的目的是解决如何使 convolutional neural network (CNN) 和 vision transformer (ViT) 模型之间进行协同学习，以实现 semantic segmentation 中的可靠知识选择和交换？</li>
<li>methods: 我们提出了一个在线知识distillation (KD) 框架，可以同时学习高效 yet 紧凑的 CNN 和 ViT 模型，并通过两个关键技术突破 CNN 和 ViT 的局限性。首先，我们提出了不同类征distillation (HFD)，以提高学生在低层特征空间的一致性，模仿 CNN 和 ViT 之间的不同特征。其次，我们提出了双向选择distillation (BSD)，可以动态地将选择的知识传递给对方。这包括1) Region-wise BSD 确定知识传递的方向在特征空间中，2) Pixel-wise BSD 确定在极值空间中传递哪些预测知识。</li>
<li>results: 我们的提出的框架在三个 benchmark 数据集上进行了广泛的实验，并与现有的在线distillation方法相比，表现出了很大的提升。此外，我们的方法还证明了在学习 ViT 和 CNN 模型之间协同学习的可能性。<details>
<summary>Abstract</summary>
In this paper, we strive to answer the question "how to collaboratively learn convolutional neural network (CNN)-based and vision transformer (ViT)-based models by selecting and exchanging the reliable knowledge between them for semantic segmentation?" Accordingly, we propose an online knowledge distillation (KD) framework that can simultaneously learn compact yet effective CNN-based and ViT-based models with two key technical breakthroughs to take full advantage of CNNs and ViT while compensating their limitations. Firstly, we propose heterogeneous feature distillation (HFD) to improve students' consistency in low-layer feature space by mimicking heterogeneous features between CNNs and ViT. Secondly, to facilitate the two students to learn reliable knowledge from each other, we propose bidirectional selective distillation (BSD) that can dynamically transfer selective knowledge. This is achieved by 1) region-wise BSD determining the directions of knowledge transferred between the corresponding regions in the feature space and 2) pixel-wise BSD discerning which of the prediction knowledge to be transferred in the logit space. Extensive experiments on three benchmark datasets demonstrate that our proposed framework outperforms the state-of-the-art online distillation methods by a large margin, and shows its efficacy in learning collaboratively between ViT-based and CNN-based models.
</details>
<details>
<summary>摘要</summary>
在本文中，我们努力回答“如何通过选择和交换可靠知识来协同学习卷积神经网络（CNN）和视觉 трансформа（ViT）模型以进行semantic segmentation？”为此，我们提出了在线知识储存（KD）框架，可同时学习高效又紧凑的CNN和ViT模型，并且具有两个关键技术突破，以全面利用CNN和ViT的优势，同时补做它们的局限性。首先，我们提出了不同类征储存（HFD），以提高学生在低层特征空间的一致性，模仿CNN和ViT之间的不同特征。其次，为了让两个学生之间学习可靠的知识，我们提出了双向选择储存（BSD），可动态传递选择的知识。这实现了1）在特征空间中确定 переда知识的方向，2）在逻辑空间中选择要传递的预测知识。我们对三个标准测试集进行了广泛的实验，结果显示，我们提出的框架在在线储存方法中升级了状态之差，并在学习协同CNN和ViT模型方面表现出了 efficacy。
</details></li>
</ul>
<hr>
<h2 id="MataDoc-Margin-and-Text-Aware-Document-Dewarping-for-Arbitrary-Boundary"><a href="#MataDoc-Margin-and-Text-Aware-Document-Dewarping-for-Arbitrary-Boundary" class="headerlink" title="MataDoc: Margin and Text Aware Document Dewarping for Arbitrary Boundary"></a>MataDoc: Margin and Text Aware Document Dewarping for Arbitrary Boundary</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12571">http://arxiv.org/abs/2307.12571</a></li>
<li>repo_url: None</li>
<li>paper_authors: Beiya Dai, Xing li, Qunyi Xie, Yulin Li, Xiameng Qin, Chengquan Zhang, Kun Yao, Junyu Han</li>
<li>for:  DocUNet, DIR300, WarpDoc datasets</li>
<li>methods:  margin regularization, background consistency, word position consistency</li>
<li>results:  superior performance on documents with incomplete boundaries<details>
<summary>Abstract</summary>
Document dewarping from a distorted camera-captured image is of great value for OCR and document understanding. The document boundary plays an important role which is more evident than the inner region in document dewarping. Current learning-based methods mainly focus on complete boundary cases, leading to poor document correction performance of documents with incomplete boundaries. In contrast to these methods, this paper proposes MataDoc, the first method focusing on arbitrary boundary document dewarping with margin and text aware regularizations. Specifically, we design the margin regularization by explicitly considering background consistency to enhance boundary perception. Moreover, we introduce word position consistency to keep text lines straight in rectified document images. To produce a comprehensive evaluation of MataDoc, we propose a novel benchmark ArbDoc, mainly consisting of document images with arbitrary boundaries in four typical scenarios. Extensive experiments confirm the superiority of MataDoc with consideration for the incomplete boundary on ArbDoc and also demonstrate the effectiveness of the proposed method on DocUNet, DIR300, and WarpDoc datasets.
</details>
<details>
<summary>摘要</summary>
文档去扭曲从扭曲捕捉的图像中是很有价值的，尤其是文档边界的角色更加重要。现有的学习型方法主要关注完整边界的文档去扭曲，导致文档修正性能较差。与这些方法不同，本文提出了MataDoc，第一种专门关注任意边界文档去扭曲方法，并添加了边界追求和文本意识regularization。具体来说，我们设计了边界追求的margin regularization，通过考虑背景一致性来增强边界感知。此外，我们引入了文本位置一致的regularization，以保持文本线条在修正后的图像中 straight。为了对MataDoc进行全面的评估，我们提出了一个新的benchmark ArbDoc，主要包括四种典型的文档场景，其中文档边界具有任意形态。广泛的实验表明，MataDoc在ArbDoc上的性能卓越，同时也在DocUNet、DIR300和WarpDoc数据集上表现出色。
</details></li>
</ul>
<hr>
<h2 id="Interpolating-between-Images-with-Diffusion-Models"><a href="#Interpolating-between-Images-with-Diffusion-Models" class="headerlink" title="Interpolating between Images with Diffusion Models"></a>Interpolating between Images with Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12560">http://arxiv.org/abs/2307.12560</a></li>
<li>repo_url: None</li>
<li>paper_authors: Clinton J. Wang, Polina Golland</li>
<li>for: 这篇论文旨在探讨图像生成和编辑中缺失的特性 interpolating between two input images，以扩展图像生成模型的创作应用。</li>
<li>methods: 该论文提出了一种基于潜在扩散模型的零shot interpolating方法，通过在潜在空间中逐渐减少噪声水平进行 interpolating，然后使用文本排序和主体姿态来进行denoising。</li>
<li>results: 该论文通过在多种主体姿态、图像风格和图像内容中进行 interpolating，并通过CLIP选择最高质量图像，以证明该方法可以获得有力的 interpolating结果。<details>
<summary>Abstract</summary>
One little-explored frontier of image generation and editing is the task of interpolating between two input images, a feature missing from all currently deployed image generation pipelines. We argue that such a feature can expand the creative applications of such models, and propose a method for zero-shot interpolation using latent diffusion models. We apply interpolation in the latent space at a sequence of decreasing noise levels, then perform denoising conditioned on interpolated text embeddings derived from textual inversion and (optionally) subject poses. For greater consistency, or to specify additional criteria, we can generate several candidates and use CLIP to select the highest quality image. We obtain convincing interpolations across diverse subject poses, image styles, and image content, and show that standard quantitative metrics such as FID are insufficient to measure the quality of an interpolation. Code and data are available at https://clintonjwang.github.io/interpolation.
</details>
<details>
<summary>摘要</summary>
一个未经探索的前ier是将两个输入图像之间进行 interpolating，这是现有的图像生成管道中缺失的一个特性。我们认为这样的特性可以扩展图像生成模型的创作应用，并提出了采用潜在扩散模型进行零 shot interpolating的方法。我们在潜在空间中逐渐减少噪声水平进行 interpolating，然后使用文本倒转和（可选）主体姿态来进行杜然处理。为了更好地保持一致性，或者设置其他参数，我们可以生成多个候选图像，并使用 CLIP 选择最高质量的图像。我们在不同的主体姿态、图像风格和图像内容中获得了令人满意的 interpolating，并证明了标准的量化指标如 FID 不够Measure  interpolating 的质量。代码和数据可以在 https://clintonjwang.github.io/interpolation 上获取。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-Event-based-Video-Frame-Interpolation"><a href="#Revisiting-Event-based-Video-Frame-Interpolation" class="headerlink" title="Revisiting Event-based Video Frame Interpolation"></a>Revisiting Event-based Video Frame Interpolation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12558">http://arxiv.org/abs/2307.12558</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaben Chen, Yichen Zhu, Dongze Lian, Jiaqi Yang, Yifu Wang, Renrui Zhang, Xinhang Liu, Shenhan Qian, Laurent Kneip, Shenghua Gao</li>
<li>for: 用于提高视频插值的精度和真实性</li>
<li>methods: 使用事件摄像头提供的高时间密度和高噪声特征进行事件导向的Optical flow refinement策略，以及一种分解器-并发的事件核心Synthesis策略</li>
<li>results: 比前方法更加可靠和真实地生成中间帧结果，并且在实验中表明了考虑事件特征的重要性<details>
<summary>Abstract</summary>
Dynamic vision sensors or event cameras provide rich complementary information for video frame interpolation. Existing state-of-the-art methods follow the paradigm of combining both synthesis-based and warping networks. However, few of those methods fully respect the intrinsic characteristics of events streams. Given that event cameras only encode intensity changes and polarity rather than color intensities, estimating optical flow from events is arguably more difficult than from RGB information. We therefore propose to incorporate RGB information in an event-guided optical flow refinement strategy. Moreover, in light of the quasi-continuous nature of the time signals provided by event cameras, we propose a divide-and-conquer strategy in which event-based intermediate frame synthesis happens incrementally in multiple simplified stages rather than in a single, long stage. Extensive experiments on both synthetic and real-world datasets show that these modifications lead to more reliable and realistic intermediate frame results than previous video frame interpolation methods. Our findings underline that a careful consideration of event characteristics such as high temporal density and elevated noise benefits interpolation accuracy.
</details>
<details>
<summary>摘要</summary>
“动态视觉传感器或事件摄像机可提供丰富的补充信息，以帮助视频帧 interpolate。现有的state-of-the-art方法通常采用组合synthesis-based和折叠网络的方法。然而，这些方法很少充分尊重事件流的内在特征。因为事件摄像机只记录了INTENSITY变化和方向，而不是颜色强度，因此从事件中估算光流 arguably 更加困难 than from RGB信息。我们因此提议将RGB信息 integrate into event-guided optical flow refinement策略。此外，由于事件摄像机提供的时间信号具有 quasi-连续性，我们提议采用分段的 divide-and-conquer策略，在多个简化的阶段中进行事件基本中间帧synthesis，而不是在单一、长阶段中进行。广泛的实验表明，这些修改可以更加可靠和真实地 interpolate 视频帧结果，than previous video frame interpolation方法。我们的发现也 подчеркивает，对事件特征，如高时间密度和提高的噪声，的仔细考虑，可以提高插值精度。”
</details></li>
</ul>
<hr>
<h2 id="MFMAN-YOLO-A-Method-for-Detecting-Pole-like-Obstacles-in-Complex-Environment"><a href="#MFMAN-YOLO-A-Method-for-Detecting-Pole-like-Obstacles-in-Complex-Environment" class="headerlink" title="MFMAN-YOLO: A Method for Detecting Pole-like Obstacles in Complex Environment"></a>MFMAN-YOLO: A Method for Detecting Pole-like Obstacles in Complex Environment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12548">http://arxiv.org/abs/2307.12548</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Cai, Hao Wang, Congling Zhou, Yongqiang Wang, Boyu Liu</li>
<li>for: 解决复杂环境中杆体物体特征信息易丢失问题，提高探测精度和实时性。</li>
<li>methods: 提出了一种多尺度混合注意力机制探测算法，通过最优运输函数孟柯诺夫（MK）函数进行匹配，并将多尺度特征分割和混合注意力机制应用于复杂环境中。</li>
<li>results: 实验结果显示，方法的检测精度、回归率和均值精度分别为94.7%、93.1%和97.4%，检测帧率达400f&#x2F;s。这种方法可以在实时和精度高的情况下探测复杂环境中的杆体物体。<details>
<summary>Abstract</summary>
In real-world traffic, there are various uncertainties and complexities in road and weather conditions. To solve the problem that the feature information of pole-like obstacles in complex environments is easily lost, resulting in low detection accuracy and low real-time performance, a multi-scale hybrid attention mechanism detection algorithm is proposed in this paper. First, the optimal transport function Monge-Kantorovich (MK) is incorporated not only to solve the problem of overlapping multiple prediction frames with optimal matching but also the MK function can be regularized to prevent model over-fitting; then, the features at different scales are up-sampled separately according to the optimized efficient multi-scale feature pyramid. Finally, the extraction of multi-scale feature space channel information is enhanced in complex environments based on the hybrid attention mechanism, which suppresses the irrelevant complex environment background information and focuses the feature information of pole-like obstacles. Meanwhile, this paper conducts real road test experiments in a variety of complex environments. The experimental results show that the detection precision, recall, and average precision of the method are 94.7%, 93.1%, and 97.4%, respectively, and the detection frame rate is 400 f/s. This research method can detect pole-like obstacles in a complex road environment in real time and accurately, which further promotes innovation and progress in the field of automatic driving.
</details>
<details>
<summary>摘要</summary>
实际交通中有各种不确定性和复杂性，以致 pole-like obstacles 的特征信息在复杂环境中易丢失，导致检测精度低下、实时性低下。为解决这个问题，本文提出了一种多尺度混合注意力机制检测算法。首先，通过 Monge-Kantorovich（MK）函数进行最佳运输函数，不仅可以解决多个预测帧的最佳匹配问题，还可以将 MK 函数进行正则化，以防止模型过拟合；然后，在不同尺度上分别更新独立的高效多尺度特征 pyramid。最后，在复杂环境中提高多尺度特征空间通道信息抽取的能力，通过混合注意力机制，抑制不相关的复杂环境背景信息，专注于检测 pole-like obstacles 的特征信息。同时，本文在实际公路测试中进行了多种复杂环境的实验，实验结果表明，该方法的检测精度、回归率和平均精度分别为 94.7%、93.1% 和 97.4%，检测帧率为 400 f/s。这种检测方法可以在复杂交通环境中实时和准确地检测 pole-like obstacles，为自动驾驶技术的进一步创新和发展做出了贡献。
</details></li>
</ul>
<hr>
<h2 id="Towards-Generalizable-Deepfake-Detection-by-Primary-Region-Regularization"><a href="#Towards-Generalizable-Deepfake-Detection-by-Primary-Region-Regularization" class="headerlink" title="Towards Generalizable Deepfake Detection by Primary Region Regularization"></a>Towards Generalizable Deepfake Detection by Primary Region Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12534">http://arxiv.org/abs/2307.12534</a></li>
<li>repo_url: None</li>
<li>paper_authors: Harry Cheng, Yangyang Guo, Tianyi Wang, Liqiang Nie, Mohan Kankanhalli</li>
<li>for: 提高深伪检测方法的泛化能力，对于未见过的伪造和修改方法进行扩展</li>
<li>methods: 使用新的调整方法来提高深伪检测方法的泛化能力，包括删除主要区域图像的调整</li>
<li>results: 与多个基eline比较，提高了平均性能表现6%，并且与一些现有的基eline竞争Here’s the breakdown of each sentence:* “for”: This sentence indicates the purpose of the paper, which is to improve the generalization ability of deepfake detection methods.* “methods”: This sentence describes the approach used in the paper to achieve the purpose, which is to use a novel regularization perspective to enhance the generalization capability of deepfake detectors.* “results”: This sentence summarizes the performance of the proposed method compared to baseline methods, showing an average improvement of 6% and competitive performance with state-of-the-art baselines.<details>
<summary>Abstract</summary>
The existing deepfake detection methods have reached a bottleneck in generalizing to unseen forgeries and manipulation approaches. Based on the observation that the deepfake detectors exhibit a preference for overfitting the specific primary regions in input, this paper enhances the generalization capability from a novel regularization perspective. This can be simply achieved by augmenting the images through primary region removal, thereby preventing the detector from over-relying on data bias. Our method consists of two stages, namely the static localization for primary region maps, as well as the dynamic exploitation of primary region masks. The proposed method can be seamlessly integrated into different backbones without affecting their inference efficiency. We conduct extensive experiments over three widely used deepfake datasets - DFDC, DF-1.0, and Celeb-DF with five backbones. Our method demonstrates an average performance improvement of 6% across different backbones and performs competitively with several state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
现有的深伪检测方法已达到泛化到未经见 forgery 和 manipulation 方法的瓶颈。基于观察到深伪检测器偏好特定的主要区域在输入中过拟合的观察，这篇文章提高了泛化能力从一个新的规范化视角。这可以简单地通过除去输入图像的主要区域来实现，以防止检测器过于依赖数据偏好。我们的方法包括两个阶段：首先，静态地LOCALIZATION FOR PRIMARY REGION MAPS，然后是动态利用主要区域面罩。我们的方法可以轻松地与不同的背bone结合使用，无需影响其推理效率。我们在DFDC、DF-1.0和Celeb-DF三个广泛使用的深伪数据集上进行了广泛的实验，我们的方法在不同的背bone上显示了平均提高6%的性能，并与一些状态机器人的基准值竞争。
</details></li>
</ul>
<hr>
<h2 id="On-the-Connection-between-Pre-training-Data-Diversity-and-Fine-tuning-Robustness"><a href="#On-the-Connection-between-Pre-training-Data-Diversity-and-Fine-tuning-Robustness" class="headerlink" title="On the Connection between Pre-training Data Diversity and Fine-tuning Robustness"></a>On the Connection between Pre-training Data Diversity and Fine-tuning Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12532">http://arxiv.org/abs/2307.12532</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vivek Ramanujan, Thao Nguyen, Sewoong Oh, Ludwig Schmidt, Ali Farhadi<br>for: 这个论文的目的是研究预训练策略对深度学习模型的泛化性的影响。methods: 作者使用了多种自然和 sintetic 数据源来生成不同的预训练分布，并通过评估这些预训练分布对下游模型的抗衰假设来研究预训练分布的影响。results: 研究发现，预训练分布中数据量的变化是主要影响下游模型的抗衰假设的因素，而其他因素对下游模型的抗衰假设具有有限的影响。例如，减少 ImageNet 预训练类别的数量，同时增加每个类别中的图像数量（即保持总数据量不变）并不会影响下游模型的抗衰假设。<details>
<summary>Abstract</summary>
Pre-training has been widely adopted in deep learning to improve model performance, especially when the training data for a target task is limited. In our work, we seek to understand the implications of this training strategy on the generalization properties of downstream models. More specifically, we ask the following question: how do properties of the pre-training distribution affect the robustness of a fine-tuned model? The properties we explore include the label space, label semantics, image diversity, data domains, and data quantity of the pre-training distribution. We find that the primary factor influencing downstream effective robustness (Taori et al., 2020) is data quantity, while other factors have limited significance. For example, reducing the number of ImageNet pre-training classes by 4x while increasing the number of images per class by 4x (that is, keeping total data quantity fixed) does not impact the robustness of fine-tuned models. We demonstrate our findings on pre-training distributions drawn from various natural and synthetic data sources, primarily using the iWildCam-WILDS distribution shift as a test for downstream robustness.
</details>
<details>
<summary>摘要</summary>
pré-entraînement a été largement adopté dans l'apprentissage profond pour améliorer les performances des modèles, especialment lorsque les données d'entraînement pour une tâche cible sont limitées. Dans notre travail, nous voulons comprendre les implications de cette stratégie d'entraînement sur les propriétés de généralisation des modèles downstream. Plus spécifiquement, nous posons la question suivante : comment les propriétés de la distribution de pré-entraînement affectent-elles la robustesse des modèles fine-tunés ? Les propriétés que nous explorons comprennent l'espace de labels, les semantiques de labels, la diversité d'images, les domaines de données et la quantité de données de la distribution de pré-entraînement. Nous trouvons que le facteur primordial influençant la robustesse effective downstream (Taori et al., 2020) est la quantité de données, tandis que les autres facteurs ont un impact limité. Par exemple, en réduisant le nombre de classes de pré-entraînement d'ImageNet de 4 fois while augmentant le nombre d'images par classe de 4 fois (c'est-à-dire en gardant la quantité totale de données fixée), n'a pas d'impact sur la robustesse des modèles fine-tunés. Nous démontrons nos résultats sur des distributions de pré-entraînement tirées de divers sources de données naturelles et synthétiques, en utilisant principalement la distribution shift iWildCam-WILDS comme un test pour la robustesse downstream.
</details></li>
</ul>
<hr>
<h2 id="Entropy-Transformer-Networks-A-Learning-Approach-via-Tangent-Bundle-Data-Manifold"><a href="#Entropy-Transformer-Networks-A-Learning-Approach-via-Tangent-Bundle-Data-Manifold" class="headerlink" title="Entropy Transformer Networks: A Learning Approach via Tangent Bundle Data Manifold"></a>Entropy Transformer Networks: A Learning Approach via Tangent Bundle Data Manifold</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12517">http://arxiv.org/abs/2307.12517</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ijcnn2023/ESTN">https://github.com/ijcnn2023/ESTN</a></li>
<li>paper_authors: Pourya Shamsolmoali, Masoumeh Zareapoor</li>
<li>for: 该 paper ocuses on developing an accurate and efficient image transformation approach for Convolutional Neural Networks (CNNs) architectures.</li>
<li>methods: 该 paper 用 novel Entropy Spatial Transformer Networks (ESTN)  interpolate on the data manifold distributions, 使用 random samples 和 tangent space 计算 transformer parameters. 同时， authors 还提出了一种简单 yet effective technique 来 normalize the non-zero values of convolution operation.</li>
<li>results:  experiments 表明， ESTN 可以在多种 computer vision tasks 中提高预测精度， 包括图像重建和分类， 而减少计算成本。<details>
<summary>Abstract</summary>
This paper focuses on an accurate and fast interpolation approach for image transformation employed in the design of CNN architectures. Standard Spatial Transformer Networks (STNs) use bilinear or linear interpolation as their interpolation, with unrealistic assumptions about the underlying data distributions, which leads to poor performance under scale variations. Moreover, STNs do not preserve the norm of gradients in propagation due to their dependency on sparse neighboring pixels. To address this problem, a novel Entropy STN (ESTN) is proposed that interpolates on the data manifold distributions. In particular, random samples are generated for each pixel in association with the tangent space of the data manifold and construct a linear approximation of their intensity values with an entropy regularizer to compute the transformer parameters. A simple yet effective technique is also proposed to normalize the non-zero values of the convolution operation, to fine-tune the layers for gradients' norm-regularization during training. Experiments on challenging benchmarks show that the proposed ESTN can improve predictive accuracy over a range of computer vision tasks, including image reconstruction, and classification, while reducing the computational cost.
</details>
<details>
<summary>摘要</summary>
To address these problems, a novel Entropy STN (ESTN) is proposed that interpolates on the data manifold distributions. Specifically, random samples are generated for each pixel in association with the tangent space of the data manifold, and a linear approximation of their intensity values is computed with an entropy regularizer to compute the transformer parameters. Additionally, a simple yet effective technique is proposed to normalize the non-zero values of the convolution operation to fine-tune the layers for gradients' norm-regularization during training.Experiments on challenging benchmarks show that the proposed ESTN can improve predictive accuracy over a range of computer vision tasks, including image reconstruction and classification, while reducing the computational cost.
</details></li>
</ul>
<hr>
<h2 id="Cross-Contrasting-Feature-Perturbation-for-Domain-Generalization"><a href="#Cross-Contrasting-Feature-Perturbation-for-Domain-Generalization" class="headerlink" title="Cross Contrasting Feature Perturbation for Domain Generalization"></a>Cross Contrasting Feature Perturbation for Domain Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12502">http://arxiv.org/abs/2307.12502</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hackmebroo/ccfp">https://github.com/hackmebroo/ccfp</a></li>
<li>paper_authors: Chenming Li, Daoan Zhang, Wenjian Huang, Jianguo Zhang<br>for: This paper focuses on the problem of domain generalization (DG) and proposes a novel framework called Cross Contrasting Feature Perturbation (CCFP) to simulate domain shift and improve the robustness of the model.methods: The proposed CCFP framework uses an online one-stage approach and generates perturbed features in the latent space while regularizing the model prediction against domain shift. The framework includes learnable feature perturbations and semantic consistency constraints to improve the quality of the perturbed features.results: The proposed method outperforms the previous state-of-the-art in the standard DomainBed benchmark with a strict evaluation protocol. Quantitative analyses show that the method can effectively alleviate the domain shift problem in out-of-distribution (OOD) scenarios.<details>
<summary>Abstract</summary>
Domain generalization (DG) aims to learn a robust model from source domains that generalize well on unseen target domains. Recent studies focus on generating novel domain samples or features to diversify distributions complementary to source domains. Yet, these approaches can hardly deal with the restriction that the samples synthesized from various domains can cause semantic distortion. In this paper, we propose an online one-stage Cross Contrasting Feature Perturbation (CCFP) framework to simulate domain shift by generating perturbed features in the latent space while regularizing the model prediction against domain shift. Different from the previous fixed synthesizing strategy, we design modules with learnable feature perturbations and semantic consistency constraints. In contrast to prior work, our method does not use any generative-based models or domain labels. We conduct extensive experiments on a standard DomainBed benchmark with a strict evaluation protocol for a fair comparison. Comprehensive experiments show that our method outperforms the previous state-of-the-art, and quantitative analyses illustrate that our approach can alleviate the domain shift problem in out-of-distribution (OOD) scenarios.
</details>
<details>
<summary>摘要</summary>
域间泛化（DG）目标是从源域学习一个可以在未看过的目标域中进行泛化的模型。最近的研究主要关注生成新的域样本或特征以增加分布的多样性，但这些方法很难处理源域样本生成的semantic扭曲问题。在这篇论文中，我们提出了在线一阶段 Cross Contrasting Feature Perturbation（CCFP）框架，通过在幂空间生成受损特征来模拟域转移，同时对模型预测进行域转移 regularization。与前一代固定生成策略不同，我们设计了可学习的特征干扰和semantic一致约束。与先前的工作不同，我们的方法不使用任何生成型模型或域标签。我们在DomainBed标准测试床上进行了广泛的实验，并对比评估准则进行严格的评估。广泛的实验结果表明，我们的方法超过了先前的最佳实现，并且量化分析表明，我们的方法可以在OOD（out-of-distribution）场景中缓解域转移问题。
</details></li>
</ul>
<hr>
<h2 id="AdvDiff-Generating-Unrestricted-Adversarial-Examples-using-Diffusion-Models"><a href="#AdvDiff-Generating-Unrestricted-Adversarial-Examples-using-Diffusion-Models" class="headerlink" title="AdvDiff: Generating Unrestricted Adversarial Examples using Diffusion Models"></a>AdvDiff: Generating Unrestricted Adversarial Examples using Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12499">http://arxiv.org/abs/2307.12499</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuelong Dai, Kaisheng Liang, Bin Xiao</li>
<li>for: 本研究旨在提出一种新的方法，即AdvDiff，用于生成不受限制的反击示例，以攻击深度学习模型和反击技术。</li>
<li>methods: 本研究使用了两种新的反击指导技术，即反击扩散模型的梯度导航和反击扩散模型的反向生成过程。这两种技术可以生成高质量、实用的反击示例，并且可以兼顾target类фика器的解释性。</li>
<li>results: 实验结果表明，AdvDiff可以高效地生成不受限制的反击示例，并且在MNIST和ImageNet datasets上的实验结果表明，AdvDiff的攻击性和生成质量都高于基于GAN的方法。<details>
<summary>Abstract</summary>
Unrestricted adversarial attacks present a serious threat to deep learning models and adversarial defense techniques. They pose severe security problems for deep learning applications because they can effectively bypass defense mechanisms. However, previous attack methods often utilize Generative Adversarial Networks (GANs), which are not theoretically provable and thus generate unrealistic examples by incorporating adversarial objectives, especially for large-scale datasets like ImageNet. In this paper, we propose a new method, called AdvDiff, to generate unrestricted adversarial examples with diffusion models. We design two novel adversarial guidance techniques to conduct adversarial sampling in the reverse generation process of diffusion models. These two techniques are effective and stable to generate high-quality, realistic adversarial examples by integrating gradients of the target classifier interpretably. Experimental results on MNIST and ImageNet datasets demonstrate that AdvDiff is effective to generate unrestricted adversarial examples, which outperforms GAN-based methods in terms of attack performance and generation quality.
</details>
<details>
<summary>摘要</summary>
深度学习模型面临无限制 adversarial 攻击的威胁，这些攻击可以快速绕过防御机制。然而，过去的攻击方法常用生成式对抗网络（GAN），这些网络不能有效地证明其可行性，特别是在大规模的数据集如 ImageNet 上。在本文中，我们提出一种新的方法，称为 AdvDiff，以使用扩散模型生成无限制 adversarial 示例。我们设计了两种新的对抗导航技术，以在扩散模型的反生成过程中进行对抗采样。这两种技术能够生成高质量、实用的 adversarial 示例，通过可见的梯度来具体化目标分类器的解释。实验结果表明，AdvDiff 在 MNIST 和 ImageNet 数据集上效果地生成了无限制 adversarial 示例，其性能和生成质量都高于基于 GAN 的方法。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Data-Distillation-Do-Not-Overlook-Calibration"><a href="#Rethinking-Data-Distillation-Do-Not-Overlook-Calibration" class="headerlink" title="Rethinking Data Distillation: Do Not Overlook Calibration"></a>Rethinking Data Distillation: Do Not Overlook Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12463">http://arxiv.org/abs/2307.12463</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dongyaozhu/calibrate-networks-trained-on-distilled-datasets">https://github.com/dongyaozhu/calibrate-networks-trained-on-distilled-datasets</a></li>
<li>paper_authors: Dongyao Zhu, Bowen Lei, Jie Zhang, Yanbo Fang, Ruqi Zhang, Yiqun Xie, Dongkuan Xu</li>
<li>for: 本文旨在解决由大型源数据抽取而得到的神经网络经常产生过于自信的输出问题，通过calibration方法来修正。</li>
<li>methods: 本文提出了Masked Temperature Scaling (MTS)和Masked Distillation Training (MDT)两种方法，可以在神经网络训练过程中进行数据缩写和混合，以提高神经网络的准确率和可靠性。</li>
<li>results: 本文的实验结果表明，使用MTS和MDT可以有效地修正由大型源数据抽取而得到的神经网络，提高其准确率和可靠性，同时保持数据缩写的效率。<details>
<summary>Abstract</summary>
Neural networks trained on distilled data often produce over-confident output and require correction by calibration methods. Existing calibration methods such as temperature scaling and mixup work well for networks trained on original large-scale data. However, we find that these methods fail to calibrate networks trained on data distilled from large source datasets. In this paper, we show that distilled data lead to networks that are not calibratable due to (i) a more concentrated distribution of the maximum logits and (ii) the loss of information that is semantically meaningful but unrelated to classification tasks. To address this problem, we propose Masked Temperature Scaling (MTS) and Masked Distillation Training (MDT) which mitigate the limitations of distilled data and achieve better calibration results while maintaining the efficiency of dataset distillation.
</details>
<details>
<summary>摘要</summary>
neural networks 经过精炼数据训练后常会产生过度自信的输出，需要使用均衡方法进行调整。现有的均衡方法，如温度升降和混合方法，对原始大规模数据训练的网络具有良好的效果。然而，我们发现这些方法对含拟合数据训练的网络无法进行均衡。在这篇论文中，我们发现了含拟合数据导致网络无法均衡的两个问题：（一）含拟合数据导致网络的最大幂值分布更加集中，（二）含拟合数据丢失了与分类任务无关 yet semantically meaningful的信息。为解决这问题，我们提出了Masked Temperature Scaling（MTS）和Masked Distillation Training（MDT）两种方法，它们可以缓解含拟合数据的限制，实现更好的均衡结果，同时保持数据精炼的效率。
</details></li>
</ul>
<hr>
<h2 id="Robust-face-anti-spoofing-framework-with-Convolutional-Vision-Transformer"><a href="#Robust-face-anti-spoofing-framework-with-Convolutional-Vision-Transformer" class="headerlink" title="Robust face anti-spoofing framework with Convolutional Vision Transformer"></a>Robust face anti-spoofing framework with Convolutional Vision Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12459">http://arxiv.org/abs/2307.12459</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunseung Lee, Youngjun Kwak, Jinho Shin</li>
<li>for: 本研究旨在提高人脸验证过程中的防御性能，对抗真实的演示攻击。</li>
<li>methods: 本研究使用自注意力和卷积层对人脸图像进行全球和局部学习，以提高人脸识别性能。</li>
<li>results: 该模型在不同数据集中的跨域设定中表现出了7.3%$p$和12.9%$p$的提升，并在九个参考模型中得到了最高的平均排名。<details>
<summary>Abstract</summary>
Owing to the advances in image processing technology and large-scale datasets, companies have implemented facial authentication processes, thereby stimulating increased focus on face anti-spoofing (FAS) against realistic presentation attacks. Recently, various attempts have been made to improve face recognition performance using both global and local learning on face images; however, to the best of our knowledge, this is the first study to investigate whether the robustness of FAS against domain shifts is improved by considering global information and local cues in face images captured using self-attention and convolutional layers. This study proposes a convolutional vision transformer-based framework that achieves robust performance for various unseen domain data. Our model resulted in 7.3%$p$ and 12.9%$p$ increases in FAS performance compared to models using only a convolutional neural network or vision transformer, respectively. It also shows the highest average rank in sub-protocols of cross-dataset setting over the other nine benchmark models for domain generalization.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:因为图像处理技术的进步和大规模数据集，公司已经实施了人脸验证过程，从而促使更多关注面对攻击（FAS）的真实演示攻击。最近，有很多尝试来提高人脸识别性能使用全球和地方学习方法，但到目前为止，这是第一个研究是否可以通过考虑全球信息和地方指示来提高人脸验证性能对域shift。这种研究提出了基于 convolutional vision transformer 框架的方法，实现了对不同数据集的 Robust 性能。我们的模型比只使用 convolutional neural network 或 vision transformer 模型使用时提高了7.3%$p$ 和 12.9%$p$ 的 FAS性能。它还在横跨数据集设定下的各个子协议中显示了最高的平均排名。
</details></li>
</ul>
<hr>
<h2 id="EnTri-Ensemble-Learning-with-Tri-level-Representations-for-Explainable-Scene-Recognition"><a href="#EnTri-Ensemble-Learning-with-Tri-level-Representations-for-Explainable-Scene-Recognition" class="headerlink" title="EnTri: Ensemble Learning with Tri-level Representations for Explainable Scene Recognition"></a>EnTri: Ensemble Learning with Tri-level Representations for Explainable Scene Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12442">http://arxiv.org/abs/2307.12442</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amirhossein Aminimehr, Amirali Molaei, Erik Cambria</li>
<li>for: 提高Scene recognition的可读性和可解释性，同时提高分类精度。</li>
<li>methods: 使用ensemble学习，包括Pixel-level、Semantic segmentation-level和Object class和frequency level的特征编码，以及不同复杂度的特征编码策略。</li>
<li>results: 在MIT67、SUN397和UIUC8 datasets上实现了87.69%、75.56%和99.17%的分类精度，比 estado-of-the-art方法有竞争力。<details>
<summary>Abstract</summary>
Scene recognition based on deep-learning has made significant progress, but there are still limitations in its performance due to challenges posed by inter-class similarities and intra-class dissimilarities. Furthermore, prior research has primarily focused on improving classification accuracy, yet it has given less attention to achieving interpretable, precise scene classification. Therefore, we are motivated to propose EnTri, an ensemble scene recognition framework that employs ensemble learning using a hierarchy of visual features. EnTri represents features at three distinct levels of detail: pixel-level, semantic segmentation-level, and object class and frequency level. By incorporating distinct feature encoding schemes of differing complexity and leveraging ensemble strategies, our approach aims to improve classification accuracy while enhancing transparency and interpretability via visual and textual explanations. To achieve interpretability, we devised an extension algorithm that generates both visual and textual explanations highlighting various properties of a given scene that contribute to the final prediction of its category. This includes information about objects, statistics, spatial layout, and textural details. Through experiments on benchmark scene classification datasets, EnTri has demonstrated superiority in terms of recognition accuracy, achieving competitive performance compared to state-of-the-art approaches, with an accuracy of 87.69%, 75.56%, and 99.17% on the MIT67, SUN397, and UIUC8 datasets, respectively.
</details>
<details>
<summary>摘要</summary>
EnTri represents features at three distinct levels of detail: pixel-level, semantic segmentation-level, and object class and frequency level. By incorporating distinct feature encoding schemes of differing complexity and leveraging ensemble strategies, our approach aims to improve classification accuracy while enhancing transparency and interpretability via visual and textual explanations.To achieve interpretability, we devised an extension algorithm that generates both visual and textual explanations highlighting various properties of a given scene that contribute to the final prediction of its category. This includes information about objects, statistics, spatial layout, and textural details.Through experiments on benchmark scene classification datasets, EnTri has demonstrated superiority in terms of recognition accuracy, achieving competitive performance compared to state-of-the-art approaches, with an accuracy of 87.69%, 75.56%, and 99.17% on the MIT67, SUN397, and UIUC8 datasets, respectively.
</details></li>
</ul>
<hr>
<h2 id="SwIPE-Efficient-and-Robust-Medical-Image-Segmentation-with-Implicit-Patch-Embeddings"><a href="#SwIPE-Efficient-and-Robust-Medical-Image-Segmentation-with-Implicit-Patch-Embeddings" class="headerlink" title="SwIPE: Efficient and Robust Medical Image Segmentation with Implicit Patch Embeddings"></a>SwIPE: Efficient and Robust Medical Image Segmentation with Implicit Patch Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12429">http://arxiv.org/abs/2307.12429</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yejia Zhang, Pengfei Gu, Nishchal Sapkota, Danny Z. Chen</li>
<li>for: 这个研究的目的是为了提出一个新的医疗影像分类方法，以改善现有的离散表示方法，并且能够获得更好的本地细节和全局形状匹配。</li>
<li>methods: 这个方法使用了隐藏 нейрон网络（INR）来学习连续表示，并且预测形状在图像级层次，而不是点级层次或全图像级层次，以获得更好的本地边界定义和全局形状匹配。</li>
<li>results: 实验结果显示，这个方法可以与现有的离散方法进行比较，并且在两个任务（2D肿瘤分类和3D腹部器官分类）上获得了更好的结果，并且需要较少的参数。此外，这个方法也展示了较好的数据效率和数据类型的适应性。<details>
<summary>Abstract</summary>
Modern medical image segmentation methods primarily use discrete representations in the form of rasterized masks to learn features and generate predictions. Although effective, this paradigm is spatially inflexible, scales poorly to higher-resolution images, and lacks direct understanding of object shapes. To address these limitations, some recent works utilized implicit neural representations (INRs) to learn continuous representations for segmentation. However, these methods often directly adopted components designed for 3D shape reconstruction. More importantly, these formulations were also constrained to either point-based or global contexts, lacking contextual understanding or local fine-grained details, respectively--both critical for accurate segmentation. To remedy this, we propose a novel approach, SwIPE (Segmentation with Implicit Patch Embeddings), that leverages the advantages of INRs and predicts shapes at the patch level--rather than at the point level or image level--to enable both accurate local boundary delineation and global shape coherence. Extensive evaluations on two tasks (2D polyp segmentation and 3D abdominal organ segmentation) show that SwIPE significantly improves over recent implicit approaches and outperforms state-of-the-art discrete methods with over 10x fewer parameters. Our method also demonstrates superior data efficiency and improved robustness to data shifts across image resolutions and datasets. Code is available on Github.
</details>
<details>
<summary>摘要</summary>
现代医学图像分割方法主要使用精度为矩阵的批处理来学习特征和生成预测。虽然有效，但这种方法具有不可修复的局限性，包括空间不灵活、高分辨率图像扩展不良、直接没有对物体形状的理解。为了解决这些限制，一些最近的研究使用了卷积神经网络（INR）来学习连续表示，以提高分割精度。然而，这些方法通常直接采用了设计 для三维形态重建的组件，而且受限于点级或全局上下文，缺乏当地细节或形态准确性。为了改善这个问题，我们提出了一种新的方法：SwIPE（分割with Implicit Patch Embeddings），它利用INR的优点，预测形状在patch水平（而不是点级或图像级），以实现准确的本地边界定义和全局形态协调。我们对两个任务（2D菌体分割和3D腹部器官分割）进行了广泛的评估，结果表明SwIPE在最近的隐式方法中显著提高，并在精度和数据效率方面超过了状态机 discrete方法。我们的方法还在数据偏移和图像分辨率之间具有更好的数据效率和数据弹性。代码可以在Github上获取。
</details></li>
</ul>
<hr>
<h2 id="Augmented-Box-Replay-Overcoming-Foreground-Shift-for-Incremental-Object-Detection"><a href="#Augmented-Box-Replay-Overcoming-Foreground-Shift-for-Incremental-Object-Detection" class="headerlink" title="Augmented Box Replay: Overcoming Foreground Shift for Incremental Object Detection"></a>Augmented Box Replay: Overcoming Foreground Shift for Incremental Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12427">http://arxiv.org/abs/2307.12427</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/YuyangSunshine/ABR_IOD">https://github.com/YuyangSunshine/ABR_IOD</a></li>
<li>paper_authors: Liu Yuyang, Cong Yang, Goswami Dipam, Liu Xialei, Joost van de Weijer<br>for: 这篇论文的目的是解决对incremental object detection（IOD）中的catastrophic forgetting问题。methods: 本文使用了一个称为Augmented Box Replay（ABR）的新方法，它将仅储存和重复过去任务中的前景物体，以避免预义遗传问题。此外，本文也提出了一种创新的注意力捕捉RoI特征的Attentive RoI Distillation损失，它使用领域特征的空间注意力来锁定现在的模型对过去模型的重要信息的注意。results: ABR有效地降低了之前任务的遗传，同时保持了目前任务的高柔养性。此外，ABR还能够储存和重复的减少储存需求，相比于标准的图像重复。实验结果表明，本文的模型在Pascal-VOC和COCO dataset上具有现场的表现。<details>
<summary>Abstract</summary>
In incremental learning, replaying stored samples from previous tasks together with current task samples is one of the most efficient approaches to address catastrophic forgetting. However, unlike incremental classification, image replay has not been successfully applied to incremental object detection (IOD). In this paper, we identify the overlooked problem of foreground shift as the main reason for this. Foreground shift only occurs when replaying images of previous tasks and refers to the fact that their background might contain foreground objects of the current task. To overcome this problem, a novel and efficient Augmented Box Replay (ABR) method is developed that only stores and replays foreground objects and thereby circumvents the foreground shift problem. In addition, we propose an innovative Attentive RoI Distillation loss that uses spatial attention from region-of-interest (RoI) features to constrain current model to focus on the most important information from old model. ABR significantly reduces forgetting of previous classes while maintaining high plasticity in current classes. Moreover, it considerably reduces the storage requirements when compared to standard image replay. Comprehensive experiments on Pascal-VOC and COCO datasets support the state-of-the-art performance of our model.
</details>
<details>
<summary>摘要</summary>
在增量学习中，重新播放之前任务的样本和当前任务的样本是解决快速卷积承忘的一种最有效的方法。然而，与增量分类不同，图像重新播放在增量物体检测（IOD）中尚未得到成功。在这篇论文中，我们认为背景变化导致的前景偏移是主要的问题。背景变化仅发生在重新播放前任务的图像时，并且指的是图像的背景中可能包含当前任务的前景对象。为解决这个问题，我们开发了一种新的和高效的增量盒子重播（ABR）方法，该方法仅存储和重播前景对象，因此可以避免前景偏移问题。此外，我们提出了一种创新的注意力捕捉的区域特征练习损失（Attentive RoI Distillation loss），该损失使当前模型通过区域特征中的注意力来约束当前模型关注到最重要的信息。ABR显著减少了之前类的忘记，同时保持当前类的高灵活性。此外，它比标准图像重新播放要减少存储需求。我们在 Pascal-VOC 和 COCO 数据集上进行了广泛的实验，并证明了我们的模型的状态级表现。
</details></li>
</ul>
<hr>
<h2 id="TransNet-Transparent-Object-Manipulation-Through-Category-Level-Pose-Estimation"><a href="#TransNet-Transparent-Object-Manipulation-Through-Category-Level-Pose-Estimation" class="headerlink" title="TransNet: Transparent Object Manipulation Through Category-Level Pose Estimation"></a>TransNet: Transparent Object Manipulation Through Category-Level Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12400">http://arxiv.org/abs/2307.12400</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huijie Zhang, Anthony Opipari, Xiaotong Chen, Jiyue Zhu, Zeren Yu, Odest Chadwicke Jenkins</li>
<li>for: 本研究旨在提高自动化透明物体检测和操作系统的可靠性和精度，特别是在透明物体上进行Category-levelpose estimation。</li>
<li>methods: 本研究提出了一种两stage管道 named TransNet，包括本地化深度完成和表面法向估计两个阶段。TransNet使用了一种新的surface normal estimation方法，并且使用了一种新的depth completion方法来提高pose estimation的准确性。</li>
<li>results: 对于一个大规模的透明物体 dataset，TransNet achieved improved pose estimation accuracy compared to a state-of-the-art category-level pose estimation approach. In addition, TransNet was used to build an autonomous transparent object manipulation system for robotic pick-and-place and pouring tasks, which demonstrated its effectiveness in real-world applications.<details>
<summary>Abstract</summary>
Transparent objects present multiple distinct challenges to visual perception systems. First, their lack of distinguishing visual features makes transparent objects harder to detect and localize than opaque objects. Even humans find certain transparent surfaces with little specular reflection or refraction, like glass doors, difficult to perceive. A second challenge is that depth sensors typically used for opaque object perception cannot obtain accurate depth measurements on transparent surfaces due to their unique reflective properties. Stemming from these challenges, we observe that transparent object instances within the same category, such as cups, look more similar to each other than to ordinary opaque objects of that same category. Given this observation, the present paper explores the possibility of category-level transparent object pose estimation rather than instance-level pose estimation. We propose \textit{\textbf{TransNet}, a two-stage pipeline that estimates category-level transparent object pose using localized depth completion and surface normal estimation. TransNet is evaluated in terms of pose estimation accuracy on a large-scale transparent object dataset and compared to a state-of-the-art category-level pose estimation approach. Results from this comparison demonstrate that TransNet achieves improved pose estimation accuracy on transparent objects. Moreover, we use TransNet to build an autonomous transparent object manipulation system for robotic pick-and-place and pouring tasks.
</details>
<details>
<summary>摘要</summary>
trasparent objects present multiple distinct challenges to visual perception systems. First, their lack of distinguishing visual features makes transparent objects harder to detect and localize than opaque objects. Even humans find certain transparent surfaces with little specular reflection or refraction, like glass doors, difficult to perceive. A second challenge is that depth sensors typically used for opaque object perception cannot obtain accurate depth measurements on transparent surfaces due to their unique reflective properties. Stemming from these challenges, we observe that transparent object instances within the same category, such as cups, look more similar to each other than to ordinary opaque objects of that same category. Given this observation, the present paper explores the possibility of category-level transparent object pose estimation rather than instance-level pose estimation. We propose \textbf{\textit{TransNet}， a two-stage pipeline that estimates category-level transparent object pose using localized depth completion and surface normal estimation. TransNet is evaluated in terms of pose estimation accuracy on a large-scale transparent object dataset and compared to a state-of-the-art category-level pose estimation approach. Results from this comparison demonstrate that TransNet achieves improved pose estimation accuracy on transparent objects. Moreover, we use TransNet to build an autonomous transparent object manipulation system for robotic pick-and-place and pouring tasks.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Iterative-Robust-Visual-Grounding-with-Masked-Reference-based-Centerpoint-Supervision"><a href="#Iterative-Robust-Visual-Grounding-with-Masked-Reference-based-Centerpoint-Supervision" class="headerlink" title="Iterative Robust Visual Grounding with Masked Reference based Centerpoint Supervision"></a>Iterative Robust Visual Grounding with Masked Reference based Centerpoint Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12392">http://arxiv.org/abs/2307.12392</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cv516buaa/ir-vg">https://github.com/cv516buaa/ir-vg</a></li>
<li>paper_authors: Menghao Li, Chunlei Wang, Wenquan Feng, Shuchang Lyu, Guangliang Cheng, Xiangtai Li, Binghao Liu, Qi Zhao<br>for: 本研究旨在解决现有的视觉固定问题，即基于给定的描述生成假阳性对象。methods: 本研究提出了一种Iterative Robust Visual Grounding（IR-VG）框架，包括多层视语融合（IMVF）和掩码参考中心点监督（MRCS）等技术，以提高对描述的匹配和对图像中的细节特征的捕捉。results: 对五个常见的视觉固定数据集和两个新提出的鲁棒视觉固定数据集进行了广泛的实验，并得到了新的最佳性能记录，相比之前的最佳方法在两个新提出的鲁棒视觉固定数据集上提高了25%和10%。此外，该方法还在五个常见的视觉固定数据集上得到了证明。<details>
<summary>Abstract</summary>
Visual Grounding (VG) aims at localizing target objects from an image based on given expressions and has made significant progress with the development of detection and vision transformer. However, existing VG methods tend to generate false-alarm objects when presented with inaccurate or irrelevant descriptions, which commonly occur in practical applications. Moreover, existing methods fail to capture fine-grained features, accurate localization, and sufficient context comprehension from the whole image and textual descriptions. To address both issues, we propose an Iterative Robust Visual Grounding (IR-VG) framework with Masked Reference based Centerpoint Supervision (MRCS). The framework introduces iterative multi-level vision-language fusion (IMVF) for better alignment. We use MRCS to ahieve more accurate localization with point-wised feature supervision. Then, to improve the robustness of VG, we also present a multi-stage false-alarm sensitive decoder (MFSD) to prevent the generation of false-alarm objects when presented with inaccurate expressions. The proposed framework is evaluated on five regular VG datasets and two newly constructed robust VG datasets. Extensive experiments demonstrate that IR-VG achieves new state-of-the-art (SOTA) results, with improvements of 25\% and 10\% compared to existing SOTA approaches on the two newly proposed robust VG datasets. Moreover, the proposed framework is also verified effective on five regular VG datasets. Codes and models will be publicly at https://github.com/cv516Buaa/IR-VG.
</details>
<details>
<summary>摘要</summary>
Visual Grounding (VG) target  objetcs  from  an  image  based  on  given  expressions  and  has  made  significant  progress  with  the  development  of  detection  and  vision  transformer.  However,  existing  VG  methods  tend  to  generate  false-alarm  objects  when  presented  with  inaccurate  or  irrelevant  descriptions,  which  commonly  occur  in  practical  applications.  Moreover,  existing  methods  fail  to  capture  fine-grained  features,  accurate  localization,  and  sufficient  context  comprehension  from  the  whole  image  and  textual  descriptions.  To  address  both  issues,  we  propose  an  Iterative  Robust  Visual  Grounding  (IR-VG)  framework  with  Masked  Reference  based  Centerpoint  Supervision  (MRCS).  The  framework  introduces  iterative  multi-level  vision-language  fusion  (IMVF)  for  better  alignment.  We  use  MRCS  to  achieve  more  accurate  localization  with  point-wised  feature  supervision.  Then,  to  improve  the  robustness  of  VG,  we  also  present  a  multi-stage  false-alarm  sensitive  decoder  (MFSD)  to  prevent  the  generation  of  false-alarm  objects  when  presented  with  inaccurate  expressions.  The  proposed  framework  is  evaluated  on  five  regular  VG  datasets  and  two  newly  constructed  robust  VG  datasets.  Extensive  experiments  demonstrate  that  IR-VG  achieves  new  state-of-the-art  (SOTA)  results,  with  improvements  of  25%  and  10%  compared  to  existing  SOTA  approaches  on  the  two  newly  proposed  robust  VG  datasets.  Moreover,  the  proposed  framework  is  also  verified  effective  on  five  regular  VG  datasets.  Codes  and  models  will  be  publicly  available  at  https://github.com/cv516Buaa/IR-VG.
</details></li>
</ul>
<hr>
<h2 id="Assessing-Intra-class-Diversity-and-Quality-of-Synthetically-Generated-Images-in-a-Biomedical-and-Non-biomedical-Setting"><a href="#Assessing-Intra-class-Diversity-and-Quality-of-Synthetically-Generated-Images-in-a-Biomedical-and-Non-biomedical-Setting" class="headerlink" title="Assessing Intra-class Diversity and Quality of Synthetically Generated Images in a Biomedical and Non-biomedical Setting"></a>Assessing Intra-class Diversity and Quality of Synthetically Generated Images in a Biomedical and Non-biomedical Setting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02505">http://arxiv.org/abs/2308.02505</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Muneeb Saad, Mubashir Husain Rehmani, Ruairi O’Reilly</li>
<li>for: This paper aims to evaluate the effectiveness of using Generative Adversarial Networks (GANs) for data augmentation in biomedical image analysis, and to investigate the impact of different sample sizes on the diversity and quality of synthetic images.</li>
<li>methods: The paper uses Multi-scale Structural Similarity Index Measure, Cosine Distance, and Frechet Inception Distance to evaluate the diversity and quality of synthetic images generated by a Deep Convolutional GAN in both biomedical and non-biomedical imaging modalities.</li>
<li>results: The results show that the metrics scores for diversity and quality vary significantly across biomedical-to-biomedical and biomedical-to-non-biomedical imaging modalities, and that the diversity and quality of synthetic images are affected by the sample size used for training the GAN.<details>
<summary>Abstract</summary>
In biomedical image analysis, data imbalance is common across several imaging modalities. Data augmentation is one of the key solutions in addressing this limitation. Generative Adversarial Networks (GANs) are increasingly being relied upon for data augmentation tasks. Biomedical image features are sensitive to evaluating the efficacy of synthetic images. These features can have a significant impact on metric scores when evaluating synthetic images across different biomedical imaging modalities. Synthetically generated images can be evaluated by comparing the diversity and quality of real images. Multi-scale Structural Similarity Index Measure and Cosine Distance are used to evaluate intra-class diversity, while Frechet Inception Distance is used to evaluate the quality of synthetic images. Assessing these metrics for biomedical and non-biomedical imaging is important to investigate an informed strategy in evaluating the diversity and quality of synthetic images. In this work, an empirical assessment of these metrics is conducted for the Deep Convolutional GAN in a biomedical and non-biomedical setting. The diversity and quality of synthetic images are evaluated using different sample sizes. This research intends to investigate the variance in diversity and quality across biomedical and non-biomedical imaging modalities. Results demonstrate that the metrics scores for diversity and quality vary significantly across biomedical-to-biomedical and biomedical-to-non-biomedical imaging modalities.
</details>
<details>
<summary>摘要</summary>
在生物医学影像分析中，数据不均衡是广泛存在的问题，而数据扩充是解决这个问题的关键方法之一。生成敌对网络（GANs）在数据扩充任务中得到了越来越多的应用。生物医学影像特征对于评估 sintetic 图像的效果非常敏感。这些特征在评估不同生物医学成像模式下的 sintetic 图像时可以有显著的影响。 sintetic 图像可以通过比较真实图像的多样性和质量来评估。多尺度结构相似度指标和夹角距离是评估内部多样性的方法，而干扰抽象距离则是评估 sintetic 图像质量的方法。为了了解 informed 策略，需要对生物医学和非生物医学成像进行评估。本研究通过对这些指标进行实际评估，研究了不同样本大小下的多样性和质量的变化。结果表明，在生物医学到生物医学和生物医学到非生物医学的转换中，指标分数差异显著。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/24/cs.CV_2023_07_24/" data-id="cloqtaeq500ffgh8852mceljk" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_07_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/24/cs.AI_2023_07_24/" class="article-date">
  <time datetime="2023-07-24T12:00:00.000Z" itemprop="datePublished">2023-07-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/24/cs.AI_2023_07_24/">cs.AI - 2023-07-24</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="QAmplifyNet-Pushing-the-Boundaries-of-Supply-Chain-Backorder-Prediction-Using-Interpretable-Hybrid-Quantum-Classical-Neural-Network"><a href="#QAmplifyNet-Pushing-the-Boundaries-of-Supply-Chain-Backorder-Prediction-Using-Interpretable-Hybrid-Quantum-Classical-Neural-Network" class="headerlink" title="QAmplifyNet: Pushing the Boundaries of Supply Chain Backorder Prediction Using Interpretable Hybrid Quantum - Classical Neural Network"></a>QAmplifyNet: Pushing the Boundaries of Supply Chain Backorder Prediction Using Interpretable Hybrid Quantum - Classical Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12906">http://arxiv.org/abs/2307.12906</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Abrar Jahin, Md Sakib Hossain Shovon, Md. Saiful Islam, Jungpil Shin, M. F. Mridha, Yuichi Okuyama<br>for: 这个研究是为了提高供应链管理中的货物预先预测，以便优化存储控制、减少成本和提高顾客满意度。methods: 本研究提出了一个新的方法ológical framework，使用量子概念的启发法来预测货物预先，并且使用量子-классиiral neural network来预测货物预先效果。results: 实验评估表明，QAmplifyNet模型在短时间和不寻常的数据集上预测货物预先的性能比 классиical models、量子组合、量子神经网和深度强化学习模型更好。此外，QAmplifyNet模型的可读性也得到了改进，使用可读性的人工智能技术。实际应用中，QAmplifyNet模型可以实现有效的存储控制、减少货物预先和提高操作效率。未来的工作包括进一步探索量子概念启发法，扩大数据集和探索其他供应链应用。<details>
<summary>Abstract</summary>
Supply chain management relies on accurate backorder prediction for optimizing inventory control, reducing costs, and enhancing customer satisfaction. However, traditional machine-learning models struggle with large-scale datasets and complex relationships, hindering real-world data collection. This research introduces a novel methodological framework for supply chain backorder prediction, addressing the challenge of handling large datasets. Our proposed model, QAmplifyNet, employs quantum-inspired techniques within a quantum-classical neural network to predict backorders effectively on short and imbalanced datasets. Experimental evaluations on a benchmark dataset demonstrate QAmplifyNet's superiority over classical models, quantum ensembles, quantum neural networks, and deep reinforcement learning. Its proficiency in handling short, imbalanced datasets makes it an ideal solution for supply chain management. To enhance model interpretability, we use Explainable Artificial Intelligence techniques. Practical implications include improved inventory control, reduced backorders, and enhanced operational efficiency. QAmplifyNet seamlessly integrates into real-world supply chain management systems, enabling proactive decision-making and efficient resource allocation. Future work involves exploring additional quantum-inspired techniques, expanding the dataset, and investigating other supply chain applications. This research unlocks the potential of quantum computing in supply chain optimization and paves the way for further exploration of quantum-inspired machine learning models in supply chain management. Our framework and QAmplifyNet model offer a breakthrough approach to supply chain backorder prediction, providing superior performance and opening new avenues for leveraging quantum-inspired techniques in supply chain management.
</details>
<details>
<summary>摘要</summary>
供应链管理需要准确预测落后订单，以优化存储控制、降低成本和提高客户满意度。然而，传统的机器学习模型在巨量数据和复杂关系下难以取得实际数据收集。本研究提出了一种新的方法ológica framework for supply chain backorder prediction，解决了处理巨量数据的挑战。我们的提出的模型，QAmplifyNet，利用量子启发技术在量子-классиical neural network中预测落后订单，效果更高于经典模型、量子集合、量子神经网络和深度奖励学习。实验评估表明，QAmplifyNet在短时间和不均衡数据上的预测性能较高，适用于实际供应链管理。为提高模型可读性，我们使用Explainable Artificial Intelligence技术。实际应用包括改善存储控制、减少落后订单和提高操作效率。QAmplifyNet可顺利 интегра到实际供应链管理系统中，允许批处理决策和有效资源分配。未来工作包括进一步探索量子启发技术、扩大数据集和探索其他供应链应用。本研究解锁了量子计算在供应链优化中的潜力，开辟了量子启发机器学习模型在供应链管理中的新 Avenues。我们的框架和QAmplifyNet模型提供了落后订单预测的突破方法，提供了更高的性能，开启了新的可能性 для利用量子启发技术在供应链管理中。
</details></li>
</ul>
<hr>
<h2 id="Towards-Bridging-the-FL-Performance-Explainability-Trade-Off-A-Trustworthy-6G-RAN-Slicing-Use-Case"><a href="#Towards-Bridging-the-FL-Performance-Explainability-Trade-Off-A-Trustworthy-6G-RAN-Slicing-Use-Case" class="headerlink" title="Towards Bridging the FL Performance-Explainability Trade-Off: A Trustworthy 6G RAN Slicing Use-Case"></a>Towards Bridging the FL Performance-Explainability Trade-Off: A Trustworthy 6G RAN Slicing Use-Case</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12903">http://arxiv.org/abs/2307.12903</a></li>
<li>repo_url: None</li>
<li>paper_authors: Swastika Roy, Hatim Chergui, Christos Verikoukis</li>
<li>for:  sixth-generation (6G) networks 与多元网络slice 共存下，AI驱动的零touch管理和orchestration (MANO) 成为重要的。但是，确保AI黑盒子在实际应用中的可靠性是问题。</li>
<li>methods:  this paper presents a novel explanation-guided in-hoc federated learning (FL) approach, which combines a constrained resource allocation model and an explainer exchange in a closed loop (CL) fashion to achieve transparent 6G network slicing resource management in a RAN-Edge setup under non-independent identically distributed (non-IID) datasets.</li>
<li>results:  the proposed approach achieves a balance between AI performance and explainability, and outperforms the unconstrained Integrated-Gradient post-hoc FL baseline in terms of faithfulness of explanations and overall training process.Here is the full answer in Simplified Chinese:</li>
<li>for:  sixth-generation (6G) networks 与多元网络slice 共存下，AI驱动的零touch管理和orchestration (MANO) 成为重要的。但是，确保AI黑盒子在实际应用中的可靠性是问题。</li>
<li>methods:  this paper presents a novel explanation-guided in-hoc federated learning (FL) approach, which combines a constrained resource allocation model and an explainer exchange in a closed loop (CL) fashion to achieve transparent 6G network slicing resource management in a RAN-Edge setup under non-independent identically distributed (non-IID) datasets.</li>
<li>results:  the proposed approach achieves a balance between AI performance and explainability, and outperforms the unconstrained Integrated-Gradient post-hoc FL baseline in terms of faithfulness of explanations and overall training process.<details>
<summary>Abstract</summary>
In the context of sixth-generation (6G) networks, where diverse network slices coexist, the adoption of AI-driven zero-touch management and orchestration (MANO) becomes crucial. However, ensuring the trustworthiness of AI black-boxes in real deployments is challenging. Explainable AI (XAI) tools can play a vital role in establishing transparency among the stakeholders in the slicing ecosystem. But there is a trade-off between AI performance and explainability, posing a dilemma for trustworthy 6G network slicing because the stakeholders require both highly performing AI models for efficient resource allocation and explainable decision-making to ensure fairness, accountability, and compliance. To balance this trade off and inspired by the closed loop automation and XAI methodologies, this paper presents a novel explanation-guided in-hoc federated learning (FL) approach where a constrained resource allocation model and an explainer exchange -- in a closed loop (CL) fashion -- soft attributions of the features as well as inference predictions to achieve a transparent 6G network slicing resource management in a RAN-Edge setup under non-independent identically distributed (non-IID) datasets. In particular, we quantitatively validate the faithfulness of the explanations via the so-called attribution-based confidence metric that is included as a constraint to guide the overall training process in the run-time FL optimization task. In this respect, Integrated-Gradient (IG) as well as Input $\times$ Gradient and SHAP are used to generate the attributions for our proposed in-hoc scheme, wherefore simulation results under different methods confirm its success in tackling the performance-explainability trade-off and its superiority over the unconstrained Integrated-Gradient post-hoc FL baseline.
</details>
<details>
<summary>摘要</summary>
在 sixth-generation（6G）网络上，多种网络slice共存的情况下，采用AI驱动的零 touched Management和Orchestration（MANO）变得非常重要。然而，在实际部署中确保AI黑obox的可靠性是一项挑战。 Explainable AI（XAI）工具可以在slice ecosystem中建立透明度，但是存在AI性能和解释性之间的负担，这种负担对于可靠的6G网络slice进行分配是一个悖论。为了平衡这个负担，我们提出了一种基于closed loop自动化和XAI方法的新的解释导向 federated learning（FL）方法。在这种方法中，一个受限的资源分配模型和一个解释器在closed loop（CL）方式交换软属性和推理预测结果，以实现透明的6G网络slice资源管理。特别是，我们在运行时FL优化任务中包含了一个受限的权重矩阵，以确保解释的准确性。在这种情况下，我们使用Integrated-Gradient（IG）、Input × Gradient和SHAP等方法生成解释，并通过在不同方法下的 simulate结果证明了我们的方法的成功。
</details></li>
</ul>
<hr>
<h2 id="As-Time-Goes-By-Adding-a-Temporal-Dimension-Towards-Resolving-Delegations-in-Liquid-Democracy"><a href="#As-Time-Goes-By-Adding-a-Temporal-Dimension-Towards-Resolving-Delegations-in-Liquid-Democracy" class="headerlink" title="As Time Goes By: Adding a Temporal Dimension Towards Resolving Delegations in Liquid Democracy"></a>As Time Goes By: Adding a Temporal Dimension Towards Resolving Delegations in Liquid Democracy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12898">http://arxiv.org/abs/2307.12898</a></li>
<li>repo_url: None</li>
<li>paper_authors: Evangelos Markakis, Georgios Papasotiropoulos</li>
<li>for: This paper aims to integrate a time horizon into decision-making problems in Liquid Democracy systems to enhance participation.</li>
<li>methods: The paper uses temporal graph theory to analyze the computational complexity of Liquid Democracy systems with a time horizon.</li>
<li>results: The paper shows that adding a time horizon can increase the number of possible delegation paths and reduce the loss of votes due to delegation cycles or abstaining agents, ultimately enhancing participation in Liquid Democracy systems.<details>
<summary>Abstract</summary>
In recent years, the study of various models and questions related to Liquid Democracy has been of growing interest among the community of Computational Social Choice. A concern that has been raised, is that current academic literature focuses solely on static inputs, concealing a key characteristic of Liquid Democracy: the right for a voter to change her mind as time goes by, regarding her options of whether to vote herself or delegate her vote to other participants, till the final voting deadline. In real life, a period of extended deliberation preceding the election-day motivates voters to adapt their behaviour over time, either based on observations of the remaining electorate or on information acquired for the topic at hand. By adding a temporal dimension to Liquid Democracy, such adaptations can increase the number of possible delegation paths and reduce the loss of votes due to delegation cycles or delegating paths towards abstaining agents, ultimately enhancing participation. Our work takes a first step to integrate a time horizon into decision-making problems in Liquid Democracy systems. Our approach, via a computational complexity analysis, exploits concepts and tools from temporal graph theory which turn out to be convenient for our framework.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Anytime-Model-Selection-in-Linear-Bandits"><a href="#Anytime-Model-Selection-in-Linear-Bandits" class="headerlink" title="Anytime Model Selection in Linear Bandits"></a>Anytime Model Selection in Linear Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12897">http://arxiv.org/abs/2307.12897</a></li>
<li>repo_url: None</li>
<li>paper_authors: Parnian Kassraie, Aldo Pacchiano, Nicolas Emmenegger, Andreas Krause</li>
<li>for: 本文研究了带剑优化中的模型选择问题，即在搜索和利用之间进行平衡，以便在不同的模型中选择最佳的一个。</li>
<li>methods: 作者使用了在线学习算法，将不同的模型当做专家进行处理。然而，现有的方法具有$\text{poly}M$的复杂度，与模型数量 $M$ 成直接相关。</li>
<li>results: 作者提出了ALEXP方法，它具有$\log M$ 的依赖关系，并且具有任何时间保证的减册。此外，ALEXP方法不需要知道预测horizon $n$，也不需要早期充分探索阶段。<details>
<summary>Abstract</summary>
Model selection in the context of bandit optimization is a challenging problem, as it requires balancing exploration and exploitation not only for action selection, but also for model selection. One natural approach is to rely on online learning algorithms that treat different models as experts. Existing methods, however, scale poorly ($\text{poly}M$) with the number of models $M$ in terms of their regret. Our key insight is that, for model selection in linear bandits, we can emulate full-information feedback to the online learner with a favorable bias-variance trade-off. This allows us to develop ALEXP, which has an exponentially improved ($\log M$) dependence on $M$ for its regret. ALEXP has anytime guarantees on its regret, and neither requires knowledge of the horizon $n$, nor relies on an initial purely exploratory stage. Our approach utilizes a novel time-uniform analysis of the Lasso, establishing a new connection between online learning and high-dimensional statistics.
</details>
<details>
<summary>摘要</summary>
模型选择在带刺优化上是一个复杂的问题，因为它需要平衡探索和利用，不仅 для动作选择，而且还为模型选择。一个自然的方法是通过在线学习算法来处理不同的模型。现有方法，然而， scales poorly（$\text{poly}M$）于模型数量 $M$ 的 regret。我们的关键发现是，在线选择器中的模型选择可以通过利用全信息反馈来让在线学习者具有有利的偏差-variance质量。这允许我们开发 ALEXP，它的 regret 有 exponentially 改进的（$\log M$）依赖于 $M$。ALEXP 具有任何时间 guarantees 的 regret，并不需要知道天数 $n$，也不需要初始阶段的纯探索阶段。我们的方法利用了一种新的时间uniform 分析，建立了在线学习和高维统计之间的新连接。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Stereotype-Identification-through-Reasoning"><a href="#Interpretable-Stereotype-Identification-through-Reasoning" class="headerlink" title="Interpretable Stereotype Identification through Reasoning"></a>Interpretable Stereotype Identification through Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00071">http://arxiv.org/abs/2308.00071</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jacob-Junqi Tian, Omkar Dige, David Emerson, Faiza Khan Khattak<br>for: 这篇研究的目的是探讨语模型中的偏见，并在其开发中整合公平性，以确保这些模型是无偏见的和公平的。methods: 本研究使用了Vicuna-13B-v1.3进行零执行刻 sterotype 识别，并评估了将13B扩展到33B的影响。results: 研究发现，从理解到执行的改善 exceeds 从扩展到33B的改善，这表明了理解可以帮助语模型在离域任务上超越扩展的法则。此外，通过选择性分析一些理解迹象，我们显示了如何理解可以提高决策的解释性。<details>
<summary>Abstract</summary>
Given that language models are trained on vast datasets that may contain inherent biases, there is a potential danger of inadvertently perpetuating systemic discrimination. Consequently, it becomes essential to examine and address biases in language models, integrating fairness into their development to ensure these models are equitable and free from bias. In this work, we demonstrate the importance of reasoning in zero-shot stereotype identification based on Vicuna-13B-v1.3. While we do observe improved accuracy by scaling from 13B to 33B, we show that the performance gain from reasoning significantly exceeds the gain from scaling up. Our findings suggest that reasoning could be a key factor that enables LLMs to trescend the scaling law on out-of-domain tasks such as stereotype identification. Additionally, through a qualitative analysis of select reasoning traces, we highlight how reasoning enhances not just accuracy but also the interpretability of the decision.
</details>
<details>
<summary>摘要</summary>
language models 可能会含有隐性偏见，这可能会导致不必要地扩大系统歧视。因此，检查和解决语言模型中的偏见变得非常重要，以确保这些模型是公正的。在这种情况下，我们展示了在 Vicuna-13B-v1.3 基础上进行零 shot 刻板印象的重要性。虽然我们确实观察到了从 13B 缩放到 33B 的性能提升，但我们发现，理解的性能提升远超过缩放的提升。我们的发现表明，理解可以使 LLMS 突破预期的性能增长。此外，通过分析 select 的理解轨迹，我们指出了如何使用理解来提高决策的可读性。
</details></li>
</ul>
<hr>
<h2 id="A-Real-World-WebAgent-with-Planning-Long-Context-Understanding-and-Program-Synthesis"><a href="#A-Real-World-WebAgent-with-Planning-Long-Context-Understanding-and-Program-Synthesis" class="headerlink" title="A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis"></a>A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12856">http://arxiv.org/abs/2307.12856</a></li>
<li>repo_url: None</li>
<li>paper_authors: Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, Aleksandra Faust</li>
<li>for: 这篇论文的目的是提出一种基于大语言模型（LLM）的自主网络浏览器，可以根据自然语言指令完成真实网站上的任务。</li>
<li>methods: 这篇论文使用了Flan-U-PaLM和HTML-T5两种大语言模型，其中Flan-U-PaLM用于固有代码生成，HTML-T5用于长HTML文档的规划和摘要。这两种模型都使用了地方和全局注意力机制，并使用了混合长时间杂化目标来进行规划和摘要。</li>
<li>results: 论文的实验表明，使用WebAgent可以在真实网站上提高成功率超过50%，并且HTML-T5模型在解决HTML基于任务上的精度高于之前的SoTA。<details>
<summary>Abstract</summary>
Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web navigation. However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML. We introduce WebAgent, an LLM-driven agent that can complete the tasks on real websites following natural language instructions. WebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via generated Python programs from those. We design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization. We empirically demonstrate that our recipe improves the success on a real website by over 50%, and that HTML-T5 is the best model to solve HTML-based tasks; achieving 14.9% higher success rate than prior SoTA on the MiniWoB web navigation benchmark and better accuracy on offline task planning evaluation.
</details>
<details>
<summary>摘要</summary>
Recently, pre-trained large language models (LLMs) have achieved better generalization and sample efficiency in autonomous web navigation. However, the performance on real-world websites is still affected by three factors: open domain, limited context length, and lack of inductive bias on HTML. To address these issues, we introduce WebAgent, an LLM-driven agent that can complete tasks on real websites based on natural language instructions.WebAgent plans ahead by breaking down instructions into canonical sub-instructions, summarizing long HTML documents into task-relevant snippets, and acting on websites through generated Python programs. We use Flan-U-PaLM for grounded code generation and HTML-T5, a new pre-trained LLM that utilizes local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization.Our empirical results show that our recipe improves the success rate on a real website by over 50%, and HTML-T5 is the best model for solving HTML-based tasks, achieving a 14.9% higher success rate than the prior state-of-the-art on the MiniWoB web navigation benchmark and better accuracy on offline task planning evaluation.
</details></li>
</ul>
<hr>
<h2 id="EPIC-KITCHENS-100-Unsupervised-Domain-Adaptation-Challenge-Mixed-Sequences-Prediction"><a href="#EPIC-KITCHENS-100-Unsupervised-Domain-Adaptation-Challenge-Mixed-Sequences-Prediction" class="headerlink" title="EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge: Mixed Sequences Prediction"></a>EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge: Mixed Sequences Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12837">http://arxiv.org/abs/2307.12837</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amirshayan Nasirimajd, Simone Alberto Peirone, Chiara Plizzari, Barbara Caputo</li>
<li>For: 本研究是为了解决Unsupervised Domain Adaptation (UDA) Challenge in Action Recognition 中的问题。* Methods: 我们采用了一种基于序列的方法，即将源频率和目标频率 randomly combine 生成一个修改后的序列，然后使用标准的 pseudo-labeling 策略提取目标频率中的动作标签。* Results: 我们的提交（名为 ‘sshayan’）可以在领导人员中找到，目前在 ‘verb’ 和 ‘noun’ 两个分类中排名第二和第四。<details>
<summary>Abstract</summary>
This report presents the technical details of our approach for the EPIC-Kitchens-100 Unsupervised Domain Adaptation (UDA) Challenge in Action Recognition. Our approach is based on the idea that the order in which actions are performed is similar between the source and target domains. Based on this, we generate a modified sequence by randomly combining actions from the source and target domains. As only unlabelled target data are available under the UDA setting, we use a standard pseudo-labeling strategy for extracting action labels for the target. We then ask the network to predict the resulting action sequence. This allows to integrate information from both domains during training and to achieve better transfer results on target. Additionally, to better incorporate sequence information, we use a language model to filter unlikely sequences. Lastly, we employed a co-occurrence matrix to eliminate unseen combinations of verbs and nouns. Our submission, labeled as 'sshayan', can be found on the leaderboard, where it currently holds the 2nd position for 'verb' and the 4th position for both 'noun' and 'action'.
</details>
<details>
<summary>摘要</summary>
Here's the Simplified Chinese translation:这份报告介绍了我们在EPIC-Kitchens-100Unsupervised Domain Adaptation（UDA）挑战中的动作识别技术细节。我们的方法基于源频率和目标频率中动作的顺序相似性。我们随机将源频率和目标频率中的动作组合在一起，然后使用标准的 Pseudo-labeling 策略提取目标频率中的动作标签。我们然后问网络预测结果的动作序列。这allow我们在训练中 integrating信息从两个频率中，以实现更好的传输结果。此外，我们还使用语言模型筛选不可能的序列，以及一个co-occurrence Matrix来消除未看到的动词和名词的组合。我们的提交，标记为'sshayan'，可以在排行榜上找到，现在在'verb'、'noun'和'action'三个分类中分别排名第二和第四。
</details></li>
</ul>
<hr>
<h2 id="Joint-speech-and-overlap-detection-a-benchmark-over-multiple-audio-setup-and-speech-domains"><a href="#Joint-speech-and-overlap-detection-a-benchmark-over-multiple-audio-setup-and-speech-domains" class="headerlink" title="Joint speech and overlap detection: a benchmark over multiple audio setup and speech domains"></a>Joint speech and overlap detection: a benchmark over multiple audio setup and speech domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13012">http://arxiv.org/abs/2307.13012</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martin Lebourdais, Théo Mariotte, Marie Tahon, Anthony Larcher, Antoine Laurent, Silvio Montresor, Sylvain Meignier, Jean-Hugh Thomas<br>for: 这篇论文主要针对的是voice activity detection和 overlap speech detection的预处理任务，以提高speaker diarization的最终 segmentation性能。methods: 这篇论文提出了一个全新的benchmark，用于评估不同的voice activity detection和overlap speech detection模型，在多个音频设置和语音频道上。这些模型结合了一个Temporal Convolutional Network和适应于设置的语音表示，可以达到 state-of-the-art的性能水平。results: 这篇论文的实验结果显示，将voice activity detection和overlap speech detection作为一个 jointly trained模型，可以提高预处理性能，同时降低了训练成本。此外，这种unique的架构还可以用于单和多通道speech处理。<details>
<summary>Abstract</summary>
Voice activity and overlapped speech detection (respectively VAD and OSD) are key pre-processing tasks for speaker diarization. The final segmentation performance highly relies on the robustness of these sub-tasks. Recent studies have shown VAD and OSD can be trained jointly using a multi-class classification model. However, these works are often restricted to a specific speech domain, lacking information about the generalization capacities of the systems. This paper proposes a complete and new benchmark of different VAD and OSD models, on multiple audio setups (single/multi-channel) and speech domains (e.g. media, meeting...). Our 2/3-class systems, which combine a Temporal Convolutional Network with speech representations adapted to the setup, outperform state-of-the-art results. We show that the joint training of these two tasks offers similar performances in terms of F1-score to two dedicated VAD and OSD systems while reducing the training cost. This unique architecture can also be used for single and multichannel speech processing.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化字符串。<</SYS>>声音活动和重叠说话检测（简称VAD和OSD）是Speaker Diagnosis的关键预处理任务。最终分 segmentation 性能强度取决于这两个子任务的稳定性。 current studies have shown that VAD and OSD can be trained together using a multi-class classification model. However, these works are often restricted to a specific speech domain, lacking information about the generalization capacities of the systems. This paper proposes a complete and new benchmark of different VAD and OSD models, on multiple audio setups (single/multi-channel) and speech domains (e.g. media, meeting...). Our 2/3-class systems, which combine a Temporal Convolutional Network with speech representations adapted to the setup, outperform state-of-the-art results. We show that the joint training of these two tasks offers similar performances in terms of F1-score to two dedicated VAD and OSD systems while reducing the training cost. This unique architecture can also be used for single and multichannel speech processing.
</details></li>
</ul>
<hr>
<h2 id="End-to-End-Deep-Transfer-Learning-for-Calibration-free-Motor-Imagery-Brain-Computer-Interfaces"><a href="#End-to-End-Deep-Transfer-Learning-for-Calibration-free-Motor-Imagery-Brain-Computer-Interfaces" class="headerlink" title="End-to-End Deep Transfer Learning for Calibration-free Motor Imagery Brain Computer Interfaces"></a>End-to-End Deep Transfer Learning for Calibration-free Motor Imagery Brain Computer Interfaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12827">http://arxiv.org/abs/2307.12827</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maryam Alimardani, Steven Kocken, Nikki Leeuwis<br>for:这个研究的目的是开发一种可以在各种应用场景中使用的无需参数调整的motor imagery brain-computer interface（MI-BCI）分类器。methods:这个研究使用了深度训练学习，并在 Raw EEG 信号上进行了一种端到端的深度学习方法。三种深度学习模型（MIN2Net、EEGNet 和 DeepConvNet）被训练并比较使用了一个公开available的数据集。results:在离势一个subject出的cross-validation中，MIN2Net 无法在新用户中分辨右手和左手的motor imagery，具有51.7%的 median 准确率。而 EEGNet 和 DeepConvNet 两种模型在其他数据集上没有进行过训练，但在这个数据集上的 median 准确率分别为62.5% 和 59.2%。这些准确率虽然不足70%，但与其他数据集上的准确率相似。<details>
<summary>Abstract</summary>
A major issue in Motor Imagery Brain-Computer Interfaces (MI-BCIs) is their poor classification accuracy and the large amount of data that is required for subject-specific calibration. This makes BCIs less accessible to general users in out-of-the-lab applications. This study employed deep transfer learning for development of calibration-free subject-independent MI-BCI classifiers. Unlike earlier works that applied signal preprocessing and feature engineering steps in transfer learning, this study adopted an end-to-end deep learning approach on raw EEG signals. Three deep learning models (MIN2Net, EEGNet and DeepConvNet) were trained and compared using an openly available dataset. The dataset contained EEG signals from 55 subjects who conducted a left- vs. right-hand motor imagery task. To evaluate the performance of each model, a leave-one-subject-out cross validation was used. The results of the models differed significantly. MIN2Net was not able to differentiate right- vs. left-hand motor imagery of new users, with a median accuracy of 51.7%. The other two models performed better, with median accuracies of 62.5% for EEGNet and 59.2% for DeepConvNet. These accuracies do not reach the required threshold of 70% needed for significant control, however, they are similar to the accuracies of these models when tested on other datasets without transfer learning.
</details>
<details>
<summary>摘要</summary>
一个主要问题在肌动幻象潜意计算机界面（MI-BCI）是其低精度分类和大量的数据需要用于特定用户的卡 Liping。这使得BCI对通用用户在室外应用中 menos  accessible。这项研究使用了深度传输学习开发了无需特定用户 calibration的Ml-BCI分类器。与之前的工作一样，这项研究不对信号预处理和特征工程步骤进行传输学习，而是使用了 Raw EEG 信号的端到端深度学习方法。研究使用了三个深度学习模型（MIN2Net、EEGNet 和 DeepConvNet），并对其进行了比较。数据集包含55名用户完成左手 vs. 右手肌动幻象任务的EEG信号。为评估每个模型的性能，使用了留一个用户之外的交叉验证。结果表明，MIN2Net 无法识别新用户的左手 vs. 右手肌动幻象，具有51.7%的 median 精度。另外两个模型则表现更好， median 精度分别为62.5% 和 59.2%。这些精度没有达到所需的70%的阈值，但与不使用传输学习的其他数据集测试的精度类似。
</details></li>
</ul>
<hr>
<h2 id="Performance-of-Large-Language-Models-in-a-Computer-Science-Degree-Program"><a href="#Performance-of-Large-Language-Models-in-a-Computer-Science-Degree-Program" class="headerlink" title="Performance of Large Language Models in a Computer Science Degree Program"></a>Performance of Large Language Models in a Computer Science Degree Program</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02432">http://arxiv.org/abs/2308.02432</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tim Krüger, Michael Gref</li>
<li>for: 这个论文的目的是评估不同大型自然语言模型在大学应用科学学士学位课程中的效iveness。</li>
<li>methods: 这个论文使用了不同大型自然语言模型作为教学工具，并通过提示模型 lecture material、运动任务和过去考试来评估它们在不同计算机科学领域的能力。</li>
<li>results: 研究发现，ChatGPT-3.5在10个测试模块中的平均分为79.9%，BingAI为68.4%，LLaMa（65亿参数变量）为20%。尽管这些结果非常有力，但even GPT-4.0不能通过学位课程 - 因为它在数学计算中存在限制。<details>
<summary>Abstract</summary>
Large language models such as ChatGPT-3.5 and GPT-4.0 are ubiquitous and dominate the current discourse. Their transformative capabilities have led to a paradigm shift in how we interact with and utilize (text-based) information. Each day, new possibilities to leverage the capabilities of these models emerge. This paper presents findings on the performance of different large language models in a university of applied sciences' undergraduate computer science degree program. Our primary objective is to assess the effectiveness of these models within the curriculum by employing them as educational aids. By prompting the models with lecture material, exercise tasks, and past exams, we aim to evaluate their proficiency across different computer science domains. We showcase the strong performance of current large language models while highlighting limitations and constraints within the context of such a degree program. We found that ChatGPT-3.5 averaged 79.9% of the total score in 10 tested modules, BingAI achieved 68.4%, and LLaMa, in the 65 billion parameter variant, 20%. Despite these convincing results, even GPT-4.0 would not pass the degree program - due to limitations in mathematical calculations.
</details>
<details>
<summary>摘要</summary>
大型语言模型如ChatGPT-3.5和GPT-4.0在当前的讨论中占据主导地位，其转换能力导致了信息处理方式的 paradigm shift。每天，新的可能性 emerge 以利用这些模型的能力。本文介绍了不同大型语言模型在大学应用科学学士学位课程中的表现。我们的主要目标是通过使用这些模型作为教育工具来评估它们在课程中的效iveness。我们在讲义材料、作业任务和过往考试中提问这些模型，以评估它们在不同的计算机科学领域中的技能。我们显示了当前大型语言模型的强大表现，并 highlighted 其中的限制和约束在这种学位课程中。我们发现ChatGPT-3.5在10个测试模块中的平均分为79.9%，BingAI为68.4%，LLaMa（65亿参数变量）为20%。尽管这些结果非常吸引人，但even GPT-4.0不能通过这种学位课程 - 因为它们在数学计算中的限制。
</details></li>
</ul>
<hr>
<h2 id="Maximal-Independent-Sets-for-Pooling-in-Graph-Neural-Networks"><a href="#Maximal-Independent-Sets-for-Pooling-in-Graph-Neural-Networks" class="headerlink" title="Maximal Independent Sets for Pooling in Graph Neural Networks"></a>Maximal Independent Sets for Pooling in Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13011">http://arxiv.org/abs/2307.13011</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stevan Stanovic, Benoit Gaüzère, Luc Brun</li>
<li>for: 本文为了解决图像Pooling问题，提出了三种基于最大独立集的Pooling方法，以避免传统图像Pooling方法的缺点。</li>
<li>methods: 本文使用了三种基于最大独立集的Pooling方法，即Maximal Independent Set Pooling（MISP）、Maximal Independent Set Aggregation（MISA）和Maximal Independent Set Based Pooling（MIBP）。</li>
<li>results: 实验结果表明，这三种Pooling方法能够减少图像的缺失和杂乱，并且能够保持图像的连通性。<details>
<summary>Abstract</summary>
Convolutional Neural Networks (CNNs) have enabled major advances in image classification through convolution and pooling. In particular, image pooling transforms a connected discrete lattice into a reduced lattice with the same connectivity and allows reduction functions to consider all pixels in an image. However, there is no pooling that satisfies these properties for graphs. In fact, traditional graph pooling methods suffer from at least one of the following drawbacks: Graph disconnection or overconnection, low decimation ratio, and deletion of large parts of graphs. In this paper, we present three pooling methods based on the notion of maximal independent sets that avoid these pitfalls. Our experimental results confirm the relevance of maximal independent set constraints for graph pooling.
</details>
<details>
<summary>摘要</summary>
convolutional neural networks (CNNs)  hanno permesso di grandi avanzamenti nella classificazione di immagini attraverso la convoluzione e la pooling. In particolare, la pooling di immagini trasforma una rete discreta connesse in una rete ridotta con la stessa connessione e consente alle funzioni di riduzione di considerare tutti i pixel dell'immagine. Tuttavia, non esiste pooling per grafi che soddisfi queste proprietà. Infatti, i metodi di pooling tradizionali per grafi soffrono almeno uno dei seguenti difetti: disconnessione del grafico o eccessiva connessione, bassa riduzione del numero di node e cancellazione di grandi porzioni del grafico. In questo paper, presentiamo tre metodi di pooling basati sulla nozione di set indipendenti massimi che evitano questi inconvenienti. I nostri risultati sperimentali confermano la rilevanza delle restrizioni di set indipendenti massimi per il pooling di grafi.
</details></li>
</ul>
<hr>
<h2 id="Analyzing-the-Strategy-of-Propaganda-using-Inverse-Reinforcement-Learning-Evidence-from-the-2022-Russian-Invasion-of-Ukraine"><a href="#Analyzing-the-Strategy-of-Propaganda-using-Inverse-Reinforcement-Learning-Evidence-from-the-2022-Russian-Invasion-of-Ukraine" class="headerlink" title="Analyzing the Strategy of Propaganda using Inverse Reinforcement Learning: Evidence from the 2022 Russian Invasion of Ukraine"></a>Analyzing the Strategy of Propaganda using Inverse Reinforcement Learning: Evidence from the 2022 Russian Invasion of Ukraine</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12788">http://arxiv.org/abs/2307.12788</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dominique Geissler, Stefan Feuerriegel</li>
<li>for: This study aims to understand the strategy behind the pro-Russian propaganda campaign on social media during the 2022 Russian invasion of Ukraine.</li>
<li>methods: The study uses an inverse reinforcement learning (IRL) approach to model online behavior as a Markov decision process and infer the underlying reward structure that guides propagandists when interacting with users with a supporting or opposing stance toward the invasion.</li>
<li>results: The study finds that bots and humans follow different strategies in responding to pro-Russian propaganda. Bots primarily respond to pro-invasion messages, suggesting they seek to drive virality, while messages indicating opposition primarily elicit responses from humans, suggesting they tend to engage in critical discussions.<details>
<summary>Abstract</summary>
The 2022 Russian invasion of Ukraine was accompanied by a large-scale, pro-Russian propaganda campaign on social media. However, the strategy behind the dissemination of propaganda has remained unclear, particularly how the online discourse was strategically shaped by the propagandists' community. Here, we analyze the strategy of the Twitter community using an inverse reinforcement learning (IRL) approach. Specifically, IRL allows us to model online behavior as a Markov decision process, where the goal is to infer the underlying reward structure that guides propagandists when interacting with users with a supporting or opposing stance toward the invasion. Thereby, we aim to understand empirically whether and how between-user interactions are strategically used to promote the proliferation of Russian propaganda. For this, we leverage a large-scale dataset with 349,455 posts with pro-Russian propaganda from 132,131 users. We show that bots and humans follow a different strategy: bots respond predominantly to pro-invasion messages, suggesting that they seek to drive virality; while messages indicating opposition primarily elicit responses from humans, suggesting that they tend to engage in critical discussions. To the best of our knowledge, this is the first study analyzing the strategy behind propaganda from the 2022 Russian invasion of Ukraine through the lens of IRL.
</details>
<details>
<summary>摘要</summary>
俄罗斯入侵乌克兰的2022年社交媒体宣传活动中，涉及到了大规模的俄罗斯支持者在社交媒体上的宣传活动。然而，这些宣传活动的战略具体如何实施，特别是如何通过在用户之间的互动来推动俄罗斯宣传的普及，这些问题仍未得到了清楚的回答。在这里，我们使用 inverse reinforcement learning（IRL）方法来分析推特社区的宣传策略。具体来说，IRL方法允许我们将在线行为模型为Markov决策过程，并且目的是从推特用户的支持或反对姿态来推断宣传者在互动中的奖励结构。因此，我们可以理解propagandists在互动中是如何用between-user互动来推动俄罗斯宣传的。为此，我们利用了349,455条推特帖子和132,131名用户的大规模数据集。我们发现，机器人和人类使用者采取了不同的策略：机器人尽量回应支持入侵的消息，表明它们想要驱动病毒性；而反对消息主要引起人类用户的回应，表明人类用户更倾向于进行批评讨论。到目前为止，这是我们分析2022年俄罗斯入侵乌克兰的宣传策略的第一项研究，通过IRL镜像来分析。
</details></li>
</ul>
<hr>
<h2 id="Is-attention-all-you-need-in-medical-image-analysis-A-review"><a href="#Is-attention-all-you-need-in-medical-image-analysis-A-review" class="headerlink" title="Is attention all you need in medical image analysis? A review"></a>Is attention all you need in medical image analysis? A review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12775">http://arxiv.org/abs/2307.12775</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giorgos Papanastasiou, Nikolaos Dikaios, Jiahao Huang, Chengjia Wang, Guang Yang</li>
<li>for: 这篇论文旨在概述现有的hybrid CNN-Transf&#x2F;Attention模型，以及对这些模型的架构设计、突破点和应用前景。</li>
<li>methods: 这篇论文使用了系统性的文献综述方法，对 hybrid CNN-Transf&#x2F;Attention模型进行了架构分析和综述，并提出了一种基于数据驱动的领域泛化和适应方法。</li>
<li>results: 论文发现了hybrid CNN-Transf&#x2F;Attention模型在医学图像分析领域的应用前景，并提出了一种基于数据驱动的领域泛化和适应方法，可以优化模型的泛化能力和适应能力。<details>
<summary>Abstract</summary>
Medical imaging is a key component in clinical diagnosis, treatment planning and clinical trial design, accounting for almost 90% of all healthcare data. CNNs achieved performance gains in medical image analysis (MIA) over the last years. CNNs can efficiently model local pixel interactions and be trained on small-scale MI data. The main disadvantage of typical CNN models is that they ignore global pixel relationships within images, which limits their generalisation ability to understand out-of-distribution data with different 'global' information. The recent progress of Artificial Intelligence gave rise to Transformers, which can learn global relationships from data. However, full Transformer models need to be trained on large-scale data and involve tremendous computational complexity. Attention and Transformer compartments (Transf/Attention) which can well maintain properties for modelling global relationships, have been proposed as lighter alternatives of full Transformers. Recently, there is an increasing trend to co-pollinate complementary local-global properties from CNN and Transf/Attention architectures, which led to a new era of hybrid models. The past years have witnessed substantial growth in hybrid CNN-Transf/Attention models across diverse MIA problems. In this systematic review, we survey existing hybrid CNN-Transf/Attention models, review and unravel key architectural designs, analyse breakthroughs, and evaluate current and future opportunities as well as challenges. We also introduced a comprehensive analysis framework on generalisation opportunities of scientific and clinical impact, based on which new data-driven domain generalisation and adaptation methods can be stimulated.
</details>
<details>
<summary>摘要</summary>
医疗影像是诊断、治疗规划和临床试验设计中的关键组成部分，占全部医疗数据的近90%。过去几年，Convolutional Neural Networks（CNN）在医疗影像分析（MIA）中获得了性能提升。CNN可以有效地模型图像中的局部像素互动，并可以在小规模的MI数据上进行训练。然而，典型的CNN模型却忽略图像中的全局像素关系，这限制了它们对不同'全球'信息的总体化能力。随着人工智能的发展，Transformers（Transformers）产生了，它可以从数据中学习全球关系。然而，全Transformers模型需要大规模的训练和巨大的计算复杂度。Attention和Transformers组件（Transf/Attention），可以保持模型全球关系的性能，被提议为轻量级的Alternative。最近，有一个增长的趋势，将Complementary local-global properties（CLGP）从CNN和Transf/Attention架构中搬运到新的混合模型中。过去几年，混合CNN-Transf/Attention模型在多种MIA问题上表现出了明显的增长。在这个系统性评论中，我们对现有的混合CNN-Transf/Attention模型进行了评论和探讨，分析了重要的架构设计、突破性和现有和未来的机遇和挑战。此外，我们还提出了一种基于总结机会的科学和临床影响分析框架，可以驱动新的数据驱动领域总结和适应方法。
</details></li>
</ul>
<hr>
<h2 id="Adaptation-of-Whisper-models-to-child-speech-recognition"><a href="#Adaptation-of-Whisper-models-to-child-speech-recognition" class="headerlink" title="Adaptation of Whisper models to child speech recognition"></a>Adaptation of Whisper models to child speech recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13008">http://arxiv.org/abs/2307.13008</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/c3imaging/whisper_child_speech">https://github.com/c3imaging/whisper_child_speech</a></li>
<li>paper_authors: Rishabh Jain, Andrei Barcovschi, Mariam Yiwere, Peter Corcoran, Horia Cucu</li>
<li>for: 提高儿童语音识别（ASR）系统对儿童语音的识别精度</li>
<li>methods: 使用现有的大量成人语音数据集来适应儿童语音，并对Whisper模型进行finetuning和自动监督学习</li>
<li>results: 在儿童语音上，使用finetuning Whisper模型和自动监督学习的wav2vec2模型可以获得显著改善的ASR性能，相比非finetuning Whisper模型<details>
<summary>Abstract</summary>
Automatic Speech Recognition (ASR) systems often struggle with transcribing child speech due to the lack of large child speech datasets required to accurately train child-friendly ASR models. However, there are huge amounts of annotated adult speech datasets which were used to create multilingual ASR models, such as Whisper. Our work aims to explore whether such models can be adapted to child speech to improve ASR for children. In addition, we compare Whisper child-adaptations with finetuned self-supervised models, such as wav2vec2. We demonstrate that finetuning Whisper on child speech yields significant improvements in ASR performance on child speech, compared to non finetuned Whisper models. Additionally, utilizing self-supervised Wav2vec2 models that have been finetuned on child speech outperforms Whisper finetuning.
</details>
<details>
<summary>摘要</summary>
自动话语识别（ASR）系统经常因缺乏儿童语音数据而困难地识别儿童语音。然而，存在巨量的注解的成人语音数据，这些数据被用来创建多语言的ASR模型，如呼叫。我们的工作是探索是否可以将这些模型适应儿童语音，以提高ASR的性能。此外，我们还比较了使用自动注解的wav2vec2模型和Whisper模型。我们的结果显示，对儿童语音进行finetuning可以大幅提高ASR性能，相比非finetuning Whisper模型。此外，使用自动注解的wav2vec2模型，经过finetuning在儿童语音上表现更好，超越了Whisper finetuning。
</details></li>
</ul>
<hr>
<h2 id="Nonparametric-Linear-Feature-Learning-in-Regression-Through-Regularisation"><a href="#Nonparametric-Linear-Feature-Learning-in-Regression-Through-Regularisation" class="headerlink" title="Nonparametric Linear Feature Learning in Regression Through Regularisation"></a>Nonparametric Linear Feature Learning in Regression Through Regularisation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12754">http://arxiv.org/abs/2307.12754</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bertillefollain/regfeal">https://github.com/bertillefollain/regfeal</a></li>
<li>paper_authors: Bertille Follain, Umut Simsekli, Francis Bach</li>
<li>for: 这个论文主要针对高维数据的自动特征选择问题，具体来说是多指标模型中的适用学习问题。</li>
<li>methods: 该论文提出了一种新的非 Parametric 方法，可以同时估计预测函数和相关的直方几何空间。该方法使用了Empirical risk minimization，并添加了函数导数的约束，以保证方法的多元性。</li>
<li>results: 该论文提供了一些实验结果，证明了RegFeaL 可以在不同的实际场景中表现出色，并且可以准确地估计相关维度。<details>
<summary>Abstract</summary>
Representation learning plays a crucial role in automated feature selection, particularly in the context of high-dimensional data, where non-parametric methods often struggle. In this study, we focus on supervised learning scenarios where the pertinent information resides within a lower-dimensional linear subspace of the data, namely the multi-index model. If this subspace were known, it would greatly enhance prediction, computation, and interpretation. To address this challenge, we propose a novel method for linear feature learning with non-parametric prediction, which simultaneously estimates the prediction function and the linear subspace. Our approach employs empirical risk minimisation, augmented with a penalty on function derivatives, ensuring versatility. Leveraging the orthogonality and rotation invariance properties of Hermite polynomials, we introduce our estimator, named RegFeaL. By utilising alternative minimisation, we iteratively rotate the data to improve alignment with leading directions and accurately estimate the relevant dimension in practical settings. We establish that our method yields a consistent estimator of the prediction function with explicit rates. Additionally, we provide empirical results demonstrating the performance of RegFeaL in various experiments.
</details>
<details>
<summary>摘要</summary>
《学习表示在自动选择特征中扮演重要角色，特别是在高维数据的情况下，非 Parametric 方法经常陷入困难。在这项研究中，我们关注supervised learning情况下，关键信息都集中在数据中的一个lower-dimensional linear subspace，即多index模型。如果这个subspace已知，它会大大提高预测、计算和解释。为解决这个挑战，我们提出了一种新的方法 для线性特征学习，同时估算预测函数和linear subspace。我们的方法使用empirical risk minimization，并添加了函数导数的罚因，以确保多样化。利用 Hermite  polynomials的正交性和旋转不变性，我们引入了我们的估计器，名为RegFeaL。通过使用alternative minimization，我们可以逐步旋转数据，以便更好地与主导方向相align并准确地估算实际情况中的相关维度。我们证明了我们的方法可以生成一个consistent的预测函数估计器，并且提供了explicit rates。此外，我们还提供了一系列实验结果，证明RegFeaL的性能。
</details></li>
</ul>
<hr>
<h2 id="Introducing-CALMED-Multimodal-Annotated-Dataset-for-Emotion-Detection-in-Children-with-Autism"><a href="#Introducing-CALMED-Multimodal-Annotated-Dataset-for-Emotion-Detection-in-Children-with-Autism" class="headerlink" title="Introducing CALMED: Multimodal Annotated Dataset for Emotion Detection in Children with Autism"></a>Introducing CALMED: Multimodal Annotated Dataset for Emotion Detection in Children with Autism</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13706">http://arxiv.org/abs/2307.13706</a></li>
<li>repo_url: None</li>
<li>paper_authors: Annanda Sousa, Karen Young, Mathieu D’aquin, Manel Zarrouk, Jennifer Holloway</li>
<li>for: 这个论文的目的是提高人际交流的自动情感识别系统，以提供个性化的用户体验。</li>
<li>methods: 本论文使用了多种数据收集和处理技术，包括录音和视频特征提取和分类。</li>
<li>results: 本论文描述了一个基于多Modal的情感识别数据集，包括8-12岁的儿童患有Autism Spectrum Disorder（ASD）的记录例子。该数据集包括4个target类别的注解，共计57,012个示例，每个示例代表200ms（0.2秒）的时间窗口。<details>
<summary>Abstract</summary>
Automatic Emotion Detection (ED) aims to build systems to identify users' emotions automatically. This field has the potential to enhance HCI, creating an individualised experience for the user. However, ED systems tend to perform poorly on people with Autism Spectrum Disorder (ASD). Hence, the need to create ED systems tailored to how people with autism express emotions. Previous works have created ED systems tailored for children with ASD but did not share the resulting dataset. Sharing annotated datasets is essential to enable the development of more advanced computer models for ED within the research community. In this paper, we describe our experience establishing a process to create a multimodal annotated dataset featuring children with a level 1 diagnosis of autism. In addition, we introduce CALMED (Children, Autism, Multimodal, Emotion, Detection), the resulting multimodal emotion detection dataset featuring children with autism aged 8-12. CALMED includes audio and video features extracted from recording files of study sessions with participants, together with annotations provided by their parents into four target classes. The generated dataset includes a total of 57,012 examples, with each example representing a time window of 200ms (0.2s). Our experience and methods described here, together with the dataset shared, aim to contribute to future research applications of affective computing in ASD, which has the potential to create systems to improve the lives of people with ASD.
</details>
<details>
<summary>摘要</summary>
自动情感检测（ED）目标是建立自动识别用户情感的系统。这个领域有可能提高人机交互（HCI），创造个性化的用户体验。然而，ED系统通常在Autism Spectrum Disorder（ASD）人群表现不佳。因此，需要开发特化于人们表达情感的ED系统。先前的工作已经创建了特化于儿童ASD的ED系统，但没有公布数据集。分享标注数据集是开发更先进的计算机模型的关键。在这篇论文中，我们描述了我们在建立一个多模态注释数据集的过程中的经验。此外，我们还介绍了CALMED（儿童Autism、多模态、情感检测）数据集，这是8-12岁的儿童ASD的多模态情感检测数据集。CALMED包括录音和视频特征，从参与者录制的文件中提取，以及由参与者的父母提供的四个目标类别的注释。生成的数据集包括57,012个示例，每个示例表示200毫秒（0.2秒）的时间窗口。我们的经验和方法描述以及分享的数据，希望能为ASD的情感计算机科学研究提供贡献，以创造用于改善ASD人群生活质量的系统。
</details></li>
</ul>
<hr>
<h2 id="MC-JEPA-A-Joint-Embedding-Predictive-Architecture-for-Self-Supervised-Learning-of-Motion-and-Content-Features"><a href="#MC-JEPA-A-Joint-Embedding-Predictive-Architecture-for-Self-Supervised-Learning-of-Motion-and-Content-Features" class="headerlink" title="MC-JEPA: A Joint-Embedding Predictive Architecture for Self-Supervised Learning of Motion and Content Features"></a>MC-JEPA: A Joint-Embedding Predictive Architecture for Self-Supervised Learning of Motion and Content Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12698">http://arxiv.org/abs/2307.12698</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrien Bardes, Jean Ponce, Yann LeCun</li>
<li>for: 学习视觉表示，强调了学习内容特征，而不是捕捉物体运动或位置信息。</li>
<li>methods: 我们提出了 MC-JEPA 方法，即共同嵌入预测架构和自动学习方法，以同时学习光流和内容特征，并证明了这两个关联目标互助彼此，从而学习了包含运动信息的内容特征。</li>
<li>results: 我们的方法可以与现有的无监督光流标准做比较，以及与常见的自动学习方法在图像和视频 semantic segmentation 任务上表现相当。<details>
<summary>Abstract</summary>
Self-supervised learning of visual representations has been focusing on learning content features, which do not capture object motion or location, and focus on identifying and differentiating objects in images and videos. On the other hand, optical flow estimation is a task that does not involve understanding the content of the images on which it is estimated. We unify the two approaches and introduce MC-JEPA, a joint-embedding predictive architecture and self-supervised learning approach to jointly learn optical flow and content features within a shared encoder, demonstrating that the two associated objectives; the optical flow estimation objective and the self-supervised learning objective; benefit from each other and thus learn content features that incorporate motion information. The proposed approach achieves performance on-par with existing unsupervised optical flow benchmarks, as well as with common self-supervised learning approaches on downstream tasks such as semantic segmentation of images and videos.
</details>
<details>
<summary>摘要</summary>
自领导学习的视觉表示学习把注意力集中在学习内容特征上，这些特征不包括物体运动或位置信息，而是通过识别和区分图像和视频中的 объек 来学习。然而，流体计算是一个不需要理解图像内容的任务。我们将这两种方法联合起来，并引入 MC-JEPA，一种共享编码器中的联合预测建筑和自领导学习方法，以便同时学习流体计算和内容特征。我们发现这两个关联的目标，即流体计算目标和自领导学习目标，互相帮助和学习，因此学习的内容特征包含运动信息。我们的方法可以与现有的无监督光流标准准确比较，以及常见的自领导学习方法在图像和视频 semantic 分割任务中的性能。
</details></li>
</ul>
<hr>
<h2 id="Addressing-the-Impact-of-Localized-Training-Data-in-Graph-Neural-Networks"><a href="#Addressing-the-Impact-of-Localized-Training-Data-in-Graph-Neural-Networks" class="headerlink" title="Addressing the Impact of Localized Training Data in Graph Neural Networks"></a>Addressing the Impact of Localized Training Data in Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12689">http://arxiv.org/abs/2307.12689</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/akanshaaga/reg_appnp">https://github.com/akanshaaga/reg_appnp</a></li>
<li>paper_authors: Singh Akansha</li>
<li>for: 本文旨在评估 Graph Neural Networks (GNNs) 在不同区域的图数据上的性能，以及如何在受限的训练数据情况下提高 GNN 的适应性和泛化能力。</li>
<li>methods: 本文提出了一种基于 distribution alignment 的正则化方法，用于解决 GNN 在本地化训练数据上的性能下降问题。</li>
<li>results: 经过广泛测试， Results 表明该正则化方法可以有效地提高 GNN 在异常数据上的性能，并且可以帮助 GNN 更好地适应和泛化到不同的图数据上。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have achieved notable success in learning from graph-structured data, owing to their ability to capture intricate dependencies and relationships between nodes. They excel in various applications, including semi-supervised node classification, link prediction, and graph generation. However, it is important to acknowledge that the majority of state-of-the-art GNN models are built upon the assumption of an in-distribution setting, which hinders their performance on real-world graphs with dynamic structures. In this article, we aim to assess the impact of training GNNs on localized subsets of the graph. Such restricted training data may lead to a model that performs well in the specific region it was trained on but fails to generalize and make accurate predictions for the entire graph. In the context of graph-based semi-supervised learning (SSL), resource constraints often lead to scenarios where the dataset is large, but only a portion of it can be labeled, affecting the model's performance. This limitation affects tasks like anomaly detection or spam detection when labeling processes are biased or influenced by human subjectivity. To tackle the challenges posed by localized training data, we approach the problem as an out-of-distribution (OOD) data issue by by aligning the distributions between the training data, which represents a small portion of labeled data, and the graph inference process that involves making predictions for the entire graph. We propose a regularization method to minimize distributional discrepancies between localized training data and graph inference, improving model performance on OOD data. Extensive tests on popular GNN models show significant performance improvement on three citation GNN benchmark datasets. The regularization approach effectively enhances model adaptation and generalization, overcoming challenges posed by OOD data.
</details>
<details>
<summary>摘要</summary>
граф neural networks (GNNs) 已经取得了很大的成功，它们可以从图структуре数据中学习，因为它们可以捕捉图中节点之间的复杂关系和依赖关系。它们在半supervised node classification、链接预测和图生成等应用中表现出色。然而，我们需要注意的是，大多数当前的状态部署GNN模型是基于图结构的内部分布式Setting中建模，这会限制它们在实际图中的性能。在这篇文章中，我们试图评估在本地化Subset中训练GNN模型的影响。这种局部训练数据可能会导致模型在特定区域中表现出色，但是它无法总结整个图中的准确预测。在图基于半supervised learning (SSL) 中，资源约束常常导致数据集很大，但只有一部分可以被标记，这会影响模型的性能。这种限制会对任务 like anomaly detection 或 spam detection 产生影响，因为标记过程可能受到人类主观性的影响。为了解决本地化训练数据所带来的挑战，我们将这种问题看作一个out-of-distribution (OOD) 数据问题，并通过对本地化训练数据和图推理过程之间的分布匹配来解决问题。我们提出一种Regularization方法，以降低本地化训练数据和图推理之间的分布差异，从而提高OOD数据上的模型表现。我们在popular GNN模型上进行了广泛的测试，并发现这种Regularization方法可以在三个文献GNN benchmark数据集上显著提高表现。这种Regularization方法可以增强模型的适应和泛化能力，从而超越OOD数据的挑战。
</details></li>
</ul>
<hr>
<h2 id="IteraTTA-An-interface-for-exploring-both-text-prompts-and-audio-priors-in-generating-music-with-text-to-audio-models"><a href="#IteraTTA-An-interface-for-exploring-both-text-prompts-and-audio-priors-in-generating-music-with-text-to-audio-models" class="headerlink" title="IteraTTA: An interface for exploring both text prompts and audio priors in generating music with text-to-audio models"></a>IteraTTA: An interface for exploring both text prompts and audio priors in generating music with text-to-audio models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13005">http://arxiv.org/abs/2307.13005</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hiromu Yakura, Masataka Goto</li>
<li>for: 帮助用户自由生成音乐音频，不需具备音乐知识，通过不同文本提示和音频先导来探索音频生成空间。</li>
<li>methods: 使用文本-到-音频生成技术，并提供用户可以进行双重探索（文本提示和音频先导），以便理解不同文本提示和音频先导对生成结果的影响，并逐渐实现用户的模糊化目标。</li>
<li>results: 通过提供特定的音频先导和文本提示，用户可以逐渐理解和探索音频生成空间，并通过反复比较不同的文本提示和音频先导来了解它们对生成结果的影响。<details>
<summary>Abstract</summary>
Recent text-to-audio generation techniques have the potential to allow novice users to freely generate music audio. Even if they do not have musical knowledge, such as about chord progressions and instruments, users can try various text prompts to generate audio. However, compared to the image domain, gaining a clear understanding of the space of possible music audios is difficult because users cannot listen to the variations of the generated audios simultaneously. We therefore facilitate users in exploring not only text prompts but also audio priors that constrain the text-to-audio music generation process. This dual-sided exploration enables users to discern the impact of different text prompts and audio priors on the generation results through iterative comparison of them. Our developed interface, IteraTTA, is specifically designed to aid users in refining text prompts and selecting favorable audio priors from the generated audios. With this, users can progressively reach their loosely-specified goals while understanding and exploring the space of possible results. Our implementation and discussions highlight design considerations that are specifically required for text-to-audio models and how interaction techniques can contribute to their effectiveness.
</details>
<details>
<summary>摘要</summary>
现代文本到音频生成技术具有让新手无需 musical knowledge 可以自由生成音乐音频的潜力。用户可以通过不同的文本提示来尝试生成音频，而不需要了解和器械进程和和谐进程。然而，与图像领域相比，了解音乐频谱中可能的音频空间是Difficult的，因为用户无法同时听到生成的音频变化。我们因此为用户提供了探索不仅文本提示而且受限于文本到音频生成过程的音频先例的机会。这种双重探索使得用户可以通过相互比较不同的文本提示和音频先例来了解它们对生成结果的影响。我们开发的界面IteraTTA特别设计为帮助用户精细调整文本提示和选择生成音频中有利的先例。通过这种方式，用户可以逐步实现自己的模糊化目标，同时了解和探索可能的结果空间。我们的实现和讨论探讨了特定于文本到音频模型的设计考虑和交互技术如何提高其效果。
</details></li>
</ul>
<hr>
<h2 id="Control-and-Monitoring-of-Artificial-Intelligence-Algorithms"><a href="#Control-and-Monitoring-of-Artificial-Intelligence-Algorithms" class="headerlink" title="Control and Monitoring of Artificial Intelligence Algorithms"></a>Control and Monitoring of Artificial Intelligence Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13705">http://arxiv.org/abs/2307.13705</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Carlos Mario Braga Ortuño, Blanza Martinez Donoso, Belén Muñiz Villanueva</li>
<li>for: 本研究阐述了在训练完成后，如何监督人工智能模型的运行，并处理可能出现的数据分布的变化。</li>
<li>methods: 本研究介绍了一些用于评估模型表现的指标，以及适当的数据分布基础。</li>
<li>results: 研究发现，监督模型的运行可以帮助检测和处理数据分布的变化，并且可以提高模型的表现。<details>
<summary>Abstract</summary>
This paper elucidates the importance of governing an artificial intelligence model post-deployment and overseeing potential fluctuations in the distribution of present data in contrast to the training data. The concepts of data drift and concept drift are explicated, along with their respective foundational distributions. Furthermore, a range of metrics is introduced, which can be utilized to scrutinize the model's performance concerning potential temporal variations.
</details>
<details>
<summary>摘要</summary>
Translation Notes:* "post-deployment" is translated as "后部署" (hòu bù zhì)* "potential fluctuations" is translated as "潜在的波动" (pán zài de bō dòng)* "data drift" is translated as "数据漂移" (shù jí qiáo yí)* "concept drift" is translated as "概念漂移" (gài yán qiáo yí)* "foundational distributions" is translated as "基础分布" (jī zhì fēn zhòu)* "scrutinize" is translated as "检查" (jiǎn chá)* "concerning potential temporal variations" is translated as "关于潜在的时间变化" (guān yù pán zài de shí huan bìng xiàng)
</details></li>
</ul>
<hr>
<h2 id="Remote-Bio-Sensing-Open-Source-Benchmark-Framework-for-Fair-Evaluation-of-rPPG"><a href="#Remote-Bio-Sensing-Open-Source-Benchmark-Framework-for-Fair-Evaluation-of-rPPG" class="headerlink" title="Remote Bio-Sensing: Open Source Benchmark Framework for Fair Evaluation of rPPG"></a>Remote Bio-Sensing: Open Source Benchmark Framework for Fair Evaluation of rPPG</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12644">http://arxiv.org/abs/2307.12644</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/remotebiosensing/rppg">https://github.com/remotebiosensing/rppg</a></li>
<li>paper_authors: Dae-Yeol Kim, Eunsu Goh, KwangKee Lee, JongEui Chae, JongHyeon Mun, Junyeong Na, Chae-bong Sohn, Do-Yup Kim</li>
<li>For: This paper aims to provide a benchmarking framework for evaluating the performance of remote photoplethysmography (rPPG) techniques across a wide range of datasets, to facilitate fair comparison and progress in the field.* Methods: The paper uses a variety of datasets and benchmarking metrics to evaluate the performance of both conventional non-deep neural network (non-DNN) and deep neural network (DNN) methods for rPPG.* Results: The paper provides a comprehensive evaluation of the performance of different rPPG techniques on a wide range of datasets, and highlights the need for fair and evaluable benchmarking to overcome challenges in the field and make meaningful progress.<details>
<summary>Abstract</summary>
rPPG (Remote photoplethysmography) is a technology that measures and analyzes BVP (Blood Volume Pulse) by using the light absorption characteristics of hemoglobin captured through a camera. Analyzing the measured BVP can derive various physiological signals such as heart rate, stress level, and blood pressure, which can be applied to various applications such as telemedicine, remote patient monitoring, and early prediction of cardiovascular disease. rPPG is rapidly evolving and attracting great attention from both academia and industry by providing great usability and convenience as it can measure biosignals using a camera-equipped device without medical or wearable devices. Despite extensive efforts and advances in this field, serious challenges remain, including issues related to skin color, camera characteristics, ambient lighting, and other sources of noise and artifacts, which degrade accuracy performance. We argue that fair and evaluable benchmarking is urgently required to overcome these challenges and make meaningful progress from both academic and commercial perspectives. In most existing work, models are trained, tested, and validated only on limited datasets. Even worse, some studies lack available code or reproducibility, making it difficult to fairly evaluate and compare performance. Therefore, the purpose of this study is to provide a benchmarking framework to evaluate various rPPG techniques across a wide range of datasets for fair evaluation and comparison, including both conventional non-deep neural network (non-DNN) and deep neural network (DNN) methods. GitHub URL: https://github.com/remotebiosensing/rppg
</details>
<details>
<summary>摘要</summary>
remote 血液氧测技术 (rPPG) 是一种利用摄像机捕捉到血液中内氧滤过特性，并分析血液量脉搏 (BVP) 的技术。通过分析测量的 BVP，可以 derivate 多种生理信号，如心率、压力和压力等，这些信号可以应用于多个应用，如远程医疗、远程患者监控和早期心血管疾病预后评估。 rPPG 在学术和业界中受到广泛关注，因为它提供了很好的使用性和便利性，可以通过摄像机设备而不需要医疗器械或戴式设备来量测生物信号。然而，这个领域仍然面临许多挑战，包括皮肤颜色、摄像机特性、环境照明和其他干扰和错误的问题，这些问题会影响性能。我们认为，对于这些挑战的公平和评估是非常重要的，以实现学术和商业上的进步。大多数现有的工作都是将模型训练、测试和验证在有限的数据集上。甚至更糟糕，一些研究缺乏可用的代码或重现性，使得公平评估和比较性能的问题更加困难。因此，本研究的目的是提供一个 benchmarking 框架，以评估不同的 rPPG 技术在广泛的数据集上的性能，以便公平评估和比较，包括非深度神经网络 (non-DNN) 和深度神经网络 (DNN) 方法。GitHub URL: <https://github.com/remotebiosensing/rppg>
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Human-like-Multi-Modal-Reasoning-A-New-Challenging-Dataset-and-Comprehensive-Framework"><a href="#Enhancing-Human-like-Multi-Modal-Reasoning-A-New-Challenging-Dataset-and-Comprehensive-Framework" class="headerlink" title="Enhancing Human-like Multi-Modal Reasoning: A New Challenging Dataset and Comprehensive Framework"></a>Enhancing Human-like Multi-Modal Reasoning: A New Challenging Dataset and Comprehensive Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12626">http://arxiv.org/abs/2307.12626</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingxuan Wei, Cheng Tan, Zhangyang Gao, Linzhuang Sun, Siyuan Li, Bihui Yu, Ruifeng Guo, Stan Z. Li</li>
<li>for:  This paper aims to address the lack of comprehensive evaluation of diverse approaches in multimodal scientific question answering, by presenting a novel dataset (COCO Multi-Modal Reasoning Dataset) that includes open-ended questions, rationales, and answers derived from the large object dataset COCO.</li>
<li>methods:  The proposed dataset pioneers the use of open-ended questions in the context of multimodal chain-of-thought, which introduces a more challenging problem that effectively assesses the reasoning capability of CoT models. The authors propose innovative techniques, including multi-hop cross-modal attention and sentence-level contrastive learning, to enhance the image and text encoders.</li>
<li>results:  Extensive experiments demonstrate the efficacy of the proposed dataset and techniques, offering novel perspectives for advancing multimodal reasoning. The proposed methods and dataset provide valuable insights and offer a more challenging problem for advancing the field of multimodal reasoning.<details>
<summary>Abstract</summary>
Multimodal reasoning is a critical component in the pursuit of artificial intelligence systems that exhibit human-like intelligence, especially when tackling complex tasks. While the chain-of-thought (CoT) technique has gained considerable attention, the existing ScienceQA dataset, which focuses on multimodal scientific questions and explanations from elementary and high school textbooks, lacks a comprehensive evaluation of diverse approaches. To address this gap, we present COCO Multi-Modal Reasoning Dataset(COCO-MMRD), a novel dataset that encompasses an extensive collection of open-ended questions, rationales, and answers derived from the large object dataset COCO. Unlike previous datasets that rely on multiple-choice questions, our dataset pioneers the use of open-ended questions in the context of multimodal CoT, introducing a more challenging problem that effectively assesses the reasoning capability of CoT models. Through comprehensive evaluations and detailed analyses, we provide valuable insights and propose innovative techniques, including multi-hop cross-modal attention and sentence-level contrastive learning, to enhance the image and text encoders. Extensive experiments demonstrate the efficacy of the proposed dataset and techniques, offering novel perspectives for advancing multimodal reasoning.
</details>
<details>
<summary>摘要</summary>
多Modal重要组成部分在人工智能系统具有人类智能的追求中，尤其是在较复杂的任务上。而链式思考（CoT）技术已经受到了广泛关注，但现有的科学QA数据集（ScienceQA），which focuses on multimodal scientific questions and explanations from elementary and high school textbooks, lacks a comprehensive evaluation of diverse approaches. To address this gap, we present COCO Multi-Modal Reasoning Dataset(COCO-MMRD), a novel dataset that encompasses an extensive collection of open-ended questions, rationales, and answers derived from the large object dataset COCO. Unlike previous datasets that rely on multiple-choice questions, our dataset pioneers the use of open-ended questions in the context of multimodal CoT, introducing a more challenging problem that effectively assesses the reasoning capability of CoT models. Through comprehensive evaluations and detailed analyses, we provide valuable insights and propose innovative techniques, including multi-hop cross-modal attention and sentence-level contrastive learning, to enhance the image and text encoders. Extensive experiments demonstrate the efficacy of the proposed dataset and techniques, offering novel perspectives for advancing multimodal reasoning.Here is the word-for-word translation of the text into Simplified Chinese:多Modal重要组成部分在人工智能系统具有人类智能的追求中，尤其是在较复杂的任务上。而链式思考（CoT）技术已经受到了广泛关注，但现有的科学QA数据集（ScienceQA），which focuses on multimodal scientific questions and explanations from elementary and high school textbooks, lacks a comprehensive evaluation of diverse approaches. To address this gap, we present COCO Multi-Modal Reasoning Dataset(COCO-MMRD), a novel dataset that encompasses an extensive collection of open-ended questions, rationales, and answers derived from the large object dataset COCO. Unlike previous datasets that rely on multiple-choice questions, our dataset pioneers the use of open-ended questions in the context of multimodal CoT, introducing a more challenging problem that effectively assesses the reasoning capability of CoT models. Through comprehensive evaluations and detailed analyses, we provide valuable insights and propose innovative techniques, including multi-hop cross-modal attention and sentence-level contrastive learning, to enhance the image and text encoders. Extensive experiments demonstrate the efficacy of the proposed dataset and techniques, offering novel perspectives for advancing multimodal reasoning.
</details></li>
</ul>
<hr>
<h2 id="De-confounding-Representation-Learning-for-Counterfactual-Inference-on-Continuous-Treatment-via-Generative-Adversarial-Network"><a href="#De-confounding-Representation-Learning-for-Counterfactual-Inference-on-Continuous-Treatment-via-Generative-Adversarial-Network" class="headerlink" title="De-confounding Representation Learning for Counterfactual Inference on Continuous Treatment via Generative Adversarial Network"></a>De-confounding Representation Learning for Counterfactual Inference on Continuous Treatment via Generative Adversarial Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12625">http://arxiv.org/abs/2307.12625</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yonghe Zhao, Qiang Huang, Haolong Zeng, Yun Pen, Huiyan Sun</li>
<li>for: 这种论文主要针对的是如何对连续型干预变量进行Counterfactual推断，而现实世界中更常见的是连续型干预变量的Counterfactual推断任务。</li>
<li>methods: 我们提出了一种基于De-confounding Representation Learning（DRL）的框架，通过生成与干预变量分离的covariate表示来消除干预变量与covariate之间的相关性。DRL是一种非 Parametric 模型，可以消除连续型干预变量与covariate之间的线性和非线性相关性。</li>
<li>results: 在 synthetic 数据集上进行了广泛的实验，发现 DRL 模型在学习分离表示的同时，也可以超越当前Counterfactual推断模型的性能。此外，我们还应用了 DRL 模型到一个实际的医疗数据集 MIMIC，并显示出了连续型红细胞宽度分布和死亡率之间的详细 causal 关系。<details>
<summary>Abstract</summary>
Counterfactual inference for continuous rather than binary treatment variables is more common in real-world causal inference tasks. While there are already some sample reweighting methods based on Marginal Structural Model for eliminating the confounding bias, they generally focus on removing the treatment's linear dependence on confounders and rely on the accuracy of the assumed parametric models, which are usually unverifiable. In this paper, we propose a de-confounding representation learning (DRL) framework for counterfactual outcome estimation of continuous treatment by generating the representations of covariates disentangled with the treatment variables. The DRL is a non-parametric model that eliminates both linear and nonlinear dependence between treatment and covariates. Specifically, we train the correlations between the de-confounded representations and the treatment variables against the correlations between the covariate representations and the treatment variables to eliminate confounding bias. Further, a counterfactual inference network is embedded into the framework to make the learned representations serve both de-confounding and trusted inference. Extensive experiments on synthetic datasets show that the DRL model performs superiorly in learning de-confounding representations and outperforms state-of-the-art counterfactual inference models for continuous treatment variables. In addition, we apply the DRL model to a real-world medical dataset MIMIC and demonstrate a detailed causal relationship between red cell width distribution and mortality.
</details>
<details>
<summary>摘要</summary>
<SYS> translate-into-simplified-chineseCounterfactual inference for continuous rather than binary treatment variables is more common in real-world causal inference tasks. While there are already some sample reweighting methods based on Marginal Structural Model for eliminating the confounding bias, they generally focus on removing the treatment's linear dependence on confounders and rely on the accuracy of the assumed parametric models, which are usually unverifiable. In this paper, we propose a de-confounding representation learning (DRL) framework for counterfactual outcome estimation of continuous treatment by generating the representations of covariates disentangled with the treatment variables. The DRL is a non-parametric model that eliminates both linear and nonlinear dependence between treatment and covariates. Specifically, we train the correlations between the de-confounded representations and the treatment variables against the correlations between the covariate representations and the treatment variables to eliminate confounding bias. Further, a counterfactual inference network is embedded into the framework to make the learned representations serve both de-confounding and trusted inference. Extensive experiments on synthetic datasets show that the DRL model performs superiorly in learning de-confounding representations and outperforms state-of-the-art counterfactual inference models for continuous treatment variables. In addition, we apply the DRL model to a real-world medical dataset MIMIC and demonstrate a detailed causal relationship between red cell width distribution and mortality.</SYS>Here's the text in Simplified Chinese:<SYS>对于连续型干预变量，Counterfactual inference更常用于实际世界 causal inference 任务中。虽然现有一些基于Marginal Structural Model的样本重新权重方法可以消除干预的相关性，但这些方法通常只是消除干预对隐藏变量的直线相关性，并且它们的假设模型通常是不可验证的。在这篇论文中，我们提出了一种基于 representation learning 的 de-confounding 框架，用于计算连续型干预下的可靠的结果。该框架使用非 Parametric 模型来消除干预和隐藏变量之间的线性和非线性相关性。具体来说，我们在 DRL 框架中训练了对干预变量和隐藏变量之间的相关性，并与隐藏变量和干预变量之间的相关性进行对比，以消除干预偏见。此外，我们还在框架中嵌入了一个 counterfactual inference 网络，以使得学习的表示可以服务于 both de-confounding 和可靠的推理。在 synthetic 数据集上进行了广泛的实验，显示 DRL 模型在学习 de-confounding 表示方面表现出色，并且超越了现有的 state-of-the-art counterfactual inference 模型。此外，我们还应用了 DRL 模型到了一个实际医疗数据集 MIMIC，并通过示出红细胞宽度分布和死亡之间的详细 causal 关系，以证明 DRL 模型的效果。</SYS>
</details></li>
</ul>
<hr>
<h2 id="Past-present-temporal-programs-over-finite-traces"><a href="#Past-present-temporal-programs-over-finite-traces" class="headerlink" title="Past-present temporal programs over finite traces"></a>Past-present temporal programs over finite traces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12620">http://arxiv.org/abs/2307.12620</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pedro Cabalar, Martín Diéguez, François Laferrière, Torsten Schaub</li>
<li>for: 这个论文旨在探讨逻辑编程的扩展，使用时间逻辑的语言结构来模型动态应用。</li>
<li>methods: 这篇论文使用了TELf语义，对过去和当前的逻辑编程规则进行研究，并将过去和当前分为不同的语义级别。</li>
<li>results: 论文提出了一种基于LTLf表达式的完成和循环式表达式的定义，以捕捉过去和当前 temporal 稳定模型。<details>
<summary>Abstract</summary>
Extensions of Answer Set Programming with language constructs from temporal logics, such as temporal equilibrium logic over finite traces (TELf), provide an expressive computational framework for modeling dynamic applications. In this paper, we study the so-called past-present syntactic subclass, which consists of a set of logic programming rules whose body references to the past and head to the present. Such restriction ensures that the past remains independent of the future, which is the case in most dynamic domains. We extend the definitions of completion and loop formulas to the case of past-present formulas, which allows capturing the temporal stable models of a set of past-present temporal programs by means of an LTLf expression.
</details>
<details>
<summary>摘要</summary>
扩展回答集编程的语言结构，如基于时间逻辑的时间平衡逻辑（TELf），提供了一种表达强大的计算框架，用于模型动态应用。本文研究一种叫做过去今天的 sintactic subclass，它包含一组逻辑编程规则，其体中参考过去，头部参考当前。这种限制保证了过去不会受到未来的影响，这是动态领域中的典型情况。我们将完成和循环式的定义扩展到过去今天的表达中，以便通过 LTLf 表达捕捉过去今天的时间稳定模型。
</details></li>
</ul>
<hr>
<h2 id="CTVIS-Consistent-Training-for-Online-Video-Instance-Segmentation"><a href="#CTVIS-Consistent-Training-for-Online-Video-Instance-Segmentation" class="headerlink" title="CTVIS: Consistent Training for Online Video Instance Segmentation"></a>CTVIS: Consistent Training for Online Video Instance Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12616">http://arxiv.org/abs/2307.12616</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kainingying/ctvis">https://github.com/kainingying/ctvis</a></li>
<li>paper_authors: Kaining Ying, Qing Zhong, Weian Mao, Zhenhua Wang, Hao Chen, Lin Yuanbo Wu, Yifan Liu, Chengxiang Fan, Yunzhi Zhuge, Chunhua Shen</li>
<li>for: 本研究旨在提高在线视频实例分割（VIS）中的实例嵌入差异，以便在不同时刻进行实例关联。</li>
<li>methods: 本研究使用了对比损失来直接监督实例嵌入学习，并使用了一种叫做“一致训练”的简单而有效的训练策略，以提高实例嵌入的可靠性。</li>
<li>results: 实验表明，使用“一致训练”策略可以提高实例嵌入的可靠性，并在三个 VIS 测试套件中提高了 SOTA 模型的性能，包括 YTVIS19（55.1% AP）、YTVIS21（50.1% AP）和 OVIS（35.5% AP）。此外，我们还发现使用 pseudo-video 从图像转换而来的模型可以训练出比 Fully-supervised 模型更加强大的模型。<details>
<summary>Abstract</summary>
The discrimination of instance embeddings plays a vital role in associating instances across time for online video instance segmentation (VIS). Instance embedding learning is directly supervised by the contrastive loss computed upon the contrastive items (CIs), which are sets of anchor/positive/negative embeddings. Recent online VIS methods leverage CIs sourced from one reference frame only, which we argue is insufficient for learning highly discriminative embeddings. Intuitively, a possible strategy to enhance CIs is replicating the inference phase during training. To this end, we propose a simple yet effective training strategy, called Consistent Training for Online VIS (CTVIS), which devotes to aligning the training and inference pipelines in terms of building CIs. Specifically, CTVIS constructs CIs by referring inference the momentum-averaged embedding and the memory bank storage mechanisms, and adding noise to the relevant embeddings. Such an extension allows a reliable comparison between embeddings of current instances and the stable representations of historical instances, thereby conferring an advantage in modeling VIS challenges such as occlusion, re-identification, and deformation. Empirically, CTVIS outstrips the SOTA VIS models by up to +5.0 points on three VIS benchmarks, including YTVIS19 (55.1% AP), YTVIS21 (50.1% AP) and OVIS (35.5% AP). Furthermore, we find that pseudo-videos transformed from images can train robust models surpassing fully-supervised ones.
</details>
<details>
<summary>摘要</summary>
<<SYS>>对于在线视频实例分割（VIS）中，实例嵌入的歧视扮演着关键的角色。我们使用对比损失来直接监督实例嵌入学习，其中对比项（CI）是一组锚点/正例/负例嵌入的集合。在线 VIS 方法通常使用一个参考帧中的 CIs，我们认为这是学习高度抽象的嵌入的不够。我们提出了一种简单 yet 有效的训练策略，called Consistent Training for Online VIS（CTVIS），它的目的是在训练和推理管道中对实例嵌入进行对应。具体来说，CTVIS 使用暂停聚合的嵌入和存储机制，并将它们添加到相关的嵌入中。这种扩展使得可以在训练和推理中对实例嵌入进行可靠的比较，从而提高对 occlusion、重复、和变形等挑战的适应能力。实验证明，CTVIS 可以超越当前 SOTA VIS 模型，提高 YTVIS19（55.1% AP）、YTVIS21（50.1% AP）和 OVIS（35.5% AP）等三个 VIS 标准套件中的成绩。此外，我们发现在图像转换成 pseudo-video 后训练的模型可以超过完全监督的模型。>>>
</details></li>
</ul>
<hr>
<h2 id="Regulating-AI-manipulation-Applying-Insights-from-behavioral-economics-and-psychology-to-enhance-the-practicality-of-the-EU-AI-Act"><a href="#Regulating-AI-manipulation-Applying-Insights-from-behavioral-economics-and-psychology-to-enhance-the-practicality-of-the-EU-AI-Act" class="headerlink" title="Regulating AI manipulation: Applying Insights from behavioral economics and psychology to enhance the practicality of the EU AI Act"></a>Regulating AI manipulation: Applying Insights from behavioral economics and psychology to enhance the practicality of the EU AI Act</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02041">http://arxiv.org/abs/2308.02041</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huixin Zhong</li>
<li>For: 这篇论文的目的是为了解释和增强欧盟人工智能法规第五条的执行，以防止人工智能操纵的可能有害后果。* Methods: 这篇论文使用了认知心理学和行为经济学的研究来解释潜意识技术和相关表达的概念，并将行为经济学中的决策简化技巧推广到操纵技术的领域。* Results: 这篇论文提出了五种经典的决策简化技巧和其相应的示例，以便用户、开发者、算法审核人和法律专业人员识别操纵技术并采取对策。此外，论文还对欧盟人工智能法规第五条的保护效果进行了批判性评估，并提出了特定的修改建议以增强保护效果。<details>
<summary>Abstract</summary>
The EU AI Act Article 5 is designed to regulate AI manipulation to prevent potential harmful consequences. However, the practical implementation of this legislation is challenging due to the ambiguous terminologies and the unclear presentations of manipulative techniques. Moreover, the Article 5 also suffers criticize of inadequate protective efficacy. This paper attempts to clarify terminologies and to enhance the protective efficacy by integrating insights from psychology and behavioral economics. Firstly, this paper employs cognitive psychology research to elucidate the term subliminal techniques and its associated representation. Additionally, this paper extends the study of heuristics: a set of thinking shortcuts which can be aroused for behavior changing from behavior economics to the realm of manipulative techniques. The elucidation and expansion of terminologies not only provide a more accurate understanding of the legal provision but also enhance its protective efficacy. Secondly, this paper proposes five classical heuristics and their associated examples to illustrate how can AI arouse those heuristics to alter users behavior. The enumeration of heuristics serves as a practical guide for stakeholders such as AI developers, algorithm auditors, users, and legal practitioners, enabling them to identify manipulative techniques and implement countermeasures. Finally, this paper critically evaluates the protective efficacy of Article 5 for both the general public and vulnerable groups. This paper argues that the current protective efficacy of Article 5 is insufficient and thus proposes specific revision suggestions to terms a and b in Article 5 to enhance its protective efficacy. This work contributes to the ongoing discourse on AI ethics and legal regulations, providing a practical guide for interpreting and applying the EU AI Act Article 5.
</details>
<details>
<summary>摘要</summary>
欧盟人工智能法 Article 5 是为了规范人工智能操纵，避免可能的有害后果。然而，实施这一法律的具体方法是困难的，因为涉及的术语抽象，以及操纵技巧的不清楚表述。此外， Article 5 还受到了不充分的保护效果的批评。这篇论文试图使用认知心理学和行为经济学的意识来减轻这些问题。首先，这篇论文使用认知心理学研究来解释“潜意识技巧”的概念，并与其相关的表达相结合。此外，这篇论文将行为经济学中的决策简化技巧（heuristics）扩展到了操纵技巧的领域。通过这种方式，不仅可以提供更加准确的法律规定的理解，还可以提高其保护效果。其次，这篇论文提出五种经典的决策简化技巧和其相应的示例，以示AI如何使用这些技巧来改变用户的行为。这些技巧的列表 serves as a practical guide for stakeholders such as AI developers, algorithm auditors, users, and legal practitioners，allowing them to identify manipulative techniques and implement countermeasures。最后，这篇论文 kritisch evaluates the protective efficacy of Article 5 for both the general public and vulnerable groups。这篇论文 argue that the current protective efficacy of Article 5 is insufficient, and thus proposes specific revision suggestions to terms a and b in Article 5 to enhance its protective efficacy。这篇论文的成果将贡献到欧盟人工智能伦理法规的继续讨论中，并提供了一份实用的指南，用于解读和应用欧盟人工智能法 Article 5。
</details></li>
</ul>
<hr>
<h2 id="Less-is-More-Focus-Attention-for-Efficient-DETR"><a href="#Less-is-More-Focus-Attention-for-Efficient-DETR" class="headerlink" title="Less is More: Focus Attention for Efficient DETR"></a>Less is More: Focus Attention for Efficient DETR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12612">http://arxiv.org/abs/2307.12612</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/huawei-noah/noah-research">https://github.com/huawei-noah/noah-research</a></li>
<li>paper_authors: Dehua Zheng, Wenhui Dong, Hailin Hu, Xinghao Chen, Yunhe Wang<br>for: 这个论文的目的是提高DETR-like模型的计算效率，同时保持模型的准确率。methods: 这个论文使用了一种名为Focus-DETR的方法，它使用了双重注意 Mechanism来注意更有用的 токен，从而提高计算效率。具体来说，它首先使用了一种名为Token Scoring Mechanism来评估每个 токен的重要性，然后使用了一种名为Enhanced Semantic Interaction Mechanism来提高对象的 semantic interaction。results:  Comparing with state-of-the-art sparse DETR-like detectors under the same setting, Focus-DETR achieves 50.4AP (+2.2) on COCO, with comparable complexity.<details>
<summary>Abstract</summary>
DETR-like models have significantly boosted the performance of detectors and even outperformed classical convolutional models. However, all tokens are treated equally without discrimination brings a redundant computational burden in the traditional encoder structure. The recent sparsification strategies exploit a subset of informative tokens to reduce attention complexity maintaining performance through the sparse encoder. But these methods tend to rely on unreliable model statistics. Moreover, simply reducing the token population hinders the detection performance to a large extent, limiting the application of these sparse models. We propose Focus-DETR, which focuses attention on more informative tokens for a better trade-off between computation efficiency and model accuracy. Specifically, we reconstruct the encoder with dual attention, which includes a token scoring mechanism that considers both localization and category semantic information of the objects from multi-scale feature maps. We efficiently abandon the background queries and enhance the semantic interaction of the fine-grained object queries based on the scores. Compared with the state-of-the-art sparse DETR-like detectors under the same setting, our Focus-DETR gets comparable complexity while achieving 50.4AP (+2.2) on COCO. The code is available at https://github.com/huawei-noah/noah-research/tree/master/Focus-DETR and https://gitee.com/mindspore/models/tree/master/research/cv/Focus-DETR.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SL-Stable-Learning-in-Source-Free-Domain-Adaption-for-Medical-Image-Segmentation"><a href="#SL-Stable-Learning-in-Source-Free-Domain-Adaption-for-Medical-Image-Segmentation" class="headerlink" title="SL: Stable Learning in Source-Free Domain Adaption for Medical Image Segmentation"></a>SL: Stable Learning in Source-Free Domain Adaption for Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12580">http://arxiv.org/abs/2307.12580</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yixin Chen, Yan Wang</li>
<li>for: 这篇论文是针对医疗影像分析中的深度学习技术，尤其是面临域名变化问题。</li>
<li>methods: 本研究提出了一个名为“稳定学习”（Stable Learning）策略，用于解决“长期训练，差化表现”的问题。这个策略包括对预测重量进行整合和增加熵。</li>
<li>results: 比较实验显示了这个策略的有效性。此外，研究者还进行了广泛的剥离实验，以评估不同方法之间的比较。<details>
<summary>Abstract</summary>
Deep learning techniques for medical image analysis usually suffer from the domain shift between source and target data. Most existing works focus on unsupervised domain adaptation (UDA). However, in practical applications, privacy issues are much more severe. For example, the data of different hospitals have domain shifts due to equipment problems, and data of the two domains cannot be available simultaneously because of privacy. In this challenge defined as Source-Free UDA, the previous UDA medical methods are limited. Although a variety of medical source-free unsupervised domain adaption (MSFUDA) methods have been proposed, we found they fall into an over-fitting dilemma called "longer training, worse performance." Therefore, we propose the Stable Learning (SL) strategy to address the dilemma. SL is a scalable method and can be integrated with other research, which consists of Weight Consolidation and Entropy Increase. First, we apply Weight Consolidation to retain domain-invariant knowledge and then we design Entropy Increase to avoid over-learning. Comparative experiments prove the effectiveness of SL. We also have done extensive ablation experiments. Besides, We will release codes including a variety of MSFUDA methods.
</details>
<details>
<summary>摘要</summary>
深度学习技术 для医疗影像分析通常受到源数据和目标数据之间的领域转移的影响。大多数现有的工作集中在无监督领域适应（UDA）。然而，在实际应用中，隐私问题更加严重。例如，医院数据之间存在领域转移，因为设备问题，两个领域的数据无法同时可用。这种挑战被称为源无法适应（Source-Free UDA），前一代的UDA医疗方法有限。虽然一些医疗源无法无监督领域适应（MSFUDA）方法已经被提出，但我们发现它们受到“长期训练，性能下降”的困惑。因此，我们提出稳定学习（SL）策略来解决这个困惑。SL是可扩展的方法，可以与其他研究集成，它包括权重团聚和Entropy增加。首先，我们将权重团聚应用于保留领域不变知识，然后我们设计Entropy增加以避免过度学习。对比性实验证明SL的效果。此外，我们还进行了广泛的减少实验。此外，我们将发布代码，包括多种MSFUDA方法。
</details></li>
</ul>
<hr>
<h2 id="Continuation-Path-Learning-for-Homotopy-Optimization"><a href="#Continuation-Path-Learning-for-Homotopy-Optimization" class="headerlink" title="Continuation Path Learning for Homotopy Optimization"></a>Continuation Path Learning for Homotopy Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12551">http://arxiv.org/abs/2307.12551</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xi-l/cpl">https://github.com/xi-l/cpl</a></li>
<li>paper_authors: Xi Lin, Zhiyuan Yang, Xiaoyuan Zhang, Qingfu Zhang</li>
<li>for: 解决复杂优化问题，提高homotopy优化的效果和可靠性。</li>
<li>methods: 提出了一种基于模型的方法，可以同时优化原始问题和所有优化子问题，并在实时生成任何中间解决方案。</li>
<li>results: 实验表明，提议的方法可以大幅提高homotopy优化的性能，并提供更多有用信息，以支持更好的决策。<details>
<summary>Abstract</summary>
Homotopy optimization is a traditional method to deal with a complicated optimization problem by solving a sequence of easy-to-hard surrogate subproblems. However, this method can be very sensitive to the continuation schedule design and might lead to a suboptimal solution to the original problem. In addition, the intermediate solutions, often ignored by classic homotopy optimization, could be useful for many real-world applications. In this work, we propose a novel model-based approach to learn the whole continuation path for homotopy optimization, which contains infinite intermediate solutions for any surrogate subproblems. Rather than the classic unidirectional easy-to-hard optimization, our method can simultaneously optimize the original problem and all surrogate subproblems in a collaborative manner. The proposed model also supports real-time generation of any intermediate solution, which could be desirable for many applications. Experimental studies on different problems show that our proposed method can significantly improve the performance of homotopy optimization and provide extra helpful information to support better decision-making.
</details>
<details>
<summary>摘要</summary>
In this work, we propose a novel model-based approach to learn the whole continuation path for homotopy optimization, which includes infinite intermediate solutions for any surrogate subproblems. Our method optimizes the original problem and all surrogate subproblems in a collaborative manner, rather than the classic unidirectional easy-to-hard optimization. The proposed model also supports real-time generation of any intermediate solution, which could be desirable for many applications.Experimental studies on different problems show that our proposed method can significantly improve the performance of homotopy optimization and provide extra helpful information to support better decision-making.
</details></li>
</ul>
<hr>
<h2 id="Knapsack-Connectedness-Path-and-Shortest-Path"><a href="#Knapsack-Connectedness-Path-and-Shortest-Path" class="headerlink" title="Knapsack: Connectedness, Path, and Shortest-Path"></a>Knapsack: Connectedness, Path, and Shortest-Path</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12547">http://arxiv.org/abs/2307.12547</a></li>
<li>repo_url: None</li>
<li>paper_authors: Palash Dey, Sudeshna Kolay, Sipra Singh</li>
<li>for: 这个论文研究了带有图理解的背包问题。 specifically, it aims to find a connected subset of items of maximum value that satisfies the knapsack constraint.</li>
<li>methods: 这个论文使用了图论的方法来解决这个问题。 specifically, it shows that the problem is strongly NP-complete even for graphs of maximum degree four and NP-complete even for star graphs.</li>
<li>results: 这个论文得到了一个时间复杂度为 $O\left(2^{tw\log tw}\cdot\text{poly}(\min{s^2,d^2})\right)$ 的算法，以及一个 $(1-\epsilon)$ 因数逼近算法时间复杂度为 $O\left(2^{tw\log tw}\cdot\text{poly}(n,1&#x2F;\epsilon)\right)$  для每个 $\epsilon&gt;0$。 Additionally, it shows that connected-knapsack is computationally hardest followed by path-knapsack and shortestpath-knapsack.<details>
<summary>Abstract</summary>
We study the knapsack problem with graph theoretic constraints. That is, we assume that there exists a graph structure on the set of items of knapsack and the solution also needs to satisfy certain graph theoretic properties on top of knapsack constraints. In particular, we need to compute in the connected knapsack problem a connected subset of items which has maximum value subject to the size of knapsack constraint. We show that this problem is strongly NP-complete even for graphs of maximum degree four and NP-complete even for star graphs. On the other hand, we develop an algorithm running in time $O\left(2^{tw\log tw}\cdot\text{poly}(\min\{s^2,d^2\})\right)$ where $tw,s,d$ are respectively treewidth of the graph, size, and target value of the knapsack. We further exhibit a $(1-\epsilon)$ factor approximation algorithm running in time $O\left(2^{tw\log tw}\cdot\text{poly}(n,1/\epsilon)\right)$ for every $\epsilon>0$. We show similar results for several other graph theoretic properties, namely path and shortest-path under the problem names path-knapsack and shortestpath-knapsack. Our results seems to indicate that connected-knapsack is computationally hardest followed by path-knapsack and shortestpath-knapsack.
</details>
<details>
<summary>摘要</summary>
我们研究了带有图 teoretic 约束的零件包问题。具体来说，我们假设存在一个图结构，其中每个零件都有一定的价值，并且解决方案还需要满足一定的图 theoretica 性质。在特定情况下，我们需要计算一个连接的零件集，其中每个零件的价值最大，并且满足零件包的大小约束。我们证明了这个问题是强NP完全的，甚至对于最大度为四的图和星型图。然而，我们开发了一个时间复杂度为 $O\left(2^{tw\log tw}\cdot\text{poly}(\min\{s^2,d^2\})\right)$ 的算法，其中 $tw,s,d$ 分别是图的树幂，零件包的大小，和目标值。此外，我们还提出了一个 $(1-\epsilon)$ 因子近似算法，其时间复杂度为 $O\left(2^{tw\log tw}\cdot\text{poly}(n,1/\epsilon)\right)$，其中 $\epsilon>0$。我们还证明了对于一些其他的图 teoretic 性质，例如路径和最短路，我们可以通过对应的问题名称来描述它们，例如路径-零件包和最短路-零件包。我们的结果表明，连接-零件包是计算最为困难的，然后是路径-零件包和最短路-零件包。
</details></li>
</ul>
<hr>
<h2 id="Towards-Video-Anomaly-Retrieval-from-Video-Anomaly-Detection-New-Benchmarks-and-Model"><a href="#Towards-Video-Anomaly-Retrieval-from-Video-Anomaly-Detection-New-Benchmarks-and-Model" class="headerlink" title="Towards Video Anomaly Retrieval from Video Anomaly Detection: New Benchmarks and Model"></a>Towards Video Anomaly Retrieval from Video Anomaly Detection: New Benchmarks and Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12545">http://arxiv.org/abs/2307.12545</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peng Wu, Jing Liu, Xiangteng He, Yuxin Peng, Peng Wang, Yanning Zhang</li>
<li>for: 这个研究的目的是为了提出一个新的错误检测任务，即错误视频搜寻（VAR），这个任务的目的是实用地搜寻错误的视频，并且使用语言描述和同步的声音进行跨Modalities的搜寻。</li>
<li>methods: 这个研究使用了一个名为“错误引导排序”的类别学习模型，它使用了一个新的错误排序方法来将错误的部分对应到视频中的特定时间点。此外，这个模型还使用了一个内建的类别排序方法来将视频中的内容与语言描述进行对应。</li>
<li>results: 实验结果显示，这个新的错误检测任务（VAR）是一个非常有挑战性的任务，并且证明了这个任务的重要性。此外，实验结果还显示了这个模型的优秀性，它可以实现高度的准确率和高效率。<details>
<summary>Abstract</summary>
Video anomaly detection (VAD) has been paid increasing attention due to its potential applications, its current dominant tasks focus on online detecting anomalies% at the frame level, which can be roughly interpreted as the binary or multiple event classification. However, such a setup that builds relationships between complicated anomalous events and single labels, e.g., ``vandalism'', is superficial, since single labels are deficient to characterize anomalous events. In reality, users tend to search a specific video rather than a series of approximate videos. Therefore, retrieving anomalous events using detailed descriptions is practical and positive but few researches focus on this. In this context, we propose a novel task called Video Anomaly Retrieval (VAR), which aims to pragmatically retrieve relevant anomalous videos by cross-modalities, e.g., language descriptions and synchronous audios. Unlike the current video retrieval where videos are assumed to be temporally well-trimmed with short duration, VAR is devised to retrieve long untrimmed videos which may be partially relevant to the given query. To achieve this, we present two large-scale VAR benchmarks, UCFCrime-AR and XDViolence-AR, constructed on top of prevalent anomaly datasets. Meanwhile, we design a model called Anomaly-Led Alignment Network (ALAN) for VAR. In ALAN, we propose an anomaly-led sampling to focus on key segments in long untrimmed videos. Then, we introduce an efficient pretext task to enhance semantic associations between video-text fine-grained representations. Besides, we leverage two complementary alignments to further match cross-modal contents. Experimental results on two benchmarks reveal the challenges of VAR task and also demonstrate the advantages of our tailored method.
</details>
<details>
<summary>摘要</summary>
视频异常检测（VAD）在过去几年中受到了越来越多的关注，因为它在实际应用中具有潜在的价值。目前主流的VAD任务都是在帧级别上进行异常检测，可以简单地 interpreted为多个事件的二分类或多类别分类。但是，这种设置不仅缺乏异常事件之间的关系，而且单个标签（如“违法行为”）无法描述异常事件的复杂性。在实际应用中，用户通常会搜索特定的视频而不是一系列相似的视频。因此，使用详细的描述来检测异常事件是有用的。在这个上下文中，我们提出了一个新的任务：视频异常检索（VAR），该任务的目标是在多modalities（如语言描述和同步声音）之间实用地检索相关的异常视频。不同于现有的视频检索，在VAR中视频不再被假设为短暂和有效的，而是可以是长不trimmed的，这些视频可能只有部分相关于给定查询。为了实现这一目标，我们提出了两个大规模的VAR benchmark，UCFCrime-AR和XDViolence-AR，它们基于常见的异常数据集。同时，我们设计了一种名为异常领导Alignment网络（ALAN）的模型，用于VAR。在ALAN中，我们提出了一种异常领导采样方法，以吸引关注关键部分在长不trimmed视频中。然后，我们引入了一种有效的假任务，以增强视频-文本细致的semantic关系。此外，我们利用了两种补偿匹配来进一步匹配cross-modal内容。实验结果表明，VAR任务具有挑战性，同时我们的定制方法也得到了优势。
</details></li>
</ul>
<hr>
<h2 id="Client-Level-Differential-Privacy-via-Adaptive-Intermediary-in-Federated-Medical-Imaging"><a href="#Client-Level-Differential-Privacy-via-Adaptive-Intermediary-in-Federated-Medical-Imaging" class="headerlink" title="Client-Level Differential Privacy via Adaptive Intermediary in Federated Medical Imaging"></a>Client-Level Differential Privacy via Adaptive Intermediary in Federated Medical Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12542">http://arxiv.org/abs/2307.12542</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/med-air/client-dp-fl">https://github.com/med-air/client-dp-fl</a></li>
<li>paper_authors: Meirui Jiang, Yuan Zhong, Anjie Le, Xiaoxiao Li, Qi Dou</li>
<li>for: This paper aims to optimize the trade-off between privacy protection and performance in federated learning (FL) for medical imaging under the context of client-level differential privacy (DP).</li>
<li>methods: The proposed approach is based on an adaptive intermediary strategy that splits clients into sub-clients, which serve as intermediaries between hospitals and the server to mitigate the noises introduced by DP without harming privacy.</li>
<li>results: The proposed approach is empirically evaluated on both classification and segmentation tasks using two public datasets, and its effectiveness is demonstrated with significant performance improvements and comprehensive analytical studies.Here’s the simplified Chinese version:</li>
<li>for: 这篇论文的目标是在医疗镜像 Federated Learning（FL）中优化 differential privacy（DP）的质量和性能的负担平衡。</li>
<li>methods: 提议的方法是基于 adaptive intermediary strategy，将客户端分成子客户端，这些子客户端将作为客户端和服务器之间的中间人，以减少 DP 引入的噪声而不危害隐私。</li>
<li>results: 提议的方法被 Empirical 评估在 two 个公共数据集上，并通过 comprehensive analytical studies 证明其效果。<details>
<summary>Abstract</summary>
Despite recent progress in enhancing the privacy of federated learning (FL) via differential privacy (DP), the trade-off of DP between privacy protection and performance is still underexplored for real-world medical scenario. In this paper, we propose to optimize the trade-off under the context of client-level DP, which focuses on privacy during communications. However, FL for medical imaging involves typically much fewer participants (hospitals) than other domains (e.g., mobile devices), thus ensuring clients be differentially private is much more challenging. To tackle this problem, we propose an adaptive intermediary strategy to improve performance without harming privacy. Specifically, we theoretically find splitting clients into sub-clients, which serve as intermediaries between hospitals and the server, can mitigate the noises introduced by DP without harming privacy. Our proposed approach is empirically evaluated on both classification and segmentation tasks using two public datasets, and its effectiveness is demonstrated with significant performance improvements and comprehensive analytical studies. Code is available at: https://github.com/med-air/Client-DP-FL.
</details>
<details>
<summary>摘要</summary>
尽管最近的进展已经提高了联合学习（FL）的隐私保护（DP），但是实际世界医疗场景中DP与性能之间的负担仍未得到充分探讨。在这篇论文中，我们提议优化DP与隐私保护之间的负担，在客户端DP上进行优化。然而，医疗影像FL通常有比其他领域（如移动设备）更少的参与者（医院），因此保持客户端的隐私是更加挑战性的。为解决这个问题，我们提议使用可适应担保策略，以提高性能而不危害隐私。具体来说，我们通过将客户端分为子客户端，使其作为医院和服务器之间的中间人，可以减少DP引入的噪声而不危害隐私。我们的提议方法在两个公共数据集上进行了实际评估，并通过了重要性能改进和完整的分析研究。代码可以在以下链接中找到：https://github.com/med-air/Client-DP-FL。
</details></li>
</ul>
<hr>
<h2 id="SelFormaly-Towards-Task-Agnostic-Unified-Anomaly-Detection"><a href="#SelFormaly-Towards-Task-Agnostic-Unified-Anomaly-Detection" class="headerlink" title="SelFormaly: Towards Task-Agnostic Unified Anomaly Detection"></a>SelFormaly: Towards Task-Agnostic Unified Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12540">http://arxiv.org/abs/2307.12540</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yujin Lee, Harin Lim, Hyunsoo Yoon</li>
<li>for: 这篇论文旨在提出一个通用且强大的问题检测框架，以扩展previous任务特定的问题检测方法。</li>
<li>methods: 这篇论文使用了自我supervised ViTs，以及back-patch masking和top k-ratio feature matching等技术来实现通用的问题检测。</li>
<li>results: 这篇论文在不同的数据集上都 achieved state-of-the-art 的结果，并且适用于多种任务，包括问题检测、Semantic anomaly detection、多类问题检测和问题聚合。<details>
<summary>Abstract</summary>
The core idea of visual anomaly detection is to learn the normality from normal images, but previous works have been developed specifically for certain tasks, leading to fragmentation among various tasks: defect detection, semantic anomaly detection, multi-class anomaly detection, and anomaly clustering. This one-task-one-model approach is resource-intensive and incurs high maintenance costs as the number of tasks increases. This paper presents SelFormaly, a universal and powerful anomaly detection framework. We emphasize the necessity of our off-the-shelf approach by pointing out a suboptimal issue with fluctuating performance in previous online encoder-based methods. In addition, we question the effectiveness of using ConvNets as previously employed in the literature and confirm that self-supervised ViTs are suitable for unified anomaly detection. We introduce back-patch masking and discover the new role of top k-ratio feature matching to achieve unified and powerful anomaly detection. Back-patch masking eliminates irrelevant regions that possibly hinder target-centric detection with representations of the scene layout. The top k-ratio feature matching unifies various anomaly levels and tasks. Finally, SelFormaly achieves state-of-the-art results across various datasets for all the aforementioned tasks.
</details>
<details>
<summary>摘要</summary>
核心思想是可视异常检测是学习正常图像的 normality，但之前的工作是为特定任务而开发，导致不同任务之间的分化。这种一个任务一个模型的方法是资源占用和维护成本高。这篇论文介绍了 SelFormaly，一个通用和强大的异常检测框架。我们强调我们的卖外方法的必要性，指出了在线编码器基于方法中的性能波动问题。此外，我们质疑了使用 ConvNets 以前在文献中使用的效果，并证明了不同类异常检测和泛化异常检测可以使用自适应 ViTs。我们引入后贴布覆盖和发现了新的顶部 k-比例特征匹配，以实现统一和强大的异常检测。后贴布覆盖 eliminates 不相关的区域，可能干扰目标中心检测表示场景布局。顶部 k-比例特征匹配 统一了不同异常水平和任务。最后，SelFormaly 在不同数据集上实现了所有上述任务的州OF-the-art 结果。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Medical-Report-Generation-Disease-Revealing-Enhancement-with-Knowledge-Graph"><a href="#Rethinking-Medical-Report-Generation-Disease-Revealing-Enhancement-with-Knowledge-Graph" class="headerlink" title="Rethinking Medical Report Generation: Disease Revealing Enhancement with Knowledge Graph"></a>Rethinking Medical Report Generation: Disease Revealing Enhancement with Knowledge Graph</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12526">http://arxiv.org/abs/2307.12526</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wangyixinxin/mrg-kg">https://github.com/wangyixinxin/mrg-kg</a></li>
<li>paper_authors: Yixin Wang, Zihao Lin, Haoyu Dong</li>
<li>for: 这个研究的目的是提高医疗报告生成（MRG）过程中的知识图（KG）的完整性和应用。</li>
<li>methods: 这个研究使用了一个完整的知识图，包括137种疾病和异常性，以帮助指导MRG过程。此外，研究还引入了一种新的增强策略，以便增强疾病类型在分布的表现。</li>
<li>results: 研究发现，使用提案的两个阶段生成方法和增强策略，可以显著提高生成报告中的疾病匹配度和多样性。这表明，这种方法可以有效地减少对疾病分布的长尾问题。<details>
<summary>Abstract</summary>
Knowledge Graph (KG) plays a crucial role in Medical Report Generation (MRG) because it reveals the relations among diseases and thus can be utilized to guide the generation process. However, constructing a comprehensive KG is labor-intensive and its applications on the MRG process are under-explored. In this study, we establish a complete KG on chest X-ray imaging that includes 137 types of diseases and abnormalities. Based on this KG, we find that the current MRG data sets exhibit a long-tailed problem in disease distribution. To mitigate this problem, we introduce a novel augmentation strategy that enhances the representation of disease types in the tail-end of the distribution. We further design a two-stage MRG approach, where a classifier is first trained to detect whether the input images exhibit any abnormalities. The classified images are then independently fed into two transformer-based generators, namely, ``disease-specific generator" and ``disease-free generator" to generate the corresponding reports. To enhance the clinical evaluation of whether the generated reports correctly describe the diseases appearing in the input image, we propose diverse sensitivity (DS), a new metric that checks whether generated diseases match ground truth and measures the diversity of all generated diseases. Results show that the proposed two-stage generation framework and augmentation strategies improve DS by a considerable margin, indicating a notable reduction in the long-tailed problem associated with under-represented diseases.
</details>
<details>
<summary>摘要</summary>
医学报告生成（MRG）中知识图грам（KG）发挥关键作用，因为它揭示疾病之间的关系，可以用于导航生成过程。然而，建立完整的KG是劳动密集的，其在MRG过程中的应用还尚未得到充分探索。在这种研究中，我们建立了包括137种疾病和异常的完整KG，基于这个KG，我们发现现有的MRG数据集表现出长尾问题。为了解决这个问题，我们提出了一种新的扩充策略，该策略可以增强疾病类型在分布的尾部的表达。我们还设计了一种两阶段的MRG方法，其中一个是使用一个分类器来检测输入图像是否存在任何异常。经过分类后，输入图像被独立地传递给两个基于转换器的生成器，即“疾病特定生成器”和“疾病无效生成器”，以生成相应的报告。为了提高生成的严肃评估，我们提出了多样敏感度（DS），一种新的度量，该度量检查生成的疾病与真实的疾病是否匹配，并且度量所有生成的疾病的多样性。结果表明，我们的两阶段生成框架和扩充策略可以大幅提高DS， indicating a remarkable reduction in the long-tailed problem associated with under-represented diseases.
</details></li>
</ul>
<hr>
<h2 id="FaFCNN-A-General-Disease-Classification-Framework-Based-on-Feature-Fusion-Neural-Networks"><a href="#FaFCNN-A-General-Disease-Classification-Framework-Based-on-Feature-Fusion-Neural-Networks" class="headerlink" title="FaFCNN: A General Disease Classification Framework Based on Feature Fusion Neural Networks"></a>FaFCNN: A General Disease Classification Framework Based on Feature Fusion Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12518">http://arxiv.org/abs/2307.12518</a></li>
<li>repo_url: None</li>
<li>paper_authors: Menglin Kong, Shaojie Zhao, Juan Cheng, Xingquan Li, Ri Su, Muzhou Hou, Cong Cao</li>
<li>for: 本研究旨在解决应用深度学习&#x2F;机器学习方法于疾病分类任务中的两个基本问题，即训练样本数量和质量的不足，以及如何有效地融合多源特征并训练稳定的分类模型。</li>
<li>methods: 我们提出了一种基于人类学习知识的Feature-aware Fusion Correlation Neural Network (FaFCNN)框架，包括特征意识互动模块和域对抗学习基于特征对齐模块。</li>
<li>results: 实验结果表明，通过使用预训练梯度提升树的扩充特征，FaFCNN在低质量 dataset 上实现了更高的性能提升，并且对于竞争对手基线方法进行了一致性优化。此外，广泛的实验还证明了提案的方法的稳定性和模型中每个组件的有效性。<details>
<summary>Abstract</summary>
There are two fundamental problems in applying deep learning/machine learning methods to disease classification tasks, one is the insufficient number and poor quality of training samples; another one is how to effectively fuse multiple source features and thus train robust classification models. To address these problems, inspired by the process of human learning knowledge, we propose the Feature-aware Fusion Correlation Neural Network (FaFCNN), which introduces a feature-aware interaction module and a feature alignment module based on domain adversarial learning. This is a general framework for disease classification, and FaFCNN improves the way existing methods obtain sample correlation features. The experimental results show that training using augmented features obtained by pre-training gradient boosting decision tree yields more performance gains than random-forest based methods. On the low-quality dataset with a large amount of missing data in our setup, FaFCNN obtains a consistently optimal performance compared to competitive baselines. In addition, extensive experiments demonstrate the robustness of the proposed method and the effectiveness of each component of the model\footnote{Accepted in IEEE SMC2023}.
</details>
<details>
<summary>摘要</summary>
“there are two fundamental problems in applying deep learning/machine learning methods to disease classification tasks, one is the insufficient number and poor quality of training samples; another one is how to effectively fuse multiple source features and thus train robust classification models. To address these problems, inspired by the process of human learning knowledge, we propose the Feature-aware Fusion Correlation Neural Network (FaFCNN), which introduces a feature-aware interaction module and a feature alignment module based on domain adversarial learning. This is a general framework for disease classification, and FaFCNN improves the way existing methods obtain sample correlation features. The experimental results show that training using augmented features obtained by pre-training gradient boosting decision tree yields more performance gains than random-forest based methods. On the low-quality dataset with a large amount of missing data in our setup, FaFCNN obtains a consistently optimal performance compared to competitive baselines. In addition, extensive experiments demonstrate the robustness of the proposed method and the effectiveness of each component of the model”。Here is the breakdown of the translation:* “two fundamental problems”(二大问题) - This phrase is translated as "two fundamental problems" to emphasize the importance of the issues being discussed.* “insufficient number and poor quality of training samples”(训练样本数量和质量不足) - This phrase is translated as "insufficient number and poor quality of training samples" to accurately convey the idea that there are not enough high-quality training samples available for training deep learning models.* “how to effectively fuse multiple source features”(如何有效地融合多源特征) - This phrase is translated as "how to effectively fuse multiple source features" to emphasize the importance of combining multiple sources of data to improve the accuracy of deep learning models.* “and thus train robust classification models”(并因此训练Robust分类模型) - This phrase is translated as "and thus train robust classification models" to emphasize the goal of training deep learning models that are robust and accurate.* “Feature-aware Fusion Correlation Neural Network”(特征意识融合相关神经网络) - This phrase is translated as "Feature-aware Fusion Correlation Neural Network" to accurately convey the name of the proposed method and its key features.* “based on domain adversarial learning”(基于域对抗学习) - This phrase is translated as "based on domain adversarial learning" to emphasize the key technique used in the proposed method.* “This is a general framework for disease classification”(这是一种普遍的疾病分类框架) - This phrase is translated as "This is a general framework for disease classification" to emphasize that the proposed method is a generalizable framework that can be applied to a wide range of disease classification tasks.* “and FaFCNN improves the way existing methods obtain sample correlation features”(而FaFCNN改进了现有方法获取样本相关特征) - This phrase is translated as "and FaFCNN improves the way existing methods obtain sample correlation features" to emphasize the key advantage of the proposed method.* “The experimental results show that training using augmented features obtained by pre-training gradient boosting decision tree yields more performance gains than random-forest based methods”(实验结果表明，使用由前期逻辑树决策树提取的增强特征进行训练，比Random Forest基于方法更多地提高性能) - This phrase is translated as "The experimental results show that training using augmented features obtained by pre-training gradient boosting decision tree yields more performance gains than random-forest based methods" to accurately convey the key findings of the experiments.* “On the low-quality dataset with a large amount of missing data in our setup, FaFCNN obtains a consistently optimal performance compared to competitive baselines”(在我们的设置下，具有大量缺失数据的低质量数据集上，FaFCNN对于竞争对手基线表现出一致优化的性能) - This phrase is translated as "On the low-quality dataset with a large amount of missing data in our setup, FaFCNN obtains a consistently optimal performance compared to competitive baselines" to emphasize the key finding that the proposed method performs well even on low-quality datasets with a large amount of missing data.* “In addition, extensive experiments demonstrate the robustness of the proposed method and the effectiveness of each component of the model”(此外，广泛的实验还证明了提议方法的稳定性和模型每个组件的有效性) - This phrase is translated as "In addition, extensive experiments demonstrate the robustness of the proposed method and the effectiveness of each component of the model" to emphasize the key finding that the proposed method is robust and effective, and to highlight the importance of each component of the model.
</details></li>
</ul>
<hr>
<h2 id="Gradient-Based-Word-Substitution-for-Obstinate-Adversarial-Examples-Generation-in-Language-Models"><a href="#Gradient-Based-Word-Substitution-for-Obstinate-Adversarial-Examples-Generation-in-Language-Models" class="headerlink" title="Gradient-Based Word Substitution for Obstinate Adversarial Examples Generation in Language Models"></a>Gradient-Based Word Substitution for Obstinate Adversarial Examples Generation in Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12507">http://arxiv.org/abs/2307.12507</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yimu Wang, Peng Shi, Hongyang Zhang</li>
<li>for:  This paper aims to address the problem of generating obstinate adversarial examples in NLP by introducing a novel word substitution method named GradObstinate, which automatically generates obstinate adversarial examples without any constraints on the search space or the need for manual design principles.</li>
<li>methods:  The proposed GradObstinate method uses a gradient-based approach to automatically generate obstinate adversarial examples. It does not rely on any manual design principles or constraints on the search space, making it more practical and applicable in real-world scenarios.</li>
<li>results:  The proposed GradObstinate method is evaluated on five representative NLP models and four benchmarks, and the results show that it generates more powerful obstinate adversarial examples with a higher attack success rate compared to antonym-based methods. Additionally, the obstinate substitutions found by GradObstinate are transferable to other models in black-box settings, including even GPT-3 and ChatGPT.Here is the answer in Simplified Chinese:</li>
<li>for: 这篇论文目的是解决NLP中的阻挡(过稳)攻击例的生成问题，提出了一种名为GradObstinate的新的词替换方法，可以自动生成阻挡攻击例而不需要人工设计原则或搜索空间的约束。</li>
<li>methods:  GradObstinate方法使用梯度基础来自动生成阻挡攻击例，不需要人工设计原则或搜索空间的约束，使其在实际应用中更加实际。</li>
<li>results:  GradObstinate方法在五种代表性的NLP模型和四个benchmark上进行了广泛的实验，结果显示，它可以生成更加强大的阻挡攻击例，攻击成功率高于antonym-based方法。此外，GradObstinate发现的阻挡替换还可以在黑盒设置下转移到其他模型中，包括GPT-3和ChatGPT。<details>
<summary>Abstract</summary>
In this paper, we study the problem of generating obstinate (over-stability) adversarial examples by word substitution in NLP, where input text is meaningfully changed but the model's prediction does not, even though it should. Previous word substitution approaches have predominantly focused on manually designed antonym-based strategies for generating obstinate adversarial examples, which hinders its application as these strategies can only find a subset of obstinate adversarial examples and require human efforts. To address this issue, in this paper, we introduce a novel word substitution method named GradObstinate, a gradient-based approach that automatically generates obstinate adversarial examples without any constraints on the search space or the need for manual design principles. To empirically evaluate the efficacy of GradObstinate, we conduct comprehensive experiments on five representative models (Electra, ALBERT, Roberta, DistillBERT, and CLIP) finetuned on four NLP benchmarks (SST-2, MRPC, SNLI, and SQuAD) and a language-grounding benchmark (MSCOCO). Extensive experiments show that our proposed GradObstinate generates more powerful obstinate adversarial examples, exhibiting a higher attack success rate compared to antonym-based methods. Furthermore, to show the transferability of obstinate word substitutions found by GradObstinate, we replace the words in four representative NLP benchmarks with their obstinate substitutions. Notably, obstinate substitutions exhibit a high success rate when transferred to other models in black-box settings, including even GPT-3 and ChatGPT. Examples of obstinate adversarial examples found by GradObstinate are available at https://huggingface.co/spaces/anonauthors/SecretLanguage.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了对话语言处理（NLP）领域中生成顽固（过度稳定）攻击示例的问题，通过单词替换而生成这些示例。现有的单词替换方法主要采用人工设计的反义策略来生成顽固攻击示例，这限制了其应用，因为这些策略只能找到一部分顽固攻击示例，并且需要人工劳动。为解决这问题，在这篇论文中，我们提出了一种新的单词替换方法，即GradObstinate，它是基于梯度的方法，可以自动生成顽固攻击示例，不需要任何限制或人工设计原则。为证明GradObstinate的有效性，我们在五种代表性模型（Electra、ALBERT、Roberta、DistillBERT和CLIP）上进行了广泛的实验，这些模型在四个NLPBenchmark（SST-2、MRPC、SNLI和SQuAD）和一个语言固定 benchmark（MSCOCO）上进行了finetuning。实验结果表明，我们提出的GradObstinate可以更好地生成顽固攻击示例，对于反义策略来说，攻击成功率更高。此外，我们还证明了GradObstinate生成的顽固替换示例在黑盒Setting中的传送性，包括GPT-3和ChatGPT等模型。详细的顽固攻击示例可以在https://huggingface.co/spaces/anonauthors/SecretLanguage上找到。
</details></li>
</ul>
<hr>
<h2 id="TF-ICON-Diffusion-Based-Training-Free-Cross-Domain-Image-Composition"><a href="#TF-ICON-Diffusion-Based-Training-Free-Cross-Domain-Image-Composition" class="headerlink" title="TF-ICON: Diffusion-Based Training-Free Cross-Domain Image Composition"></a>TF-ICON: Diffusion-Based Training-Free Cross-Domain Image Composition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12493">http://arxiv.org/abs/2307.12493</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Shilin-LU/TF-ICON">https://github.com/Shilin-LU/TF-ICON</a></li>
<li>paper_authors: Shilin Lu, Yanzhu Liu, Adams Wai-Kin Kong</li>
<li>for: 这个论文旨在提出一个无需训练的图像调和框架，将文本驱动的填充模型应用于跨领域图像导向作业。</li>
<li>methods: 这个框架使用的方法是使用文本驱动的填充模型，不需要进一步的训练、调整或优化。</li>
<li>results: 实验结果显示，将Stable Diffusion与特别提示（Exceptional Prompt）搭配可以超越现有的对应方法，而TF-ICON在多个视觉领域中也表现出优越的可 versatility。<details>
<summary>Abstract</summary>
Text-driven diffusion models have exhibited impressive generative capabilities, enabling various image editing tasks. In this paper, we propose TF-ICON, a novel Training-Free Image COmpositioN framework that harnesses the power of text-driven diffusion models for cross-domain image-guided composition. This task aims to seamlessly integrate user-provided objects into a specific visual context. Current diffusion-based methods often involve costly instance-based optimization or finetuning of pretrained models on customized datasets, which can potentially undermine their rich prior. In contrast, TF-ICON can leverage off-the-shelf diffusion models to perform cross-domain image-guided composition without requiring additional training, finetuning, or optimization. Moreover, we introduce the exceptional prompt, which contains no information, to facilitate text-driven diffusion models in accurately inverting real images into latent representations, forming the basis for compositing. Our experiments show that equipping Stable Diffusion with the exceptional prompt outperforms state-of-the-art inversion methods on various datasets (CelebA-HQ, COCO, and ImageNet), and that TF-ICON surpasses prior baselines in versatile visual domains. Code is available at https://github.com/Shilin-LU/TF-ICON
</details>
<details>
<summary>摘要</summary>
文本驱动的扩散模型已经展示出了吸引人的生成能力，可以完成多种图像编辑任务。在这篇论文中，我们提出了TF-ICON，一种新的培成freeImage COmpositioN框架，利用文本驱动扩散模型进行交域图像引导组合。这个任务的目标是将用户提供的对象顺利地 интеGRATE到特定的视觉上下文中。当前的扩散基于方法通常需要费时的实例基于优化或特定数据集上的Finetuning pretrained模型，这可能会损害它们的丰富先天知识。相比之下，TF-ICON可以不需要额外的培成或优化，通过使用off-the-shelf扩散模型来完成交域图像引导组合。此外，我们引入了Exceptional prompt，它不含任何信息，以便文本驱动扩散模型可以准确地将真实图像转化为幂等表示，这为组合提供了基础。我们的实验表明，在不同的图像 datasets（CelebA-HQ、COCO和ImageNet）上，使用 Exceptional prompt 的 Stable Diffusion 方法可以超越现有的抽象方法，而TF-ICON 也可以在多种视觉领域中超越先前的基eline。代码可以在https://github.com/Shilin-LU/TF-ICON 中找到。
</details></li>
</ul>
<hr>
<h2 id="ChatGPT-for-Software-Security-Exploring-the-Strengths-and-Limitations-of-ChatGPT-in-the-Security-Applications"><a href="#ChatGPT-for-Software-Security-Exploring-the-Strengths-and-Limitations-of-ChatGPT-in-the-Security-Applications" class="headerlink" title="ChatGPT for Software Security: Exploring the Strengths and Limitations of ChatGPT in the Security Applications"></a>ChatGPT for Software Security: Exploring the Strengths and Limitations of ChatGPT in the Security Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12488">http://arxiv.org/abs/2307.12488</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhilong Wang, Lan Zhang, Peng Liu</li>
<li>for: The paper aims to evaluate ChatGPT’s capabilities in security-oriented program analysis, specifically from the perspectives of both attackers and security analysts.</li>
<li>methods: The paper uses a case study approach, presenting several security-oriented program analysis tasks and deliberately introducing challenges to assess ChatGPT’s responses.</li>
<li>results: The paper examines the quality of answers provided by ChatGPT and gains a clearer understanding of its strengths and limitations in the realm of security-oriented program analysis.Here’s the same information in Simplified Chinese text:</li>
<li>for: 本文旨在评估ChatGPT在安全关注程序分析方面的能力，具体来说是从攻击者和安全分析员两个角度出发。</li>
<li>methods: 本文采用 caso study方法，通过提出多个安全关注程序分析任务，故意引入挑战来评估ChatGPT的回答质量。</li>
<li>results: 本文通过分析ChatGPT的回答来了解它在安全关注程序分析方面的优劣点。<details>
<summary>Abstract</summary>
ChatGPT, as a versatile large language model, has demonstrated remarkable potential in addressing inquiries across various domains. Its ability to analyze, comprehend, and synthesize information from both online sources and user inputs has garnered significant attention. Previous research has explored ChatGPT's competence in code generation and code reviews. In this paper, we delve into ChatGPT's capabilities in security-oriented program analysis, focusing on perspectives from both attackers and security analysts. We present a case study involving several security-oriented program analysis tasks while deliberately introducing challenges to assess ChatGPT's responses. Through an examination of the quality of answers provided by ChatGPT, we gain a clearer understanding of its strengths and limitations in the realm of security-oriented program analysis.
</details>
<details>
<summary>摘要</summary>
chatgpt 作为一种多能语言模型，在各个领域的问题上表现出了惊人的潜力。它可以分析、理解和合成来自线上源和用户输入的信息，吸引了广泛的关注。以前的研究探讨了 chatgpt 在代码生成和代码审查方面的能力。在这篇论文中，我们探究 chatgpt 在安全关注程序分析方面的能力，具体来说是从攻击者和安全分析员的视角来评估 chatgpt 的回答质量。我们通过对多个安全关注程序分析任务的挑战性评估，了解 chatgpt 在安全关注程序分析领域的优势和局限性。
</details></li>
</ul>
<hr>
<h2 id="ProtoFL-Unsupervised-Federated-Learning-via-Prototypical-Distillation"><a href="#ProtoFL-Unsupervised-Federated-Learning-via-Prototypical-Distillation" class="headerlink" title="ProtoFL: Unsupervised Federated Learning via Prototypical Distillation"></a>ProtoFL: Unsupervised Federated Learning via Prototypical Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12450">http://arxiv.org/abs/2307.12450</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hansol Kim, Youngjun Kwak, Minyoung Jung, Jinho Shin, Youngsung Kim, Changick Kim</li>
<li>for: 提高数据隐私保护和一类分类性能</li>
<li>methods: 提出了基于原型 repreentation 的 federated learning 方法，以增强全球模型的表示能力，降低通信成本</li>
<li>results: 对五种广泛使用的基准数据集进行了广泛的实验，证明了提议的框架在先前的方法中表现出色<details>
<summary>Abstract</summary>
Federated learning (FL) is a promising approach for enhancing data privacy preservation, particularly for authentication systems. However, limited round communications, scarce representation, and scalability pose significant challenges to its deployment, hindering its full potential. In this paper, we propose 'ProtoFL', Prototypical Representation Distillation based unsupervised Federated Learning to enhance the representation power of a global model and reduce round communication costs. Additionally, we introduce a local one-class classifier based on normalizing flows to improve performance with limited data. Our study represents the first investigation of using FL to improve one-class classification performance. We conduct extensive experiments on five widely used benchmarks, namely MNIST, CIFAR-10, CIFAR-100, ImageNet-30, and Keystroke-Dynamics, to demonstrate the superior performance of our proposed framework over previous methods in the literature.
</details>
<details>
<summary>摘要</summary>
《联合学习（FL）是一种有前途的方法，能够提高数据隐私保护，特别是 для 身份验证系统。然而，有限的回合通信，珍贵的表示和扩展性带来了其部署的挑战，使其全部潜力受限。在本文中，我们提出了“ProtoFL”，基于 прототипиаль表示储存的无监督联合学习，以提高全球模型的表示力和减少回合通信成本。此外，我们还引入了基于 нормализа函数的本地一类分类器，以提高有限数据下的性能。我们的研究是首次利用FL提高一类分类性能的研究。我们在五种广泛使用的标准数据集上进行了广泛的实验，包括MNIST、CIFAR-10、CIFAR-100、ImageNet-30和键盘动作，以示出我们提posed的框架在过去的方法中的超越性。》Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese. If you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="SCRAPS-Speech-Contrastive-Representations-of-Acoustic-and-Phonetic-Spaces"><a href="#SCRAPS-Speech-Contrastive-Representations-of-Acoustic-and-Phonetic-Spaces" class="headerlink" title="SCRAPS: Speech Contrastive Representations of Acoustic and Phonetic Spaces"></a>SCRAPS: Speech Contrastive Representations of Acoustic and Phonetic Spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12445">http://arxiv.org/abs/2307.12445</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ivan Vallés-Pérez, Grzegorz Beringer, Piotr Bilinski, Gary Cook, Roberto Barra-Chicote</li>
<li>for: 本研究旨在将CLIP模型应用到语音领域，以学习共同的phonetic和acoustic空间表示。</li>
<li>methods: 本研究使用CLIP模型，通过对语音资料进行对映和调整，以学习共同的phonetic和acoustic空间表示。</li>
<li>results: 研究获得的结果显示，提案的模型具有辨识phonetic变化的能力，并且具有对不同类型噪音的抗响性。此外，研究还证明了模型的对下游应用的有用性，如语音识别和生成等。<details>
<summary>Abstract</summary>
Numerous examples in the literature proved that deep learning models have the ability to work well with multimodal data. Recently, CLIP has enabled deep learning systems to learn shared latent spaces between images and text descriptions, with outstanding zero- or few-shot results in downstream tasks. In this paper we explore the same idea proposed by CLIP but applied to the speech domain, where the phonetic and acoustic spaces usually coexist. We train a CLIP-based model with the aim to learn shared representations of phonetic and acoustic spaces. The results show that the proposed model is sensible to phonetic changes, with a 91% of score drops when replacing 20% of the phonemes at random, while providing substantial robustness against different kinds of noise, with a 10% performance drop when mixing the audio with 75% of Gaussian noise. We also provide empirical evidence showing that the resulting embeddings are useful for a variety of downstream applications, such as intelligibility evaluation and the ability to leverage rich pre-trained phonetic embeddings in speech generation task. Finally, we discuss potential applications with interesting implications for the speech generation and recognition fields.
</details>
<details>
<summary>摘要</summary>
多种例子在文献中证明深度学习模型可以好地处理多Modal数据。近期，CLIP使得深度学习系统可以学习图像和文本描述之间的共享幂等空间，显示出 Zero-或几个Shot结果在下游任务中。在这篇论文中，我们探索了与CLIP相同的想法，但是应用到语音频域中， где音频和语音空间通常共存。我们使用CLIP基于模型，以学习共享phonetic和Acoustic空间的表示。结果显示，我们的提posed模型对phonetic变化敏感，在Random中替换20%的Phoneemes时，得分下降91%，同时在混合音频75%的加aussian噪音时，表现下降10%。我们还提供了实验证明，表明得到的嵌入是下游应用中有用，如智能评估和Speech生成任务中的Rich预训练phonetic嵌入。最后，我们讨论了可能的应用，具有Speech生成和识别领域的 interessante implikationen。
</details></li>
</ul>
<hr>
<h2 id="AMaizeD-An-End-to-End-Pipeline-for-Automatic-Maize-Disease-Detection"><a href="#AMaizeD-An-End-to-End-Pipeline-for-Automatic-Maize-Disease-Detection" class="headerlink" title="AMaizeD: An End to End Pipeline for Automatic Maize Disease Detection"></a>AMaizeD: An End to End Pipeline for Automatic Maize Disease Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03766">http://arxiv.org/abs/2308.03766</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anish Mall, Sanchit Kabra, Ankur Lhila, Pawan Ajmera</li>
<li>for:  automatización de la detección de enfermedades en maíz utilizando imágenes multiespectrales obtenidas desde drones.</li>
<li>methods: combina redes neuronales convolucionales (CNNs) como extractores de características y técnicas de segmentación para identificar las plantas de maíz y sus enfermedades asociadas.</li>
<li>results: detecta una variedad de enfermedades de maíz, incluyendo la roya, el antracnosis y la podredumbre foliar, con un rendimiento estado del arte en el conjunto de datos personalizado.<details>
<summary>Abstract</summary>
This research paper presents AMaizeD: An End to End Pipeline for Automatic Maize Disease Detection, an automated framework for early detection of diseases in maize crops using multispectral imagery obtained from drones. A custom hand-collected dataset focusing specifically on maize crops was meticulously gathered by expert researchers and agronomists. The dataset encompasses a diverse range of maize varieties, cultivation practices, and environmental conditions, capturing various stages of maize growth and disease progression. By leveraging multispectral imagery, the framework benefits from improved spectral resolution and increased sensitivity to subtle changes in plant health. The proposed framework employs a combination of convolutional neural networks (CNNs) as feature extractors and segmentation techniques to identify both the maize plants and their associated diseases. Experimental results demonstrate the effectiveness of the framework in detecting a range of maize diseases, including powdery mildew, anthracnose, and leaf blight. The framework achieves state-of-the-art performance on the custom hand-collected dataset and contributes to the field of automated disease detection in agriculture, offering a practical solution for early identification of diseases in maize crops advanced machine learning techniques and deep learning architectures.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Framing-Relevance-for-Safety-Critical-Autonomous-Systems"><a href="#Framing-Relevance-for-Safety-Critical-Autonomous-Systems" class="headerlink" title="Framing Relevance for Safety-Critical Autonomous Systems"></a>Framing Relevance for Safety-Critical Autonomous Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14355">http://arxiv.org/abs/2307.14355</a></li>
<li>repo_url: None</li>
<li>paper_authors: Astrid Rakow</li>
<li>for: 这个论文是为了研究如何确定自动化系统在当前任务中需要的信息，以建立适当的世界观并实现任务目标。</li>
<li>methods: 这篇论文使用了正式方法来确定自动化系统需要的信息，包括对各种信息源的分类和选择，以及如何将信息整合到自动化系统中。</li>
<li>results: 这篇论文的结果表明，使用正式方法可以有效地确定自动化系统需要的信息，并且可以帮助自动化系统在充满信息的环境中做出更加有效的决策。<details>
<summary>Abstract</summary>
We are in the process of building complex highly autonomous systems that have build-in beliefs, perceive their environment and exchange information. These systems construct their respective world view and based on it they plan their future manoeuvres, i.e., they choose their actions in order to establish their goals based on their prediction of the possible futures. Usually these systems face an overwhelming flood of information provided by a variety of sources where by far not everything is relevant. The goal of our work is to develop a formal approach to determine what is relevant for a safety critical autonomous system at its current mission, i.e., what information suffices to build an appropriate world view to accomplish its mission goals.
</details>
<details>
<summary>摘要</summary>
我们正在建设复杂高自动化系统，这些系统具有内置的信念和环境感知功能，并且能够交换信息。这些系统根据自己的世界观建立未来行动计划，即选择行动以实现目标基于预测未来的可能性。通常这些系统面临着极大的信息泛洪，其中大多数信息并不相关。我们的工作目标是开发一种正式的方法，以确定一个安全关键自动化系统当前任务中需要的信息，以建立合适的世界观以实现任务目标。
</details></li>
</ul>
<hr>
<h2 id="Implementing-Smart-Contracts-The-case-of-NFT-rental-with-pay-per-like"><a href="#Implementing-Smart-Contracts-The-case-of-NFT-rental-with-pay-per-like" class="headerlink" title="Implementing Smart Contracts: The case of NFT-rental with pay-per-like"></a>Implementing Smart Contracts: The case of NFT-rental with pay-per-like</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02424">http://arxiv.org/abs/2308.02424</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/asopi/rental-project">https://github.com/asopi/rental-project</a></li>
<li>paper_authors: Alfred Sopi, Johannes Schneider, Jan vom Brocke</li>
<li>For: The paper aims to address the challenges of lending and renting non-fungible tokens (NFTs) for marketing purposes, such as the risk of items not being returned and the difficulty in anticipating the impact of artworks.* Methods: The paper introduces an NFT rental solution based on a pay-per-like pricing model using blockchain technology and smart contracts based on the Ethereum chain.* Results: The paper finds that blockchain solutions enjoy many advantages, but also observes dark sides such as large blockchain fees, which can be unfair to niche artists and potentially hamper cultural diversity. Additionally, a trust-cost tradeoff arises to handle fraud caused by manipulation from parties outside the blockchain.Here are the three points in Simplified Chinese text:* For: 论文目的是解决非 fungible tokens（NFTs）的借领和租赁问题，如物品不返还和艺术作品的影响预测困难。* Methods: 论文提出了基于 pays-per-like 价格模式的 NFT 租赁解决方案，使用了区块链技术和基于 Ethereum 链的智能合约。* Results: 论文发现区块链解决方案具有许多优点，但也注意到了一些黑暗的面向，如大量区块链费用，可能对专业艺术家不公平，可能妨碍文化多样性。此外，面临滥用和 manipulate 等问题，需要考虑信任成本tradeoff。<details>
<summary>Abstract</summary>
Non-fungible tokens(NFTs) are on the rise. They can represent artworks exhibited for marketing purposes on webpages of companies or online stores -- analogously to physical artworks. Lending of NFTs is an attractive form of passive income for owners but comes with risks (e.g., items are not returned) and costs for escrow agents. Similarly, renters have difficulties in anticipating the impact of artworks, e.g., how spectators of NFTs perceive them. To address these challenges, we introduce an NFT rental solution based on a pay-per-like pricing model using blockchain technology, i.e., smart contracts based on the Ethereum chain. We find that blockchain solutions enjoy many advantages also reported for other applications, but interestingly, we also observe dark sides of (large) blockchain fees. Blockchain solutions appear unfair to niche artists and potentially hamper cultural diversity. Furthermore, a trust-cost tradeoff arises to handle fraud caused by manipulation from parties outside the blockchain. All code for the solution is publicly available at: https://github.com/asopi/rental-project
</details>
<details>
<summary>摘要</summary>
非可转换token(NFT)在升温。它们可以代表公司或在线商店的网页上展示的艺术作品，类似于 físichen艺术作品。NFT的借用是持有者的有利预想的 passive income，但是也有风险（例如，物品不返回）和代理人的成本。同时，租户困难预测NFT的影响，例如，如何评估NF的观众。为解决这些挑战，我们介绍了基于付费模式的NFT租赁解决方案，使用区块链技术和Ethereum链上的智能合约。我们发现，区块链解决方案在其他应用程序中报道的优点也存在，但是有趣的是，我们还观察到大型区块链费用的黑暗面。区块链解决方案可能不公平对尼希艺术家和文化多样性。此外，为了处理外部干扰所引起的诈骗，需要处理信任成本。所有的代码都可以在GitHub上找到：https://github.com/asopi/rental-project。
</details></li>
</ul>
<hr>
<h2 id="Validation-of-a-Zero-Shot-Learning-Natural-Language-Processing-Tool-for-Data-Abstraction-from-Unstructured-Healthcare-Data"><a href="#Validation-of-a-Zero-Shot-Learning-Natural-Language-Processing-Tool-for-Data-Abstraction-from-Unstructured-Healthcare-Data" class="headerlink" title="Validation of a Zero-Shot Learning Natural Language Processing Tool for Data Abstraction from Unstructured Healthcare Data"></a>Validation of a Zero-Shot Learning Natural Language Processing Tool for Data Abstraction from Unstructured Healthcare Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00107">http://arxiv.org/abs/2308.00107</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kaufmannb/PDF-Extractor">https://github.com/kaufmannb/PDF-Extractor</a></li>
<li>paper_authors: Basil Kaufmann, Dallin Busby, Chandan Krushna Das, Neeraja Tillu, Mani Menon, Ashutosh K. Tewari, Michael A. Gorin</li>
<li>For: The paper is written to describe the development and validation of a zero-shot learning natural language processing (NLP) tool for abstracting data from unstructured text contained within PDF documents, such as those found within electronic health records.* Methods: The data abstraction tool was based on the GPT-3.5 model from OpenAI, and was compared to three physician human abstractors in terms of time to task completion and accuracy for abstracting data on 14 unique variables from a set of 199 de-identified radical prostatectomy pathology reports. The reports were processed by the software tool in vectorized and scanned formats to establish the impact of optical character recognition on data abstraction.* Results: The software tool required a mean of 12.8 s to process the vectorized reports and a mean of 15.8 s to process the scanned reports, which was significantly faster than the human abstractors (mean time of 101 s). The tool had an overall accuracy of 94.2% for the vectorized reports and 88.7% for the scanned reports, which was non-inferior to 2 out of 3 human abstractors.<details>
<summary>Abstract</summary>
Objectives: To describe the development and validation of a zero-shot learning natural language processing (NLP) tool for abstracting data from unstructured text contained within PDF documents, such as those found within electronic health records. Materials and Methods: A data abstraction tool based on the GPT-3.5 model from OpenAI was developed and compared to three physician human abstractors in terms of time to task completion and accuracy for abstracting data on 14 unique variables from a set of 199 de-identified radical prostatectomy pathology reports. The reports were processed by the software tool in vectorized and scanned formats to establish the impact of optical character recognition on data abstraction. The tool was assessed for superiority for data abstraction speed and non-inferiority for accuracy. Results: The human abstractors required a mean of 101s per report for data abstraction, with times varying from 15 to 284 s. In comparison, the software tool required a mean of 12.8 s to process the vectorized reports and a mean of 15.8 to process the scanned reports (P < 0.001). The overall accuracies of the three human abstractors were 94.7%, 97.8%, and 96.4% for the combined set of 2786 datapoints. The software tool had an overall accuracy of 94.2% for the vectorized reports, proving to be non-inferior to the human abstractors at a margin of -10% ($\alpha$=0.025). The tool had a slightly lower accuracy of 88.7% using the scanned reports, proving to be non-inferiority to 2 out of 3 human abstractors. Conclusion: The developed zero-shot learning NLP tool affords researchers comparable levels of accuracy to that of human abstractors, with significant time savings benefits. Because of the lack of need for task-specific model training, the developed tool is highly generalizable and can be used for a wide variety of data abstraction tasks, even outside the field of medicine.
</details>
<details>
<summary>摘要</summary>
Materials and Methods:  We developed a data abstraction tool based on the GPT-3.5 model from OpenAI and compared its performance to three human abstractors in terms of time and accuracy for abstracting data from 14 variables in 199 de-identified radical prostatectomy pathology reports. The reports were processed in both vectorized and scanned formats to assess the impact of optical character recognition (OCR) on data abstraction. We evaluated the tool's superiority in speed and non-inferiority in accuracy.Results:  The human abstractors took a mean of 101 seconds per report, with times ranging from 15 to 284 seconds. In contrast, the software tool took a mean of 12.8 seconds to process vectorized reports and 15.8 seconds for scanned reports (p < 0.001). The tool's overall accuracy was 94.2% for vectorized reports, proving non-inferiority to the human abstractors at a margin of -10% (α = 0.025). For scanned reports, the tool's accuracy was 88.7%, proving non-inferiority to two out of three human abstractors.Conclusion:  The developed zero-shot learning NLP tool provides researchers with a time-saving solution that affords comparable levels of accuracy to human abstractors. The tool's lack of need for task-specific model training makes it highly generalizable and suitable for a wide range of data abstraction tasks, both within and outside the field of medicine.
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-aware-Grounded-Action-Transformation-towards-Sim-to-Real-Transfer-for-Traffic-Signal-Control"><a href="#Uncertainty-aware-Grounded-Action-Transformation-towards-Sim-to-Real-Transfer-for-Traffic-Signal-Control" class="headerlink" title="Uncertainty-aware Grounded Action Transformation towards Sim-to-Real Transfer for Traffic Signal Control"></a>Uncertainty-aware Grounded Action Transformation towards Sim-to-Real Transfer for Traffic Signal Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12388">http://arxiv.org/abs/2307.12388</a></li>
<li>repo_url: None</li>
<li>paper_authors: Longchao Da, Hao Mei, Romir Sharma, Hua Wei</li>
<li>for: 提高RL在实际道路上的应用性能</li>
<li>methods: 使用 simulations-to-real-world (sim-to-real) 转移方法，动态将模拟环境中学习的策略转移到实际环境中，以 Mitigate domain gap</li>
<li>results: 在模拟交通环境中评估了UGAT方法，显示UGAT方法可以在实际环境中提高RL策略的性能<details>
<summary>Abstract</summary>
Traffic signal control (TSC) is a complex and important task that affects the daily lives of millions of people. Reinforcement Learning (RL) has shown promising results in optimizing traffic signal control, but current RL-based TSC methods are mainly trained in simulation and suffer from the performance gap between simulation and the real world. In this paper, we propose a simulation-to-real-world (sim-to-real) transfer approach called UGAT, which transfers a learned policy trained from a simulated environment to a real-world environment by dynamically transforming actions in the simulation with uncertainty to mitigate the domain gap of transition dynamics. We evaluate our method on a simulated traffic environment and show that it significantly improves the performance of the transferred RL policy in the real world.
</details>
<details>
<summary>摘要</summary>
交通信号控制（TSC）是一项复杂且重要的任务，影响了数百万人的日常生活。人工智能学习（RL）已经在优化交通信号控制方面表现出了扎实的成果，但现有RL基于TSC方法主要在模拟环境中训练，它们在实际世界中的性能差异很大。在这篇论文中，我们提出了一种从模拟环境到实际世界（sim-to-real）的转移方法，称为UGAT，它通过在模拟环境中动态地转换行动，以减少领域差距，将学习的RL策略在实际世界中表现出较好的性能。我们对一个模拟交通环境进行评估，并显示了UGAT方法在实际世界中的性能提升。
</details></li>
</ul>
<hr>
<h2 id="In-Context-Learning-in-Large-Language-Models-Learns-Label-Relationships-but-Is-Not-Conventional-Learning"><a href="#In-Context-Learning-in-Large-Language-Models-Learns-Label-Relationships-but-Is-Not-Conventional-Learning" class="headerlink" title="In-Context Learning in Large Language Models Learns Label Relationships but Is Not Conventional Learning"></a>In-Context Learning in Large Language Models Learns Label Relationships but Is Not Conventional Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12375">http://arxiv.org/abs/2307.12375</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jannik Kossen, Tom Rainforth, Yarin Gal</li>
<li>for: 这种研究旨在探讨大语言模型（LLMs）在下游任务中表现提高的原因，以及如何更好地理解和调控LLMs的行为。</li>
<li>methods: 这篇论文使用了实验方法，检查了LLMs在不同情况下如何处理输入和标签的关系，并分析了模型如何在不同阶段学习和使用标签信息。</li>
<li>results: 研究发现，LLMs通常会在输入中使用标签信息，但是在预训练和输入中的标签关系是不同的，并且模型不会对所有输入信息进行平等处理。这些结论可以帮助我们更好地理解和调控LLMs的行为。<details>
<summary>Abstract</summary>
The performance of Large Language Models (LLMs) on downstream tasks often improves significantly when including examples of the input-label relationship in the context. However, there is currently no consensus about how this in-context learning (ICL) ability of LLMs works: for example, while Xie et al. (2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022b) argue ICL does not even learn label relationships from in-context examples. In this paper, we study (1) how labels of in-context examples affect predictions, (2) how label relationships learned during pre-training interact with input-label examples provided in-context, and (3) how ICL aggregates label information across in-context examples. Our findings suggests LLMs usually incorporate information from in-context labels, but that pre-training and in-context label relationships are treated differently, and that the model does not consider all in-context information equally. Our results give insights into understanding and aligning LLM behavior.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在下游任务中表现往往会得到显著改善，当包含输入-标签关系的示例在内部时。然而，目前没有一致的看法，即如何实现这种在 контексте学习（ICL）能力：例如，希耶等（2021）将 ICL 比作一种通用学习算法，而民等（2022b）则认为 ICL 不会从内部示例中学习标签关系。在这篇论文中，我们研究以下几点：1. 如何 Labels of in-context examples affect predictions.2. 如何在预训练时学习的标签关系与输入-标签示例提供在内部交互。3. ICL 如何对各个内部示例的标签信息进行汇总。我们的发现表明，LLMs 通常会在内部示例中使用标签信息，但是预训练和内部标签关系被处理不同，并且模型不会对所有内部信息进行平等考虑。我们的结果为理解和调整 LLM 行为提供了新的视角。
</details></li>
</ul>
<hr>
<h2 id="Early-Prediction-of-Alzheimers-Disease-Leveraging-Symptom-Occurrences-from-Longitudinal-Electronic-Health-Records-of-US-Military-Veterans"><a href="#Early-Prediction-of-Alzheimers-Disease-Leveraging-Symptom-Occurrences-from-Longitudinal-Electronic-Health-Records-of-US-Military-Veterans" class="headerlink" title="Early Prediction of Alzheimers Disease Leveraging Symptom Occurrences from Longitudinal Electronic Health Records of US Military Veterans"></a>Early Prediction of Alzheimers Disease Leveraging Symptom Occurrences from Longitudinal Electronic Health Records of US Military Veterans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12369">http://arxiv.org/abs/2307.12369</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rumeng Li, Xun Wang, Dan Berlowitz, Brian Silver, Wen Hu, Heather Keating, Raelene Goodwin, Weisong Liu, Honghuang Lin, Hong Yu<br>for: 这个研究的目的是使用机器学习方法来分析患有阿尔茨杰病（AD）的长期电子医疗纪录（EHR），以预测AD的发病更早。methods: 这个研究使用了一种case-control设计，使用了来自美国卫生部VA卫生管理局（VHA）的长期EHR数据从2004年到2021年。研究使用了一组AD相关关键词，并分析了这些词的时间发展特征以预测AD的发病。results: 研究发现，与AD诊断日期相关的AD相关关键词的出现频率在患有AD的患者中增长迅速，从约10年增长到超过40年。此外，研究还发现了一些年龄、性别和种族&#x2F;民族 subgroup的差异。最佳模型可以具有高度的排除率（ROCAUC 0.997），并且在不同的 subgroup 中具有良好的准确率和均匀性。<details>
<summary>Abstract</summary>
Early prediction of Alzheimer's disease (AD) is crucial for timely intervention and treatment. This study aims to use machine learning approaches to analyze longitudinal electronic health records (EHRs) of patients with AD and identify signs and symptoms that can predict AD onset earlier. We used a case-control design with longitudinal EHRs from the U.S. Department of Veterans Affairs Veterans Health Administration (VHA) from 2004 to 2021. Cases were VHA patients with AD diagnosed after 1/1/2016 based on ICD-10-CM codes, matched 1:9 with controls by age, sex and clinical utilization with replacement. We used a panel of AD-related keywords and their occurrences over time in a patient's longitudinal EHRs as predictors for AD prediction with four machine learning models. We performed subgroup analyses by age, sex, and race/ethnicity, and validated the model in a hold-out and "unseen" VHA stations group. Model discrimination, calibration, and other relevant metrics were reported for predictions up to ten years before ICD-based diagnosis. The study population included 16,701 cases and 39,097 matched controls. The average number of AD-related keywords (e.g., "concentration", "speaking") per year increased rapidly for cases as diagnosis approached, from around 10 to over 40, while remaining flat at 10 for controls. The best model achieved high discriminative accuracy (ROCAUC 0.997) for predictions using data from at least ten years before ICD-based diagnoses. The model was well-calibrated (Hosmer-Lemeshow goodness-of-fit p-value = 0.99) and consistent across subgroups of age, sex and race/ethnicity, except for patients younger than 65 (ROCAUC 0.746). Machine learning models using AD-related keywords identified from EHR notes can predict future AD diagnoses, suggesting its potential use for identifying AD risk using EHR notes, offering an affordable way for early screening on large population.
</details>
<details>
<summary>摘要</summary>
早期预测阿尔茨海默病（AD）非常重要，以便在时间上采取措施和治疗。这项研究的目的是使用机器学习方法分析患者的长期电子医疗纪录（EHR），以预测AD的发病。我们采用了一种case-control设计，使用了2004年至2021年美国卫生部 veterans Health Administration（VHA）的长期EHR数据。cases是在2016年1月1日以后根据ICD-10-CM代码被诊断出AD的VHA患者，与年龄、性别和临床使用相同的9名控制组进行匹配。我们使用了AD相关关键词的Panel，并分析这些关键词在患者的长期EHR中的出现情况，以预测AD的发病。我们进行了年龄、性别和种族/民族 subgroup分析，并在一个封闭和“未看到”的VHA站组中验证了模型。我们测试了四种机器学习模型，并计算了预测正确率、准确率和其他相关指标。研究人口包括16,701个case和39,097个匹配控制组。case的AD相关关键词每年增长迅速，从约10到超过40，而控制组保持在10个。最佳模型在使用至少10年前ICD-based诊断时的预测中 дости到了高度的抗抑词率（ROCAUC 0.997）。模型具有良好的准确率（Hosmer-Lemeshow好准确性值 = 0.99）和年龄、性别和种族/民族 subgroup中的一致性，除了年龄少于65的患者（ROCAUC 0.746）。机器学习模型使用从EHR笔记中提取的AD相关关键词可以预测未来AD诊断，这表明了这种方法的潜在应用价值，可以通过分析EHR笔记来预测AD风险，这是一种可靠且经济的预测方法。
</details></li>
</ul>
<hr>
<h2 id="Deployment-of-Leader-Follower-Automated-Vehicle-Systems-for-Smart-Work-Zone-Applications-with-a-Queuing-based-Traffic-Assignment-Approach"><a href="#Deployment-of-Leader-Follower-Automated-Vehicle-Systems-for-Smart-Work-Zone-Applications-with-a-Queuing-based-Traffic-Assignment-Approach" class="headerlink" title="Deployment of Leader-Follower Automated Vehicle Systems for Smart Work Zone Applications with a Queuing-based Traffic Assignment Approach"></a>Deployment of Leader-Follower Automated Vehicle Systems for Smart Work Zone Applications with a Queuing-based Traffic Assignment Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03764">http://arxiv.org/abs/2308.03764</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qing Tang, Xianbiao Hu</li>
<li>for: 这篇论文旨在优化Autonomous Truck Mounted Attenuator（ATMA）车辆系统中的routing，以最小化在交通基础设施维护期间的系统成本。</li>
<li>methods: 这篇论文使用了连接和自动化车辆技术，并提出了一种基于队列的交通分配方法，以考虑ATMA车辆的运行速度差异。</li>
<li>results: 研究发现，通过模拟不同路线的选择，ATMA车辆系统可以减少交通系统的成本，并且可以通过队列基于的交通分配方法来实现这一目标。<details>
<summary>Abstract</summary>
The emerging technology of the Autonomous Truck Mounted Attenuator (ATMA), a leader-follower style vehicle system, utilizes connected and automated vehicle capabilities to enhance safety during transportation infrastructure maintenance in work zones. However, the speed difference between ATMA vehicles and general vehicles creates a moving bottleneck that reduces capacity and increases queue length, resulting in additional delays. The different routes taken by ATMA cause diverse patterns of time-varying capacity drops, which may affect the user equilibrium traffic assignment and lead to different system costs. This manuscript focuses on optimizing the routing for ATMA vehicles in a network to minimize the system cost associated with the slow-moving operation.   To achieve this, a queuing-based traffic assignment approach is proposed to identify the system cost caused by the ATMA system. A queuing-based time-dependent (QBTD) travel time function, considering capacity drop, is introduced and applied in the static user equilibrium traffic assignment problem, with a result of adding dynamic characteristics. Subsequently, we formulate the queuing-based traffic assignment problem and solve it using a modified path-based algorithm. The methodology is validated using a small-size and a large-size network and compared with two benchmark models to analyze the benefit of capacity drop modeling and QBTD travel time function. Furthermore, the approach is applied to quantify the impact of different routes on the traffic system and identify an optimal route for ATMA vehicles performing maintenance work. Finally, sensitivity analysis is conducted to explore how the impact changes with variations in traffic demand and capacity reduction.
</details>
<details>
<summary>摘要</summary>
新兴技术自动化卡车拥挤器（ATMA），一种领头随员式车辆系统，通过连接和自动化车辆能力来提高交通基础设施维护工区的安全性。然而，ATMA车辆的速度与普通车辆的速度差距创造了运动瓶颈，导致交通容量下降和排队较长，从而增加延迟。ATMA车辆采取不同的路线，导致时间变化的容量下降，这可能影响用户均衡交通分配和导致不同的系统成本。本文关注优化ATMA车辆网络路径，以最小化由慢速运行引起的系统成本。为此，我们提出了基于队列的交通分配方法，以识别由ATMA系统引起的系统成本。我们引入了考虑容量下降的队列基于时间依赖（QBTD）旅行时间函数，并应用于静态用户均衡交通分配问题。通过修改的路径基本算法，我们解决了队列基于交通分配问题。我们验证了方法使用小型和大型网络，并与两个参考模型进行比较，以分析容器下降模型和QBTD旅行时间函数的利好。此外，我们还应用该方法来评估不同路线对交通系统的影响，并确定最佳维护工区路线。最后，我们进行敏感分析，以explore系统成本变化的影响因素。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/24/cs.AI_2023_07_24/" data-id="cloqtaelt0017gh888ia57v31" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_07_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/24/cs.CL_2023_07_24/" class="article-date">
  <time datetime="2023-07-24T11:00:00.000Z" itemprop="datePublished">2023-07-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/24/cs.CL_2023_07_24/">cs.CL - 2023-07-24</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Corrections-of-Zipf’s-and-Heaps’-Laws-Derived-from-Hapax-Rate-Models"><a href="#Corrections-of-Zipf’s-and-Heaps’-Laws-Derived-from-Hapax-Rate-Models" class="headerlink" title="Corrections of Zipf’s and Heaps’ Laws Derived from Hapax Rate Models"></a>Corrections of Zipf’s and Heaps’ Laws Derived from Hapax Rate Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12896">http://arxiv.org/abs/2307.12896</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lukasz-debowski/zipfanatomy">https://github.com/lukasz-debowski/zipfanatomy</a></li>
<li>paper_authors: Łukasz Dębowski</li>
<li>for: 这篇论文旨在修正Zipf和Hep的法则，基于系统模型的废弃率。</li>
<li>methods: 这篇论文使用了两个假设：第一个是标准杯模型，认为短文本的边缘频率分布与长文本中的词汇频率分布相同；第二个假设是废弃率的函数是文本大小的简单函数。</li>
<li>results: 这篇论文显示，使用了Logistic模型可以得到最佳的适应。<details>
<summary>Abstract</summary>
The article introduces corrections to Zipf's and Heaps' laws based on systematic models of the hapax rate. The derivation rests on two assumptions: The first one is the standard urn model which predicts that marginal frequency distributions for shorter texts look as if word tokens were sampled blindly from a given longer text. The second assumption posits that the rate of hapaxes is a simple function of the text size. Four such functions are discussed: the constant model, the Davis model, the linear model, and the logistic model. It is shown that the logistic model yields the best fit.
</details>
<details>
<summary>摘要</summary>
文章介绍了对Zipf和堆法的修正，基于系统性模型的唯一urn模型和文本大小的函数模型。两个假设是：首先，假设短文本中的单词分布遵循着 longer text中的样本采样；其次，假设 hapax 的速率是文本大小的简单函数。文章提出了四种函数模型：常数模型、Davis模型、线性模型和ilogistic模型。结果表明，ilogistic模型得到了最佳的适应。
</details></li>
</ul>
<hr>
<h2 id="Joint-Dropout-Improving-Generalizability-in-Low-Resource-Neural-Machine-Translation-through-Phrase-Pair-Variables"><a href="#Joint-Dropout-Improving-Generalizability-in-Low-Resource-Neural-Machine-Translation-through-Phrase-Pair-Variables" class="headerlink" title="Joint Dropout: Improving Generalizability in Low-Resource Neural Machine Translation through Phrase Pair Variables"></a>Joint Dropout: Improving Generalizability in Low-Resource Neural Machine Translation through Phrase Pair Variables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12835">http://arxiv.org/abs/2307.12835</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Araabi, Vlad Niculae, Christof Monz</li>
<li>for: 提高低资源语言对翻译机器翻译的性能</li>
<li>methods: 使用联合Dropout方法，将短语替换为变量，提高翻译机器翻译的可组合性</li>
<li>results: 对低资源语言对翻译机器翻译进行了重要改进，为语言对翻译机器翻译带来了显著提高，并且在不同领域中也具有了更好的鲁棒性和适应性。<details>
<summary>Abstract</summary>
Despite the tremendous success of Neural Machine Translation (NMT), its performance on low-resource language pairs still remains subpar, partly due to the limited ability to handle previously unseen inputs, i.e., generalization. In this paper, we propose a method called Joint Dropout, that addresses the challenge of low-resource neural machine translation by substituting phrases with variables, resulting in significant enhancement of compositionality, which is a key aspect of generalization. We observe a substantial improvement in translation quality for language pairs with minimal resources, as seen in BLEU and Direct Assessment scores. Furthermore, we conduct an error analysis, and find Joint Dropout to also enhance generalizability of low-resource NMT in terms of robustness and adaptability across different domains
</details>
<details>
<summary>摘要</summary>
尽管神经机器翻译（NMT）已经取得了很大的成功，但它在低资源语言对的表现仍然较差，一个原因是对未经见过的输入的处理能力有限，即通用性。在这篇论文中，我们提出了一种方法called Joint Dropout，该方法通过将短语替换为变量，从而提高了语言对的复合性，这是通用性的关键特征。我们发现，对具有最少资源的语言对，使用Joint Dropout可以得到显著提高翻译质量，按照BLEU和直接评估得分来看。此外，我们进行了错误分析，发现Joint Dropout还可以提高低资源NMT的通用性，包括鲁棒性和适应性 across different domains。
</details></li>
</ul>
<hr>
<h2 id="Guidance-in-Radiology-Report-Summarization-An-Empirical-Evaluation-and-Error-Analysis"><a href="#Guidance-in-Radiology-Report-Summarization-An-Empirical-Evaluation-and-Error-Analysis" class="headerlink" title="Guidance in Radiology Report Summarization: An Empirical Evaluation and Error Analysis"></a>Guidance in Radiology Report Summarization: An Empirical Evaluation and Error Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12803">http://arxiv.org/abs/2307.12803</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jantrienes/inlg2023-radsum">https://github.com/jantrienes/inlg2023-radsum</a></li>
<li>paper_authors: Jan Trienes, Paul Youssef, Jörg Schlötterer, Christin Seifert</li>
<li>for:  automatization of radiology report summarization to reduce clinicians’ manual work and improve reporting consistency</li>
<li>methods:  variable-length extractive summaries as a domain-agnostic guidance signal, competitive with domain-specific methods</li>
<li>results:  improved summarization quality compared to unguided summarization, but still limited by content selection and corpus-level inconsistencies<details>
<summary>Abstract</summary>
Automatically summarizing radiology reports into a concise impression can reduce the manual burden of clinicians and improve the consistency of reporting. Previous work aimed to enhance content selection and factuality through guided abstractive summarization. However, two key issues persist. First, current methods heavily rely on domain-specific resources to extract the guidance signal, limiting their transferability to domains and languages where those resources are unavailable. Second, while automatic metrics like ROUGE show progress, we lack a good understanding of the errors and failure modes in this task. To bridge these gaps, we first propose a domain-agnostic guidance signal in form of variable-length extractive summaries. Our empirical results on two English benchmarks demonstrate that this guidance signal improves upon unguided summarization while being competitive with domain-specific methods. Additionally, we run an expert evaluation of four systems according to a taxonomy of 11 fine-grained errors. We find that the most pressing differences between automatic summaries and those of radiologists relate to content selection including omissions (up to 52%) and additions (up to 57%). We hypothesize that latent reporting factors and corpus-level inconsistencies may limit models to reliably learn content selection from the available data, presenting promising directions for future work.
</details>
<details>
<summary>摘要</summary>
自动概括 radiology 报告可以减少临床医生的手动劳动和提高报告的一致性。过去的工作是通过引导抽象SUMMARIZATION提高内容选择和事实性。然而，两个关键问题仍然存在。首先，当前的方法听命于域特定资源提取指导信号，限制其在领域和语言中的传输性。其次，虽然自动度量器Like ROUGE表现出进步，但我们对这个任务中的错误和失败模式几乎没有良好的理解。为了bridging这些差距，我们首先提议一种域无关的引导信号，即变量长抽取SUMMARIES。我们的实验结果表明，这种引导信号可以超过无引导抽取SUMMARIES，并与域特定方法竞争。此外，我们运行了四种系统的专家评估，根据报告11种细腻错误的税onomy。我们发现，自动报告与医生的报告之间最主要的差异在于内容选择，包括漏掉（最多52%）和添加（最多57%）。我们推测，隐藏的报告因素和 corpus 级别的不一致性可能限制模型从可用数据中学习内容选择，提供了可能的未来工作方向。
</details></li>
</ul>
<hr>
<h2 id="RRAML-Reinforced-Retrieval-Augmented-Machine-Learning"><a href="#RRAML-Reinforced-Retrieval-Augmented-Machine-Learning" class="headerlink" title="RRAML: Reinforced Retrieval Augmented Machine Learning"></a>RRAML: Reinforced Retrieval Augmented Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12798">http://arxiv.org/abs/2307.12798</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrea Bacciu, Florin Cuconasu, Federico Siciliano, Fabrizio Silvestri, Nicola Tonellotto, Giovanni Trappolini</li>
<li>for: 本研究旨在推广人工智能领域中的大语言模型（LLMs）的应用，提高其在理解、生成和修改人语言方面的能力。</li>
<li>methods: 本研究提出了一种新的框架，即强化检索增强机器学习（RRAML），它将LLMs的理解能力与一个特制的检索器连接起来，从一个大量的用户提供的数据库中提取支持信息。</li>
<li>results: RRAML可以减少LLMs的训练和重新训练的需求，同时也可以避免访问LLMs的梯度，从而提高其应用的效率和可扩展性。此外，RRAML还可以减少检索结果中的幻见和不相关信息，提高检索的准确率和有用性。<details>
<summary>Abstract</summary>
The emergence of large language models (LLMs) has revolutionized machine learning and related fields, showcasing remarkable abilities in comprehending, generating, and manipulating human language. However, their conventional usage through API-based text prompt submissions imposes certain limitations in terms of context constraints and external source availability. To address these challenges, we propose a novel framework called Reinforced Retrieval Augmented Machine Learning (RRAML). RRAML integrates the reasoning capabilities of LLMs with supporting information retrieved by a purpose-built retriever from a vast user-provided database. By leveraging recent advancements in reinforcement learning, our method effectively addresses several critical challenges. Firstly, it circumvents the need for accessing LLM gradients. Secondly, our method alleviates the burden of retraining LLMs for specific tasks, as it is often impractical or impossible due to restricted access to the model and the computational intensity involved. Additionally we seamlessly link the retriever's task with the reasoner, mitigating hallucinations and reducing irrelevant, and potentially damaging retrieved documents. We believe that the research agenda outlined in this paper has the potential to profoundly impact the field of AI, democratizing access to and utilization of LLMs for a wide range of entities.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的出现对机器学习和相关领域产生了革命性的变革，展示了人类语言理解、生成和修改的强大能力。然而，通过 API 提交文本提示来使用 LLM 存在一些限制，包括上下文约束和外部资源的可用性。为解决这些挑战，我们提出了一个新的框架 called Reinforced Retrieval Augmented Machine Learning（RRAML）。RRAML 将 LLM 的理解能力与用户提供的大量数据库中的支持信息结合起来，通过利用最近的回归学术进行有效地解决多个关键问题。首先，它绕过了访问 LLM 的梯度的需求。其次，我们的方法减轻了特定任务的 LLM 重新训练的压力，因为在访问模型和计算浩瀚性方面存在限制。此外，我们将检索器的任务与理解者联系在一起，以避免幻想和减少不相关和可能有害的检索文档。我们认为这篇论文的研究议程具有潜在的影响力，可以广泛影响 AI 领域，使 LLM 的访问和利用更加普遍和便捷。
</details></li>
</ul>
<hr>
<h2 id="Code-Switched-Urdu-ASR-for-Noisy-Telephonic-Environment-using-Data-Centric-Approach-with-Hybrid-HMM-and-CNN-TDNN"><a href="#Code-Switched-Urdu-ASR-for-Noisy-Telephonic-Environment-using-Data-Centric-Approach-with-Hybrid-HMM-and-CNN-TDNN" class="headerlink" title="Code-Switched Urdu ASR for Noisy Telephonic Environment using Data Centric Approach with Hybrid HMM and CNN-TDNN"></a>Code-Switched Urdu ASR for Noisy Telephonic Environment using Data Centric Approach with Hybrid HMM and CNN-TDNN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12759">http://arxiv.org/abs/2307.12759</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sage-khan/code-switched-noisy-urdu-asr">https://github.com/sage-khan/code-switched-noisy-urdu-asr</a></li>
<li>paper_authors: Muhammad Danyal Khan, Raheem Ali, Arshad Aziz</li>
<li>for: 这个论文是为了提出一种资源有效的自动语音识别系统，以便在呼叫中心环境中准确地转录电话对话，并且能够进行自动化的电话监测、关键词搜索和情感分析。</li>
<li>methods: 这个论文使用了链式混合HMM和CNN-TDNN来实现资源有效的自动语音识别系统，并且在呼叫中心环境中进行了测试和评估。</li>
<li>results: 根据论文的描述，在呼叫中心环境中，使用链式混合HMM和CNN-TDNN来实现自动语音识别系统，可以达到5.2%的Word Error Rate（WER），包括干净环境和噪音环境下的 isolated words和连续杂音speech。<details>
<summary>Abstract</summary>
Call Centers have huge amount of audio data which can be used for achieving valuable business insights and transcription of phone calls is manually tedious task. An effective Automated Speech Recognition system can accurately transcribe these calls for easy search through call history for specific context and content allowing automatic call monitoring, improving QoS through keyword search and sentiment analysis. ASR for Call Center requires more robustness as telephonic environment are generally noisy. Moreover, there are many low-resourced languages that are on verge of extinction which can be preserved with help of Automatic Speech Recognition Technology. Urdu is the $10^{th}$ most widely spoken language in the world, with 231,295,440 worldwide still remains a resource constrained language in ASR. Regional call-center conversations operate in local language, with a mix of English numbers and technical terms generally causing a "code-switching" problem. Hence, this paper describes an implementation framework of a resource efficient Automatic Speech Recognition/ Speech to Text System in a noisy call-center environment using Chain Hybrid HMM and CNN-TDNN for Code-Switched Urdu Language. Using Hybrid HMM-DNN approach allowed us to utilize the advantages of Neural Network with less labelled data. Adding CNN with TDNN has shown to work better in noisy environment due to CNN's additional frequency dimension which captures extra information from noisy speech, thus improving accuracy. We collected data from various open sources and labelled some of the unlabelled data after analysing its general context and content from Urdu language as well as from commonly used words from other languages, primarily English and were able to achieve WER of 5.2% with noisy as well as clean environment in isolated words or numbers as well as in continuous spontaneous speech.
</details>
<details>
<summary>摘要</summary>
Call Centers possess vast amounts of audio data that can be leveraged for gaining valuable business insights, and the manual transcription of phone calls is a tedious task. An effective Automatic Speech Recognition (ASR) system can accurately transcribe these calls, enabling easy search through call history for specific context and content, and allowing for automatic call monitoring, improving quality of service (QoS) through keyword search and sentiment analysis. However, ASR systems for call centers must be more robust due to the noisy telephonic environment. Moreover, there are many low-resource languages that are on the verge of extinction, and ASR technology can help preserve these languages. Urdu, the 10th most widely spoken language in the world with 231,295,440 speakers, remains a resource-constrained language in ASR. Regional call-center conversations often operate in local languages, with a mix of English and technical terms, causing a "code-switching" problem.To address these challenges, this paper proposes an implementation framework for a resource-efficient ASR/Speech-to-Text system in a noisy call-center environment using Chain Hybrid HMM and CNN-TDNN for Code-Switched Urdu Language. By combining Hybrid HMM-DNN and CNN-TDNN, we can leverage the advantages of neural networks with less labeled data. Additionally, the CNN-TDNN approach has shown to work better in noisy environments due to the CNN's additional frequency dimension, which captures extra information from noisy speech, improving accuracy.We collected data from various open sources and labeled some of the unlabeled data after analyzing its general context and content from Urdu language as well as from commonly used words from other languages, primarily English. Our results achieved a Word Error Rate (WER) of 5.2% with both noisy and clean environments in isolated words or numbers as well as in continuous spontaneous speech.
</details></li>
</ul>
<hr>
<h2 id="A-Model-for-Every-User-and-Budget-Label-Free-and-Personalized-Mixed-Precision-Quantization"><a href="#A-Model-for-Every-User-and-Budget-Label-Free-and-Personalized-Mixed-Precision-Quantization" class="headerlink" title="A Model for Every User and Budget: Label-Free and Personalized Mixed-Precision Quantization"></a>A Model for Every User and Budget: Label-Free and Personalized Mixed-Precision Quantization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12659">http://arxiv.org/abs/2307.12659</a></li>
<li>repo_url: None</li>
<li>paper_authors: Edward Fish, Umberto Michieli, Mete Ozay</li>
<li>for: 这个研究是为了提高自动语音识别（ASR）模型的实时部署，以及实现个人化模型适应。</li>
<li>methods: 这个研究使用了混合精度优化方法（myQASR），可以根据不同的用户和内存需求生成个性化的混合精度优化方案。myQASR 使用了全精度活动值分析来自动评估网络层的优化感受，并生成个性化的混合精度优化方案。</li>
<li>results: 研究结果显示，使用 myQASR 可以提高特定的性别、语言和说话者的表现，并且不需要组数调整。<details>
<summary>Abstract</summary>
Recent advancement in Automatic Speech Recognition (ASR) has produced large AI models, which become impractical for deployment in mobile devices. Model quantization is effective to produce compressed general-purpose models, however such models may only be deployed to a restricted sub-domain of interest. We show that ASR models can be personalized during quantization while relying on just a small set of unlabelled samples from the target domain. To this end, we propose myQASR, a mixed-precision quantization method that generates tailored quantization schemes for diverse users under any memory requirement with no fine-tuning. myQASR automatically evaluates the quantization sensitivity of network layers by analysing the full-precision activation values. We are then able to generate a personalised mixed-precision quantization scheme for any pre-determined memory budget. Results for large-scale ASR models show how myQASR improves performance for specific genders, languages, and speakers.
</details>
<details>
<summary>摘要</summary>
myQASR evaluates the quantization sensitivity of network layers by analyzing full-precision activation values, and generates a personalized mixed-precision quantization scheme for any pre-determined memory budget. Our results show that myQASR improves performance for specific genders, languages, and speakers.
</details></li>
</ul>
<hr>
<h2 id="Fake-News-Detection-Through-Graph-based-Neural-Networks-A-Survey"><a href="#Fake-News-Detection-Through-Graph-based-Neural-Networks-A-Survey" class="headerlink" title="Fake News Detection Through Graph-based Neural Networks: A Survey"></a>Fake News Detection Through Graph-based Neural Networks: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12639">http://arxiv.org/abs/2307.12639</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuzhi Gong, Richard O. Sinnott, Jianzhong Qi, Cecile Paris</li>
<li>for: 本研究主要旨在探讨基于图структуры和深度学习技术的假新闻检测方法。</li>
<li>methods: 图结构基于的方法包括知识驱动方法、传播基于方法和多元社交 контекст基于方法，它们根据不同的图结构来模型新闻相关信息的流动。</li>
<li>results: 研究发现，图结构基于方法在假新闻检测中得到了显著的成果，特别是在模elling社交媒体宣传过程中。但是，还存在一些挑战和未解决的问题，如假新闻的定义和识别、社交媒体平台的不同性和数据的可靠性等。<details>
<summary>Abstract</summary>
The popularity of online social networks has enabled rapid dissemination of information. People now can share and consume information much more rapidly than ever before. However, low-quality and/or accidentally/deliberately fake information can also spread rapidly. This can lead to considerable and negative impacts on society. Identifying, labelling and debunking online misinformation as early as possible has become an increasingly urgent problem. Many methods have been proposed to detect fake news including many deep learning and graph-based approaches. In recent years, graph-based methods have yielded strong results, as they can closely model the social context and propagation process of online news. In this paper, we present a systematic review of fake news detection studies based on graph-based and deep learning-based techniques. We classify existing graph-based methods into knowledge-driven methods, propagation-based methods, and heterogeneous social context-based methods, depending on how a graph structure is constructed to model news related information flows. We further discuss the challenges and open problems in graph-based fake news detection and identify future research directions.
</details>
<details>
<summary>摘要</summary>
在线社交网络的流行化使得信息的传播变得非常快速，人们可以更快地分享和消耗信息。然而，低质量和/或意外或故意假的信息也可以快速传播，这可能会对社会产生重大和负面的影响。正确地识别、标注和驳斥在线谣言已成为一项急需解决的问题。许多方法已经被提议来检测假新闻，其中包括深度学习和图基于的方法。在过去几年中，图基于的方法在检测假新闻方面取得了强劲的结果，因为它们可以准确地模拟在线新闻的社交上下文和传播过程。本文提供一个系统性的审查，检测基于图和深度学习的假新闻检测研究。我们将现有的图基于方法分为知识驱动的方法、传播基于方法和多元社交上下文基于方法，根据如何构建图来模型新闻相关信息的流动。我们还讨论了假新闻检测中的挑战和未解决的问题，并确定了未来研究的方向。
</details></li>
</ul>
<hr>
<h2 id="Tachikuma-Understading-Complex-Interactions-with-Multi-Character-and-Novel-Objects-by-Large-Language-Models"><a href="#Tachikuma-Understading-Complex-Interactions-with-Multi-Character-and-Novel-Objects-by-Large-Language-Models" class="headerlink" title="Tachikuma: Understading Complex Interactions with Multi-Character and Novel Objects by Large Language Models"></a>Tachikuma: Understading Complex Interactions with Multi-Character and Novel Objects by Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12573">http://arxiv.org/abs/2307.12573</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanzhi Liang, Linchao Zhu, Yi Yang<br>for:这篇论文旨在提高人工智能代理人在虚拟世界中的互动复杂性和灵活性，特别是在多个角色和新型对象的情况下。methods:该论文提出了在人工智能代理人世界模型中引入虚拟游戏主持人（GM）的想法，以增强信息把关、估计玩家的意图、提供环境描述和给予反馈等功能，从而补做当前世界模型的缺陷。results:该论文提出了一个名为Tachikuma的 benchmark，包括一个多个角色和新型对象基于互动 estimation（MOE）任务和一个相关的数据集。MOE挑战模型理解角色的意图并准确地确定他们在复杂情况下的行为。此外，数据集capture了在游戏即时通信中的实际交流记录，为未来的探索提供了多样、根据实际情况的复杂互动。最后，论文提出了一个简单的提示基线，并评估了其性能，示出其在促进互动理解方面的效果。<details>
<summary>Abstract</summary>
Recent advancements in natural language and Large Language Models (LLMs) have enabled AI agents to simulate human-like interactions within virtual worlds. However, these interactions still face limitations in complexity and flexibility, particularly in scenarios involving multiple characters and novel objects. Pre-defining all interactable objects in the agent's world model presents challenges, and conveying implicit intentions to multiple characters through complex interactions remains difficult. To address these issues, we propose integrating virtual Game Masters (GMs) into the agent's world model, drawing inspiration from Tabletop Role-Playing Games (TRPGs). GMs play a crucial role in overseeing information, estimating players' intentions, providing environment descriptions, and offering feedback, compensating for current world model deficiencies. To facilitate future explorations for complex interactions, we introduce a benchmark named Tachikuma, comprising a Multiple character and novel Object based interaction Estimation (MOE) task and a supporting dataset. MOE challenges models to understand characters' intentions and accurately determine their actions within intricate contexts involving multi-character and novel object interactions. Besides, the dataset captures log data from real-time communications during gameplay, providing diverse, grounded, and complex interactions for further explorations. Finally, we present a simple prompting baseline and evaluate its performance, demonstrating its effectiveness in enhancing interaction understanding. We hope that our dataset and task will inspire further research in complex interactions with natural language, fostering the development of more advanced AI agents.
</details>
<details>
<summary>摘要</summary>
To facilitate future explorations for complex interactions, we introduce a benchmark named Tachikuma, comprising a Multiple character and novel Object based interaction Estimation (MOE) task and a supporting dataset. MOE challenges models to understand characters' intentions and accurately determine their actions within intricate contexts involving multi-character and novel object interactions. Besides, the dataset captures log data from real-time communications during gameplay, providing diverse, grounded, and complex interactions for further explorations.Finally, we present a simple prompting baseline and evaluate its performance, demonstrating its effectiveness in enhancing interaction understanding. We hope that our dataset and task will inspire further research in complex interactions with natural language, fostering the development of more advanced AI agents.Translation in Simplified Chinese:最近的自然语言和大型语言模型（LLMs）的进步，使得AI代理人能够在虚拟世界中模拟人类化的互动。然而，这些互动仍面临复杂性和灵活性的限制，特别是在多个角色和新的物品的情况下。将所有互动的物品都嵌入代理人的世界模型中存在挑战，而且通过复杂的互动传递多个角色的意图仍然具有挑战性。为解决这些问题，我们提出了在代理人的世界模型中 integrate 虚拟游戏大师（GMs）的想法， draw  inspirations from 桌上角色扮演游戏（TRPGs）。GMs 在虚拟世界中扮演着重要的角色，负责资讯的监督、玩家的意图的估计、环境描述和回应，以补偿现有世界模型的不足。为了促进未来的复杂互动探索，我们提出了一个名为 Tachikuma 的benchmark，包括一个多个角色和新的物品基本互动Estimation（MOE）任务和一个支持 datasets。MOE 挑战模型能够理解角色的意图和精确地决定他们在复杂的多个角色和新的物品互动中的动作。此外， datasets  capture 游戏中的实时通讯记录，提供多样化、根据现实的互动进行探索。最后，我们提出了一个简单的提示基eline，评估其表现，证明其能够增强互动理解。我们希望这个dataset和任务能够鼓励更多的研究在复杂互动中的自然语言，推动更进步的 AI 代理人的发展。
</details></li>
</ul>
<hr>
<h2 id="Towards-Generalising-Neural-Topical-Representations"><a href="#Towards-Generalising-Neural-Topical-Representations" class="headerlink" title="Towards Generalising Neural Topical Representations"></a>Towards Generalising Neural Topical Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12564">http://arxiv.org/abs/2307.12564</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaohao Yang, He Zhao, Dinh Phung, Lan Du</li>
<li>for: 提高神经话题模型（NTM）的泛化能力，使其在不同文库和任务中产生质量话题表示。</li>
<li>methods: 使用数据扩充 durante el entrenamiento para模型 similar documents，并使用 Hierarchical Topic Transport Distance (HOTT) 测量文档之间的semantical distance。</li>
<li>results: 对多个NTMs进行了广泛的实验，并证明了框架可以significantly improve neural topical representation的泛化能力 across corpora。<details>
<summary>Abstract</summary>
Topic models have evolved from conventional Bayesian probabilistic models to Neural Topic Models (NTMs) over the last two decays. Although NTMs have achieved promising performance when trained and tested on a specific corpus, their generalisation ability across corpora is rarely studied. In practice, we often expect that an NTM trained on a source corpus can still produce quality topical representation for documents in a different target corpus without retraining. In this work, we aim to improve NTMs further so that their benefits generalise reliably across corpora and tasks. To do so, we propose to model similar documents by minimising their semantical distance when training NTMs. Specifically, similar documents are created by data augmentation during training; The semantical distance between documents is measured by the Hierarchical Topic Transport Distance (HOTT), which computes the Optimal Transport (OT) distance between the topical representations. Our framework can be readily applied to most NTMs as a plug-and-play module. Extensive experiments show that our framework significantly improves the generalisation ability regarding neural topical representation across corpora.
</details>
<details>
<summary>摘要</summary>
To achieve this, we propose to model similar documents by minimizing their semantic distance during training. Specifically, we create similar documents by performing data augmentation during training, and we measure the semantic distance between documents using the Hierarchical Topic Transport Distance (HOTT), which computes the Optimal Transport (OT) distance between the topical representations. Our framework can be easily applied to most NTMs as a plug-and-play module.Extensive experiments show that our framework significantly improves the generalization ability of neural topical representation across corpora.
</details></li>
</ul>
<hr>
<h2 id="Lost-In-Translation-Generating-Adversarial-Examples-Robust-to-Round-Trip-Translation"><a href="#Lost-In-Translation-Generating-Adversarial-Examples-Robust-to-Round-Trip-Translation" class="headerlink" title="Lost In Translation: Generating Adversarial Examples Robust to Round-Trip Translation"></a>Lost In Translation: Generating Adversarial Examples Robust to Round-Trip Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12520">http://arxiv.org/abs/2307.12520</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/neelbhandari6/nmt_text_attack">https://github.com/neelbhandari6/nmt_text_attack</a></li>
<li>paper_authors: Neel Bhandari, Pin-Yu Chen</li>
<li>for: This paper aims to study the robustness of current text adversarial attacks to round-trip translation and to introduce an intervention-based solution to improve the robustness of adversarial examples.</li>
<li>methods: The paper uses six state-of-the-art text-based adversarial attacks and integrates machine translation into the process of adversarial example generation to improve the robustness of adversarial examples.</li>
<li>results: The paper demonstrates that finding adversarial examples robust to translation can help identify the insufficiency of language models that is common across languages, and motivate further research into multilingual adversarial attacks.Here’s the text in Simplified Chinese:</li>
<li>for: 这篇论文目标是研究当前文本 adversarial 攻击的翻译循环Robustness，并提出一种干预方法来提高攻击示例的Robustness。</li>
<li>methods: 论文使用了六种当前最佳文本基于攻击方法，并将机器翻译integrated into the process of adversarial example generation以提高攻击示例的Robustness。</li>
<li>results: 论文表明，找到可以在翻译中维持Robustness的攻击示例可以帮助发现语言模型的共同缺陷，并促进多语言攻击的研究。<details>
<summary>Abstract</summary>
Language Models today provide a high accuracy across a large number of downstream tasks. However, they remain susceptible to adversarial attacks, particularly against those where the adversarial examples maintain considerable similarity to the original text. Given the multilingual nature of text, the effectiveness of adversarial examples across translations and how machine translations can improve the robustness of adversarial examples remain largely unexplored. In this paper, we present a comprehensive study on the robustness of current text adversarial attacks to round-trip translation. We demonstrate that 6 state-of-the-art text-based adversarial attacks do not maintain their efficacy after round-trip translation. Furthermore, we introduce an intervention-based solution to this problem, by integrating Machine Translation into the process of adversarial example generation and demonstrating increased robustness to round-trip translation. Our results indicate that finding adversarial examples robust to translation can help identify the insufficiency of language models that is common across languages, and motivate further research into multilingual adversarial attacks.
</details>
<details>
<summary>摘要</summary>
现代语言模型在许多下游任务上具有高准确率，但它们仍然易受到恶意攻击，特别是那些保留了原文的相似性。由于文本的多语言特性，对翻译后的恶意攻击的效iveness和机器翻译如何提高恶意攻击的Robustness remains largely unexplored。在这篇论文中，我们提供了round-trip translation对当前文本恶意攻击的全面研究。我们发现了6种现状顶尖文本基于攻击不具有翻译后的效力。此外，我们还介绍了一种利用机器翻译的解决方案，通过将机器翻译 integrate into the process of generating adversarial examples，并证明了该方法可以提高恶意攻击的Robustness。我们的结果表明，找到可以抵抗翻译的恶意攻击可以帮助发现语言模型的共同缺陷，并促进更多的关于多语言恶意攻击的研究。
</details></li>
</ul>
<hr>
<h2 id="Robust-Automatic-Speech-Recognition-via-WavAugment-Guided-Phoneme-Adversarial-Training"><a href="#Robust-Automatic-Speech-Recognition-via-WavAugment-Guided-Phoneme-Adversarial-Training" class="headerlink" title="Robust Automatic Speech Recognition via WavAugment Guided Phoneme Adversarial Training"></a>Robust Automatic Speech Recognition via WavAugment Guided Phoneme Adversarial Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12498">http://arxiv.org/abs/2307.12498</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/WAPATASR/WAPAT">https://github.com/WAPATASR/WAPAT</a></li>
<li>paper_authors: Gege Qi, Yuefeng Chen, Xiaofeng Mao, Xiaojun Jia, Ranjie Duan, Rong Zhang, Hui Xue</li>
<li>for: 提高自动话语识别（ASR）模型的实际抗坏性，使其在小量干扰和大范围频率变化下保持原有性能。</li>
<li>methods: 提出了一种新的WavAugment导向的phoneme adversarial Training（wapat）方法，通过在phoneme空间中使用对抗示例来使模型具有小量干扰和大范围频率变化下的抗坏性，并且通过使用振荡示例来引导对抗生成，以获得更好的普适性。</li>
<li>results: 在End-to-end Speech Challenge Benchmark（ESB）上进行了广泛的实验，结果表明，SpeechLM-wapat模型比原始模型减少了6.28%的Word Error Rate（WER），达到了新的状态态-of-the-art。<details>
<summary>Abstract</summary>
Developing a practically-robust automatic speech recognition (ASR) is challenging since the model should not only maintain the original performance on clean samples, but also achieve consistent efficacy under small volume perturbations and large domain shifts. To address this problem, we propose a novel WavAugment Guided Phoneme Adversarial Training (wapat). wapat use adversarial examples in phoneme space as augmentation to make the model invariant to minor fluctuations in phoneme representation and preserve the performance on clean samples. In addition, wapat utilizes the phoneme representation of augmented samples to guide the generation of adversaries, which helps to find more stable and diverse gradient-directions, resulting in improved generalization. Extensive experiments demonstrate the effectiveness of wapat on End-to-end Speech Challenge Benchmark (ESB). Notably, SpeechLM-wapat outperforms the original model by 6.28% WER reduction on ESB, achieving the new state-of-the-art.
</details>
<details>
<summary>摘要</summary>
开发一个实用robust的自动语音识别（ASR）系统是具有搅乱的挑战，因为模型需要不仅保持干净样本的原始性能，还需要在小量扰动和大域转换下实现一致的效果。为解决这个问题，我们提出了一种新的WavAugment导向的phoneme adversarial training（wapat）方法。wapat使用phoneme空间的对抗样本作为增强元素，使模型对phoneme表示的小变化具有抗衰减性，并保持干净样本的性能。此外，wapat利用增强后的phoneme表示导向对抗生成，以找到更稳定和多样的梯度方向，从而提高泛化能力。广泛的实验表明，wapat在End-to-end Speech Challenge Benchmark（ESB）上具有显著的效果，SpeechLM-wapat比原始模型减少6.28%的WRR，实现新的州际顶峰性。
</details></li>
</ul>
<hr>
<h2 id="On-the-Effectiveness-of-Offline-RL-for-Dialogue-Response-Generation"><a href="#On-the-Effectiveness-of-Offline-RL-for-Dialogue-Response-Generation" class="headerlink" title="On the Effectiveness of Offline RL for Dialogue Response Generation"></a>On the Effectiveness of Offline RL for Dialogue Response Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12425">http://arxiv.org/abs/2307.12425</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/asappresearch/dialogue-offline-rl">https://github.com/asappresearch/dialogue-offline-rl</a></li>
<li>paper_authors: Paloma Sodhi, Felix Wu, Ethan R. Elenberg, Kilian Q. Weinberger, Ryan McDonald</li>
<li>for: 研究 teacher forcing 的替代方法，以提高对话响应生成的性能。</li>
<li>methods: 使用了多种离线束规学学习（RL）方法，以优化对话响应生成的序列水平目标。</li>
<li>results: 研究发现，离线RL可以明显提高对话响应生成的性能，而不会导致训练不稳定或减少实际训练时间。<details>
<summary>Abstract</summary>
A common training technique for language models is teacher forcing (TF). TF attempts to match human language exactly, even though identical meanings can be expressed in different ways. This motivates use of sequence-level objectives for dialogue response generation. In this paper, we study the efficacy of various offline reinforcement learning (RL) methods to maximize such objectives. We present a comprehensive evaluation across multiple datasets, models, and metrics. Offline RL shows a clear performance improvement over teacher forcing while not inducing training instability or sacrificing practical training budgets.
</details>
<details>
<summary>摘要</summary>
一种常见的语言模型训练技巧是教师强制（TF）。TF尝试匹配人类语言完全一致，即使同一个意思可以表达在不同的方式。这种motivation使我们使用序列级目标来生成对话响应。在这篇论文中，我们研究了多种离线强化学习（RL）方法，以最大化这些目标。我们在多个数据集、模型和指标上进行了全面的评估。离线RL显示了与教师强制相比的表现提升，而不会导致训练不稳定或浪费实际训练预算。
</details></li>
</ul>
<hr>
<h2 id="Testing-Hateful-Speeches-against-Policies"><a href="#Testing-Hateful-Speeches-against-Policies" class="headerlink" title="Testing Hateful Speeches against Policies"></a>Testing Hateful Speeches against Policies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12418">http://arxiv.org/abs/2307.12418</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/htytewx/softcam">https://github.com/htytewx/softcam</a></li>
<li>paper_authors: Jiangrui Zheng, Xueqing Liu, Girish Budhrani, Wei Yang, Ravishka Rathnasuriya</li>
<li>for: 这个论文的目的是研究基于深度学习技术的 AI 系统如何对于基于自然语言规则的需求或政策进行行为。</li>
<li>methods: 该论文使用了人工批准和 OpenAI 的大语言模型自动匹配新的示例和政策来扩展 HateModerate 数据集。</li>
<li>results: 研究发现现有的 hate speech 检测软件对于某些政策有高失败率，而自动匹配新的示例和政策可以提高 AI 系统对于需求或政策的traceability。<details>
<summary>Abstract</summary>
In the recent years, many software systems have adopted AI techniques, especially deep learning techniques. Due to their black-box nature, AI-based systems brought challenges to traceability, because AI system behaviors are based on models and data, whereas the requirements or policies are rules in the form of natural or programming language. To the best of our knowledge, there is a limited amount of studies on how AI and deep neural network-based systems behave against rule-based requirements/policies. This experience paper examines deep neural network behaviors against rule-based requirements described in natural language policies. In particular, we focus on a case study to check AI-based content moderation software against content moderation policies. First, using crowdsourcing, we collect natural language test cases which match each moderation policy, we name this dataset HateModerate; second, using the test cases in HateModerate, we test the failure rates of state-of-the-art hate speech detection software, and we find that these models have high failure rates for certain policies; finally, since manual labeling is costly, we further proposed an automated approach to augument HateModerate by finetuning OpenAI's large language models to automatically match new examples to policies. The dataset and code of this work can be found on our anonymous website: \url{https://sites.google.com/view/content-moderation-project}.
</details>
<details>
<summary>摘要</summary>
Recently, many software systems have adopted AI techniques, especially deep learning techniques. Due to their black-box nature, AI-based systems have brought challenges to traceability, as their behaviors are based on models and data, whereas the requirements or policies are rules in the form of natural or programming language. To the best of our knowledge, there is a limited amount of studies on how AI and deep neural network-based systems behave against rule-based requirements/policies. This experience paper examines deep neural network behaviors against rule-based requirements described in natural language policies. In particular, we focus on a case study to check AI-based content moderation software against content moderation policies. First, using crowdsourcing, we collect natural language test cases that match each moderation policy, which we name HateModerate; second, using the test cases in HateModerate, we test the failure rates of state-of-the-art hate speech detection software and find that these models have high failure rates for certain policies; finally, since manual labeling is costly, we further propose an automated approach to augment HateModerate by finetuning OpenAI's large language models to automatically match new examples to policies. The dataset and code of this work can be found on our anonymous website: [https://sites.google.com/view/content-moderation-project](https://sites.google.com/view/content-moderation-project).Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and other countries. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="CommonsenseVIS-Visualizing-and-Understanding-Commonsense-Reasoning-Capabilities-of-Natural-Language-Models"><a href="#CommonsenseVIS-Visualizing-and-Understanding-Commonsense-Reasoning-Capabilities-of-Natural-Language-Models" class="headerlink" title="CommonsenseVIS: Visualizing and Understanding Commonsense Reasoning Capabilities of Natural Language Models"></a>CommonsenseVIS: Visualizing and Understanding Commonsense Reasoning Capabilities of Natural Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12382">http://arxiv.org/abs/2307.12382</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingbo Wang, Renfei Huang, Zhihua Jin, Tianqing Fang, Huamin Qu</li>
<li>for: 本研究旨在提供一个可视化的解释系统，以协助NLPT专家对模型对概念的关系进行可视化分析。</li>
<li>methods: 本研究使用了外部常识库来将模型的行为与人类知识相互对映，以提高模型的可视化解释。</li>
<li>results: 经过User Study，我们发现CommonsenseVIS可以帮助NLPT专家在不同情况下进行系统性和批量的可视化分析，从而更好地理解模型对概念的关系。<details>
<summary>Abstract</summary>
Recently, large pretrained language models have achieved compelling performance on commonsense benchmarks. Nevertheless, it is unclear what commonsense knowledge the models learn and whether they solely exploit spurious patterns. Feature attributions are popular explainability techniques that identify important input concepts for model outputs. However, commonsense knowledge tends to be implicit and rarely explicitly presented in inputs. These methods cannot infer models' implicit reasoning over mentioned concepts. We present CommonsenseVIS, a visual explanatory system that utilizes external commonsense knowledge bases to contextualize model behavior for commonsense question-answering. Specifically, we extract relevant commonsense knowledge in inputs as references to align model behavior with human knowledge. Our system features multi-level visualization and interactive model probing and editing for different concepts and their underlying relations. Through a user study, we show that CommonsenseVIS helps NLP experts conduct a systematic and scalable visual analysis of models' relational reasoning over concepts in different situations.
</details>
<details>
<summary>摘要</summary>
To address this challenge, we propose CommonsenseVIS, a visual explanatory system that leverages external common sense knowledge bases to contextualize model behavior for common sense question-answering. Specifically, we extract relevant common sense knowledge from inputs and use it to align the model's behavior with human knowledge. Our system features multi-level visualization and interactive model probing and editing for different concepts and their underlying relations.Through a user study, we demonstrate that CommonsenseVIS helps NLP experts conduct a systematic and scalable visual analysis of the models' relational reasoning over concepts in different situations. By providing a visual interface for exploring the models' behavior, CommonsenseVIS enables experts to gain a deeper understanding of how the models are using common sense knowledge to make predictions. This can help improve the models' performance and ensure that they are making accurate and informed decisions.
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Emotional-Nuances-in-Dialogue-Summarization"><a href="#Evaluating-Emotional-Nuances-in-Dialogue-Summarization" class="headerlink" title="Evaluating Emotional Nuances in Dialogue Summarization"></a>Evaluating Emotional Nuances in Dialogue Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12371">http://arxiv.org/abs/2307.12371</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongxin Zhou, Fabien Ringeval, François Portet</li>
<li>for: 本研究旨在提高对人工对话的自动概要summarization，以保留对话中情感内容的信息。</li>
<li>methods: 本文提出了一组名为$PEmo$的衡量量表，用于衡量对话概要中情感内容的保留情况。</li>
<li>results: 研究发现，现有的概要模型不太好地保留对话中情感内容，而且通过减少训练集中不情感对话，可以更好地保留情感内容，同时保留最重要的事实信息。<details>
<summary>Abstract</summary>
Automatic dialogue summarization is a well-established task that aims to identify the most important content from human conversations to create a short textual summary. Despite recent progress in the field, we show that most of the research has focused on summarizing the factual information, leaving aside the affective content, which can yet convey useful information to analyse, monitor, or support human interactions. In this paper, we propose and evaluate a set of measures $PEmo$, to quantify how much emotion is preserved in dialog summaries. Results show that, summarization models of the state-of-the-art do not preserve well the emotional content in the summaries. We also show that by reducing the training set to only emotional dialogues, the emotional content is better preserved in the generated summaries, while conserving the most salient factual information.
</details>
<details>
<summary>摘要</summary>
自动对话摘要是一个已经成熟的任务，目的是从人类对话中提取最重要的内容，创建简短的文本摘要。尽管最近的进步在这个领域，但大多数研究仍然专注于摘要的事实信息，忽略了情感内容，这种内容可以带来有用的信息，分析、监测或支持人类交流。在这篇论文中，我们提出并评估了一组测量方法$PEmo$,以量化对话摘要中情感内容的保留程度。结果表明，现有的摘要模型并不能很好地保留对话中的情感内容。我们还表明，通过将训练集限制为只包含情感对话，可以更好地保留对话摘要中的情感内容，同时保留最重要的事实信息。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/24/cs.CL_2023_07_24/" data-id="cloqtaent0088gh88bed55ktm" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/71/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/70/">70</a><a class="page-number" href="/page/71/">71</a><span class="page-number current">72</span><a class="page-number" href="/page/73/">73</a><a class="page-number" href="/page/74/">74</a><span class="space">&hellip;</span><a class="page-number" href="/page/88/">88</a><a class="extend next" rel="next" href="/page/73/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">120</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">59</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">117</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">68</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">50</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
