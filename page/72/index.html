
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/72/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-eess.IV_2023_07_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/19/eess.IV_2023_07_19/" class="article-date">
  <time datetime="2023-07-19T09:00:00.000Z" itemprop="datePublished">2023-07-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/19/eess.IV_2023_07_19/">eess.IV - 2023-07-19</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Flexible-Physical-Unclonable-Functions-based-on-non-deterministically-distributed-Dye-Doped-Fibers-and-Droplets"><a href="#Flexible-Physical-Unclonable-Functions-based-on-non-deterministically-distributed-Dye-Doped-Fibers-and-Droplets" class="headerlink" title="Flexible Physical Unclonable Functions based on non-deterministically distributed Dye-Doped Fibers and Droplets"></a>Flexible Physical Unclonable Functions based on non-deterministically distributed Dye-Doped Fibers and Droplets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11000">http://arxiv.org/abs/2308.11000</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mauro Daniel Luigi Bruno, Giuseppe Emanuele Lio, Antonio Ferraro, Sara Nocentini, Giuseppe Papuzzo, Agostino Forestiero, Giovanni Desiderio, Maria Penelope De Santo, Diederik Sybolt Wiersma, Roberto Caputo, Giovanni Golemme, Francesco Riboli, Riccardo Cristoforo Barberi</li>
<li>for: 这个论文的目的是开发一种新的防伪技术，以保护日常生活中的商品免受伪制品的侵害。</li>
<li>methods: 这篇论文使用了电子涂敷和电子喷涂技术，制备了具有不同Physical Unclonable Function（PUF）密钥的灵活自由浮现膜。</li>
<li>results: 这种新技术可以生成三种加密级别的抗伪标签：一、用激发辐射的聚合物滴定粒子图案，非 deterministic 地分布在聚合物纤维中；二、每个标签具有独特的辐射特征；三、使用强大的物理不可克隆功能进行挑战-响应协议（CRP）标识。这种颜色的聚合物标签具有简单、便宜的制造过程，以及多级身份验证，因此成为实用的安全保护解决方案。<details>
<summary>Abstract</summary>
The development of new anti-counterfeiting solutions is a constant challenge and involves several research fields. Much interest is devoted to systems that are impossible to clone, based on the Physical Unclonable Function (PUF) paradigm. In this work, new strategies based on electrospinning and electrospraying of dye-doped polymeric materials are presented for the manufacturing of flexible free-standing films that embed different PUF keys. Films can be used to fabricate anticounterfeiting labels having three encryption levels: i) a map of fluorescent polymer droplets, with non deterministic positions on a dense yarn of polymer nanofibers; ii) a characteristic fluorescence spectrum for each label; iii) a challenge-response pairs (CRPs) identification protocol based on the strong nature of the physical unclonable function. The intrinsic uniqueness introduced by the deposition techniques encodes enough complexity into the optical anti-counterfeiting tag to generate thousands of cryptographic keys. The simple and cheap fabrication process as well as the multilevel authentication makes such colored polymeric unclonable tags a practical solution in the secure protection of merchandise in our daily life.
</details>
<details>
<summary>摘要</summary>
发展新的反伪技术是一constant challenge，涉及多个研究领域。大量的研究着眼于基于物理不可克隆函数（PUF）理念的系统，以实现不可克隆的安全标签。在这项工作中，我们提出了基于电子涂敷和电子涂敷的染料含有聚合物材料的新策略，用于制造弹性自由standing films，以实现不同PUF钥匙的嵌入。这些涂敷可以用于制造反伪标签，具有三级加密：1. 染料液体的地图，具有不确定的位置在密集的聚合物纤维中；2. 每个标签的特有荧光谱，作为唯一标识符；3. 基于物理不可克隆函数的挑战回应协议（CRP），用于实现多级身份验证。电子涂敷和电子涂敷的固有独特性会将复杂度嵌入到光学反伪标签中，生成数千个 криптографиic钥匙。这种简单便宜的制造过程以及多级身份验证，使得这种颜色含有聚合物的不可克隆标签成为日常生活中安全财物保护的实用解决方案。
</details></li>
</ul>
<hr>
<h2 id="Blind-Image-Quality-Assessment-Using-Multi-Stream-Architecture-with-Spatial-and-Channel-Attention"><a href="#Blind-Image-Quality-Assessment-Using-Multi-Stream-Architecture-with-Spatial-and-Channel-Attention" class="headerlink" title="Blind Image Quality Assessment Using Multi-Stream Architecture with Spatial and Channel Attention"></a>Blind Image Quality Assessment Using Multi-Stream Architecture with Spatial and Channel Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09857">http://arxiv.org/abs/2307.09857</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hassan Khalid, Nisar Ahmed</li>
<li>for: 本研究旨在提出一种多流程空间频道注意力基于的盲图像质量评估算法，以提高图像质量评估的准确性和人类评估的相关性。</li>
<li>methods: 该算法首先使用两种不同的背景网络生成混合特征，然后通过空间和频道注意力来为重要区域提供高重量。</li>
<li>results: 研究使用四个传统图像质量评估数据集验证了该提议的效果，并通过真实和synthetic零损像库展示了该方法在不同类型的损害下的普适性和准确性。<details>
<summary>Abstract</summary>
BIQA (Blind Image Quality Assessment) is an important field of study that evaluates images automatically. Although significant progress has been made, blind image quality assessment remains a difficult task since images vary in content and distortions. Most algorithms generate quality without emphasizing the important region of interest. In order to solve this, a multi-stream spatial and channel attention-based algorithm is being proposed. This algorithm generates more accurate predictions with a high correlation to human perceptual assessment by combining hybrid features from two different backbones, followed by spatial and channel attention to provide high weights to the region of interest. Four legacy image quality assessment datasets are used to validate the effectiveness of our proposed approach. Authentic and synthetic distortion image databases are used to demonstrate the effectiveness of the proposed method, and we show that it has excellent generalization properties with a particular focus on the perceptual foreground information.
</details>
<details>
<summary>摘要</summary>
BIQA (自动图像质量评估) 是一个重要的研究领域，它自动评估图像的质量。虽然有了 significante 进步，但图像质量评估仍然是一个困难的任务，因为图像的内容和扭曲都很多。大多数算法生成的质量不强调关键区域的兴趣。为解决这个问题，我们提出了一种多流扩展 spatial 和通道注意力基于算法。这个算法可以将混合特征从两个不同的背板组合起来，然后使用空间和通道注意力提供高重要性的区域。我们使用四个传统的图像质量评估数据集来验证我们的提议的有效性。我们还使用了authentic和synthetic扭曲图像库来证明我们的方法的普适性，并显示它在特定的感知前景信息方面具有出色的泛化性。
</details></li>
</ul>
<hr>
<h2 id="Cryo-forum-A-framework-for-orientation-recovery-with-uncertainty-measure-with-the-application-in-cryo-EM-image-analysis"><a href="#Cryo-forum-A-framework-for-orientation-recovery-with-uncertainty-measure-with-the-application-in-cryo-EM-image-analysis" class="headerlink" title="Cryo-forum: A framework for orientation recovery with uncertainty measure with the application in cryo-EM image analysis"></a>Cryo-forum: A framework for orientation recovery with uncertainty measure with the application in cryo-EM image analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09847">http://arxiv.org/abs/2307.09847</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/phonchi/cryo-forum">https://github.com/phonchi/cryo-forum</a></li>
<li>paper_authors: Szu-Chi Chung</li>
<li>for: 这个论文的目的是提出一种新的方法来确定二维晶体电子显微镜图像的方向参数，以便重建三维结构。</li>
<li>methods: 这种方法使用深度学习算法，将2D图像的方向参数表示为10维特征向量，并使用几何约束来预测方向参数。</li>
<li>results: 数学分析表明，这种方法可以从2D晶体电子显微镜图像中有效地回收方向参数，并且可以直接从3D数据中清理噪声。此外，这种方法还提供了一个用户友好的软件套件，名为cryo-forum，以便方便开发者使用。<details>
<summary>Abstract</summary>
In single-particle cryo-electron microscopy (cryo-EM), the efficient determination of orientation parameters for 2D projection images poses a significant challenge yet is crucial for reconstructing 3D structures. This task is complicated by the high noise levels present in the cryo-EM datasets, which often include outliers, necessitating several time-consuming 2D clean-up processes. Recently, solutions based on deep learning have emerged, offering a more streamlined approach to the traditionally laborious task of orientation estimation. These solutions often employ amortized inference, eliminating the need to estimate parameters individually for each image. However, these methods frequently overlook the presence of outliers and may not adequately concentrate on the components used within the network. This paper introduces a novel approach that uses a 10-dimensional feature vector to represent the orientation and applies a Quadratically-Constrained Quadratic Program to derive the predicted orientation as a unit quaternion, supplemented by an uncertainty metric. Furthermore, we propose a unique loss function that considers the pairwise distances between orientations, thereby enhancing the accuracy of our method. Finally, we also comprehensively evaluate the design choices involved in constructing the encoder network, a topic that has not received sufficient attention in the literature. Our numerical analysis demonstrates that our methodology effectively recovers orientations from 2D cryo-EM images in an end-to-end manner. Importantly, the inclusion of uncertainty quantification allows for direct clean-up of the dataset at the 3D level. Lastly, we package our proposed methods into a user-friendly software suite named cryo-forum, designed for easy accessibility by the developers.
</details>
<details>
<summary>摘要</summary>
Single-particle cryo-electron microscopy (cryo-EM) 的高精度Orientation parameter determination for 2D projection images is a significant challenge, yet it is crucial for reconstructing 3D structures. This task is complicated by the high noise levels present in cryo-EM datasets, which often include outliers, necessitating several time-consuming 2D clean-up processes. Recently, deep learning-based solutions have emerged, offering a more streamlined approach to the traditionally laborious task of orientation estimation. These solutions often employ amortized inference, eliminating the need to estimate parameters individually for each image. However, these methods frequently overlook the presence of outliers and may not adequately concentrate on the components used within the network.This paper introduces a novel approach that uses a 10-dimensional feature vector to represent orientation and applies a Quadratically-Constrained Quadratic Program to derive the predicted orientation as a unit quaternion, supplemented by an uncertainty metric. Furthermore, we propose a unique loss function that considers the pairwise distances between orientations, thereby enhancing the accuracy of our method. Finally, we comprehensively evaluate the design choices involved in constructing the encoder network, a topic that has not received sufficient attention in the literature. Our numerical analysis demonstrates that our methodology effectively recovers orientations from 2D cryo-EM images in an end-to-end manner. Importantly, the inclusion of uncertainty quantification allows for direct clean-up of the dataset at the 3D level. Lastly, we package our proposed methods into a user-friendly software suite named cryo-forum, designed for easy accessibility by developers.
</details></li>
</ul>
<hr>
<h2 id="Compressive-Image-Scanning-Microscope"><a href="#Compressive-Image-Scanning-Microscope" class="headerlink" title="Compressive Image Scanning Microscope"></a>Compressive Image Scanning Microscope</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09841">http://arxiv.org/abs/2307.09841</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ajay Gunalan, Marco Castello, Simonluca Piazza, Shunlei Li, Alberto Diaspro, Leonardo S. Mattos, Paolo Bianchini</li>
<li>for: 提高laser扫描镜微scopic imaging（ISM）的图像质量和数据获取速度</li>
<li>methods: 使用单 photon 致暴流器（SPAD）数组探测器和固定抽象策略，通过并行扫描来提高图像重建质量，同时降低计算样式矩阵的时间和数据获取时间</li>
<li>results: 实验结果表明，该方法可以提供高质量的图像，同时降低数据获取时间和可能性的辐照热问题<details>
<summary>Abstract</summary>
We present a novel approach to implement compressive sensing in laser scanning microscopes (LSM), specifically in image scanning microscopy (ISM), using a single-photon avalanche diode (SPAD) array detector. Our method addresses two significant limitations in applying compressive sensing to LSM: the time to compute the sampling matrix and the quality of reconstructed images. We employ a fixed sampling strategy, skipping alternate rows and columns during data acquisition, which reduces the number of points scanned by a factor of four and eliminates the need to compute different sampling matrices. By exploiting the parallel images generated by the SPAD array, we improve the quality of the reconstructed compressive-ISM images compared to standard compressive confocal LSM images. Our results demonstrate the effectiveness of our approach in producing higher-quality images with reduced data acquisition time and potential benefits in reducing photobleaching.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法，用于在激光扫描镜icroscopes（LSM）中实现压缩感知，具体是在图像扫描微scopy（ISM）中使用单 photon 爆发层（SPAD）数组探测器。我们的方法解决了应用压缩感知到 LSM 的两个主要限制：计算样本矩阵的时间和重建图像的质量。我们采用固定样本策略，在数据收集过程中跳过 alternate 行和列，从而将数据点数减少为四分之一，并消除计算不同样本矩阵的需求。通过利用 SPAD 数组产生的平行图像，我们提高了压缩-ISM 图像的重建质量，相比标准压缩扫描LSM 图像。我们的结果表明我们的方法可以生成高质量图像，降低数据收集时间和避免photobleaching。
</details></li>
</ul>
<hr>
<h2 id="Fix-your-downsampling-ASAP-Be-natively-more-robust-via-Aliasing-and-Spectral-Artifact-free-Pooling"><a href="#Fix-your-downsampling-ASAP-Be-natively-more-robust-via-Aliasing-and-Spectral-Artifact-free-Pooling" class="headerlink" title="Fix your downsampling ASAP! Be natively more robust via Aliasing and Spectral Artifact free Pooling"></a>Fix your downsampling ASAP! Be natively more robust via Aliasing and Spectral Artifact free Pooling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09804">http://arxiv.org/abs/2307.09804</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julia Grabinski, Janis Keuper, Margret Keuper</li>
<li>for: 本文旨在提高卷积神经网络的Native robustness，即对常见损害和攻击而言的抗颤性。</li>
<li>methods: 本文使用了FLC pooling和ASAP pooling两种方法来降采样，其中ASAP pooling在频域降采样中具有较高的抗颤性。</li>
<li>results: 对于常见损害和攻击，使用ASAP pooling的网络展现出较高的Native robustness，并且与基eline相当或者超过基eline的clean accuracy。<details>
<summary>Abstract</summary>
Convolutional neural networks encode images through a sequence of convolutions, normalizations and non-linearities as well as downsampling operations into potentially strong semantic embeddings. Yet, previous work showed that even slight mistakes during sampling, leading to aliasing, can be directly attributed to the networks' lack in robustness. To address such issues and facilitate simpler and faster adversarial training, [12] recently proposed FLC pooling, a method for provably alias-free downsampling - in theory. In this work, we conduct a further analysis through the lens of signal processing and find that such current pooling methods, which address aliasing in the frequency domain, are still prone to spectral leakage artifacts. Hence, we propose aliasing and spectral artifact-free pooling, short ASAP. While only introducing a few modifications to FLC pooling, networks using ASAP as downsampling method exhibit higher native robustness against common corruptions, a property that FLC pooling was missing. ASAP also increases native robustness against adversarial attacks on high and low resolution data while maintaining similar clean accuracy or even outperforming the baseline.
</details>
<details>
<summary>摘要</summary>
convolutional neural networks 通过一系列卷积、 нор化和非线性转换，以及下采样操作转化图像为强式 semantic embedding。然而，前一些研究表明， Even slight mistakes during sampling, leading to aliasing, can be directly attributed to the networks' lack in robustness。 To address such issues and facilitate simpler and faster adversarial training, 在 [12] 提出了FLC pooling，一种可以 theoretically guarantee alias-free downsampling的方法。在这项工作中，我们通过信号处理的视角进行进一步的分析，发现现有的 pooling 方法，通过频域 Addressing aliasing, still prone to spectral leakage artifacts。因此，我们提出了防止抽象和频域泄漏的 pooling，简称ASAP。尽管ASAP只对 FLC pooling 进行了一些修改，但是使用ASAP作为下采样方法的网络在常见损害和 adversarial attack 上表现出更高的Native robustness，同时保持相似的清洁精度或者甚至超过基eline。
</details></li>
</ul>
<hr>
<h2 id="DiffDP-Radiotherapy-Dose-Prediction-via-a-Diffusion-Model"><a href="#DiffDP-Radiotherapy-Dose-Prediction-via-a-Diffusion-Model" class="headerlink" title="DiffDP: Radiotherapy Dose Prediction via a Diffusion Model"></a>DiffDP: Radiotherapy Dose Prediction via a Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09794">http://arxiv.org/abs/2307.09794</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/scufzh/DiffDP">https://github.com/scufzh/DiffDP</a></li>
<li>paper_authors: Zhenghao Feng, Lu Wen, Peng Wang, Binyu Yan, Xi Wu, Jiliu Zhou, Yan Wang</li>
<li>for: 这个研究旨在提高辐射疗法规划中的剂量分布预测精度，并解决现有方法的过滤问题，使其能够更好地捕捉病人组织内部的剂量分布。</li>
<li>methods: 本研究提出了一个扩散基于的剂量预测模型（DiffDP），它包括一个前向过程和一个反向过程。在前向过程中，DiffDP将剂量分布图transformed为 Gaussian 噪声，并训练一个噪声预测器来预测加入每个时间步骤中的噪声。在反向过程中，它从原始 Gaussian 噪声中移除噪声，并使用已经训练好的噪声预测器，在多个步骤中重新生成原始剂量分布图。</li>
<li>results: 实验结果显示，DiffDP 模型能够更好地预测病人的剂量分布，并且能够更好地捕捉病人组织内部的剂量分布。此外，DiffDP 模型还能够对病人的临床特点进行更好的考虑，例如临床检查结果和病人的生物物理特点。<details>
<summary>Abstract</summary>
Currently, deep learning (DL) has achieved the automatic prediction of dose distribution in radiotherapy planning, enhancing its efficiency and quality. However, existing methods suffer from the over-smoothing problem for their commonly used L_1 or L_2 loss with posterior average calculations. To alleviate this limitation, we innovatively introduce a diffusion-based dose prediction (DiffDP) model for predicting the radiotherapy dose distribution of cancer patients. Specifically, the DiffDP model contains a forward process and a reverse process. In the forward process, DiffDP gradually transforms dose distribution maps into Gaussian noise by adding small noise and trains a noise predictor to predict the noise added in each timestep. In the reverse process, it removes the noise from the original Gaussian noise in multiple steps with the well-trained noise predictor and finally outputs the predicted dose distribution map. To ensure the accuracy of the prediction, we further design a structure encoder to extract anatomical information from patient anatomy images and enable the noise predictor to be aware of the dose constraints within several essential organs, i.e., the planning target volume and organs at risk. Extensive experiments on an in-house dataset with 130 rectum cancer patients demonstrate the s
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="CPCM-Contextual-Point-Cloud-Modeling-for-Weakly-supervised-Point-Cloud-Semantic-Segmentation"><a href="#CPCM-Contextual-Point-Cloud-Modeling-for-Weakly-supervised-Point-Cloud-Semantic-Segmentation" class="headerlink" title="CPCM: Contextual Point Cloud Modeling for Weakly-supervised Point Cloud Semantic Segmentation"></a>CPCM: Contextual Point Cloud Modeling for Weakly-supervised Point Cloud Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10316">http://arxiv.org/abs/2307.10316</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lizhaoliu-Lec/CPCM">https://github.com/lizhaoliu-Lec/CPCM</a></li>
<li>paper_authors: Lizhao Liu, Zhuangwei Zhuang, Shangxin Huang, Xunlong Xiao, Tianhang Xiang, Cen Chen, Jingdong Wang, Mingkui Tan</li>
<li>for: 提高 weakly-supervised 点云Semantic segmentation 的精度，减少 dense annotations 的成本。</li>
<li>methods: 使用 RegionMask 策略和 CMT 方法，结合 masked modeling 技术，从稀疏标注点Cloud中提取上下文信息。</li>
<li>results: 在 ScanNet V2 和 S3DIS 测试集上，CPCM 方法比 state-of-the-art 高效。<details>
<summary>Abstract</summary>
We study the task of weakly-supervised point cloud semantic segmentation with sparse annotations (e.g., less than 0.1% points are labeled), aiming to reduce the expensive cost of dense annotations. Unfortunately, with extremely sparse annotated points, it is very difficult to extract both contextual and object information for scene understanding such as semantic segmentation. Motivated by masked modeling (e.g., MAE) in image and video representation learning, we seek to endow the power of masked modeling to learn contextual information from sparsely-annotated points. However, directly applying MAE to 3D point clouds with sparse annotations may fail to work. First, it is nontrivial to effectively mask out the informative visual context from 3D point clouds. Second, how to fully exploit the sparse annotations for context modeling remains an open question. In this paper, we propose a simple yet effective Contextual Point Cloud Modeling (CPCM) method that consists of two parts: a region-wise masking (RegionMask) strategy and a contextual masked training (CMT) method. Specifically, RegionMask masks the point cloud continuously in geometric space to construct a meaningful masked prediction task for subsequent context learning. CMT disentangles the learning of supervised segmentation and unsupervised masked context prediction for effectively learning the very limited labeled points and mass unlabeled points, respectively. Extensive experiments on the widely-tested ScanNet V2 and S3DIS benchmarks demonstrate the superiority of CPCM over the state-of-the-art.
</details>
<details>
<summary>摘要</summary>
我们研究弱监督点云 semantic segmentation的任务，将点云中的少于0.1%的点为标签，以减少密集标签的成本。然而，具有极端罕 annotated points 的情况下，很难从点云中提取情感和物件信息，进而实现Scene Understanding。我们受到遮罩modeling（例如MAE）的启发，尝试将这种能力应用到弱监督点云中。然而，直接将MAE应用到3D点云中可能无法工作。首先，从3D点云中效iveness地遮罩出有用的视觉上下文是一个问题。其次，如何充分利用仅有的罕 annotated points 进行上下文学习仍然是一个开问题。在这篇论文中，我们提出了一个简单 yet effective的Contextual Point Cloud Modeling（CPCM）方法，包括两个部分：RegionMask 策略和CMT 方法。具体来说，RegionMask 策略在 geometric space 中连续遮罩点云，以建立有意义的遮罩预测任务，并且CMT 方法将这些预测任务与仅有的罕 annotated points 进行协同学习，以实现有限的标签点和大量的点云中的上下文学习。我们对 ScanNet V2 和 S3DIS 两个测试集进行了广泛的实验，结果显示CPCM 方法在State-of-the-art 之上。
</details></li>
</ul>
<hr>
<h2 id="NTIRE-2023-Quality-Assessment-of-Video-Enhancement-Challenge"><a href="#NTIRE-2023-Quality-Assessment-of-Video-Enhancement-Challenge" class="headerlink" title="NTIRE 2023 Quality Assessment of Video Enhancement Challenge"></a>NTIRE 2023 Quality Assessment of Video Enhancement Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09729">http://arxiv.org/abs/2307.09729</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaohong Liu, Xiongkuo Min, Wei Sun, Yulun Zhang, Kai Zhang, Radu Timofte, Guangtao Zhai, Yixuan Gao, Yuqin Cao, Tengchuan Kou, Yunlong Dong, Ziheng Jia, Yilin Li, Wei Wu, Shuming Hu, Sibin Deng, Pengxiang Xiao, Ying Chen, Kai Li, Kai Zhao, Kun Yuan, Ming Sun, Heng Cong, Hao Wang, Lingzhi Fu, Yusheng Zhang, Rongyu Zhang, Hang Shi, Qihang Xu, Longan Xiao, Zhiliang Ma, Mirko Agarla, Luigi Celona, Claudio Rota, Raimondo Schettini, Zhiwei Huang, Yanan Li, Xiaotao Wang, Lei Lei, Hongye Liu, Wei Hong, Ironhead Chuang, Allen Lin, Drake Guan, Iris Chen, Kae Lou, Willy Huang, Yachun Tasi, Yvonne Kao, Haotian Fan, Fangyuan Kong, Shiqi Zhou, Hao Liu, Yu Lai, Shanshan Chen, Wenqi Wang, Haoning Wu, Chaofeng Chen, Chunzheng Zhu, Zekun Guo, Shiling Zhao, Haibing Yin, Hongkui Wang, Hanene Brachemi Meftah, Sid Ahmed Fezza, Wassim Hamidouche, Olivier Déforges, Tengfei Shi, Azadeh Mansouri, Hossein Motamednia, Amir Hossein Bakhtiari, Ahmad Mahmoudi Aznaveh</li>
<li>for: 这个论文旨在提出一个视频提升质量评估挑战，用于解决视频处理领域中的主要挑战之一——视频质量评估（VQA）。</li>
<li>methods: 这个挑战使用了VDPVE数据集，包括1211个加强视频，其中600个视频有颜色、亮度和对比度加强，310个视频有去锐加强，301个视频有去偏移加强。挑战共有167个注册参与者。</li>
<li>results: 挑战共收到了61个参与者的预测结果，共计3168个提交。最终，37个参与者在最终测试阶段提交了176个模型和相关的说明文档，并详细介绍了他们使用的方法。一些方法的结果比基线方法更好，而赢家的方法还有出色的预测性能。<details>
<summary>Abstract</summary>
This paper reports on the NTIRE 2023 Quality Assessment of Video Enhancement Challenge, which will be held in conjunction with the New Trends in Image Restoration and Enhancement Workshop (NTIRE) at CVPR 2023. This challenge is to address a major challenge in the field of video processing, namely, video quality assessment (VQA) for enhanced videos. The challenge uses the VQA Dataset for Perceptual Video Enhancement (VDPVE), which has a total of 1211 enhanced videos, including 600 videos with color, brightness, and contrast enhancements, 310 videos with deblurring, and 301 deshaked videos. The challenge has a total of 167 registered participants. 61 participating teams submitted their prediction results during the development phase, with a total of 3168 submissions. A total of 176 submissions were submitted by 37 participating teams during the final testing phase. Finally, 19 participating teams submitted their models and fact sheets, and detailed the methods they used. Some methods have achieved better results than baseline methods, and the winning methods have demonstrated superior prediction performance.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Uncertainty-Driven-Multi-Scale-Feature-Fusion-Network-for-Real-time-Image-Deraining"><a href="#Uncertainty-Driven-Multi-Scale-Feature-Fusion-Network-for-Real-time-Image-Deraining" class="headerlink" title="Uncertainty-Driven Multi-Scale Feature Fusion Network for Real-time Image Deraining"></a>Uncertainty-Driven Multi-Scale Feature Fusion Network for Real-time Image Deraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09728">http://arxiv.org/abs/2307.09728</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ming Tong, Xuefeng Yan, Yongzhen Wang</li>
<li>for: 提高雨水影响下视觉测量系统的精度和可靠性。</li>
<li>methods: 提出了一种基于不确定性的多尺度特征融合网络（UMFFNet），通过learns probability mapping distribution между对应图像来估计不确定性。具体来说，我们引入了一种不确定性特征融合块（UFFB），通过利用不确定性信息来动态增强获取的特征和着眼于雨斑干扰的模糊区域，从而降低预测错误。</li>
<li>results: 对比 existed 图像抽取方法，UMFFNet 在雨水影响下的图像抽取 task 上表现出了显著的性能提升，并且具有少量参数和高效的特点。<details>
<summary>Abstract</summary>
Visual-based measurement systems are frequently affected by rainy weather due to the degradation caused by rain streaks in captured images, and existing imaging devices struggle to address this issue in real-time. While most efforts leverage deep networks for image deraining and have made progress, their large parameter sizes hinder deployment on resource-constrained devices. Additionally, these data-driven models often produce deterministic results, without considering their inherent epistemic uncertainty, which can lead to undesired reconstruction errors. Well-calibrated uncertainty can help alleviate prediction errors and assist measurement devices in mitigating risks and improving usability. Therefore, we propose an Uncertainty-Driven Multi-Scale Feature Fusion Network (UMFFNet) that learns the probability mapping distribution between paired images to estimate uncertainty. Specifically, we introduce an uncertainty feature fusion block (UFFB) that utilizes uncertainty information to dynamically enhance acquired features and focus on blurry regions obscured by rain streaks, reducing prediction errors. In addition, to further boost the performance of UMFFNet, we fused feature information from multiple scales to guide the network for efficient collaborative rain removal. Extensive experiments demonstrate that UMFFNet achieves significant performance improvements with few parameters, surpassing state-of-the-art image deraining methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Visual-based measurement systems 常常受到雨水的影响，因为捕捉图像中的雨斑会导致图像质量下降。现有的捕捉设备很难在实时中解决这个问题。大多数努力都是基于深度网络进行图像排除雨水，但这些模型很大，不能在资源受限的设备上部署。此外，这些数据驱动的模型经常生成决定性的结果，不考虑其内在的 epistemic 不确定性，这可能导致不想要的重建错误。Well-calibrated uncertainty 可以减轻预测错误和助measurement设备减轻风险，提高可用性。因此，我们提出了一个 Uncertainty-Driven Multi-Scale Feature Fusion Network (UMFFNet)，它学习了图像对的映射分布，以便估计 uncertainty。specifically，我们引入了一个 uncertainty 特征融合块 (UFFB)，它利用 uncertainty 信息来动态增强获取的特征，并专注于雨斑遮盖的模糊区域，从而减少预测错误。此外，为了进一步提高 UMFFNet 的性能，我们将特征信息从多个级别集成到导向网络，以便有效地协同除雨。广泛的实验表明，UMFFNet 可以在几个参数下达到显著性能提升，超过当前的图像排除雨水方法。
</details></li>
</ul>
<hr>
<h2 id="Flexible-single-multimode-fiber-imaging-using-white-LED"><a href="#Flexible-single-multimode-fiber-imaging-using-white-LED" class="headerlink" title="Flexible single multimode fiber imaging using white LED"></a>Flexible single multimode fiber imaging using white LED</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09714">http://arxiv.org/abs/2307.09714</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minyu Fan, Kun Liu, Jie Zhu, Yu Cao, Sha Wang</li>
<li>for: This research aims to improve the imaging capabilities of multimode fibers (MMFs) using white LEDs and cascaded convolutional neural networks (CNNs) to mitigate the effects of mode coupling and modal dispersion.</li>
<li>methods: The proposed method uses a MMF as the imaging medium, a white LED as the light source, and a cascaded CNN to reconstruct the images. The channel stitching technology is used to concatenate the output speckle patterns in three different color channels of the CCD camera.</li>
<li>results: The experimental results show that the proposed method achieves high-quality image reconstruction with an average Pearson correlation coefficient (PCC) of 0.83 on the Fashion-MINIST dataset. The method also demonstrates good robustness properties, maintaining an average PCC of 0.83 even after completely changing the shape of the MMF.<details>
<summary>Abstract</summary>
Multimode fiber (MMF) has been proven to have good potential in imaging and optical communication because of its advantages of small diameter and large mode numbers. However, due to the mode coupling and modal dispersion, it is very sensitive to environmental changes. Minor changes in the fiber shape can lead to difficulties in information reconstruction. Here, white LED and cascaded Unet are used to achieve MMF imaging to eliminate the effect of fiber perturbations. The output speckle patterns in three different color channels of the CCD camera produced by transferring images through the MMF are concatenated and inputted into the cascaded Unet using channel stitching technology to improve the reconstruction effects. The average Pearson correlation coefficient (PCC) of the reconstructed images from the Fashion-MINIST dataset is 0.83. In order to check the flexibility of such a system, perturbation tests on the image reconstruction capability by changing the fiber shapes are conducted. The experimental results show that the MMF imaging system has good robustness properties, i. e. the average PCC remains 0.83 even after completely changing the shape of the MMF. This research potentially provides a flexible approach for the practical application of MMF imaging.
</details>
<details>
<summary>摘要</summary>
多模式纤维（MMF）因其小直径和大模数的优点，在成像和光学通信中具有良好的潜力。然而，由于模式相互作用和模态扩散，MMF受环境变化的影响很敏感。小变化纤维形状可能导致信息重建困难。在本研究中，使用白色LED和堆式UNet实现MMF成像，以消除纤维异常的影响。通过将三个不同颜色通道的CCD相机输出的 спе克尔图像 concatenate 并输入到堆式UNet中，使用通道缝合技术进行改进重建效果。实验结果表明，MMF成像系统具有良好的灵活性特性，即平均相关系数（PCC）保持在0.83，即使完全改变纤维形状。这些研究可能为MMF成像的实际应用提供一种灵活的方法。
</details></li>
</ul>
<hr>
<h2 id="Transformer-based-Dual-domain-Network-for-Few-view-Dedicated-Cardiac-SPECT-Image-Reconstructions"><a href="#Transformer-based-Dual-domain-Network-for-Few-view-Dedicated-Cardiac-SPECT-Image-Reconstructions" class="headerlink" title="Transformer-based Dual-domain Network for Few-view Dedicated Cardiac SPECT Image Reconstructions"></a>Transformer-based Dual-domain Network for Few-view Dedicated Cardiac SPECT Image Reconstructions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09624">http://arxiv.org/abs/2307.09624</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huidong Xie, Bo Zhou, Xiongchao Chen, Xueqi Guo, Stephanie Thorn, Yi-Hwa Liu, Ge Wang, Albert Sinusas, Chi Liu<br>for:* 这个论文旨在提高使用GE 530&#x2F;570c专用卡ди亚斯PECT仪器进行心血管疾病诊断中的图像质量。methods:* 该方法使用了3D transformer-based dual-domain网络（TIP-Net）来实现高质量3D卡ди亚斯PECT图像重建。results:* 对人体研究来说，该方法可以提高卡ди亚斯PECT图像中心疾病潜在性的显示，并且与先前的基线方法进行比较，得到了更高的卡ди亚斯疾病潜在性图像质量。<details>
<summary>Abstract</summary>
Cardiovascular disease (CVD) is the leading cause of death worldwide, and myocardial perfusion imaging using SPECT has been widely used in the diagnosis of CVDs. The GE 530/570c dedicated cardiac SPECT scanners adopt a stationary geometry to simultaneously acquire 19 projections to increase sensitivity and achieve dynamic imaging. However, the limited amount of angular sampling negatively affects image quality. Deep learning methods can be implemented to produce higher-quality images from stationary data. This is essentially a few-view imaging problem. In this work, we propose a novel 3D transformer-based dual-domain network, called TIP-Net, for high-quality 3D cardiac SPECT image reconstructions. Our method aims to first reconstruct 3D cardiac SPECT images directly from projection data without the iterative reconstruction process by proposing a customized projection-to-image domain transformer. Then, given its reconstruction output and the original few-view reconstruction, we further refine the reconstruction using an image-domain reconstruction network. Validated by cardiac catheterization images, diagnostic interpretations from nuclear cardiologists, and defect size quantified by an FDA 510(k)-cleared clinical software, our method produced images with higher cardiac defect contrast on human studies compared with previous baseline methods, potentially enabling high-quality defect visualization using stationary few-view dedicated cardiac SPECT scanners.
</details>
<details>
<summary>摘要</summary>
Cardiovascular disease (CVD) is the leading cause of death worldwide, and myocardial perfusion imaging using SPECT has been widely used in the diagnosis of CVDs. The GE 530/570c dedicated cardiac SPECT scanners use a stationary geometry to simultaneously acquire 19 projections to increase sensitivity and achieve dynamic imaging. However, the limited amount of angular sampling negatively affects image quality. Deep learning methods can be implemented to produce higher-quality images from stationary data. This is essentially a few-view imaging problem. In this work, we propose a novel 3D transformer-based dual-domain network, called TIP-Net, for high-quality 3D cardiac SPECT image reconstructions. Our method aims to first reconstruct 3D cardiac SPECT images directly from projection data without the iterative reconstruction process by proposing a customized projection-to-image domain transformer. Then, given its reconstruction output and the original few-view reconstruction, we further refine the reconstruction using an image-domain reconstruction network. Validated by cardiac catheterization images, diagnostic interpretations from nuclear cardiologists, and defect size quantified by an FDA 510(k)-cleared clinical software, our method produced images with higher cardiac defect contrast on human studies compared with previous baseline methods, potentially enabling high-quality defect visualization using stationary few-view dedicated cardiac SPECT scanners.
</details></li>
</ul>
<hr>
<h2 id="A-comparative-analysis-of-SRGAN-models"><a href="#A-comparative-analysis-of-SRGAN-models" class="headerlink" title="A comparative analysis of SRGAN models"></a>A comparative analysis of SRGAN models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09456">http://arxiv.org/abs/2307.09456</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fatemeh Rezapoor Nikroo, Ajinkya Deshmukh, Anantha Sharma, Adrian Tam, Kaarthik Kumar, Cleo Norris, Aditya Dangi<br>for:* 这项研究评估了多种state-of-the-art SRGAN模型（ESRGAN、Real-ESRGAN和EDSR）在一个实际图像受损pipeline上的性能。methods:* 这些模型使用了state-of-the-art SRGAN网络 architecture。results:* 我们的结果表明，ESDR-BASE模型从huggingface库中出现，在量化指标和主观视觉质量评估中都有较高的表现，同时具有最小的计算负担。 Specifically, EDSR生成的图像具有更高的峰峰信号噪声比（PSNR）和结构相似性指标（SSIM）值，并且可以通过Tesseract OCR引擎返回高质量的OCR结果。这些发现表明，ESDR是一种稳定有效的单图像超分辨方法，可能特别适合需要高质量视觉准确性和优化计算的应用。<details>
<summary>Abstract</summary>
In this study, we evaluate the performance of multiple state-of-the-art SRGAN (Super Resolution Generative Adversarial Network) models, ESRGAN, Real-ESRGAN and EDSR, on a benchmark dataset of real-world images which undergo degradation using a pipeline. Our results show that some models seem to significantly increase the resolution of the input images while preserving their visual quality, this is assessed using Tesseract OCR engine. We observe that EDSR-BASE model from huggingface outperforms the remaining candidate models in terms of both quantitative metrics and subjective visual quality assessments with least compute overhead. Specifically, EDSR generates images with higher peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) values and are seen to return high quality OCR results with Tesseract OCR engine. These findings suggest that EDSR is a robust and effective approach for single-image super-resolution and may be particularly well-suited for applications where high-quality visual fidelity is critical and optimized compute.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们评估了多种现代SRGAN（超分解生成 adversarial网络）模型，ESRGAN、Real-ESRGAN和EDSR，在一个实际图像减退管道中的 benchmark 数据集上表现。我们的结果表明，一些模型可以明显提高输入图像的分辨率，同时保持视觉质量，这被评估使用 Tesseract OCR 引擎。我们发现，huggingface 中的 EDSR-BASE 模型在对比其他候选模型时，在量化指标和主观视觉质量评估中具有最低的计算开销。具体来说，EDSR 生成的图像具有更高的峰峰信号噪声比（PSNR）和结构相似性指标（SSIM）值，并且可以返回高质量 OCR 结果。这些发现表明，EDSR 是一种稳定和有效的单图超分解方法，可能适用于需要高品质视觉质量和优化计算的应用。
</details></li>
</ul>
<hr>
<h2 id="Measuring-Student-Behavioral-Engagement-using-Histogram-of-Actions"><a href="#Measuring-Student-Behavioral-Engagement-using-Histogram-of-Actions" class="headerlink" title="Measuring Student Behavioral Engagement using Histogram of Actions"></a>Measuring Student Behavioral Engagement using Histogram of Actions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09420">http://arxiv.org/abs/2307.09420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Abdelkawy, Islam Alkabbany, Asem Ali, Aly Farag</li>
<li>for: 本研究旨在开发一种新的学生行为参与度测量技术，通过识别学生的动作来预测学生行为参与度水平。</li>
<li>methods: 本研究使用人体骨骼模型来模拟学生姿势和上半身运动，并使用3D-CNN模型来学习学生上半身动作的动力学。然后，对每段2分钟视频中的动作进行识别，并将这些动作组织成一个历史gram仓库，用于SVM分类器来判断学生是否参与度高或低。</li>
<li>results: 实验结果表明，可以使用3D-CNN模型和SVM分类器来识别学生动作并预测学生参与度，并且可以 Capture the average engagement of the class。<details>
<summary>Abstract</summary>
In this paper, we propose a novel technique for measuring behavioral engagement through students' actions recognition. The proposed approach recognizes student actions then predicts the student behavioral engagement level. For student action recognition, we use human skeletons to model student postures and upper body movements. To learn the dynamics of student upper body, a 3D-CNN model is used. The trained 3D-CNN model is used to recognize actions within every 2minute video segment then these actions are used to build a histogram of actions which encodes the student actions and their frequencies. This histogram is utilized as an input to SVM classifier to classify whether the student is engaged or disengaged. To evaluate the proposed framework, we build a dataset consisting of 1414 2-minute video segments annotated with 13 actions and 112 video segments annotated with two engagement levels. Experimental results indicate that student actions can be recognized with top 1 accuracy 83.63% and the proposed framework can capture the average engagement of the class.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的方法来测量学生的行为参与度 durch 学生的动作识别。我们的提议方法首先识别学生的动作，然后预测学生的行为参与度水平。为了识别学生的动作，我们使用人体骨架来模拟学生的姿势和上半身运动。为了学习学生上半身的动力学，我们使用3D-CNN模型进行训练。训练完成后，我们使用3D-CNN模型来识别每个2分钟视频段中的动作，并将这些动作组织成一个动作分布图，该图表示学生的动作和其频率。这个动作分布图被用作SVM分类器的输入，以判断学生是否参与度。为评估我们的方框架，我们建立了一个数据集，该数据集包含1414个2分钟视频段，每个视频段被标注为13种动作，以及112个视频段被标注为两个参与度水平。实验结果表明，我们的方法可以识别学生的动作 WITH top 1 准确率83.63%，并且可以捕捉教室的平均参与度。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/19/eess.IV_2023_07_19/" data-id="cloimipgl011ws4884h1g1xnb" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_07_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/18/cs.SD_2023_07_18/" class="article-date">
  <time datetime="2023-07-18T15:00:00.000Z" itemprop="datePublished">2023-07-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/18/cs.SD_2023_07_18/">cs.SD - 2023-07-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Efficient-representation-of-head-related-transfer-functions-in-continuous-space-frequency-domains"><a href="#Efficient-representation-of-head-related-transfer-functions-in-continuous-space-frequency-domains" class="headerlink" title="Efficient representation of head-related transfer functions in continuous space-frequency domains"></a>Efficient representation of head-related transfer functions in continuous space-frequency domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09352">http://arxiv.org/abs/2307.09352</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam Szwajcowski</li>
<li>for: 这篇论文旨在探讨将圆柱卷积函数（SH）领域的继承方法替换为四维连续函数模型，以实现空间上的连续性。</li>
<li>methods: 论文使用了四维卷积函数（HSH）和圆柱卷积函数（SH） merged with one-dimensional basis functions，并对这两种方法进行比较。</li>
<li>results: 研究发现，使用四维连续函数模型可以将HRTF的大小谱спектроgram表示为一小组含义强的系数，这些系数可以在任何方向和频率下解码。HSH和SH merged with reverse Fourier-Bessel series表现最佳，其中HSH具有更好的压缩能力，对低级数据进行更高精度的重建。这些模型可以用于HRTF的 interpol、压缩和参数化，以及其他类型的直接函数的应用。<details>
<summary>Abstract</summary>
Utilizing spherical harmonic (SH) domain has been established as the default method of obtaining continuity over space in head-related transfer functions (HRTFs). This paper concerns different variants of extending this solution by replacing SHs with four-dimensional (4D) continuous functional models in which frequency is imagined as another physical dimension. Recently developed hyperspherical harmonic (HSH) representation is compared with models defined in spherindrical coordinate system by merging SHs with one-dimensional basis functions. The efficiency of both approaches is evaluated based on the reproduction errors for individual HRTFs from HUTUBS database, including detailed analysis of its dependency on chosen orders of approximation in frequency and space. Employing continuous functional models defined in 4D coordinate systems allows HRTF magnitude spectra to be expressed as a small set of coefficients which can be decoded back into values at any direction and frequency. The best performance was noted for HSHs and SHs merged with reverse Fourier-Bessel series, with the former featuring better compression abilities, achieving slightly higher accuracy for low number of coefficients. The presented models can serve multiple purposes, such as interpolation, compression or parametrization for machine learning applications, and can be applied not only to HRTFs but also to other types of directivity functions, e.g. sound source directivity.
</details>
<details>
<summary>摘要</summary>
utilizing 球面幂函数（SH）域已经被确立为HEAD相关传送函数（HRTF）的默认方法，这篇论文考虑了不同的扩展方案，其中 replacing SHs with four-dimensional（4D）连续函数模型，在这个模型中，频率被想象为空间中的另一个物理维度。最近开发的对称幂函数（HSH）表示与在圆柱坐标系中定义的模型相比较，并进行了详细的分析。这两种方法的效率被评估基于HRTF数据库中的重建错误，包括选择的频率和空间纬度的顺序方法的依赖关系。使用在4D坐标系中定义的连续函数模型，可以将HRTF的 магниту드 спектrum表示为一小组含义的系数，这些系数可以在任何方向和频率上解码。最佳性能被观察到在HSH和SH与反傅立埃尔-贝塞尔列表相加的情况下，前者具有更好的压缩能力，在低数量的系数下达到了微scopic的准确性。这些模型可以用于多种目的，如 interpolation、compression 或 parametrization for machine learning applications，并可以应用于其他类型的直径函数，如声源直径函数。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Timbre-Synthesis-using-Variational-Autoencoders-Regularized-on-Timbre-Descriptors"><a href="#Interpretable-Timbre-Synthesis-using-Variational-Autoencoders-Regularized-on-Timbre-Descriptors" class="headerlink" title="Interpretable Timbre Synthesis using Variational Autoencoders Regularized on Timbre Descriptors"></a>Interpretable Timbre Synthesis using Variational Autoencoders Regularized on Timbre Descriptors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10283">http://arxiv.org/abs/2307.10283</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anastasia Natsiou, Luca Longo, Sean O’Leary</li>
<li>for: 研究控制音色synthesizer的方法</li>
<li>methods: 使用深度神经网络和变量自动编码器（VAEs）生成音色表示</li>
<li>results: 提出了一种含timbre描述的准则化VAEs，并使用音色的幂数内容来缩小维度Here’s the translation in English for reference:</li>
<li>for: Research on controllable timbre synthesis methods</li>
<li>methods: Using deep neural networks and Variational Autoencoders (VAEs) to generate timbre representations</li>
<li>results: Proposed a regularized VAE-based latent space that incorporates timbre descriptors, and utilized harmonic content to reduce the dimensionality of the latent space.<details>
<summary>Abstract</summary>
Controllable timbre synthesis has been a subject of research for several decades, and deep neural networks have been the most successful in this area. Deep generative models such as Variational Autoencoders (VAEs) have the ability to generate a high-level representation of audio while providing a structured latent space. Despite their advantages, the interpretability of these latent spaces in terms of human perception is often limited. To address this limitation and enhance the control over timbre generation, we propose a regularized VAE-based latent space that incorporates timbre descriptors. Moreover, we suggest a more concise representation of sound by utilizing its harmonic content, in order to minimize the dimensionality of the latent space.
</details>
<details>
<summary>摘要</summary>
控制性 timbre 合成已经是研究的主题之一，深度神经网络是这个领域中最成功的。深度生成模型如 Variational Autoencoders (VAEs) 可以生成高级别的声音表示，并提供结构化的幂轨空间。然而，这些幂轨空间在人类听觉上的解释能力往往有限。为了解决这个限制并提高声音生成控制，我们提议使用带有声音描述符的幂轨空间，并提出一种更简洁的声音表示方法，通过利用声音的和弦内容来减少幂轨空间的维度。
</details></li>
</ul>
<hr>
<h2 id="On-Computing-In-the-Network-Covid-19-Coughs-Detection-Case-Study"><a href="#On-Computing-In-the-Network-Covid-19-Coughs-Detection-Case-Study" class="headerlink" title="On Computing In the Network: Covid-19 Coughs Detection Case Study"></a>On Computing In the Network: Covid-19 Coughs Detection Case Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08902">http://arxiv.org/abs/2307.08902</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soukaina Ouledsidi Ali, Zakaria Ait Hmitti, Halima Elbiaze, Roch Glitho</li>
<li>for: 该论文旨在探讨在网络设备（如交换机和网络interface卡）上进行计算，以实现时间敏感应用的质量服务目标。</li>
<li>methods: 论文比较了在云端-边缘-浮动 kontinuum 中进行计算和缓存任务的优势，并对 covid-19 警示应用在机场设置中进行了一个关键的使用场景。</li>
<li>results: 通过实验比较，论文表明，在网络设备内部进行计算可以更好地降低往返时间（RTT）和流量筛选。<details>
<summary>Abstract</summary>
Computing in the network (COIN) is a promising technology that allows processing to be carried out within network devices such as switches and network interface cards. Time sensitive application can achieve their quality of service (QoS) target by flexibly distributing the caching and computing tasks in the cloud-edge-mist continuum. This paper highlights the advantages of in-network computing, comparing to edge computing, in terms of latency and traffic filtering. We consider a critical use case related to Covid-19 alert application in an airport setting. Arriving travelers are monitored through cough analysis so that potentially infected cases can be detected and isolated for medical tests. A performance comparison has been done between an architecture using in-network computing and another one using edge computing. We show using simulations that in-network computing outperforms edge computing in terms of Round Trip Time (RTT) and traffic filtering.
</details>
<details>
<summary>摘要</summary>
计算在网络（COIN）是一种有前途的技术，允许在网络设备 such as 交换机和网络接口卡上进行处理。时间敏感应用可以通过在云端-边缘-浮云 continuum 中flexibly分配缓存和计算任务来实现质量服务（QoS）目标。本文highlights COIN的优势，比如边缘计算，在延迟和流量筛选方面。我们考虑了一个关键的covid-19预警应用场景，在机场设置下，来往旅客通过喊叫分析进行监测，检测和隔离涉疫患者进行医学测试。我们通过 simulated comparison 表明，使用COIN的architecture 在延迟（RTT）和流量筛选方面表现出色，高于边缘计算。
</details></li>
</ul>
<hr>
<h2 id="Multilingual-Speech-to-Speech-Translation-into-Multiple-Target-Languages"><a href="#Multilingual-Speech-to-Speech-Translation-into-Multiple-Target-Languages" class="headerlink" title="Multilingual Speech-to-Speech Translation into Multiple Target Languages"></a>Multilingual Speech-to-Speech Translation into Multiple Target Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08655">http://arxiv.org/abs/2307.08655</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongyu Gong, Ning Dong, Sravya Popuri, Vedanuj Goswami, Ann Lee, Juan Pino</li>
<li>for: 多语言speech-to-speech翻译 (S2ST)，即在不同语言之间的口头交流。</li>
<li>methods: 利用最新的直接S2ST技术，包括speech-to-unit和 vocoder，并将这些关键组件增加多语言能力。 specifically, the paper proposes a multilingual extension of S2U, called speech-to-masked-unit (S2MU), which applies masking to units that do not belong to the given target language to reduce language interference. Additionally, the paper proposes a multilingual vocoder trained with language embedding and the auxiliary loss of language identification.</li>
<li>results: 在多种benchmark翻译测试集上，提出的多语言模型比双语模型在英语到16种目标语言的翻译中表现更出色。<details>
<summary>Abstract</summary>
Speech-to-speech translation (S2ST) enables spoken communication between people talking in different languages. Despite a few studies on multilingual S2ST, their focus is the multilinguality on the source side, i.e., the translation from multiple source languages to one target language. We present the first work on multilingual S2ST supporting multiple target languages. Leveraging recent advance in direct S2ST with speech-to-unit and vocoder, we equip these key components with multilingual capability. Speech-to-masked-unit (S2MU) is the multilingual extension of S2U, which applies masking to units which don't belong to the given target language to reduce the language interference. We also propose multilingual vocoder which is trained with language embedding and the auxiliary loss of language identification. On benchmark translation testsets, our proposed multilingual model shows superior performance than bilingual models in the translation from English into $16$ target languages.
</details>
<details>
<summary>摘要</summary>
听说-听写翻译（S2ST）可以帮助人们通过不同语言的口语沟通。虽有一些关于多语言S2ST的研究，但他们的重点都是源语言的多语言性，即从多种源语言翻译到一个目标语言。我们提出了首个支持多个目标语言的多语言S2ST模型。我们利用了直接S2ST的最新进展，包括speech-to-unit（S2U）和 vocalsoder，并将这些关键组件具备多语言能力。speech-to-masked-unit（S2MU）是S2U的多语言扩展，它对不属于目标语言的单元应用掩蔽，以降低语言干扰。我们还提出了多语言 vocalsoder，它在语言嵌入和语言认识的 auxillary 损失下训练。在标准翻译测试集上，我们的提议的多语言模型在英语到16种目标语言的翻译中表现出色，超过了双语模型的表现。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/18/cs.SD_2023_07_18/" data-id="cloimipd700s4s488bu6oao5v" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_07_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/18/eess.AS_2023_07_18/" class="article-date">
  <time datetime="2023-07-18T14:00:00.000Z" itemprop="datePublished">2023-07-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/18/eess.AS_2023_07_18/">eess.AS - 2023-07-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Low-bit-rate-binaural-link-for-improved-ultra-low-latency-low-complexity-multichannel-speech-enhancement-in-Hearing-Aids"><a href="#Low-bit-rate-binaural-link-for-improved-ultra-low-latency-low-complexity-multichannel-speech-enhancement-in-Hearing-Aids" class="headerlink" title="Low bit rate binaural link for improved ultra low-latency low-complexity multichannel speech enhancement in Hearing Aids"></a>Low bit rate binaural link for improved ultra low-latency low-complexity multichannel speech enhancement in Hearing Aids</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08858">http://arxiv.org/abs/2307.08858</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nils L. Westhausen, Bernd T. Meyer</li>
<li>for: 提高听力器的听音质量</li>
<li>methods: 使用深度学习模型和 Filter-and-Sum 处理，并采用量化承载和集群通信来减少模型大小和计算负担</li>
<li>results: 可以匹配 oracle 双耳 LCMV 扬声器在非低延迟配置下的表现，且只需2毫秒的延迟Here’s the Chinese text in the format you requested:</li>
<li>for: 提高听力器的听音质量</li>
<li>methods: 使用深度学习模型和 Filter-and-Sum 处理，并采用量化承载和集群通信来减少模型大小和计算负担</li>
<li>results: 可以匹配 oracle 双耳 LCMV 扬声器在非低延迟配置下的表现，且只需2毫秒的延迟<details>
<summary>Abstract</summary>
Speech enhancement in hearing aids is a challenging task since the hardware limits the number of possible operations and the latency needs to be in the range of only a few milliseconds. We propose a deep-learning model compatible with these limitations, which we refer to as Group-Communication Filter-and-Sum Network (GCFSnet). GCFSnet is a causal multiple-input single output enhancement model using filter-and-sum processing in the time-frequency domain and a multi-frame deep post filter. All filters are complex-valued and are estimated by a deep-learning model using weight-sharing through Group Communication and quantization-aware training for reducing model size and computational footprint. For a further increase in performance, a low bit rate binaural link for delayed binaural features is proposed to use binaural information while retaining a latency of 2ms. The performance of an oracle binaural LCMV beamformer in non-low-latency configuration can be matched even by a unilateral configuration of the GCFSnet in terms of objective metrics.
</details>
<details>
<summary>摘要</summary>
听见助手中的语音提升是一项具有挑战性的任务，因为硬件的限制只能进行一定数量的操作，并且响应时间需要在几毫秒内。我们提议一种深度学习模型，称之为群组通信滤波和总网络（GCFSnet）。GCFSnet是一种 causal 多输入单出提升模型，使用时域频域的滤波和总处理，并使用多帧深度后 filters。所有滤波器都是复数值的，并由深度学习模型使用 weight-sharing 和量化感知训练来 estimates。为了进一步提高性能，我们提议使用低比特率双耳链接，以使用双耳信息而不超过2毫秒的响应时间。GCFSnet 的单边配置可以与非低延迟配置下的 oracle 双耳 LCMV 扫描器匹配，以对象指标来衡量。
</details></li>
</ul>
<hr>
<h2 id="Semi-supervised-multi-channel-speaker-diarization-with-cross-channel-attention"><a href="#Semi-supervised-multi-channel-speaker-diarization-with-cross-channel-attention" class="headerlink" title="Semi-supervised multi-channel speaker diarization with cross-channel attention"></a>Semi-supervised multi-channel speaker diarization with cross-channel attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08688">http://arxiv.org/abs/2307.08688</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shilong Wu, Jun Du, Maokui He, Shutong Niu, Hang Chen, Haitao Tang, Chin-Hui Lee</li>
<li>for: 本研究提出了一种半监督式的Speaker diarization系统，以利用大规模多通道训练数据，并生成pseudo标签来labels无标签数据。</li>
<li>methods: 本研究引入了 Cross-channel attention机制，以更好地学习speaker embedding的通道上下文信息。</li>
<li>results: 在CHiME-7 Mixer6数据集上，我们的系统比基于分 clustering 模型的开发集上的相对减少率为57.01%。在CHiME-6数据集上，当使用80%和50%标签的训练数据时，我们的系统与使用100%标签的训练数据相对性能相似。<details>
<summary>Abstract</summary>
Most neural speaker diarization systems rely on sufficient manual training data labels, which are hard to collect under real-world scenarios. This paper proposes a semi-supervised speaker diarization system to utilize large-scale multi-channel training data by generating pseudo-labels for unlabeled data. Furthermore, we introduce cross-channel attention into the Neural Speaker Diarization Using Memory-Aware Multi-Speaker Embedding (NSD-MA-MSE) to learn channel contextual information of speaker embeddings better. Experimental results on the CHiME-7 Mixer6 dataset which only contains partial speakers' labels of the training set, show that our system achieved 57.01% relative DER reduction compared to the clustering-based model on the development set. We further conducted experiments on the CHiME-6 dataset to simulate the scenario of missing partial training set labels. When using 80% and 50% labeled training data, our system performs comparably to the results obtained using 100% labeled data for training.
</details>
<details>
<summary>摘要</summary>
大多数神经网络发音分类系统都需要充足的手动训练数据标签，这些标签在实际场景下很难收集。这篇论文提议一种半监督的发音分类系统，利用大规模多通道训练数据生成 pseudo-标签，以便于无标签数据的学习。此外，我们在 Neural Speaker Diarization Using Memory-Aware Multi-Speaker Embedding (NSD-MA-MSE) 中引入了ChannelContextual Information的权重学习，以更好地学习发音者特征的通道信息。实验结果表明，在 CHiME-7 Mixer6 数据集上，我们的系统与 clustering-based 模型在开发集上实现了57.01%的相对减少性能。我们进一步在 CHiME-6 数据集上进行了 simulate 失去部分训练集标签的场景，当使用 80% 和 50% 标签过滤的训练数据时，我们的系统与使用 100% 标签训练的结果相当。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/18/eess.AS_2023_07_18/" data-id="cloimipfd00yfs488fgba9mv9" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_07_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/18/cs.CV_2023_07_18/" class="article-date">
  <time datetime="2023-07-18T13:00:00.000Z" itemprop="datePublished">2023-07-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/18/cs.CV_2023_07_18/">cs.CV - 2023-07-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Plug-the-Leaks-Advancing-Audio-driven-Talking-Face-Generation-by-Preventing-Unintended-Information-Flow"><a href="#Plug-the-Leaks-Advancing-Audio-driven-Talking-Face-Generation-by-Preventing-Unintended-Information-Flow" class="headerlink" title="Plug the Leaks: Advancing Audio-driven Talking Face Generation by Preventing Unintended Information Flow"></a>Plug the Leaks: Advancing Audio-driven Talking Face Generation by Preventing Unintended Information Flow</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09368">http://arxiv.org/abs/2307.09368</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dogucan Yaman, Fevziye Irem Eyiokur, Leonard Bärmann, Hazim Kemal Ekenel, Alexander Waibel</li>
<li>for: 这个论文的目的是提出一种基于音频的讲话脸面生成方法，以提高讲话脸面的视觉质量和音频视频同步。</li>
<li>methods: 这篇论文提出了一些现有的同步方法的问题，如lip和pose信息的不必要流动，以及训练过程中的不稳定性。具体来说，它们包括一个静音lip引用图生成器，一个适应 triplet损失，以及一种稳定的同步损失函数。</li>
<li>results: 根据实验结果，这种方法可以在LRS2和LRW上实现状态之最的表现，同时在视觉质量和音频视频同步两个方面均有显著提高。此外，通过不同的减少实验，证明了这些改进的共同作用和它们的个别贡献。<details>
<summary>Abstract</summary>
Audio-driven talking face generation is the task of creating a lip-synchronized, realistic face video from given audio and reference frames. This involves two major challenges: overall visual quality of generated images on the one hand, and audio-visual synchronization of the mouth part on the other hand. In this paper, we start by identifying several problematic aspects of synchronization methods in recent audio-driven talking face generation approaches. Specifically, this involves unintended flow of lip and pose information from the reference to the generated image, as well as instabilities during model training. Subsequently, we propose various techniques for obviating these issues: First, a silent-lip reference image generator prevents leaking of lips from the reference to the generated image. Second, an adaptive triplet loss handles the pose leaking problem. Finally, we propose a stabilized formulation of synchronization loss, circumventing aforementioned training instabilities while additionally further alleviating the lip leaking issue. Combining the individual improvements, we present state-of-the art performance on LRS2 and LRW in both synchronization and visual quality. We further validate our design in various ablation experiments, confirming the individual contributions as well as their complementary effects.
</details>
<details>
<summary>摘要</summary>
audio-driven talking face生成任务是创建一个 lip-synchronized、realistic 的面部视频，从给定的音频和参考帧开始。这涉及到两个主要挑战：一是生成图像的总Visual质量，二是音频-视频同步的口部部分。在这篇论文中，我们开始是通过找到最近的音频驱动 talking face生成方法中的一些问题。特别是，这些问题包括往返图像中的lip和姿势信息的不良流动，以及在训练模型时的不稳定性。然后，我们提出了多种解决方案：首先，一个 silent-lip 参考图像生成器防止了 lip 信息从参考图像流向生成图像。其次，一种适应 triplet 损失处理 pose 泄露问题。最后，我们提出了一种稳定化同步损失的形式， circumventing aforementioned training instabilities 而并且进一步缓解 lip 泄露问题。将各种改进相结合，我们展示了 state-of-the-art 性能在 LRS2 和 LRW 上，同时在同步和Visual质量两个方面均有提高。我们进一步验证了我们的设计，通过多个减少实验，证明了各个贡献以及它们的补偿效果。
</details></li>
</ul>
<hr>
<h2 id="An-Evaluation-of-Zero-Cost-Proxies-–-from-Neural-Architecture-Performance-to-Model-Robustness"><a href="#An-Evaluation-of-Zero-Cost-Proxies-–-from-Neural-Architecture-Performance-to-Model-Robustness" class="headerlink" title="An Evaluation of Zero-Cost Proxies – from Neural Architecture Performance to Model Robustness"></a>An Evaluation of Zero-Cost Proxies – from Neural Architecture Performance to Model Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09365">http://arxiv.org/abs/2307.09365</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jovita Lukasik, Michael Moeller, Margret Keuper</li>
<li>for: 研究 zero-cost 代理的用途和能力，尤其是用于寻找性能好吃的 neural architecture 搜索。</li>
<li>methods: 使用 zero-cost 代理来预测模型的性能，包括单个预测任务和多任务协同预测。</li>
<li>results: 分析了常见 zero-cost 代理的性能预测能力，发现 predicting 模型的稳定性更加困难，需要结合多个代理来预测。<details>
<summary>Abstract</summary>
Zero-cost proxies are nowadays frequently studied and used to search for neural architectures. They show an impressive ability to predict the performance of architectures by making use of their untrained weights. These techniques allow for immense search speed-ups. So far the joint search for well-performing and robust architectures has received much less attention in the field of NAS. Therefore, the main focus of zero-cost proxies is the clean accuracy of architectures, whereas the model robustness should play an evenly important part. In this paper, we analyze the ability of common zero-cost proxies to serve as performance predictors for robustness in the popular NAS-Bench-201 search space. We are interested in the single prediction task for robustness and the joint multi-objective of clean and robust accuracy. We further analyze the feature importance of the proxies and show that predicting the robustness makes the prediction task from existing zero-cost proxies more challenging. As a result, the joint consideration of several proxies becomes necessary to predict a model's robustness while the clean accuracy can be regressed from a single such feature.
</details>
<details>
<summary>摘要</summary>
现在，零成本代理常被研究和使用来搜索神经网络架构。它们能够很好地预测架构的性能，只使用未训练的权重。这些技术可以提供很大的搜索速度减少。而在神经网络搜索（NAS）领域中， JOINT 搜索良好性和稳定性的架构尚未受到很多关注。因此，零成本代理的主要关注点是架构的净精度，而模型的稳定性应该具有相等的重要性。在这篇论文中，我们分析了常见的零成本代理是否能够用来预测架构的稳定性，以及它们的多目标任务。我们还分析了代理的特征重要性，并显示了预测稳定性使得预测任务更加困难。因此，需要结合多个代理来预测模型的稳定性，而净精度可以从单个代理中进行回归。
</details></li>
</ul>
<hr>
<h2 id="Disentangle-then-Parse-Night-time-Semantic-Segmentation-with-Illumination-Disentanglement"><a href="#Disentangle-then-Parse-Night-time-Semantic-Segmentation-with-Illumination-Disentanglement" class="headerlink" title="Disentangle then Parse:Night-time Semantic Segmentation with Illumination Disentanglement"></a>Disentangle then Parse:Night-time Semantic Segmentation with Illumination Disentanglement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09362">http://arxiv.org/abs/2307.09362</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhixiang Wei, Lin Chen, Tao Tu, Huaian Chen, Pengyang Ling, Yi Jin</li>
<li>for: 提高夜间 semantic segmentation 性能</li>
<li>methods: 提出了一种新的夜间 semantic segmentation 方法，即 disentangle then parse (DTP)，通过分离夜间图像中的光度和反射组成部分，并基于它们的自适应 fusión 来认知 semantics。</li>
<li>results: DTP 在夜间 segmentation 任务中表现出色，与state-of-the-art 方法相比，具有更高的准确率和更好的一致性。<details>
<summary>Abstract</summary>
Most prior semantic segmentation methods have been developed for day-time scenes, while typically underperforming in night-time scenes due to insufficient and complicated lighting conditions. In this work, we tackle this challenge by proposing a novel night-time semantic segmentation paradigm, i.e., disentangle then parse (DTP). DTP explicitly disentangles night-time images into light-invariant reflectance and light-specific illumination components and then recognizes semantics based on their adaptive fusion. Concretely, the proposed DTP comprises two key components: 1) Instead of processing lighting-entangled features as in prior works, our Semantic-Oriented Disentanglement (SOD) framework enables the extraction of reflectance component without being impeded by lighting, allowing the network to consistently recognize the semantics under cover of varying and complicated lighting conditions. 2) Based on the observation that the illumination component can serve as a cue for some semantically confused regions, we further introduce an Illumination-Aware Parser (IAParser) to explicitly learn the correlation between semantics and lighting, and aggregate the illumination features to yield more precise predictions. Extensive experiments on the night-time segmentation task with various settings demonstrate that DTP significantly outperforms state-of-the-art methods. Furthermore, with negligible additional parameters, DTP can be directly used to benefit existing day-time methods for night-time segmentation.
</details>
<details>
<summary>摘要</summary>
大多数先前的 semantic segmentation 方法都是在白天场景下开发的，而在夜晚场景下表现不佳，主要因为夜晚场景中的灯光条件不充分和复杂。在这项工作中，我们解决了这个挑战，我们提出了一种新的夜晚 semantic segmentation 方法，即分解并解析 (DTP)。DTP 方法分解夜晚图像为不变的反射和特定灯光组成部分，然后基于这两个部分的 adaptive 融合来认定 semantics。具体来说，我们的 DTP 方法包括两个关键组成部分：1. 而不是在先前的工作中处理灯光束结合的特征，我们的 Semantic-Oriented Disentanglement (SOD) 框架允许在灯光条件下提取反射组成部分，使网络可以在不同和复杂的灯光条件下一直认定 semantics。2. 基于我们发现，灯光组成部分可以作为一些具有混淆的区域的征标，我们进一步引入了 Illumination-Aware Parser (IAParser) 来显式学习灯光和 semantics 之间的相关性，并将灯光特征聚合到更加精确的预测中。我们对夜晚 segmentation 任务进行了广泛的实验，结果表明，DTP 方法在不同的设置下显著超过了当前的方法。此外，DTP 方法可以在不加增加参数的情况下直接应用于当前的日间方法中，以提高夜晚 segmentation 性能。
</details></li>
</ul>
<hr>
<h2 id="OnlineRefer-A-Simple-Online-Baseline-for-Referring-Video-Object-Segmentation"><a href="#OnlineRefer-A-Simple-Online-Baseline-for-Referring-Video-Object-Segmentation" class="headerlink" title="OnlineRefer: A Simple Online Baseline for Referring Video Object Segmentation"></a>OnlineRefer: A Simple Online Baseline for Referring Video Object Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09356">http://arxiv.org/abs/2307.09356</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wudongming97/onlinerefer">https://github.com/wudongming97/onlinerefer</a></li>
<li>paper_authors: Dongming Wu, Tiancai Wang, Yuang Zhang, Xiangyu Zhang, Jianbing Shen</li>
<li>for: 本研究旨在提高视频对象 segmentation（RVOS）的精度和效率，通过人工指令来 segment object 在视频中。</li>
<li>methods: 我们提出了一种简单 yet effective 的在线模型，称为 OnlineRefer，使用显式的查询传播来提高当前帧的引用预测精度。我们还扩展了我们的在线模型，使其兼容于视频基础模型。</li>
<li>results: 我们在四个 benchmark 上评估了我们的方法，包括 Refer-Youtube-VOS、Refer-DAVIS17、A2D-Sentences 和 JHMDB-Sentences。与其他Offline方法相比，我们的OnlineRefer  WITH Swin-L 背景模型在 Refer-Youtube-VOS 和 Refer-DAVIS17 上达到了 63.5 J&amp;F 和 64.8 J&amp;F 的最高分，即使没有使用任何辅助工具。<details>
<summary>Abstract</summary>
Referring video object segmentation (RVOS) aims at segmenting an object in a video following human instruction. Current state-of-the-art methods fall into an offline pattern, in which each clip independently interacts with text embedding for cross-modal understanding. They usually present that the offline pattern is necessary for RVOS, yet model limited temporal association within each clip. In this work, we break up the previous offline belief and propose a simple yet effective online model using explicit query propagation, named OnlineRefer. Specifically, our approach leverages target cues that gather semantic information and position prior to improve the accuracy and ease of referring predictions for the current frame. Furthermore, we generalize our online model into a semi-online framework to be compatible with video-based backbones. To show the effectiveness of our method, we evaluate it on four benchmarks, \ie, Refer-Youtube-VOS, Refer-DAVIS17, A2D-Sentences, and JHMDB-Sentences. Without bells and whistles, our OnlineRefer with a Swin-L backbone achieves 63.5 J&F and 64.8 J&F on Refer-Youtube-VOS and Refer-DAVIS17, outperforming all other offline methods.
</details>
<details>
<summary>摘要</summary>
“参考视频物体分割（RVOS）目标在实现视频中人工指令下分割物体。现今顶尖方法都是在离线模式下进行，每个片段独立地与文本嵌入进行跨Modal理解。它们通常表明离线模式是RVOS必要的，但是模型内部的时间相互关联受限。在这个工作中，我们打破了先前的离线信念，并提出了一个简单又有效的在线模型，使用Explicit Query Propagation（具体询问传播），名为OnlineRefer。我们的方法利用目标传递获取 semantic 信息和位置偏好，以提高当前帧的引用预测精度。此外，我们将我们的在线模型转换为半在线框架，以适应视频基础模型。为证明我们的方法的有效性，我们在四个 benchmark 上进行评估，包括Refer-Youtube-VOS、Refer-DAVIS17、A2D-Sentences 和 JHMDB-Sentences。 Without 钟表和套路，我们的OnlineRefer 使用 Swin-L 基础模型在 Refer-Youtube-VOS 和 Refer-DAVIS17 上 achieved 63.5 J&F 和 64.8 J&F，比所有其他离线方法高。”
</details></li>
</ul>
<hr>
<h2 id="SphereNet-Learning-a-Noise-Robust-and-General-Descriptor-for-Point-Cloud-Registration"><a href="#SphereNet-Learning-a-Noise-Robust-and-General-Descriptor-for-Point-Cloud-Registration" class="headerlink" title="SphereNet: Learning a Noise-Robust and General Descriptor for Point Cloud Registration"></a>SphereNet: Learning a Noise-Robust and General Descriptor for Point Cloud Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09351">http://arxiv.org/abs/2307.09351</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guiyu Zhao, Zhentao Guo, Xin Wang, Hongbin Ma</li>
<li>for: 本研究旨在提出一种robust和可 generalized的点云注册方法，以便在不同视角下收集的点云之间进行准确的注册。</li>
<li>methods: 我们提出了一种基于学习的点云注册方法，称为SphereNet。该方法使用了圆柱体生成器来编码初始特征，然后使用圆形 interpolate 来实现鲁棒性 against 噪声。最后，一种新的圆形卷积神经网络with spherical integrity padding 完成了描述符的提取，这有助于减少丢失的特征并完全捕捉点云的几何特征。</li>
<li>results: 我们在两个indoor和outdoor dataset上进行了广泛的实验，并在高强度噪声下，SphereNet 能够提高特征匹配回快度比超过25个百分点。此外，SphereNet 在3DMatch和3DLoMatchbenchmark上达到了93.5%的注册回快度和75.6%的总回快度，并且在未见数据上有最好的泛化能力。<details>
<summary>Abstract</summary>
Point cloud registration is to estimate a transformation to align point clouds collected in different perspectives. In learning-based point cloud registration, a robust descriptor is vital for high-accuracy registration. However, most methods are susceptible to noise and have poor generalization ability on unseen datasets. Motivated by this, we introduce SphereNet to learn a noise-robust and unseen-general descriptor for point cloud registration. In our method, first, the spheroid generator builds a geometric domain based on spherical voxelization to encode initial features. Then, the spherical interpolation of the sphere is introduced to realize robustness against noise. Finally, a new spherical convolutional neural network with spherical integrity padding completes the extraction of descriptors, which reduces the loss of features and fully captures the geometric features. To evaluate our methods, a new benchmark 3DMatch-noise with strong noise is introduced. Extensive experiments are carried out on both indoor and outdoor datasets. Under high-intensity noise, SphereNet increases the feature matching recall by more than 25 percentage points on 3DMatch-noise. In addition, it sets a new state-of-the-art performance for the 3DMatch and 3DLoMatch benchmarks with 93.5\% and 75.6\% registration recall and also has the best generalization ability on unseen datasets.
</details>
<details>
<summary>摘要</summary>
点云注册是将多个视角中收集的点云进行对齐。在学习基于的点云注册中，一个可靠的描述符是关键，但大多数方法容易受到噪声的影响，对未看到的数据集的普适性很差。为了解决这个问题，我们提出了圆球网（SphereNet），用于学习具有噪声抗性和未看到数据集普适性的点云注册描述符。在我们的方法中，首先，圆球生成器将点云转换为圆球形式，以便在圆球分割中编码初始特征。然后，圆球 interpolating 技术是引入的，以实现对噪声的抗性。最后，一种新的圆球卷积神经网络，具有圆球完整裁剪，完成了特征EXTRACTING操作，从而减少特征丢失和完全捕捉几何特征。为评估我们的方法，我们引入了一个新的标准测试集3DMatch-noise，这个测试集具有强大的噪声。我们在室内和室外 dataset上进行了广泛的实验。在高强度噪声下，SphereNet提高了特征匹配回归率超过25个百分点。此外，它在3DMatch和3DLoMatch测试集上达到了93.5%和75.6%的注册回归率，同时具有最好的普适性。
</details></li>
</ul>
<hr>
<h2 id="Visual-Validation-versus-Visual-Estimation-A-Study-on-the-Average-Value-in-Scatterplots"><a href="#Visual-Validation-versus-Visual-Estimation-A-Study-on-the-Average-Value-in-Scatterplots" class="headerlink" title="Visual Validation versus Visual Estimation: A Study on the Average Value in Scatterplots"></a>Visual Validation versus Visual Estimation: A Study on the Average Value in Scatterplots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09330">http://arxiv.org/abs/2307.09330</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Braun, Ashley Suh, Remco Chang, Michael Gleicher, Tatiana von Landesberger</li>
<li>for: 这个论文 investigate了人们可以通过视觉方式验证统计模型是否适合数据。</li>
<li>methods: 研究使用了两个人口（公告和志愿者），参与者需要同时通过视觉估算和视觉验证（接受或拒绝）常见的均值模型。</li>
<li>results: 结果表明参与者们的验证和估算没有偏见，并且自然的批判点（接受或拒绝给定均值）与95%的自信区间边界相当接近，表明视觉感知的自信区间与统计标准相符。<details>
<summary>Abstract</summary>
We investigate the ability of individuals to visually validate statistical models in terms of their fit to the data. While visual model estimation has been studied extensively, visual model validation remains under-investigated. It is unknown how well people are able to visually validate models, and how their performance compares to visual and computational estimation. As a starting point, we conducted a study across two populations (crowdsourced and volunteers). Participants had to both visually estimate (i.e, draw) and visually validate (i.e., accept or reject) the frequently studied model of averages. Across both populations, the level of accuracy of the models that were considered valid was lower than the accuracy of the estimated models. We find that participants' validation and estimation were unbiased. Moreover, their natural critical point between accepting and rejecting a given mean value is close to the boundary of its 95% confidence interval, indicating that the visually perceived confidence interval corresponds to a common statistical standard. Our work contributes to the understanding of visual model validation and opens new research opportunities.
</details>
<details>
<summary>摘要</summary>
我们研究人员是否可以通过视觉方式验证统计模型的合适性。虽然视觉模型估计已经得到了广泛的研究，但视觉模型验证还受到了不足的研究。人们是否能够通过视觉方式验证模型，并且与计算机机器估计相比如何？为了开始，我们在两个人口中进行了研究（公众和志愿者）。参与者需要同时视觉估计（即绘制）以及视觉验证（即接受或拒绝）常见的平均值模型。在两个人口中，被认为是有效的模型的准确率较低于估计模型的准确率。我们发现参与者的验证和估计无偏见。此外，他们的自然批判点（between accepting and rejecting a given mean value）与其95%信息Interval的边缘几乎相同，表明视觉感知的信息Interval与统计标准相吻合。我们的研究对视觉模型验证的理解做出了贡献，开启了新的研究机遇。
</details></li>
</ul>
<hr>
<h2 id="Towards-a-performance-analysis-on-pre-trained-Visual-Question-Answering-models-for-autonomous-driving"><a href="#Towards-a-performance-analysis-on-pre-trained-Visual-Question-Answering-models-for-autonomous-driving" class="headerlink" title="Towards a performance analysis on pre-trained Visual Question Answering models for autonomous driving"></a>Towards a performance analysis on pre-trained Visual Question Answering models for autonomous driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09329">http://arxiv.org/abs/2307.09329</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kaavyarekanar/towards-a-performance-analysis-on-pre-trained-vqa-models-for-autonomous-driving">https://github.com/kaavyarekanar/towards-a-performance-analysis-on-pre-trained-vqa-models-for-autonomous-driving</a></li>
<li>paper_authors: Kaavya Rekanar, Ciarán Eising, Ganesh Sistu, Martin Hayes</li>
<li>for: 这篇短篇论文主要探讨了三种受欢迎的视觉问答模型（ViLBERT、ViLT、LXMERT）在解决自动驾驶场景中的问题上的表现。</li>
<li>methods: 这篇论文使用了对多模态架构中变换器的使用进行分析，并通过对参考答案与计算机视觉专家提供的答案进行比较来评估这些模型的性能。</li>
<li>results: 结果表明，包含对多模态进行交叉感知和较晚的融合技术的模型在自动驾驶场景中表现出了良好的潜力，可能为自动驾驶领域提供更好的答案。<details>
<summary>Abstract</summary>
This short paper presents a preliminary analysis of three popular Visual Question Answering (VQA) models, namely ViLBERT, ViLT, and LXMERT, in the context of answering questions relating to driving scenarios. The performance of these models is evaluated by comparing the similarity of responses to reference answers provided by computer vision experts. Model selection is predicated on the analysis of transformer utilization in multimodal architectures. The results indicate that models incorporating cross-modal attention and late fusion techniques exhibit promising potential for generating improved answers within a driving perspective. This initial analysis serves as a launchpad for a forthcoming comprehensive comparative study involving nine VQA models and sets the scene for further investigations into the effectiveness of VQA model queries in self-driving scenarios. Supplementary material is available at https://github.com/KaavyaRekanar/Towards-a-performance-analysis-on-pre-trained-VQA-models-for-autonomous-driving.
</details>
<details>
<summary>摘要</summary>
这篇短文提供了三种流行的视觉问答（VQA）模型的初步分析，即ViLBERT、ViLT和LXMERT，在回答关于驾驶场景的问题上。这些模型的性能被评估通过与计算机视觉专家提供的参考答案之间的相似性进行比较。模型选择基于多Modal arquitectures中的变换器使用情况的分析。结果表明，包含对Modal attention和迟至合并技术的模型表现出了优秀的潜力，用于生成改进的答案从驾驶角度来看。这个初步分析将为后续 Comparative study involving nine VQA models 作为起点，进一步调查VQA模型在自动驾驶场景中的效果。补充材料可以在https://github.com/KaavyaRekanar/Towards-a-performance-analysis-on-pre-trained-VQA-models-for-autonomous-driving 中找到。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Region-Aware-Neural-Radiance-Fields-for-High-Fidelity-Talking-Portrait-Synthesis"><a href="#Efficient-Region-Aware-Neural-Radiance-Fields-for-High-Fidelity-Talking-Portrait-Synthesis" class="headerlink" title="Efficient Region-Aware Neural Radiance Fields for High-Fidelity Talking Portrait Synthesis"></a>Efficient Region-Aware Neural Radiance Fields for High-Fidelity Talking Portrait Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09323">http://arxiv.org/abs/2307.09323</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fictionarry/er-nerf">https://github.com/fictionarry/er-nerf</a></li>
<li>paper_authors: Jiahe Li, Jiawei Zhang, Xiao Bai, Jun Zhou, Lin Gu</li>
<li>for: 实现高质量、快速渲染和小型模型的对话人物synthesis</li>
<li>methods: 使用Conditional Neural Radiance Fields (NeRF)建立Explicit Region-based NeRF (ER-NeRF)架构，并引入Tri-Plane Hash Representation和Region Attention Module等技术来提高对话人物模型的准确性和效率</li>
<li>results: 对比 précédente方法，ER-NeRF可以实现更高的高质量和audio-lips同步的对话人物视频生成，并且具有更高的效率和更小的模型大小<details>
<summary>Abstract</summary>
This paper presents ER-NeRF, a novel conditional Neural Radiance Fields (NeRF) based architecture for talking portrait synthesis that can concurrently achieve fast convergence, real-time rendering, and state-of-the-art performance with small model size. Our idea is to explicitly exploit the unequal contribution of spatial regions to guide talking portrait modeling. Specifically, to improve the accuracy of dynamic head reconstruction, a compact and expressive NeRF-based Tri-Plane Hash Representation is introduced by pruning empty spatial regions with three planar hash encoders. For speech audio, we propose a Region Attention Module to generate region-aware condition feature via an attention mechanism. Different from existing methods that utilize an MLP-based encoder to learn the cross-modal relation implicitly, the attention mechanism builds an explicit connection between audio features and spatial regions to capture the priors of local motions. Moreover, a direct and fast Adaptive Pose Encoding is introduced to optimize the head-torso separation problem by mapping the complex transformation of the head pose into spatial coordinates. Extensive experiments demonstrate that our method renders better high-fidelity and audio-lips synchronized talking portrait videos, with realistic details and high efficiency compared to previous methods.
</details>
<details>
<summary>摘要</summary>
To achieve this, we propose a compact and expressive NeRF-based Tri-Plane Hash Representation, which prunes empty spatial regions using three planar hash encoders. This allows for more efficient rendering and improved accuracy.For speech audio, we introduce a Region Attention Module to generate region-aware condition features via an attention mechanism. This approach explicitly connects audio features and spatial regions, allowing for more effective capture of local motions.Furthermore, we propose a direct and fast Adaptive Pose Encoding to optimize the head-torso separation problem by mapping the complex transformation of the head pose into spatial coordinates. This approach allows for more efficient and accurate rendering of talking portraits.Extensive experiments demonstrate that our method produces high-fidelity and audio-lips synchronized talking portrait videos with realistic details and high efficiency, outperforming previous methods.
</details></li>
</ul>
<hr>
<h2 id="Towards-Automated-Semantic-Segmentation-in-Mammography-Images"><a href="#Towards-Automated-Semantic-Segmentation-in-Mammography-Images" class="headerlink" title="Towards Automated Semantic Segmentation in Mammography Images"></a>Towards Automated Semantic Segmentation in Mammography Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10296">http://arxiv.org/abs/2307.10296</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cesar A. Sierra-Franco, Jan Hurtado, Victor de A. Thomaz, Leonardo C. da Cruz, Santiago V. Silva, Alberto B. Raposo</li>
<li>for: 检测非感知乳腺癌，提供诊断和评估图像质量的机会。</li>
<li>methods: 使用深度学习框架自动 segmenting 乳腺、肌肉、肉组织和脂肪组织。</li>
<li>results: 实现了高精度的 segmentation 性能，适用于多样化和复杂的案例，可以应用于临床实践。<details>
<summary>Abstract</summary>
Mammography images are widely used to detect non-palpable breast lesions or nodules, preventing cancer and providing the opportunity to plan interventions when necessary. The identification of some structures of interest is essential to make a diagnosis and evaluate image adequacy. Thus, computer-aided detection systems can be helpful in assisting medical interpretation by automatically segmenting these landmark structures. In this paper, we propose a deep learning-based framework for the segmentation of the nipple, the pectoral muscle, the fibroglandular tissue, and the fatty tissue on standard-view mammography images. We introduce a large private segmentation dataset and extensive experiments considering different deep-learning model architectures. Our experiments demonstrate accurate segmentation performance on variate and challenging cases, showing that this framework can be integrated into clinical practice.
</details>
<details>
<summary>摘要</summary>
乳影像广泛用于探测不可触感乳腺肿块或肿瘤，预防癌症并提供诊断和治疗计划时的机会。正确识别一些关键结构是诊断和评估图像质量的关键。因此，计算机支持的检测系统可以帮助医疗解读，自动将关键结构分割出来。在这篇论文中，我们提出了基于深度学习的框架，用于标准视图乳影像中的胸肌、乳腺组织和脂肪组织的自动分割。我们提供了大量私人分割数据集和详细的实验，考虑了不同的深度学习模型架构。我们的实验表明，这个框架可以在多种和复杂的案例中提供准确的分割表现，并且可以在临床实践中应用。
</details></li>
</ul>
<hr>
<h2 id="MarS3D-A-Plug-and-Play-Motion-Aware-Model-for-Semantic-Segmentation-on-Multi-Scan-3D-Point-Clouds"><a href="#MarS3D-A-Plug-and-Play-Motion-Aware-Model-for-Semantic-Segmentation-on-Multi-Scan-3D-Point-Clouds" class="headerlink" title="MarS3D: A Plug-and-Play Motion-Aware Model for Semantic Segmentation on Multi-Scan 3D Point Clouds"></a>MarS3D: A Plug-and-Play Motion-Aware Model for Semantic Segmentation on Multi-Scan 3D Point Clouds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09316">http://arxiv.org/abs/2307.09316</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cvmi-lab/mars3d">https://github.com/cvmi-lab/mars3d</a></li>
<li>paper_authors: Jiahui Liu, Chirui Chang, Jianhui Liu, Xiaoyang Wu, Lan Ma, Xiaojuan Qi</li>
<li>for: 这种研究旨在提高多扫描大规模点云semantic segmentation的精度，以便在自动化系统中提高自主驾驶能力。</li>
<li>methods: 该研究提出了一种名为MarS3D的插件式运动相关模块，可以让单扫模型具备多扫观察能力。该模块包括两个关键设计：横幅特征嵌入模块和运动相关特征学习模块。</li>
<li>results: 实验表明，MarS3D可以大幅提高基线模型的性能。代码可以在<a target="_blank" rel="noopener" href="https://github.com/CVMI-Lab/MarS3D%E4%B8%AD%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/CVMI-Lab/MarS3D中下载。</a><details>
<summary>Abstract</summary>
3D semantic segmentation on multi-scan large-scale point clouds plays an important role in autonomous systems. Unlike the single-scan-based semantic segmentation task, this task requires distinguishing the motion states of points in addition to their semantic categories. However, methods designed for single-scan-based segmentation tasks perform poorly on the multi-scan task due to the lacking of an effective way to integrate temporal information. We propose MarS3D, a plug-and-play motion-aware module for semantic segmentation on multi-scan 3D point clouds. This module can be flexibly combined with single-scan models to allow them to have multi-scan perception abilities. The model encompasses two key designs: the Cross-Frame Feature Embedding module for enriching representation learning and the Motion-Aware Feature Learning module for enhancing motion awareness. Extensive experiments show that MarS3D can improve the performance of the baseline model by a large margin. The code is available at https://github.com/CVMI-Lab/MarS3D.
</details>
<details>
<summary>摘要</summary>
三维semantic segmentation在多扫描大规模点云中扮演着重要的角色，这个任务不同于单扫描任务，需要分别处理点cloud中的运动态和semantic category。然而，针对单扫描任务设计的方法在多扫描任务中表现不佳，主要因为缺乏有效的时间信息集成方法。我们提议了MarS3D，一个可插入式运动意识模块，用于semantic segmentation在多扫描3D点云中。这个模块可以与单扫描模型结合，让它们拥有多扫描视觉能力。模型包括两个关键设计：横幅嵌入模块和运动意识学习模块。广泛的实验表明，MarS3D可以大幅提高基eline模型的性能。代码可以在https://github.com/CVMI-Lab/MarS3D中下载。
</details></li>
</ul>
<hr>
<h2 id="EigenTrajectory-Low-Rank-Descriptors-for-Multi-Modal-Trajectory-Forecasting"><a href="#EigenTrajectory-Low-Rank-Descriptors-for-Multi-Modal-Trajectory-Forecasting" class="headerlink" title="EigenTrajectory: Low-Rank Descriptors for Multi-Modal Trajectory Forecasting"></a>EigenTrajectory: Low-Rank Descriptors for Multi-Modal Trajectory Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09306">http://arxiv.org/abs/2307.09306</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/inhwanbae/eigentrajectory">https://github.com/inhwanbae/eigentrajectory</a></li>
<li>paper_authors: Inhwan Bae, Jean Oh, Hae-Gon Jeon</li>
<li>for: 预测人行道径的方法</li>
<li>methods: 使用新的轨迹描述符将行人运动转换为一个紧凑的空间（称为$\mathbb{ET}$空间），并使用低级别approximation减少轨迹描述符的复杂性。</li>
<li>results: 比较 existed trajectory forecasting models 的预测精度和可靠性，并通过anchor-based refinement方法覆盖所有可能的未来。Here’s the full text in Simplified Chinese:</li>
<li>for: 预测人行道径的方法</li>
<li>methods: 使用新的轨迹描述符将行人运动转换为一个紧凑的空间（称为$\mathbb{ET}$空间），并使用低级别approximation减少轨迹描述符的复杂性。</li>
<li>results: 比较 existed trajectory forecasting models 的预测精度和可靠性，并通过anchor-based refinement方法覆盖所有可能的未来。<details>
<summary>Abstract</summary>
Capturing high-dimensional social interactions and feasible futures is essential for predicting trajectories. To address this complex nature, several attempts have been devoted to reducing the dimensionality of the output variables via parametric curve fitting such as the B\'ezier curve and B-spline function. However, these functions, which originate in computer graphics fields, are not suitable to account for socially acceptable human dynamics. In this paper, we present EigenTrajectory ($\mathbb{ET}$), a trajectory prediction approach that uses a novel trajectory descriptor to form a compact space, known here as $\mathbb{ET}$ space, in place of Euclidean space, for representing pedestrian movements. We first reduce the complexity of the trajectory descriptor via a low-rank approximation. We transform the pedestrians' history paths into our $\mathbb{ET}$ space represented by spatio-temporal principle components, and feed them into off-the-shelf trajectory forecasting models. The inputs and outputs of the models as well as social interactions are all gathered and aggregated in the corresponding $\mathbb{ET}$ space. Lastly, we propose a trajectory anchor-based refinement method to cover all possible futures in the proposed $\mathbb{ET}$ space. Extensive experiments demonstrate that our EigenTrajectory predictor can significantly improve both the prediction accuracy and reliability of existing trajectory forecasting models on public benchmarks, indicating that the proposed descriptor is suited to represent pedestrian behaviors. Code is publicly available at https://github.com/inhwanbae/EigenTrajectory .
</details>
<details>
<summary>摘要</summary>
capturing高维社交互动和可能的未来是预测轨迹的关键。为了解决这种复杂的性质，许多尝试都是减少输出变量的维度via参数曲线拟合，如B\'ezier曲线和B-spline函数。然而，这些函数，起源于计算机图形领域，不适合考虑社会接受的人类动态。在这篇论文中，我们提出了EigenTrajectory（$\mathbb{ET}$），一种轨迹预测方法，使用一种新的轨迹描述符来形成一个紧凑的空间，称为$\mathbb{ET}$空间，以代替欧几何空间，来表示步行者的运动。我们首先减少轨迹描述符的复杂性via低级应对。将步行者历史路径转换为我们的$\mathbb{ET}$空间，表示的是时空原则Components，并将其输入到市场上可用的轨迹预测模型中。输入和输出模型以及社交互动都被聚集和聚合在相应的$\mathbb{ET}$空间中。最后，我们提出了一种轨迹锚点基于的修正方法，以覆盖所有可能的未来在提posed $\mathbb{ET}$空间中。广泛的实验表明，我们的EigenTrajectory预测器可以在公共的benchmark上显著提高现有轨迹预测模型的预测精度和可靠性， indicating that the proposed descriptor is suitable to represent pedestrian behaviors。代码可以在https://github.com/inhwanbae/EigenTrajectory上获取。
</details></li>
</ul>
<hr>
<h2 id="Conformal-prediction-under-ambiguous-ground-truth"><a href="#Conformal-prediction-under-ambiguous-ground-truth" class="headerlink" title="Conformal prediction under ambiguous ground truth"></a>Conformal prediction under ambiguous ground truth</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09302">http://arxiv.org/abs/2307.09302</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Stutz, Abhijit Guha Roy, Tatiana Matejovicova, Patricia Strachan, Ali Taylan Cemgil, Arnaud Doucet</li>
<li>for: 这个研究是为了应对安全承认分类任务中的不确定性量化，提供信心集包括真实类别，并且可以根据使用者指定的概率。</li>
<li>methods: 这个研究使用了对复现预测，即提供信心集，包括真实类别，并且可以根据使用者指定的概率。然而，这个方法通常需要一个专属的验证集，以获得实际的真实类别。</li>
<li>results: 这个研究提出了一个基于对复现 posterior distribution 的类别分类框架，可以在不确定类别设定下进行预测。实验结果显示，这个方法可以在实际应用中提供更好的不确定性量化，并且可以在不同的数据集上进行预测。<details>
<summary>Abstract</summary>
In safety-critical classification tasks, conformal prediction allows to perform rigorous uncertainty quantification by providing confidence sets including the true class with a user-specified probability. This generally assumes the availability of a held-out calibration set with access to ground truth labels. Unfortunately, in many domains, such labels are difficult to obtain and usually approximated by aggregating expert opinions. In fact, this holds true for almost all datasets, including well-known ones such as CIFAR and ImageNet. Applying conformal prediction using such labels underestimates uncertainty. Indeed, when expert opinions are not resolvable, there is inherent ambiguity present in the labels. That is, we do not have ``crisp'', definitive ground truth labels and this uncertainty should be taken into account during calibration. In this paper, we develop a conformal prediction framework for such ambiguous ground truth settings which relies on an approximation of the underlying posterior distribution of labels given inputs. We demonstrate our methodology on synthetic and real datasets, including a case study of skin condition classification in dermatology.
</details>
<details>
<summary>摘要</summary>
在安全关键分类任务中，协Forms prediction可以进行严格的uncertainty量化，提供包含真实类型的信任集，其中用户可以指定概率。通常，这假设有一个保留的Calibration集可以获得真实标签。然而，在许多领域，这些标签很难获得，通常通过专家意见的汇总来估算。事实上，这是大多数数据集的情况，包括CIFAR和ImageNet。在使用这些标签进行协Forms prediction时，会低估uncertainty。实际上，当专家意见不可分解时，存在labels中的不确定性。即，我们没有“精炼”的、明确的真实标签，这种uncertainty应该在calibration中被考虑。在这篇论文中，我们开发了一种协Forms prediction框架，用于这种不确定真实标签的设置，该框架基于输入的标签 posterior distribution的approximation。我们在 sintetic和实际数据集上验证了我们的方法，包括一个皮肤状况分类的case study。
</details></li>
</ul>
<hr>
<h2 id="RepViT-Revisiting-Mobile-CNN-From-ViT-Perspective"><a href="#RepViT-Revisiting-Mobile-CNN-From-ViT-Perspective" class="headerlink" title="RepViT: Revisiting Mobile CNN From ViT Perspective"></a>RepViT: Revisiting Mobile CNN From ViT Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09283">http://arxiv.org/abs/2307.09283</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jameslahm/RepViT">https://github.com/jameslahm/RepViT</a></li>
<li>paper_authors: Ao Wang, Hui Chen, Zijia Lin, Hengjun Pu, Guiguang Ding</li>
<li>for: 该研究旨在探讨轻量级视Transformers（ViTs）和轻量级卷积神经网络（CNNs）在移动设备上的性能和延迟性能。</li>
<li>methods: 该研究使用了一种名为RepViT的新家族的纯轻量级CNN，通过结合轻量级ViTs的有效的建筑设计，实现了更高的性能和更低的延迟。</li>
<li>results: 实验结果表明，RepViT在多种视觉任务中表现出色，并且在iPhone 12上实现了80.4%的顶部1 accuracy，延迟只有1.3毫秒，是轻量级模型中首次达到这个水平。<details>
<summary>Abstract</summary>
Recently, lightweight Vision Transformers (ViTs) demonstrate superior performance and lower latency compared with lightweight Convolutional Neural Networks (CNNs) on resource-constrained mobile devices. This improvement is usually attributed to the multi-head self-attention module, which enables the model to learn global representations. However, the architectural disparities between lightweight ViTs and lightweight CNNs have not been adequately examined. In this study, we revisit the efficient design of lightweight CNNs and emphasize their potential for mobile devices. We incrementally enhance the mobile-friendliness of a standard lightweight CNN, specifically MobileNetV3, by integrating the efficient architectural choices of lightweight ViTs. This ends up with a new family of pure lightweight CNNs, namely RepViT. Extensive experiments show that RepViT outperforms existing state-of-the-art lightweight ViTs and exhibits favorable latency in various vision tasks. On ImageNet, RepViT achieves over 80\% top-1 accuracy with nearly 1ms latency on an iPhone 12, which is the first time for a lightweight model, to the best of our knowledge. Our largest model, RepViT-M3, obtains 81.4\% accuracy with only 1.3ms latency. The code and trained models are available at \url{https://github.com/jameslahm/RepViT}.
</details>
<details>
<summary>摘要</summary>
最近，轻量级视Transformers（ViTs）在有限的移动设备上表现出色，比较于轻量级卷积神经网络（CNNs）更具有优势，主要归功于自注意机制，允许模型学习全局表示。然而，这两种模型的建构差异尚未得到充分探讨。在本研究中，我们重新审视了轻量级CNNs的有效设计，强调其在移动设备上的潜在优势。我们逐步提高了标准轻量级CNNs的MobileNetV3的移动友好性，通过 интеграating了轻量级ViTs的有效建构选择。这使得我们获得了一个新的家族的纯轻量级CNNs，称之为RepViT。我们进行了广泛的实验，发现RepViT可以在各种视觉任务中超越现有的状态域轻量级ViTs，并且在iPhone 12上实现了80.4%的top-1准确率，延迟只有约1ms。我们最大的模型RepViT-M3可以达到81.4%的准确率，延迟只有1.3ms。代码和训练模型可以在<https://github.com/jameslahm/RepViT>上获取。
</details></li>
</ul>
<hr>
<h2 id="Regression-free-Blind-Image-Quality-Assessment"><a href="#Regression-free-Blind-Image-Quality-Assessment" class="headerlink" title="Regression-free Blind Image Quality Assessment"></a>Regression-free Blind Image Quality Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09279">http://arxiv.org/abs/2307.09279</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/XiaoqiWang/regression-free-iqa">https://github.com/XiaoqiWang/regression-free-iqa</a></li>
<li>paper_authors: Xiaoqi Wang, Jian Xiong, Hao Gao, Weisi Lin</li>
<li>for: 提高图像质量评估模型的准确性，降低由偏袋训练样本引起的偏袋问题。</li>
<li>methods: 基于Retrieving Similar Instances的框架，结合semantic和distortion特征来评估图像质量。</li>
<li>results: 与state-of-the-art regression-based模型相比，提出的模型可以显著提高图像质量评估的准确性。<details>
<summary>Abstract</summary>
Regression-based blind image quality assessment (IQA) models are susceptible to biased training samples, leading to a biased estimation of model parameters. To mitigate this issue, we propose a regression-free framework for image quality evaluation, which is founded upon retrieving similar instances by incorporating semantic and distortion features. The motivation behind this approach is rooted in the observation that the human visual system (HVS) has analogous visual responses to semantically similar image contents degraded by the same distortion. The proposed framework comprises two classification-based modules: semantic-based classification (SC) module and distortion-based classification (DC) module. Given a test image and an IQA database, the SC module retrieves multiple pristine images based on semantic similarity. The DC module then retrieves instances based on distortion similarity from the distorted images that correspond to each retrieved pristine image. Finally, the predicted quality score is derived by aggregating the subjective quality scores of multiple retrieved instances. Experimental results on four benchmark databases validate that the proposed model can remarkably outperform the state-of-the-art regression-based models.
</details>
<details>
<summary>摘要</summary>
征inct-based盲目图像质量评估（IQA）模型容易受到偏向训练样本的影响，导致模型参数的偏向估计。为解决这个问题，我们提议一种无回归的框架 для图像质量评估，基于检索相似的实例。这种方法的动机在于人类视觉系统（HVS）在semantically相似的图像内容受到同样的损害后，有相似的视觉响应。提议的框架包括两个分类基 module：semantic-based分类（SC）模块和损害基分类（DC）模块。给定一个测试图像和IQA数据库，SC模块将多个无损图像根据semantic similarity retrieved。然后，DC模块将从对应每个损害图像中 Retrieves instances based on distortion similarity。最后，预测的质量分数由多个检索到的实例的主观质量分数的汇总得到。实验结果表明，提议的模型可以remarkably exceed state-of-the-art regression-based模型的性能。
</details></li>
</ul>
<hr>
<h2 id="Distilling-Coarse-to-Fine-Semantic-Matching-Knowledge-for-Weakly-Supervised-3D-Visual-Grounding"><a href="#Distilling-Coarse-to-Fine-Semantic-Matching-Knowledge-for-Weakly-Supervised-3D-Visual-Grounding" class="headerlink" title="Distilling Coarse-to-Fine Semantic Matching Knowledge for Weakly Supervised 3D Visual Grounding"></a>Distilling Coarse-to-Fine Semantic Matching Knowledge for Weakly Supervised 3D Visual Grounding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09267">http://arxiv.org/abs/2307.09267</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zehan Wang, Haifeng Huang, Yang Zhao, Linjun Li, Xize Cheng, Yichen Zhu, Aoxiong Yin, Zhou Zhao</li>
<li>for: 本研究的目的是提出一种基于弱监督的3D视觉固定方法，以便使用简单的文本描述来找到3D场景中的目标对象。</li>
<li>methods: 我们提出了一种使用弱监督的Semantic Matching模型来学习3D视觉固定模型，其中使用粗细的场景文本对应关系来链接对象和文本。我们还提出了一种将弱监督的semantic Matching知识融入到传统的两阶段3D视觉固定模型中，以提高性能和降低推理成本。</li>
<li>results: 我们在ScanRefer、Nr3D和Sr3D等 datasets上进行了广泛的实验，并证明了我们的提出方法的效果。<details>
<summary>Abstract</summary>
3D visual grounding involves finding a target object in a 3D scene that corresponds to a given sentence query. Although many approaches have been proposed and achieved impressive performance, they all require dense object-sentence pair annotations in 3D point clouds, which are both time-consuming and expensive. To address the problem that fine-grained annotated data is difficult to obtain, we propose to leverage weakly supervised annotations to learn the 3D visual grounding model, i.e., only coarse scene-sentence correspondences are used to learn object-sentence links. To accomplish this, we design a novel semantic matching model that analyzes the semantic similarity between object proposals and sentences in a coarse-to-fine manner. Specifically, we first extract object proposals and coarsely select the top-K candidates based on feature and class similarity matrices. Next, we reconstruct the masked keywords of the sentence using each candidate one by one, and the reconstructed accuracy finely reflects the semantic similarity of each candidate to the query. Additionally, we distill the coarse-to-fine semantic matching knowledge into a typical two-stage 3D visual grounding model, which reduces inference costs and improves performance by taking full advantage of the well-studied structure of the existing architectures. We conduct extensive experiments on ScanRefer, Nr3D, and Sr3D, which demonstrate the effectiveness of our proposed method.
</details>
<details>
<summary>摘要</summary>
三维视觉根据 involves 找到一个给定句子查询对应的目标对象在三维场景中。虽然许多方法已经提出并实现了出色的性能，但它们都需要密集的对象句子对 annotation 在三维点云中，这些数据均是时间consuming 和昂贵的。为了解决细化的 annotated data 困难以获得，我们提议使用弱supervised annotation 学习三维视觉根据模型，即只使用句子-场景对应的粗细 correlations 来学习对象-句子链接。为实现这一目标，我们设计了一种新的semantic matching模型，该模型在粗细-精度排序的方式下分析对象提案和句子之间的semantic similarity。具体来说，我们首先提取对象提案，然后使用特征和类型相似度矩阵来粗细选择前top-K candidates。接着，我们一个一个地使用每个候选对象来重建句子中的掩码关键词，重建准确率就是每个候选对象与查询句子之间的semantic similarity的表现。此外，我们将粗细-精度匹配知识蒸馏到一个常见的两阶段三维视觉根据模型中，从而降低推理成本和提高性能，同时利用现有的建筑结构。我们在ScanRefer、Nr3D和Sr3D上进行了广泛的实验，结果表明我们的提议方法的效果。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Distillation-for-Object-Detection-from-generic-to-remote-sensing-datasets"><a href="#Knowledge-Distillation-for-Object-Detection-from-generic-to-remote-sensing-datasets" class="headerlink" title="Knowledge Distillation for Object Detection: from generic to remote sensing datasets"></a>Knowledge Distillation for Object Detection: from generic to remote sensing datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09264">http://arxiv.org/abs/2307.09264</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hoàng-Ân Lê, Minh-Tan Pham</li>
<li>for: 评估多种常用计算机视觉数据集上开发的物体检测知识填充方法在远程感知中的表现。</li>
<li>methods: 包括逻辑模仿和特征模仿方法，用于车辆检测，并在xView和VEDAI数据集上进行了广泛的实验。</li>
<li>results: 实验结果表明，这些方法在远程感知 datasets 上表现出了高度的变化，并且confirm了结果聚合和cross validation的重要性。<details>
<summary>Abstract</summary>
Knowledge distillation, a well-known model compression technique, is an active research area in both computer vision and remote sensing communities. In this paper, we evaluate in a remote sensing context various off-the-shelf object detection knowledge distillation methods which have been originally developed on generic computer vision datasets such as Pascal VOC. In particular, methods covering both logit mimicking and feature imitation approaches are applied for vehicle detection using the well-known benchmarks such as xView and VEDAI datasets. Extensive experiments are performed to compare the relative performance and interrelationships of the methods. Experimental results show high variations and confirm the importance of result aggregation and cross validation on remote sensing datasets.
</details>
<details>
<summary>摘要</summary>
知识填充（knowledge distillation）是计算机视觉和远程感知领域中广泛研究的一种模型压缩技术。在这篇论文中，我们在远程感知上评估了一些 generic 计算机视觉数据集上开发的物体检测知识填充方法。特别是，我们应用了两种方法：ilogit 模仿和特征模仿。我们使用了 xView 和 VEDAI 数据集进行车辆检测。我们进行了广泛的实验比较这些方法的相对性能和相互关系。实验结果表明，存在很大的变化，并证明了在远程感知数据集上的结果聚合和十分 validate 的重要性。
</details></li>
</ul>
<hr>
<h2 id="Neuromorphic-spintronics-simulated-using-an-unconventional-data-driven-Thiele-equation-approach"><a href="#Neuromorphic-spintronics-simulated-using-an-unconventional-data-driven-Thiele-equation-approach" class="headerlink" title="Neuromorphic spintronics simulated using an unconventional data-driven Thiele equation approach"></a>Neuromorphic spintronics simulated using an unconventional data-driven Thiele equation approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09262">http://arxiv.org/abs/2307.09262</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anatole Moureaux, Simon de Wergifosse, Chloé Chopin, Flavio Abreu Araujo</li>
<li>for: 这个研究旨在开发一种量化描述磁矩扭矩气体oscillators（STVOs）动态的数学模型，以加速STVO-based neuromorphic computing设备的设计和计算成本减少。</li>
<li>methods: 这个研究使用了一种不寻常的模型，即TEA和MMS数据的组合，以解决STVO动态学问题。这种模型可以将计算时间减少9个数量级，同时保持同等精度。</li>
<li>results: 研究人员通过模拟STVO-based neural network来解决一个分类任务，并评估其性能对输入信号电流强度和噪声的影响。结果显示，这种方法可以加速STVO-based neuromorphic computing设备的设计和计算成本减少，同时维持同等精度。<details>
<summary>Abstract</summary>
In this study, we developed a quantitative description of the dynamics of spin-torque vortex nano-oscillators (STVOs) through an unconventional model based on the combination of the Thiele equation approach (TEA) and data from micromagnetic simulations (MMS). Solving the STVO dynamics with our analytical model allows to accelerate the simulations by 9 orders of magnitude compared to MMS while reaching the same level of accuracy. Here, we showcase our model by simulating a STVO-based neural network for solving a classification task. We assess its performance with respect to the input signal current intensity and the level of noise that might affect such a system. Our approach is promising for accelerating the design of STVO-based neuromorphic computing devices while decreasing drastically its computational cost.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们开发了一种量化描述STVOs的动态模型，通过TEA和MMS数据的组合来实现。我们的分析模型可以将STVO动态 simulations加速9个数量级，与MMS达到同等精度。在这里，我们通过模拟一个基于STVO的神经网络来解决一个分类任务。我们评估了这个系统对输入信号强度和噪声的影响。我们的方法可以加速STVO基于神经网络设备的设计，同时减少计算成本。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Topological-Feature-via-Persistent-Homology-Filtration-Learning-for-Point-Clouds"><a href="#Adaptive-Topological-Feature-via-Persistent-Homology-Filtration-Learning-for-Point-Clouds" class="headerlink" title="Adaptive Topological Feature via Persistent Homology: Filtration Learning for Point Clouds"></a>Adaptive Topological Feature via Persistent Homology: Filtration Learning for Point Clouds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09259">http://arxiv.org/abs/2307.09259</a></li>
<li>repo_url: None</li>
<li>paper_authors: Naoki Nishikawa, Yuichi Ike, Kenji Yamanishi</li>
<li>for: 提高机器学习方法对点云的精度，通过 incorporating 全球拓扑特征。</li>
<li>methods: 提出了一种基于神经网络的自适应筛选方法，以实现 persistent homology 的准确计算。</li>
<li>results: 在多个分类任务中，实验结果表明了我们的框架的效果。<details>
<summary>Abstract</summary>
Machine learning for point clouds has been attracting much attention, with many applications in various fields, such as shape recognition and material science. To enhance the accuracy of such machine learning methods, it is known to be effective to incorporate global topological features, which are typically extracted by persistent homology. In the calculation of persistent homology for a point cloud, we need to choose a filtration for the point clouds, an increasing sequence of spaces. Because the performance of machine learning methods combined with persistent homology is highly affected by the choice of a filtration, we need to tune it depending on data and tasks. In this paper, we propose a framework that learns a filtration adaptively with the use of neural networks. In order to make the resulting persistent homology isometry-invariant, we develop a neural network architecture with such invariance. Additionally, we theoretically show a finite-dimensional approximation result that justifies our architecture. Experimental results demonstrated the efficacy of our framework in several classification tasks.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:机器学习点云拥有很多应用，包括形状识别和材料科学等领域。将全球数学特征（Persistent homology）与机器学习方法结合，可以提高精度。在点云中计算 persistent homology 时，需要选择一个滤子，这是一个增加的序列空间。由于选择滤子会对机器学习方法的性能产生很大的影响，因此需要根据数据和任务进行调整。在这篇论文中，我们提出了一个自适应的滤子学习框架，使用神经网络来实现。为了使得结果的 persistent homology 是尺度不变的，我们开发了一个具有尺度不变的神经网络架构。此外，我们也理论上显示了一个有限维度近似结果，证明了我们的架构。实验结果显示了我们的框架在多个分类任务中的效果。
</details></li>
</ul>
<hr>
<h2 id="Generation-of-High-Spatial-Resolution-Terrestrial-Surface-from-Low-Spatial-Resolution-Elevation-Contour-Maps-via-Hierarchical-Computation-of-Median-Elevation-Regions"><a href="#Generation-of-High-Spatial-Resolution-Terrestrial-Surface-from-Low-Spatial-Resolution-Elevation-Contour-Maps-via-Hierarchical-Computation-of-Median-Elevation-Regions" class="headerlink" title="Generation of High Spatial Resolution Terrestrial Surface from Low Spatial Resolution Elevation Contour Maps via Hierarchical Computation of Median Elevation Regions"></a>Generation of High Spatial Resolution Terrestrial Surface from Low Spatial Resolution Elevation Contour Maps via Hierarchical Computation of Median Elevation Regions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09239">http://arxiv.org/abs/2307.09239</a></li>
<li>repo_url: None</li>
<li>paper_authors: Geetika Barman, B. S. Daya Sagar</li>
<li>for: 将稀疏的数字地形图（DEM）转换为密集的数字地形图。</li>
<li>methods: 使用 median contours 进行转换，包括 DEM 的含义情况下的分解、非负无权重的中值高程区域（MER）的计算和高程面的推算。</li>
<li>results: 可以在高分辨率上生成高精度的地形图。该方法考虑了现有的高程面信息，可以在新的地形表面上 interpolate 高程面，直到无需生成高程面为止。这种新的方法是low-cost和可靠的，并且使用高程面来进行推算。<details>
<summary>Abstract</summary>
We proposed a simple yet effective morphological approach to convert a sparse Digital Elevation Model (DEM) to a dense Digital Elevation Model. The conversion is similar to that of the generation of high-resolution DEM from its low-resolution DEM. The approach involves the generation of median contours to achieve the purpose. It is a sequential step of the I) decomposition of the existing sparse Contour map into the maximum possible Threshold Elevation Region (TERs). II) Computing all possible non-negative and non-weighted Median Elevation Region (MER) hierarchically between the successive TER decomposed from a sparse contour map. III) Computing the gradient of all TER, and MER computed from previous steps would yield the predicted intermediate elevation contour at a higher spatial resolution. We presented this approach initially with some self-made synthetic data to show how the contour prediction works and then experimented with the available contour map of Washington, NH to justify its usefulness. This approach considers the geometric information of existing contours and interpolates the elevation contour at a new spatial region of a topographic surface until no elevation contours are necessary to generate. This novel approach is also very low-cost and robust as it uses elevation contours.
</details>
<details>
<summary>摘要</summary>
我们提出了一种简单 yet有效的 morphological 方法，将稀疏的数字高程模型（DEM）转换成稠密的数字高程模型。这种转换类似于生成高分辨率 DEM 从其低分辨率 DEM 中。方法包括以下步骤：1. 将现有的稀疏 Contour 地图 decomposed 成最大可能的 Threshold Elevation Region (TER)。2. 计算所有可能的非负和无权重的 Median Elevation Region (MER) 在缺省 TER 中进行层次搜索。3. 计算所有 TER 和 MER 的梯度，并将之前步骤计算的 MER 作为输入，可以预测高程Contour 的预测值。我们首先使用自己制作的一些自然数据来示例ify 如何实现 contour 预测，然后对有效的 Contour 地图 of Washington, NH 进行了实验，以证明该方法的有用性。该方法考虑了现有 Contour 的 геометрической信息，并在 topographic 表面上 interpolate 高程Contour ntil no 高程Contour 是必须生成的。这种新的方法是非常低成本和可靠，因为它使用高程Contour。
</details></li>
</ul>
<hr>
<h2 id="Fusing-Hand-and-Body-Skeletons-for-Human-Action-Recognition-in-Assembly"><a href="#Fusing-Hand-and-Body-Skeletons-for-Human-Action-Recognition-in-Assembly" class="headerlink" title="Fusing Hand and Body Skeletons for Human Action Recognition in Assembly"></a>Fusing Hand and Body Skeletons for Human Action Recognition in Assembly</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09238">http://arxiv.org/abs/2307.09238</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dustin Aganian, Mona Köhler, Benedict Stephan, Markus Eisenbach, Horst-Michael Gross</li>
<li>for: 这个论文是为了提高人机合作的效果而写的。</li>
<li>methods: 这个论文使用了肢体骨架和手套骨架两种方法，并使用了通信网络和变换器来提高人机合作的效果。</li>
<li>results: 这个论文的实验结果表明，将肢体骨架和手套骨架结合使用可以提高人机合作的效果，并且可以更好地识别人员的动作。<details>
<summary>Abstract</summary>
As collaborative robots (cobots) continue to gain popularity in industrial manufacturing, effective human-robot collaboration becomes crucial. Cobots should be able to recognize human actions to assist with assembly tasks and act autonomously. To achieve this, skeleton-based approaches are often used due to their ability to generalize across various people and environments. Although body skeleton approaches are widely used for action recognition, they may not be accurate enough for assembly actions where the worker's fingers and hands play a significant role. To address this limitation, we propose a method in which less detailed body skeletons are combined with highly detailed hand skeletons. We investigate CNNs and transformers, the latter of which are particularly adept at extracting and combining important information from both skeleton types using attention. This paper demonstrates the effectiveness of our proposed approach in enhancing action recognition in assembly scenarios.
</details>
<details>
<summary>摘要</summary>
随着协作机器人（cobot）在工业生产中的普及，有效的人机合作变得非常重要。cobot应该能够识别人类行为，以帮助完成组装任务，并且能够自主行动。为了实现这一点，skeleton-based方法经常使用，因为它们能够通过不同的人和环境进行泛化。虽然身体骨架方法广泛用于动作识别，但它们可能无法准确地识别组装动作，因为工人的手和手指在这些动作中扮演着重要的角色。为了解决这一限制，我们提议一种方法，即将较为简单的身体骨架与高级细节的手骨架结合。我们研究了CNN和转换器，其中转换器特别适合将两种骨架中的重要信息拼接起来，使用吸引注意力。本文证明了我们提议的方法可以在组装场景中提高动作识别的效果。
</details></li>
</ul>
<hr>
<h2 id="Augmenting-CLIP-with-Improved-Visio-Linguistic-Reasoning"><a href="#Augmenting-CLIP-with-Improved-Visio-Linguistic-Reasoning" class="headerlink" title="Augmenting CLIP with Improved Visio-Linguistic Reasoning"></a>Augmenting CLIP with Improved Visio-Linguistic Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09233">http://arxiv.org/abs/2307.09233</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samyadeep Basu, Maziar Sanjabi, Daniela Massiceti, Shell Xu Hu, Soheil Feizi</li>
<li>for: 提高CLIP模型的compositional visio-linguistic理解能力</li>
<li>methods: 使用可微的图像参数化细化CLIP模型，通过带有扩散对象的梯度整理法，从大型文本到图像生成模型中抽取可以进行visio-linguistic理解任务的能力</li>
<li>results: 在Winoground和ARO datasets上，OUR方法可以提高CLIP模型的绝对visio-linguistic性能，在Winoground上提高了7%，在ARO上提高了3%，同时，OUR方法也微幅提高了CLIP模型的零shot性能在多个下游任务上。<details>
<summary>Abstract</summary>
Image-text contrastive models such as CLIP are useful for a variety of downstream applications including zero-shot classification, image-text retrieval and transfer learning. However, these contrastively trained vision-language models often fail on compositional visio-linguistic tasks such as Winoground with performance equivalent to random chance. In our paper, we address this issue and propose a sample-efficient light-weight method called SDS-CLIP to improve the compositional visio-linguistic reasoning capabilities of CLIP. The core idea of our method is to use differentiable image parameterizations to fine-tune CLIP with a distillation objective from large text-to-image generative models such as Stable-Diffusion which are relatively good at visio-linguistic reasoning tasks. On the challenging Winoground compositional reasoning benchmark, our method improves the absolute visio-linguistic performance of different CLIP models by up to 7%, while on the ARO dataset, our method improves the visio-linguistic performance by upto 3%. As a byproduct of inducing visio-linguistic reasoning into CLIP, we also find that the zero-shot performance improves marginally on a variety of downstream datasets. Our method reinforces that carefully designed distillation objectives from generative models can be leveraged to extend existing contrastive image-text models with improved visio-linguistic reasoning capabilities.
</details>
<details>
<summary>摘要</summary>
“图文共同模型如CLIP可以用于多种下游应用，包括零 shot 分类、图文搜寻和传播学习。然而，这些对照训练的视力语言模型经常在成本复杂的图文共同任务中表现不佳，例如 Winoground，其表现相当于随机几率。在我们的论文中，我们解决这个问题，并提出一个轻量级、可靠的方法called SDS-CLIP，以提高 CLIP 的图文共同推理能力。我们的方法的核心思想是使用可微的图像参数来精致地调整 CLIP，使其能够从大型文本至图生成模型中获得更好的推理能力。在 Winoground 的挑战性图文共同任务中，我们的方法可以提高不同 CLIP 模型的绝对图文推理性能 by up to 7%，而在 ARO dataset 中，我们的方法可以提高图文推理性能 by up to 3%。另外，我们发现，将 visio-linguistic 推理引入 CLIP 中，可以微小地提高零 shot 下的表现。我们的方法证明了，通过将生成模型的数据分配给对照训练的视力语言模型，可以延展现有的对照训练模型，以提高图文共同推理能力。”
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Open-Vocabulary-Detection-and-Segmentation-Past-Present-and-Future"><a href="#A-Survey-on-Open-Vocabulary-Detection-and-Segmentation-Past-Present-and-Future" class="headerlink" title="A Survey on Open-Vocabulary Detection and Segmentation: Past, Present, and Future"></a>A Survey on Open-Vocabulary Detection and Segmentation: Past, Present, and Future</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09220">http://arxiv.org/abs/2307.09220</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chaoyang Zhu, Long Chen</li>
<li>for: 这篇论文主要是为了提供一个对开放词汇检测（OVD）和开放词汇分割（OVS）的全面回顾和评论。</li>
<li>methods: 这篇论文使用了一种基于任务和方法的分类法来总结各种方法，包括视觉语义空间映射、新视觉特征合成、区域意识训练、pseudo标签、知识储存承诺和传输学习等。</li>
<li>results: 这篇论文对各种任务进行了全面的讨论，包括对象检测、semantic&#x2F;instance&#x2F;panoptic segmentation、3D场景和视频理解等，并对每个任务进行了细节的描述、主要挑战、发展路径、优点和缺点的评论。<details>
<summary>Abstract</summary>
As the most fundamental tasks of computer vision, object detection and segmentation have made tremendous progress in the deep learning era. Due to the expensive manual labeling, the annotated categories in existing datasets are often small-scale and pre-defined, i.e., state-of-the-art detectors and segmentors fail to generalize beyond the closed-vocabulary. To resolve this limitation, the last few years have witnessed increasing attention toward Open-Vocabulary Detection (OVD) and Segmentation (OVS). In this survey, we provide a comprehensive review on the past and recent development of OVD and OVS. To this end, we develop a taxonomy according to the type of task and methodology. We find that the permission and usage of weak supervision signals can well discriminate different methodologies, including: visual-semantic space mapping, novel visual feature synthesis, region-aware training, pseudo-labeling, knowledge distillation-based, and transfer learning-based. The proposed taxonomy is universal across different tasks, covering object detection, semantic/instance/panoptic segmentation, 3D scene and video understanding. In each category, its main principles, key challenges, development routes, strengths, and weaknesses are thoroughly discussed. In addition, we benchmark each task along with the vital components of each method. Finally, several promising directions are provided to stimulate future research.
</details>
<details>
<summary>摘要</summary>
Computer vision 的基本任务中，对象检测和分割在深度学习时代取得了巨大进步。然而，由于手动标注成本高昂，现有数据集中的标注类型通常是小规模的预定的，即现状顶尖的检测器和分割器无法泛化。为解决这种限制，过去几年内，开放词汇检测（OVD）和分割（OVS）受到了逐渐增长的关注。在这种调查中，我们提供了对过去和最近发展的OVD和OVS的全面评论。为此，我们开发了一个分类方法，根据任务类型和方法学习。我们发现，使用弱级指导信号的许可和使用可以很好地区分不同的方法，包括：视觉Semantic空间映射、新视觉特征合成、区域意识训练、pseudo-labeling、知识泛化学习和传输学习。提出的分类方法 universal across different tasks，涵盖对象检测、semantic/instance/panoptic分割、3D场景和视频理解。在每个类型中，我们详细讨论了主要原则、关键挑战、发展途径、优点和缺点。此外，我们对每个任务进行了评估，并与每种方法的重要组成部分进行了比较。最后，我们提供了一些有前途的方向，以促进未来的研究。
</details></li>
</ul>
<hr>
<h2 id="You’ve-Got-Two-Teachers-Co-evolutionary-Image-and-Report-Distillation-for-Semi-supervised-Anatomical-Abnormality-Detection-in-Chest-X-ray"><a href="#You’ve-Got-Two-Teachers-Co-evolutionary-Image-and-Report-Distillation-for-Semi-supervised-Anatomical-Abnormality-Detection-in-Chest-X-ray" class="headerlink" title="You’ve Got Two Teachers: Co-evolutionary Image and Report Distillation for Semi-supervised Anatomical Abnormality Detection in Chest X-ray"></a>You’ve Got Two Teachers: Co-evolutionary Image and Report Distillation for Semi-supervised Anatomical Abnormality Detection in Chest X-ray</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09184">http://arxiv.org/abs/2307.09184</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinghan Sun, Dong Wei, Zhe Xu, Donghuan Lu, Hong Liu, Liansheng Wang, Yefeng Zheng<br>for:* 这个论文旨在帮助抑制验图像检测和诊断呼吸系统和心血管疾病的 radiologic 发现。methods:* 这个方法使用了 semi-supervised 图像检测和文本分类方法，通过对报告和图像进行交互式融合，提高检测精度。results:* 实验结果表明，这个方法在 MIMIC-CXR 数据集上表现出色，较前者weakly和semi-supervised方法有更高的性能。<details>
<summary>Abstract</summary>
Chest X-ray (CXR) anatomical abnormality detection aims at localizing and characterising cardiopulmonary radiological findings in the radiographs, which can expedite clinical workflow and reduce observational oversights. Most existing methods attempted this task in either fully supervised settings which demanded costly mass per-abnormality annotations, or weakly supervised settings which still lagged badly behind fully supervised methods in performance. In this work, we propose a co-evolutionary image and report distillation (CEIRD) framework, which approaches semi-supervised abnormality detection in CXR by grounding the visual detection results with text-classified abnormalities from paired radiology reports, and vice versa. Concretely, based on the classical teacher-student pseudo label distillation (TSD) paradigm, we additionally introduce an auxiliary report classification model, whose prediction is used for report-guided pseudo detection label refinement (RPDLR) in the primary vision detection task. Inversely, we also use the prediction of the vision detection model for abnormality-guided pseudo classification label refinement (APCLR) in the auxiliary report classification task, and propose a co-evolution strategy where the vision and report models mutually promote each other with RPDLR and APCLR performed alternatively. To this end, we effectively incorporate the weak supervision by reports into the semi-supervised TSD pipeline. Besides the cross-modal pseudo label refinement, we further propose an intra-image-modal self-adaptive non-maximum suppression, where the pseudo detection labels generated by the teacher vision model are dynamically rectified by high-confidence predictions by the student. Experimental results on the public MIMIC-CXR benchmark demonstrate CEIRD's superior performance to several up-to-date weakly and semi-supervised methods.
</details>
<details>
<summary>摘要</summary>
胸部X射影（CXR）解剖异常检测目的在于寻找和描述胸部X射影中的呼吸和心脏 radiological 发现，以减少观察偏见和提高诊断工作流程。大多数现有方法都对这个任务进行了完全监督学习（fully supervised learning），需要高成本的大量偏例资料（per-abnormality annotations），或弱监督学习（weakly supervised learning），其性能仍然落后于完全监督学习方法。在这个工作中，我们提出了一个共演化图像和报告蒸发（CEIRD）框架，它通过将视觉检测结果与文本标示的异常发现（report-guided pseudo detection label refinement，RPDLR）和视觉检测模型的预测（abnormality-guided pseudo classification label refinement，APCLR）进行互动，以实现半监督学习的异常检测。此外，我们还提出了一个共演化策略，让视觉和报告模型互相推广 each other，通过对应的RPDLR和APCLR进行交替进行。这样，我们实现了对报告的弱监督学习，并将其 integrate 到 semi-supervised TSD 管道中。除了跨模式 pseudo label refinement 外，我们还提出了一个内部图像模式自适应非最大 suppress，其中 pseudo detection labels 生成的教师视觉模型的预测被动地修正了由学生高信度预测的高信度预测。实验结果显示，CEIRD 在公共 MIMIC-CXR benchmark 上表现出色，较上一代弱监督和半监督方法更高。
</details></li>
</ul>
<hr>
<h2 id="Pixel-wise-Graph-Attention-Networks-for-Person-Re-identification"><a href="#Pixel-wise-Graph-Attention-Networks-for-Person-Re-identification" class="headerlink" title="Pixel-wise Graph Attention Networks for Person Re-identification"></a>Pixel-wise Graph Attention Networks for Person Re-identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09183">http://arxiv.org/abs/2307.09183</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wenyu1009/pganet">https://github.com/wenyu1009/pganet</a></li>
<li>paper_authors: Wenyu Zhang, Qing Ding, Jian Hu, Yi Ma, Mingzhe Lu</li>
<li>for: 本研究用于探讨图像特征提取中使用图 convolutional networks (GCN) 和图注意力网络 (GAT)，以提高图像识别性能。</li>
<li>methods: 本研究提出了一种新的图生成算法，可以将图像转换成图形，并使用GAT更新节点特征。这两个步骤组成了一个图像精度注意模块 (PGA)，可以与传统的图像特征提取方法结合使用。基于PGA和ResNet，提出了一种新的像素精度graph注意网络 (PGANet)，用于人识别任务。</li>
<li>results: 对于Market1501、DukeMTMC-reID和Occluded-DukeMTMC等 datasets，PGANet可以达到最佳性能，比前一个state-of-the-art的0.8%、1.1%和11%。<details>
<summary>Abstract</summary>
Graph convolutional networks (GCN) is widely used to handle irregular data since it updates node features by using the structure information of graph. With the help of iterated GCN, high-order information can be obtained to further enhance the representation of nodes. However, how to apply GCN to structured data (such as pictures) has not been deeply studied. In this paper, we explore the application of graph attention networks (GAT) in image feature extraction. First of all, we propose a novel graph generation algorithm to convert images into graphs through matrix transformation. It is one magnitude faster than the algorithm based on K Nearest Neighbors (KNN). Then, GAT is used on the generated graph to update the node features. Thus, a more robust representation is obtained. These two steps are combined into a module called pixel-wise graph attention module (PGA). Since the graph obtained by our graph generation algorithm can still be transformed into a picture after processing, PGA can be well combined with CNN. Based on these two modules, we consulted the ResNet and design a pixel-wise graph attention network (PGANet). The PGANet is applied to the task of person re-identification in the datasets Market1501, DukeMTMC-reID and Occluded-DukeMTMC (outperforms state-of-the-art by 0.8\%, 1.1\% and 11\% respectively, in mAP scores). Experiment results show that it achieves the state-of-the-art performance. \href{https://github.com/wenyu1009/PGANet}{The code is available here}.
</details>
<details>
<summary>摘要</summary>
Graph convolutional networks (GCN) 广泛应用于不规则数据处理，因为它利用图structure信息来更新节点特征。然而，如何将 GCN 应用于结构数据（如图片）尚未得到深入研究。在这篇论文中，我们探索使用图注意网络（GAT）在图像特征提取中的应用。首先，我们提出了一种新的图生成算法，通过矩阵变换将图像转换为图。这个算法比基于 K Nearest Neighbors（KNN）的算法一 magnitude faster。然后，GAT 在生成的图上更新节点特征。因此，我们获得了一个更加稳定的表示。这两个步骤组合在一起，我们称之为像素级别图注意模块（PGA）。由于生成的图仍可以转换为图像 после处理，PGA 可以与 CNN 集成。基于这两个模块，我们咨询了 ResNet，并设计了像素级别图注意网络（PGANet）。PGANet 在 Market1501、DukeMTMC-reID 和 Occluded-DukeMTMC 数据集上进行人重复识别任务，与状态艺术的比例分别高于 0.8\%、1.1\% 和 11\%。实验结果表明，它达到了状态艺术性能。 链接：<https://github.com/wenyu1009/PGANet>
</details></li>
</ul>
<hr>
<h2 id="Jean-Luc-Picard-at-Touche-2023-Comparing-Image-Generation-Stance-Detection-and-Feature-Matching-for-Image-Retrieval-for-Arguments"><a href="#Jean-Luc-Picard-at-Touche-2023-Comparing-Image-Generation-Stance-Detection-and-Feature-Matching-for-Image-Retrieval-for-Arguments" class="headerlink" title="Jean-Luc Picard at Touché 2023: Comparing Image Generation, Stance Detection and Feature Matching for Image Retrieval for Arguments"></a>Jean-Luc Picard at Touché 2023: Comparing Image Generation, Stance Detection and Feature Matching for Image Retrieval for Arguments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09172">http://arxiv.org/abs/2307.09172</a></li>
<li>repo_url: None</li>
<li>paper_authors: Max Moebius, Maximilian Enderling, Sarah T. Bachinger</li>
<li>for: 参加共同任务”图像检索论据”，我们使用了不同的图像检索管道，包括图像生成、立场检测、预选和特征匹配。我们提交了四个不同的运行，并与基准值进行比较。我们的管道表现和基准值类似。</li>
<li>methods: 我们使用了不同的图像检索管道，包括图像生成、立场检测、预选和特征匹配。</li>
<li>results: 我们的管道表现和基准值类似。<details>
<summary>Abstract</summary>
Participating in the shared task "Image Retrieval for arguments", we used different pipelines for image retrieval containing Image Generation, Stance Detection, Preselection and Feature Matching. We submitted four different runs with different pipeline layout and compare them to given baseline. Our pipelines perform similarly to the baseline.
</details>
<details>
<summary>摘要</summary>
参加了“图像检索 для论点”共同任务，我们使用了不同的图像检索管道，包括图像生成、立场检测、预选和特征匹配。我们提交了四个不同的运行，并与给定的基线进行比较。我们的管道表现和基线相似。Note that the word "baseline" in the original text was translated as "基线" in Simplified Chinese, which is a common way to refer to a reference point or a standard for comparison.
</details></li>
</ul>
<hr>
<h2 id="ECSIC-Epipolar-Cross-Attention-for-Stereo-Image-Compression"><a href="#ECSIC-Epipolar-Cross-Attention-for-Stereo-Image-Compression" class="headerlink" title="ECSIC: Epipolar Cross Attention for Stereo Image Compression"></a>ECSIC: Epipolar Cross Attention for Stereo Image Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10284">http://arxiv.org/abs/2307.10284</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthias Wödlinger, Jan Kotera, Manuel Keglevic, Jan Xu, Robert Sablatnig</li>
<li>for: 这 paper 的目的是提出一种新的学习方法 для双眼图像压缩。</li>
<li>methods: 该方法使用一种新的 crossed attention（SCA）模块和两个双眼上下文模块来同时压缩左右图像。SCA模块在对应的投影线上进行权重 Restricted cross-attention 处理，并在平行进行处理。双眼上下文模块使用第一个图像作为上下文来提高第二个编码图像的Entropy 估计。</li>
<li>results: ECSIC 在 Cityscapes 和 InStereo2k 两个流行的双眼图像数据集上达到了当今最佳性能，而且具有快速编码和解码功能，因此在实时应用中非常实用。<details>
<summary>Abstract</summary>
In this paper, we present ECSIC, a novel learned method for stereo image compression. Our proposed method compresses the left and right images in a joint manner by exploiting the mutual information between the images of the stereo image pair using a novel stereo cross attention (SCA) module and two stereo context modules. The SCA module performs cross-attention restricted to the corresponding epipolar lines of the two images and processes them in parallel. The stereo context modules improve the entropy estimation of the second encoded image by using the first image as a context. We conduct an extensive ablation study demonstrating the effectiveness of the proposed modules and a comprehensive quantitative and qualitative comparison with existing methods. ECSIC achieves state-of-the-art performance among stereo image compression models on the two popular stereo image datasets Cityscapes and InStereo2k while allowing for fast encoding and decoding, making it highly practical for real-time applications.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的学习方法ECSIC，用于压缩立体图像。我们的提议方法同时压缩左右图像，通过利用立体图像对的补做和两个立体上下文模块来提高压缩率。SCA模块在对应的epipolar线上进行交叉注意力限制，并在平行进行处理。立体上下文模块使得第二个编码图像的Entropy估计得到改善，通过使用第一个图像作为上下文。我们进行了广泛的减少研究，证明提议的模块的有效性，并对现有方法进行了全面的量化和质量性比较。ECSIC在Cityscapes和InStereo2k两个流行的立体图像数据集上实现了最新的性能，同时允许快速编码和解码，因此在实时应用中非常实用。
</details></li>
</ul>
<hr>
<h2 id="Towards-Trustworthy-Dataset-Distillation"><a href="#Towards-Trustworthy-Dataset-Distillation" class="headerlink" title="Towards Trustworthy Dataset Distillation"></a>Towards Trustworthy Dataset Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09165">http://arxiv.org/abs/2307.09165</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shijie Ma, Fei Zhu, Zhen Cheng, Xu-Yao Zhang<br>for:Trustworthy Dataset Distillation (TrustDD) aims to reduce training costs and enhance model trustworthiness in real-world applications by distilling both in-distribution (InD) samples and outliers.methods:The proposed method uses dataset distillation (DD) to reduce the training dataset to a tiny synthetic dataset, and simultaneously considers in-distribution (InD) classification and out-of-distribution (OOD) detection. To generate pseudo-outliers, InD samples are corrupted to introduce Pseudo-Outlier Exposure (POE).results:Comprehensive experiments on various settings demonstrate the effectiveness of TrustDD, and the proposed POE surpasses state-of-the-art method Outlier Exposure (OE). TrustDD is more trustworthy and applicable to real open-world scenarios compared to previous dataset distillation methods.Here is the format you requested:for:  Trustworthy Dataset Distillation (TrustDD)methods:  dataset distillation (DD) + Pseudo-Outlier Exposure (POE)results:  effective in InD classification and OOD detection, surpasses state-of-the-art OE method.<details>
<summary>Abstract</summary>
Efficiency and trustworthiness are two eternal pursuits when applying deep learning in real-world applications. With regard to efficiency, dataset distillation (DD) endeavors to reduce training costs by distilling the large dataset into a tiny synthetic dataset. However, existing methods merely concentrate on in-distribution (InD) classification in a closed-world setting, disregarding out-of-distribution (OOD) samples. On the other hand, OOD detection aims to enhance models' trustworthiness, which is always inefficiently achieved in full-data settings. For the first time, we simultaneously consider both issues and propose a novel paradigm called Trustworthy Dataset Distillation (TrustDD). By distilling both InD samples and outliers, the condensed datasets are capable to train models competent in both InD classification and OOD detection. To alleviate the requirement of real outlier data and make OOD detection more practical, we further propose to corrupt InD samples to generate pseudo-outliers and introduce Pseudo-Outlier Exposure (POE). Comprehensive experiments on various settings demonstrate the effectiveness of TrustDD, and the proposed POE surpasses state-of-the-art method Outlier Exposure (OE). Compared with the preceding DD, TrustDD is more trustworthy and applicable to real open-world scenarios. Our code will be publicly available.
</details>
<details>
<summary>摘要</summary>
“效率和可靠性是深度学习应用实际场景中的两大永恒追求。在这个领域，数据集缩写（DD）是一种减少训练成本的方法，它将大量数据集缩写成一个小型的合成数据集。然而，现有方法仅专注于闭合世界的内分布（InD）类别，忽略了外分布（OOD）样本。而OOD检测则是提高模型的可靠性的一种不fficient的方法，通常在全数据设置下进行。为了解决这两个问题，我们同时考虑了两者，并提出了一种新的思路——可靠的数据集缩写（TrustDD）。通过缩写InD样本和异常样本，缩写后的数据集能够训练可以在InD类别和OOD检测中具备竞争力。为了减少实际异常数据的需求和使OOD检测更加实用，我们进一步提议在InD样本中做质量损害，并引入假异常样本的暴露（POE）。经过了多种场景的实验，我们发现TrustDD比前一代DD更加可靠和适用于真正的开放世界场景。我们的代码将公开。”
</details></li>
</ul>
<hr>
<h2 id="CG-fusion-CAM-Online-segmentation-of-laser-induced-damage-on-large-aperture-optics"><a href="#CG-fusion-CAM-Online-segmentation-of-laser-induced-damage-on-large-aperture-optics" class="headerlink" title="CG-fusion CAM: Online segmentation of laser-induced damage on large-aperture optics"></a>CG-fusion CAM: Online segmentation of laser-induced damage on large-aperture optics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09161">http://arxiv.org/abs/2307.09161</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yueyue Han, Yingyan Huang, Hangcheng Dong, Fengdong Chen, Fa Zeng, Zhitao Peng, Qihua Zhu, Guodong Liu</li>
<li>for: 这个论文是为了解决大光学望远镜上高功率激光器所产生的激光损害问题，特别是在线分 segmentation方面。</li>
<li>methods: 这个论文使用了一种弱类别Semantic segmentation算法，即Continuous Gradient CAM和其多尺度融合（CG-fusion CAM）。这种算法可以使用仅仅有图像级别标签，并且可以生成高精度的类 activation maps。</li>
<li>results: 实验结果表明，该算法可以与全程supervised算法相比，达到相同的分 segmentation性能。<details>
<summary>Abstract</summary>
Online segmentation of laser-induced damage on large-aperture optics in high-power laser facilities is challenged by complicated damage morphology, uneven illumination and stray light interference. Fully supervised semantic segmentation algorithms have achieved state-of-the-art performance, but rely on plenty of pixel-level labels, which are time-consuming and labor-consuming to produce. LayerCAM, an advanced weakly supervised semantic segmentation algorithm, can generate pixel-accurate results using only image-level labels, but its scattered and partially under-activated class activation regions degrade segmentation performance. In this paper, we propose a weakly supervised semantic segmentation method with Continuous Gradient CAM and its nonlinear multi-scale fusion (CG-fusion CAM). The method redesigns the way of back-propagating gradients and non-linearly activates the multi-scale fused heatmaps to generate more fine-grained class activation maps with appropriate activation degree for different sizes of damage sites. Experiments on our dataset show that the proposed method can achieve segmentation performance comparable to that of fully supervised algorithms.
</details>
<details>
<summary>摘要</summary>
在高功率激光设施中，大光学口径上的激光引起的损害分 segmentation 是由于损害形态复杂、不均匀照明和干扰噪音的问题困难。全程supervised的semantic segmentation算法已经达到了状态的前沿性，但是它们需要大量的像素级标签，生成这些标签是时间consuming和劳动密集的。LayerCAM是一种先进的弱supervised semantic segmentation算法，可以生成像素精度的结果，但是它的分布式和部分地下活化的热点区域会降低分 segmentation 性能。在这篇论文中，我们提出了一种弱supervised semantic segmentation方法，使用Continuous Gradient CAM和非线性多尺度融合（CG-fusion CAM）。该方法重新定义了后向推导的方式和非线性激活多尺度融合的热点映射，以生成更细致的类活动地图，并且对不同的损害 Site 的大小进行适度的激活。实验表明，我们的方法可以与全程supervised算法相当的segmentation性能。
</details></li>
</ul>
<hr>
<h2 id="Constraining-Depth-Map-Geometry-for-Multi-View-Stereo-A-Dual-Depth-Approach-with-Saddle-shaped-Depth-Cells"><a href="#Constraining-Depth-Map-Geometry-for-Multi-View-Stereo-A-Dual-Depth-Approach-with-Saddle-shaped-Depth-Cells" class="headerlink" title="Constraining Depth Map Geometry for Multi-View Stereo: A Dual-Depth Approach with Saddle-shaped Depth Cells"></a>Constraining Depth Map Geometry for Multi-View Stereo: A Dual-Depth Approach with Saddle-shaped Depth Cells</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09160">http://arxiv.org/abs/2307.09160</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dive128/dmvsnet">https://github.com/dive128/dmvsnet</a></li>
<li>paper_authors: Xinyi Ye, Weiyue Zhao, Tianqi Liu, Zihao Huang, Zhiguo Cao, Xin Li</li>
<li>for: 本研究旨在提高多视图深度（MVS）方法的准确性和完整性，通过适合的深度几何来提高深度估计的精度。</li>
<li>methods: 我们提出了一种基于学习的多视图深度方法，即DUAL-MVSNet，它可以生成oscillating深度平面。技术上，我们预测每个像素两个深度值（双深度），并提出了一种新的损失函数和检查板形式的选择策略来限制预测的深度几何。</li>
<li>results: 与现有方法相比，DUAL-MVSNet在DTU benchmark上得到了高排名，并在复杂的场景下（如坦克和寺庐）达到了最高性能，这表明了我们的方法具有强大的表现和泛化能力。我们的方法还指出了考虑深度几何在MVS方面的新研究方向。<details>
<summary>Abstract</summary>
Learning-based multi-view stereo (MVS) methods deal with predicting accurate depth maps to achieve an accurate and complete 3D representation. Despite the excellent performance, existing methods ignore the fact that a suitable depth geometry is also critical in MVS. In this paper, we demonstrate that different depth geometries have significant performance gaps, even using the same depth prediction error. Therefore, we introduce an ideal depth geometry composed of Saddle-Shaped Cells, whose predicted depth map oscillates upward and downward around the ground-truth surface, rather than maintaining a continuous and smooth depth plane. To achieve it, we develop a coarse-to-fine framework called Dual-MVSNet (DMVSNet), which can produce an oscillating depth plane. Technically, we predict two depth values for each pixel (Dual-Depth), and propose a novel loss function and a checkerboard-shaped selecting strategy to constrain the predicted depth geometry. Compared to existing methods,DMVSNet achieves a high rank on the DTU benchmark and obtains the top performance on challenging scenes of Tanks and Temples, demonstrating its strong performance and generalization ability. Our method also points to a new research direction for considering depth geometry in MVS.
</details>
<details>
<summary>摘要</summary>
To achieve this, we develop a coarse-to-fine framework called Dual-MVSNet (DMVSNet), which predicts two depth values for each pixel (Dual-Depth) and uses a novel loss function and checkerboard-shaped selecting strategy to constrain the predicted depth geometry. Compared to existing methods, DMVSNet achieves a high rank on the DTU benchmark and obtains the top performance on challenging scenes of Tanks and Temples, demonstrating its strong performance and generalization ability. Our method also highlights the importance of considering depth geometry in MVS and opens up a new research direction in this area.
</details></li>
</ul>
<hr>
<h2 id="Class-relation-Knowledge-Distillation-for-Novel-Class-Discovery"><a href="#Class-relation-Knowledge-Distillation-for-Novel-Class-Discovery" class="headerlink" title="Class-relation Knowledge Distillation for Novel Class Discovery"></a>Class-relation Knowledge Distillation for Novel Class Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09158">http://arxiv.org/abs/2307.09158</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kleinzcy/cr-kd-ncd">https://github.com/kleinzcy/cr-kd-ncd</a></li>
<li>paper_authors: Gu Peiyan, Zhang Chuyu, Xu Ruiji, He Xuming</li>
<li>for: 本研究目标是无监督学习新类，通过已知类数据来学习未知类。</li>
<li>methods: 我们引入了一个基于预测模型已知类分布的类关系表示，并使用知识填充框架来正则化学习新类。我们还开发了一种可学习的权重函数，以适应每个数据点在新类中的知识传递。</li>
<li>results: 我们在多个 benchmark 上进行了广泛的实验，并证明了我们的方法可以与之前的状态时间比对较高。<details>
<summary>Abstract</summary>
We tackle the problem of novel class discovery, which aims to learn novel classes without supervision based on labeled data from known classes. A key challenge lies in transferring the knowledge in the known-class data to the learning of novel classes. Previous methods mainly focus on building a shared representation space for knowledge transfer and often ignore modeling class relations. To address this, we introduce a class relation representation for the novel classes based on the predicted class distribution of a model trained on known classes. Empirically, we find that such class relation becomes less informative during typical discovery training. To prevent such information loss, we propose a novel knowledge distillation framework, which utilizes our class-relation representation to regularize the learning of novel classes. In addition, to enable a flexible knowledge distillation scheme for each data point in novel classes, we develop a learnable weighting function for the regularization, which adaptively promotes knowledge transfer based on the semantic similarity between the novel and known classes. To validate the effectiveness and generalization of our method, we conduct extensive experiments on multiple benchmarks, including CIFAR100, Stanford Cars, CUB, and FGVC-Aircraft datasets. Our results demonstrate that the proposed method outperforms the previous state-of-the-art methods by a significant margin on almost all benchmarks. Code is available at \href{https://github.com/kleinzcy/Cr-KD-NCD}{here}.
</details>
<details>
<summary>摘要</summary>
我们解决了一个新类发现问题，即在无监督下学习新类，基于已知类的标注数据。知情挑战在传递已知类数据中的知识到新类学习中。以前的方法主要集中在建立已知类和新类之间的共享表示空间中，frequently ignore 新类之间的类关系。为了解决这一点，我们引入一个基于已知类模型预测的新类分布的类关系表示。我们发现，在普通的发现训练中，这个类关系表示变得 menos informative。为了避免这种信息损失，我们提出了一种新的知识塑化框架，利用我们的类关系表示来塑化新类的学习。此外，我们开发了一个可学习的权重函数，以便在每个新类数据点上进行自适应的知识传递优化。为了证明我们的方法的有效性和普适性，我们在多个benchmark上进行了广泛的实验，包括CIFAR100、Stanford Cars、CUB和FGVC-Aircraft等数据集。我们的结果表明，我们的方法在大多数benchmark上与之前的状态OF THE ART方法之间的差距非常大。代码可以在\href{https://github.com/kleinzcy/Cr-KD-NCD}{这里}找到。
</details></li>
</ul>
<hr>
<h2 id="MLF-DET-Multi-Level-Fusion-for-Cross-Modal-3D-Object-Detection"><a href="#MLF-DET-Multi-Level-Fusion-for-Cross-Modal-3D-Object-Detection" class="headerlink" title="MLF-DET: Multi-Level Fusion for Cross-Modal 3D Object Detection"></a>MLF-DET: Multi-Level Fusion for Cross-Modal 3D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09155">http://arxiv.org/abs/2307.09155</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zewei Lin, Yanqing Shen, Sanping Zhou, Shitao Chen, Nanning Zheng</li>
<li>for: 这 paper 的目的是提出一种高性能的跨模态3D物体检测方法，以便更好地利用图像中的信息。</li>
<li>methods: 这 paper 使用了多级融合网络（MLF-DET），包括特征级融合和决策级融合两个部分。特征级融合使用多scale voxel图像融合模块（MVI），决策级融合使用轻量级特征引导修正模块（FCR）。此外，paper 还提出了一种有效的数据采样策略，即遮挡对准GT采样（OGS），以增加训练场景中的样本数量，从而降低过拟合。</li>
<li>results: EXTENSIVE experiments 表明，我们的方法在 KITTI 数据集上达到了 82.89% 的中等 AP 值，并在不具备特殊功能的情况下达到了领先的性能。<details>
<summary>Abstract</summary>
In this paper, we propose a novel and effective Multi-Level Fusion network, named as MLF-DET, for high-performance cross-modal 3D object DETection, which integrates both the feature-level fusion and decision-level fusion to fully utilize the information in the image. For the feature-level fusion, we present the Multi-scale Voxel Image fusion (MVI) module, which densely aligns multi-scale voxel features with image features. For the decision-level fusion, we propose the lightweight Feature-cued Confidence Rectification (FCR) module which further exploits image semantics to rectify the confidence of detection candidates. Besides, we design an effective data augmentation strategy termed Occlusion-aware GT Sampling (OGS) to reserve more sampled objects in the training scenes, so as to reduce overfitting. Extensive experiments on the KITTI dataset demonstrate the effectiveness of our method. Notably, on the extremely competitive KITTI car 3D object detection benchmark, our method reaches 82.89% moderate AP and achieves state-of-the-art performance without bells and whistles.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的和有效的多级融合网络，称为MLF-DET，用于高性能跨模态3D对象检测。我们在这种网络中结合了功能级融合和决策级融合，以完全利用图像中的信息。对于功能级融合，我们提出了多尺度精细对齐模块（MVI），用于 densely aligning multi-scale voxel features with image features。对于决策级融合，我们提出了轻量级特征引导修正模块（FCR），以进一步利用图像 semantics 来修正检测候选者的信任度。此外，我们设计了一种有效的数据增强策略，称为Occlusion-aware GT Sampling（OGS），以保留更多的批处理对象在训练场景中，从而降低过拟合。广泛的实验表明，我们的方法在KITTI数据集上达到了82.89%的中等AP值，并在无论钟铃铃的情况下实现了state-of-the-art性能。
</details></li>
</ul>
<hr>
<h2 id="OPHAvatars-One-shot-Photo-realistic-Head-Avatars"><a href="#OPHAvatars-One-shot-Photo-realistic-Head-Avatars" class="headerlink" title="OPHAvatars: One-shot Photo-realistic Head Avatars"></a>OPHAvatars: One-shot Photo-realistic Head Avatars</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09153">http://arxiv.org/abs/2307.09153</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lsx0101/ophavatars">https://github.com/lsx0101/ophavatars</a></li>
<li>paper_authors: Shaoxu Li</li>
<li>for: 该paper的目的是创建一种从单个肖像图像 synthesize photo-realistic digital avatars的方法。</li>
<li>methods: 该方法使用驱动关键点特征来生成一个粗糙的说话头视频，然后使用扭曲神经辐射场来生成粗糙的说话头模型。通过更新优化的图像，该方法可以重新训练更高质量的模型。</li>
<li>results: 该方法可以在多个主题下对多个主题进行高效的量化和质量上的比较，并且可以在不同的照明和摄影条件下提供高质量的数字人物模型。<details>
<summary>Abstract</summary>
We propose a method for synthesizing photo-realistic digital avatars from only one portrait as the reference. Given a portrait, our method synthesizes a coarse talking head video using driving keypoints features. And with the coarse video, our method synthesizes a coarse talking head avatar with a deforming neural radiance field. With rendered images of the coarse avatar, our method updates the low-quality images with a blind face restoration model. With updated images, we retrain the avatar for higher quality. After several iterations, our method can synthesize a photo-realistic animatable 3D neural head avatar. The motivation of our method is deformable neural radiance field can eliminate the unnatural distortion caused by the image2video method. Our method outperforms state-of-the-art methods in quantitative and qualitative studies on various subjects.
</details>
<details>
<summary>摘要</summary>
我们提出一种方法，可以从单一的肖像中生成真实的数字人物。给定一个肖像，我们的方法可以生成一个驱动关键点特征的粗糙说话头视频。然后，我们的方法可以使用扭曲神经辐射场来生成粗糙说话头人物。使用渲染出的粗糙人物图像，我们的方法可以通过盲人脸修复模型来更新低质量图像。经过多次迭代，我们的方法可以生成一个真实的渲染3D神经头人物。我们的方法的动机是使用扭曲神经辐射场可以消除图像2视频方法中的不自然的扭曲。与现状的方法进行比较，我们的方法在量化和质量上都有较高的表现。
</details></li>
</ul>
<hr>
<h2 id="Semi-supervised-Cycle-GAN-for-face-photo-sketch-translation-in-the-wild"><a href="#Semi-supervised-Cycle-GAN-for-face-photo-sketch-translation-in-the-wild" class="headerlink" title="Semi-supervised Cycle-GAN for face photo-sketch translation in the wild"></a>Semi-supervised Cycle-GAN for face photo-sketch translation in the wild</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10281">http://arxiv.org/abs/2307.10281</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chaofengc/Face-Sketch-SCG">https://github.com/chaofengc/Face-Sketch-SCG</a></li>
<li>paper_authors: Chaofeng Chen, Wei Liu, Xiao Tan, Kwan-Yee K. Wong</li>
<li>for: 本研究旨在提高面图翻译效果，使用深度神经网络和GAN方法。</li>
<li>methods: 我们提出了一种半监督方法，使用噪音插入策略，称为半周期GAN（SCG）。我们首先对输入图片进行 pseudo sketch 特征表示，并使用这些 pseudo pairs 来监督图片到素描 generator $G_{p2s}$。然后，$G_{p2s}$ 的输出可以用来自动地训练素描到图片 generator $G_{s2p}$。这样就可以使用一小reference set of photo-sketch pairs和一大面积的人脸图像集（没有ground-truth sketches）来训练 $G_{p2s}$ 和 $G_{s2p}$。</li>
<li>results: 实验结果表明，SCG 可以在公共 bencmark 上达到竞争性的性能，并在野外的图像上得到更加有reasonable的素描-to-图片结果，具有较少的过拟合问题。<details>
<summary>Abstract</summary>
The performance of face photo-sketch translation has improved a lot thanks to deep neural networks. GAN based methods trained on paired images can produce high-quality results under laboratory settings. Such paired datasets are, however, often very small and lack diversity. Meanwhile, Cycle-GANs trained with unpaired photo-sketch datasets suffer from the \emph{steganography} phenomenon, which makes them not effective to face photos in the wild. In this paper, we introduce a semi-supervised approach with a noise-injection strategy, named Semi-Cycle-GAN (SCG), to tackle these problems. For the first problem, we propose a {\em pseudo sketch feature} representation for each input photo composed from a small reference set of photo-sketch pairs, and use the resulting {\em pseudo pairs} to supervise a photo-to-sketch generator $G_{p2s}$. The outputs of $G_{p2s}$ can in turn help to train a sketch-to-photo generator $G_{s2p}$ in a self-supervised manner. This allows us to train $G_{p2s}$ and $G_{s2p}$ using a small reference set of photo-sketch pairs together with a large face photo dataset (without ground-truth sketches). For the second problem, we show that the simple noise-injection strategy works well to alleviate the \emph{steganography} effect in SCG and helps to produce more reasonable sketch-to-photo results with less overfitting than fully supervised approaches. Experiments show that SCG achieves competitive performance on public benchmarks and superior results on photos in the wild.
</details>
<details>
<summary>摘要</summary>
《面部照片翻译表现得到了深度神经网络的改进，使得GAN基于方法在实验室设置下可以生成高质量的结果。然而，这些配对数据集通常很小，缺乏多样性。同时，基于不配对照片翻译数据集的Cycle-GANs受到了《隐写》现象的影响，使其对面部照片在野外不 efective。本文提出了一种半指导式方法，名为半周期GAN（SCG），以解决这些问题。首先，我们提出了一种《假绘制特征》表示方法，用于每个输入照片中组成一个小型参考集的绘制对。然后，我们使用这些《假对》来监督一个照片到绘制的生成器$G_{p2s}$。$G_{p2s}$的输出可以在自动化的方式下帮助训练一个绘制到照片的生成器$G_{s2p}$。这样就可以在一个小型的照片-绘制对参考集和一大量的面部照片数据集（无ground truth绘制）之间共同训练 $G_{p2s}$ 和 $G_{s2p}$。其次，我们发现了一种简单的噪声注入策略可以减轻SCG中的《隐写》现象，并帮助生成更加合理的绘制到照片结果，降低过拟合。实验表明，SCG可以在公共的benchmark上达到竞争性的表现，并在野外的照片上达到更高的表现。》
</details></li>
</ul>
<hr>
<h2 id="PRO-Face-S-Privacy-preserving-Reversible-Obfuscation-of-Face-Images-via-Secure-Flow"><a href="#PRO-Face-S-Privacy-preserving-Reversible-Obfuscation-of-Face-Images-via-Secure-Flow" class="headerlink" title="PRO-Face S: Privacy-preserving Reversible Obfuscation of Face Images via Secure Flow"></a>PRO-Face S: Privacy-preserving Reversible Obfuscation of Face Images via Secure Flow</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09146">http://arxiv.org/abs/2307.09146</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lin Yuan, Kai Liang, Xiao Pu, Yan Zhang, Jiaxu Leng, Tao Wu, Nannan Wang, Xinbo Gao</li>
<li>for: 防止面部隐私泄露</li>
<li>methods: 使用逆向神经网络（INN）进行隐私保护，并在网络中注入密钥以确保原始图像仅可以通过同一模型和正确的密钥进行恢复。</li>
<li>results: 对于多个公共图像集进行了广泛的实验，证明提议的框架在对多种现有方法的比较中具有超越性。<details>
<summary>Abstract</summary>
This paper proposes a novel paradigm for facial privacy protection that unifies multiple characteristics including anonymity, diversity, reversibility and security within a single lightweight framework. We name it PRO-Face S, short for Privacy-preserving Reversible Obfuscation of Face images via Secure flow-based model. In the framework, an Invertible Neural Network (INN) is utilized to process the input image along with its pre-obfuscated form, and generate the privacy protected image that visually approximates to the pre-obfuscated one, thus ensuring privacy. The pre-obfuscation applied can be in diversified form with different strengths and styles specified by users. Along protection, a secret key is injected into the network such that the original image can only be recovered from the protection image via the same model given the correct key provided. Two modes of image recovery are devised to deal with malicious recovery attempts in different scenarios. Finally, extensive experiments conducted on three public image datasets demonstrate the superiority of the proposed framework over multiple state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
In this framework, an Invertible Neural Network (INN) is used to process the input image and its pre-obfuscated form, and generate a privacy-protected image that visually resembles the pre-obfuscated one, ensuring privacy. The pre-obfuscation can be applied in diverse forms with different strengths and styles specified by users. Additionally, a secret key is injected into the network, allowing only the original image to be recovered from the protected image through the same model if the correct key is provided. To address malicious recovery attempts, two modes of image recovery are designed.Experiments conducted on three public image datasets demonstrate the superiority of the proposed framework compared to multiple state-of-the-art approaches.
</details></li>
</ul>
<hr>
<h2 id="MVA2023-Small-Object-Detection-Challenge-for-Spotting-Birds-Dataset-Methods-and-Results"><a href="#MVA2023-Small-Object-Detection-Challenge-for-Spotting-Birds-Dataset-Methods-and-Results" class="headerlink" title="MVA2023 Small Object Detection Challenge for Spotting Birds: Dataset, Methods, and Results"></a>MVA2023 Small Object Detection Challenge for Spotting Birds: Dataset, Methods, and Results</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09143">http://arxiv.org/abs/2307.09143</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iim-ttij/mva2023smallobjectdetection4spottingbirds">https://github.com/iim-ttij/mva2023smallobjectdetection4spottingbirds</a></li>
<li>paper_authors: Yuki Kondo, Norimichi Ukita, Takayuki Yamaguchi, Hao-Yu Hou, Mu-Yi Shen, Chia-Chi Hsu, En-Ming Huang, Yu-Chen Huang, Yu-Cheng Xia, Chien-Yao Wang, Chun-Yi Lee, Da Huo, Marc A. Kastner, Tingwei Liu, Yasutomo Kawanishi, Takatsugu Hirayama, Takahiro Komamizu, Ichiro Ide, Yosuke Shinya, Xinyao Liu, Guang Liang, Syusuke Yasui</li>
<li>for: 本研究旨在提供一个新的小对象检测数据集，用于鸟类检测。</li>
<li>methods: 本研究使用了一种新的小对象检测方法，包括提出了一个新的数据集。</li>
<li>results: 本研究的实验结果显示，这种新的检测方法可以准确地检测到远距离的鸟类。<details>
<summary>Abstract</summary>
Small Object Detection (SOD) is an important machine vision topic because (i) a variety of real-world applications require object detection for distant objects and (ii) SOD is a challenging task due to the noisy, blurred, and less-informative image appearances of small objects. This paper proposes a new SOD dataset consisting of 39,070 images including 137,121 bird instances, which is called the Small Object Detection for Spotting Birds (SOD4SB) dataset. The detail of the challenge with the SOD4SB dataset is introduced in this paper. In total, 223 participants joined this challenge. This paper briefly introduces the award-winning methods. The dataset, the baseline code, and the website for evaluation on the public testset are publicly available.
</details>
<details>
<summary>摘要</summary>
小物体检测（SOD）是机器视觉中重要的话题，因为（i）许多现实世界应用需要对远距离物体进行检测，（ii）SOD是一项复杂的任务，因为小物体的图像表现具有噪音、模糊和不具有准确信息的特点。本文提出了一个新的SOD数据集，包括39,070张图像和137,121个鸟具体实例，称为小物体检测 для鸟兽（SOD4SB）数据集。本文介绍了SOD4SB数据集的挑战。总共有223名参与者参加了这个挑战。本文 briefly介绍了奖励方法。数据集、基线代码和评估网站对公共测试集进行评估是公共可用的。
</details></li>
</ul>
<hr>
<h2 id="Light-Weight-Vision-Transformer-with-Parallel-Local-and-Global-Self-Attention"><a href="#Light-Weight-Vision-Transformer-with-Parallel-Local-and-Global-Self-Attention" class="headerlink" title="Light-Weight Vision Transformer with Parallel Local and Global Self-Attention"></a>Light-Weight Vision Transformer with Parallel Local and Global Self-Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09120">http://arxiv.org/abs/2307.09120</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikolas Ebert, Laurenz Reichardt, Didier Stricker, Oliver Wasenmüller</li>
<li>for: 这个研究的目的是将现代computer vision中的Transformer架构重新设计为适合具有有限资源的硬件上进行自动驾驶任务，并且可以在实时性要求下执行。</li>
<li>methods: 我们在这个研究中提出了一些改进PLG-ViT架构的方法，以减少其中的参数和浮点运算数。我们识别了PLG-ViT架构中的computationally expensive block，并提出了一些改进以减少这些对象的参数和浮点运算数。</li>
<li>results: 我们的研究获得了以下结果：我们可以将PLG-ViT架构缩小到原本的5倍小，并且只有500万个参数，可以在ImageNet-1K分类标准库中获得79.5%的顶部1个精度。我们的网络在COCO实例分类标准库中也实现了良好的表现。此外，我们还实现了一系列的实验，评估了我们的方法在自动驾驶和交通领域中的应用潜力。<details>
<summary>Abstract</summary>
While transformer architectures have dominated computer vision in recent years, these models cannot easily be deployed on hardware with limited resources for autonomous driving tasks that require real-time-performance. Their computational complexity and memory requirements limits their use, especially for applications with high-resolution inputs. In our work, we redesign the powerful state-of-the-art Vision Transformer PLG-ViT to a much more compact and efficient architecture that is suitable for such tasks. We identify computationally expensive blocks in the original PLG-ViT architecture and propose several redesigns aimed at reducing the number of parameters and floating-point operations. As a result of our redesign, we are able to reduce PLG-ViT in size by a factor of 5, with a moderate drop in performance. We propose two variants, optimized for the best trade-off between parameter count to runtime as well as parameter count to accuracy. With only 5 million parameters, we achieve 79.5$\%$ top-1 accuracy on the ImageNet-1K classification benchmark. Our networks demonstrate great performance on general vision benchmarks like COCO instance segmentation. In addition, we conduct a series of experiments, demonstrating the potential of our approach in solving various tasks specifically tailored to the challenges of autonomous driving and transportation.
</details>
<details>
<summary>摘要</summary>
“储在限制型硬件上部署的自动驾驶任务需要实时性和轻量级的模型，但是transformer架构在最近几年内在计算机见识领域中占了主导地位。然而，这些模型的计算复杂度和内存需求限制了它们的使用，特别是在高分辨率输入的应用中。在我们的工作中，我们重新设计了原PLG-ViT架构，将其转换为更加快速和高效的架构，适合这些任务。我们识别PLG-ViT架构中最 computationally expensive的对象，并提出了多种改进，以减少参数和浮点运算数。因此，我们成功地将PLG-ViT的大小减少了5倍，但是损失了一些性能。我们提出了两种版本，适合在parameter count和runtime之间寻找最佳平衡，以及parameter count和准确率之间寻找最佳平衡。仅有500万个参数，我们在ImageNet-1K分类标准库中 achieved 79.5%的top-1准确率。我们的网络在通用的见识测试中也表现出色，并进行了一系列关于自动驾驶和交通的实验，显示了我们的方法在这些任务中的潜力。”
</details></li>
</ul>
<hr>
<h2 id="NU-MCC-Multiview-Compressive-Coding-with-Neighborhood-Decoder-and-Repulsive-UDF"><a href="#NU-MCC-Multiview-Compressive-Coding-with-Neighborhood-Decoder-and-Repulsive-UDF" class="headerlink" title="NU-MCC: Multiview Compressive Coding with Neighborhood Decoder and Repulsive UDF"></a>NU-MCC: Multiview Compressive Coding with Neighborhood Decoder and Repulsive UDF</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09112">http://arxiv.org/abs/2307.09112</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Lionar, Xiangyu Xu, Min Lin, Gim Hee Lee</li>
<li>for: 单视图RGB-D输入的3D重建 tasks（3D reconstruction from single-view RGB-D inputs）</li>
<li>methods: 提出了一种新方法 called NU-MCC，包括两个关键创新：一个邻域解码器和一个排斥未分配距离函数（Repulsive UDF）</li>
<li>results: 实验结果表明，NU-MCC可以学习出一个强大的3D表示，提高了单视图3D重建的状态方法，相比MCC，NU-MCC在CO3D-v2数据集上的F1分数提高9.7%，运行速度高于MCC的5倍。<details>
<summary>Abstract</summary>
Remarkable progress has been made in 3D reconstruction from single-view RGB-D inputs. MCC is the current state-of-the-art method in this field, which achieves unprecedented success by combining vision Transformers with large-scale training. However, we identified two key limitations of MCC: 1) The Transformer decoder is inefficient in handling large number of query points; 2) The 3D representation struggles to recover high-fidelity details. In this paper, we propose a new approach called NU-MCC that addresses these limitations. NU-MCC includes two key innovations: a Neighborhood decoder and a Repulsive Unsigned Distance Function (Repulsive UDF). First, our Neighborhood decoder introduces center points as an efficient proxy of input visual features, allowing each query point to only attend to a small neighborhood. This design not only results in much faster inference speed but also enables the exploitation of finer-scale visual features for improved recovery of 3D textures. Second, our Repulsive UDF is a novel alternative to the occupancy field used in MCC, significantly improving the quality of 3D object reconstruction. Compared to standard UDFs that suffer from holes in results, our proposed Repulsive UDF can achieve more complete surface reconstruction. Experimental results demonstrate that NU-MCC is able to learn a strong 3D representation, significantly advancing the state of the art in single-view 3D reconstruction. Particularly, it outperforms MCC by 9.7% in terms of the F1-score on the CO3D-v2 dataset with more than 5x faster running speed.
</details>
<details>
<summary>摘要</summary>
“单一视野RGB-D输入的3D重建技术已经做出了杰出的进步。MCC是这个领域的现行州Of-The-Art方法，通过结合Computer Vision Transformer和大规模训练而取得了前无seen的成功。然而，我们发现了MCC的两个关键限制：1）Transformer decoder在处理大量查询点时不具有高效性；2）3D表现仅能够重建高精度的细节。在这篇论文中，我们提出了一个新的方法called NU-MCC，它包括两个关键创新：一个邻居decoder和一个Repulsive Unsigned Distance Function（Repulsive UDF）。首先，我们的邻居decoder将输入的视觉特征转换为中心点，让每个查询点只需要关注当地小区域。这个设计不仅提高了单位执行时间的速度，而且允许更好地利用更细节的视觉特征，以提高3D文字的重建。其次，我们的Repulsive UDF是一种新型的对应场，它在MCC中使用的标准UDF遭受到洞的问题，而我们的提案可以实现更完整的表面重建。实验结果显示，NU-MCC可以对单一视野3D重建进行更好的学习，并在CO3D-v2dataset上比MCC提高9.7%的F1分数，且比MCC更快速执行。”
</details></li>
</ul>
<hr>
<h2 id="Mining-of-Single-Class-by-Active-Learning-for-Semantic-Segmentation"><a href="#Mining-of-Single-Class-by-Active-Learning-for-Semantic-Segmentation" class="headerlink" title="Mining of Single-Class by Active Learning for Semantic Segmentation"></a>Mining of Single-Class by Active Learning for Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09109">http://arxiv.org/abs/2307.09109</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hugues Lambert, Emma Slade</li>
<li>for: 这 paper 是为了提出一种新的活动学习（AL）策略，帮助寻找最有用的样本，并增加模型的性能。</li>
<li>methods: 这 paper 使用了深度强化学习来构建一个 AL 策略，并利用量精度相关性来建立高性能模型。</li>
<li>results: 这 paper 的结果表明，MiSiCAL 能够在 COCO10k 中的 150 个类中超越随机策略，而最强的基eline 只能在 101 个类中超越随机策略。<details>
<summary>Abstract</summary>
Several Active Learning (AL) policies require retraining a target model several times in order to identify the most informative samples and rarely offer the option to focus on the acquisition of samples from underrepresented classes. Here the Mining of Single-Class by Active Learning (MiSiCAL) paradigm is introduced where an AL policy is constructed through deep reinforcement learning and exploits quantity-accuracy correlations to build datasets on which high-performance models can be trained with regards to specific classes. MiSiCAL is especially helpful in the case of very large batch sizes since it does not require repeated model training sessions as is common in other AL methods. This is thanks to its ability to exploit fixed representations of the candidate data points. We find that MiSiCAL is able to outperform a random policy on 150 out of 171 COCO10k classes, while the strongest baseline only outperforms random on 101 classes.
</details>
<details>
<summary>摘要</summary>
几种活动学习（AL）策略需要重新训练目标模型数次以确定最有用的样本，并很少提供针对异常类别的样本收集的选项。在这里，我们引入了单类挖掘学习（MiSiCAL）模式，其中一个AL策略通过深度强化学习构建，利用量精度相关性来建立高性能模型可以在特定类别上训练。MiSiCAL特别有利于大批量时，因为它不需要重复的模型训练会议。这是因为它可以利用固定表示的候选数据点。我们发现MiSiCAL能够在COCO10k类型中击败随机策略的150个类型，而最强基eline只能在101个类型上击败随机策略。
</details></li>
</ul>
<hr>
<h2 id="Division-Gets-Better-Learning-Brightness-Aware-and-Detail-Sensitive-Representations-for-Low-Light-Image-Enhancement"><a href="#Division-Gets-Better-Learning-Brightness-Aware-and-Detail-Sensitive-Representations-for-Low-Light-Image-Enhancement" class="headerlink" title="Division Gets Better: Learning Brightness-Aware and Detail-Sensitive Representations for Low-Light Image Enhancement"></a>Division Gets Better: Learning Brightness-Aware and Detail-Sensitive Representations for Low-Light Image Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09104">http://arxiv.org/abs/2307.09104</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huake Wang, Xiaoyang Yan, Xingsong Hou, Junhui Li, Yujie Dun, Kaibing Zhang</li>
<li>for: 提高低光照图像的对比度、颜色和纹理的Restoration</li>
<li>methods: 提出了一种基于两个分支网络的新方法LCDBNet，其中一个分支网络负责调整亮度，另一个分支网络负责修复颜色和纹理。</li>
<li>results: 对七个标准测试集进行了广泛的实验，结果显示LCDBNet的表现比其他当前领先方法更佳，并且在多种参考&#x2F;非参考质量评价指标中获得了更高的分数。<details>
<summary>Abstract</summary>
Low-light image enhancement strives to improve the contrast, adjust the visibility, and restore the distortion in color and texture. Existing methods usually pay more attention to improving the visibility and contrast via increasing the lightness of low-light images, while disregarding the significance of color and texture restoration for high-quality images. Against above issue, we propose a novel luminance and chrominance dual branch network, termed LCDBNet, for low-light image enhancement, which divides low-light image enhancement into two sub-tasks, e.g., luminance adjustment and chrominance restoration. Specifically, LCDBNet is composed of two branches, namely luminance adjustment network (LAN) and chrominance restoration network (CRN). LAN takes responsibility for learning brightness-aware features leveraging long-range dependency and local attention correlation. While CRN concentrates on learning detail-sensitive features via multi-level wavelet decomposition. Finally, a fusion network is designed to blend their learned features to produce visually impressive images. Extensive experiments conducted on seven benchmark datasets validate the effectiveness of our proposed LCDBNet, and the results manifest that LCDBNet achieves superior performance in terms of multiple reference/non-reference quality evaluators compared to other state-of-the-art competitors. Our code and pretrained model will be available.
</details>
<details>
<summary>摘要</summary>
低光照图像增强尝试提高图像的对比度、调整视觉效果和恢复颜色和纹理的损害。现有方法通常更关注提高低光照图像的可见度和对比度，而忽略了高质量图像的颜色和纹理恢复的重要性。为了解决这个问题，我们提议一种新的抽象和彩度分支网络（LCDBNet），用于低光照图像增强。LCDBNet将低光照图像增强分为两个子任务：亮度调整和颜色恢复。具体来说，LCDBNet由两个分支组成：亮度调整网络（LAN）和颜色恢复网络（CRN）。LAN负责学习亮度意识的特征，利用长距离关系和本地关注相互关系。而CRN则专注于学习细节敏感的特征，通过多层wavelet分解。最后，我们设计了一个混合网络，将它们学习的特征融合，以生成视觉卓越的图像。我们在七个标准测试数据集上进行了广泛的实验，结果表明LCDBNet在多个参考/非参考质量评价指标上表现出色，超过了当前状态的竞争对手。我们的代码和预训练模型将可以获得。
</details></li>
</ul>
<hr>
<h2 id="Reclaiming-the-Horizon-Novel-Visualization-Designs-for-Time-Series-Data-with-Large-Value-Ranges"><a href="#Reclaiming-the-Horizon-Novel-Visualization-Designs-for-Time-Series-Data-with-Large-Value-Ranges" class="headerlink" title="Reclaiming the Horizon: Novel Visualization Designs for Time-Series Data with Large Value Ranges"></a>Reclaiming the Horizon: Novel Visualization Designs for Time-Series Data with Large Value Ranges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10278">http://arxiv.org/abs/2307.10278</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Braun, Rita Borgo, Max Sondag, Tatiana von Landesberger</li>
<li>for: 支持实践者在时间序列数据中进行标识和分类任务，即在很大的值范围内进行标识和分类。</li>
<li>methods: 提出了两种新的视觉设计：第一种是维度范围图，它是 классиic 的背景图的扩展；第二种是带有很大值范围的 log-line 图。这两种新的视觉设计可以让实践者更好地对很大的值范围进行视觉化。</li>
<li>results: 在四种常见的时间序列分析和大值范围视觉化任务中，新的维度范围图表现更好或等于所有其他设计，包括标识、分类、估计和趋势检测等任务。只有趋势检测任务中，传统的背景图表现更好。结果具有领域独立性，只需要时间序列数据具有很大的值范围即可。<details>
<summary>Abstract</summary>
We introduce two novel visualization designs to support practitioners in performing identification and discrimination tasks on large value ranges (i.e., several orders of magnitude) in time-series data: (1) The order of magnitude horizon graph, which extends the classic horizon graph; and (2) the order of magnitude line chart, which adapts the log-line chart. These new visualization designs visualize large value ranges by explicitly splitting the mantissa m and exponent e of a value v = m * 10e . We evaluate our novel designs against the most relevant state-of-the-art visualizations in an empirical user study. It focuses on four main tasks commonly employed in the analysis of time-series and large value ranges visualization: identification, discrimination, estimation, and trend detection. For each task we analyse error, confidence, and response time. The new order of magnitude horizon graph performs better or equal to all other designs in identification, discrimination, and estimation tasks. Only for trend detection tasks, the more traditional horizon graphs reported better performance. Our results are domain-independent, only requiring time-series data with large value ranges.
</details>
<details>
<summary>摘要</summary>
我们介绍了两种新的视觉化设计，用于支持实践者在时间序列数据中进行标识和分类任务，即在很大的值范围内（几个数量级）：（1）扩展类 horizon graph，和（2）适应 log-line 图表。这两种新的视觉化设计将大值范围视觉化为显式地将值分解成整数部分m和指数部分e，使得v = m * 10e 的值范围变得更加明了。我们对最相关的现有视觉化进行了实验用户研究，这种研究包括四个主要任务，即标识、分类、估算和趋势检测。对每个任务，我们分析了错误率、自信度和响应时间。新的 ORDER OF MAGNITUDE  horizon graph 在标识、分类和估算任务中表现更好或相等于所有其他设计。只有趋势检测任务中，传统的 horizon graphs 表现更好。我们的结果是领域独立的，只需要时间序列数据具有大值范围即可。
</details></li>
</ul>
<hr>
<h2 id="PixelHuman-Animatable-Neural-Radiance-Fields-from-Few-Images"><a href="#PixelHuman-Animatable-Neural-Radiance-Fields-from-Few-Images" class="headerlink" title="PixelHuman: Animatable Neural Radiance Fields from Few Images"></a>PixelHuman: Animatable Neural Radiance Fields from Few Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09070">http://arxiv.org/abs/2307.09070</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gyumin Shim, Jaeseong Lee, Junha Hyung, Jaegul Choo</li>
<li>for: 这 paper 是为了实现从几个人像中生成可动人景的 novel 模型。</li>
<li>methods: 该方法使用 neural radiance field 和 pose-aware pixel-aligned features，通过数据驱动的方式学习了折叠场景，以实现从几个不同的视图和姿势中生成可动人景。</li>
<li>results: 实验结果显示，该方法可以在多视图和新姿势synthesis中实现state-of-the-art表现，只需要几个图像来训练。<details>
<summary>Abstract</summary>
In this paper, we propose PixelHuman, a novel human rendering model that generates animatable human scenes from a few images of a person with unseen identity, views, and poses. Previous work have demonstrated reasonable performance in novel view and pose synthesis, but they rely on a large number of images to train and are trained per scene from videos, which requires significant amount of time to produce animatable scenes from unseen human images. Our method differs from existing methods in that it can generalize to any input image for animatable human synthesis. Given a random pose sequence, our method synthesizes each target scene using a neural radiance field that is conditioned on a canonical representation and pose-aware pixel-aligned features, both of which can be obtained through deformation fields learned in a data-driven manner. Our experiments show that our method achieves state-of-the-art performance in multiview and novel pose synthesis from few-shot images.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了PixelHuman，一种新的人体渲染模型，该模型可以从几张人物图像中生成可动人体场景，无需seen人物视角和姿势。先前的工作已经证明了在新视角和姿势合成方面的可靠性，但它们需要大量图像进行训练，并且需要相当长的时间来生成从未seen的人物图像中的可动场景。我们的方法与现有方法不同，它可以对任意输入图像进行可动人体合成。给定一个随机姿势序列，我们的方法使用神经辐射场来synthesize每个目标场景，该场景被 conditioned 于一个拟合表示和pose-aware像素对齐特征，这些特征可以通过在数据驱动的方式下学习的扭形场所获得。我们的实验表明，我们的方法在多视图和新姿势合成方面具有状态级表现。
</details></li>
</ul>
<hr>
<h2 id="Evaluate-Fine-tuning-Strategies-for-Fetal-Head-Ultrasound-Image-Segmentation-with-U-Net"><a href="#Evaluate-Fine-tuning-Strategies-for-Fetal-Head-Ultrasound-Image-Segmentation-with-U-Net" class="headerlink" title="Evaluate Fine-tuning Strategies for Fetal Head Ultrasound Image Segmentation with U-Net"></a>Evaluate Fine-tuning Strategies for Fetal Head Ultrasound Image Segmentation with U-Net</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09067">http://arxiv.org/abs/2307.09067</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/13204942/ft_methods_for_fetal_head_segmentation">https://github.com/13204942/ft_methods_for_fetal_head_segmentation</a></li>
<li>paper_authors: Fangyijie Wang, Guénolé Silvestre, Kathleen M. Curran<br>for:这份研究目的是提高胎头环circumference（HC）的测量效率，以便更好地监控胎生长。methods:我们提出了一种将MobileNet作为encoder，并通过精确调整（FT）U-Net网络来进行胎头 segmentation的方法。这种方法可以实现限制 Parameters的培训，并且比对于从头开始训练的网络模型来得到更好的性能。results:我们发现，这种FT方法可以与从头开始训练的网络模型相比，具有与其相似的性能（85.8%），并且具有较小的trainable parameter size（Below 4.4 million）。这显示了我们的FT方法可以实现胎头 segmentation的目的，并且可以应对实际应用中的限制。<details>
<summary>Abstract</summary>
Fetal head segmentation is a crucial step in measuring the fetal head circumference (HC) during gestation, an important biometric in obstetrics for monitoring fetal growth. However, manual biometry generation is time-consuming and results in inconsistent accuracy. To address this issue, convolutional neural network (CNN) models have been utilized to improve the efficiency of medical biometry. But training a CNN network from scratch is a challenging task, we proposed a Transfer Learning (TL) method. Our approach involves fine-tuning (FT) a U-Net network with a lightweight MobileNet as the encoder to perform segmentation on a set of fetal head ultrasound (US) images with limited effort. This method addresses the challenges associated with training a CNN network from scratch. It suggests that our proposed FT strategy yields segmentation performance that is comparable when trained with a reduced number of parameters by 85.8%. And our proposed FT strategy outperforms other strategies with smaller trainable parameter sizes below 4.4 million. Thus, we contend that it can serve as a dependable FT approach for reducing the size of models in medical image analysis. Our key findings highlight the importance of the balance between model performance and size in developing Artificial Intelligence (AI) applications by TL methods. Code is available at https://github.com/13204942/FT_Methods_for_Fetal_Head_Segmentation.
</details>
<details>
<summary>摘要</summary>
产前胎头分割是评估胎头圆周长（HC）的关键步骤，是妊娠期内重要的生物指标，可以对胎子增长进行监测。然而，手动生物学测量是时间consuming且准确性不稳定。为了解决这个问题，批处神经网络（CNN）模型已经被应用于医学生物学测量中，以提高生物学测量的效率。然而，训练CNN网络从零开始是一项困难的任务。我们提出了传输学习（TL）方法。我们的方法包括 fine-tuning（FT）一个U-Net网络，使其在一组有限的胎头ultrasound（US）图像上进行分割，而不需要大量的努力。这种方法可以解决训练CNN网络从零开始的挑战。我们发现，我们的提议的FT策略可以在限制参数数量的情况下，达到相当于85.8%的分割性能。此外，我们的FT策略还可以比其他策略下面4.4万个可 Trainable参数更好地表现。因此，我们认为这种FT方法可以在医学生物学测量中用于减小模型的大小。我们的关键发现表明了在使用TL方法开发人工智能应用程序时，模型性能和大小之间的平衡是非常重要的。代码可以在https://github.com/13204942/FT_Methods_for_Fetal_Head_Segmentation中找到。
</details></li>
</ul>
<hr>
<h2 id="PatchCT-Aligning-Patch-Set-and-Label-Set-with-Conditional-Transport-for-Multi-Label-Image-Classification"><a href="#PatchCT-Aligning-Patch-Set-and-Label-Set-with-Conditional-Transport-for-Multi-Label-Image-Classification" class="headerlink" title="PatchCT: Aligning Patch Set and Label Set with Conditional Transport for Multi-Label Image Classification"></a>PatchCT: Aligning Patch Set and Label Set with Conditional Transport for Multi-Label Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09066">http://arxiv.org/abs/2307.09066</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/keepgoingjkg/patchct">https://github.com/keepgoingjkg/patchct</a></li>
<li>paper_authors: Miaoge Li, Dongsheng Wang, Xinyang Liu, Zequn Zeng, Ruiying Lu, Bo Chen, Mingyuan Zhou</li>
<li>for: 这个论文目的是提出一种基于 Conditional Transport (CT) 理论的多标签图像分类方法，以实现更好地利用图像和标签Semantic Space的互动。</li>
<li>methods: 该方法使用 CT 理议来bridging图像和标签域的语义空间，并通过定义前向和后向导航器来学习和对Alignment those two semantic sets。</li>
<li>results: 根据实验结果，提出的方法在三个公共图像benchmark上 consistently outperform了之前的方法。<details>
<summary>Abstract</summary>
Multi-label image classification is a prediction task that aims to identify more than one label from a given image. This paper considers the semantic consistency of the latent space between the visual patch and linguistic label domains and introduces the conditional transport (CT) theory to bridge the acknowledged gap. While recent cross-modal attention-based studies have attempted to align such two representations and achieved impressive performance, they required carefully-designed alignment modules and extra complex operations in the attention computation. We find that by formulating the multi-label classification as a CT problem, we can exploit the interactions between the image and label efficiently by minimizing the bidirectional CT cost. Specifically, after feeding the images and textual labels into the modality-specific encoders, we view each image as a mixture of patch embeddings and a mixture of label embeddings, which capture the local region features and the class prototypes, respectively. CT is then employed to learn and align those two semantic sets by defining the forward and backward navigators. Importantly, the defined navigators in CT distance model the similarities between patches and labels, which provides an interpretable tool to visualize the learned prototypes. Extensive experiments on three public image benchmarks show that the proposed model consistently outperforms the previous methods.
</details>
<details>
<summary>摘要</summary>
多标签图像分类是一个预测任务，旨在从给定图像中标识多个标签。本文考虑了图像和文本标签域之间的semantic consistency，并引入了conditional transport（CT）理论来bridging这些两个域之间的知识渠道。相比之下， latest cross-modal attention-based studies尝试了将这两个表示空间对齐，并实现了出色的表现，但是它们需要设计了复杂的对齐模块和附加的复杂操作。我们发现，将多标签分类定义为CT问题，可以充分利用图像和标签之间的交互，并通过最小化bidirectional CT成本来捕捉到这些交互。具体来说，我们将图像和文本标签 feed到模式特异Encoder中，然后视图每个图像为一个mixture of patch embeddings和一个mixture of label embeddings，这两个 embedding capture了图像的局部区域特征和类型豁达，分别。然后，我们使用CT来学习和对这两个semantic sets进行对齐，定义了前向和后向探险者。特别是，定义的探险者在CT距离模型中模型了图像和标签之间的相似性，这提供了可读的工具来可视化学习的prototype。我们在三个公共图像 benchmark上进行了广泛的实验，结果显示，我们的方法在前一个方法之上具有稳定的性能优势。
</details></li>
</ul>
<hr>
<h2 id="Learning-Adaptive-Neighborhoods-for-Graph-Neural-Networks"><a href="#Learning-Adaptive-Neighborhoods-for-Graph-Neural-Networks" class="headerlink" title="Learning Adaptive Neighborhoods for Graph Neural Networks"></a>Learning Adaptive Neighborhoods for Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09065">http://arxiv.org/abs/2307.09065</a></li>
<li>repo_url: None</li>
<li>paper_authors: Avishkar Saha, Oscar Mendez, Chris Russell, Richard Bowden</li>
<li>for: 本 paper 是为了提出一种可 diferenciable 图结构生成器，帮助GCNs在图结构数据上进行端到端学习。</li>
<li>methods: 本 paper 使用了一种novel end-to-end differentiable graph generator，该模块可以将图结构学习到GCNs中，并且可以将每个节点的邻居和大小选择为其自己。</li>
<li>results: 本 paper 的实验结果表明，该模块可以在多种dataset和GCN背景下提高结果的准确率，并且可以与其他结构学习方法相比。<details>
<summary>Abstract</summary>
Graph convolutional networks (GCNs) enable end-to-end learning on graph structured data. However, many works assume a given graph structure. When the input graph is noisy or unavailable, one approach is to construct or learn a latent graph structure. These methods typically fix the choice of node degree for the entire graph, which is suboptimal. Instead, we propose a novel end-to-end differentiable graph generator which builds graph topologies where each node selects both its neighborhood and its size. Our module can be readily integrated into existing pipelines involving graph convolution operations, replacing the predetermined or existing adjacency matrix with one that is learned, and optimized, as part of the general objective. As such it is applicable to any GCN. We integrate our module into trajectory prediction, point cloud classification and node classification pipelines resulting in improved accuracy over other structure-learning methods across a wide range of datasets and GCN backbones.
</details>
<details>
<summary>摘要</summary>
图像卷积网络（GCNs）允许端到端学习在图结构数据上。然而，许多工作假设给定的图结构。当输入图是噪音或无法获得时，一种方法是构建或学习隐藏图结构。这些方法通常固定整个图的节点度，这是不优化的。相反，我们提出了一种新的终端可 differentiable图生成器，它可以在建立图结构时让每个节点选择自己的邻居和大小。我们的模块可以轻松地整合到现有的GCN执行推导管线中，将预先确定或现有的相对位矩阵 replaced 为一个学习和优化的一部分，因此可以应用于任何GCN。我们将我们的模块集成到轨迹预测、点云分类和节点分类管线中，在各种数据集和GCN背景下得到了相对于其他结构学习方法的提高准确性。
</details></li>
</ul>
<hr>
<h2 id="Deep-learning-for-unsupervised-domain-adaptation-in-medical-imaging-Recent-advancements-and-future-perspectives"><a href="#Deep-learning-for-unsupervised-domain-adaptation-in-medical-imaging-Recent-advancements-and-future-perspectives" class="headerlink" title="Deep learning for unsupervised domain adaptation in medical imaging: Recent advancements and future perspectives"></a>Deep learning for unsupervised domain adaptation in medical imaging: Recent advancements and future perspectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01265">http://arxiv.org/abs/2308.01265</a></li>
<li>repo_url: None</li>
<li>paper_authors: Suruchi Kumari, Pravendra Singh</li>
<li>for: 本研究写作的目的是对医疗影像领域内的深度学习方法进行评论和概述，尤其是在过去几年内的不监督领域适应（UDA）技术发展。</li>
<li>methods: 本研究主要探讨了医疗影像领域内的不监督领域适应技术，包括特征对焦、影像转换、自我监督、分离表示方法等。</li>
<li>results: 本研究给出了医疗影像领域内不监督领域适应技术的综观和评论，包括六种不同类型的方法，以及各自的数据集使用情况。<details>
<summary>Abstract</summary>
Deep learning has demonstrated remarkable performance across various tasks in medical imaging. However, these approaches primarily focus on supervised learning, assuming that the training and testing data are drawn from the same distribution. Unfortunately, this assumption may not always hold true in practice. To address these issues, unsupervised domain adaptation (UDA) techniques have been developed to transfer knowledge from a labeled domain to a related but unlabeled domain. In recent years, significant advancements have been made in UDA, resulting in a wide range of methodologies, including feature alignment, image translation, self-supervision, and disentangled representation methods, among others. In this paper, we provide a comprehensive literature review of recent deep UDA approaches in medical imaging from a technical perspective. Specifically, we categorize current UDA research in medical imaging into six groups and further divide them into finer subcategories based on the different tasks they perform. We also discuss the respective datasets used in the studies to assess the divergence between the different domains. Finally, we discuss emerging areas and provide insights and discussions on future research directions to conclude this survey.
</details>
<details>
<summary>摘要</summary>
深度学习在医疗影像领域已经表现出非常出色。然而，这些方法主要是基于指导学习，假设训练和测试数据都来自同一个分布。可是，这个假设在实践中可能并不成立。为解决这些问题，无监督领域适应（UDA）技术得到了广泛应用。在最近几年，UDA领域在医疗影像领域的研究得到了 significative进步，包括特征对齐、图像翻译、自我指导、分解表示方法等。在这篇论文中，我们提供了医疗影像领域最新的深度UDA策略的全面文献综述。特别是，我们将当前UDA研究分为六个组，并将它们进一步分为不同任务的子类别。我们还讨论了不同研究使用的数据集，以评估不同领域之间的差异。最后，我们介绍了未来研究的前景和见解，并结束这篇报告。
</details></li>
</ul>
<hr>
<h2 id="Outlier-Robust-Tensor-Low-Rank-Representation-for-Data-Clustering"><a href="#Outlier-Robust-Tensor-Low-Rank-Representation-for-Data-Clustering" class="headerlink" title="Outlier-Robust Tensor Low-Rank Representation for Data Clustering"></a>Outlier-Robust Tensor Low-Rank Representation for Data Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09055">http://arxiv.org/abs/2307.09055</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tong Wu</li>
<li>for: 本文针对损受噪音或标本特有混淆的维度资料进行了恢复和聚类分析。</li>
<li>methods: 本文提出了一种基于维度对称分解（t-SVD）的噪音抗性维度低维表示（OR-TLRR）方法，用于同时检测噪音和维度资料的聚类分析。</li>
<li>results: 本文的实验结果显示，OR-TLRR方法可以对损受噪音或标本特有混淆的维度资料进行有效的恢复和聚类分析，并且可以处理部分资料欠拓实验结果。<details>
<summary>Abstract</summary>
Low-rank tensor analysis has received widespread attention with many practical applications. However, the tensor data are often contaminated by outliers or sample-specific corruptions. How to recover the tensor data that are corrupted by outliers and perform data clustering remains a challenging problem. This paper develops an outlier-robust tensor low-rank representation (OR-TLRR) method for simultaneous outlier detection and tensor data clustering based on the tensor singular value decomposition (t-SVD) algebraic framework. It is motivated by the recently proposed tensor-tensor product induced by invertible linear transforms that satisfy certain conditions. For tensor observations with arbitrary outlier corruptions, OR-TLRR has provable performance guarantee for exactly recovering the row space of clean data and detecting outliers under mild conditions. Moreover, an extension of OR-TLRR is also proposed to handle the case when parts of the data are missing. Finally, extensive experimental results on both synthetic and real data demonstrate the effectiveness of the proposed algorithms.
</details>
<details>
<summary>摘要</summary>
低级tensor分析已经受到广泛关注，有很多实际应用。然而，tensor数据经常受到异常值或样本特有的腐朽影响。如何recover受损tensor数据并进行数据归类是一个具有挑战性的问题。这篇论文开发了一种具有异常鲁棒性的tensor低级表示法（OR-TLRR），用于同时检测异常值和tensor数据归类，基于tensor特征值分解（t-SVD）的代数框架。它是基于最近提出的tensor-tensor产品导出的减法，其满足某些条件。对于受到任意异常损害的tensor观测值，OR-TLRR有证明性的性能保证可以准确地恢复clean数据的行空间，并在某些条件下检测异常值。此外，对于缺失数据的情况，我们还提出了OR-TLRR的扩展。最后，我们在 synthetic和实际数据上进行了广泛的实验，并证明了提案的算法的效果。
</details></li>
</ul>
<hr>
<h2 id="Connections-between-Operator-splitting-Methods-and-Deep-Neural-Networks-with-Applications-in-Image-Segmentation"><a href="#Connections-between-Operator-splitting-Methods-and-Deep-Neural-Networks-with-Applications-in-Image-Segmentation" class="headerlink" title="Connections between Operator-splitting Methods and Deep Neural Networks with Applications in Image Segmentation"></a>Connections between Operator-splitting Methods and Deep Neural Networks with Applications in Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09052">http://arxiv.org/abs/2307.09052</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Liu, Xue-Cheng Tai, Raymond Chan</li>
<li>for: 这篇论文的目的是为了提供深度神经网络的数学解释，以及将深度神经网络与数学算法联系起来的方法。</li>
<li>methods: 这篇论文使用了分解策略和多普雷德法则来解释深度神经网络。它还提出了两种基于分解策略的网络方案，用于解决图像分割问题。</li>
<li>results: 实验结果表明，这两种网络方案具有良好的性能，可以有效地解决图像分割问题。<details>
<summary>Abstract</summary>
Deep neural network is a powerful tool for many tasks. Understanding why it is so successful and providing a mathematical explanation is an important problem and has been one popular research direction in past years. In the literature of mathematical analysis of deep deep neural networks, a lot of works are dedicated to establishing representation theories. How to make connections between deep neural networks and mathematical algorithms is still under development. In this paper, we give an algorithmic explanation for deep neural networks, especially in their connection with operator splitting and multigrid methods. We show that with certain splitting strategies, operator-splitting methods have the same structure as networks. Utilizing this connection and the Potts model for image segmentation, two networks inspired by operator-splitting methods are proposed. The two networks are essentially two operator-splitting algorithms solving the Potts model. Numerical experiments are presented to demonstrate the effectiveness of the proposed networks.
</details>
<details>
<summary>摘要</summary>
深度神经网络是许多任务的 poderous工具。理解它的成功原因以及提供数学解释是一个重要的研究方向，在过去几年中受到了广泛的关注。在深度神经网络的数学分析文献中，许多研究都是建立表示理论的。但是，如何将深度神经网络与数学算法连接起来仍然是一个开发中的问题。在这篇论文中，我们提供了深度神经网络的算法解释，特别是与分解算法和多格rid方法之间的连接。我们表明，使用某些拆分策略，Operator-splitting方法和神经网络之间存在同构关系。利用这种连接和Potts模型，我们提出了两种基于Operator-splitting方法的神经网络，它们实际上是两种解决Potts模型的算法。我们对这两种网络进行了数学实验，以证明它们的有效性。
</details></li>
</ul>
<hr>
<h2 id="PottsMGNet-A-Mathematical-Explanation-of-Encoder-Decoder-Based-Neural-Networks"><a href="#PottsMGNet-A-Mathematical-Explanation-of-Encoder-Decoder-Based-Neural-Networks" class="headerlink" title="PottsMGNet: A Mathematical Explanation of Encoder-Decoder Based Neural Networks"></a>PottsMGNet: A Mathematical Explanation of Encoder-Decoder Based Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09039">http://arxiv.org/abs/2307.09039</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xue-Cheng Tai, Hao Liu, Raymond Chan</li>
<li>for: 这篇论文主要是为了解释基于编码器-解码器架构的效果神经网络，以及其在图像分割领域的应用。</li>
<li>methods: 该论文使用了两阶段尼特斯模型来解释编码器-解码器架构，并使用了多普逊法和运算符拆分方法来离散化连续控制模型。</li>
<li>results: 研究发现，将Soft-Threshold-Dynamics作为正则化项 incorporated into the PottsMGNet，可以使其在各种网络参数（如网络宽度和深度）下表现出色，并在各种大量噪声的数据集上达到了Remarkable performance。在大多数实验中，新网络 sempre perfoms better or as good as existing networks for image segmentation on accuracy and dice score。<details>
<summary>Abstract</summary>
For problems in image processing and many other fields, a large class of effective neural networks has encoder-decoder-based architectures. Although these networks have made impressive performances, mathematical explanations of their architectures are still underdeveloped. In this paper, we study the encoder-decoder-based network architecture from the algorithmic perspective and provide a mathematical explanation. We use the two-phase Potts model for image segmentation as an example for our explanations. We associate the segmentation problem with a control problem in the continuous setting. Then, multigrid method and operator splitting scheme, the PottsMGNet, are used to discretize the continuous control model. We show that the resulting discrete PottsMGNet is equivalent to an encoder-decoder-based network. With minor modifications, it is shown that a number of the popular encoder-decoder-based neural networks are just instances of the proposed PottsMGNet. By incorporating the Soft-Threshold-Dynamics into the PottsMGNet as a regularizer, the PottsMGNet has shown to be robust with the network parameters such as network width and depth and achieved remarkable performance on datasets with very large noise. In nearly all our experiments, the new network always performs better or as good on accuracy and dice score than existing networks for image segmentation.
</details>
<details>
<summary>摘要</summary>
对于图像处理和其他领域的问题，一大类效果强大的神经网络有编码器-解码器基本架构。 although these networks have made impressive performances, mathematical explanations of their architectures are still underdeveloped. In this paper, we study the encoder-decoder-based network architecture from the algorithmic perspective and provide a mathematical explanation. We use the two-phase Potts model for image segmentation as an example for our explanations. We associate the segmentation problem with a control problem in the continuous setting. Then, multigrid method and operator splitting scheme, the PottsMGNet, are used to discretize the continuous control model. We show that the resulting discrete PottsMGNet is equivalent to an encoder-decoder-based network. With minor modifications, it is shown that a number of the popular encoder-decoder-based neural networks are just instances of the proposed PottsMGNet. By incorporating the Soft-Threshold-Dynamics into the PottsMGNet as a regularizer, the PottsMGNet has shown to be robust with the network parameters such as network width and depth and achieved remarkable performance on datasets with very large noise. In nearly all our experiments, the new network always performs better or as good on accuracy and dice score than existing networks for image segmentation.Note: The translation is in Simplified Chinese, which is one of the two standardized Chinese languages. The other is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Online-Self-Supervised-Thermal-Water-Segmentation-for-Aerial-Vehicles"><a href="#Online-Self-Supervised-Thermal-Water-Segmentation-for-Aerial-Vehicles" class="headerlink" title="Online Self-Supervised Thermal Water Segmentation for Aerial Vehicles"></a>Online Self-Supervised Thermal Water Segmentation for Aerial Vehicles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09027">http://arxiv.org/abs/2307.09027</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/connorlee77/uav-thermal-water-segmentation">https://github.com/connorlee77/uav-thermal-water-segmentation</a></li>
<li>paper_authors: Connor Lee, Jonathan Gustafsson Frennert, Lu Gan, Matthew Anderson, Soon-Jo Chung</li>
<li>for: 这个论文的目的是提出一种新的方法，使RGB训练的水分割网络可以适应目标域空气热图像，并通过在线自我指导来使用文本和运动参数作为监督信号。</li>
<li>methods: 该方法使用了在线自我指导，将RGB训练的水分割网络应用到目标域空气热图像上，并使用文本和运动参数作为监督信号。</li>
<li>results: 该方法可以在夜间、无法训练数据的情况下，使current autonomous aerial robots在近岸环境中进行视觉导航、测量和流追踪等任务。另外，该方法还可以在实时上下文中运行，并且在Nvidia Jetson嵌入式计算平台上实现了实时应用。<details>
<summary>Abstract</summary>
We present a new method to adapt an RGB-trained water segmentation network to target-domain aerial thermal imagery using online self-supervision by leveraging texture and motion cues as supervisory signals. This new thermal capability enables current autonomous aerial robots operating in near-shore environments to perform tasks such as visual navigation, bathymetry, and flow tracking at night. Our method overcomes the problem of scarce and difficult-to-obtain near-shore thermal data that prevents the application of conventional supervised and unsupervised methods. In this work, we curate the first aerial thermal near-shore dataset, show that our approach outperforms fully-supervised segmentation models trained on limited target-domain thermal data, and demonstrate real-time capabilities onboard an Nvidia Jetson embedded computing platform. Code and datasets used in this work will be available at: https://github.com/connorlee77/uav-thermal-water-segmentation.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法，用于将RGB搜索到的水分割网络适应目标域空气热图像上的自动超vision。这种新的热能力使得现有的无人飞行机器人在夜晚近岸环境中执行视觉导航、水深测量和流追踪等任务。我们的方法解决了 conventional 监督和无监督方法中 scarce 和 difficult-to-obtain 的近岸热数据问题。在这项工作中，我们创建了首个空气热近岸数据集，证明我们的方法超过了限定目标域热数据的彻底监督模型，并在 Nvidia Jetson 嵌入式计算平台上实现了实时功能。代码和数据集将在：https://github.com/connorlee77/uav-thermal-water-segmentation 上提供。
</details></li>
</ul>
<hr>
<h2 id="ActionPrompt-Action-Guided-3D-Human-Pose-Estimation-With-Text-and-Pose-Prompting"><a href="#ActionPrompt-Action-Guided-3D-Human-Pose-Estimation-With-Text-and-Pose-Prompting" class="headerlink" title="ActionPrompt: Action-Guided 3D Human Pose Estimation With Text and Pose Prompting"></a>ActionPrompt: Action-Guided 3D Human Pose Estimation With Text and Pose Prompting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09026">http://arxiv.org/abs/2307.09026</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongwei Zheng, Han Li, Bowen Shi, Wenrui Dai, Botao Wan, Yu Sun, Min Guo, Hongkai Xiong</li>
<li>for: 提高视频基于2D-to-3D人姿估计（HPE）的性能，解决深度歧义问题。</li>
<li>methods: 提出了一个名为动作提示模块（APM）的插件模块，可以有效地挖掘不同类型的动作准则，以提高3D HPE的性能。</li>
<li>results: 实验表明，APM可以大幅提高大多数视频基于2D-to-3D HPE框架的性能。<details>
<summary>Abstract</summary>
Recent 2D-to-3D human pose estimation (HPE) utilizes temporal consistency across sequences to alleviate the depth ambiguity problem but ignore the action related prior knowledge hidden in the pose sequence. In this paper, we propose a plug-and-play module named Action Prompt Module (APM) that effectively mines different kinds of action clues for 3D HPE. The highlight is that, the mining scheme of APM can be widely adapted to different frameworks and bring consistent benefits. Specifically, we first present a novel Action-related Text Prompt module (ATP) that directly embeds action labels and transfers the rich language information in the label to the pose sequence. Besides, we further introduce Action-specific Pose Prompt module (APP) to mine the position-aware pose pattern of each action, and exploit the correlation between the mined patterns and input pose sequence for further pose refinement. Experiments show that APM can improve the performance of most video-based 2D-to-3D HPE frameworks by a large margin.
</details>
<details>
<summary>摘要</summary>
最近的2D-to-3D人姿估算（HPE）利用时间连续性来减轻深度不确定性问题，但是它们忽略了动作相关的先前知识。在这篇论文中，我们提出了一个插件式模块 named Action Prompt Module (APM)，可以有效地挖掘不同类型的动作提示。特别是，我们首先提出了一种新的动作相关文本提示模块（ATP），直接嵌入动作标签，将written language信息传递到人姿序列中。此外，我们还引入了动作特定的姿势提示模块（APP），挖掘每种动作的位置感知姿势模式，并利用输入姿势序列和挖掘的模式之间的相关性进行进一步的姿势纠正。实验显示，APM可以提高大多数基于视频的2D-to-3D HPE框架的性能，增加了一定的改进空间。
</details></li>
</ul>
<hr>
<h2 id="LA-Net-Landmark-Aware-Learning-for-Reliable-Facial-Expression-Recognition-under-Label-Noise"><a href="#LA-Net-Landmark-Aware-Learning-for-Reliable-Facial-Expression-Recognition-under-Label-Noise" class="headerlink" title="LA-Net: Landmark-Aware Learning for Reliable Facial Expression Recognition under Label Noise"></a>LA-Net: Landmark-Aware Learning for Reliable Facial Expression Recognition under Label Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09023">http://arxiv.org/abs/2307.09023</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiyu Wu, Jinshi Cui</li>
<li>for: 提高人脸表情识别（FER）的性能，解决实际应用中的标签噪声问题。</li>
<li>methods: 利用人脸特征点（landmark）来减少标签噪声的影响，从两个角度进行处理：首先，使用landmark信息来抑制表情空间中的uncertainty，并通过邻域聚合来提高每个样本的训练指导质量；其次，将landmark信息integrated到表情表示中，使表情特征提取器更加不敏感于标签噪声。</li>
<li>results: 对于在野外 dataset和synthetic noisy dataset的广泛实验，我们示出了LA-Net可以达到领先的性能水平。<details>
<summary>Abstract</summary>
Facial expression recognition (FER) remains a challenging task due to the ambiguity of expressions. The derived noisy labels significantly harm the performance in real-world scenarios. To address this issue, we present a new FER model named Landmark-Aware Net~(LA-Net), which leverages facial landmarks to mitigate the impact of label noise from two perspectives. Firstly, LA-Net uses landmark information to suppress the uncertainty in expression space and constructs the label distribution of each sample by neighborhood aggregation, which in turn improves the quality of training supervision. Secondly, the model incorporates landmark information into expression representations using the devised expression-landmark contrastive loss. The enhanced expression feature extractor can be less susceptible to label noise. Our method can be integrated with any deep neural network for better training supervision without introducing extra inference costs. We conduct extensive experiments on both in-the-wild datasets and synthetic noisy datasets and demonstrate that LA-Net achieves state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
面部表达识别（FER）仍然是一项具有挑战性的任务，主要是因为表达的模糊性。 derivated的噪声标签在实际应用场景中会产生很大的影响。为解决这个问题，我们提出了一种新的FER模型，即Landmark-Aware Net（LA-Net），该模型利用面部特征点来减少表达空间中的uncertainty，并通过邻域聚合来提高每个样本的训练指导质量。其次，模型通过我们提出的表达-特征点对比损失来将特征点信息 incorporated into表达表示，从而使表达特征EXTractor更加抵抗噪声标签的影响。我们的方法可以与任何深度神经网络结合使用，不需要额外的推理成本。我们在野外数据集和静态噪声数据集上进行了广泛的实验，并证明了LA-Net可以达到当前最佳性能。
</details></li>
</ul>
<hr>
<h2 id="Face-PAST-Facial-Pose-Awareness-and-Style-Transfer-Networks"><a href="#Face-PAST-Facial-Pose-Awareness-and-Style-Transfer-Networks" class="headerlink" title="Face-PAST: Facial Pose Awareness and Style Transfer Networks"></a>Face-PAST: Facial Pose Awareness and Style Transfer Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09020">http://arxiv.org/abs/2307.09020</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sunder Ali Khowaja, Ghulam Mujtaba, Jiseok Yoon, Ik Hyun Lee</li>
<li>for: 提出一种基于 StyleGAN 的 facial style transfer 网络，以保持 facial 图像的细节和结构，并生成高质量的样式化图像。</li>
<li>methods: 使用预训练的样式生成网络、循环优化器和门控制单元，以及 facial 结构、身份和分割损失来保持 facial 细节和结构。</li>
<li>results: 通过对存在较少数据的 facial 图像进行样式转移，并且可以生成高质量的样式化图像，而不会过拟合样式或添加artefacts。<details>
<summary>Abstract</summary>
Facial style transfer has been quite popular among researchers due to the rise of emerging technologies such as eXtended Reality (XR), Metaverse, and Non-Fungible Tokens (NFTs). Furthermore, StyleGAN methods along with transfer-learning strategies have reduced the problem of limited data to some extent. However, most of the StyleGAN methods overfit the styles while adding artifacts to facial images. In this paper, we propose a facial pose awareness and style transfer (Face-PAST) network that preserves facial details and structures while generating high-quality stylized images. Dual StyleGAN inspires our work, but in contrast, our work uses a pre-trained style generation network in an external style pass with a residual modulation block instead of a transform coding block. Furthermore, we use the gated mapping unit and facial structure, identity, and segmentation losses to preserve the facial structure and details. This enables us to train the network with a very limited amount of data while generating high-quality stylized images. Our training process adapts curriculum learning strategy to perform efficient and flexible style mixing in the generative space. We perform extensive experiments to show the superiority of Face-PAST in comparison to existing state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
Facial style transfer已经非常受研究人员欢迎，因为emerging technologies如XR、Metaverse和NFTs的出现。此外，StyleGAN方法和传播学习策略有助于缓解有限数据的问题。然而，大多数StyleGAN方法会过滤式，导致facial image中的瑕疵和错误。在这篇论文中，我们提出了一个名为Face-PAST的 facial pose awareness和style transfer网络，能够保留facial detail和结构，同时生成高品质的类型化图像。我们的作业受到了Dual StyleGAN的灵感，但是我们使用了一个预训式的style生成网络，而不是一个transform coding block。此外，我们使用了闸道 mapping单元和facial结构、认知和分类损失，以保留facial structure和瑕疵。这使得我们可以在有限数据的情况下训练网络，并生成高品质的类型化图像。我们的训练过程使用了curriculum learning策略，以实现有效和灵活的style混合在生成空间中。我们进行了广泛的实验，以显示Face-PAST在与现有的州际状态方法相比之下的superiority。
</details></li>
</ul>
<hr>
<h2 id="U-shaped-Transformer-Retain-High-Frequency-Context-in-Time-Series-Analysis"><a href="#U-shaped-Transformer-Retain-High-Frequency-Context-in-Time-Series-Analysis" class="headerlink" title="U-shaped Transformer: Retain High Frequency Context in Time Series Analysis"></a>U-shaped Transformer: Retain High Frequency Context in Time Series Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09019">http://arxiv.org/abs/2307.09019</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qingkui Chen, Yiqin Zhang</li>
<li>for: 这篇论文旨在提出一种基于 transformer 框架的时间序列预测模型，利用 skip-layer 连接和 patch 合并分割操作提高模型的精度和效率。</li>
<li>methods: 该模型采用了 traditional transformer 框架，并加入了 skip-layer 连接和 patch 合并分割操作，以提高模型的精度和效率。</li>
<li>results: 实验结果表明，该模型在多个数据集上达到了高水平的预测性能，而且比传统的 transformer 模型更高效。<details>
<summary>Abstract</summary>
Time series prediction plays a crucial role in various industrial fields. In recent years, neural networks with a transformer backbone have achieved remarkable success in many domains, including computer vision and NLP. In time series analysis domain, some studies have suggested that even the simplest MLP networks outperform advanced transformer-based networks on time series forecast tasks. However, we believe these findings indicate there to be low-rank properties in time series sequences. In this paper, we consider the low-pass characteristics of transformers and try to incorporate the advantages of MLP. We adopt skip-layer connections inspired by Unet into traditional transformer backbone, thus preserving high-frequency context from input to output, namely U-shaped Transformer. We introduce patch merge and split operation to extract features with different scales and use larger datasets to fully make use of the transformer backbone. Our experiments demonstrate that the model performs at an advanced level across multiple datasets with relatively low cost.
</details>
<details>
<summary>摘要</summary>
时序序列预测在各个行业中扮演着关键的角色。在最近几年，基于transformer结构的神经网络在计算机视觉和自然语言处理等领域取得了很大的成功。然而，一些研究表明，简单的MLP网络可以在时序序列预测任务上表现更出色于高级的transformer-based网络。我们认为这些发现表明时序序列序列具有低级属性。在这篇论文中，我们考虑了transformer的低通过性特性，并尝试将MLP网络的优点与transformer结构相结合。我们采用了 skip-layer 连接，以保持输入到输出的高频上下文，即U-shaped Transformer。我们还引入了补丁合并和分裂操作，以提取不同尺度的特征，并使用更大的数据集，以全面利用transformer结构。我们的实验表明，模型在多个数据集上达到了高水平的性能，而且相对成本较低。
</details></li>
</ul>
<hr>
<h2 id="Survey-on-Controlable-Image-Synthesis-with-Deep-Learning"><a href="#Survey-on-Controlable-Image-Synthesis-with-Deep-Learning" class="headerlink" title="Survey on Controlable Image Synthesis with Deep Learning"></a>Survey on Controlable Image Synthesis with Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10275">http://arxiv.org/abs/2307.10275</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Shixiong Zhang, Jiao Li, Lu Yang</li>
<li>for: 本研究旨在investigate low-level controllable image synthesis problem, 用于精细图像渲染和修改任务。</li>
<li>methods: 本文使用deep learning技术, 尤其是生成模型, 来实现可控图像生成方法。</li>
<li>results: 本文对3D可控图像生成进行了评估, 并结合了评价指标和数据集。 更进一步, 本文还 briefly summarized 相关应用、产品和资源 для实践者。<details>
<summary>Abstract</summary>
Image synthesis has attracted emerging research interests in academic and industry communities. Deep learning technologies especially the generative models greatly inspired controllable image synthesis approaches and applications, which aim to generate particular visual contents with latent prompts. In order to further investigate low-level controllable image synthesis problem which is crucial for fine image rendering and editing tasks, we present a survey of some recent works on 3D controllable image synthesis using deep learning. We first introduce the datasets and evaluation indicators for 3D controllable image synthesis. Then, we review the state-of-the-art research for geometrically controllable image synthesis in two aspects: 1) Viewpoint/pose-controllable image synthesis; 2) Structure/shape-controllable image synthesis. Furthermore, the photometrically controllable image synthesis approaches are also reviewed for 3D re-lighting researches. While the emphasis is on 3D controllable image synthesis algorithms, the related applications, products and resources are also briefly summarized for practitioners.
</details>
<details>
<summary>摘要</summary>
Image合成已经吸引了学术和工业社区的新兴研究兴趣。特别是深度学习技术，包括生成模型，对可控图像生成方法和应用产生了启发。为了进一步调查低级可控图像生成问题，这问题对细节图像渲染和修订任务是关键的。我们现在介绍一些最新的3D可控图像生成使用深度学习的研究。我们首先介绍了3D可控图像生成数据集和评价指标。然后，我们对3D可控图像生成的两个方面进行了回顾：1）视点/姿态可控图像生成; 2）结构/形状可控图像生成。此外，我们还回顾了3D重新照明研究中的光学可控图像生成方法。虽然我们的重点是3D可控图像生成算法，但我们还 briefly summarized了相关应用、产品和资源，为实践者提供参考。
</details></li>
</ul>
<hr>
<h2 id="Soft-IntroVAE-for-Continuous-Latent-space-Image-Super-Resolution"><a href="#Soft-IntroVAE-for-Continuous-Latent-space-Image-Super-Resolution" class="headerlink" title="Soft-IntroVAE for Continuous Latent space Image Super-Resolution"></a>Soft-IntroVAE for Continuous Latent space Image Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09008">http://arxiv.org/abs/2307.09008</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhi-Song Liu, Zijia Wang, Zhen Jia</li>
<li>for: 这篇论文旨在提出一种基于Variational AutoEncoder的连续图像超分辨率（SR）方法，用于实现实用和灵活的图像缩放 для各种显示器。</li>
<li>methods: 该方法使用了Local implicit image representation，并基于Variational AutoEncoder进行 latent space interpolation。另外，一种新的潜在空间对抗训练方法是使用的，以实现照片真实的图像修复。</li>
<li>results: 对比其他方法，提出的Soft-introVAE-SR方法可以更好地提高图像的质量，并且可以扩展到噪声除除和实际图像超分辨率领域。<details>
<summary>Abstract</summary>
Continuous image super-resolution (SR) recently receives a lot of attention from researchers, for its practical and flexible image scaling for various displays. Local implicit image representation is one of the methods that can map the coordinates and 2D features for latent space interpolation. Inspired by Variational AutoEncoder, we propose a Soft-introVAE for continuous latent space image super-resolution (SVAE-SR). A novel latent space adversarial training is achieved for photo-realistic image restoration. To further improve the quality, a positional encoding scheme is used to extend the original pixel coordinates by aggregating frequency information over the pixel areas. We show the effectiveness of the proposed SVAE-SR through quantitative and qualitative comparisons, and further, illustrate its generalization in denoising and real-image super-resolution.
</details>
<details>
<summary>摘要</summary>
continuous image super-resolution (SR) 近期吸引了许多研究人员的关注，因为它可以实现多种显示器上的图像缩放。本地隐式图像表示是一种可以将坐标和2D特征映射到隐藏空间的方法。 inspirited by Variational AutoEncoder, we propose a Soft-introVAE for continuous latent space image super-resolution (SVAE-SR).一种新的隐藏空间对抗训练方法是实现 фото真实图像修复。为了进一步提高质量，我们使用位置编码方案来扩展原始像素坐标，并将频率信息聚合到像素区域上。我们通过量化和质量比较，证明了我们提出的 SVAE-SR 的效果。此外，我们还ILLUSTRATE 其泛化性在噪声除除和真实图像超分解中。
</details></li>
</ul>
<hr>
<h2 id="Frequency-mixed-Single-source-Domain-Generalization-for-Medical-Image-Segmentation"><a href="#Frequency-mixed-Single-source-Domain-Generalization-for-Medical-Image-Segmentation" class="headerlink" title="Frequency-mixed Single-source Domain Generalization for Medical Image Segmentation"></a>Frequency-mixed Single-source Domain Generalization for Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09005">http://arxiv.org/abs/2307.09005</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liamheng/non-iid_medical_image_segmentation">https://github.com/liamheng/non-iid_medical_image_segmentation</a></li>
<li>paper_authors: Heng Li, Haojin Li, Wei Zhao, Huazhu Fu, Xiuyun Su, Yan Hu, Jiang Liu</li>
<li>for: 提高医疗影像分类模型的一般化性，尤其是当标注数据稀缺时。</li>
<li>methods: 提出了一种新的频率混合单源领域一致化方法（FreeSDG），通过分析频率对领域差异的影响，利用混合频率 спектル增强单源领域。同时，建立了自我监督来学习具有Robust特征的分类表示。</li>
<li>results: 透过实验证明，FreeSDG比前一项方法高效，可以增强医疗影像分类模型的一般化性，特别是当标注数据稀缺时。<details>
<summary>Abstract</summary>
The annotation scarcity of medical image segmentation poses challenges in collecting sufficient training data for deep learning models. Specifically, models trained on limited data may not generalize well to other unseen data domains, resulting in a domain shift issue. Consequently, domain generalization (DG) is developed to boost the performance of segmentation models on unseen domains. However, the DG setup requires multiple source domains, which impedes the efficient deployment of segmentation algorithms in clinical scenarios. To address this challenge and improve the segmentation model's generalizability, we propose a novel approach called the Frequency-mixed Single-source Domain Generalization method (FreeSDG). By analyzing the frequency's effect on domain discrepancy, FreeSDG leverages a mixed frequency spectrum to augment the single-source domain. Additionally, self-supervision is constructed in the domain augmentation to learn robust context-aware representations for the segmentation task. Experimental results on five datasets of three modalities demonstrate the effectiveness of the proposed algorithm. FreeSDG outperforms state-of-the-art methods and significantly improves the segmentation model's generalizability. Therefore, FreeSDG provides a promising solution for enhancing the generalization of medical image segmentation models, especially when annotated data is scarce. The code is available at https://github.com/liamheng/Non-IID_Medical_Image_Segmentation.
</details>
<details>
<summary>摘要</summary>
医学图像分割涉及到缺乏注释的问题，这会影响深度学习模型的训练数据收集。具体来说，由限制数据训练的模型可能无法在其他未见数据域上泛化良好，导致领域变化问题。为了解决这个问题并提高分割模型的泛化性，我们提出了一种新的方法 called Frequency-mixed Single-source Domain Generalization method (FreeSDG)。通过分析频谱的效果，FreeSDG利用了混合频谱来扩展单源频谱。此外，我们还构建了自我超vision来学习robust的上下文感知表示。实验结果表明，提出的方法可以在五个数据集上三种模式上达到最佳性能。FreeSDG比 estado-of-the-art 方法更高效，并显著提高了分割模型的泛化性。因此，FreeSDG 提供了医学图像分割模型的泛化问题中的一个有前途的解决方案，特别是当注释数据缺乏时。代码可以在 <https://github.com/liamheng/Non-IID_Medical_Image_Segmentation> 上找到。
</details></li>
</ul>
<hr>
<h2 id="TractCloud-Registration-free-tractography-parcellation-with-a-novel-local-global-streamline-point-cloud-representation"><a href="#TractCloud-Registration-free-tractography-parcellation-with-a-novel-local-global-streamline-point-cloud-representation" class="headerlink" title="TractCloud: Registration-free tractography parcellation with a novel local-global streamline point cloud representation"></a>TractCloud: Registration-free tractography parcellation with a novel local-global streamline point cloud representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09000">http://arxiv.org/abs/2307.09000</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/SlicerDMRI/TractCloud">https://github.com/SlicerDMRI/TractCloud</a></li>
<li>paper_authors: Tengfei Xue, Yuqian Chen, Chaoyi Zhang, Alexandra J. Golby, Nikos Makris, Yogesh Rathi, Weidong Cai, Fan Zhang, Lauren J. O’Donnell</li>
<li>for: 这篇论文的目的是提出一种无需注册的束分割方法，以便在各个个体空间中进行束分割，并且能够在大规模数据集上高效地进行分析。</li>
<li>methods: 该方法使用了一种新的、可学习的、本地-全局束表示方法，利用周围的束和整个脑部的束来描述本地解剖学和全脑姿态。</li>
<li>results: 论文在五个独立获得的测试数据集上进行了测试，并与多种状态的人群和疾病进行了比较，并显示了与之前的状态分割方法相比的显著优势。<details>
<summary>Abstract</summary>
Diffusion MRI tractography parcellation classifies streamlines into anatomical fiber tracts to enable quantification and visualization for clinical and scientific applications. Current tractography parcellation methods rely heavily on registration, but registration inaccuracies can affect parcellation and the computational cost of registration is high for large-scale datasets. Recently, deep-learning-based methods have been proposed for tractography parcellation using various types of representations for streamlines. However, these methods only focus on the information from a single streamline, ignoring geometric relationships between the streamlines in the brain. We propose TractCloud, a registration-free framework that performs whole-brain tractography parcellation directly in individual subject space. We propose a novel, learnable, local-global streamline representation that leverages information from neighboring and whole-brain streamlines to describe the local anatomy and global pose of the brain. We train our framework on a large-scale labeled tractography dataset, which we augment by applying synthetic transforms including rotation, scaling, and translations. We test our framework on five independently acquired datasets across populations and health conditions. TractCloud significantly outperforms several state-of-the-art methods on all testing datasets. TractCloud achieves efficient and consistent whole-brain white matter parcellation across the lifespan (from neonates to elderly subjects, including brain tumor patients) without the need for registration. The robustness and high inference speed of TractCloud make it suitable for large-scale tractography data analysis. Our project page is available at https://tractcloud.github.io/.
</details>
<details>
<summary>摘要</summary>
Diffusion MRI tractography parcellation 分类ifies streamlines into anatomical fiber tracts, allowing for quantification and visualization in clinical and scientific applications. Current tractography parcellation methods rely heavily on registration, but registration inaccuracies can affect parcellation and increase computational cost for large-scale datasets. Recently, deep-learning-based methods have been proposed for tractography parcellation using various types of streamline representations. However, these methods only focus on information from a single streamline, ignoring geometric relationships between streamlines in the brain.We propose TractCloud, a registration-free framework that performs whole-brain tractography parcellation directly in individual subject space. We use a novel, learnable, local-global streamline representation that leverages information from neighboring and whole-brain streamlines to describe the local anatomy and global pose of the brain. We train our framework on a large-scale labeled tractography dataset and augment it with synthetic transforms including rotation, scaling, and translations. We test our framework on five independently acquired datasets across populations and health conditions.TractCloud significantly outperforms several state-of-the-art methods on all testing datasets, achieving efficient and consistent whole-brain white matter parcellation across the lifespan (from neonates to elderly subjects, including brain tumor patients) without the need for registration. The robustness and high inference speed of TractCloud make it suitable for large-scale tractography data analysis. For more information, please visit our project page at <https://tractcloud.github.io/>.
</details></li>
</ul>
<hr>
<h2 id="Towards-Authentic-Face-Restoration-with-Iterative-Diffusion-Models-and-Beyond"><a href="#Towards-Authentic-Face-Restoration-with-Iterative-Diffusion-Models-and-Beyond" class="headerlink" title="Towards Authentic Face Restoration with Iterative Diffusion Models and Beyond"></a>Towards Authentic Face Restoration with Iterative Diffusion Models and Beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08996">http://arxiv.org/abs/2307.08996</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Zhao, Tingbo Hou, Yu-Chuan Su, Xuhui Jia. Yandong Li, Matthias Grundmann</li>
<li>for:  This paper aims to propose an authentic face restoration system that can generate high-quality and realistic faces from low-quality ones, which is important in various computer vision applications such as image enhancement, video communication, and taking portrait.</li>
<li>methods:  The proposed method, called $\textbf{IDM}$, is based on denoising diffusion models (DDMs) and uses iterative learning to achieve authentic face restoration. The method has two aspects of intrinsic iterative refinement and extrinsic iterative enhancement to preserve the content and gradually refine the high-quality details.</li>
<li>results:  The proposed method demonstrates superior performance on blind face restoration tasks and can also clean the data to improve the restoration task. Additionally, the authentically cleaned data generated by the proposed method is found to be helpful for image generation tasks, achieving better quality than state-of-the-art on FFHQ and ImageNet generation using either GANs or diffusion models without modifying the models.<details>
<summary>Abstract</summary>
An authentic face restoration system is becoming increasingly demanding in many computer vision applications, e.g., image enhancement, video communication, and taking portrait. Most of the advanced face restoration models can recover high-quality faces from low-quality ones but usually fail to faithfully generate realistic and high-frequency details that are favored by users. To achieve authentic restoration, we propose $\textbf{IDM}$, an $\textbf{I}$teratively learned face restoration system based on denoising $\textbf{D}$iffusion $\textbf{M}$odels (DDMs). We define the criterion of an authentic face restoration system, and argue that denoising diffusion models are naturally endowed with this property from two aspects: intrinsic iterative refinement and extrinsic iterative enhancement. Intrinsic learning can preserve the content well and gradually refine the high-quality details, while extrinsic enhancement helps clean the data and improve the restoration task one step further. We demonstrate superior performance on blind face restoration tasks. Beyond restoration, we find the authentically cleaned data by the proposed restoration system is also helpful to image generation tasks in terms of training stabilization and sample quality. Without modifying the models, we achieve better quality than state-of-the-art on FFHQ and ImageNet generation using either GANs or diffusion models.
</details>
<details>
<summary>摘要</summary>
一个真实的脸部恢复系统在许多计算机视觉应用中日益增加要求，例如图像提高、视频通信和拍照。大多数高级脸部恢复模型可以从低质量脸部恢复出高质量脸部，但通常无法准确地生成用户喜欢的真实和高频率细节。为实现真实的恢复，我们提出了 $\textbf{IDM}$，一种基于杂化扩散模型（DDM）的迭代学习face restoration系统。我们定义了真实的脸部恢复系统的标准，并论证DDM自然拥有这种属性，从两个方面：内在迭代细化和外在迭代增强。内在学习可以保持内容良好，逐渐细化高质量细节，而外在增强可以清洁数据，提高恢复任务一步更进。我们在盲目脸部恢复任务上展示了superior性能。此外，我们发现由我们提posed的恢复系统 authentically cleaned的数据不仅有助于图像生成任务的训练稳定和样本质量，而且可以在使用GANs或扩散模型时达到更高的质量。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-Latent-Space-of-GAN-Inversion-for-Real-Image-Editing"><a href="#Revisiting-Latent-Space-of-GAN-Inversion-for-Real-Image-Editing" class="headerlink" title="Revisiting Latent Space of GAN Inversion for Real Image Editing"></a>Revisiting Latent Space of GAN Inversion for Real Image Editing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08995">http://arxiv.org/abs/2307.08995</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kai Katsumata, Duc Minh Vo, Bei Liu, Hideki Nakayama</li>
<li>for: 这个研究旨在解决StyleGANs中的实像增强和实像增强之间的贸易问题，提供一个新的实像增强方法。</li>
<li>methods: 本研究使用StyleGANs的抽象层 $\mathcal{Z}$ 和高能量的潜在空间，建立一个新的合成空间 $\mathcal{F}&#x2F;\mathcal{Z}^{+}$，以实现实像增强而不丧失图像质量。</li>
<li>results: 实验结果显示， $\mathcal{Z}^{+}$ 可以取代常用的 $\mathcal{W}$, $\mathcal{W}^{+}$, 和 $\mathcal{S}$ 空间，保持图像重建质量，并且实现Semantic editing。<details>
<summary>Abstract</summary>
The exploration of the latent space in StyleGANs and GAN inversion exemplify impressive real-world image editing, yet the trade-off between reconstruction quality and editing quality remains an open problem. In this study, we revisit StyleGANs' hyperspherical prior $\mathcal{Z}$ and combine it with highly capable latent spaces to build combined spaces that faithfully invert real images while maintaining the quality of edited images. More specifically, we propose $\mathcal{F}/\mathcal{Z}^{+}$ space consisting of two subspaces: $\mathcal{F}$ space of an intermediate feature map of StyleGANs enabling faithful reconstruction and $\mathcal{Z}^{+}$ space of an extended StyleGAN prior supporting high editing quality. We project the real images into the proposed space to obtain the inverted codes, by which we then move along $\mathcal{Z}^{+}$, enabling semantic editing without sacrificing image quality. Comprehensive experiments show that $\mathcal{Z}^{+}$ can replace the most commonly-used $\mathcal{W}$, $\mathcal{W}^{+}$, and $\mathcal{S}$ spaces while preserving reconstruction quality, resulting in reduced distortion of edited images.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate the following text into Simplified Chinese:The exploration of the latent space in StyleGANs and GAN inversion exemplify impressive real-world image editing, yet the trade-off between reconstruction quality and editing quality remains an open problem. In this study, we revisit StyleGANs' hyperspherical prior $\mathcal{Z}$ and combine it with highly capable latent spaces to build combined spaces that faithfully invert real images while maintaining the quality of edited images. More specifically, we propose $\mathcal{F}/\mathcal{Z}^{+}$ space consisting of two subspaces: $\mathcal{F}$ space of an intermediate feature map of StyleGANs enabling faithful reconstruction and $\mathcal{Z}^{+}$ space of an extended StyleGAN prior supporting high editing quality. We project the real images into the proposed space to obtain the inverted codes, by which we then move along $\mathcal{Z}^{+}$, enabling semantic editing without sacrificing image quality. Comprehensive experiments show that $\mathcal{Z}^{+}$ can replace the most commonly-used $\mathcal{W}$, $\mathcal{W}^{+}$, and $\mathcal{S}$ spaces while preserving reconstruction quality, resulting in reduced distortion of edited images.Translation: StyleGANs 的latent空间的探索和GAN倒计时的实际图像编辑，却存在一个公共的问题，即重建质量和编辑质量之间的权衡。在这种研究中，我们回到StyleGANs的高维度先验 Space $\mathcal{Z}$ 和高能量的latent空间，并将其组合成一个faithful的图像重建和高质量编辑的共同空间。我们提出了 $\mathcal{F}/\mathcal{Z}^{+}$ 空间，包括两个子空间： $\mathcal{F}$ 空间是 StyleGANs 中间特征图的intermediate feature map，可以实现 faithful reconstruction，而 $\mathcal{Z}^{+}$ 空间是 StyleGANs 的扩展先验空间，可以支持高质量的编辑。我们将实际图像映射到我们提出的空间中，并在 $\mathcal{Z}^{+}$ 空间中移动，以实现 semantics编辑而无需牺牲图像质量。我们的全面实验表明， $\mathcal{Z}^{+}$ 可以取代通常使用的 $\mathcal{W}$, $\mathcal{W}^{+}$ 和 $\mathcal{S}$ 空间，保持重建质量，并减少编辑图像的扭曲。
</details></li>
</ul>
<hr>
<h2 id="Human-Action-Recognition-in-Still-Images-Using-ConViT"><a href="#Human-Action-Recognition-in-Still-Images-Using-ConViT" class="headerlink" title="Human Action Recognition in Still Images Using ConViT"></a>Human Action Recognition in Still Images Using ConViT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08994">http://arxiv.org/abs/2307.08994</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seyed Rohollah Hosseyni, Hasan Taheri, Sanaz Seyedin, Ali Ahmad Rahmani</li>
<li>for: 本研究旨在提高图像识别 tasks 中不同部分之间的关系理解，以提高人体动作识别精度。</li>
<li>methods: 本研究提出了一种新的模块，它使用 Vision Transformer (ViT) 来模型图像各部分之间的关系。该模型包括一个深度卷积网络，用于提取图像高级空间特征，以及一个 Vision Transformer，用于使用特征图来捕捉图像各部分之间的关系。</li>
<li>results: 本研究在 Stanford40 和 PASCAL VOC 2012 动作数据集上进行了评估，并 achieved 95.5% mAP 和 91.5% mAP 结果，这些结果与其他当前领先方法相比较出色。<details>
<summary>Abstract</summary>
Understanding the relationship between different parts of the image plays a crucial role in many visual recognition tasks. Despite the fact that Convolutional Neural Networks (CNNs) have demonstrated impressive results in detecting single objects, they lack the capability to extract the relationship between various regions of an image, which is a crucial factor in human action recognition. To address this problem, this paper proposes a new module that functions like a convolutional layer using Vision Transformer (ViT). The proposed action recognition model comprises two components: the first part is a deep convolutional network that extracts high-level spatial features from the image, and the second component of the model utilizes a Vision Transformer that extracts the relationship between various regions of the image using the feature map generated by the CNN output. The proposed model has been evaluated on the Stanford40 and PASCAL VOC 2012 action datasets and has achieved 95.5% mAP and 91.5% mAP results, respectively, which are promising compared to other state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
理解图像中不同部分之间的关系在许多视觉识别任务中扮演着关键角色。尽管卷积神经网络（CNN）在检测单个 объек 上表现出了惊人的成绩，但它缺乏抽象图像中不同部分之间的关系抽取能力，这是人类动作识别中的关键因素。为解决这个问题，本文提出了一个新的模块，该模块使用视力 трансформер（ViT）来实现卷积操作。该模型包括两个部分：第一部分是一个深度卷积神经网络，该神经网络从图像中提取高级空间特征；第二部分的模型使用 feature map 生成于 CNN 输出来抽取图像中不同部分之间的关系。该模型在 Standford40 和 PASCAL VOC 2012 动作数据集上进行了评估，并 achieved 95.5% mAP 和 91.5% mAP 的结果，这些结果与其他当前状态的方法相当出色。
</details></li>
</ul>
<hr>
<h2 id="Arbitrary-point-cloud-upsampling-via-Dual-Back-Projection-Network"><a href="#Arbitrary-point-cloud-upsampling-via-Dual-Back-Projection-Network" class="headerlink" title="Arbitrary point cloud upsampling via Dual Back-Projection Network"></a>Arbitrary point cloud upsampling via Dual Back-Projection Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08992">http://arxiv.org/abs/2307.08992</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhi-Song Liu, Zijia Wang, Zhen Jia</li>
<li>for: 该论文主要针对精度低的点云数据进行重建，以提高点云的密度并重建细节 geometric 信息。</li>
<li>methods: 该方法基于 Dual Back-Projection 网络（DBPnet），通过在上下文中具有 Point cloud upsampling 的方式进行减小点云的重建误差。</li>
<li>results: 实验结果显示，该方法可以在不同的upsampling因子（例如 4x、5.5x）下实现最低的点集匹配损失，并且成功地验证了非均匀点云的重建。<details>
<summary>Abstract</summary>
Point clouds acquired from 3D sensors are usually sparse and noisy. Point cloud upsampling is an approach to increase the density of the point cloud so that detailed geometric information can be restored. In this paper, we propose a Dual Back-Projection network for point cloud upsampling (DBPnet). A Dual Back-Projection is formulated in an up-down-up manner for point cloud upsampling. It not only back projects feature residues but also coordinates residues so that the network better captures the point correlations in the feature and space domains, achieving lower reconstruction errors on both uniform and non-uniform sparse point clouds. Our proposed method is also generalizable for arbitrary upsampling tasks (e.g. 4x, 5.5x). Experimental results show that the proposed method achieves the lowest point set matching losses with respect to the benchmark. In addition, the success of our approach demonstrates that generative networks are not necessarily needed for non-uniform point clouds.
</details>
<details>
<summary>摘要</summary>
点云数据通常是稀疏的和噪声污染的。点云upsampling是一种方法来增加点云的密度，以便从重建细节的几何信息。在这篇论文中，我们提出了双向反投影网络（DBPnet）。双向反投影是在上下两个方向进行的，用于点云upsampling。它不仅反投影特征剩余，还反投影坐标剩余，从而使网络更好地捕捉点云之间的相关性，实现了较低的重建错误率。我们提出的方法可应用于任意的upsampling任务（例如4倍、5.5倍）。实验结果显示，我们的方法实现了对比准标的最低点集匹配损失。此外，我们的成功表明了生成网络并不一定需要非均匀点云。
</details></li>
</ul>
<hr>
<h2 id="EgoVM-Achieving-Precise-Ego-Localization-using-Lightweight-Vectorized-Maps"><a href="#EgoVM-Achieving-Precise-Ego-Localization-using-Lightweight-Vectorized-Maps" class="headerlink" title="EgoVM: Achieving Precise Ego-Localization using Lightweight Vectorized Maps"></a>EgoVM: Achieving Precise Ego-Localization using Lightweight Vectorized Maps</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08991">http://arxiv.org/abs/2307.08991</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuzhe He, Shuang Liang, Xiaofei Rui, Chengying Cai, Guowei Wan</li>
<li>for: 本研究旨在提供一种高精度、轻量级的 egolocalization 方法，以满足自动驾驶技术的需求。</li>
<li>methods: 本方法使用了vectorized maps，与传统的点云地图相比，具有较低的计算复杂度和存储量。具体来说，我们首先从多视图图像和 LiDAR 点云中提取 BEV 特征，然后使用一组学习式的semantic embedding来编码地图元素的semantic类型，并通过semantic segmentation进行超出检查。接着，我们将地图查询，包括学习式的semantic embedding和地图元素坐标，传递给 transformer decoder进行交叉模态匹配。最后，我们采用了一种可靠的 histogram-based pose solver，以搜索所有可能的姿态，并优化最佳姿态。</li>
<li>results: 我们在 nuScenes 数据集和新收集的数据集上进行了广泛的验证，结果显示，我们的方法可以实现厘米级的 lokalisierung 精度，并在使用 vectorized maps 的情况下，与现有方法相比，提供了较大的提升。此外，我们的模型已经在一大群自动驾驶车辆中进行了广泛的测试，并在多种复杂的城市场景下表现出色。<details>
<summary>Abstract</summary>
Accurate and reliable ego-localization is critical for autonomous driving. In this paper, we present EgoVM, an end-to-end localization network that achieves comparable localization accuracy to prior state-of-the-art methods, but uses lightweight vectorized maps instead of heavy point-based maps. To begin with, we extract BEV features from online multi-view images and LiDAR point cloud. Then, we employ a set of learnable semantic embeddings to encode the semantic types of map elements and supervise them with semantic segmentation, to make their feature representation consistent with BEV features. After that, we feed map queries, composed of learnable semantic embeddings and coordinates of map elements, into a transformer decoder to perform cross-modality matching with BEV features. Finally, we adopt a robust histogram-based pose solver to estimate the optimal pose by searching exhaustively over candidate poses. We comprehensively validate the effectiveness of our method using both the nuScenes dataset and a newly collected dataset. The experimental results show that our method achieves centimeter-level localization accuracy, and outperforms existing methods using vectorized maps by a large margin. Furthermore, our model has been extensively tested in a large fleet of autonomous vehicles under various challenging urban scenes.
</details>
<details>
<summary>摘要</summary>
准确可靠的自驾车导航启用需要精准的ego-localization。在这篇论文中，我们提出了EgoVM，一种终端到端的地图localization网络，可以与之前的State-of-the-art方法相比，但使用轻量级的 вектор化地图而不是重量级的点云地图。我们从多视图图像和LiDAR点云中提取了BEV特征，然后使用一组可学习的semantic embedding来编码地图元素的semantic类型，并使用semantic segmentation来监督它们的特征表示相符合BEV特征。接着，我们将地图查询，由learnable semantic embedding和地图元素坐标组成，传递给一个transformer解码器进行跨模态匹配与BEV特征。最后，我们采用一种稳定的 histogram-based pose解决方案来估算最佳pose，通过搜索所有候选pose来找到最佳pose。我们对使用nuScenes数据集和新收集的数据集进行了广泛验证，结果表明我们的方法可以实现厘米级准确的localization，并与使用 вектор化地图的现有方法相比，大幅提高性能。此外，我们的模型在一大群自动驾车车辆下进行了广泛的测试，并在各种复杂的城市场景下运行。
</details></li>
</ul>
<hr>
<h2 id="In-Defense-of-Clip-based-Video-Relation-Detection"><a href="#In-Defense-of-Clip-based-Video-Relation-Detection" class="headerlink" title="In Defense of Clip-based Video Relation Detection"></a>In Defense of Clip-based Video Relation Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08984">http://arxiv.org/abs/2307.08984</a></li>
<li>repo_url: None</li>
<li>paper_authors: Meng Wei, Long Chen, Wei Ji, Xiaoyu Yue, Roger Zimmermann</li>
<li>for: 本研究旨在提高视频视关系检测（VidVRD）的精度和效率，通过空间矩阵和时间边界来检测视频中的视觉关系 triplets。</li>
<li>methods: 该研究使用clip-based方法，在不同的clip中分别进行视觉关系的检测和融合，以达到更高的检测精度和效率。</li>
<li>results: 对于两个 VidVRD 测试 benchmark 进行了广泛的实验，并证明了使用clip tubelets可以达到更高的性能，而且clip tubelets在模型设计方面具有更多的灵活性，可以更好地alleviate long-term object tracking问题和视频Tubelet特征压缩中的时间信息损失。<details>
<summary>Abstract</summary>
Video Visual Relation Detection (VidVRD) aims to detect visual relationship triplets in videos using spatial bounding boxes and temporal boundaries. Existing VidVRD methods can be broadly categorized into bottom-up and top-down paradigms, depending on their approach to classifying relations. Bottom-up methods follow a clip-based approach where they classify relations of short clip tubelet pairs and then merge them into long video relations. On the other hand, top-down methods directly classify long video tubelet pairs. While recent video-based methods utilizing video tubelets have shown promising results, we argue that the effective modeling of spatial and temporal context plays a more significant role than the choice between clip tubelets and video tubelets. This motivates us to revisit the clip-based paradigm and explore the key success factors in VidVRD. In this paper, we propose a Hierarchical Context Model (HCM) that enriches the object-based spatial context and relation-based temporal context based on clips. We demonstrate that using clip tubelets can achieve superior performance compared to most video-based methods. Additionally, using clip tubelets offers more flexibility in model designs and helps alleviate the limitations associated with video tubelets, such as the challenging long-term object tracking problem and the loss of temporal information in long-term tubelet feature compression. Extensive experiments conducted on two challenging VidVRD benchmarks validate that our HCM achieves a new state-of-the-art performance, highlighting the effectiveness of incorporating advanced spatial and temporal context modeling within the clip-based paradigm.
</details>
<details>
<summary>摘要</summary>
视频视关系检测（VidVRD）的目标是在视频中检测视关系三元组使用空间矩形框和时间边界。现有的 VidVRD 方法可以分为底层和顶层两类，它们根据他们如何分类关系来进行分类。底层方法采用帧基本方法，先将短 clip 对组分类，然后将它们合并成为长视频关系。相反，顶层方法直接将长视频对组分类。虽然最近的视频基本方法使用视频封装件（Tubelet）已经显示出了有利的成绩，但我们认为在空间和时间上更好地模型上下文比clip tubelets和视频封装件之间的选择更重要。这种情况 motivates 我们重新评估clip-based paradigm，并探索关键成功因素。在这篇论文中，我们提出了层次上下文模型（HCM），它在clip中增强了对象空间上下文和关系时间上下文。我们示出，使用clip tubelets可以在大多数视频基本方法中获得更高的性能，并且使用clip tubelets的模型设计具有更多的灵活性，可以解决视频封装件中的长期对象跟踪问题和长期封装件特征压缩中的时间信息损失问题。我们在两个 VidVRD benchmark 上进行了广泛的实验，并证明了我们的 HCM 实现了新的州Of-The-Art性能，这highlights 了在clip-based paradigm中包含先进的空间和时间上下文模型的效iveness。
</details></li>
</ul>
<hr>
<h2 id="Learned-Scalable-Video-Coding-For-Humans-and-Machines"><a href="#Learned-Scalable-Video-Coding-For-Humans-and-Machines" class="headerlink" title="Learned Scalable Video Coding For Humans and Machines"></a>Learned Scalable Video Coding For Humans and Machines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08978">http://arxiv.org/abs/2307.08978</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hadi Hadizadeh, Ivan V. Bajić</li>
<li>for: 这个论文是为了支持自动视频分析，而不是只是为了人类视觉。</li>
<li>methods: 该论文使用了深度神经网络（DNNs）来实现Conditional Coding，以实现更好的压缩效果。</li>
<li>results: 实验结果表明，该 Framework 在四个标准视频数据集上表现出了更好的压缩效果，而且可以保持与人类视觉任务的相似性。<details>
<summary>Abstract</summary>
Video coding has traditionally been developed to support services such as video streaming, videoconferencing, digital TV, and so on. The main intent was to enable human viewing of the encoded content. However, with the advances in deep neural networks (DNNs), encoded video is increasingly being used for automatic video analytics performed by machines. In applications such as automatic traffic monitoring, analytics such as vehicle detection, tracking and counting, would run continuously, while human viewing could be required occasionally to review potential incidents. To support such applications, a new paradigm for video coding is needed that will facilitate efficient representation and compression of video for both machine and human use in a scalable manner. In this manuscript, we introduce the first end-to-end learnable video codec that supports a machine vision task in its base layer, while its enhancement layer supports input reconstruction for human viewing. The proposed system is constructed based on the concept of conditional coding to achieve better compression gains. Comprehensive experimental evaluations conducted on four standard video datasets demonstrate that our framework outperforms both state-of-the-art learned and conventional video codecs in its base layer, while maintaining comparable performance on the human vision task in its enhancement layer. We will provide the implementation of the proposed system at www.github.com upon completion of the review process.
</details>
<details>
<summary>摘要</summary>
<<SYS>启用简化中文表示法。</SYS>视频编码传统上是为服务如视频流传输、视频会议、数字电视等服务开发的。主要目的是启用人类观看编码内容。然而，随着深度神经网络（DNNs）的发展，编码的视频现在越来越被用于自动视频分析，这些分析由机器完成。在应用中，自动交通监测等场景中，机器会不断地进行视频分析，而人类可能会periodically审查可能的事件。为支持这些应用，我们需要一个新的视频编码 paradigma，以便高效地表示和压缩视频，以便机器和人类都可以使用。在这篇论文中，我们介绍了首个可学习的视频编码系统，该系统的基层支持机器视觉任务，而其增强层支持人类视觉。我们根据条件编码的概念设计了该系统，以实现更好的压缩减少。我们在四个标准视频集上进行了广泛的实验评估，结果表明，我们的框架在基层上比现有的学习视频编码和传统视频编码更高效，而且在人类视觉任务中保持相对的稳定性。我们将在www.github.com上提供该系统的实现，请等待审核过程结束。
</details></li>
</ul>
<hr>
<h2 id="Deep-Physics-Guided-Unrolling-Generalization-for-Compressed-Sensing"><a href="#Deep-Physics-Guided-Unrolling-Generalization-for-Compressed-Sensing" class="headerlink" title="Deep Physics-Guided Unrolling Generalization for Compressed Sensing"></a>Deep Physics-Guided Unrolling Generalization for Compressed Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08950">http://arxiv.org/abs/2307.08950</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/guaishou74851/prl">https://github.com/guaishou74851/prl</a></li>
<li>paper_authors: Bin Chen, Jiechong Song, Jingfen Xie, Jian Zhang</li>
<li>for: 这篇论文旨在提出一种新的深度学习方法，用于解决图像压缩感知问题。</li>
<li>methods: 该方法基于深度学习的核心思想，通过将传统的迭代回归模型扩展到高维特征空间，以提高网络的容量和实时预测速度。</li>
<li>results: 实验表明，提出的PRL网络比其他现有方法表现出更高的性能和效率，并且具有大量的可进一步改进和实际应用于其他反向图像问题或优化模型。<details>
<summary>Abstract</summary>
By absorbing the merits of both the model- and data-driven methods, deep physics-engaged learning scheme achieves high-accuracy and interpretable image reconstruction. It has attracted growing attention and become the mainstream for inverse imaging tasks. Focusing on the image compressed sensing (CS) problem, we find the intrinsic defect of this emerging paradigm, widely implemented by deep algorithm-unrolled networks, in which more plain iterations involving real physics will bring enormous computation cost and long inference time, hindering their practical application. A novel deep $\textbf{P}$hysics-guided un$\textbf{R}$olled recovery $\textbf{L}$earning ($\textbf{PRL}$) framework is proposed by generalizing the traditional iterative recovery model from image domain (ID) to the high-dimensional feature domain (FD). A compact multiscale unrolling architecture is then developed to enhance the network capacity and keep real-time inference speeds. Taking two different perspectives of optimization and range-nullspace decomposition, instead of building an algorithm-specific unrolled network, we provide two implementations: $\textbf{PRL-PGD}$ and $\textbf{PRL-RND}$. Experiments exhibit the significant performance and efficiency leading of PRL networks over other state-of-the-art methods with a large potential for further improvement and real application to other inverse imaging problems or optimization models.
</details>
<details>
<summary>摘要</summary>
By absorbing the advantages of both model-driven and data-driven methods, the deep physics-engaged learning scheme achieves high accuracy and interpretable image reconstruction, and has attracted growing attention and become the mainstream for inverse imaging tasks. However, the image compressed sensing (CS) problem, which is widely implemented by deep algorithm-unrolled networks, has an inherent defect: more plain iterations involving real physics will lead to enormous computation cost and long inference time, hindering their practical application.To address this issue, a novel deep $\textbf{P}$hysics-guided un$\textbf{R}$olled recovery $\textbf{L}$earning ($\textbf{PRL}$) framework is proposed, which generalizes the traditional iterative recovery model from the image domain (ID) to the high-dimensional feature domain (FD). Additionally, a compact multiscale unrolling architecture is developed to enhance the network capacity and maintain real-time inference speeds.Two implementations of the PRL framework are provided: $\textbf{PRL-PGD}$ and $\textbf{PRL-RND}$, which use two different perspectives of optimization and range-nullspace decomposition. Experimental results show that the PRL networks significantly outperform other state-of-the-art methods, with a large potential for further improvement and real application to other inverse imaging problems or optimization models.
</details></li>
</ul>
<hr>
<h2 id="Experimental-Security-Analysis-of-DNN-based-Adaptive-Cruise-Control-under-Context-Aware-Perception-Attacks"><a href="#Experimental-Security-Analysis-of-DNN-based-Adaptive-Cruise-Control-under-Context-Aware-Perception-Attacks" class="headerlink" title="Experimental Security Analysis of DNN-based Adaptive Cruise Control under Context-Aware Perception Attacks"></a>Experimental Security Analysis of DNN-based Adaptive Cruise Control under Context-Aware Perception Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08939">http://arxiv.org/abs/2307.08939</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xugui Zhou, Anqi Chen, Maxfield Kouzel, Haotian Ren, Morgan McCarty, Cristina Nita-Rotaru, Homa Alemzadeh<br>for: 评估深度神经网络（DNN）基于自适应播速控制系统（ACC）的安全性，以防止恶意投入摄像头数据，引起前方碰撞。methods: 提出了一种结合知识驱动和数据驱动的方法，选择最重要的时刻进行攻击，并在运行时使用优化算法生成适应性的图像干扰。results: 实验结果表明，提posed攻击可以在实际驾驶数据集和真实 simulate平台上 достиieves 142.9倍高的成功率，并被89.6%的安全特性减弱，同时具有适应性和robustness。这种攻击可以考虑到Operator的干预和基本安全特性，并提供了防御攻击的策略。<details>
<summary>Abstract</summary>
Adaptive Cruise Control (ACC) is a widely used driver assistance feature for maintaining desired speed and safe distance to the leading vehicles. This paper evaluates the security of the deep neural network (DNN) based ACC systems under stealthy perception attacks that strategically inject perturbations into camera data to cause forward collisions. We present a combined knowledge-and-data-driven approach to design a context-aware strategy for the selection of the most critical times for triggering the attacks and a novel optimization-based method for the adaptive generation of image perturbations at run-time. We evaluate the effectiveness of the proposed attack using an actual driving dataset and a realistic simulation platform with the control software from a production ACC system and a physical-world driving simulator while considering interventions by the driver and safety features such as Automatic Emergency Braking (AEB) and Forward Collision Warning (FCW). Experimental results show that the proposed attack achieves 142.9x higher success rate in causing accidents than random attacks and is mitigated 89.6% less by the safety features while being stealthy and robust to real-world factors and dynamic changes in the environment. This study provides insights into the role of human operators and basic safety interventions in preventing attacks.
</details>
<details>
<summary>摘要</summary>
这篇研究文章评估了基于深度神经网络（DNN）的自适应速度控制（ACC）系统的安全性，在潜在攻击下维持预期的速度和安全距离。我们提出了一种结合知识驱动和数据驱动的方法，以选择最重要的时刻进行攻击，并使用一种基于优化的方法生成runtime的图像扰动。我们使用实际驾驶数据和一个真实的游戏平台，包括生产ACC系统的控制软件和物理世界驾驶 simulator，评估了我们的攻击效果。实验结果显示，我们的攻击可以导致事故的成功率比随机攻击高出142.9倍，并且受到安全功能如自动紧急刹车（AEB）和前方冲撞警示（FCW）的抑制89.6%。这篇研究提供了人类操作员和基本安全功能的抗攻击策略，并探讨了环境的动态变化和实际因素的影响。
</details></li>
</ul>
<hr>
<h2 id="CSSL-RHA-Contrastive-Self-Supervised-Learning-for-Robust-Handwriting-Authentication"><a href="#CSSL-RHA-Contrastive-Self-Supervised-Learning-for-Robust-Handwriting-Authentication" class="headerlink" title="CSSL-RHA: Contrastive Self-Supervised Learning for Robust Handwriting Authentication"></a>CSSL-RHA: Contrastive Self-Supervised Learning for Robust Handwriting Authentication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11100">http://arxiv.org/abs/2307.11100</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingyao Wang, Luntian Mou, Changwen Zheng, Wen Gao</li>
<li>for: 防止诈骗和文化遗产保护等领域中的手写认证任务</li>
<li>methods: 提出了一种基于自我超vised学习的Contrastive Self-Supervised Learning框架（CSSL-RHA），可以学习复杂但重要的特征，并准确地预测作者标识</li>
<li>results: 对五个基准数据集和自行标注的EN-HA数据集进行了广泛的实验，证明了我们的CSSL-RHA在基础线上性能较高，并且在异常情况下（如数据forge和损坏）仍然能够有效地进行认证<details>
<summary>Abstract</summary>
Handwriting authentication is a valuable tool used in various fields, such as fraud prevention and cultural heritage protection. However, it remains a challenging task due to the complex features, severe damage, and lack of supervision. In this paper, we propose a novel Contrastive Self-Supervised Learning framework for Robust Handwriting Authentication (CSSL-RHA) to address these issues. It can dynamically learn complex yet important features and accurately predict writer identities. Specifically, to remove the negative effects of imperfections and redundancy, we design an information-theoretic filter for pre-processing and propose a novel adaptive matching scheme to represent images as patches of local regions dominated by more important features. Through online optimization at inference time, the most informative patch embeddings are identified as the "most important" elements. Furthermore, we employ contrastive self-supervised training with a momentum-based paradigm to learn more general statistical structures of handwritten data without supervision. We conduct extensive experiments on five benchmark datasets and our manually annotated dataset EN-HA, which demonstrate the superiority of our CSSL-RHA compared to baselines. Additionally, we show that our proposed model can still effectively achieve authentication even under abnormal circumstances, such as data falsification and corruption.
</details>
<details>
<summary>摘要</summary>
《手写文本认证：一种值得信赖的工具》手写文本认证是多个领域中的一种重要工具，用于防止 fraud 和保护文化遗产。然而，由于手写文本的复杂特征、严重损害以及缺乏监督，这是一项挑战性的任务。在这篇论文中，我们提出了一种新的 Contrastive Self-Supervised Learning 框架，用于 Robust Handwriting Authentication (CSSL-RHA)。该框架可以动态学习复杂的重要特征，并准确地预测作者标识。为了解决负面影响和重复性的问题，我们设计了一种信息论Filter 来预处理图像，并提出了一种新的自适应匹配方案，用于将图像转换为具有更重要特征的 patches 的地方区域。通过在推理时进行在线优化，我们可以快速地identify 最有用的 patch embeddings 作为 "最重要" 的元素。此外，我们采用了一种带有摘要的自我超vision 训练方法，以学习不监督的手写数据的更加通用的统计结构。我们在五个 benchmark 数据集和我们手动标注的 EN-HA 数据集上进行了广泛的实验，并证明了我们的 CSSL-RHA 与基线相比更高效。此外，我们还证明了我们的提案的模型可以在不正常的情况下，如数据伪造和损害，仍然有效地进行身份验证。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Sample-Tasks-for-Meta-Learning"><a href="#Learning-to-Sample-Tasks-for-Meta-Learning" class="headerlink" title="Learning to Sample Tasks for Meta Learning"></a>Learning to Sample Tasks for Meta Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08924">http://arxiv.org/abs/2307.08924</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ZJLAB-AMMI/HS-OMRL">https://github.com/ZJLAB-AMMI/HS-OMRL</a></li>
<li>paper_authors: Jingyao Wang, Zeen Song, Xingzhe Su, Lingyu Si, Hongwei Dong, Wenwen Qiang, Changwen Zheng</li>
<li>for: 通过对各种元学习方法、任务采样器和几个少shot学习任务进行实验，这篇论文得出了三个结论。</li>
<li>methods: 第一，没有一个通用的任务采样策略可以保证元学习模型的表现。第二，任务多样性会导致模型在训练过程中 Either underfit 或 overfit。第三，模型的总结表现受到任务分化、任务熵和任务难度的影响。</li>
<li>results: 作为回应，我们提出了一种新的任务采样器called Adaptive Sampler (ASr)。ASr是一个可插入的任务采样器，它根据任务分化、任务熵和任务难度来采样任务。为了优化ASr，我们提出了一种简单的通用元学习算法。最后，论文通过大量实验证明了提议的ASr的有效性。<details>
<summary>Abstract</summary>
Through experiments on various meta-learning methods, task samplers, and few-shot learning tasks, this paper arrives at three conclusions. Firstly, there are no universal task sampling strategies to guarantee the performance of meta-learning models. Secondly, task diversity can cause the models to either underfit or overfit during training. Lastly, the generalization performance of the models are influenced by task divergence, task entropy, and task difficulty. In response to these findings, we propose a novel task sampler called Adaptive Sampler (ASr). ASr is a plug-and-play task sampler that takes task divergence, task entropy, and task difficulty to sample tasks. To optimize ASr, we rethink and propose a simple and general meta-learning algorithm. Finally, a large number of empirical experiments demonstrate the effectiveness of the proposed ASr.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>There is no universal task sampling strategy that can guarantee the performance of meta-learning models.2. Task diversity can cause the models to either underfit or overfit during training.3. The generalization performance of the models is influenced by task divergence, task entropy, and task difficulty.In response to these findings, we propose a novel task sampler called Adaptive Sampler (ASr). ASr is a plug-and-play task sampler that takes task divergence, task entropy, and task difficulty into consideration when sampling tasks. To optimize ASr, we propose a simple and general meta-learning algorithm.Empirical experiments on a large number of tasks demonstrate the effectiveness of the proposed ASr.</details></li>
</ol>
<hr>
<h2 id="Accuracy-versus-time-frontiers-of-semi-supervised-and-self-supervised-learning-on-medical-images"><a href="#Accuracy-versus-time-frontiers-of-semi-supervised-and-self-supervised-learning-on-medical-images" class="headerlink" title="Accuracy versus time frontiers of semi-supervised and self-supervised learning on medical images"></a>Accuracy versus time frontiers of semi-supervised and self-supervised learning on medical images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08919">http://arxiv.org/abs/2307.08919</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tufts-ml/ssl-vs-ssl-benchmark">https://github.com/tufts-ml/ssl-vs-ssl-benchmark</a></li>
<li>paper_authors: Zhe Huang, Ruijie Jiang, Shuchin Aeron, Michael C. Hughes</li>
<li>for: 本研究的目的是为了提供一个可靠的benchmark，帮助实践者在有限的标签数据和训练时间的情况下，尽可能提高分类器的性能。</li>
<li>methods: 本研究使用了两种主要的研究方向：自监学习和semi-supervised learning。自监学习先在无标签数据上培养有用的表示，然后使用标签数据进行细化；semi-supervised learning直接使用标签和无标签数据同时训练分类器。</li>
<li>results: 本研究的结果表明，使用additional的无标签数据可以提高分类器的性能，并且使用MixMatch、SimCLR和BYOL等方法可以获得最佳性能。在3个医学图像集合上，6种semi-supervised方法和5种自监学习方法与强的标签只方法进行比较，并提供了在新的医学任务上提高性能的设置。<details>
<summary>Abstract</summary>
For many applications of classifiers to medical images, a trustworthy label for each image can be difficult or expensive to obtain. In contrast, images without labels are more readily available. Two major research directions both promise that additional unlabeled data can improve classifier performance: self-supervised learning pretrains useful representations on unlabeled data only, then fine-tunes a classifier on these representations via the labeled set; semi-supervised learning directly trains a classifier on labeled and unlabeled data simultaneously. Recent methods from both directions have claimed significant gains on non-medical tasks, but do not systematically assess medical images and mostly compare only to methods in the same direction. This study contributes a carefully-designed benchmark to help answer a practitioner's key question: given a small labeled dataset and a limited budget of hours to spend on training, what gains from additional unlabeled images are possible and which methods best achieve them? Unlike previous benchmarks, ours uses realistic-sized validation sets to select hyperparameters, assesses runtime-performance tradeoffs, and bridges two research fields. By comparing 6 semi-supervised methods and 5 self-supervised methods to strong labeled-only baselines on 3 medical datasets with 30-1000 labels per class, we offer insights to resource-constrained, results-focused practitioners: MixMatch, SimCLR, and BYOL represent strong choices that were not surpassed by more recent methods. After much effort selecting hyperparameters on one dataset, we publish settings that enable strong methods to perform well on new medical tasks within a few hours, with further search over dozens of hours delivering modest additional gains.
</details>
<details>
<summary>摘要</summary>
For many medical image classification applications, obtaining trustworthy labels for each image can be difficult or expensive. In contrast, images without labels are more readily available. Two major research directions promise that additional unlabeled data can improve classifier performance: self-supervised learning pretrains useful representations on unlabeled data only, then fine-tunes a classifier on these representations via the labeled set; semi-supervised learning directly trains a classifier on labeled and unlabeled data simultaneously. Recent methods from both directions have claimed significant gains on non-medical tasks, but do not systematically assess medical images and mostly compare only to methods in the same direction. This study contributes a carefully-designed benchmark to help answer a practitioner's key question: given a small labeled dataset and a limited budget of hours to spend on training, what gains from additional unlabeled images are possible and which methods best achieve them? Unlike previous benchmarks, ours uses realistic-sized validation sets to select hyperparameters, assesses runtime-performance tradeoffs, and bridges two research fields. By comparing 6 semi-supervised methods and 5 self-supervised methods to strong labeled-only baselines on 3 medical datasets with 30-1000 labels per class, we offer insights to resource-constrained, results-focused practitioners: MixMatch, SimCLR, and BYOL represent strong choices that were not surpassed by more recent methods. After much effort selecting hyperparameters on one dataset, we publish settings that enable strong methods to perform well on new medical tasks within a few hours, with further search over dozens of hours delivering modest additional gains.
</details></li>
</ul>
<hr>
<h2 id="Towards-the-Sparseness-of-Projection-Head-in-Self-Supervised-Learning"><a href="#Towards-the-Sparseness-of-Projection-Head-in-Self-Supervised-Learning" class="headerlink" title="Towards the Sparseness of Projection Head in Self-Supervised Learning"></a>Towards the Sparseness of Projection Head in Self-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08913">http://arxiv.org/abs/2307.08913</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeen Song, Xingzhe Su, Jingyao Wang, Wenwen Qiang, Changwen Zheng, Fuchun Sun</li>
<li>for: 本研究旨在探讨自动学习（Self-Supervised Learning，SSL）中的一种成功方法——对比学习，以及对其中的参数化投影头的内部机制和维度归一化现象的研究。</li>
<li>methods: 本研究通过实验分析和理论调查，探讨对比学习中的投影头对 representation 质量的影响，并提出了假设只需要一 subset of features 来最小化对比损失。</li>
<li>results: 实验结果表明，束之 sparse projection head 可以增强对比学习的性能，并且可以轻松地与现有的 SSL 方法结合使用。<details>
<summary>Abstract</summary>
In recent years, self-supervised learning (SSL) has emerged as a promising approach for extracting valuable representations from unlabeled data. One successful SSL method is contrastive learning, which aims to bring positive examples closer while pushing negative examples apart. Many current contrastive learning approaches utilize a parameterized projection head. Through a combination of empirical analysis and theoretical investigation, we provide insights into the internal mechanisms of the projection head and its relationship with the phenomenon of dimensional collapse. Our findings demonstrate that the projection head enhances the quality of representations by performing contrastive loss in a projected subspace. Therefore, we propose an assumption that only a subset of features is necessary when minimizing the contrastive loss of a mini-batch of data. Theoretical analysis further suggests that a sparse projection head can enhance generalization, leading us to introduce SparseHead - a regularization term that effectively constrains the sparsity of the projection head, and can be seamlessly integrated with any self-supervised learning (SSL) approaches. Our experimental results validate the effectiveness of SparseHead, demonstrating its ability to improve the performance of existing contrastive methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="What-Can-Simple-Arithmetic-Operations-Do-for-Temporal-Modeling"><a href="#What-Can-Simple-Arithmetic-Operations-Do-for-Temporal-Modeling" class="headerlink" title="What Can Simple Arithmetic Operations Do for Temporal Modeling?"></a>What Can Simple Arithmetic Operations Do for Temporal Modeling?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08908">http://arxiv.org/abs/2307.08908</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/whwu95/ATM">https://github.com/whwu95/ATM</a></li>
<li>paper_authors: Wenhao Wu, Yuxin Song, Zhun Sun, Jingdong Wang, Chang Xu, Wanli Ouyang</li>
<li>for: 本研究旨在探讨视频内容中的时间模型化问题，采用简单的四则数学操作来建立时间关系。</li>
<li>methods: 我们首先从视频帧特征中提取auxiliary时间cue，并使用加减乘除四则数学操作来提取相关特征。然后，我们将这些特征与原始的时间不关注域进行对比，以便进一步提高视频识别性能。我们称之为Arithmetic Temporal Module（ATM），它可以与CNNs和ViTs两种不同的架构结合使用。</li>
<li>results: 我们在Something-Something V1、V2和Kinetics-400等视频识别Benchmark上进行了广泛的ablation研究，并证明ATM模块可以在低计算成本下提供强大的时间模型化能力。此外，ATM模块可以与不同的架构结合使用，并在 Something-Something V1、V2和Kinetics-400等Benchmark上达到了65.6%、74.6%和89.4%的top-1准确率。代码可以在<a target="_blank" rel="noopener" href="https://github.com/whwu95/ATM%E4%B8%AD%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/whwu95/ATM中下载。</a><details>
<summary>Abstract</summary>
Temporal modeling plays a crucial role in understanding video content. To tackle this problem, previous studies built complicated temporal relations through time sequence thanks to the development of computationally powerful devices. In this work, we explore the potential of four simple arithmetic operations for temporal modeling. Specifically, we first capture auxiliary temporal cues by computing addition, subtraction, multiplication, and division between pairs of extracted frame features. Then, we extract corresponding features from these cues to benefit the original temporal-irrespective domain. We term such a simple pipeline as an Arithmetic Temporal Module (ATM), which operates on the stem of a visual backbone with a plug-and-play style. We conduct comprehensive ablation studies on the instantiation of ATMs and demonstrate that this module provides powerful temporal modeling capability at a low computational cost. Moreover, the ATM is compatible with both CNNs- and ViTs-based architectures. Our results show that ATM achieves superior performance over several popular video benchmarks. Specifically, on Something-Something V1, V2 and Kinetics-400, we reach top-1 accuracy of 65.6%, 74.6%, and 89.4% respectively. The code is available at https://github.com/whwu95/ATM.
</details>
<details>
<summary>摘要</summary>
temporald modeling plays a crucial role in understanding video content. To tackle this problem, previous studies built complicated temporal relations through time sequence thanks to the development of computationally powerful devices. In this work, we explore the potential of four simple arithmetic operations for temporal modeling. Specifically, we first capture auxiliary temporal cues by computing addition, subtraction, multiplication, and division between pairs of extracted frame features. Then, we extract corresponding features from these cues to benefit the original temporal-irrelevant domain. We term such a simple pipeline as an Arithmetic Temporal Module (ATM), which operates on the stem of a visual backbone with a plug-and-play style. We conduct comprehensive ablation studies on the instantiation of ATMs and demonstrate that this module provides powerful temporal modeling capability at a low computational cost. Moreover, the ATM is compatible with both CNNs- and ViTs-based architectures. Our results show that ATM achieves superior performance over several popular video benchmarks. Specifically, on Something-Something V1, V2 and Kinetics-400, we reach top-1 accuracy of 65.6%, 74.6%, and 89.4% respectively. The code is available at https://github.com/whwu95/ATM.Here's the translation in Traditional Chinese:时间模型化在视频内容理解中扮演重要角色。以往的研究通过时间序列建立复杂的时间关系，感谢computationally powerful devices的发展。在这个工作中，我们探索四个简单的算术操作的潜力 для时间模型化。具体来说，我们首先 capture auxiliary temporal cues by computing addition, subtraction, multiplication, and division between pairs of extracted frame features。然后，我们从这些cue中提取相应的特征，以帮助原始时间不适用的领域。我们给这个简单管道命名为Arithmetic Temporal Module (ATM)，它在视觉背bone上运作，并且具有plug-and-play的风格。我们进行了广泛的ablation study，证明这个模组提供了强大的时间模型化能力，同时computational cost较低。此外，ATM适用于CNNs-和ViTs-based architecture。我们的结果显示，ATM在Something-Something V1, V2和Kinetics-400上达到了top-1准确率的65.6%, 74.6%, 和89.4%。代码可以在https://github.com/whwu95/ATM中找到。
</details></li>
</ul>
<hr>
<h2 id="Modular-Neural-Network-Approaches-for-Surgical-Image-Recognition"><a href="#Modular-Neural-Network-Approaches-for-Surgical-Image-Recognition" class="headerlink" title="Modular Neural Network Approaches for Surgical Image Recognition"></a>Modular Neural Network Approaches for Surgical Image Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08880">http://arxiv.org/abs/2307.08880</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nosseiba Ben Salem, Younes Bennani, Joseph Karkazan, Abir Barbara, Charles Dacheux, Thomas Gregory</li>
<li>for: 这个论文的目的是提出一种基于深度学习的模块学习方法，用于解决DCSS不稳定性分类问题。</li>
<li>methods: 这个论文使用了自我训练和模块学习方法来解决DCSS不稳定性分类问题。</li>
<li>results: 实验结果显示，模块学习方法可以提高分类性能，而Weighted Modular方法达到了几乎完美的分类效果。<details>
<summary>Abstract</summary>
Deep learning-based applications have seen a lot of success in recent years. Text, audio, image, and video have all been explored with great success using deep learning approaches. The use of convolutional neural networks (CNN) in computer vision, in particular, has yielded reliable results. In order to achieve these results, a large amount of data is required. However, the dataset cannot always be accessible. Moreover, annotating data can be difficult and time-consuming. Self-training is a semi-supervised approach that managed to alleviate this problem and achieve state-of-the-art performances. Theoretical analysis even proved that it may result in a better generalization than a normal classifier. Another problem neural networks can face is the increasing complexity of modern problems, requiring a high computational and storage cost. One way to mitigate this issue, a strategy that has been inspired by human cognition known as modular learning, can be employed. The principle of the approach is to decompose a complex problem into simpler sub-tasks. This approach has several advantages, including faster learning, better generalization, and enables interpretability.   In the first part of this paper, we introduce and evaluate different architectures of modular learning for Dorsal Capsulo-Scapholunate Septum (DCSS) instability classification. Our experiments have shown that modular learning improves performances compared to non-modular systems. Moreover, we found that weighted modular, that is to weight the output using the probabilities from the gating module, achieved an almost perfect classification. In the second part, we present our approach for data labeling and segmentation with self-training applied on shoulder arthroscopy images.
</details>
<details>
<summary>摘要</summary>
深度学习基本应用在最近几年内得到了很大成功。文本、音频、图像和视频都被使用深度学习方法进行了成功的探索。特别是计算机视觉领域中使用卷积神经网络（CNN）的应用，已经取得了可靠的结果。但是，获取数据往往是一个大问题，因为数据集经常不可 accessible。此外，对数据进行标注也可能是一项困难和时间consuming的任务。自我帮助是一种半监督的方法，可以解决这个问题，并达到状态 искусственный智能的性能。然而，神经网络还面临着现代问题的增长复杂度，需要大量的计算和存储资源。为了解决这个问题，我们可以采用一种人类认知的灵感---模块学习的方法。这种方法的原则是将复杂问题分解成更简单的子任务。这种方法有很多优点，包括更快的学习速度、更好的泛化性和可读性。在本文中，我们首先介绍和评估不同的模块学习架构在DCSS不稳定分类问题上。我们的实验表明，模块学习可以提高性能，并且weighted模块可以达到几乎完美的分类结果。在第二部分，我们介绍了我们的自动标注和分割方法，使用自我帮助在Shoulder镜像上进行应用。
</details></li>
</ul>
<hr>
<h2 id="LiDAR-BEVMTN-Real-Time-LiDAR-Bird’s-Eye-View-Multi-Task-Perception-Network-for-Autonomous-Driving"><a href="#LiDAR-BEVMTN-Real-Time-LiDAR-Bird’s-Eye-View-Multi-Task-Perception-Network-for-Autonomous-Driving" class="headerlink" title="LiDAR-BEVMTN: Real-Time LiDAR Bird’s-Eye View Multi-Task Perception Network for Autonomous Driving"></a>LiDAR-BEVMTN: Real-Time LiDAR Bird’s-Eye View Multi-Task Perception Network for Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08850">http://arxiv.org/abs/2307.08850</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sambit Mohapatra, Senthil Yogamani, Varun Ravi Kumar, Stefan Milz, Heinrich Gotzig, Patrick Mäder</li>
<li>for: 这篇论文旨在提出一种实时多任务深度学习网络，用于自动驾驶中的3D场景识别。</li>
<li>methods: 该方法使用了一种共享encoder和任务特定decoder的架构，实现了 joint representation learning。还提出了一种新的Semantic Weighting and Guidance（SWAG）模块，以提高对象检测的准确性。</li>
<li>results: 该方法在NVIDIA Xavier平台上实现了3ms的延迟时间，并在两个任务中达到了状态的较好的表现（semantic segmentation和动作分割），并且在3D对象检测任务中达到了状态的较好的表现（仅次于状态之最佳表现）。<details>
<summary>Abstract</summary>
LiDAR is crucial for robust 3D scene perception in autonomous driving. LiDAR perception has the largest body of literature after camera perception. However, multi-task learning across tasks like detection, segmentation, and motion estimation using LiDAR remains relatively unexplored, especially on automotive-grade embedded platforms. We present a real-time multi-task convolutional neural network for LiDAR-based object detection, semantics, and motion segmentation. The unified architecture comprises a shared encoder and task-specific decoders, enabling joint representation learning. We propose a novel Semantic Weighting and Guidance (SWAG) module to transfer semantic features for improved object detection selectively. Our heterogeneous training scheme combines diverse datasets and exploits complementary cues between tasks. The work provides the first embedded implementation unifying these key perception tasks from LiDAR point clouds achieving 3ms latency on the embedded NVIDIA Xavier platform. We achieve state-of-the-art results for two tasks, semantic and motion segmentation, and close to state-of-the-art performance for 3D object detection. By maximizing hardware efficiency and leveraging multi-task synergies, our method delivers an accurate and efficient solution tailored for real-world automated driving deployment. Qualitative results can be seen at https://youtu.be/H-hWRzv2lIY.
</details>
<details>
<summary>摘要</summary>
“LiDAR 是自动驾驶中Robust 3D 场景识别的关键技术。LiDAR 识别有大量文献，仅次于摄像头识别。然而，使用 LiDAR 进行多任务学习，特别是在汽车级别的嵌入式平台上，尚未得到充分的研究。我们提出了一种实时多任务卷积神经网络，用于基于 LiDAR 的 объек特殊、类别和运动分割。该架构包括共享Encoder和任务特定的Decoder，允许 JOINT 表征学习。我们提出了一种新的Semantic Weighting and Guidance（SWAG）模块，用于 selectively 提高对象检测的准确性。我们的不同数据集合并利用了各种任务之间的补做作用。我们的方法在 NVIDIA Xavier 平台上实现了3ms 延迟，并实现了对 semantic 和运动分割两个任务的state-of-the-art 性能，以及对 3D 对象检测的近似性性能。通过最大化硬件效率和多任务 synergies，我们的方法提供了一个准确和高效的解决方案，适用于实际自动驾驶部署。详细结果可以参考 <https://youtu.be/H-hWRzv2lIY>。”
</details></li>
</ul>
<hr>
<h2 id="DARTS-Double-Attention-Reference-based-Transformer-for-Super-resolution"><a href="#DARTS-Double-Attention-Reference-based-Transformer-for-Super-resolution" class="headerlink" title="DARTS: Double Attention Reference-based Transformer for Super-resolution"></a>DARTS: Double Attention Reference-based Transformer for Super-resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08837">http://arxiv.org/abs/2307.08837</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bia006/darts">https://github.com/bia006/darts</a></li>
<li>paper_authors: Masoomeh Aslahishahri, Jordan Ubbens, Ian Stavness</li>
<li>for: 提高低分辨率图像的内容质量</li>
<li>methods: 使用转换器模型，学习对两个图像分布进行 JOINT 表示，通过匹配对应关系学习高分辨率图像来提高低分辨率图像的内容质量</li>
<li>results: 在 SUN80 数据集上达到了状态机器人模型的水平，PSNR&#x2F;SSIM 分别为 29.83 &#x2F; 0.809，表明单独使用注意力机制可以实现参照基于图像超分辨率任务，不需要多种特殊设计Sub网络、知识储存或多Stage训练。<details>
<summary>Abstract</summary>
We present DARTS, a transformer model for reference-based image super-resolution. DARTS learns joint representations of two image distributions to enhance the content of low-resolution input images through matching correspondences learned from high-resolution reference images. Current state-of-the-art techniques in reference-based image super-resolution are based on a multi-network, multi-stage architecture. In this work, we adapt the double attention block from the GAN literature, processing the two visual streams separately and combining self-attention and cross-attention blocks through a gating attention strategy. Our work demonstrates how the attention mechanism can be adapted for the particular requirements of reference-based image super-resolution, significantly simplifying the architecture and training pipeline. We show that our transformer-based model performs competitively with state-of-the-art models, while maintaining a simpler overall architecture and training process. In particular, we obtain state-of-the-art on the SUN80 dataset, with a PSNR/SSIM of 29.83 / .809. These results show that attention alone is sufficient for the RSR task, without multiple purpose-built subnetworks, knowledge distillation, or multi-stage training.
</details>
<details>
<summary>摘要</summary>
我们介绍了DARTS，一种基于 transformer 模型的参照型图像超分辨模型。DARTS 学习了两个图像分布的共同表示，以增强输入图像的内容。当前领导技术在参照型图像超分辨中使用多个网络、多个阶段架构。在这种工作中，我们从 GAN 文献中采用了双注意块，处理两个视觉流 separately，并通过阻塞注意力策略将自注意块和交叉注意块组合在一起。我们的工作表明了注意力机制在参照型图像超分辨中可以进行适应，大大简化架构和训练流程。我们显示了我们的 transformer 基于模型与状态革新模型相当，而且具有更简单的总体架构和训练过程。特别是，我们在 SUN80 数据集上获得了状态革新的 PSNR/SSIM 值为 29.83 / .809。这些结果表明，注意力机制alone 是RSR 任务中的 suficient，不需要多个专门设计的子网络、知识继承或多个阶段训练。
</details></li>
</ul>
<hr>
<h2 id="Harnessing-the-Power-of-AI-based-Image-Generation-Model-DALLE-2-in-Agricultural-Settings"><a href="#Harnessing-the-Power-of-AI-based-Image-Generation-Model-DALLE-2-in-Agricultural-Settings" class="headerlink" title="Harnessing the Power of AI based Image Generation Model DALLE 2 in Agricultural Settings"></a>Harnessing the Power of AI based Image Generation Model DALLE 2 in Agricultural Settings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08789">http://arxiv.org/abs/2307.08789</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ranjan Sapkota</li>
<li>for: 这项研究旨在探讨人工智能（AI）在农业领域视觉进程的提升方面，使用开源AI图像生成器DALLE 2。</li>
<li>methods: 该研究使用了chatGPT的自然语言处理能力和DALLE 2模型，实现了将文本描述器转换为真实的视觉内容的创新方法。</li>
<li>results: 研究发现，使用DALLE 2模型可以提高农业视觉进程的质量和准确性，帮助农业决策更加 Informed，并改善资源分配。结果表明，AI将在精度农业领域产生快速发展。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
This study investigates the potential impact of artificial intelligence (AI) on the enhancement of visualization processes in the agricultural sector, using the advanced AI image generator, DALLE 2, developed by OpenAI. By synergistically utilizing the natural language processing proficiency of chatGPT and the generative prowess of the DALLE 2 model, which employs a Generative Adversarial Networks (GANs) framework, our research offers an innovative method to transform textual descriptors into realistic visual content. Our rigorously assembled datasets include a broad spectrum of agricultural elements such as fruits, plants, and scenarios differentiating crops from weeds, maintained for AI-generated versus original images. The quality and accuracy of the AI-generated images were evaluated via established metrics including mean squared error (MSE), peak signal-to-noise ratio (PSNR), and feature similarity index (FSIM). The results underline the significant role of the DALLE 2 model in enhancing visualization processes in agriculture, aiding in more informed decision-making, and improving resource distribution. The outcomes of this research highlight the imminent rise of an AI-led transformation in the realm of precision agriculture.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="The-FathomNet2023-Competition-Dataset"><a href="#The-FathomNet2023-Competition-Dataset" class="headerlink" title="The FathomNet2023 Competition Dataset"></a>The FathomNet2023 Competition Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08781">http://arxiv.org/abs/2307.08781</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fathomnet/fgvc-comp-2023">https://github.com/fathomnet/fgvc-comp-2023</a></li>
<li>paper_authors: Eric Orenstein, Kevin Barnard, Lonny Lundsten, Geneviève Patterson, Benjamin Woodward, Kakani Katija</li>
<li>for: study marine organisms and environmental monitoring</li>
<li>methods: automatic processing of visual data</li>
<li>results: recognition of new organisms and assessment of out-of-sample data<details>
<summary>Abstract</summary>
Ocean scientists have been collecting visual data to study marine organisms for decades. These images and videos are extremely valuable both for basic science and environmental monitoring tasks. There are tools for automatically processing these data, but none that are capable of handling the extreme variability in sample populations, image quality, and habitat characteristics that are common in visual sampling of the ocean. Such distribution shifts can occur over very short physical distances and in narrow time windows. Creating models that are able to recognize when an image or video sequence contains a new organism, an unusual collection of animals, or is otherwise out-of-sample is critical to fully leverage visual data in the ocean. The FathomNet2023 competition dataset presents a realistic scenario where the set of animals in the target data differs from the training data. The challenge is both to identify the organisms in a target image and assess whether it is out-of-sample.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:海洋科学家已经在数十年内收集视频数据来研究海洋生物。这些图像和视频非常有价值，不仅为基础科学研究，还为环境监测任务。然而，存在一些工具可以自动处理这些数据，但是无法处理海洋视频样本中的极大变化，包括样本人口、图像质量和生物群体特征等。这些变化可能在非常短的物理距离和时间窗口内发生。创建能够识别目标图像中的新生物、不寻常的动物群体或者是否外样的模型是海洋视频数据的核心。FathomNet2023比赛数据集提供了一个真实的enario，其中目标数据中的生物集合与训练数据不同。挑战是both识别目标图像中的生物和判断图像是否外样。
</details></li>
</ul>
<hr>
<h2 id="Similarity-Min-Max-Zero-Shot-Day-Night-Domain-Adaptation"><a href="#Similarity-Min-Max-Zero-Shot-Day-Night-Domain-Adaptation" class="headerlink" title="Similarity Min-Max: Zero-Shot Day-Night Domain Adaptation"></a>Similarity Min-Max: Zero-Shot Day-Night Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08779">http://arxiv.org/abs/2307.08779</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Red-Fairy/ZeroShotDayNightDA">https://github.com/Red-Fairy/ZeroShotDayNightDA</a></li>
<li>paper_authors: Rundong Luo, Wenjing Wang, Wenhan Yang, Jiaying Liu</li>
<li>for: 这篇论文旨在解决黑暗环境下影像识别和分类等夜间视觉任务中的模型性能降低问题。</li>
<li>methods: 本论文提出了一个统一的架构，协助实现零数据黑暗领域适应。它首先使用黑暗图像来减少特征相似性，然后将模型适应到黑暗图像和正常照明图像之间的特征相似性。</li>
<li>results: 实验结果显示，本方法可以优化模型的通用化能力，并在不同的夜间视觉任务中实现显著的改善。包括分类、 semantic segmentation、visual place recognition和video action recognition等多种夜间视觉任务都能够得到良好的表现。<details>
<summary>Abstract</summary>
Low-light conditions not only hamper human visual experience but also degrade the model's performance on downstream vision tasks. While existing works make remarkable progress on day-night domain adaptation, they rely heavily on domain knowledge derived from the task-specific nighttime dataset. This paper challenges a more complicated scenario with border applicability, i.e., zero-shot day-night domain adaptation, which eliminates reliance on any nighttime data. Unlike prior zero-shot adaptation approaches emphasizing either image-level translation or model-level adaptation, we propose a similarity min-max paradigm that considers them under a unified framework. On the image level, we darken images towards minimum feature similarity to enlarge the domain gap. Then on the model level, we maximize the feature similarity between the darkened images and their normal-light counterparts for better model adaptation. To the best of our knowledge, this work represents the pioneering effort in jointly optimizing both aspects, resulting in a significant improvement of model generalizability. Extensive experiments demonstrate our method's effectiveness and broad applicability on various nighttime vision tasks, including classification, semantic segmentation, visual place recognition, and video action recognition. Code and pre-trained models are available at https://red-fairy.github.io/ZeroShotDayNightDA-Webpage/.
</details>
<details>
<summary>摘要</summary>
低光照条件不仅影响人类视觉经验，还会下降模型在下游视觉任务中的性能。现有的工作做出了显著的进步在日夜域适应中，但是它们依赖于任务特定的夜间数据集的知识。本文挑战了更复杂的场景，即零shot日夜域适应，即无需夜间数据集来适应。与先前的零shot适应方法不同，我们提出了一种相似度最大化思想，它考虑了图像级和模型级的适应。在图像级别，我们使用最小特征相似性来抑制图像，以扩大域之间的差距。然后，在模型级别，我们使用最大化特征相似性来进行模型适应。根据我们所知，这种方法是首次同时优化图像和模型级别的适应，从而提高模型的通用性。我们的方法在不同的夜视任务中，包括分类、 semantic segmentation、视觉地标识和视频动作识别等，都有广泛的应用和实验证明了其效果。代码和预训练模型可以在 <https://red-fairy.github.io/ZeroShotDayNightDA-Webpage/> 中下载。
</details></li>
</ul>
<hr>
<h2 id="UPSCALE-Unconstrained-Channel-Pruning"><a href="#UPSCALE-Unconstrained-Channel-Pruning" class="headerlink" title="UPSCALE: Unconstrained Channel Pruning"></a>UPSCALE: Unconstrained Channel Pruning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08771">http://arxiv.org/abs/2307.08771</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/apple/ml-upscale">https://github.com/apple/ml-upscale</a></li>
<li>paper_authors: Alvin Wan, Hanxiang Hao, Kaushik Patnaik, Yueyang Xu, Omer Hadad, David Güera, Zhile Ren, Qi Shan</li>
<li>for: 降低卷积神经网络的执行速度，通过频道剪裁来提高模型的压缩率。</li>
<li>methods: 使用频道剪裁技术，但是对多个分支段的模型来说，频道剪裁可能会导致执行时间复制。为了解决这个问题，通常是将某些频道约束在一起，以完全消除执行时间复制，但是这会导致减少模型的准确性。</li>
<li>results: 根据我们的发现，可以在出口时重新排序频道，以降低执行时间复制并提高模型的准确性。我们提出的普适算法 UPSCALE 可以适应任何剪裁模式，并在 ImageNet 上提高后训练剪裁模型的平均准确性 by 2.1 点。此外，UPSCALE 还可以提高执行速度，相比基eline export 可以达到两倍的提升。<details>
<summary>Abstract</summary>
As neural networks grow in size and complexity, inference speeds decline. To combat this, one of the most effective compression techniques -- channel pruning -- removes channels from weights. However, for multi-branch segments of a model, channel removal can introduce inference-time memory copies. In turn, these copies increase inference latency -- so much so that the pruned model can be slower than the unpruned model. As a workaround, pruners conventionally constrain certain channels to be pruned together. This fully eliminates memory copies but, as we show, significantly impairs accuracy. We now have a dilemma: Remove constraints but increase latency, or add constraints and impair accuracy. In response, our insight is to reorder channels at export time, (1) reducing latency by reducing memory copies and (2) improving accuracy by removing constraints. Using this insight, we design a generic algorithm UPSCALE to prune models with any pruning pattern. By removing constraints from existing pruners, we improve ImageNet accuracy for post-training pruned models by 2.1 points on average -- benefiting DenseNet (+16.9), EfficientNetV2 (+7.9), and ResNet (+6.2). Furthermore, by reordering channels, UPSCALE improves inference speeds by up to 2x over a baseline export.
</details>
<details>
<summary>摘要</summary>
Our insight is to reorder channels at export time to address this issue. By reducing latency by reducing memory copies and improving accuracy by removing constraints, we can achieve better performance. We have designed a generic algorithm called UPSCALE to prune models with any pruning pattern. By removing constraints from existing pruners, we have improved ImageNet accuracy for post-training pruned models by an average of 2.1 points - benefiting DenseNet, EfficientNetV2, and ResNet. Furthermore, by reordering channels, UPSCALE improves inference speeds by up to 2x over a baseline export.
</details></li>
</ul>
<hr>
<h2 id="Video-Mined-Task-Graphs-for-Keystep-Recognition-in-Instructional-Videos"><a href="#Video-Mined-Task-Graphs-for-Keystep-Recognition-in-Instructional-Videos" class="headerlink" title="Video-Mined Task Graphs for Keystep Recognition in Instructional Videos"></a>Video-Mined Task Graphs for Keystep Recognition in Instructional Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08763">http://arxiv.org/abs/2307.08763</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kumar Ashutosh, Santhosh Kumar Ramakrishnan, Triantafyllos Afouras, Kristen Grauman</li>
<li>for: 这篇论文旨在提高人工智能对教程视频中人体动作的理解，以便更好地执行 DIY 修理任务和食谱等。</li>
<li>methods: 论文提出自动从教程视频中挖掘任务图грам，并使用这个图грам来规范化键步认识。</li>
<li>results: 在多个实际教程视频 datasets 上，论文显示了更加可靠的零基础键步定位和改进的视频表示学习，超过了现状势。<details>
<summary>Abstract</summary>
Procedural activity understanding requires perceiving human actions in terms of a broader task, where multiple keysteps are performed in sequence across a long video to reach a final goal state -- such as the steps of a recipe or a DIY fix-it task. Prior work largely treats keystep recognition in isolation of this broader structure, or else rigidly confines keysteps to align with a predefined sequential script. We propose discovering a task graph automatically from how-to videos to represent probabilistically how people tend to execute keysteps, and then leverage this graph to regularize keystep recognition in novel videos. On multiple datasets of real-world instructional videos, we show the impact: more reliable zero-shot keystep localization and improved video representation learning, exceeding the state of the art.
</details>
<details>
<summary>摘要</summary>
执行活动理解需要感知人类行为，视为一个更广泛的任务，涉及多个键步骤在视频中顺序执行，以达到最终目标状态，例如预约的食谱或 DIY 修理任务。现有的工作大多数都是隔离这个更广泛的结构，或者固定地将键步骤与预先定义的顺序脚本相对应。我们提议自动从教程视频中发现任务图，表示人们执行键步骤的概率方式，然后利用这个图来规范新视频中的键步骤识别。在多个实际教程视频数据集上，我们表明了影响：更可靠的零基础键步骤定位和改进的视频表示学习，超越了现状的最佳。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Models-Beat-GANs-on-Image-Classification"><a href="#Diffusion-Models-Beat-GANs-on-Image-Classification" class="headerlink" title="Diffusion Models Beat GANs on Image Classification"></a>Diffusion Models Beat GANs on Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08702">http://arxiv.org/abs/2307.08702</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soumik Mukhopadhyay, Matthew Gwilliam, Vatsal Agarwal, Namitha Padmanabhan, Archana Swaminathan, Srinidhi Hegde, Tianyi Zhou, Abhinav Shrivastava</li>
<li>For: The paper explores the possibility of a unified representation learner that can address both generative and discriminative tasks simultaneously, using diffusion models as a prime candidate.* Methods: The paper uses a U-Net architecture to train a diffusion model for image generation, denoising, inpainting, super-resolution, and manipulation tasks, and demonstrates that the resulting model can generate high-fidelity, diverse, and novel images. The paper also explores optimal methods for extracting and using the embeddings generated by the model for classification tasks.* Results: The paper shows that the diffusion model outperforms comparable generative-discriminative methods such as BigBiGAN for classification tasks, and that with careful feature selection and pooling, the model achieves promising results on several fine-grained visual classification datasets.<details>
<summary>Abstract</summary>
While many unsupervised learning models focus on one family of tasks, either generative or discriminative, we explore the possibility of a unified representation learner: a model which uses a single pre-training stage to address both families of tasks simultaneously. We identify diffusion models as a prime candidate. Diffusion models have risen to prominence as a state-of-the-art method for image generation, denoising, inpainting, super-resolution, manipulation, etc. Such models involve training a U-Net to iteratively predict and remove noise, and the resulting model can synthesize high fidelity, diverse, novel images. The U-Net architecture, as a convolution-based architecture, generates a diverse set of feature representations in the form of intermediate feature maps. We present our findings that these embeddings are useful beyond the noise prediction task, as they contain discriminative information and can also be leveraged for classification. We explore optimal methods for extracting and using these embeddings for classification tasks, demonstrating promising results on the ImageNet classification task. We find that with careful feature selection and pooling, diffusion models outperform comparable generative-discriminative methods such as BigBiGAN for classification tasks. We investigate diffusion models in the transfer learning regime, examining their performance on several fine-grained visual classification datasets. We compare these embeddings to those generated by competing architectures and pre-trainings for classification tasks.
</details>
<details>
<summary>摘要</summary>
多数不监督学习模型都专注于一种家族的任务，可是我们探索一个综合表示学习者：一种使用单一预训练阶段来同时解决两种家族任务的模型。我们认为扩散模型是最佳候选人选。扩散模型已经在图像生成、降噪、填充、超解像、修改等任务中脱颖而出，这些模型通过训练 U-Net 来逐步预测和除掉噪音，并且生成出高准确率、多样化、新颖的图像。U-Net 架构是一种基于 convolution 的架构，生成了多样化的特征表示，我们发现这些嵌入有用于分类任务，它们包含描述性信息，可以用于分类。我们探索如何提取和使用这些嵌入来进行分类任务，并实现了在 ImageNet 分类任务上的成功。我们发现，通过精心选择和 pooling，扩散模型在分类任务上超过相对的生成-分类方法，如 BigBiGAN。我们 investigate 扩散模型在转移学习 режи度下的性能，对多个细腻视觉分类任务进行比较。我们比较这些嵌入与其他架构和预训练生成的嵌入，并发现扩散模型在分类任务上表现出优异的result。
</details></li>
</ul>
<hr>
<h2 id="Flow-Matching-in-Latent-Space"><a href="#Flow-Matching-in-Latent-Space" class="headerlink" title="Flow Matching in Latent Space"></a>Flow Matching in Latent Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08698">http://arxiv.org/abs/2307.08698</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vinairesearch/lfm">https://github.com/vinairesearch/lfm</a></li>
<li>paper_authors: Quan Dao, Hao Phung, Binh Nguyen, Anh Tran</li>
<li>for: 这 paper 的目的是提出一种基于流匹配的生成模型，用于高分辨率图像生成。这种方法可以在受限的计算资源下进行训练，并且可以在不同的 conditional generation 任务中实现高质量的图像生成。</li>
<li>methods: 这 paper 使用了流匹配方法，并在预训练 autoencoder 的 latent space 中进行训练。这种方法可以更好地利用计算资源，并且可以在高分辨率图像生成 tasks 中实现更好的效果。</li>
<li>results: 这 paper 的实验结果表明，流匹配方法可以在不同的 conditional generation 任务中实现高质量的图像生成。具体来说，这 paper 在 CelebA-HQ、FFHQ、LSUN Church &amp; Bedroom 和 ImageNet 等数据集上实现了优秀的quantitative 和 qualitative 结果。此外，这 paper 还提供了一种 theoretically 控制的 Wasserstein-2 距离，用于证明流匹配目标下的 latent flow distribution 和 true data distribution 之间的关系。<details>
<summary>Abstract</summary>
Flow matching is a recent framework to train generative models that exhibits impressive empirical performance while being relatively easier to train compared with diffusion-based models. Despite its advantageous properties, prior methods still face the challenges of expensive computing and a large number of function evaluations of off-the-shelf solvers in the pixel space. Furthermore, although latent-based generative methods have shown great success in recent years, this particular model type remains underexplored in this area. In this work, we propose to apply flow matching in the latent spaces of pretrained autoencoders, which offers improved computational efficiency and scalability for high-resolution image synthesis. This enables flow-matching training on constrained computational resources while maintaining their quality and flexibility. Additionally, our work stands as a pioneering contribution in the integration of various conditions into flow matching for conditional generation tasks, including label-conditioned image generation, image inpainting, and semantic-to-image generation. Through extensive experiments, our approach demonstrates its effectiveness in both quantitative and qualitative results on various datasets, such as CelebA-HQ, FFHQ, LSUN Church & Bedroom, and ImageNet. We also provide a theoretical control of the Wasserstein-2 distance between the reconstructed latent flow distribution and true data distribution, showing it is upper-bounded by the latent flow matching objective. Our code will be available at https://github.com/VinAIResearch/LFM.git.
</details>
<details>
<summary>摘要</summary>
“流行匹配”是一种最近的框架，用于训练生成模型，具有让人感到惊叹的实际性能，而且训练更加容易。然而，先前的方法仍面临计算成本高和批处空间评估函数评估的挑战。另外，半Hidden Markov model（HMM）在这个领域中的应用还很少。在这个工作中，我们提议将流行匹配应用于预训练 autoencoder 的latent空间中，从而提高计算效率和可扩展性，以便在高分辨率图像生成中进行流行匹配训练。此外，我们的工作是在流行匹配中 интеGRATION 多种条件的先驱性贡献，包括标签Conditional image generation、图像缺失和semantic-to-image generation。经过广泛的实验，我们的方法在不同的数据集上都达到了良好的量化和质量结果，如 celebA-HQ、FFHQ、LSUN Church & Bedroom 和 ImageNet。我们还提供了对 Wasserstein-2 距离真实数据分布的控制，证明它是上界于流行匹配目标。我们的代码将在 GitHub 上公开，请参考 https://github.com/VinAIResearch/LFM.git。
</details></li>
</ul>
<hr>
<h2 id="Neural-Video-Depth-Stabilizer"><a href="#Neural-Video-Depth-Stabilizer" class="headerlink" title="Neural Video Depth Stabilizer"></a>Neural Video Depth Stabilizer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08695">http://arxiv.org/abs/2307.08695</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/raymondwang987/nvds">https://github.com/raymondwang987/nvds</a></li>
<li>paper_authors: Yiran Wang, Min Shi, Jiaqi Li, Zihao Huang, Zhiguo Cao, Jianming Zhang, Ke Xian, Guosheng Lin</li>
<li>for: 实时深度测量的维度准确性，以提高视频中的深度推导精度。</li>
<li>methods: 提出一个名为神经网络深度稳定器（NVDS）的插件化框架，可以对不同的单一图像深度模型进行稳定化，并且不需要额外的训练或准确性检查。</li>
<li>results: 在大规模的自然场景视频深度数据集（VDW）上进行评估，以及两个公共的benchmark上，与之前的方法进行比较，实现了更高的一致性、精度和效率。<details>
<summary>Abstract</summary>
Video depth estimation aims to infer temporally consistent depth. Some methods achieve temporal consistency by finetuning a single-image depth model during test time using geometry and re-projection constraints, which is inefficient and not robust. An alternative approach is to learn how to enforce temporal consistency from data, but this requires well-designed models and sufficient video depth data. To address these challenges, we propose a plug-and-play framework called Neural Video Depth Stabilizer (NVDS) that stabilizes inconsistent depth estimations and can be applied to different single-image depth models without extra effort. We also introduce a large-scale dataset, Video Depth in the Wild (VDW), which consists of 14,203 videos with over two million frames, making it the largest natural-scene video depth dataset to our knowledge. We evaluate our method on the VDW dataset as well as two public benchmarks and demonstrate significant improvements in consistency, accuracy, and efficiency compared to previous approaches. Our work serves as a solid baseline and provides a data foundation for learning-based video depth models. We will release our dataset and code for future research.
</details>
<details>
<summary>摘要</summary>
视频深度估计目标是获取时间一致的深度。一些方法在测试时使用图形和投影约束来精微调整单张图像深度模型，这是不高效且不稳定的。另一种方法是通过数据来强制实现时间一致，但这需要设计出功能强大的模型并具备足够的视频深度数据。为解决这些挑战，我们提出了名为神经视频深度稳定器（NVDS）的插件化框架，可以快速稳定不一致的深度估计，并可以适用于不同的单张图像深度模型无需额外努力。我们还推出了一个大规模的数据集，视频深度在野（VDW），该数据集包含14203个视频和超过200万帧，这是我们所知道的最大的自然场景视频深度数据集。我们对VDW数据集以及两个公共的标准测试集进行评估，并示出了和前方法相比的显著改进。我们的工作作为一个坚实的基础和未来研究的数据基础，我们将在未来发布我们的数据集和代码。
</details></li>
</ul>
<hr>
<h2 id="SEMI-DiffusionInst-A-Diffusion-Model-Based-Approach-for-Semiconductor-Defect-Classification-and-Segmentation"><a href="#SEMI-DiffusionInst-A-Diffusion-Model-Based-Approach-for-Semiconductor-Defect-Classification-and-Segmentation" class="headerlink" title="SEMI-DiffusionInst: A Diffusion Model Based Approach for Semiconductor Defect Classification and Segmentation"></a>SEMI-DiffusionInst: A Diffusion Model Based Approach for Semiconductor Defect Classification and Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08693">http://arxiv.org/abs/2307.08693</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vic De Ridder, Bappaditya Dey, Sandip Halder, Bartel Van Waeyenberge</li>
<li>for: 本研究旨在提出一种新的半导体缺陷检测框架”SEMI-DiffusionInst”,并与之前的框架进行比较。</li>
<li>methods: 该研究使用了一种扩散模型，并 investigate了不同的特征提取器网络和数据采样策略以实现一个平衡的质量和计算效率。</li>
<li>results: 该模型在总的mAP和准确地分割mAP方面都有所提高，并在大多数缺陷类型中表现比以前的工作更好或相当。特别是在检测任务中，线扩散和薄桥缺陷的准确率提高约15%。此外，通过调整推理参数，推理时间可以得到显著提高，无需妥协模型精度。<details>
<summary>Abstract</summary>
With continuous progression of Moore's Law, integrated circuit (IC) device complexity is also increasing. Scanning Electron Microscope (SEM) image based extensive defect inspection and accurate metrology extraction are two main challenges in advanced node (2 nm and beyond) technology. Deep learning (DL) algorithm based computer vision approaches gained popularity in semiconductor defect inspection over last few years. In this research work, a new semiconductor defect inspection framework "SEMI-DiffusionInst" is investigated and compared to previous frameworks. To the best of the authors' knowledge, this work is the first demonstration to accurately detect and precisely segment semiconductor defect patterns by using a diffusion model. Different feature extractor networks as backbones and data sampling strategies are investigated towards achieving a balanced trade-off between precision and computing efficiency. Our proposed approach outperforms previous work on overall mAP and performs comparatively better or as per for almost all defect classes (per class APs). The bounding box and segmentation mAPs achieved by the proposed SEMI-DiffusionInst model are improved by 3.83% and 2.10%, respectively. Among individual defect types, precision on line collapse and thin bridge defects are improved approximately 15\% on detection task for both defect types. It has also been shown that by tuning inference hyperparameters, inference time can be improved significantly without compromising model precision. Finally, certain limitations and future work strategy to overcome them are discussed.
</details>
<details>
<summary>摘要</summary>
随着Moore的法则不断进步，集成电路（IC）设备复杂度也在增加。扫描电子镜（SEM）图像基于广泛的缺陷检测和精确的测量提取是进程技术的两大挑战。在过去几年中，用于半导体缺陷检测的深度学习（DL）算法基于计算机视觉方法得到了广泛的应用。本研究工作中，一种新的半导体缺陷检测框架“SEMI-DiffusionInst”被调查和比较了前一些框架。据作者所知，这是第一次使用扩散模型准确地检测和精确地分割半导体缺陷模式。不同的特征提取网络作为后端和数据采样策略的调查，以实现精度和计算效率之间的平衡负担。我们的提议方法在总的map和每个缺陷类ap方面的性能有所提高，并且与前一些工作相比，对大多数缺陷类型的性能具有较好的性能。SEMI-DiffusionInst模型在bounding box和分割map方面的性能提高了3.83%和2.10%。对于特定的缺陷类型，我们对检测任务的精度有15%的提高。此外，通过调整推理超参数，可以在不妥协精度的前提下大幅提高推理时间。最后，本研究的一些局限性和未来工作策略以及如何超越它们也被讨论。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Counting-from-Self-Collages"><a href="#Semantic-Counting-from-Self-Collages" class="headerlink" title="Semantic Counting from Self-Collages"></a>Semantic Counting from Self-Collages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08727">http://arxiv.org/abs/2307.08727</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lukasknobel/selfcollages">https://github.com/lukasknobel/selfcollages</a></li>
<li>paper_authors: Lukas Knobel, Tengda Han, Yuki M. Asano</li>
<li>for: 不需要手动标注数据，可以学习对象数量计算任务。</li>
<li>methods: 使用自带的”SelfCollages”图像作为训练样本，利用现有的无监督表示和分割技术进行学习。</li>
<li>results: 比基线模型和通用模型强，可以与有监督学习模型相匹配。<details>
<summary>Abstract</summary>
While recent supervised methods for reference-based object counting continue to improve the performance on benchmark datasets, they have to rely on small datasets due to the cost associated with manually annotating dozens of objects in images. We propose Unsupervised Counter (UnCo), a model that can learn this task without requiring any manual annotations. To this end, we construct "SelfCollages", images with various pasted objects as training samples, that provide a rich learning signal covering arbitrary object types and counts. Our method builds on existing unsupervised representations and segmentation techniques to successfully demonstrate the ability to count objects without manual supervision. Our experiments show that our method not only outperforms simple baselines and generic models such as FasterRCNN, but also matches the performance of supervised counting models in some domains.
</details>
<details>
<summary>摘要</summary>
近期的监督学习方法可以继续提高参考基于对象计数的性能，但它们需要依靠小型数据集，因为 manually annotating dozens of objects in images 是成本很高的。我们提议一种无监督的对象计数模型（Unsupervised Counter，UnCo），不需要任何手动纠正。为此，我们构建了“SelfCollages”，它们是各种粘贴过的对象图像，提供了丰富的学习信号，覆盖了任意对象类型和计数。我们的方法基于现有的无监督表示和分割技术，成功地实现了没有手动纠正的对象计数。我们的实验表明，我们的方法不仅超过了简单的基准和通用模型如 FasterRCNN，还与有监督 counting 模型在一些领域匹配性能。
</details></li>
</ul>
<hr>
<h2 id="Implementation-of-a-perception-system-for-autonomous-vehicles-using-a-detection-segmentation-network-in-SoC-FPGA"><a href="#Implementation-of-a-perception-system-for-autonomous-vehicles-using-a-detection-segmentation-network-in-SoC-FPGA" class="headerlink" title="Implementation of a perception system for autonomous vehicles using a detection-segmentation network in SoC FPGA"></a>Implementation of a perception system for autonomous vehicles using a detection-segmentation network in SoC FPGA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08682">http://arxiv.org/abs/2307.08682</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vision-agh/mt_kria">https://github.com/vision-agh/mt_kria</a></li>
<li>paper_authors: Maciej Baczmanski, Mateusz Wasala, Tomasz Kryjak</li>
<li>for: 本研究旨在开发一种高效、实时、能效的感知控制系统，用于自动驾驶汽车。</li>
<li>methods: 该系统基于MultiTaskV3检测分类网络，并在AMD Xilinx Kria KV260视觉AI嵌入式平台上实现了并行加速。</li>
<li>results: 该系统在对Mock城市道路环境中进行测试时，实现了对 объекts的检测精度高于97%，以及图像分割精度高于90%。同时，该系统具有低功耗和小型化的优点。<details>
<summary>Abstract</summary>
Perception and control systems for autonomous vehicles are an active area of scientific and industrial research. These solutions should be characterised by high efficiency in recognising obstacles and other environmental elements in different road conditions, real-time capability, and energy efficiency. Achieving such functionality requires an appropriate algorithm and a suitable computing platform. In this paper, we have used the MultiTaskV3 detection-segmentation network as the basis for a perception system that can perform both functionalities within a single architecture. It was appropriately trained, quantised, and implemented on the AMD Xilinx Kria KV260 Vision AI embedded platform. By using this device, it was possible to parallelise and accelerate the computations. Furthermore, the whole system consumes relatively little power compared to a CPU-based implementation (an average of 5 watts, compared to the minimum of 55 watts for weaker CPUs, and the small size (119mm x 140mm x 36mm) of the platform allows it to be used in devices where the amount of space available is limited. It also achieves an accuracy higher than 97% of the mAP (mean average precision) for object detection and above 90% of the mIoU (mean intersection over union) for image segmentation. The article also details the design of the Mecanum wheel vehicle, which was used to test the proposed solution in a mock-up city.
</details>
<details>
<summary>摘要</summary>
自动驾驶车辆的感知和控制系统是科学和工业领域的活跃领域。这些解决方案应具有高效地识别障碍物和其他环境元素，实时性和能效性。实现这种功能需要适当的算法和适当的计算平台。在这篇文章中，我们使用了MultiTaskV3检测-分割网络作为感知系统的基础，该系统可以同时完成这两个功能。它被正确地训练、量化和在AMD Xilinx Kria KV260 Vision AI嵌入式平台上实现。通过使用这台设备，可以并行化和加速计算。此外，整个系统的功耗相对较低，比CPU-基本实现（最低55 wat），而且尺寸很小（119mm x 140mm x 36mm），使其适用于有限空间的设备。此外，它的准确率高于97%的mAP（平均检测精度）和90%的mIoU（图像分割精度）。文章还详细介绍了使用的 mécanum 轮胎汽车，该车被用来测试提议的解决方案在模拟城市中。
</details></li>
</ul>
<hr>
<h2 id="CohortFinder-an-open-source-tool-for-data-driven-partitioning-of-biomedical-image-cohorts-to-yield-robust-machine-learning-models"><a href="#CohortFinder-an-open-source-tool-for-data-driven-partitioning-of-biomedical-image-cohorts-to-yield-robust-machine-learning-models" class="headerlink" title="CohortFinder: an open-source tool for data-driven partitioning of biomedical image cohorts to yield robust machine learning models"></a>CohortFinder: an open-source tool for data-driven partitioning of biomedical image cohorts to yield robust machine learning models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08673">http://arxiv.org/abs/2307.08673</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fan Fan, Georgia Martinez, Thomas Desilvio, John Shin, Yijiang Chen, Bangchen Wang, Takaya Ozeki, Maxime W. Lafarge, Viktor H. Koelzer, Laura Barisoni, Anant Madabhushi, Satish E. Viswanath, Andrew Janowczyk</li>
<li>for: 这个论文是为了 Mitigating batch effects (BEs) in machine learning (ML) models, specifically through data-driven cohort partitioning.</li>
<li>methods: 该论文使用了一个开源工具叫做 CohortFinder, 通过数据驱动的 cohort 分区来缓解 BEs.</li>
<li>results: 论文表明，使用 CohortFinder 可以提高下游医疗影像处理任务中 ML 模型的性能。<details>
<summary>Abstract</summary>
Batch effects (BEs) refer to systematic technical differences in data collection unrelated to biological variations whose noise is shown to negatively impact machine learning (ML) model generalizability. Here we release CohortFinder, an open-source tool aimed at mitigating BEs via data-driven cohort partitioning. We demonstrate CohortFinder improves ML model performance in downstream medical image processing tasks. CohortFinder is freely available for download at cohortfinder.com.
</details>
<details>
<summary>摘要</summary>
批处效应（BE）指的是数据收集过程中的系统性技术差异，不 relacionados con variaciones biológicas，这些噪声会负面影响机器学习（ML）模型的泛化性。我们现在发布了一个开源工具，名为 cohortfinder，用于减少BE的影响。我们示例了 cohortfinder 可以提高下游医学影像处理任务中 ML 模型的性能。cohortfinder 可以免费下载于 cohortfinder.com。
</details></li>
</ul>
<hr>
<h2 id="PolyGNN-Polyhedron-based-Graph-Neural-Network-for-3D-Building-Reconstruction-from-Point-Clouds"><a href="#PolyGNN-Polyhedron-based-Graph-Neural-Network-for-3D-Building-Reconstruction-from-Point-Clouds" class="headerlink" title="PolyGNN: Polyhedron-based Graph Neural Network for 3D Building Reconstruction from Point Clouds"></a>PolyGNN: Polyhedron-based Graph Neural Network for 3D Building Reconstruction from Point Clouds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08636">http://arxiv.org/abs/2307.08636</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chenzhaiyu/polygnn">https://github.com/chenzhaiyu/polygnn</a></li>
<li>paper_authors: Zhaiyu Chen, Yilei Shi, Liangliang Nan, Zhitong Xiong, Xiao Xiang Zhu</li>
<li>for: 本研究旨在开发一种基于多面体的图 neural network，用于从点云数据中进行3D建筑重建。</li>
<li>methods: 本方法使用多面体分解获取 primitives，然后通过图节点分类来学习这些primitives的组合。为了有效地表示任意形状的多面体，我们提出了三种不同的采样策略，以选择表示多面体的有效点。此外，我们还 incorporate 多面体间的邻接关系来增强图节点的分类。</li>
<li>results: 我们在大规模的 sintetic dataset上进行了大规模的重建，并进行了对比分析。结果表明，我们的方法可以快速和高效地进行大规模的重建，并且可以提供高质量的重建结果。此外，我们还进行了对实际点云数据进行重建的实验，并发现我们的方法可以在不同城市的点云数据上进行有效的重建。<details>
<summary>Abstract</summary>
We present PolyGNN, a polyhedron-based graph neural network for 3D building reconstruction from point clouds. PolyGNN learns to assemble primitives obtained by polyhedral decomposition via graph node classification, achieving a watertight, compact, and weakly semantic reconstruction. To effectively represent arbitrary-shaped polyhedra in the neural network, we propose three different sampling strategies to select representative points as polyhedron-wise queries, enabling efficient occupancy inference. Furthermore, we incorporate the inter-polyhedron adjacency to enhance the classification of the graph nodes. We also observe that existing city-building models are abstractions of the underlying instances. To address this abstraction gap and provide a fair evaluation of the proposed method, we develop our method on a large-scale synthetic dataset covering 500k+ buildings with well-defined ground truths of polyhedral class labels. We further conduct a transferability analysis across cities and on real-world point clouds. Both qualitative and quantitative results demonstrate the effectiveness of our method, particularly its efficiency for large-scale reconstructions. The source code and data of our work are available at https://github.com/chenzhaiyu/polygnn.
</details>
<details>
<summary>摘要</summary>
我们介绍PolyGNN，一种基于多面体的图 neural network，用于从点云数据中进行3D建筑重建。PolyGNN通过图节点分类学习粗粒结构，实现了水密、紧凑、弱 semantic reconstruction。为了有效地表示任意形状多面体在神经网络中，我们提出了三种不同的抽象策略，选择表示多面体的重要点作为 queries，以实现高效的占据推断。此外，我们还 incorporate了多面体间邻接关系，以提高图节点的分类。我们还发现现有的城市建筑模型都是对下面的实例进行抽象。为了 Addressing this abstraction gap and provide a fair evaluation of our method, we develop our method on a large-scale synthetic dataset covering 500k+ buildings with well-defined ground truths of polyhedral class labels. We further conduct a transferability analysis across cities and on real-world point clouds. Both qualitative and quantitative results demonstrate the effectiveness of our method, particularly its efficiency for large-scale reconstructions. 我们的代码和数据可以在https://github.com/chenzhaiyu/polygnn中获取。
</details></li>
</ul>
<hr>
<h2 id="Deficiency-Aware-Masked-Transformer-for-Video-Inpainting"><a href="#Deficiency-Aware-Masked-Transformer-for-Video-Inpainting" class="headerlink" title="Deficiency-Aware Masked Transformer for Video Inpainting"></a>Deficiency-Aware Masked Transformer for Video Inpainting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08629">http://arxiv.org/abs/2307.08629</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yeates/dmt">https://github.com/yeates/dmt</a></li>
<li>paper_authors: Yongsheng Yu, Heng Fan, Libo Zhang</li>
<li>for: 这篇论文的目的是提出一种能够处理视频填充问题的方法，具体来说是提出一种能够在视频中填充损坏的部分的方法。</li>
<li>methods: 这篇论文使用了一种叫做Deficiency-aware Masked Transformer（DMT）的框架，该框架具有三个优势：首先，通过预训练一个图像填充模型DMT_img，以便将其用作视频模型DMT_vid的预设，从而提高了补假 случа件的填充效果。其次，使用自身注意力模块 selectively 把 spatiotemporal 标识符包含在推理中，以加速推理和除除噪信号。第三，将一种简单 yet effective的 Receptive Field Contextualizer 集成到 DMT 中，进一步提高了性能。</li>
<li>results: 对于 YouTube-VOS 和 DAVIS 等 datasets，DMT_vid 具有显著的性能优势，与之前的解决方案相比，具有更高的准确率和更好的稳定性。<details>
<summary>Abstract</summary>
Recent video inpainting methods have made remarkable progress by utilizing explicit guidance, such as optical flow, to propagate cross-frame pixels. However, there are cases where cross-frame recurrence of the masked video is not available, resulting in a deficiency. In such situation, instead of borrowing pixels from other frames, the focus of the model shifts towards addressing the inverse problem. In this paper, we introduce a dual-modality-compatible inpainting framework called Deficiency-aware Masked Transformer (DMT), which offers three key advantages. Firstly, we pretrain a image inpainting model DMT_img serve as a prior for distilling the video model DMT_vid, thereby benefiting the hallucination of deficiency cases. Secondly, the self-attention module selectively incorporates spatiotemporal tokens to accelerate inference and remove noise signals. Thirdly, a simple yet effective Receptive Field Contextualizer is integrated into DMT, further improving performance. Extensive experiments conducted on YouTube-VOS and DAVIS datasets demonstrate that DMT_vid significantly outperforms previous solutions. The code and video demonstrations can be found at github.com/yeates/DMT.
</details>
<details>
<summary>摘要</summary>
现代视频填充方法已经取得了显著进步，通过使用显式导向，如光流，来传播帧之间像素。然而，有些情况下，跨帧回归的masked视频不可用，导致不足。在这种情况下，而不是借鉴其他帧的像素，模型的注意力转移到了解决反问题。在这篇论文中，我们介绍了一个双Modal可能性兼容的填充框架，即缺失意识的Transformer（DMT），它具有三个关键优势。首先，我们在DMT_img模型的预训练中，使用image填充模型DMT_img作为后续的拟合模型DMT_vid的先验，从而提高了缺失情况的描述能力。其次，自我注意力模块 selectively incorporates spatiotemporal tokens，以加速推理和消除噪声信号。最后，我们采用了一个简单 yet effective的Receptive Field Contextualizer，进一步提高性能。我们在YouTube-VOS和DAVIS数据集上进行了广泛的实验，并证明了DMT_vid在前一些解决方案之上显著超越。代码和视频示例可以在github.com/yeates/DMT中找到。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-fixed-length-Fingerprint-Representations-across-different-Embedding-Sizes-and-Sensor-Types"><a href="#Benchmarking-fixed-length-Fingerprint-Representations-across-different-Embedding-Sizes-and-Sensor-Types" class="headerlink" title="Benchmarking fixed-length Fingerprint Representations across different Embedding Sizes and Sensor Types"></a>Benchmarking fixed-length Fingerprint Representations across different Embedding Sizes and Sensor Types</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08615">http://arxiv.org/abs/2307.08615</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tim-rohwedder/fixed-length-fingerprint-extractors">https://github.com/tim-rohwedder/fixed-length-fingerprint-extractors</a></li>
<li>paper_authors: Tim Rohwedder, Daile Osorio-Roig, Christian Rathgeb, Christoph Busch</li>
<li>for: 这个论文的目的是提高指纹识别的计算效率，通过减少纹理信息的维度来保持高度的生物ometric表现。</li>
<li>methods: 该论文使用了深度神经网络提取指纹的固定长度嵌入。</li>
<li>results: 实验结果表明，使用512个特征元素的纹理基于嵌入部分的fixed-length指纹表示可以保持高度的识别性，并且可以看到不同感知器类型之间的性能差异。<details>
<summary>Abstract</summary>
Traditional minutiae-based fingerprint representations consist of a variable-length set of minutiae. This necessitates a more complex comparison causing the drawback of high computational cost in one-to-many comparison. Recently, deep neural networks have been proposed to extract fixed-length embeddings from fingerprints. In this paper, we explore to what extent fingerprint texture information contained in such embeddings can be reduced in terms of dimension while preserving high biometric performance. This is of particular interest since it would allow to reduce the number of operations incurred at comparisons. We also study the impact in terms of recognition performance of the fingerprint textural information for two sensor types, i.e. optical and capacitive. Furthermore, the impact of rotation and translation of fingerprint images on the extraction of fingerprint embeddings is analysed. Experimental results conducted on a publicly available database reveal an optimal embedding size of 512 feature elements for the texture-based embedding part of fixed-length fingerprint representations. In addition, differences in performance between sensor types can be perceived.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/18/cs.CV_2023_07_18/" data-id="cloimip8a00eis4886jfx0azp" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_07_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/18/cs.AI_2023_07_18/" class="article-date">
  <time datetime="2023-07-18T12:00:00.000Z" itemprop="datePublished">2023-07-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/18/cs.AI_2023_07_18/">cs.AI - 2023-07-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="CertPri-Certifiable-Prioritization-for-Deep-Neural-Networks-via-Movement-Cost-in-Feature-Space"><a href="#CertPri-Certifiable-Prioritization-for-Deep-Neural-Networks-via-Movement-Cost-in-Feature-Space" class="headerlink" title="CertPri: Certifiable Prioritization for Deep Neural Networks via Movement Cost in Feature Space"></a>CertPri: Certifiable Prioritization for Deep Neural Networks via Movement Cost in Feature Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09375">http://arxiv.org/abs/2307.09375</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haibin Zheng, Jinyin Chen, Haibo Jin</li>
<li>for: 提高深度神经网络（DNN）软件系统的质量，特别是通过测试输入优先级来检测和修复DNN中的误行为。</li>
<li>methods: 基于测试输入运动成本的视角，提出了一种名为CertPri的测试输入优先级技术，该技术可以提供正式的Robustness保证，并且可以在不同的任务、数据、模型和场景下进行应用。</li>
<li>results: 对于两个任务（分类和回归）、六种数据形式、四种模型结构和两种场景（白盒和黑盒）进行了广泛的评估，结果显示CertPri的优先级效果明显高于基elines，例如平均提高53.97%的优先级效果。其稳定性和通用性分别高于基elines的1.41-2.00倍和1.33-3.39倍的平均值。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) have demonstrated their outperformance in various software systems, but also exhibit misbehavior and even result in irreversible disasters. Therefore, it is crucial to identify the misbehavior of DNN-based software and improve DNNs' quality. Test input prioritization is one of the most appealing ways to guarantee DNNs' quality, which prioritizes test inputs so that more bug-revealing inputs can be identified earlier with limited time and manual labeling efforts. However, the existing prioritization methods are still limited from three aspects: certifiability, effectiveness, and generalizability. To overcome the challenges, we propose CertPri, a test input prioritization technique designed based on a movement cost perspective of test inputs in DNNs' feature space. CertPri differs from previous works in three key aspects: (1) certifiable: it provides a formal robustness guarantee for the movement cost; (2) effective: it leverages formally guaranteed movement costs to identify malicious bug-revealing inputs; and (3) generic: it can be applied to various tasks, data, models, and scenarios. Extensive evaluations across 2 tasks (i.e., classification and regression), 6 data forms, 4 model structures, and 2 scenarios (i.e., white-box and black-box) demonstrate CertPri's superior performance. For instance, it significantly improves 53.97% prioritization effectiveness on average compared with baselines. Its robustness and generalizability are 1.41~2.00 times and 1.33~3.39 times that of baselines on average, respectively.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>证明性：它提供了 DNN 中输入的形式化Robustness garantue，以确保移动成本的正确性。2. 有效性：它利用 formally guaranteed 的移动成本来 Identify DNN 中的恶意 bug-revealing 输入。3. 普遍性：它可以应用于不同的任务、数据、模型和场景。我们在 2 个任务（分类和回归）、6 种数据形式、4 种模型结构和 2 种场景（白盒和黑盒）进行了广泛的评估。结果表明，CertPri 在 average 上提高了 53.97% 的优先级效果，相比基eline。它的Robustness和普遍性分别高于基eline 的 1.41-2.00 倍和 1.33-3.39 倍的平均值。</details></li>
</ol>
<hr>
<h2 id="Local-Minima-Drive-Communications-in-Cooperative-Interaction"><a href="#Local-Minima-Drive-Communications-in-Cooperative-Interaction" class="headerlink" title="Local Minima Drive Communications in Cooperative Interaction"></a>Local Minima Drive Communications in Cooperative Interaction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09364">http://arxiv.org/abs/2307.09364</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roger K. Moore</li>
<li>for: 这种研究旨在探讨人机合作中，何时决定通信，尤其是在合作任务中。</li>
<li>methods: 这种研究使用了感知控制理论（PCT），发现在共同完成任务的情况下，只要有共同的目标，并且合作行为足够完成任务，而不需要间接通信。但是，当任务存在本地最小点时，global解只能通过适时的交流寻找。</li>
<li>results: 在计算机基础的 simulate 环境中，两个独立的一维Agent通过合作解决了两个维度的路径找问题。<details>
<summary>Abstract</summary>
An important open question in human-robot interaction (HRI) is precisely when an agent should decide to communicate, particularly in a cooperative task. Perceptual Control Theory (PCT) tells us that agents are able to cooperate on a joint task simply by sharing the same 'intention', thereby distributing the effort required to complete the task among the agents. This is even true for agents that do not possess the same abilities, so long as the goal is observable, the combined actions are sufficient to complete the task, and there is no local minimum in the search space. If these conditions hold, then a cooperative task can be accomplished without any communication between the contributing agents. However, for tasks that do contain local minima, the global solution can only be reached if at least one of the agents adapts its intention at the appropriate moments, and this can only be achieved by appropriately timed communication. In other words, it is hypothesised that in cooperative tasks, the function of communication is to coordinate actions in a complex search space that contains local minima. These principles have been verified in a computer-based simulation environment in which two independent one-dimensional agents are obliged to cooperate in order to solve a two-dimensional path-finding task.
</details>
<details>
<summary>摘要</summary>
人机交互（HRI）中一个重要的问题是决定何时通信，特别是在合作任务中。感知控制理论（PCT）告诉我们，只要 agents 共享同一个 '意图'，就可以在合作任务中分配完成任务所需的努力。这是甚至适用于不具备同样能力的代理人，只要目标可见，合作行动充分，并且无地点最低点。如果这些条件成立，那么合作任务就可以无需交流完成。但是，对于包含地点最低点的任务，全球解决方案只能通过至少一个代理人在合适时间修改其意图来实现。这意味着，在合作任务中，交流的函数是协调在复杂的搜索空间中的行动。这些原则在一个基于计算机的模拟环境中已经得到了验证，在两个独立的一维代理人之间进行了一个两维路径找到任务。
</details></li>
</ul>
<hr>
<h2 id="MOCA-Self-supervised-Representation-Learning-by-Predicting-Masked-Online-Codebook-Assignments"><a href="#MOCA-Self-supervised-Representation-Learning-by-Predicting-Masked-Online-Codebook-Assignments" class="headerlink" title="MOCA: Self-supervised Representation Learning by Predicting Masked Online Codebook Assignments"></a>MOCA: Self-supervised Representation Learning by Predicting Masked Online Codebook Assignments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09361">http://arxiv.org/abs/2307.09361</a></li>
<li>repo_url: None</li>
<li>paper_authors: Spyros Gidaris, Andrei Bursuc, Oriane Simeoni, Antonin Vobecky, Nikos Komodakis, Matthieu Cord, Patrick Pérez</li>
<li>for: 降低干扰Transformer网络的贪婪需求，使用自我监督学习。</li>
<li>methods: 使用Masked Image Modeling策略和对比策略，并将两种学习方法融合在一起。</li>
<li>results: 在低样本设定下 achieve新的状态调研结果，并在多种评价 протокол中显示出强大的实验结果，训练时间至少三倍于先前方法。<details>
<summary>Abstract</summary>
Self-supervised learning can be used for mitigating the greedy needs of Vision Transformer networks for very large fully-annotated datasets. Different classes of self-supervised learning offer representations with either good contextual reasoning properties, e.g., using masked image modeling strategies, or invariance to image perturbations, e.g., with contrastive methods. In this work, we propose a single-stage and standalone method, MOCA, which unifies both desired properties using novel mask-and-predict objectives defined with high-level features (instead of pixel-level details). Moreover, we show how to effectively employ both learning paradigms in a synergistic and computation-efficient way. Doing so, we achieve new state-of-the-art results on low-shot settings and strong experimental results in various evaluation protocols with a training that is at least 3 times faster than prior methods.
</details>
<details>
<summary>摘要</summary>
自我监督学习可以用于 mitigating 视图转换器网络的贪婪需求，不需要很大的完全标注数据集。不同类型的自我监督学习提供了不同的表示，如使用遮盲图像模型策略，或者对图像干扰的不变性。在这个工作中，我们提出了一种单stage和独立的方法，MOCA，它通过使用新的遮盲预测目标定义高级特征（而不是像素级别的细节）来实现良好的上下文理解性和图像干扰不变性。此外，我们还证明了如何有效地结合这两种学习方法，以实现更高的性能。通过这种方法，我们在低预测设定下达到了新的状态状态Record和强大的实验性能，并且训练时间至少三次 faster than priori方法。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Select-SAT-Encodings-for-Pseudo-Boolean-and-Linear-Integer-Constraints"><a href="#Learning-to-Select-SAT-Encodings-for-Pseudo-Boolean-and-Linear-Integer-Constraints" class="headerlink" title="Learning to Select SAT Encodings for Pseudo-Boolean and Linear Integer Constraints"></a>Learning to Select SAT Encodings for Pseudo-Boolean and Linear Integer Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09342">http://arxiv.org/abs/2307.09342</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/felixvuo/lease-data">https://github.com/felixvuo/lease-data</a></li>
<li>paper_authors: Felix Ulrich-Oltean, Peter Nightingale, James Alfred Walker</li>
<li>for: 解决复杂的允许满足和优化问题</li>
<li>methods: 使用超级vised机器学习方法选择编码</li>
<li>results: 选择编码效果优于AutoFolio，并且在未看过的问题类型上也能获得良好的结果<details>
<summary>Abstract</summary>
Many constraint satisfaction and optimisation problems can be solved effectively by encoding them as instances of the Boolean Satisfiability problem (SAT). However, even the simplest types of constraints have many encodings in the literature with widely varying performance, and the problem of selecting suitable encodings for a given problem instance is not trivial. We explore the problem of selecting encodings for pseudo-Boolean and linear constraints using a supervised machine learning approach. We show that it is possible to select encodings effectively using a standard set of features for constraint problems; however we obtain better performance with a new set of features specifically designed for the pseudo-Boolean and linear constraints. In fact, we achieve good results when selecting encodings for unseen problem classes. Our results compare favourably to AutoFolio when using the same feature set. We discuss the relative importance of instance features to the task of selecting the best encodings, and compare several variations of the machine learning method.
</details>
<details>
<summary>摘要</summary>
许多约束满足和优化问题可以有效地通过将其编码为布尔满足问题（SAT）来解决。然而，即使最简单的约束也有许多文献中的编码方法，性能各异，选择适合的编码方法 для给定问题实例是一个不轻松的问题。我们使用监督式机器学习方法来选择编码方法，并证明可以使用标准的约束问题特征集来选择编码方法，并且使用新的特征集来选择 Pseudo-布尔和线性约束的编码方法。实际上，我们可以对未看过的问题类型进行有效的编码选择。我们的结果与AutoFolio相比，使用同一个特征集时比较好。我们讨论实例特征对选择最佳编码的任务的重要性，并对不同的机器学习方法进行比较。
</details></li>
</ul>
<hr>
<h2 id="Company2Vec-–-German-Company-Embeddings-based-on-Corporate-Websites"><a href="#Company2Vec-–-German-Company-Embeddings-based-on-Corporate-Websites" class="headerlink" title="Company2Vec – German Company Embeddings based on Corporate Websites"></a>Company2Vec – German Company Embeddings based on Corporate Websites</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09332">http://arxiv.org/abs/2307.09332</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christopher Gerling</li>
<li>for: 这个论文提出了一种新的应用场景，即 Representation Learning。模型使用 Word2Vec 和维度减少分析企业活动记录，保持语言结构的含义，并创建细致的行业嵌入。</li>
<li>methods: 这个论文使用 Word2Vec 和维度减少来分析企业活动记录，创建细致的行业嵌入。</li>
<li>results: 这个论文得到了高效的企业嵌入，可以用于多种银行应用程序，如顶尖词汇分析等。此外，论文还提出了三种算法used for peer-firm identification，包括 firm-centric、industry-centric 和 portfolio-centric。<details>
<summary>Abstract</summary>
With Company2Vec, the paper proposes a novel application in representation learning. The model analyzes business activities from unstructured company website data using Word2Vec and dimensionality reduction. Company2Vec maintains semantic language structures and thus creates efficient company embeddings in fine-granular industries. These semantic embeddings can be used for various applications in banking. Direct relations between companies and words allow semantic business analytics (e.g. top-n words for a company). Furthermore, industry prediction is presented as a supervised learning application and evaluation method. The vectorized structure of the embeddings allows measuring companies similarities with the cosine distance. Company2Vec hence offers a more fine-grained comparison of companies than the standard industry labels (NACE). This property is relevant for unsupervised learning tasks, such as clustering. An alternative industry segmentation is shown with k-means clustering on the company embeddings. Finally, this paper proposes three algorithms for (1) firm-centric, (2) industry-centric and (3) portfolio-centric peer-firm identification.
</details>
<details>
<summary>摘要</summary>
With Company2Vec, the paper proposes a novel application in representation learning. The model analyzes business activities from unstructured company website data using Word2Vec and dimensionality reduction. Company2Vec maintains semantic language structures and thus creates efficient company embeddings in fine-granular industries. These semantic embeddings can be used for various applications in banking. Direct relations between companies and words allow semantic business analytics (e.g. top-n words for a company). Furthermore, industry prediction is presented as a supervised learning application and evaluation method. The vectorized structure of the embeddings allows measuring companies similarities with the cosine distance. Company2Vec hence offers a more fine-grained comparison of companies than the standard industry labels (NACE). This property is relevant for unsupervised learning tasks, such as clustering. An alternative industry segmentation is shown with k-means clustering on the company embeddings. Finally, this paper proposes three algorithms for (1) firm-centric, (2) industry-centric and (3) portfolio-centric peer-firm identification.Here's the translation in Traditional Chinese:With Company2Vec, the paper proposes a novel application in representation learning. The model analyzes business activities from unstructured company website data using Word2Vec and dimensionality reduction. Company2Vec maintains semantic language structures and thus creates efficient company embeddings in fine-granular industries. These semantic embeddings can be used for various applications in banking. Direct relations between companies and words allow semantic business analytics (e.g. top-n words for a company). Furthermore, industry prediction is presented as a supervised learning application and evaluation method. The vectorized structure of the embeddings allows measuring companies similarities with the cosine distance. Company2Vec hence offers a more fine-grained comparison of companies than the standard industry labels (NACE). This property is relevant for unsupervised learning tasks, such as clustering. An alternative industry segmentation is shown with k-means clustering on the company embeddings. Finally, this paper proposes three algorithms for (1) firm-centric, (2) industry-centric and (3) portfolio-centric peer-firm identification.
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Field-Dependencies-for-Learning-on-Categorical-Data"><a href="#Exploiting-Field-Dependencies-for-Learning-on-Categorical-Data" class="headerlink" title="Exploiting Field Dependencies for Learning on Categorical Data"></a>Exploiting Field Dependencies for Learning on Categorical Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09321">http://arxiv.org/abs/2307.09321</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/csiro-robotics/mdl">https://github.com/csiro-robotics/mdl</a></li>
<li>paper_authors: Zhibin Li, Piotr Koniusz, Lu Zhang, Daniel Edward Pagendam, Peyman Moghadam</li>
<li>for: 本研究旨在利用领域关系学习 categorical 数据，以优化预测性能。</li>
<li>methods: 我们提出了一种新的方法，即在 global field dependency matrix 中学习领域关系，并通过 local dependency modelling 在实例级别进行修改，以提高预测性能。</li>
<li>results: 我们的方法在 six 个popular dataset上进行了比较，与state-of-the-art方法进行了比较，并且得到了更好的性能。<details>
<summary>Abstract</summary>
Traditional approaches for learning on categorical data underexploit the dependencies between columns (\aka fields) in a dataset because they rely on the embedding of data points driven alone by the classification/regression loss. In contrast, we propose a novel method for learning on categorical data with the goal of exploiting dependencies between fields. Instead of modelling statistics of features globally (i.e., by the covariance matrix of features), we learn a global field dependency matrix that captures dependencies between fields and then we refine the global field dependency matrix at the instance-wise level with different weights (so-called local dependency modelling) w.r.t. each field to improve the modelling of the field dependencies. Our algorithm exploits the meta-learning paradigm, i.e., the dependency matrices are refined in the inner loop of the meta-learning algorithm without the use of labels, whereas the outer loop intertwines the updates of the embedding matrix (the matrix performing projection) and global dependency matrix in a supervised fashion (with the use of labels). Our method is simple yet it outperforms several state-of-the-art methods on six popular dataset benchmarks. Detailed ablation studies provide additional insights into our method.
</details>
<details>
<summary>摘要</summary>
传统方法学习 categorical 数据会忽略数据集中列（即字段）之间的依赖关系，因为它们基于单独的分类/回归损失来驱动数据点的嵌入。然而，我们提出了一种新的方法，旨在利用数据集中列之间的依赖关系。而不是基于特征的全局统计（即特征 covariance 矩阵）来模型特征，我们学习一个全局字段依赖矩阵，然后在每个实例级别使用不同的权重（即本地依赖模型）来改进字段依赖的模型。我们的算法利用了元学习概念，即依赖矩阵在内Loop中被反复更新，而外层Loop则在有标签的情况下进行监督式更新（使用标签）。我们的方法简单，但它在六个流行的数据集benchmark上表现出色，并且在详细的减少研究中提供了更多的减少研究。
</details></li>
</ul>
<hr>
<h2 id="Biomaker-CA-a-Biome-Maker-project-using-Cellular-Automata"><a href="#Biomaker-CA-a-Biome-Maker-project-using-Cellular-Automata" class="headerlink" title="Biomaker CA: a Biome Maker project using Cellular Automata"></a>Biomaker CA: a Biome Maker project using Cellular Automata</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09320">http://arxiv.org/abs/2307.09320</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ettore Randazzo, Alexander Mordvintsev</li>
<li>for: 这个论文旨在描述一个基于细胞自动机（CA）的生物制造项目，即生物制造CA（Biomaker CA），用于模拟复杂的生态系统。</li>
<li>methods: 该论文使用了Python JAX框架来平台化CA规则的计算，并在GPU上并行计算。它还提供了多种环境和物理法则，以及不同的模型建筑和突变策略。</li>
<li>results: 论文展示了植物代理人在不同的环境下可以生长、存活、繁殖和演化，形成稳定和不稳定的生态系统。它还介绍了如何使用元进化来让模型在恶劣环境中存活，以及如何通过互动进化来让用户直接参与演化过程。<details>
<summary>Abstract</summary>
We introduce Biomaker CA: a Biome Maker project using Cellular Automata (CA). In Biomaker CA, morphogenesis is a first class citizen and small seeds need to grow into plant-like organisms to survive in a nutrient starved environment and eventually reproduce with variation so that a biome survives for long timelines. We simulate complex biomes by means of CA rules in 2D grids and parallelize all of its computation on GPUs through the Python JAX framework. We show how this project allows for several different kinds of environments and laws of 'physics', alongside different model architectures and mutation strategies. We further analyze some configurations to show how plant agents can grow, survive, reproduce, and evolve, forming stable and unstable biomes. We then demonstrate how one can meta-evolve models to survive in a harsh environment either through end-to-end meta-evolution or by a more surgical and efficient approach, called Petri dish meta-evolution. Finally, we show how to perform interactive evolution, where the user decides how to evolve a plant model interactively and then deploys it in a larger environment. We open source Biomaker CA at: https://tinyurl.com/2x8yu34s .
</details>
<details>
<summary>摘要</summary>
我们介绍生物创造者CA（Biomaker CA）：一个基于细胞自动机（CA）的生物创造项目。在生物创造者CA中，形态形成是一等级公民，小种子需要在营养不足环境中成长为植物подіб的生物，以存活和 eventually reproduce 繁殖，以确保生物群落的长期存储。我们使用2D网格和Python JAX框架进行CA规则的并行计算，并可以模拟复杂的生物群落。我们还显示了不同环境和物理法则，以及不同的模型架构和变异策略。我们进一步分析了一些配置，详细介绍植物代理如何成长、存活、繁殖和演化，形成稳定和不稳定的生物群落。最后，我们显示了如何使用终端进化或对应演化来让模型在严峻环境中存活，并且允许用户互动地进化植物模型，然后将其部署到更大的环境中。我们将生物创造者CA开源发布在以下网址：https://tinyurl.com/2x8yu34s。
</details></li>
</ul>
<hr>
<h2 id="Rumor-Detection-with-Diverse-Counterfactual-Evidence"><a href="#Rumor-Detection-with-Diverse-Counterfactual-Evidence" class="headerlink" title="Rumor Detection with Diverse Counterfactual Evidence"></a>Rumor Detection with Diverse Counterfactual Evidence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09296">http://arxiv.org/abs/2307.09296</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vicinity111/dce-rd">https://github.com/vicinity111/dce-rd</a></li>
<li>paper_authors: Kaiwei Zhang, Junchi Yu, Haichao Shi, Jian Liang, Xiao-Yu Zhang</li>
<li>for: 这个研究旨在提出一个能够有效地检测谣传的方法，以应对社交媒体中对个人和社区的威胁增加。</li>
<li>methods: 本研究使用graph neural networks (GNNs)以探索谣传的传播模式，并利用多个构图来构成多元的探索结果。</li>
<li>results: 研究获得了更好的性能，并且可以提供多元的探索结果，以增强谣传检测的可解性和稳定性。<details>
<summary>Abstract</summary>
The growth in social media has exacerbated the threat of fake news to individuals and communities. This draws increasing attention to developing efficient and timely rumor detection methods. The prevailing approaches resort to graph neural networks (GNNs) to exploit the post-propagation patterns of the rumor-spreading process. However, these methods lack inherent interpretation of rumor detection due to the black-box nature of GNNs. Moreover, these methods suffer from less robust results as they employ all the propagation patterns for rumor detection. In this paper, we address the above issues with the proposed Diverse Counterfactual Evidence framework for Rumor Detection (DCE-RD). Our intuition is to exploit the diverse counterfactual evidence of an event graph to serve as multi-view interpretations, which are further aggregated for robust rumor detection results. Specifically, our method first designs a subgraph generation strategy to efficiently generate different subgraphs of the event graph. We constrain the removal of these subgraphs to cause the change in rumor detection results. Thus, these subgraphs naturally serve as counterfactual evidence for rumor detection. To achieve multi-view interpretation, we design a diversity loss inspired by Determinantal Point Processes (DPP) to encourage diversity among the counterfactual evidence. A GNN-based rumor detection model further aggregates the diverse counterfactual evidence discovered by the proposed DCE-RD to achieve interpretable and robust rumor detection results. Extensive experiments on two real-world datasets show the superior performance of our method. Our code is available at https://github.com/Vicinity111/DCE-RD.
</details>
<details>
<summary>摘要</summary>
随着社交媒体的增长，假新闻对个人和社区而言变得更加严重，引起了开发有效和及时干预假新闻的研究的关注。现有的方法主要利用图神经网络（GNN）来利用假新闻传播过程中的后传 Pattern，但这些方法缺乏假新闻检测的自然解释，同时Results也较为不稳定。在这篇论文中，我们提出了一种基于多元解释的假新闻检测方法，即多元解释假新闻检测框架（DCE-RD）。我们的假设是利用事件图中的多个子图来提供多元解释，然后将这些多元解释集成为可靠的假新闻检测结果。具体来说，我们首先设计了一种子图生成策略，以生成不同的子图。我们限制了这些子图的移除，以避免影响假新闻检测结果。因此，这些子图自然成为假新闻检测中的反例证据。为了实现多元解释，我们设计了一种基于杂度点过程（DPP）的多元解释损失，以鼓励多元解释的涌现。一个基于GNN的假新闻检测模型进一步聚合了DCE-RD所发现的多元解释，以实现可靠和可解释的假新闻检测结果。我们的实验结果表明，我们的方法在两个真实的数据集上具有更高的性能。我们的代码可以在https://github.com/Vicinity111/DCE-RD上找到。
</details></li>
</ul>
<hr>
<h2 id="The-Language-Labyrinth-Constructive-Critique-on-the-Terminology-Used-in-the-AI-Discourse"><a href="#The-Language-Labyrinth-Constructive-Critique-on-the-Terminology-Used-in-the-AI-Discourse" class="headerlink" title="The Language Labyrinth: Constructive Critique on the Terminology Used in the AI Discourse"></a>The Language Labyrinth: Constructive Critique on the Terminology Used in the AI Discourse</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10292">http://arxiv.org/abs/2307.10292</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rainer Rehak</li>
<li>for: 本研究旨在探讨人工智能（AI）领域中的术语问题，即现有的术语如“训练”、“学习”或“决策”等，对于AI的发展和应用产生了潜在的歧视和误导。</li>
<li>methods: 本研究采用了 критиче理解和语言哲学等方法，对AI辩论中关键的概念进行分析和批判，并提出了更合适的术语来促进更有价值的讨论。</li>
<li>results: 本研究发现，AI辩论中的术语问题导致了对AI的应用和责任的歧视和误导，从而妨碍了更加深入和系统的讨论。通过提出更合适的术语，本研究期望促进更有价值的AI讨论和应用。<details>
<summary>Abstract</summary>
In the interdisciplinary field of artificial intelligence (AI) the problem of clear terminology is especially momentous. This paper claims, that AI debates are still characterised by a lack of critical distance to metaphors like 'training', 'learning' or 'deciding'. As consequence, reflections regarding responsibility or potential use-cases are greatly distorted. Yet, if relevant decision-makers are convinced that AI can develop an 'understanding' or properly 'interpret' issues, its regular use for sensitive tasks like deciding about social benefits or judging court cases looms. The chapter argues its claim by analysing central notions of the AI debate and tries to contribute by proposing more fitting terminology and hereby enabling more fruitful debates. It is a conceptual work at the intersection of critical computer science and philosophy of language.
</details>
<details>
<summary>摘要</summary>
在人工智能（AI）的跨学科领域中，术语问题特别突出。这篇论文认为，AI讨论仍然受到'"训练"、'"学习"或'"决策"等 мета征的影响，从而导致责任或实际应用场景的反映变得扭曲。然而，如果有关决策者被感受到AI可以'"理解"或'"正确地解读"问题，那么对社会福利或法律案件的决策将变得更加常见。本章 argue其主张，通过分析中心AI讨论概念，并提出更合适的术语，以便促进更有益的讨论。这是一篇在计算机科学与语言哲学交叉领域的概念性工作。
</details></li>
</ul>
<hr>
<h2 id="Llama-2-Open-Foundation-and-Fine-Tuned-Chat-Models"><a href="#Llama-2-Open-Foundation-and-Fine-Tuned-Chat-Models" class="headerlink" title="Llama 2: Open Foundation and Fine-Tuned Chat Models"></a>Llama 2: Open Foundation and Fine-Tuned Chat Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09288">http://arxiv.org/abs/2307.09288</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/llama">https://github.com/facebookresearch/llama</a></li>
<li>paper_authors: Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom</li>
<li>for: 这篇论文是为了开发和发布一个大语言模型（LLM）的集合，覆盖 Parameters 的范围在 70 亿到 700 亿之间。</li>
<li>methods: 该论文使用了预训练和精度调整的大语言模型（LLM），并对对话用例进行优化。</li>
<li>results: 该论文的模型在大多数测试 benchmark 上表现更好，并且根据人类评估，它们在帮助和安全性方面表现出色，可能成为关闭源模型的可行替代品。<details>
<summary>Abstract</summary>
In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们开发并发布了LLama 2，一个包含预训练和精度调整的大语言模型（LLM）的收藏，其中参数的规模从70亿到700亿。我们的精度调整的LLM，称为Llama 2-Chat，是对话用例的优化。我们的模型在我们测试的大多数标准准则上表现出色，并且根据我们的人工评估，Llama 2-Chat在帮助性和安全性方面可能成为关闭源模型的可接受替补。我们提供了细节的方法和安全改进的描述，以便社区可以基于我们的工作而进一步发展和贡献到责任的LLM开发。
</details></li>
</ul>
<hr>
<h2 id="Improving-Text-Semantic-Similarity-Modeling-through-a-3D-Siamese-Network"><a href="#Improving-Text-Semantic-Similarity-Modeling-through-a-3D-Siamese-Network" class="headerlink" title="Improving Text Semantic Similarity Modeling through a 3D Siamese Network"></a>Improving Text Semantic Similarity Modeling through a 3D Siamese Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09274">http://arxiv.org/abs/2307.09274</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianxiang Zang, Hui Liu</li>
<li>for: 文章主要用于提出一种基于三维 siamese 网络的文本 semantic similarity 模型，以提高现有方法的精度和可扩展性。</li>
<li>methods: 该模型使用了一种三维 siamese 网络，将文本semantic information 映射到更高维空间，以保留更多的特征领域信息和空间信息。模型还引入了一些功能扩充模块，包括特征提取、注意力和特征融合，以强化模型的表现。</li>
<li>results: 经过广泛的实验 validate，模型在四个文本 semantic similarity 标准 bencmarks 上显示出了高效和高精度的表现。<details>
<summary>Abstract</summary>
Siamese networks have gained popularity as a method for modeling text semantic similarity. Traditional methods rely on pooling operation to compress the semantic representations from Transformer blocks in encoding, resulting in two-dimensional semantic vectors and the loss of hierarchical semantic information from Transformer blocks. Moreover, this limited structure of semantic vectors is akin to a flattened landscape, which restricts the methods that can be applied in downstream modeling, as they can only navigate this flat terrain. To address this issue, we propose a novel 3D Siamese network for text semantic similarity modeling, which maps semantic information to a higher-dimensional space. The three-dimensional semantic tensors not only retains more precise spatial and feature domain information but also provides the necessary structural condition for comprehensive downstream modeling strategies to capture them. Leveraging this structural advantage, we introduce several modules to reinforce this 3D framework, focusing on three aspects: feature extraction, attention, and feature fusion. Our extensive experiments on four text semantic similarity benchmarks demonstrate the effectiveness and efficiency of our 3D Siamese Network.
</details>
<details>
<summary>摘要</summary>
塞内链模型在文本Semantic相似性领域的应用得到了广泛的关注。传统方法通过pooling操作来压缩Transformer块中的semantic表示，导致得到的是两维semantic вектор，这限制了downstream模型的应用。此外，这种压缩的semantic表示结构类似于平坦的 terrain，这限制了可以应用的方法，它们只能在这个平坦的地形上行走。为了解决这个问题，我们提出了一种新的3D塞内链网络 для文本Semantic相似性模型，它将semantic信息映射到更高维空间。三维semantic tensor不仅保留了更精确的空间和特征领域信息，还提供了下游模型capture这些信息所需的结构条件。利用这种结构优势，我们在3D框架中引入了多个模块，集中在三个方面：特征提取、注意力和特征融合。我们对四个文本Semantic相似性benchmark进行了广泛的实验， demonstarted the effectiveness and efficiency of our 3D Siamese Network.
</details></li>
</ul>
<hr>
<h2 id="UniTabE-Pretraining-a-Unified-Tabular-Encoder-for-Heterogeneous-Tabular-Data"><a href="#UniTabE-Pretraining-a-Unified-Tabular-Encoder-for-Heterogeneous-Tabular-Data" class="headerlink" title="UniTabE: Pretraining a Unified Tabular Encoder for Heterogeneous Tabular Data"></a>UniTabE: Pretraining a Unified Tabular Encoder for Heterogeneous Tabular Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09249">http://arxiv.org/abs/2307.09249</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yazheng Yang, Yuqi Wang, Guang Liu, Ledell Wu, Qi Liu</li>
<li>for: 本研究旨在推广预训示法的应用，将其应用到表格数据领域，以提高表格数据的含义表示。</li>
<li>methods: 本研究使用了UniTabE方法，UniTabE方法通过将每个基本表格元素表示为Module，并使用Transformer Encoder进行更精确的表示。</li>
<li>results: 实验结果显示，UniTabE方法在多种 benchmark 数据集上表现出色，超过了多个基eline模型。这表明UniTabE方法可以有效地提高表格数据的含义表示，为表格数据分析带来 significiant progress.<details>
<summary>Abstract</summary>
Recent advancements in Natural Language Processing (NLP) have witnessed the groundbreaking impact of pretrained models, yielding impressive outcomes across various tasks. This study seeks to extend the power of pretraining methodologies to tabular data, a domain traditionally overlooked, yet inherently challenging due to the plethora of table schemas intrinsic to different tasks. The primary research questions underpinning this work revolve around the adaptation to heterogeneous table structures, the establishment of a universal pretraining protocol for tabular data, the generalizability and transferability of learned knowledge across tasks, the adaptation to diverse downstream applications, and the incorporation of incremental columns over time. In response to these challenges, we introduce UniTabE, a pioneering method designed to process tables in a uniform manner, devoid of constraints imposed by specific table structures. UniTabE's core concept relies on representing each basic table element with a module, termed TabUnit. This is subsequently followed by a Transformer encoder to refine the representation. Moreover, our model is designed to facilitate pretraining and finetuning through the utilization of free-form prompts. In order to implement the pretraining phase, we curated an expansive tabular dataset comprising approximately 13 billion samples, meticulously gathered from the Kaggle platform. Rigorous experimental testing and analyses were performed under a myriad of scenarios to validate the effectiveness of our methodology. The experimental results demonstrate UniTabE's superior performance against several baseline models across a multitude of benchmark datasets. This, therefore, underscores UniTabE's potential to significantly enhance the semantic representation of tabular data, thereby marking a significant stride in the field of tabular data analysis.
</details>
<details>
<summary>摘要</summary>
近期的自然语言处理（NLP）技术发展，目睹了革命性的影响，在不同任务上呈现出卓越的表现。本研究旨在扩展预训练方法的应用范围，从传统上被忽视的表格数据中获得突破性的结果。本研究的主要研究问题包括适应不同表格结构、建立通用的预训练协议、学习知识的泛化和传递性、适应多样化下游应用和逐渐增加的列数据。为解决这些挑战，我们提出了UniTabE方法，可以统一处理表格数据，不受特定表格结构的限制。UniTabE的核心思想是通过表格元素模块（TabUnit）来表示每个基本表格元素，然后使用TransformerEncoder来细化表示。此外，我们的模型设计了适应预训练和精化的机制，通过自由形式的提示来进行。为实现预训练阶段，我们细心筛选了 approximate 130亿个样本的广泛表格数据集，从Kaggle平台收集。经过严谨的实验测试和分析，我们证明UniTabE方法在多个benchmark数据集上表现出优于多个基eline模型。这些结果证明UniTabE方法可以显著提高表格数据的semantic表示，从而在表格数据分析中做出重要突破。
</details></li>
</ul>
<hr>
<h2 id="Towards-Sustainable-Deep-Learning-for-Multi-Label-Classification-on-NILM"><a href="#Towards-Sustainable-Deep-Learning-for-Multi-Label-Classification-on-NILM" class="headerlink" title="Towards Sustainable Deep Learning for Multi-Label Classification on NILM"></a>Towards Sustainable Deep Learning for Multi-Label Classification on NILM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09244">http://arxiv.org/abs/2307.09244</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anže Pirnat, Blaž Bertalanič, Gregor Cerar, Mihael Mohorčič, Carolina Fortuna</li>
<li>for: 本研究旨在提高非侵入式电力监测（NILM）的计算和能量效率，并且使用深度学习（DL）技术进行多个标签分类。</li>
<li>methods: 本研究使用了一种新的DL模型，以提高NILM的计算和能量效率。此外，本研究还提出了一种测试方法ологи，用于比较不同模型的性能，该方法使用了从测量数据集Synthesized来更好地模拟实际情况。</li>
<li>results: 相比之前的状态艺术，提出的模型可以降低碳脚印的二十三分之一，并且在REFIT和UK-DALE数据集上测试时，其性能提高了大约8个百分点。<details>
<summary>Abstract</summary>
Non-intrusive load monitoring (NILM) is the process of obtaining appliance-level data from a single metering point, measuring total electricity consumption of a household or a business. Appliance-level data can be directly used for demand response applications and energy management systems as well as for awareness raising and motivation for improvements in energy efficiency and reduction in the carbon footprint. Recently, classical machine learning and deep learning (DL) techniques became very popular and proved as highly effective for NILM classification, but with the growing complexity these methods are faced with significant computational and energy demands during both their training and operation. In this paper, we introduce a novel DL model aimed at enhanced multi-label classification of NILM with improved computation and energy efficiency. We also propose a testing methodology for comparison of different models using data synthesized from the measurement datasets so as to better represent real-world scenarios. Compared to the state-of-the-art, the proposed model has its carbon footprint reduced by more than 23% while providing on average approximately 8 percentage points in performance improvement when testing on data derived from REFIT and UK-DALE datasets.
</details>
<details>
<summary>摘要</summary>
非侵入式电力监控（NILM）是从单一监控点获取家用电器或商业用电器的总电力消耗数据。家用电器或商业用电器的总电力消耗数据可以直接用于需求回应应用程序和能源管理系统，以及对能源效率和碳足迹的意识和鼓励。现在，经典机器学习和深度学习（DL）技术在NILM分类中成为非常受欢迎和高效的方法，但是随着模型的复杂度增加，它们面临训练和运行过程中的巨大计算和能源需求。本文提出了一个新的深度学习模型，以提高多标签分类的NILM性能，并提出了一个比较不同模型的试验方法，使用从测量数据中Synthesized的数据，以更好地反映实际情况。与现有的state-of-the-art相比，提出的模型可以降低碳踪数据的碳踪比例超过23%，并在REFIT和UK-DALE数据集上的测试中平均提高了约8%的性能。
</details></li>
</ul>
<hr>
<h2 id="De-Re-and-De-Dicto-Knowledge-in-Egocentric-Setting"><a href="#De-Re-and-De-Dicto-Knowledge-in-Egocentric-Setting" class="headerlink" title="De Re and De Dicto Knowledge in Egocentric Setting"></a>De Re and De Dicto Knowledge in Egocentric Setting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00001">http://arxiv.org/abs/2308.00001</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pavel Naumov, Anna Ovchinnikova</li>
<li>for: 本研究探讨了逻辑系统中的自我中心性，即研究代理人的性质而不是可能世界的性质。</li>
<li>methods: 本研究使用了两种不同的模态，分别表示de re和de dicto的知识，并证明了这两种模态之间不可definable。</li>
<li>results: 本研究证明了逻辑系统中的自我中心性不可definable，即不可以通过一种模式来表示另一种模式。<details>
<summary>Abstract</summary>
Prior proposes the term "egocentric" for logical systems that study properties of agents rather than properties of possible worlds. In such a setting, the paper introduces two different modalities capturing de re and de dicto knowledge and proves that these two modalities are not definable through each other.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Human-Body-Digital-Twin-A-Master-Plan"><a href="#Human-Body-Digital-Twin-A-Master-Plan" class="headerlink" title="Human Body Digital Twin: A Master Plan"></a>Human Body Digital Twin: A Master Plan</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09225">http://arxiv.org/abs/2307.09225</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenyu Tang, Shuo Gao, Luigi G. Occhipinti</li>
<li>for: 这篇论文旨在探讨人体数据技术（DT）的当前状况和未来发展前景，并提出了一个五级路图来引导未来的发展。</li>
<li>methods: 论文使用了各种技术和方法，包括智能监测器、数据收集、数据分析和决策系统的开发。</li>
<li>results: 论文提出了一个五级路图，这个路图覆盖了各种组件的开发，如智能监测器、数据收集、数据分析和决策系统的开发。<details>
<summary>Abstract</summary>
The human body DT has the potential to revolutionize healthcare and wellness, but its responsible and effective implementation requires consideration of various factors. This article presents a comprehensive overview of the current status and future prospects of the human body DT and proposes a five-level roadmap for its development. The roadmap covers the development of various components, such as wearable devices, data collection, data analysis, and decision-making systems. The article also highlights the necessary support, security, cost, and ethical considerations that must be addressed in order to ensure responsible and effective implementation of the human body DT. The proposed roadmap provides a framework for guiding future development and offers a unique perspective on the future of the human body DT, facilitating new interdisciplinary research and innovative solutions in this rapidly evolving field.
</details>
<details>
<summary>摘要</summary>
人体DT有可能革新医疗和健康领域，但其负责和有效实施需要考虑多种因素。这篇文章提供了人体DT当前状况和未来前景的全面概述，并提出了五级路线图，以帮助未来的发展。这个路线图覆盖了不同组件的开发，如便携设备、数据采集、数据分析和决策系统。文章还强调了必须解决的支持、安全、成本和伦理考虑，以确保负责和有效的人体DT实施。提议的路线图为未来发展提供了框架，并且降低了新兴领域的研究和创新的难度。
</details></li>
</ul>
<hr>
<h2 id="Automated-Ableism-An-Exploration-of-Explicit-Disability-Biases-in-Sentiment-and-Toxicity-Analysis-Models"><a href="#Automated-Ableism-An-Exploration-of-Explicit-Disability-Biases-in-Sentiment-and-Toxicity-Analysis-Models" class="headerlink" title="Automated Ableism: An Exploration of Explicit Disability Biases in Sentiment and Toxicity Analysis Models"></a>Automated Ableism: An Exploration of Explicit Disability Biases in Sentiment and Toxicity Analysis Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09209">http://arxiv.org/abs/2307.09209</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pranav Narayanan Venkit, Mukund Srinath, Shomir Wilson</li>
<li>for: 本研究使用 perturbation sensitivity analysis 方法检测社交媒体平台 Twitter 和 Reddit 上关于残疾人的对话中的隐含残疾偏见。</li>
<li>methods: 我们使用 BITS 集合（Bias Identification Test in Sentiment）来评估这些 sentiment analysis 和攻击性检测模型中的显式残疾偏见。</li>
<li>results: 我们的研究发现所有这些模型都表现出对残疾人的显式偏见。<details>
<summary>Abstract</summary>
We analyze sentiment analysis and toxicity detection models to detect the presence of explicit bias against people with disability (PWD). We employ the bias identification framework of Perturbation Sensitivity Analysis to examine conversations related to PWD on social media platforms, specifically Twitter and Reddit, in order to gain insight into how disability bias is disseminated in real-world social settings. We then create the \textit{Bias Identification Test in Sentiment} (BITS) corpus to quantify explicit disability bias in any sentiment analysis and toxicity detection models. Our study utilizes BITS to uncover significant biases in four open AIaaS (AI as a Service) sentiment analysis tools, namely TextBlob, VADER, Google Cloud Natural Language API, DistilBERT and two toxicity detection models, namely two versions of Toxic-BERT. Our findings indicate that all of these models exhibit statistically significant explicit bias against PWD.
</details>
<details>
<summary>摘要</summary>
我们分析情感分析和恶意检测模型，以检测对人际残疾（PWD）的直接偏见。我们使用干扰敏感分析框架来检查社交媒体平台上关于PWD的对话，以获得实际社会中残疾偏见的分析。然后，我们创建了《偏见标准测试集》（BITS），以量化任何情感分析和恶意检测模型中的直接残疾偏见。我们的研究使用BITS来揭露四个开源AIaaS（AI作为服务）情感分析工具——TextBlob、VADER、Google Cloud Natural Language API和DistilBERT——以及两个恶意检测模型——两个版本的Toxic-BERT——中的显著偏见。我们的发现表明所有这些模型都表现出了对PWD的 statistically significant direct bias。
</details></li>
</ul>
<hr>
<h2 id="ESMC-Entire-Space-Multi-Task-Model-for-Post-Click-Conversion-Rate-via-Parameter-Constraint"><a href="#ESMC-Entire-Space-Multi-Task-Model-for-Post-Click-Conversion-Rate-via-Parameter-Constraint" class="headerlink" title="ESMC: Entire Space Multi-Task Model for Post-Click Conversion Rate via Parameter Constraint"></a>ESMC: Entire Space Multi-Task Model for Post-Click Conversion Rate via Parameter Constraint</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09193">http://arxiv.org/abs/2307.09193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenhao Jiang, Biao Zeng, Hao Feng, Jin Liu, Jicong Fan, Jie Zhang, Jia Jia, Ning Hu, Xingyu Chen, Xuguang Lan</li>
<li>for: 这篇论文主要针对大规模在线推荐系统中的Click-Through Rate (CTR)和Post-Click Conversion Rate (CVR)估计问题。</li>
<li>methods: 该论文提出了一种基于Entire Space Model的多任务模型，通过跟踪用户决策过程“曝光_点击_购买”来解决传统CVR估计器存在样本选择偏见和数据稀缺问题。</li>
<li>results: 实验表明，提出的方法在大规模推荐系统中显著超过了现有方法的性能。此外，对实际世界数据集进行了证明和验证。<details>
<summary>Abstract</summary>
Large-scale online recommender system spreads all over the Internet being in charge of two basic tasks: Click-Through Rate (CTR) and Post-Click Conversion Rate (CVR) estimations. However, traditional CVR estimators suffer from well-known Sample Selection Bias and Data Sparsity issues. Entire space models were proposed to address the two issues via tracing the decision-making path of "exposure_click_purchase". Further, some researchers observed that there are purchase-related behaviors between click and purchase, which can better draw the user's decision-making intention and improve the recommendation performance. Thus, the decision-making path has been extended to "exposure_click_in-shop action_purchase" and can be modeled with conditional probability approach. Nevertheless, we observe that the chain rule of conditional probability does not always hold. We report Probability Space Confusion (PSC) issue and give a derivation of difference between ground-truth and estimation mathematically. We propose a novel Entire Space Multi-Task Model for Post-Click Conversion Rate via Parameter Constraint (ESMC) and two alternatives: Entire Space Multi-Task Model with Siamese Network (ESMS) and Entire Space Multi-Task Model in Global Domain (ESMG) to address the PSC issue. Specifically, we handle "exposure_click_in-shop action" and "in-shop action_purchase" separately in the light of characteristics of in-shop action. The first path is still treated with conditional probability while the second one is treated with parameter constraint strategy. Experiments on both offline and online environments in a large-scale recommendation system illustrate the superiority of our proposed methods over state-of-the-art models. The real-world datasets will be released.
</details>
<details>
<summary>摘要</summary>
大规模在互联网上的推荐系统遍布全网，负责两个基本任务：点击率（CTR）和后click conversión率（CVR）的估计。然而，传统的CVR估计器受到Well-knownSample Selection Bias和Data Sparsity问题的影响。Entire space模型被提出以解决这两个问题，通过跟踪用户做出决策的“曝光•点击•购买”的决策路径。此外，一些研究人员发现，在点击和购买之间存在购买相关行为，可以更好地捕捉用户做出决策的INTENTION，提高推荐性能。因此，决策路径被扩展为“曝光•点击•在店动作•购买”，可以通过条件概率方法模型。然而，我们发现链式法则不一定成立，我们报告了概率空间混乱（PSC）问题，并给出了数学上的解释。我们提出了一种新的Entire Space Multi-Task Model for Post-Click Conversion Rate via Parameter Constraint（ESMC），以及两种alternative：Entire Space Multi-Task Model with Siamese Network（ESMS）和Entire Space Multi-Task Model in Global Domain（ESMG），以解决PSC问题。具体来说，我们在“曝光•点击•在店动作”和“在店动作•购买”两个路径上处理它们分别，在它们的特点上进行处理。第一个路径仍然采用条件概率方法，第二个路径采用参数约束策略。在大规模推荐系统中进行了实验，我们的提出的方法在对state-of-the-art模型的比较中表现出色。真实的数据集将被发布。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Network-Slicing-Architectures-with-Machine-Learning-Security-Sustainability-and-Experimental-Networks-Integration"><a href="#Enhancing-Network-Slicing-Architectures-with-Machine-Learning-Security-Sustainability-and-Experimental-Networks-Integration" class="headerlink" title="Enhancing Network Slicing Architectures with Machine Learning, Security, Sustainability and Experimental Networks Integration"></a>Enhancing Network Slicing Architectures with Machine Learning, Security, Sustainability and Experimental Networks Integration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09151">http://arxiv.org/abs/2307.09151</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/romoreira/sfi2-energy-sustainability">https://github.com/romoreira/sfi2-energy-sustainability</a></li>
<li>paper_authors: Joberto S. B. Martins, Tereza C. Carvalho, Rodrigo Moreira, Cristiano Both, Adnei Donatti, João H. Corrêa, José A. Suruagy, Sand L. Corrêa, Antonio J. G. Abelem, Moisés R. N. Ribeiro, Jose-Marcos Nogueira, Luiz C. S. Magalhães, Juliano Wickboldt, Tiago Ferreto, Ricardo Mello, Rafael Pasquini, Marcos Schwarz, Leobino N. Sampaio, Daniel F. Macedo, José F. de Rezende, Kleber V. Cardoso, Flávio O. Silva<br>for:NS architectures are proposed to optimize and customize scarce resources for 5G&#x2F;6G applications, but they often have limited domain-specific functionality.methods:The SFI2 architecture proposal integrates experimental networks, enhances NS with ML native optimizations, energy-efficient slicing, and slicing-tailored security functionalities.results:The SFI2 reference architecture instantiations will enhance multi-domain and multi-technology integrated experimental network deployment with native ML optimization, energy-efficient aware slicing, and slicing-tailored security functionalities for practical domains.Here’s the Chinese text in the format you requested:for: NS architectures 是为了优化和自定义5G&#x2F;6G应用程序中的稀缺资源，但它们通常具有特定领域的限制性。methods: SFI2架构提案 integrates experimental networks, enhances NS with ML native optimizations, energy-efficient slicing, and slicing-tailored security functionalities.results: SFI2 reference architecture instantiations will enhance multi-domain and multi-technology integrated experimental network deployment with native ML optimization, energy-efficient aware slicing, and slicing-tailored security functionalities for practical domains.<details>
<summary>Abstract</summary>
Network Slicing (NS) is an essential technique extensively used in 5G networks computing strategies, mobile edge computing, mobile cloud computing, and verticals like the Internet of Vehicles and industrial IoT, among others. NS is foreseen as one of the leading enablers for 6G futuristic and highly demanding applications since it allows the optimization and customization of scarce and disputed resources among dynamic, demanding clients with highly distinct application requirements. Various standardization organizations, like 3GPP's proposal for new generation networks and state-of-the-art 5G/6G research projects, are proposing new NS architectures. However, new NS architectures have to deal with an extensive range of requirements that inherently result in having NS architecture proposals typically fulfilling the needs of specific sets of domains with commonalities. The Slicing Future Internet Infrastructures (SFI2) architecture proposal explores the gap resulting from the diversity of NS architectures target domains by proposing a new NS reference architecture with a defined focus on integrating experimental networks and enhancing the NS architecture with Machine Learning (ML) native optimizations, energy-efficient slicing, and slicing-tailored security functionalities. The SFI2 architectural main contribution includes the utilization of the slice-as-a-service paradigm for end-to-end orchestration of resources across multi-domains and multi-technology experimental networks. In addition, the SFI2 reference architecture instantiations will enhance the multi-domain and multi-technology integrated experimental network deployment with native ML optimization, energy-efficient aware slicing, and slicing-tailored security functionalities for the practical domain.
</details>
<details>
<summary>摘要</summary>
Slicing Future Internet Infrastructures (SFI2) 架构提案对 NS 架构的差异进行了探索，并提出了一个新的 NS 参考架构，旨在整合实验网络和增强 NS 架构，并将机器学习 (ML) 、能源效率和slice-tailored 安全功能纳入 NS 架构。SFI2 架构的主要贡献包括使用 slice-as-a-service 模式来实现端到端资源的整合和管理，以及对多域多技术的实验网络进行了Native ML优化、能源效率和slice-tailored 安全功能的增强。在实际领域中，SFI2 架构的实现将提供多域多技术集成的实验网络，并将Native ML优化、能源效率和slice-tailored 安全功能纳入 NS 架构。这将实现在多域多技术集成的实验网络中，实现了端到端资源的整合和管理，并提供了对特定领域的Native ML优化、能源效率和slice-tailored 安全功能。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-for-SAT-Restricted-Heuristics-and-New-Graph-Representations"><a href="#Machine-Learning-for-SAT-Restricted-Heuristics-and-New-Graph-Representations" class="headerlink" title="Machine Learning for SAT: Restricted Heuristics and New Graph Representations"></a>Machine Learning for SAT: Restricted Heuristics and New Graph Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09141">http://arxiv.org/abs/2307.09141</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mikhail Shirokikh, Ilya Shenbin, Anton Alekseev, Sergey Nikolenko</li>
<li>for: 这 paper 是解决Boolean satisfiability (SAT) 问题的基础NP-完全问题，包括自动规划和调度。</li>
<li>methods: 这 paper 使用机器学习 (ML) 模型来改进 SAT 解决器的规划策略，以减少步骤数和总时间。</li>
<li>results: 这 paper 提出了一种策略，在使用 ML 模型开始解决 SAT 问题后，将控制转交给经典的规划策略，以简化冷启动和减少总时间。此外，paper 还介绍了一种针对 SAT 问题转化自其他领域的 Graph-Q-SAT 修改。<details>
<summary>Abstract</summary>
Boolean satisfiability (SAT) is a fundamental NP-complete problem with many applications, including automated planning and scheduling. To solve large instances, SAT solvers have to rely on heuristics, e.g., choosing a branching variable in DPLL and CDCL solvers. Such heuristics can be improved with machine learning (ML) models; they can reduce the number of steps but usually hinder the running time because useful models are relatively large and slow. We suggest the strategy of making a few initial steps with a trained ML model and then releasing control to classical heuristics; this simplifies cold start for SAT solving and can decrease both the number of steps and overall runtime, but requires a separate decision of when to release control to the solver. Moreover, we introduce a modification of Graph-Q-SAT tailored to SAT problems converted from other domains, e.g., open shop scheduling problems. We validate the feasibility of our approach with random and industrial SAT problems.
</details>
<details>
<summary>摘要</summary>
布尔满足（SAT）是一个基本NP完全问题，具有许多应用，包括自动规划和计划。为解决大规模实例，SAT解决器需要依赖于规则，如选择分支变量在DPLL和CDCL解决器中。这些规则可以通过机器学习（ML）模型进行改进，它们可以减少步骤数量，但通常会增加运行时间，因为有用的模型通常比较大 и慢。我们建议一种策略，即在一个训练好的ML模型的几个初始步骤后，将控制转移给类别规则；这种策略可以简化冰封开始 дляSAT解决和降低总时间，但需要分离的决定何时将控制转移给解决器。此外，我们介绍了基于图形Query-SAT的修改，适用于从其他领域转化的SAT问题，如开放式制造计划问题。我们验证了我们的方法的可行性，使用随机和工业SAT问题。
</details></li>
</ul>
<hr>
<h2 id="DropMix-Reducing-Class-Dependency-in-Mixed-Sample-Data-Augmentation"><a href="#DropMix-Reducing-Class-Dependency-in-Mixed-Sample-Data-Augmentation" class="headerlink" title="DropMix: Reducing Class Dependency in Mixed Sample Data Augmentation"></a>DropMix: Reducing Class Dependency in Mixed Sample Data Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09136">http://arxiv.org/abs/2307.09136</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haeil Lee, Hansang Lee, Junmo Kim</li>
<li>for: 提高多种任务的性能，但有些类受到MSDA的影响而下降。</li>
<li>methods: 提议DropMix方法，通过排除MSDA计算中的特定百分比数据来减少类依赖性。</li>
<li>results: 在CIFAR-100和ImageNet两个数据集上，通过组合MSDA和非MSDA数据进行训练，提高了以前受到MSDA影响的类的性能，并将总平均准确率提高。<details>
<summary>Abstract</summary>
Mixed sample data augmentation (MSDA) is a widely used technique that has been found to improve performance in a variety of tasks. However, in this paper, we show that the effects of MSDA are class-dependent, with some classes seeing an improvement in performance while others experience a decline. To reduce class dependency, we propose the DropMix method, which excludes a specific percentage of data from the MSDA computation. By training on a combination of MSDA and non-MSDA data, the proposed method not only improves the performance of classes that were previously degraded by MSDA, but also increases overall average accuracy, as shown in experiments on two datasets (CIFAR-100 and ImageNet) using three MSDA methods (Mixup, CutMix and PuzzleMix).
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Cloud-native-RStudio-on-Kubernetes-for-Hopsworks"><a href="#Cloud-native-RStudio-on-Kubernetes-for-Hopsworks" class="headerlink" title="Cloud-native RStudio on Kubernetes for Hopsworks"></a>Cloud-native RStudio on Kubernetes for Hopsworks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09132">http://arxiv.org/abs/2307.09132</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gibson Chikafa, Sina Sheikholeslami, Salman Niazi, Jim Dowling, Vladimir Vlassov<br>for:这篇论文主要目标是提供一个基于多户分布式系统的RStudio Server，以提高云计算中R语言开发环境的可用性和可扩展性。methods: authors使用了 Docker和Kubernetes等云原生技术来解决多户分布式环境中的性能隔离、安全性和扩展性问题。他们还实现了在RStudio Server实例中安全地共享数据，以便在RStudio用户之间进行数据共享和协作。results: authors在Google云平台上测试了他们的系统，并证明了可以同时运行44个RStudio Server实例，每个实例具有2GB的RAM。此外，他们的系统可以扩展到支持数百个同时运行的RStudio Server实例，只需要添加更多资源（CPU和RAM）到集群或系统中即可。<details>
<summary>Abstract</summary>
In order to fully benefit from cloud computing, services are designed following the "multi-tenant" architectural model, which is aimed at maximizing resource sharing among users. However, multi-tenancy introduces challenges of security, performance isolation, scaling, and customization. RStudio server is an open-source Integrated Development Environment (IDE) accessible over a web browser for the R programming language. We present the design and implementation of a multi-user distributed system on Hopsworks, a data-intensive AI platform, following the multi-tenant model that provides RStudio as Software as a Service (SaaS). We use the most popular cloud-native technologies: Docker and Kubernetes, to solve the problems of performance isolation, security, and scaling that are present in a multi-tenant environment. We further enable secure data sharing in RStudio server instances to provide data privacy and allow collaboration among RStudio users. We integrate our system with Apache Spark, which can scale and handle Big Data processing workloads. Also, we provide a UI where users can provide custom configurations and have full control of their own RStudio server instances. Our system was tested on a Google Cloud Platform cluster with four worker nodes, each with 30GB of RAM allocated to them. The tests on this cluster showed that 44 RStudio servers, each with 2GB of RAM, can be run concurrently. Our system can scale out to potentially support hundreds of concurrently running RStudio servers by adding more resources (CPUs and RAM) to the cluster or system.
</details>
<details>
<summary>摘要</summary>
为了完全利用云计算，服务采用“多户”建筑模型，以最大化用户之间资源共享。然而，多户性带来安全性、性能隔离、扩展和自定义等挑战。RStudio服务器是一个开源的集成开发环境（IDE），可以通过网页浏览器访问R编程语言。我们提出了在Hopsworks上实现多用户分布式系统，以多户模型提供RStudio作为服务（SaaS）。我们使用云原生技术：Docker和Kubernetes解决多户环境中存在的性能隔离、安全性和扩展等问题。此外，我们还启用了安全数据共享在RStudio服务器实例中，以保障数据隐私和允许RStudio用户之间的合作。我们将系统与Apache Spark集成，可以扩展和处理大规模数据处理任务。此外，我们还提供了用户可定制配置和完全控制自己的RStudio服务器实例的UI。我们的系统在Google云平台集群上进行了测试，每个工作节点有30GB的RAM，并测试了44个RStudio服务器，每个服务器具有2GB的RAM。我们的系统可以扩展到可能支持百个同时运行的RStudio服务器，通过添加更多资源（CPU和RAM）到集群或系统。
</details></li>
</ul>
<hr>
<h2 id="BOLD-A-Benchmark-for-Linked-Data-User-Agents-and-a-Simulation-Framework-for-Dynamic-Linked-Data-Environments"><a href="#BOLD-A-Benchmark-for-Linked-Data-User-Agents-and-a-Simulation-Framework-for-Dynamic-Linked-Data-Environments" class="headerlink" title="BOLD: A Benchmark for Linked Data User Agents and a Simulation Framework for Dynamic Linked Data Environments"></a>BOLD: A Benchmark for Linked Data User Agents and a Simulation Framework for Dynamic Linked Data Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09114">http://arxiv.org/abs/2307.09114</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tobias Käfer, Victor Charpenay, Andreas Harth</li>
<li>for: 本研究为Linked Data代理人提供了BOLD（Buildings on Linked Data）benchmark，以及一个用于模拟动态Linked Data环境的框架。</li>
<li>methods: 本研究使用了一个读写Linked Data接口来模拟一座智能建筑，包括互联网时间、占用情况和感知器等。</li>
<li>results: 在Linked Data表示环境下，代理人执行了一些具体任务，如灯光控制，并且通过测试环境来检查任务执行是否正确，以及代理人性能的测量。<details>
<summary>Abstract</summary>
The paper presents the BOLD (Buildings on Linked Data) benchmark for Linked Data agents, next to the framework to simulate dynamic Linked Data environments, using which we built BOLD. The BOLD benchmark instantiates the BOLD framework by providing a read-write Linked Data interface to a smart building with simulated time, occupancy movement and sensors and actuators around lighting. On the Linked Data representation of this environment, agents carry out several specified tasks, such as controlling illumination. The simulation environment provides means to check for the correct execution of the tasks and to measure the performance of agents. We conduct measurements on Linked Data agents based on condition-action rules.
</details>
<details>
<summary>摘要</summary>
文章介绍了BOLD（建筑物 Linked Data）benchmark，用于测试链接数据代理人的能力，同时还提供了模拟动态链接数据环境的框架。BOLD benchmark通过提供一个阅读写链接数据接口，使用模拟时间、占用情况和感知器等来模拟智能建筑环境。在这个链接数据表示中，代理人执行了一些Specified任务，如控制照明。模拟环境提供了检查任务执行是否正确的方式和评估代理人性能的方法。我们对基于条件动作规则的链接数据代理人进行了测量。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Multi-Objective-Neural-Architecture-Search"><a href="#A-Survey-on-Multi-Objective-Neural-Architecture-Search" class="headerlink" title="A Survey on Multi-Objective Neural Architecture Search"></a>A Survey on Multi-Objective Neural Architecture Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09099">http://arxiv.org/abs/2307.09099</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seyed Mahdi Shariatzadeh, Mahmood Fathy, Reza Berangi, Mohammad Shahverdy</li>
<li>for: 本文提供了针对多目标神经网络搜索（MONAS）领域的概述，包括主要工作和现状报告，以及未来发展方向。</li>
<li>methods: 本文使用了多目标神经网络搜索（MONAS）的主要方法，包括分类和排序、搜索空间和搜索策略等方面的研究。</li>
<li>results: 本文通过分析多个目标的关系和优化，揭示了MONAS在不同领域的应用和优势。同时，还提出了未来发展方向和挑战。<details>
<summary>Abstract</summary>
Recently, the expert-crafted neural architectures is increasing overtaken by the utilization of neural architecture search (NAS) and automatic generation (and tuning) of network structures which has a close relation to the Hyperparameter Optimization and Auto Machine Learning (AutoML). After the earlier NAS attempts to optimize only the prediction accuracy, Multi-Objective Neural architecture Search (MONAS) has been attracting attentions which considers more goals such as computational complexity, power consumption, and size of the network for optimization, reaching a trade-off between the accuracy and other features like the computational cost. In this paper, we present an overview of principal and state-of-the-art works in the field of MONAS. Starting from a well-categorized taxonomy and formulation for the NAS, we address and correct some miscategorizations in previous surveys of the NAS field. We also provide a list of all known objectives used and add a number of new ones and elaborate their specifications. We have provides analyses about the most important objectives and shown that the stochastic properties of some the them should be differed from deterministic ones in the multi-objective optimization procedure of NAS. We finalize this paper with a number of future directions and topics in the field of MONAS.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="DiTTO-Diffusion-inspired-Temporal-Transformer-Operator"><a href="#DiTTO-Diffusion-inspired-Temporal-Transformer-Operator" class="headerlink" title="DiTTO: Diffusion-inspired Temporal Transformer Operator"></a>DiTTO: Diffusion-inspired Temporal Transformer Operator</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09072">http://arxiv.org/abs/2307.09072</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oded Ovadia, Eli Turkel, Adar Kahana, George Em Karniadakis</li>
<li>for: 解决时间依赖的partial differential equations (PDEs) 问题，使用数据驱动的方法。</li>
<li>methods: 提出了一种基于操作学习概念的方法，称为DiTTO，它采用了潜在扩散模型的时间条件机制，并结合了Transformer架构的元素以提高其能力。</li>
<li>results: 对多维PDE问题进行了广泛的测试，包括1-D Burgers’方程、2-D Navier-Stokes方程以及2-D和3-D的声波方程，DiTTO得到了最佳的精度 результа。此外，还提出了一种使用快速抽象概念来提高DiTTO的性能的方法，并证明了DiTTO可以准确地进行零shot超解析。<details>
<summary>Abstract</summary>
Solving partial differential equations (PDEs) using a data-driven approach has become increasingly common. The recent development of the operator learning paradigm has enabled the solution of a broader range of PDE-related problems. We propose an operator learning method to solve time-dependent PDEs continuously in time without needing any temporal discretization. The proposed approach, named DiTTO, is inspired by latent diffusion models. While diffusion models are usually used in generative artificial intelligence tasks, their time-conditioning mechanism is extremely useful for PDEs. The diffusion-inspired framework is combined with elements from the Transformer architecture to improve its capabilities.   We demonstrate the effectiveness of the new approach on a wide variety of PDEs in multiple dimensions, namely the 1-D Burgers' equation, 2-D Navier-Stokes equations, and the acoustic wave equation in 2-D and 3-D. DiTTO achieves state-of-the-art results in terms of accuracy for these problems. We also present a method to improve the performance of DiTTO by using fast sampling concepts from diffusion models. Finally, we show that DiTTO can accurately perform zero-shot super-resolution in time.
</details>
<details>
<summary>摘要</summary>
解决部分梯度方程（PDE）使用数据驱动方法已成为日益普遍。最近的运算学学 paradigm的发展已经使得解决更广泛的 PDE 相关问题变得可能。我们提议一种运算学学方法，名为DiTTO，可以不需要任何时间细分化地解决时间相依的 PDE。我们的方法受某种扩散模型的启发，而这种模型通常用于生成人工智能任务中。扩散模型在时间条件下的机制非常有用于 PDE。我们将扩散模型与Transformer架构的元素组合以提高其能力。我们在多维度中的一维布格尔方程、二维 Navier-Stokes 方程以及二维和三维的声波方程上进行了广泛的试验。DiTTO 在这些问题上达到了最新的精度标准。我们还提出了使用 diffusion 模型快速采样概念来提高 DiTTO 的性能的方法。最后，我们展示了 DiTTO 可以高精度地进行零shot超解析。
</details></li>
</ul>
<hr>
<h2 id="Unleashing-the-Imagination-of-Text-A-Novel-Framework-for-Text-to-image-Person-Retrieval-via-Exploring-the-Power-of-Words"><a href="#Unleashing-the-Imagination-of-Text-A-Novel-Framework-for-Text-to-image-Person-Retrieval-via-Exploring-the-Power-of-Words" class="headerlink" title="Unleashing the Imagination of Text: A Novel Framework for Text-to-image Person Retrieval via Exploring the Power of Words"></a>Unleashing the Imagination of Text: A Novel Framework for Text-to-image Person Retrieval via Exploring the Power of Words</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09059">http://arxiv.org/abs/2307.09059</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Delong-liu-bupt/UIT">https://github.com/Delong-liu-bupt/UIT</a></li>
<li>paper_authors: Delong Liu, Haiwen Li</li>
<li>for: 这个论文主要针对文本描述和图像之间的对应关系进行研究，以实现文本描述与图像的对应。</li>
<li>methods: 该论文提出了一种新的框架，名为“允许文本 imagination 激活”（UIT），以便充分利用文本描述中的表达力。该框架使用预训练的全 CLIP 模型作为图像和文本的双encoder，利用先前的交叉模态对齐知识。此外，该论文还提出了一种文本指导图像修复 auxiliary task，以帮助将抽象的文本描述映射到具体的图像区域。</li>
<li>results: 该论文的提议方法在三个popular benchmark dataset上实现了状态的最佳 result，并将源代码公开发布在短时间内。<details>
<summary>Abstract</summary>
The goal of Text-to-image person retrieval is to retrieve person images from a large gallery that match the given textual descriptions. The main challenge of this task lies in the significant differences in information representation between the visual and textual modalities. The textual modality conveys abstract and precise information through vocabulary and grammatical structures, while the visual modality conveys concrete and intuitive information through images. To fully leverage the expressive power of textual representations, it is essential to accurately map abstract textual descriptions to specific images.   To address this issue, we propose a novel framework to Unleash the Imagination of Text (UIT) in text-to-image person retrieval, aiming to fully explore the power of words in sentences. Specifically, the framework employs the pre-trained full CLIP model as a dual encoder for the images and texts , taking advantage of prior cross-modal alignment knowledge. The Text-guided Image Restoration auxiliary task is proposed with the aim of implicitly mapping abstract textual entities to specific image regions, facilitating alignment between textual and visual embeddings. Additionally, we introduce a cross-modal triplet loss tailored for handling hard samples, enhancing the model's ability to distinguish minor differences.   To focus the model on the key components within sentences, we propose a novel text data augmentation technique. Our proposed methods achieve state-of-the-art results on three popular benchmark datasets, and the source code will be made publicly available shortly.
</details>
<details>
<summary>摘要</summary>
文本到图像人识别的目标是从大量图库中 retrieve 匹配给定的文本描述图像。主要挑战在于视觉和文本modalities之间的信息表示有所不同。文本modalities通过语言和 grammatical structure 提供 precises 和抽象的信息，而视觉modalities通过图像提供具体和直观的信息。为了充分利用文本表示的表达力，需要准确地将抽象的文本描述映射到具体的图像。为解决这个问题，我们提出了一种新的 Unleash the Imagination of Text 框架（UIT），目的是全面利用文本表示的力量。具体来说，该框架使用预训练的 full CLIP 模型作为图像和文本的双Encoder，利用对modalities的先前交叉Alignment知识。我们还提出了 Text-guided Image Restoration 辅助任务，以帮助将抽象的文本实体映射到特定的图像区域，从而使得文本和视觉嵌入匹配。此外，我们还引入了特定 для处理困难样本的交叉模式 triplet loss，提高模型对微妙差别的识别能力。为了聚焦模型在句子中的关键组件，我们提出了一种新的文本数据增强技术。我们的提出的方法在三个流行的benchmark dataset上达到了状态精度，源代码即将公开。
</details></li>
</ul>
<hr>
<h2 id="QMNet-Importance-Aware-Message-Exchange-for-Decentralized-Multi-Agent-Reinforcement-Learning"><a href="#QMNet-Importance-Aware-Message-Exchange-for-Decentralized-Multi-Agent-Reinforcement-Learning" class="headerlink" title="QMNet: Importance-Aware Message Exchange for Decentralized Multi-Agent Reinforcement Learning"></a>QMNet: Importance-Aware Message Exchange for Decentralized Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09051">http://arxiv.org/abs/2307.09051</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiufeng Huang, Sheng Zhou</li>
<li>for: 提高多智能体强化学习下的无线资源约束下的性能</li>
<li>methods: 提出了一种消息重要性度量和一种重要度意识的调度策略，使得agent可以更有效地交换消息，并且利用消息重要性度量来处理随机访问冲突。</li>
<li>results: 在交通拐点环境中评估了提议的方案，发现可以在仅30%的agent可以发送消息的情况下提高系统性能，并且通过消息预测机制可以进一步降低无线资源的使用率。<details>
<summary>Abstract</summary>
To improve the performance of multi-agent reinforcement learning under the constraint of wireless resources, we propose a message importance metric and design an importance-aware scheduling policy to effectively exchange messages. The key insight is spending the precious communication resources on important messages. The message importance depends not only on the messages themselves, but also on the needs of agents who receive them. Accordingly, we propose a query-message-based architecture, called QMNet. Agents generate queries and messages with the environment observation. Sharing queries can help calculate message importance. Exchanging messages can help agents cooperate better. Besides, we exploit the message importance to deal with random access collisions in decentralized systems. Furthermore, a message prediction mechanism is proposed to compensate for messages that are not transmitted. Finally, we evaluate the proposed schemes in a traffic junction environment, where only a fraction of agents can send messages due to limited wireless resources. Results show that QMNet can extract valuable information to guarantee the system performance even when only $30\%$ of agents can share messages. By exploiting message prediction, the system can further save $40\%$ of wireless resources. The importance-aware decentralized multi-access mechanism can effectively avoid collisions, achieving almost the same performance as centralized scheduling.
</details>
<details>
<summary>摘要</summary>
translate into Simplified Chinese:为了提高多机器人学习下限制无线资源的性能，我们提出了消息重要度度量和重要度感知调度策略，以有效地交换消息。关键发现是在有限的无线资源下花费珍贵的通信资源。消息重要度取决于消息本身，以及接收者需要。我们提出了问题消息架构，称为QMNet。代理人根据环境观察生成问题和消息。共享问题可以帮助计算消息重要度。交换消息可以帮助代理人合作更好。此外，我们利用消息重要度来处理分布式系统中的随机访问冲突。此外，我们还提出了消息预测机制，以补偿未传输消息。最后，我们在交通枢纽环境中评估我们的方案，其中仅有一部分代理人可以发送消息due to limited wireless resources.结果表明，QMNet可以提取有价值的信息，以保证系统性能，即使仅有30%的代理人可以分享消息。通过利用消息预测，系统可以进一步降低40%的无线资源。我们的重要度感知分布式多访问机制可以有效避免冲突，实现与中央调度相似的性能。
</details></li>
</ul>
<hr>
<h2 id="R-Cut-Enhancing-Explainability-in-Vision-Transformers-with-Relationship-Weighted-Out-and-Cut"><a href="#R-Cut-Enhancing-Explainability-in-Vision-Transformers-with-Relationship-Weighted-Out-and-Cut" class="headerlink" title="R-Cut: Enhancing Explainability in Vision Transformers with Relationship Weighted Out and Cut"></a>R-Cut: Enhancing Explainability in Vision Transformers with Relationship Weighted Out and Cut</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09050">http://arxiv.org/abs/2307.09050</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yingjie Niu, Ming Ding, Maoning Ge, Robin Karlsson, Yuxiao Zhang, Kazuya Takeda</li>
<li>for: 这paper的目的是提高Transformer模型的可解释性，帮助用户更深入理解模型在图像分类任务中的决策过程。</li>
<li>methods: 这paper提出了两个模块来提高可解释性：“Relationship Weighted Out”模块和“Cut”模块。“Relationship Weighted Out”模块通过提取中间层的类别特征来强调相关的信息，而“Cut”模块则进行细腻的特征分解，考虑因素如位置、文本ure、颜色等。通过 integrate these two modules, we generate dense class-specific visual explainability maps。</li>
<li>results: 该paper通过了extensive的量化和质量性实验，证明了其方法在ImageNet dataset上的效果明显提高，并在LRN dataset上进行了大量的实验，以评估其方法在复杂背景下的可解释性。结果表明其方法在这些场景下具有显著的改善。此外，paper还进行了简要的抑制实验，以验证每个模块的效果，并证明了它们的共同作用。<details>
<summary>Abstract</summary>
Transformer-based models have gained popularity in the field of natural language processing (NLP) and are extensively utilized in computer vision tasks and multi-modal models such as GPT4. This paper presents a novel method to enhance the explainability of Transformer-based image classification models. Our method aims to improve trust in classification results and empower users to gain a deeper understanding of the model for downstream tasks by providing visualizations of class-specific maps. We introduce two modules: the ``Relationship Weighted Out" and the ``Cut" modules. The ``Relationship Weighted Out" module focuses on extracting class-specific information from intermediate layers, enabling us to highlight relevant features. Additionally, the ``Cut" module performs fine-grained feature decomposition, taking into account factors such as position, texture, and color. By integrating these modules, we generate dense class-specific visual explainability maps. We validate our method with extensive qualitative and quantitative experiments on the ImageNet dataset. Furthermore, we conduct a large number of experiments on the LRN dataset, specifically designed for automatic driving danger alerts, to evaluate the explainability of our method in complex backgrounds. The results demonstrate a significant improvement over previous methods. Moreover, we conduct ablation experiments to validate the effectiveness of each module. Through these experiments, we are able to confirm the respective contributions of each module, thus solidifying the overall effectiveness of our proposed approach.
</details>
<details>
<summary>摘要</summary>
带有变换器基础的模型在自然语言处理（NLP）领域得到广泛应用，同时也在计算机视觉任务和多模态模型中得到广泛应用，如GPT4。这篇论文提出了一种新的方法来提高变换器基础的图像分类模型的可解释性。我们的方法旨在提高分类结果的信任度和让用户更深入地理解模型，以便在下游任务中获得更多的信息。我们提出了两个模块：“关系权重外”模块和“割”模块。“关系权重外”模块专注于从中间层提取类别特定的信息，使得我们能够高亮相关的特征。而“割”模块对细化特征进行了分析，考虑了位置、xture、颜色等因素。通过将这两个模块集成，我们生成了密集的类别特定的可解释性图。我们验证了我们的方法通过大量的质量和量测试在ImageNet数据集上，并在LRN数据集上进行了大量的实验，以评估我们的方法在复杂背景下的可解释性。结果表明我们的方法与之前的方法相比有了显著的改善。此外，我们还进行了减少模块的实验，以验证每个模块的各自贡献，从而确认整体方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="FedDefender-Client-Side-Attack-Tolerant-Federated-Learning"><a href="#FedDefender-Client-Side-Attack-Tolerant-Federated-Learning" class="headerlink" title="FedDefender: Client-Side Attack-Tolerant Federated Learning"></a>FedDefender: Client-Side Attack-Tolerant Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09048">http://arxiv.org/abs/2307.09048</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/deu30303/feddefender">https://github.com/deu30303/feddefender</a></li>
<li>paper_authors: Sungwon Park, Sungwon Han, Fangzhao Wu, Sundong Kim, Bin Zhu, Xing Xie, Meeyoung Cha</li>
<li>for: 防止模型毒素攻击，增强 federated learning 的鲁棒性</li>
<li>methods: 客户端端Side defense 策略，包括攻击忍受本地模型更新和全局知识填充</li>
<li>results: 在实际场景中评估多个数据集，结果表明提议的方法可以增强 federated learning 对模型毒素攻击的鲁棒性。<details>
<summary>Abstract</summary>
Federated learning enables learning from decentralized data sources without compromising privacy, which makes it a crucial technique. However, it is vulnerable to model poisoning attacks, where malicious clients interfere with the training process. Previous defense mechanisms have focused on the server-side by using careful model aggregation, but this may not be effective when the data is not identically distributed or when attackers can access the information of benign clients. In this paper, we propose a new defense mechanism that focuses on the client-side, called FedDefender, to help benign clients train robust local models and avoid the adverse impact of malicious model updates from attackers, even when a server-side defense cannot identify or remove adversaries. Our method consists of two main components: (1) attack-tolerant local meta update and (2) attack-tolerant global knowledge distillation. These components are used to find noise-resilient model parameters while accurately extracting knowledge from a potentially corrupted global model. Our client-side defense strategy has a flexible structure and can work in conjunction with any existing server-side strategies. Evaluations of real-world scenarios across multiple datasets show that the proposed method enhances the robustness of federated learning against model poisoning attacks.
</details>
<details>
<summary>摘要</summary>
federated learning 可以从分散式数据来源学习而不需要遗失隐私，这使其成为一种重要的技术。然而，它受到模型毒化攻击的威胁，恶意客户端可以在训练过程中干扰。先前的防御机制将重点放在服务器端，使用精确的模型聚合，但这可能无法有效地防止数据不对称或攻击者可以访问正常客户端的资讯。在这篇论文中，我们提出了一个新的防御机制，将重点放在客户端，称为FedDefender，以帮助正常的客户端训练稳定的地方模型，并避免由攻击者发送的错误模型更新对正常客户端的影响。我们的方法包括两个主要的元件：（1）攻击忍耐的地方元更新和（2）攻击忍耐的全球知识传递。这两个元件用于找到防护感知的模型参数，并精确地传递全球模型中的知识。我们的客户端防御策略具有可以与任何现有的服务器端策略结合使用的灵活结构。实际应用数据显示，提案的方法可以增强 Federated Learning 中的模型毒化攻击防御能力。
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Machine-Learning-for-Extraction-of-Theorems-and-Proofs-in-the-Scientific-Literature"><a href="#Multimodal-Machine-Learning-for-Extraction-of-Theorems-and-Proofs-in-the-Scientific-Literature" class="headerlink" title="Multimodal Machine Learning for Extraction of Theorems and Proofs in the Scientific Literature"></a>Multimodal Machine Learning for Extraction of Theorems and Proofs in the Scientific Literature</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09047">http://arxiv.org/abs/2307.09047</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mv96/mm_extraction">https://github.com/mv96/mm_extraction</a></li>
<li>paper_authors: Shrey Mishra, Antoine Gauquier, Pierre Senellart</li>
<li>for: 本研究旨在提取数学文章中的定理和证明，使用多modal类型的机器学习方法。</li>
<li>methods: 本研究使用文本、字体特征和bitmap图像的多modal特征，并通过晚期结合特征进行融合，以提取定理和证明。文本模块使用自然语言模型进行预训练，并在小规模数据上进行微调。字体特征使用长短折衔LSTM模型，而bitmap图像使用EfficientNetv2深度网络进行处理。最后，使用CRF模型将多modal特征与块序列信息结合。</li>
<li>results: 实验结果表明，使用多modal方法比使用单modal方法有较好的性能，并且使用CRF模型可以提高性能。<details>
<summary>Abstract</summary>
Scholarly articles in mathematical fields feature mathematical statements such as theorems, propositions, etc., as well as their proofs. Extracting them from the PDF representation of the articles requires understanding of scientific text along with visual and font-based indicators. We pose this problem as a multimodal classification problem using text, font features, and bitmap image rendering of the PDF as different modalities. In this paper we propose a multimodal machine learning approach for extraction of theorem-like environments and proofs, based on late fusion of features extracted by individual unimodal classifiers, taking into account the sequential succession of blocks in the document. For the text modality, we pretrain a new language model on a 11 GB scientific corpus; experiments shows similar performance for our task than a model (RoBERTa) pretrained on 160 GB, with faster convergence while requiring much less fine-tuning data. Font-based information relies on training a 128-cell LSTM on the sequence of font names and sizes within each block. Bitmap renderings are dealt with using an EfficientNetv2 deep network tuned to classify each image block. Finally, a simple CRF-based approach uses the features of the multimodal model along with information on block sequences. Experimental results show the benefits of using a multimodal approach vs any single modality, as well as major performance improvements using the CRF modeling of block sequences.
</details>
<details>
<summary>摘要</summary>
学术论文在数学领域中经常包含数学陈述，如定理、命题等，以及其证明。从PDF文档中提取这些陈述和证明需要科学文本的理解以及视觉和字体指示器。我们将这个问题作为多modal分类问题来处理，使用文本、字体特征和Bitmap图像渲染作为不同的Modalities。在这篇论文中，我们提出了一种多modal机器学习方法，用于提取定理-like环境和证明，基于延迟融合多modal特征扩展的方法。文本模式下，我们预训练了一个新的语言模型，使用11GB的科学 corpus;实验表明，我们的任务性能与使用160GB的RoBERTa模型相似，而且具有更快的融合速度和需要的精度训练数据更少。基于字体信息，我们使用128个LSTM单元训练字体名称和大小序列内每个块的模型。Bitmap渲染方面，我们使用EfficientNetv2深度网络，对每个图像块进行分类。最后，我们使用一个简单的CRF模型，使用多modal模型的特征以及块序列信息。实验结果表明，使用多modal方法比使用单一模式有更大的优势，以及使用CRF模型对块序列信息的处理可以提高性能。
</details></li>
</ul>
<hr>
<h2 id="Emotional-Intelligence-of-Large-Language-Models"><a href="#Emotional-Intelligence-of-Large-Language-Models" class="headerlink" title="Emotional Intelligence of Large Language Models"></a>Emotional Intelligence of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09042">http://arxiv.org/abs/2307.09042</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuena Wang, Xueting Li, Zi Yin, Yue Wu, Liu Jia</li>
<li>for: 这研究是为了评估大语言模型（LLMs）在情感智能方面的能力，以及这些模型是否与人类情感和价值观Alignment。</li>
<li>methods: 研究人员首先开发了一种新的心理测试，旨在评估大语言模型情感理解能力，这种测试包括识别、解释和理解复杂情感的能力。此外，研究人员还使用了一个参照框架， constructed from over 500 adults，来测试不同的主流大语言模型。</li>
<li>results: 研究结果表明，大多数大语言模型在情感理解方面的能力强于人类平均水平，其中GPT-4的情感智能指数（EQ）达到了89%的人类参与者水平。此外，一种多变量模式分析发现，一些大语言模型并没有采用人类类似的机制来实现人类水平的表现，其表达模式与人类有所不同。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have demonstrated remarkable abilities across numerous disciplines, primarily assessed through tasks in language generation, knowledge utilization, and complex reasoning. However, their alignment with human emotions and values, which is critical for real-world applications, has not been systematically evaluated. Here, we assessed LLMs' Emotional Intelligence (EI), encompassing emotion recognition, interpretation, and understanding, which is necessary for effective communication and social interactions. Specifically, we first developed a novel psychometric assessment focusing on Emotion Understanding (EU), a core component of EI, suitable for both humans and LLMs. This test requires evaluating complex emotions (e.g., surprised, joyful, puzzled, proud) in realistic scenarios (e.g., despite feeling underperformed, John surprisingly achieved a top score). With a reference frame constructed from over 500 adults, we tested a variety of mainstream LLMs. Most achieved above-average EQ scores, with GPT-4 exceeding 89% of human participants with an EQ of 117. Interestingly, a multivariate pattern analysis revealed that some LLMs apparently did not reply on the human-like mechanism to achieve human-level performance, as their representational patterns were qualitatively distinct from humans. In addition, we discussed the impact of factors such as model size, training method, and architecture on LLMs' EQ. In summary, our study presents one of the first psychometric evaluations of the human-like characteristics of LLMs, which may shed light on the future development of LLMs aiming for both high intellectual and emotional intelligence. Project website: https://emotional-intelligence.github.io/
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs）已经展现出了很多领域的出色表现，主要是通过语言生成、知识利用和复杂的推理等任务评估。然而，它们与人类情感和价值观Alignment不够系统地评估。在这里，我们评估了LLMs的情感智能（EI），包括情感认知、解释和理解，这些是实际应用中的重要 фактор。具体来说，我们开发了一种新的心理测试，旨在评估LLMs的情感理解能力（EU），这是EI的核心组成部分。这个测试需要评估复杂情感（例如，感到受到了不公平待遇，然而突然获得了优秀成绩）在真实的场景中。我们使用了来自500多名成年人的参考框架，测试了多种主流LLMs。大多数LLMs的EQ分数高于人类参与者的平均分数，GPT-4的EQ分数达117，超过89%的人类参与者。另外，我们通过多变量模式分析发现，一些LLMs并不是通过人类类似的机制来实现人类水平的表现，其表示方式与人类有所不同。此外，我们还讨论了因素 such as模型大小、训练方法和架构对LLMs的EQ具有影响。总之，我们的研究提供了一个可能是LLMs寻求高智商和情感智能的未来发展的首个心理测试。项目网站：<https://emotional-intelligence.github.io/>
</details></li>
</ul>
<hr>
<h2 id="PromptMagician-Interactive-Prompt-Engineering-for-Text-to-Image-Creation"><a href="#PromptMagician-Interactive-Prompt-Engineering-for-Text-to-Image-Creation" class="headerlink" title="PromptMagician: Interactive Prompt Engineering for Text-to-Image Creation"></a>PromptMagician: Interactive Prompt Engineering for Text-to-Image Creation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09036">http://arxiv.org/abs/2307.09036</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yingchaojiefeng/promptmagician">https://github.com/yingchaojiefeng/promptmagician</a></li>
<li>paper_authors: Yingchaojie Feng, Xingbo Wang, Kam Kwai Wong, Sijia Wang, Yuhong Lu, Minfeng Zhu, Baicheng Wang, Wei Chen</li>
<li>for: 该论文旨在提供一种可视化分析系统，帮助用户探索与输入提示相关的图像结果，并进行个性化的提示细化。</li>
<li>methods: 该系统基于一种提示建议模型，通过对用户提示进行分析，从DiffusionDB中检索相似的提示-图像对，并将特殊提示关键词标出。系统还提供了多级可视化工具，以便用户在跨模态空间中进行交互式的提示细化。</li>
<li>results: 两个使用场景、一个用户研究和专家采访表明，PromptMagician系统能够有效地支持用户在生成文本到图像模型中进行提示工程，并且提高了这种创造支持的效果。<details>
<summary>Abstract</summary>
Generative text-to-image models have gained great popularity among the public for their powerful capability to generate high-quality images based on natural language prompts. However, developing effective prompts for desired images can be challenging due to the complexity and ambiguity of natural language. This research proposes PromptMagician, a visual analysis system that helps users explore the image results and refine the input prompts. The backbone of our system is a prompt recommendation model that takes user prompts as input, retrieves similar prompt-image pairs from DiffusionDB, and identifies special (important and relevant) prompt keywords. To facilitate interactive prompt refinement, PromptMagician introduces a multi-level visualization for the cross-modal embedding of the retrieved images and recommended keywords, and supports users in specifying multiple criteria for personalized exploration. Two usage scenarios, a user study, and expert interviews demonstrate the effectiveness and usability of our system, suggesting it facilitates prompt engineering and improves the creativity support of the generative text-to-image model.
</details>
<details>
<summary>摘要</summary>
自然语言提示的生成图像模型在公众中得到了广泛的推广和应用，但是开发有效的提示语言可以是一项挑战，因为自然语言的复杂性和抽象性。这项研究提出了PromptMagician，一个可视化分析系统，帮助用户探索图像结果和调整输入提示语言。PromptMagician的核心是一个提示建议模型，接受用户提示语言作为输入，从DiffusionDB中检索相似的提示语言-图像对，并标识特殊（重要和相关）的提示关键词。为了促进交互式提示调整，PromptMagician引入了多级可视化，用于跨Modal的嵌入图像和推荐关键词的可视化，并支持用户指定多个标准 для个性化探索。两个使用场景、一项用户研究和专家采访表明PromptMagician可以帮助提高生成图像模型的创作支持和提示工程。
</details></li>
</ul>
<hr>
<h2 id="Exploring-acceptance-of-autonomous-vehicle-policies-using-KeyBERT-and-SNA-Targeting-engineering-students"><a href="#Exploring-acceptance-of-autonomous-vehicle-policies-using-KeyBERT-and-SNA-Targeting-engineering-students" class="headerlink" title="Exploring acceptance of autonomous vehicle policies using KeyBERT and SNA: Targeting engineering students"></a>Exploring acceptance of autonomous vehicle policies using KeyBERT and SNA: Targeting engineering students</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09014">http://arxiv.org/abs/2307.09014</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wangbuera/f230778c-6292-4e4f-97ab-6edac0901476">https://github.com/wangbuera/f230778c-6292-4e4f-97ab-6edac0901476</a></li>
<li>paper_authors: Jinwoo Ha, Dongsoo Kim</li>
<li>for: 本研究旨在探讨自动驾驶车（AV）政策的用户接受度，通过提升文本挖掘方法来填补政策制定者未充分考虑的用户需求。</li>
<li>methods: 本研究采用了两种文本挖掘方法，一是基于TF-IWF和 dice积分的协occurrence网络分析（CNA），另一是基于KeyBERT提取关键词和双cosinus相似度的上下文Semantic网络分析（C-SNA）。</li>
<li>results: 结果表明，C-SNA提供了更好地理解用户声音的信息，使用 fewer nodes and features  than CNA。用户通过预先理解AV政策基于其工程知识和给出的文本，披露了AV事故政策的潜在风险。本研究建议管理这些风险，以支持AV在公共道路上成功部署。<details>
<summary>Abstract</summary>
This study aims to explore user acceptance of Autonomous Vehicle (AV) policies with improved text-mining methods. Recently, South Korean policymakers have viewed Autonomous Driving Car (ADC) and Autonomous Driving Robot (ADR) as next-generation means of transportation that will reduce the cost of transporting passengers and goods. They support the construction of V2I and V2V communication infrastructures for ADC and recognize that ADR is equivalent to pedestrians to promote its deployment into sidewalks. To fill the gap where end-user acceptance of these policies is not well considered, this study applied two text-mining methods to the comments of graduate students in the fields of Industrial, Mechanical, and Electronics-Electrical-Computer. One is the Co-occurrence Network Analysis (CNA) based on TF-IWF and Dice coefficient, and the other is the Contextual Semantic Network Analysis (C-SNA) based on both KeyBERT, which extracts keywords that contextually represent the comments, and double cosine similarity. The reason for comparing these approaches is to balance interest not only in the implications for the AV policies but also in the need to apply quality text mining to this research domain. Significantly, the limitation of frequency-based text mining, which does not reflect textual context, and the trade-off of adjusting thresholds in Semantic Network Analysis (SNA) were considered. As the results of comparing the two approaches, the C-SNA provided the information necessary to understand users' voices using fewer nodes and features than the CNA. The users who pre-emptively understood the AV policies based on their engineering literacy and the given texts revealed potential risks of the AV accident policies. This study adds suggestions to manage these risks to support the successful deployment of AVs on public roads.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="How-is-ChatGPT’s-behavior-changing-over-time"><a href="#How-is-ChatGPT’s-behavior-changing-over-time" class="headerlink" title="How is ChatGPT’s behavior changing over time?"></a>How is ChatGPT’s behavior changing over time?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09009">http://arxiv.org/abs/2307.09009</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lchen001/llmdrift">https://github.com/lchen001/llmdrift</a></li>
<li>paper_authors: Lingjiao Chen, Matei Zaharia, James Zou<br>for:* The paper evaluates the performance and behavior of GPT-3.5 and GPT-4 on various tasks over time.methods:* The authors use several diverse tasks to evaluate the models’ performance, including math problems, sensitive&#x2F;dangerous questions, opinion surveys, multi-hop knowledge-intensive questions, generating code, US Medical License tests, and visual reasoning.results:* The performance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time, with some tasks showing improvement and others showing decline.* GPT-4 (June 2023) had poor performance on identifying prime vs. composite numbers compared to GPT-4 (March 2023).* GPT-4 became less willing to answer sensitive questions and opinion survey questions in June than in March.* GPT-4 performed better at multi-hop questions in June than in March, while GPT-3.5’s performance dropped on this task.* Both GPT-4 and GPT-3.5 had more formatting mistakes in code generation in June than in March.I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
GPT-3.5 and GPT-4 are the two most widely used large language model (LLM) services. However, when and how these models are updated over time is opaque. Here, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on several diverse tasks: 1) math problems, 2) sensitive/dangerous questions, 3) opinion surveys, 4) multi-hop knowledge-intensive questions, 5) generating code, 6) US Medical License tests, and 7) visual reasoning. We find that the performance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time. For example, GPT-4 (March 2023) was reasonable at identifying prime vs. composite numbers (84% accuracy) but GPT-4 (June 2023) was poor on these same questions (51% accuracy). This is partly explained by a drop in GPT-4's amenity to follow chain-of-thought prompting. Interestingly, GPT-3.5 was much better in June than in March in this task. GPT-4 became less willing to answer sensitive questions and opinion survey questions in June than in March. GPT-4 performed better at multi-hop questions in June than in March, while GPT-3.5's performance dropped on this task. Both GPT-4 and GPT-3.5 had more formatting mistakes in code generation in June than in March. Overall, our findings show that the behavior of the "same" LLM service can change substantially in a relatively short amount of time, highlighting the need for continuous monitoring of LLMs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Ord2Seq-Regarding-Ordinal-Regression-as-Label-Sequence-Prediction"><a href="#Ord2Seq-Regarding-Ordinal-Regression-as-Label-Sequence-Prediction" class="headerlink" title="Ord2Seq: Regarding Ordinal Regression as Label Sequence Prediction"></a>Ord2Seq: Regarding Ordinal Regression as Label Sequence Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09004">http://arxiv.org/abs/2307.09004</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinhong Wang, Yi Cheng, Jintai Chen, Tingting Chen, Danny Chen, Jian Wu</li>
<li>for: 这篇论文主要针对的是ordinal regression问题，即将对象实例分类到ordinal类别中。</li>
<li>methods: 这篇论文提出了一种简单的序列预测框架 Ord2Seq，它将每个ordinal类别标签转换成特殊标签序列，从而将ordinal regression任务转化为一个序列预测任务。</li>
<li>results: 实验表明，Ord2Seq可以很好地 distinguishing adjacent categories，并且在四个不同的场景中超过了当前state-of-the-art的性能。<details>
<summary>Abstract</summary>
Ordinal regression refers to classifying object instances into ordinal categories. It has been widely studied in many scenarios, such as medical disease grading, movie rating, etc. Known methods focused only on learning inter-class ordinal relationships, but still incur limitations in distinguishing adjacent categories thus far. In this paper, we propose a simple sequence prediction framework for ordinal regression called Ord2Seq, which, for the first time, transforms each ordinal category label into a special label sequence and thus regards an ordinal regression task as a sequence prediction process. In this way, we decompose an ordinal regression task into a series of recursive binary classification steps, so as to subtly distinguish adjacent categories. Comprehensive experiments show the effectiveness of distinguishing adjacent categories for performance improvement and our new approach exceeds state-of-the-art performances in four different scenarios. Codes are available at https://github.com/wjh892521292/Ord2Seq.
</details>
<details>
<summary>摘要</summary>
ordinal 回归指的是将对象实例分类到ordinal类别中。它在许多场景中受到广泛研究，如医疗疾病等级分类、电影评分等。知名的方法只关注学习间类关系，但然而仍然存在区分邻Category的限制。在这篇论文中，我们提出了一种简单的序列预测框架 для ordinal 回归，称为Ord2Seq，它将每个ordinal类别标签转化为特殊标签序列，从而将ordinal 回归任务转化为一个序列预测过程。这样，我们将ordinal 回归任务分解成一系列的回归binary分类步骤，以便细腻地区分邻Category。经过全面的实验，我们发现可以明显提高区分邻Category的性能，并且我们的新方法在四个不同的场景中超越了当前最佳性能。代码可以在https://github.com/wjh892521292/Ord2Seq中找到。
</details></li>
</ul>
<hr>
<h2 id="EVIL-Evidential-Inference-Learning-for-Trustworthy-Semi-supervised-Medical-Image-Segmentation"><a href="#EVIL-Evidential-Inference-Learning-for-Trustworthy-Semi-supervised-Medical-Image-Segmentation" class="headerlink" title="EVIL: Evidential Inference Learning for Trustworthy Semi-supervised Medical Image Segmentation"></a>EVIL: Evidential Inference Learning for Trustworthy Semi-supervised Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08988">http://arxiv.org/abs/2307.08988</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yingyu Chen, Ziyuan Yang, Chenyu Shen, Zhiwen Wang, Yang Qin, Yi Zhang</li>
<li>for: 提高 semi-supervised medical image segmentation 的准确性和可靠性，并提供 theoretically guaranteed 的解决方案。</li>
<li>methods: 基于 Dempster-Shafer Theory of Evidence (DST) 的 Evidential Inference Learning (EVIL) 方法，通过在一次前进中实现准确性量化和不确定性评估，并采用 consistency regularization-based 的训练方法来提高泛化能力。</li>
<li>results: 在公共数据集上实验表明，EVIL 方法与一些状态顶方法相比，具有竞争性的表现，并且可以提供 trustworthy pseudo labels on unlabeled data。<details>
<summary>Abstract</summary>
Recently, uncertainty-aware methods have attracted increasing attention in semi-supervised medical image segmentation. However, current methods usually suffer from the drawback that it is difficult to balance the computational cost, estimation accuracy, and theoretical support in a unified framework. To alleviate this problem, we introduce the Dempster-Shafer Theory of Evidence (DST) into semi-supervised medical image segmentation, dubbed Evidential Inference Learning (EVIL). EVIL provides a theoretically guaranteed solution to infer accurate uncertainty quantification in a single forward pass. Trustworthy pseudo labels on unlabeled data are generated after uncertainty estimation. The recently proposed consistency regularization-based training paradigm is adopted in our framework, which enforces the consistency on the perturbed predictions to enhance the generalization with few labeled data. Experimental results show that EVIL achieves competitive performance in comparison with several state-of-the-art methods on the public dataset.
</details>
<details>
<summary>摘要</summary>
最近，uncertainty-aware方法在半supervised医学图像分割中受到了越来越多的关注。然而，当前方法通常受到难以平衡计算成本、估计准确性和理论支持的问题。为了解决这个问题，我们在半supervised医学图像分割中引入了Dempster-Shafer理论（DST），称之为Evidential Inference Learning（EVIL）。EVIL提供了一个在单个前进 pass中 theoretically guarantee 的解决方案，以便Quantification of uncertainty accurate。在不经过标注数据的基础上生成可靠的 Pseudolabel 。我们的框架中采用了最近提出的一致Regularization-based training paradigm，以便在具有少量标注数据的情况下进行加强通用性。实验结果显示，EVIL在比较一些国际前沿方法的公共数据集上实现了竞争性的表现。
</details></li>
</ul>
<hr>
<h2 id="AI-assisted-Improved-Service-Provisioning-for-Low-latency-XR-over-5G-NR"><a href="#AI-assisted-Improved-Service-Provisioning-for-Low-latency-XR-over-5G-NR" class="headerlink" title="AI-assisted Improved Service Provisioning for Low-latency XR over 5G NR"></a>AI-assisted Improved Service Provisioning for Low-latency XR over 5G NR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08987">http://arxiv.org/abs/2307.08987</a></li>
<li>repo_url: None</li>
<li>paper_authors: Moyukh Laha, Dibbendu Roy, Sourav Dutta, Goutam Das</li>
<li>for: This paper aims to address the challenges of ensuring low latency, high data rate, and reliability in supporting Extended Reality (XR) services in 5G&#x2F;6G networks.</li>
<li>methods: The proposed AI-assisted service provisioning scheme leverages predicted frames for processing, virtually increasing the network delay budget and improving service provisioning.</li>
<li>results: The proposed scheme is validated by extensive simulations, demonstrating a multi-fold increase in supported XR users and providing crucial network design insights.Here’s the Chinese translation of the three key points:</li>
<li>for: 本文旨在解决5G&#x2F;6G网络中支持扩展现实（XR）服务的低延迟、高数据速率和可靠性问题。</li>
<li>methods: 该方案利用预测帧进行处理，虚拟增加网络延迟预算，提高服务提供。</li>
<li>results: 该方案由于广泛的 simulations  validate，能够支持多重XR用户，并提供重要的网络设计理念。<details>
<summary>Abstract</summary>
Extended Reality (XR) is one of the most important 5G/6G media applications that will fundamentally transform human interactions. However, ensuring low latency, high data rate, and reliability to support XR services poses significant challenges. This letter presents a novel AI-assisted service provisioning scheme that leverages predicted frames for processing rather than relying solely on actual frames. This method virtually increases the network delay budget and consequently improves service provisioning, albeit at the expense of minor prediction errors. The proposed scheme is validated by extensive simulations demonstrating a multi-fold increase in supported XR users and also provides crucial network design insights.
</details>
<details>
<summary>摘要</summary>
伸展现实（XR）是5G/6G媒体应用之一，将fundamentally改变人类互动方式。然而，确保低延迟、高数据速率和可靠性来支持XR服务具有挑战。这封信提出了一种基于预测帧的AI助け服务提供方案，而不是完全依赖实际帧。这种方法虚拟增加网络延迟预算，从而改善服务提供，但是会出现一定的预测错误。提议的方案通过广泛的 simulations 验证，证明可以支持多ples XR用户，同时提供重要的网络设计理念。
</details></li>
</ul>
<hr>
<h2 id="PromptCrafter-Crafting-Text-to-Image-Prompt-through-Mixed-Initiative-Dialogue-with-LLM"><a href="#PromptCrafter-Crafting-Text-to-Image-Prompt-through-Mixed-Initiative-Dialogue-with-LLM" class="headerlink" title="PromptCrafter: Crafting Text-to-Image Prompt through Mixed-Initiative Dialogue with LLM"></a>PromptCrafter: Crafting Text-to-Image Prompt through Mixed-Initiative Dialogue with LLM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08985">http://arxiv.org/abs/2307.08985</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seungho Baek, Hyerin Im, Jiseung Ryu, Juhyeong Park, Takyeon Lee</li>
<li>for: 这篇论文旨在提出一种新的混合式系统，帮助用户efficiently探索模型的能力和创建有效的提示。</li>
<li>methods: 该系统使用了步骤式的crafting方法，让用户可以逐步地制定文本到图像提示，并通过答复各种问题来细化意图。</li>
<li>results: 该系统可以帮助用户快速探索模型的能力，并帮助用户创建有效的提示，从而提高了用户的使用体验。<details>
<summary>Abstract</summary>
Text-to-image generation model is able to generate images across a diverse range of subjects and styles based on a single prompt. Recent works have proposed a variety of interaction methods that help users understand the capabilities of models and utilize them. However, how to support users to efficiently explore the model's capability and to create effective prompts are still open-ended research questions. In this paper, we present PromptCrafter, a novel mixed-initiative system that allows step-by-step crafting of text-to-image prompt. Through the iterative process, users can efficiently explore the model's capability, and clarify their intent. PromptCrafter also supports users to refine prompts by answering various responses to clarifying questions generated by a Large Language Model. Lastly, users can revert to a desired step by reviewing the work history. In this workshop paper, we discuss the design process of PromptCrafter and our plans for follow-up studies.
</details>
<details>
<summary>摘要</summary>
文本至图生成模型可以生成具有多样化主题和风格的图像，根据单个提示。 latest works have proposed various interaction methods to help users understand the model's capabilities and utilize them. However, how to efficiently explore the model's capability and create effective prompts are still open-ended research questions.在这篇论文中，我们介绍PromptCrafter，一种新的混合 iniciative 系统，允许用户逐步制作文本至图 prompt。通过迭代过程，用户可以高效地探索模型的能力，并明确自己的意图。PromptCrafter 还支持用户对提示进行细化，通过一个大型自然语言模型生成的各种回答问题来解释。最后，用户可以通过查看工作历史来返回到感兴趣的步骤。在这篇工作论文中，我们介绍PromptCrafter 的设计过程和我们的跟进计划。
</details></li>
</ul>
<hr>
<h2 id="Generative-Visual-Question-Answering"><a href="#Generative-Visual-Question-Answering" class="headerlink" title="Generative Visual Question Answering"></a>Generative Visual Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10405">http://arxiv.org/abs/2307.10405</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chojw/genb">https://github.com/chojw/genb</a></li>
<li>paper_authors: Ethan Shen, Scotty Singh, Bhavesh Kumar</li>
<li>for: 该论文目的是要研究多 modal 任务中的视觉语言深度学习模型在未来数据分布下的一致性，并提出一个可行的方法来创建一个高级的视觉问题回答（VQA）模型，以便在未来数据分布下获得成功的结果。</li>
<li>methods: 该论文使用了七种基eline和进步的VQA模型，并将这些模型应用在一个新的扩展 dataset 上，named GenVQA，这个 dataset 使用了 VQAv2 和 MS-COCO dataset 中的图像和描述来生成新的图像，并使用稳定扩展来测试这些模型的一致性。</li>
<li>results: 研究发现，这些成功的 VQA 模型在未来数据分布下的一致性较差，但是通过分析这些模型的架构，发现了一些常见的设计选择，可以帮助这些模型在未来数据分布下优化一致性。<details>
<summary>Abstract</summary>
Multi-modal tasks involving vision and language in deep learning continue to rise in popularity and are leading to the development of newer models that can generalize beyond the extent of their training data. The current models lack temporal generalization which enables models to adapt to changes in future data. This paper discusses a viable approach to creating an advanced Visual Question Answering (VQA) model which can produce successful results on temporal generalization. We propose a new data set, GenVQA, utilizing images and captions from the VQAv2 and MS-COCO dataset to generate new images through stable diffusion. This augmented dataset is then used to test a combination of seven baseline and cutting edge VQA models. Performance evaluation focuses on questions mirroring the original VQAv2 dataset, with the answers having been adjusted to the new images. This paper's purpose is to investigate the robustness of several successful VQA models to assess their performance on future data distributions. Model architectures are analyzed to identify common stylistic choices that improve generalization under temporal distribution shifts. This research highlights the importance of creating a large-scale future shifted dataset. This data can enhance the robustness of VQA models, allowing their future peers to have improved ability to adapt to temporal distribution shifts.
</details>
<details>
<summary>摘要</summary>
多modal任务涉及视觉和语言在深度学习中继续升温，并且导致 newer模型可以泛化到训练数据之外。现有模型缺乏时间泛化能力，使得模型能够适应未来数据的变化。这篇论文提出了创建高级Visual Question Answering（VQA）模型的可能方法，以便在未来数据分布下取得成功。我们提出了一个新的数据集，GenVQA，使用VQAv2和MS-COCO图像和caption生成新图像，并通过稳定扩散来测试七种基eline和潮流VQA模型。性能评估专注于原VQAv2数据集中的问题，并将答案调整到新图像中。本文的目的是investigate Several successful VQA模型的Robustness，以评估它们在未来数据分布下的性能。模型架构的分析，找到了一些通用的风格选择，可以提高模型在时间分布shift下的泛化能力。这种研究强调了创建大规模未来偏移数据的重要性，以提高VQA模型的未来几代的适应能力。
</details></li>
</ul>
<hr>
<h2 id="Development-of-the-ChatGPT-Generative-Artificial-Intelligence-and-Natural-Large-Language-Models-for-Accountable-Reporting-and-Use-CANGARU-Guidelines"><a href="#Development-of-the-ChatGPT-Generative-Artificial-Intelligence-and-Natural-Large-Language-Models-for-Accountable-Reporting-and-Use-CANGARU-Guidelines" class="headerlink" title="Development of the ChatGPT, Generative Artificial Intelligence and Natural Large Language Models for Accountable Reporting and Use (CANGARU) Guidelines"></a>Development of the ChatGPT, Generative Artificial Intelligence and Natural Large Language Models for Accountable Reporting and Use (CANGARU) Guidelines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08974">http://arxiv.org/abs/2307.08974</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giovanni E. Cacciamani, Michael B. Eppler, Conner Ganjavi, Asli Pekan, Brett Biedermann, Gary S. Collins, Inderbir S. Gill<br>for: 这个研究的目的是提出一个跨学科全包的global inclusive consensus，以便在学术研究中负责使用、披透和报告生成人工智能（GAI）&#x2F;生成预训练变换器（GPT）&#x2F;大语言模型（LLM）技术。methods: 这个研究使用了系统性文献评估，以理解相关的想法、发现和报道标准在学术研究中，并制定了使用和披透GAI&#x2F;GPT&#x2F;LLM技术的指南。同时，该研究还进行了一个 bibliometric 分析，以评估现有的作者指南，分析其建议的不一致，并从Delphi调查中提取了共识。results: 该研究通过系统性文献评估和Delphi调查，成功地建立了一个全球 inclusive consensus，以便在学术研究中负责使用、披透和报告GAI&#x2F;GPT&#x2F;LLM技术。这些指南将帮助保证GAI&#x2F;GPT&#x2F;LLM技术的负责使用、披透和报告，以确保学术研究的可靠性和可重复性。<details>
<summary>Abstract</summary>
The swift progress and ubiquitous adoption of Generative AI (GAI), Generative Pre-trained Transformers (GPTs), and large language models (LLMs) like ChatGPT, have spurred queries about their ethical application, use, and disclosure in scholarly research and scientific productions. A few publishers and journals have recently created their own sets of rules; however, the absence of a unified approach may lead to a 'Babel Tower Effect,' potentially resulting in confusion rather than desired standardization. In response to this, we present the ChatGPT, Generative Artificial Intelligence, and Natural Large Language Models for Accountable Reporting and Use Guidelines (CANGARU) initiative, with the aim of fostering a cross-disciplinary global inclusive consensus on the ethical use, disclosure, and proper reporting of GAI/GPT/LLM technologies in academia. The present protocol consists of four distinct parts: a) an ongoing systematic review of GAI/GPT/LLM applications to understand the linked ideas, findings, and reporting standards in scholarly research, and to formulate guidelines for its use and disclosure, b) a bibliometric analysis of existing author guidelines in journals that mention GAI/GPT/LLM, with the goal of evaluating existing guidelines, analyzing the disparity in their recommendations, and identifying common rules that can be brought into the Delphi consensus process, c) a Delphi survey to establish agreement on the items for the guidelines, ensuring principled GAI/GPT/LLM use, disclosure, and reporting in academia, and d) the subsequent development and dissemination of the finalized guidelines and their supplementary explanation and elaboration documents.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate_language: zh-CNThe swift progress and widespread adoption of Generative AI (GAI), Generative Pre-trained Transformers (GPTs), and large language models (LLMs) like ChatGPT, have triggered questions about their ethical application, use, and disclosure in scholarly research and scientific productions. A few publishers and journals have recently established their own sets of rules; however, the lack of a unified approach may lead to a 'Babel Tower Effect,' potentially resulting in confusion rather than desired standardization. In response to this, we present the ChatGPT, Generative Artificial Intelligence, and Natural Large Language Models for Accountable Reporting and Use Guidelines (CANGARU) initiative, with the aim of fostering a cross-disciplinary global inclusive consensus on the ethical use, disclosure, and proper reporting of GAI/GPT/LLM technologies in academia. The present protocol consists of four distinct parts:a) an ongoing systematic review of GAI/GPT/LLM applications to understand the linked ideas, findings, and reporting standards in scholarly research, and to formulate guidelines for its use and disclosure,b) a bibliometric analysis of existing author guidelines in journals that mention GAI/GPT/LLM, with the goal of evaluating existing guidelines, analyzing the disparity in their recommendations, and identifying common rules that can be brought into the Delphi consensus process,c) a Delphi survey to establish agreement on the items for the guidelines, ensuring principled GAI/GPT/LLM use, disclosure, and reporting in academia, andd) the subsequent development and dissemination of the finalized guidelines and their supplementary explanation and elaboration documents.
</details></li>
</ul>
<hr>
<h2 id="Landscape-Surrogate-Learning-Decision-Losses-for-Mathematical-Optimization-Under-Partial-Information"><a href="#Landscape-Surrogate-Learning-Decision-Losses-for-Mathematical-Optimization-Under-Partial-Information" class="headerlink" title="Landscape Surrogate: Learning Decision Losses for Mathematical Optimization Under Partial Information"></a>Landscape Surrogate: Learning Decision Losses for Mathematical Optimization Under Partial Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08964">http://arxiv.org/abs/2307.08964</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/lancer">https://github.com/facebookresearch/lancer</a></li>
<li>paper_authors: Arman Zharmagambetov, Brandon Amos, Aaron Ferber, Taoan Huang, Bistra Dilkina, Yuandong Tian</li>
<li>for: 这 paper 是为了解决部分观察或通用优化器无需专家调整时的优化问题而写的。</li>
<li>methods: 该 paper 使用学习优化器 $\mathbf{g}$ 来解决这些复杂问题，并使用知识优化解 $\mathbf{g}$ 来监督学习。</li>
<li>results: 该 paper 测试了该方法在synthetic问题和实际问题上，并得到了与当前基准相当或更好的目标值，同时减少了 $\mathbf{g}$ 的调用次数。特别是，该方法在高维ensional问题上表现出优于现有方法。<details>
<summary>Abstract</summary>
Recent works in learning-integrated optimization have shown promise in settings where the optimization problem is only partially observed or where general-purpose optimizers perform poorly without expert tuning. By learning an optimizer $\mathbf{g}$ to tackle these challenging problems with $f$ as the objective, the optimization process can be substantially accelerated by leveraging past experience. The optimizer can be trained with supervision from known optimal solutions or implicitly by optimizing the compound function $f\circ \mathbf{g}$. The implicit approach may not require optimal solutions as labels and is capable of handling problem uncertainty; however, it is slow to train and deploy due to frequent calls to optimizer $\mathbf{g}$ during both training and testing. The training is further challenged by sparse gradients of $\mathbf{g}$, especially for combinatorial solvers. To address these challenges, we propose using a smooth and learnable Landscape Surrogate $M$ as a replacement for $f\circ \mathbf{g}$. This surrogate, learnable by neural networks, can be computed faster than the solver $\mathbf{g}$, provides dense and smooth gradients during training, can generalize to unseen optimization problems, and is efficiently learned via alternating optimization. We test our approach on both synthetic problems, including shortest path and multidimensional knapsack, and real-world problems such as portfolio optimization, achieving comparable or superior objective values compared to state-of-the-art baselines while reducing the number of calls to $\mathbf{g}$. Notably, our approach outperforms existing methods for computationally expensive high-dimensional problems.
</details>
<details>
<summary>摘要</summary>
现代学习整合优化方法已经在部分观察到的优化问题或通用优化器无需专家调整时表现出了搭配性。通过学习一个优化器 $\mathbf{g}$ 以解决这些复杂的问题，优化过程可以得到加速。 $\mathbf{g}$ 可以在知道优化解的情况下进行监督学习，或者通过优化函数 $f\circ \mathbf{g}$ 进行启发式学习。后者可以不需要优化解作为标签，并且能够处理问题不确定性。然而，这种方法的训练和部署可能需要频繁地调用优化器 $\mathbf{g}$，这会导致训练和部署变慢。此外，$\mathbf{g}$ 的迭代次数可能会增加，尤其是在使用分解优化器时。为解决这些挑战，我们提议使用一个可学习的地形函数 $M$ 作为 $\mathbf{g}$ 的替换。这个函数可以通过神经网络学习，在训练中提供稠密和平滑的梯度，可以泛化到未看过的优化问题，并通过交互式优化快速学习。我们在 synthetic 问题和实际问题上进行测试，比如最优投资等，实现了与当前标准基准相当或更高的目标函数值，同时减少了对 $\mathbf{g}$ 的调用次数。尤其是在高维ensional Computationally 昂贵的问题上，我们的方法表现出了优于现有方法。
</details></li>
</ul>
<hr>
<h2 id="REX-Rapid-Exploration-and-eXploitation-for-AI-Agents"><a href="#REX-Rapid-Exploration-and-eXploitation-for-AI-Agents" class="headerlink" title="REX: Rapid Exploration and eXploitation for AI Agents"></a>REX: Rapid Exploration and eXploitation for AI Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08962">http://arxiv.org/abs/2307.08962</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rithesh Murthy, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Le Xue, Weiran Yao, Yihao Feng, Zeyuan Chen, Akash Gokul, Devansh Arpit, Ran Xu, Phil Mui, Huan Wang, Caiming Xiong, Silvio Savarese<br>for:REX is proposed to address the limitations of existing AutoGPT-style techniques in decision-making and to improve the efficiency and practicality of AI agent performance.methods:REX introduces an additional layer of rewards and integrates concepts similar to Upper Confidence Bound (UCB) scores to enhance AI agent performance. It also utilizes offline behaviors from logs and does not require any model fine-tuning.results:REX-based methods demonstrate comparable performance with existing methods such as Chain-of-Thoughts(CoT) and Reasoning viA Planning(RAP) in certain cases, and exhibit remarkable reductions in execution time, making it more practical and efficient in a diverse range of scenarios.<details>
<summary>Abstract</summary>
In this paper, we propose an enhanced approach for Rapid Exploration and eXploitation for AI Agents called REX. Existing AutoGPT-style techniques have inherent limitations, such as a heavy reliance on precise descriptions for decision-making, and the lack of a systematic approach to leverage try-and-fail procedures akin to traditional Reinforcement Learning (RL). REX introduces an additional layer of rewards and integrates concepts similar to Upper Confidence Bound (UCB) scores, leading to more robust and efficient AI agent performance. This approach has the advantage of enabling the utilization of offline behaviors from logs and allowing seamless integration with existing foundation models while it does not require any model fine-tuning. Through comparative analysis with existing methods such as Chain-of-Thoughts(CoT) and Reasoning viA Planning(RAP), REX-based methods demonstrate comparable performance and, in certain cases, even surpass the results achieved by these existing techniques. Notably, REX-based methods exhibit remarkable reductions in execution time, enhancing their practical applicability across a diverse set of scenarios.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种改进后的快速探索和尝试（Rapid Exploration and eXploitation，REX）方法，用于AI代理。现有的AutoGPT类型技术存在着内置的局限性，如决策过程中的精确描述过重，以及缺乏传统强化学习（Reinforcement Learning，RL）中的系统化尝试和失败处理机制。REX增加了一层奖励，并integrated Upper Confidence Bound（UCB）类概念，从而提高AI代理的稳定性和效率。这种方法可以利用日志中的offline行为，并可以与现有基础模型无需任何模型微调进行集成。与现有方法如Chain-of-Thoughts（CoT）和Reasoning viA Planning（RAP）进行比较分析，REX基于方法在相同的场景下达到了相当的表现水平，甚至在某些情况下超越了现有技术的结果。另外，REX基于方法在执行时间方面也表现出了很大的改善，从而提高了其在多样化场景下的实际应用性。
</details></li>
</ul>
<hr>
<h2 id="Siamese-Networks-for-Weakly-Supervised-Human-Activity-Recognition"><a href="#Siamese-Networks-for-Weakly-Supervised-Human-Activity-Recognition" class="headerlink" title="Siamese Networks for Weakly Supervised Human Activity Recognition"></a>Siamese Networks for Weakly Supervised Human Activity Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08944">http://arxiv.org/abs/2307.08944</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taoran Sheng, Manfred Huber</li>
<li>for: 人体活动识别</li>
<li>methods: 使用多个siamesenet，无需明确标签数据进行训练</li>
<li>results: 可以作为各种不同的聚类算法的度量，并在三个数据集上进行评估，以验证其效果性。Here’s the breakdown of each point:1. for: The paper is written for human activity recognition, specifically using deep learning methods without explicit labels.2. methods: The paper proposes a model using multiple siamese networks to train a distance metric that can be used for clustering, without requiring explicit labels.3. results: The trained model can be used as a metric for a wide range of clustering algorithms, and the authors evaluate its effectiveness on three datasets for activity segmentation and recognition.<details>
<summary>Abstract</summary>
Deep learning has been successfully applied to human activity recognition. However, training deep neural networks requires explicitly labeled data which is difficult to acquire. In this paper, we present a model with multiple siamese networks that are trained by using only the information about the similarity between pairs of data samples without knowing the explicit labels. The trained model maps the activity data samples into fixed size representation vectors such that the distance between the vectors in the representation space approximates the similarity of the data samples in the input space. Thus, the trained model can work as a metric for a wide range of different clustering algorithms. The training process minimizes a similarity loss function that forces the distance metric to be small for pairs of samples from the same kind of activity, and large for pairs of samples from different kinds of activities. We evaluate the model on three datasets to verify its effectiveness in segmentation and recognition of continuous human activity sequences.
</details>
<details>
<summary>摘要</summary>
深度学习已经成功应用于人类活动识别。然而，训练深度神经网络需要显式标注数据，这是困难的获得。在这篇论文中，我们提出了一种使用多个对称网络训练的模型，只使用数据对的相似性信息进行训练。训练后的模型将活动数据样本映射到固定大小的表示向量中，使得表示空间中的距离approximate输入空间中的相似性。因此，训练过的模型可以作为各种不同聚类算法的度量。训练过程中的相似损失函数使得距离度量在同类活动样本对应小，不同类活动样本对应大。我们在三个数据集上验证了模型的效果，以确认其在连续人类活动序列的分割和识别中的有效性。
</details></li>
</ul>
<hr>
<h2 id="IxDRL-A-Novel-Explainable-Deep-Reinforcement-Learning-Toolkit-based-on-Analyses-of-Interestingness"><a href="#IxDRL-A-Novel-Explainable-Deep-Reinforcement-Learning-Toolkit-based-on-Analyses-of-Interestingness" class="headerlink" title="IxDRL: A Novel Explainable Deep Reinforcement Learning Toolkit based on Analyses of Interestingness"></a>IxDRL: A Novel Explainable Deep Reinforcement Learning Toolkit based on Analyses of Interestingness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08933">http://arxiv.org/abs/2307.08933</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sri-aic/23-xai-ixdrl-data">https://github.com/sri-aic/23-xai-ixdrl-data</a></li>
<li>paper_authors: Pedro Sequeira, Melinda Gervasio</li>
<li>for: 这个论文旨在解决 Deep Reinforcement Learning (RL) 中存在的可解释性问题，提供人工智能专家在协作人机设置中更加了解RL机器人的能力和局限性，以便更 Informed 的决策。</li>
<li>methods: 该论文提出了一种基于吸引力分析的新框架，可以为 RL 机器人提供多种能力探测方法，并且可以Native 支持 popular RLLib 工具包。</li>
<li>results: 该论文通过应用该框架，对 RL 机器人的行为模式和能力进行了全面的探测和分析，并且可以帮助人工智能专家更好地理解 RL 机器人的能力和局限性，以便更好地决策。<details>
<summary>Abstract</summary>
In recent years, advances in deep learning have resulted in a plethora of successes in the use of reinforcement learning (RL) to solve complex sequential decision tasks with high-dimensional inputs. However, existing systems lack the necessary mechanisms to provide humans with a holistic view of their competence, presenting an impediment to their adoption, particularly in critical applications where the decisions an agent makes can have significant consequences. Yet, existing RL-based systems are essentially competency-unaware in that they lack the necessary interpretation mechanisms to allow human operators to have an insightful, holistic view of their competency. Towards more explainable Deep RL (xDRL), we propose a new framework based on analyses of interestingness. Our tool provides various measures of RL agent competence stemming from interestingness analysis and is applicable to a wide range of RL algorithms, natively supporting the popular RLLib toolkit. We showcase the use of our framework by applying the proposed pipeline in a set of scenarios of varying complexity. We empirically assess the capability of the approach in identifying agent behavior patterns and competency-controlling conditions, and the task elements mostly responsible for an agent's competence, based on global and local analyses of interestingness. Overall, we show that our framework can provide agent designers with insights about RL agent competence, both their capabilities and limitations, enabling more informed decisions about interventions, additional training, and other interactions in collaborative human-machine settings.
</details>
<details>
<summary>摘要</summary>
To address this issue, we propose a new framework based on interestingness analysis to provide a more explainable deep reinforcement learning (xDRL) system. Our tool offers various measures of RL agent competence and is applicable to a wide range of RL algorithms, natively supporting the popular RLLib toolkit. We demonstrate the use of our framework in a variety of scenarios of varying complexity, showcasing its ability to identify agent behavior patterns and competency-controlling conditions, as well as the task elements most responsible for an agent's competence, based on both global and local analyses of interestingness.Our framework provides agent designers with valuable insights into RL agent competence, enabling more informed decisions about interventions, additional training, and other interactions in collaborative human-machine settings. With our proposed xDRL framework, we aim to improve the transparency and accountability of RL systems, ultimately leading to more reliable and trustworthy decision-making processes.
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Deep-Graph-Matching-Based-on-Cycle-Consistency"><a href="#Unsupervised-Deep-Graph-Matching-Based-on-Cycle-Consistency" class="headerlink" title="Unsupervised Deep Graph Matching Based on Cycle Consistency"></a>Unsupervised Deep Graph Matching Based on Cycle Consistency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08930">http://arxiv.org/abs/2307.08930</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siddharth Tourani, Carsten Rother, Muhammad Haris Khan, Bogdan Savchynskyy</li>
<li>for: 图像中键点匹配无监督学习</li>
<li>methods: 自我监督法，不需要对准对应关系</li>
<li>results: 新状态作图像键点匹配，设置新纪录Here’s the summary in English for reference:</li>
<li>for: Keypoint matching in images without supervision</li>
<li>methods: Self-supervised method that enforces consistency of matchings between images of the same object category</li>
<li>results: New state-of-the-art for unsupervised graph matching<details>
<summary>Abstract</summary>
We contribute to the sparsely populated area of unsupervised deep graph matching with application to keypoint matching in images. Contrary to the standard \emph{supervised} approach, our method does not require ground truth correspondences between keypoint pairs. Instead, it is self-supervised by enforcing consistency of matchings between images of the same object category. As the matching and the consistency loss are discrete, their derivatives cannot be straightforwardly used for learning. We address this issue in a principled way by building our method upon the recent results on black-box differentiation of combinatorial solvers. This makes our method exceptionally flexible, as it is compatible with arbitrary network architectures and combinatorial solvers. Our experimental evaluation suggests that our technique sets a new state-of-the-art for unsupervised graph matching.
</details>
<details>
<summary>摘要</summary>
我们在受限的领域中贡献了无监督深度图匹配，具体来说是图像中锚点匹配。与惯常的监督方法不同，我们的方法不需要图像对对的基准 truth 对应。而是通过确保图像类别之间匹配的一致性来自我监督。由于匹配和一致性损失是离散的，它们的导数不可直接用于学习。我们在理性的方式解决这个问题，基于最近的黑盒异构分析器的结果。这使得我们的方法非常灵活，可以与任意网络架构和异构分析器集成。我们的实验评估表明，我们的技术已经创造了无监督图匹配的新州 OF THE ART。
</details></li>
</ul>
<hr>
<h2 id="Multi-Stage-Cable-Routing-through-Hierarchical-Imitation-Learning"><a href="#Multi-Stage-Cable-Routing-through-Hierarchical-Imitation-Learning" class="headerlink" title="Multi-Stage Cable Routing through Hierarchical Imitation Learning"></a>Multi-Stage Cable Routing through Hierarchical Imitation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08927">http://arxiv.org/abs/2307.08927</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianlan Luo, Charles Xu, Xinyang Geng, Gilbert Feng, Kuan Fang, Liam Tan, Stefan Schaal, Sergey Levine</li>
<li>for: 这种研究旨在解决多个阶段机器人 manipulate 任务中的问题，特别是电缆 Routing 任务，机器人需要通过一系列的clip来路径电缆。</li>
<li>methods: 该研究使用 imitation learning 方法，从示例中学习 vision-based 政策，包括下一步的Sequencing 级别和下一步的motor control 级别。</li>
<li>results: 研究表明，使用这种方法可以在非常困难的clip placement variation中表现出色，并且可以恢复从失败中 recovered 和修正错误。Here are the three main points in English:</li>
<li>for: The research aims to solve the problem of multi-stage robotic manipulation tasks, particularly the cable routing task, where the robot needs to route a cable through a series of clips.</li>
<li>methods: The study uses imitation learning methods, learning vision-based policies from demonstrations at both the lower (motor control) and upper (sequencing) levels.</li>
<li>results: The research shows that the method can perform excellently in very challenging clip placement variations and can recover from failure and correct errors.<details>
<summary>Abstract</summary>
We study the problem of learning to perform multi-stage robotic manipulation tasks, with applications to cable routing, where the robot must route a cable through a series of clips. This setting presents challenges representative of complex multi-stage robotic manipulation scenarios: handling deformable objects, closing the loop on visual perception, and handling extended behaviors consisting of multiple steps that must be executed successfully to complete the entire task. In such settings, learning individual primitives for each stage that succeed with a high enough rate to perform a complete temporally extended task is impractical: if each stage must be completed successfully and has a non-negligible probability of failure, the likelihood of successful completion of the entire task becomes negligible. Therefore, successful controllers for such multi-stage tasks must be able to recover from failure and compensate for imperfections in low-level controllers by smartly choosing which controllers to trigger at any given time, retrying, or taking corrective action as needed. To this end, we describe an imitation learning system that uses vision-based policies trained from demonstrations at both the lower (motor control) and the upper (sequencing) level, present a system for instantiating this method to learn the cable routing task, and perform evaluations showing great performance in generalizing to very challenging clip placement variations. Supplementary videos, datasets, and code can be found at https://sites.google.com/view/cablerouting.
</details>
<details>
<summary>摘要</summary>
我们研究多阶段机器人操作任务的学习问题，具体是用于电缆 Routing 的情况，机器人需要通过一系列的夹具来 rout 电缆。这个设定提供了复杂的多阶段机器人操作enario，包括处理可扩展物体、关闭视觉感知的关键和处理多步骤的延展 behaviors。在这种情况下，学习单一的基本步骤 для每个阶段是不实用的：如果每个阶段都需要成功完成，并且有一定的失败几率，则完成整个任务的可能性会极低。因此，成功的控制器 для这种多阶段任务必须能够重新启动、补偿低层控制器的缺陷，并在需要时进行订正动作。为此，我们描述了一个模仿学习系统，使用视觉政策来从示例中学习并实现 cable routing 任务，并在评估中表现出非常出色地一致性。请参考 https://sites.google.com/view/cablerouting 获取补充影片、数据和代码。
</details></li>
</ul>
<hr>
<h2 id="Federated-Large-Language-Model-A-Position-Paper"><a href="#Federated-Large-Language-Model-A-Position-Paper" class="headerlink" title="Federated Large Language Model: A Position Paper"></a>Federated Large Language Model: A Position Paper</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08925">http://arxiv.org/abs/2307.08925</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chaochao Chen, Xiaohua Feng, Jun Zhou, Jianwei Yin, Xiaolin Zheng</li>
<li>for: 这篇论文的目的是探讨大规模自然语言模型（LLM）在实际应用中遇到的挑战，以及如何使用联邦学习（FL）技术来解决这些挑战。</li>
<li>methods: 这篇论文提出了三个关键 ком成分，即联邦LLM预训练、联邦LLM精度调整和联邦LLM提示工程。每个 ком成分都有优点比传统LLM训练方法，并提出了具体的工程战略来实现。</li>
<li>results: 这篇论文分析了联邦LLM的新问题和挑战，并评估了现有的解决方案和可能的阻碍因素。<details>
<summary>Abstract</summary>
Large scale language models (LLM) have received significant attention and found diverse applications across various domains, but their development encounters challenges in real-world scenarios. These challenges arise due to the scarcity of public domain data availability and the need to maintain privacy with respect to private domain data. To address these issues, federated learning (FL) has emerged as a promising technology that enables collaborative training of shared models while preserving decentralized data. We propose the concept of federated LLM, which comprises three key components, i.e., federated LLM pre-training, federated LLM fine-tuning, and federated LLM prompt engineering. For each component, we discuss its advantage over traditional LLM training methods and propose specific engineering strategies for implementation. Furthermore, we explore the novel challenges introduced by the integration of FL and LLM. We analyze existing solutions and identify potential obstacles faced by these solutions within the context of federated LLM.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Continuous-Time-Reinforcement-Learning-New-Design-Algorithms-with-Theoretical-Insights-and-Performance-Guarantees"><a href="#Continuous-Time-Reinforcement-Learning-New-Design-Algorithms-with-Theoretical-Insights-and-Performance-Guarantees" class="headerlink" title="Continuous-Time Reinforcement Learning: New Design Algorithms with Theoretical Insights and Performance Guarantees"></a>Continuous-Time Reinforcement Learning: New Design Algorithms with Theoretical Insights and Performance Guarantees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08920">http://arxiv.org/abs/2307.08920</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brent A. Wallace, Jennie Si</li>
<li>for: 这个论文的目的是提出一种新的连续时间非线性优化控制方法，用于控制非线性系统。</li>
<li>methods: 这些方法包括分解物理系统为小问题，以提高设计直观性和约化维度。它们还使用新的刺激框架，以提高 persistency of excitation 和数值稳定性性能。</li>
<li>results: 这些方法在控制一个不稳定、非最小频响 hypersonic vehicle (HSV) 上得到了证明和实验 guarantees，并且在这个应用中表现出了良好的性能。<details>
<summary>Abstract</summary>
Continuous-time nonlinear optimal control problems hold great promise in real-world applications. After decades of development, reinforcement learning (RL) has achieved some of the greatest successes as a general nonlinear control design method. However, a recent comprehensive analysis of state-of-the-art continuous-time RL (CT-RL) methods, namely, adaptive dynamic programming (ADP)-based CT-RL algorithms, reveals they face significant design challenges due to their complexity, numerical conditioning, and dimensional scaling issues. Despite advanced theoretical results, existing ADP CT-RL synthesis methods are inadequate in solving even small, academic problems. The goal of this work is thus to introduce a suite of new CT-RL algorithms for control of affine nonlinear systems. Our design approach relies on two important factors. First, our methods are applicable to physical systems that can be partitioned into smaller subproblems. This constructive consideration results in reduced dimensionality and greatly improved intuitiveness of design. Second, we introduce a new excitation framework to improve persistence of excitation (PE) and numerical conditioning performance via classical input/output insights. Such a design-centric approach is the first of its kind in the ADP CT-RL community. In this paper, we progressively introduce a suite of (decentralized) excitable integral reinforcement learning (EIRL) algorithms. We provide convergence and closed-loop stability guarantees, and we demonstrate these guarantees on a significant application problem of controlling an unstable, nonminimum phase hypersonic vehicle (HSV).
</details>
<details>
<summary>摘要</summary>
The goal of this work is to introduce a suite of new CT-RL algorithms for controlling affine nonlinear systems. Our design approach focuses on two key factors:1. Applicability to physical systems that can be partitioned into smaller subproblems, resulting in reduced dimensionality and improved intuitiveness of design.2. Introduction of a new excitation framework to improve persistence of excitation (PE) and numerical conditioning performance via classical input/output insights.This design-centric approach is novel in the ADP CT-RL community. We progressively introduce a suite of decentralized excitable integral reinforcement learning (EIRL) algorithms, providing convergence and closed-loop stability guarantees. We demonstrate these guarantees on a significant application problem of controlling an unstable, nonminimum phase hypersonic vehicle (HSV).
</details></li>
</ul>
<hr>
<h2 id="Solving-multiphysics-based-inverse-problems-with-learned-surrogates-and-constraints"><a href="#Solving-multiphysics-based-inverse-problems-with-learned-surrogates-and-constraints" class="headerlink" title="Solving multiphysics-based inverse problems with learned surrogates and constraints"></a>Solving multiphysics-based inverse problems with learned surrogates and constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11099">http://arxiv.org/abs/2307.11099</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyi Yin, Rafael Orozco, Mathias Louboutin, Felix J. Herrmann</li>
<li>for: 这个研究旨在解决对地质碳储监控中的多物理 inverse problem，当multimodal时间径数据贵重和numerical simulation too costly时，我们使用computationally cheap learned surrogates和learned constraints相结合，以获得高精度的permeability数据。</li>
<li>methods: 我们使用了一个结合learned surrogates和learned constraints的扩展方法，包括一个训练好的深度神经网（normalizing flow），强制模型迭代维持在distribution中，以保证训练了Fourier neural operator的精度。</li>
<li>results: 我们透过试验集中心在地质碳储问题上，使用了时间径数据和时间径地震数据两种不同的数据模式，并评估了这两种数据模式的组合效果。结果显示，这种结合方法可以提供高精度的permeability数据和CO2气泡预测，包括监控井点附近和远 away的地区。<details>
<summary>Abstract</summary>
Solving multiphysics-based inverse problems for geological carbon storage monitoring can be challenging when multimodal time-lapse data are expensive to collect and costly to simulate numerically. We overcome these challenges by combining computationally cheap learned surrogates with learned constraints. Not only does this combination lead to vastly improved inversions for the important fluid-flow property, permeability, it also provides a natural platform for inverting multimodal data including well measurements and active-source time-lapse seismic data. By adding a learned constraint, we arrive at a computationally feasible inversion approach that remains accurate. This is accomplished by including a trained deep neural network, known as a normalizing flow, which forces the model iterates to remain in-distribution, thereby safeguarding the accuracy of trained Fourier neural operators that act as surrogates for the computationally expensive multiphase flow simulations involving partial differential equation solves. By means of carefully selected experiments, centered around the problem of geological carbon storage, we demonstrate the efficacy of the proposed constrained optimization method on two different data modalities, namely time-lapse well and time-lapse seismic data. While permeability inversions from both these two modalities have their pluses and minuses, their joint inversion benefits from either, yielding valuable superior permeability inversions and CO2 plume predictions near, and far away, from the monitoring wells.
</details>
<details>
<summary>摘要</summary>
解决基于多物理的反向问题可能会存在挑战，特别是当时间延迟数据成本高昂且计算成本高昂时。我们通过将计算成本低的学习模型与学习约束结合，不仅能够大幅提高含液流速度的重要参数含液性，还提供了自然的多模态数据混合 inverse 平台。通过添加学习约束，我们得到一个可行的推算方法，保持精度。这是通过包含训练好的深度神经网络（normalizing flow），让模型迭代器的输出呈现在有效范围内，以保证训练过 Fourier neural operator 的精度。通过选择合适的实验，我们在 geological carbon storage 问题上验证了我们的受限优化方法，并在不同数据模式下进行了两种不同的吞吐量推算。虽然吞吐量推算从两种数据模式中各有优缺点，但是两者的联合推算却能够提供更加优秀的含液性推算和 CO2 气泡预测，尤其是在监测井附近和远离监测井的地方。
</details></li>
</ul>
<hr>
<h2 id="Basal-Bolus-Advisor-for-Type-1-Diabetes-T1D-Patients-Using-Multi-Agent-Reinforcement-Learning-RL-Methodology"><a href="#Basal-Bolus-Advisor-for-Type-1-Diabetes-T1D-Patients-Using-Multi-Agent-Reinforcement-Learning-RL-Methodology" class="headerlink" title="Basal-Bolus Advisor for Type 1 Diabetes (T1D) Patients Using Multi-Agent Reinforcement Learning (RL) Methodology"></a>Basal-Bolus Advisor for Type 1 Diabetes (T1D) Patients Using Multi-Agent Reinforcement Learning (RL) Methodology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08897">http://arxiv.org/abs/2307.08897</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mehrad Jaloli, Marzia Cescon</li>
<li>for: 这个研究旨在开发一种基于多代理人学习（RL）的个人化血糖控制方法，以改善型1糖尿病（T1D）患者的血糖水平。</li>
<li>methods: 这个方法使用一个关闭链系统，包括一个血糖代谢模型和一个多代理人弹性算法对策。</li>
<li>results: 研究结果显示，RL基于的基础-胶囊导师可以有效改善血糖控制，减少血糖波动和增加在目标范围（70-180 mg&#x2F;dL）中的时间。低血糖事件得到有效预防，并减少严重高血糖事件。此外，RL方法也导致了与传统治疗相比的平均每天基础胰岛素剂量的 statistically significant 减少。这些发现显示RL方法在实现更好的血糖控制和减少高血糖的风险方面是有效的。<details>
<summary>Abstract</summary>
This paper presents a novel multi-agent reinforcement learning (RL) approach for personalized glucose control in individuals with type 1 diabetes (T1D). The method employs a closed-loop system consisting of a blood glucose (BG) metabolic model and a multi-agent soft actor-critic RL model acting as the basal-bolus advisor. Performance evaluation is conducted in three scenarios, comparing the RL agents to conventional therapy. Evaluation metrics include glucose levels (minimum, maximum, and mean), time spent in different BG ranges, and average daily bolus and basal insulin dosages. Results demonstrate that the RL-based basal-bolus advisor significantly improves glucose control, reducing glycemic variability and increasing time spent within the target range (70-180 mg/dL). Hypoglycemia events are effectively prevented, and severe hyperglycemia events are reduced. The RL approach also leads to a statistically significant reduction in average daily basal insulin dosage compared to conventional therapy. These findings highlight the effectiveness of the multi-agent RL approach in achieving better glucose control and mitigating the risk of severe hyperglycemia in individuals with T1D.
</details>
<details>
<summary>摘要</summary>
The results show that the RL-based basal-bolus advisor significantly improves glucose control, reducing glycemic variability and increasing time spent within the target range (70-180 mg/dL). Hypoglycemia events are effectively prevented, and severe hyperglycemia events are reduced. Additionally, the RL approach leads to a statistically significant reduction in average daily basal insulin dosage compared to conventional therapy. These findings demonstrate the effectiveness of the multi-agent RL approach in achieving better glucose control and mitigating the risk of severe hyperglycemia in individuals with T1D.
</details></li>
</ul>
<hr>
<h2 id="AI-for-the-Generation-and-Testing-of-Ideas-Towards-an-AI-Supported-Knowledge-Development-Environment"><a href="#AI-for-the-Generation-and-Testing-of-Ideas-Towards-an-AI-Supported-Knowledge-Development-Environment" class="headerlink" title="AI for the Generation and Testing of Ideas Towards an AI Supported Knowledge Development Environment"></a>AI for the Generation and Testing of Ideas Towards an AI Supported Knowledge Development Environment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08876">http://arxiv.org/abs/2307.08876</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ted Selker</li>
<li>for: 这篇论文主要是为了探讨如何使用机器学习技术创建大量语言模型，以便在不同的通信形式中预测Sequential information，并且通过Transformers生成文本或视觉输出，以模拟人类的回应。</li>
<li>methods: 这篇论文使用的方法主要是基于Transformers的生成AI，它可以在不同的语言模型中生成文本或视觉输出，并且可以模拟人类的回应。</li>
<li>results: 这篇论文的结果主要表明，通过将生成AI与互联网源的追溯功能相结合，可以创造出更加有价值的解决方案，并且可以减少人类偏见。此外，论文还提出了一种名为“Generate And Search Test”的系统，可以帮助知识工作者更加快速地创建高质量的解决方案。<details>
<summary>Abstract</summary>
New systems employ Machine Learning to sift through large knowledge sources, creating flexible Large Language Models. These models discern context and predict sequential information in various communication forms. Generative AI, leveraging Transformers, generates textual or visual outputs mimicking human responses. It proposes one or multiple contextually feasible solutions for a user to contemplate. However, generative AI does not currently support traceability of ideas, a useful feature provided by search engines indicating origin of information. The narrative style of generative AI has gained positive reception. People learn from stories. Yet, early ChatGPT efforts had difficulty with truth, reference, calculations, and aspects like accurate maps. Current capabilities of referencing locations and linking to apps seem to be better catered by the link-centric search methods we've used for two decades. Deploying truly believable solutions extends beyond simulating contextual relevance as done by generative AI. Combining the creativity of generative AI with the provenance of internet sources in hybrid scenarios could enhance internet usage. Generative AI, viewed as drafts, stimulates thinking, offering alternative ideas for final versions or actions. Scenarios for information requests are considered. We discuss how generative AI can boost idea generation by eliminating human bias. We also describe how search can verify facts, logic, and context. The user evaluates these generated ideas for selection and usage. This paper introduces a system for knowledge workers, Generate And Search Test, enabling individuals to efficiently create solutions previously requiring top collaborations of experts.
</details>
<details>
<summary>摘要</summary>
新系统雇用机器学习探索大量知识源，创造 flexible 大语言模型。这些模型能够辨识 контекст和预测 sequential information 在不同的通讯形式中。生成 AI，运用 transformers，生成文字或图像出力，模拟人类回应。它提供一或多个 contextually feasible 解决方案，让用户思考。但是，生成 AI 目前不支持追溯想法的功能，这是搜索引擎提供的有用功能，它可以显示信息的来源。生成 AI 的 narrative 式受到了正面的评价，人们从故事中学习。然而， early ChatGPT 努力遇到了真实、参考、计算和正确地图等问题。现在的连结中心搜寻方法可以更好地适应连结中心搜寻方法，这些方法在过去二十年中被使用。实现真实的解决方案需要进一步推进，不只是模拟上下文相关的 relevance，生成 AI 可以与互联网的来源混合使用。生成 AI 被视为稿件，刺激思维，提供选择和行动的代替想法。在信息请求的情况下，我们讨论了如何使用生成 AI 增强想法生成，消除人类偏见。我们还描述了如何使用搜寻验证 факти、逻辑和 context。用户评估这些生成的想法，选择和使用。这篇文章介绍了一个专为知识工作者的系统，创新 Test，允许个人快速创建以前需要多个专家合作的解决方案。
</details></li>
</ul>
<hr>
<h2 id="An-Alternative-to-Variance-Gini-Deviation-for-Risk-averse-Policy-Gradient"><a href="#An-Alternative-to-Variance-Gini-Deviation-for-Risk-averse-Policy-Gradient" class="headerlink" title="An Alternative to Variance: Gini Deviation for Risk-averse Policy Gradient"></a>An Alternative to Variance: Gini Deviation for Risk-averse Policy Gradient</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08873">http://arxiv.org/abs/2307.08873</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yudong Luo, Guiliang Liu, Pascal Poupart, Yangchen Pan</li>
<li>for: 降低奖励回报的方差，以避免风险投入RL中的潜在风险。</li>
<li>methods: 使用新的风险度量，帕尼-迪弗扬分布，代替直接使用奖励回报的方差。</li>
<li>results: 在具体的域中，我们的算法可以避免方差基础的限制，并实现高回报低风险。<details>
<summary>Abstract</summary>
Restricting the variance of a policy's return is a popular choice in risk-averse Reinforcement Learning (RL) due to its clear mathematical definition and easy interpretability. Traditional methods directly restrict the total return variance. Recent methods restrict the per-step reward variance as a proxy. We thoroughly examine the limitations of these variance-based methods, such as sensitivity to numerical scale and hindering of policy learning, and propose to use an alternative risk measure, Gini deviation, as a substitute. We study various properties of this new risk measure and derive a policy gradient algorithm to minimize it. Empirical evaluation in domains where risk-aversion can be clearly defined, shows that our algorithm can mitigate the limitations of variance-based risk measures and achieves high return with low risk in terms of variance and Gini deviation when others fail to learn a reasonable policy.
</details>
<details>
<summary>摘要</summary>
限制策略返回的方差是常见的选择在偏离风险学习（RL）中，因为它有明确的数学定义和易于理解的解释。传统方法直接限制总返回方差。现代方法则限制每步奖励方差作为代理。我们仔细检查了这些方差基于的方法的局限性，如数值归一化的敏感性和策略学习压抑，并提出了一种替代风险度量，幂差偏度，并 derivation of a policy gradient algorithm to minimize it。我们在可以明确定义风险偏好的领域进行了实证评估，显示我们的算法可以减少方差基于的风险度量的局限性，并在其他策略无法学习合理策略时达到高返回低风险水平。
</details></li>
</ul>
<hr>
<h2 id="Curriculum-Learning-for-Graph-Neural-Networks-A-Multiview-Competence-based-Approach"><a href="#Curriculum-Learning-for-Graph-Neural-Networks-A-Multiview-Competence-based-Approach" class="headerlink" title="Curriculum Learning for Graph Neural Networks: A Multiview Competence-based Approach"></a>Curriculum Learning for Graph Neural Networks: A Multiview Competence-based Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08859">http://arxiv.org/abs/2307.08859</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/CLU-UML/MCCL">https://github.com/CLU-UML/MCCL</a></li>
<li>paper_authors: Nidhi Vakil, Hadi Amiri</li>
<li>for: 这篇论文旨在提出一种基于图形复杂性ormalization的课程学习方法，以提高语言应用中的图形神经网络训练效率。</li>
<li>methods: 本篇论文使用了一种基于图形复杂性和模型能力的课程学习方法，包括一个调度方案，将不同的图形难度和模型能力考虑在训练中。</li>
<li>results: 实验结果显示，提案的方法可以在实际的连接预测和节点分类任务上提高图形神经网络的训练效率。<details>
<summary>Abstract</summary>
A curriculum is a planned sequence of learning materials and an effective one can make learning efficient and effective for both humans and machines. Recent studies developed effective data-driven curriculum learning approaches for training graph neural networks in language applications. However, existing curriculum learning approaches often employ a single criterion of difficulty in their training paradigms. In this paper, we propose a new perspective on curriculum learning by introducing a novel approach that builds on graph complexity formalisms (as difficulty criteria) and model competence during training. The model consists of a scheduling scheme which derives effective curricula by accounting for different views of sample difficulty and model competence during training. The proposed solution advances existing research in curriculum learning for graph neural networks with the ability to incorporate a fine-grained spectrum of graph difficulty criteria in their training paradigms. Experimental results on real-world link prediction and node classification tasks illustrate the effectiveness of the proposed approach.
</details>
<details>
<summary>摘要</summary>
一个课程是一种规划的学习材料序列，一个有效的课程可以使学习变得高效和有效，不分人类和机器。现在的研究已经开发了使用数据驱动的课程学习方法来训练语言应用中的图神经网络。然而，现有的课程学习方法通常使用单一的困难度标准来训练。在这篇论文中，我们提出了一新的课程学习视角，基于图复杂性形式学（作为困难度标准）和模型能力的建议。我们的方法包括一种安排方案，通过考虑不同的样本困难度和模型能力来 derivation 有效的课程。我们的解决方案超越了现有的研究，使得图神经网络的课程学习可以包含细致的图困难度标准在内。实验结果表明，在真实世界中的链接预测和节点分类任务上，我们的方法具有显著的效果。
</details></li>
</ul>
<hr>
<h2 id="Autoregressive-Diffusion-Model-for-Graph-Generation"><a href="#Autoregressive-Diffusion-Model-for-Graph-Generation" class="headerlink" title="Autoregressive Diffusion Model for Graph Generation"></a>Autoregressive Diffusion Model for Graph Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08849">http://arxiv.org/abs/2307.08849</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lingkai Kong, Jiaming Cui, Haotian Sun, Yuchen Zhuang, B. Aditya Prakash, Chao Zhang</li>
<li>for: 本研究旨在提出一种基于扩散的图生成模型，以提高图生成的效果和速度。</li>
<li>methods: 我们提出了一种名为“自适应扩散”的模型，它在图生成过程中直接在离散图空间进行扩散过程，而不是在归一化的邻接矩阵空间进行。我们还设计了一个“扩散排序网络”和一个“减噪网络”，以便在前向和反向生成过程中进行有效的图生成。</li>
<li>results: 我们在六个不同的通用图数据集和两个分子数据集上进行了实验，并证明了我们的模型可以在图生成中 дости得更好的或相当的效果，同时具有快速的生成速度。<details>
<summary>Abstract</summary>
Diffusion-based graph generative models have recently obtained promising results for graph generation. However, existing diffusion-based graph generative models are mostly one-shot generative models that apply Gaussian diffusion in the dequantized adjacency matrix space. Such a strategy can suffer from difficulty in model training, slow sampling speed, and incapability of incorporating constraints. We propose an \emph{autoregressive diffusion} model for graph generation. Unlike existing methods, we define a node-absorbing diffusion process that operates directly in the discrete graph space. For forward diffusion, we design a \emph{diffusion ordering network}, which learns a data-dependent node absorbing ordering from graph topology. For reverse generation, we design a \emph{denoising network} that uses the reverse node ordering to efficiently reconstruct the graph by predicting the node type of the new node and its edges with previously denoised nodes at a time. Based on the permutation invariance of graph, we show that the two networks can be jointly trained by optimizing a simple lower bound of data likelihood. Our experiments on six diverse generic graph datasets and two molecule datasets show that our model achieves better or comparable generation performance with previous state-of-the-art, and meanwhile enjoys fast generation speed.
</details>
<details>
<summary>摘要</summary>
Diffusion-based图像生成模型最近已经取得了图像生成的出色成绩。然而，现有的扩散基于的图像生成模型大多是单一的一步生成模型，通过 Gaussian扩散在解Quantized邻接矩阵空间中进行。这种策略可能会导致模型训练困难、扩散速度慢和约束不能具体化。我们提出了一种“自适应扩散”模型，与现有方法不同，我们直接在零唯一图像空间中定义节点吸收扩散过程。对于前向扩散，我们设计了一个“扩散排序网络”，该网络学习基于图像结构的数据依赖节点吸收排序。对于反向生成，我们设计了一个“释放网络”，该网络使用反向节点排序来快速重建图像，先预测新节点的节点类型和与已经预测过的节点的边。基于图像的卷积 invariants，我们证明了这两个网络可以共同训练，通过优化数据可能的下界来优化模型。我们在六种不同的通用图像dataset和两种分子dataset上进行了实验，发现我们的模型可以与之前的状态前所未有的或相似的生成性能，同时具有快速生成速度。
</details></li>
</ul>
<hr>
<h2 id="Towards-Accelerating-Benders-Decomposition-via-Reinforcement-Learning-Surrogate-Models"><a href="#Towards-Accelerating-Benders-Decomposition-via-Reinforcement-Learning-Surrogate-Models" class="headerlink" title="Towards Accelerating Benders Decomposition via Reinforcement Learning Surrogate Models"></a>Towards Accelerating Benders Decomposition via Reinforcement Learning Surrogate Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08816">http://arxiv.org/abs/2307.08816</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephen Mak, Kyle Mana, Parisa Zehtabi, Michael Cashmore, Daniele Magazzeni, Manuela Veloso</li>
<li>for: 这篇论文是为了解决在不确定性下做出最佳决策的问题。</li>
<li>methods: 这篇论文使用的方法是Benders decomposion（BD），它将随机估计问题分解为多个更小的子问题。</li>
<li>results: 这篇论文提出了一种使用代理模型来加速BD的加速方法，并证明了这种方法可以提高BD的速度，具体来说，它比其他加速BD实现方法rapidly convergence的速度快30%。<details>
<summary>Abstract</summary>
Stochastic optimization (SO) attempts to offer optimal decisions in the presence of uncertainty. Often, the classical formulation of these problems becomes intractable due to (a) the number of scenarios required to capture the uncertainty and (b) the discrete nature of real-world planning problems. To overcome these tractability issues, practitioners turn to decomposition methods that divide the problem into smaller, more tractable sub-problems. The focal decomposition method of this paper is Benders decomposition (BD), which decomposes stochastic optimization problems on the basis of scenario independence. In this paper we propose a method of accelerating BD with the aid of a surrogate model in place of an NP-hard integer master problem. Through the acceleration method we observe 30% faster average convergence when compared to other accelerated BD implementations. We introduce a reinforcement learning agent as a surrogate and demonstrate how it can be used to solve a stochastic inventory management problem.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>The number of scenarios required to capture the uncertainty.2. The discrete nature of real-world planning problems.To address these tractability issues, practitioners often use decomposition methods that break down the problem into smaller, more manageable sub-problems. One such method is Benders decomposition (BD), which decomposes stochastic optimization problems based on scenario independence.In this paper, we propose a method to accelerate BD using a surrogate model in place of an NP-hard integer master problem. Our approach leads to an average convergence rate 30% faster compared to other accelerated BD implementations.We introduce a reinforcement learning agent as a surrogate and demonstrate its use in solving a stochastic inventory management problem. By leveraging the surrogate model, we can efficiently solve the problem and achieve better performance.</details></li>
</ol>
<hr>
<h2 id="Operator-Guidance-Informed-by-AI-Augmented-Simulations"><a href="#Operator-Guidance-Informed-by-AI-Augmented-Simulations" class="headerlink" title="Operator Guidance Informed by AI-Augmented Simulations"></a>Operator Guidance Informed by AI-Augmented Simulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08810">http://arxiv.org/abs/2307.08810</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samuel J. Edwards, Michael Levine</li>
<li>for: 这篇论文是用来估算船舶响应统计数据的多优异、数据适应方法。</li>
<li>methods: 该方法使用了一个Long Short-Term Memory（LSTM）神经网络，使用了一个快速的低精度工具SimpleCode和一个更高精度的工具LAMP进行估算。</li>
<li>results: 经过训练LSTM神经网络以及对SimpleCode和LAMP数据进行比较，研究发现该方法可以准确地估算船舶响应统计数据。<details>
<summary>Abstract</summary>
This paper will present a multi-fidelity, data-adaptive approach with a Long Short-Term Memory (LSTM) neural network to estimate ship response statistics in bimodal, bidirectional seas. The study will employ a fast low-fidelity, volume-based tool SimpleCode and a higher-fidelity tool known as the Large Amplitude Motion Program (LAMP). SimpleCode and LAMP data were generated by common bi-modal, bi-directional sea conditions in the North Atlantic as training data. After training an LSTM network with LAMP ship motion response data, a sample route was traversed and randomly sampled historical weather was input into SimpleCode and the LSTM network, and compared against the higher fidelity results.
</details>
<details>
<summary>摘要</summary>
这篇论文将提出一种多优劣、数据适应的方法，使用长Short-Term Memory（LSTM）神经网络来估算船舶响应统计在二模性、双向海域中。这项研究将使用一个快速低精度的代码工具SimpleCode，以及一个更高精度的工具——Large Amplitude Motion Program（LAMP）。SimpleCode和LAMP数据都是通过常见的二模性、双向海域Conditions在北大西洋进行训练的。接下来，我们将在LAMP船舶响应数据上训练LSTM网络，然后在SimpleCode和LSTM网络中随机选择历史天气数据，并与更高精度结果进行比较。
</details></li>
</ul>
<hr>
<h2 id="Local-or-Global-Selective-Knowledge-Assimilation-for-Federated-Learning-with-Limited-Labels"><a href="#Local-or-Global-Selective-Knowledge-Assimilation-for-Federated-Learning-with-Limited-Labels" class="headerlink" title="Local or Global: Selective Knowledge Assimilation for Federated Learning with Limited Labels"></a>Local or Global: Selective Knowledge Assimilation for Federated Learning with Limited Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08809">http://arxiv.org/abs/2307.08809</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yae Jee Cho, Gauri Joshi, Dimitrios Dimitriadis</li>
<li>for: 实际上的 Federated Learning (FL) 方法假设每个客户端都有完整的标签数据，但在实际情况下，客户端的标签数据仅具有有限的量，因为标签的过程是时consuming 和劳动的。</li>
<li>methods: 我们提出了 FedLabel，一种基于选择性标签的方法，让客户端选择使用本地或全球模型 pseudo-label 其未标的数据，具体来说，客户端根据其数据的特性选择使用哪一个模型，以确保模型对数据的掌握程度最高。我们还使用了全球和本地模型的知识，通过全球-本地一致调整，以降低两个模型在标签未知数据时的差异。</li>
<li>results: 我们在cross-device 和 cross-silo 设定下，与其他半结构化 FL 基elines 比较，获得了8%-24%的提高，甚至在仅有5%-20%的标签数据下，可以超越标准的完全监督 FL 基elines ($100%$ 标签数据)。<details>
<summary>Abstract</summary>
Many existing FL methods assume clients with fully-labeled data, while in realistic settings, clients have limited labels due to the expensive and laborious process of labeling. Limited labeled local data of the clients often leads to their local model having poor generalization abilities to their larger unlabeled local data, such as having class-distribution mismatch with the unlabeled data. As a result, clients may instead look to benefit from the global model trained across clients to leverage their unlabeled data, but this also becomes difficult due to data heterogeneity across clients. In our work, we propose FedLabel where clients selectively choose the local or global model to pseudo-label their unlabeled data depending on which is more of an expert of the data. We further utilize both the local and global models' knowledge via global-local consistency regularization which minimizes the divergence between the two models' outputs when they have identical pseudo-labels for the unlabeled data. Unlike other semi-supervised FL baselines, our method does not require additional experts other than the local or global model, nor require additional parameters to be communicated. We also do not assume any server-labeled data or fully labeled clients. For both cross-device and cross-silo settings, we show that FedLabel outperforms other semi-supervised FL baselines by $8$-$24\%$, and even outperforms standard fully supervised FL baselines ($100\%$ labeled data) with only $5$-$20\%$ of labeled data.
</details>
<details>
<summary>摘要</summary>
许多现有的FL方法假设客户端拥有完整的标签数据，然而在现实中，客户端通常有限的标签数据，因为标签数据的生成和评估是成本和劳动 INTENSIVE 的过程。客户端的局部模型通常因为地方数据的不充分标注而导致其在大量未标注数据上的泛化能力差。为了解决这个问题，客户端可能会寻求从全球模型中吸取优势，但是由于客户端数据的不一致，这也变得困难。在我们的工作中，我们提出了FedLabel方法，其中客户端选择地ocal或全球模型来 Pseudo-label 其未标注数据，根据哪一个模型更加熟悉数据。我们还利用了两个模型的知识，通过全球-局部一致准则来减少两个模型输出不同的偏差。与其他半导导FL基线相比，我们的方法不需要额外的专家，也不需要额外的参数进行通信。我们也不假设服务器拥有完整的标签数据或全部客户端拥有完整的标签数据。在跨设备和跨笼设置下，我们展示了FedLabel方法可以与其他半导导FL基线相比，提高$8$-$24\%$，甚至超过了完全半导导FL基线（$100\%$ 标签数据），只需要$5$-$20\%$ 的标签数据。
</details></li>
</ul>
<hr>
<h2 id="Non-Stationary-Policy-Learning-for-Multi-Timescale-Multi-Agent-Reinforcement-Learning"><a href="#Non-Stationary-Policy-Learning-for-Multi-Timescale-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Non-Stationary Policy Learning for Multi-Timescale Multi-Agent Reinforcement Learning"></a>Non-Stationary Policy Learning for Multi-Timescale Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08794">http://arxiv.org/abs/2307.08794</a></li>
<li>repo_url: None</li>
<li>paper_authors: Patrick Emami, Xiangyu Zhang, David Biagioni, Ahmed S. Zamzam</li>
<li>for: 这个论文是为了学习多时间步长的多代理RL（Multi-timescale Multi-agent Reinforcement Learning）中的非站ARY政策而写的。</li>
<li>methods: 这个论文使用了可用代理时间尺度信息来定义周期时间编码，并通过 periodic multi-agent policy 来学习非站ARY政策。</li>
<li>results: 论文通过 theoretically 和实验 validate 了 periodic multi-agent policy 的学习效果，并在 gridworld 和建筑能源管理环境中证明了该方法的有效性。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
In multi-timescale multi-agent reinforcement learning (MARL), agents interact across different timescales. In general, policies for time-dependent behaviors, such as those induced by multiple timescales, are non-stationary. Learning non-stationary policies is challenging and typically requires sophisticated or inefficient algorithms. Motivated by the prevalence of this control problem in real-world complex systems, we introduce a simple framework for learning non-stationary policies for multi-timescale MARL. Our approach uses available information about agent timescales to define a periodic time encoding. In detail, we theoretically demonstrate that the effects of non-stationarity introduced by multiple timescales can be learned by a periodic multi-agent policy. To learn such policies, we propose a policy gradient algorithm that parameterizes the actor and critic with phase-functioned neural networks, which provide an inductive bias for periodicity. The framework's ability to effectively learn multi-timescale policies is validated on a gridworld and building energy management environment.
</details>
<details>
<summary>摘要</summary>
在多时间步长多代理人学习（MARL）中，代理人之间的交互具有不同的时间步长。通常，由多个时间步长引起的政策是非站ARY的。学习非站ARY的政策是困难的，通常需要复杂或不效率的算法。为了解决实际世界中这种控制问题，我们提出了一个简单的框架 для学习非站ARY的多时间步长MARL政策。我们利用代理人的时间步长信息来定义 periodic 时间编码。在详细的演示中，我们证明了由多个时间步长引起的非站ARY效果可以通过 periodic 多代理人政策学习。为了学习这种政策，我们提议一种基于phasic 神经网络的策略梯度算法，该算法在actor和critic中使用phasic 神经网络作为参数，从而提供了periodicity的偏好。该框架在Gridworld和建筑能源管理环境中证明了有效地学习多时间步长政策。
</details></li>
</ul>
<hr>
<h2 id="On-the-Real-Time-Semantic-Segmentation-of-Aphid-Clusters-in-the-Wild"><a href="#On-the-Real-Time-Semantic-Segmentation-of-Aphid-Clusters-in-the-Wild" class="headerlink" title="On the Real-Time Semantic Segmentation of Aphid Clusters in the Wild"></a>On the Real-Time Semantic Segmentation of Aphid Clusters in the Wild</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10267">http://arxiv.org/abs/2307.10267</a></li>
<li>repo_url: None</li>
<li>paper_authors: Raiyan Rahman, Christopher Indris, Tianxiao Zhang, Kaidong Li, Brian McCornack, Daniel Flippo, Ajay Sharda, Guanghui Wang</li>
<li>for:  addresses the urgent need for an intelligent autonomous system to locate and spray aphid infestations in wheat and sorghum fields, reducing pesticide use and environmental impact.</li>
<li>methods:  uses real-time semantic segmentation models to segment clusters of aphids in complex crop canopies, with a multiscale dataset to allow for learning at different scales.</li>
<li>results:  compares the segmentation speeds and accuracy of four state-of-the-art real-time semantic segmentation models on an aphid cluster dataset, demonstrating the effectiveness of a real-time solution for pest detection and reducing inefficient pesticide use.<details>
<summary>Abstract</summary>
Aphid infestations can cause extensive damage to wheat and sorghum fields and spread plant viruses, resulting in significant yield losses in agriculture. To address this issue, farmers often rely on chemical pesticides, which are inefficiently applied over large areas of fields. As a result, a considerable amount of pesticide is wasted on areas without pests, while inadequate amounts are applied to areas with severe infestations. The paper focuses on the urgent need for an intelligent autonomous system that can locate and spray infestations within complex crop canopies, reducing pesticide use and environmental impact. We have collected and labeled a large aphid image dataset in the field, and propose the use of real-time semantic segmentation models to segment clusters of aphids. A multiscale dataset is generated to allow for learning the clusters at different scales. We compare the segmentation speeds and accuracy of four state-of-the-art real-time semantic segmentation models on the aphid cluster dataset, benchmarking them against nonreal-time models. The study results show the effectiveness of a real-time solution, which can reduce inefficient pesticide use and increase crop yields, paving the way towards an autonomous pest detection system.
</details>
<details>
<summary>摘要</summary>
螨子感染可以对小麦和粟米田场造成广泛的损害，并传播植物病毒，导致农业生产的生产损失。为解决这个问题，农民们经常依靠化学杀虫剂，但这些杀虫剂通常在大面积的田场上不充分应用，导致大量的杀虫剂浪费在没有螨子的区域，而应用到螨子严重感染的区域不充分。本文强调需要一个智能自动化系统，可以在复杂的作物顶层中找到和喷洒感染区域，从而减少杀虫剂的使用和环境的影响。我们收集了大量螨子图像数据集，并提议使用实时semantic segmentation模型来 segment clusters of aphids。我们生成了多尺度的数据集，以便在不同的尺度上学习螨子集。我们对四种当前最佳实时semantic segmentation模型进行比较，以测试它们在aphid cluster数据集上的segmentation速度和准确率。研究结果表明，实时解决方案可以减少不必要的杀虫剂使用，提高作物的收成，为自动检测螨子系统开创了道路。
</details></li>
</ul>
<hr>
<h2 id="GEAR-Augmenting-Language-Models-with-Generalizable-and-Efficient-Tool-Resolution"><a href="#GEAR-Augmenting-Language-Models-with-Generalizable-and-Efficient-Tool-Resolution" class="headerlink" title="GEAR: Augmenting Language Models with Generalizable and Efficient Tool Resolution"></a>GEAR: Augmenting Language Models with Generalizable and Efficient Tool Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08775">http://arxiv.org/abs/2307.08775</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yining610/gear">https://github.com/yining610/gear</a></li>
<li>paper_authors: Yining Lu, Haoping Yu, Daniel Khashabi</li>
<li>for: 提高大语言模型（LLM）的性能 across 多种任务。</li>
<li>methods: 使用外部工具，但之前的工作过于依赖于任务特定的示范，这限制了其通用性和计算成本。我们介绍了一种名为 GEAR 的计算效率高的问题工具固定算法，可以在多种任务中使用不同的工具。</li>
<li>results: 在 14 个数据集和 6 个下游任务上进行了评估，并示出了强大的通用性，可以在新任务、工具和不同的 SLM 上进行更好的工具固定。尽管提供更高效的计算，但 GEAR 可以达到更高的工具固定精度，从而提高下游精度，例如，使用 GEAR 修改 GPT-J 和 GPT-3 可以超过对应的工具修改基eline。<details>
<summary>Abstract</summary>
Augmenting large language models (LLM) to use external tools enhances their performance across a variety of tasks. However, prior works over-rely on task-specific demonstration of tool use that limits their generalizability and computational cost due to making many calls to large-scale LLMs. We introduce GEAR, a computationally efficient query-tool grounding algorithm that is generalizable to various tasks that require tool use while not relying on task-specific demonstrations. GEAR achieves better efficiency by delegating tool grounding and execution to small language models (SLM) and LLM, respectively; while leveraging semantic and pattern-based evaluation at both question and answer levels for generalizable tool grounding. We evaluate GEAR on 14 datasets across 6 downstream tasks, demonstrating its strong generalizability to novel tasks, tools and different SLMs. Despite offering more efficiency, GEAR achieves higher precision in tool grounding compared to prior strategies using LLM prompting, thus improving downstream accuracy at a reduced computational cost. For example, we demonstrate that GEAR-augmented GPT-J and GPT-3 outperform counterpart tool-augmented baselines because of better tool use.
</details>
<details>
<summary>摘要</summary>
加强大语言模型（LLM）使用外部工具可以提高其在多种任务上的表现。然而，先前的工作过于依赖任务特定的工具使用示例，这限制了它们的普遍性和计算成本，因为它们需要访问大规模的 LLM。我们介绍了一种计算效率高的问题工具固定算法（GEAR），可以在多种需要工具使用的任务上实现更好的普遍性，而不需要任务特定的示例。GEAR通过委托工具固定和执行给小语言模型（SLM）和 LLM 分别执行，同时利用 semantic 和 pattern 基于的评估方法来实现更好的工具固定。我们在 14 个数据集上进行了 6 个下游任务的评估，并证明了 GEAR 在新任务、工具和不同的 SLM 上都具有强大的普遍性。尽管 GEAR 提供更高的效率，但它在工具固定精度方面仍然高于先前使用 LLM 提示的策略，因此在减少计算成本的情况下提高了下游精度。例如，我们表明了 GEAR 对 GPT-J 和 GPT-3 的加强可以超过相应的工具加强基eline。
</details></li>
</ul>
<hr>
<h2 id="AI-empowering-research-10-ways-how-science-can-benefit-from-AI"><a href="#AI-empowering-research-10-ways-how-science-can-benefit-from-AI" class="headerlink" title="AI empowering research: 10 ways how science can benefit from AI"></a>AI empowering research: 10 ways how science can benefit from AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10265">http://arxiv.org/abs/2307.10265</a></li>
<li>repo_url: None</li>
<li>paper_authors: César França</li>
<li>for: 这篇论文探讨人工智能（AI）在科学研究中的转变性影响。</li>
<li>methods: 论文介绍了AI在科学研究中的10种应用，包括强大的引用工具、更好地理解研究问题、增强研究问题生成、优化研究设计、数据生成、数据转换、高级数据分析和AI协助报告。</li>
<li>results: 论文指出，AI可以帮助科学家增强创造力，但需要考虑偏见、隐私问题和人AI合作。<details>
<summary>Abstract</summary>
This article explores the transformative impact of artificial intelligence (AI) on scientific research. It highlights ten ways in which AI is revolutionizing the work of scientists, including powerful referencing tools, improved understanding of research problems, enhanced research question generation, optimized research design, stub data generation, data transformation, advanced data analysis, and AI-assisted reporting. While AI offers numerous benefits, challenges such as bias, privacy concerns, and the need for human-AI collaboration must be considered. The article emphasizes that AI can augment human creativity in science but not replace it.
</details>
<details>
<summary>摘要</summary>
这篇文章探讨人工智能（AI）在科学研究中的转变性影响。文章提出了10种AI在科学家工作中的改变方式，包括强大的引用工具、更好地理解研究问题、增强研究问题生成、优化研究设计、干扰数据生成、数据转换、高级数据分析和AI助手报告。虽然AI具有许多优势，但需要考虑偏见、隐私问题和人机合作。文章表达AI可以补充人类创造力，但不能取代它。Here's the translation of the text into Traditional Chinese:这篇文章探讨人工智能（AI）在科学研究中的转换性影响。文章提出了10种AI在科学家工作中的改变方式，包括强大的引用工具、更好地理解研究问题、增强研究问题生成、优化研究设计、干扰数据生成、数据转换、高级数据分析和AI助手报告。处理AI具有许多优势，但需要考虑偏见、隐私问题和人机合作。文章表达AI可以补充人类创造力，但不能取代它。
</details></li>
</ul>
<hr>
<h2 id="Reflections-from-the-Workshop-on-AI-Assisted-Decision-Making-for-Conservation"><a href="#Reflections-from-the-Workshop-on-AI-Assisted-Decision-Making-for-Conservation" class="headerlink" title="Reflections from the Workshop on AI-Assisted Decision Making for Conservation"></a>Reflections from the Workshop on AI-Assisted Decision Making for Conservation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08774">http://arxiv.org/abs/2307.08774</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lily Xu, Esther Rolf, Sara Beery, Joseph R. Bennett, Tanya Berger-Wolf, Tanya Birch, Elizabeth Bondi-Kelly, Justin Brashares, Melissa Chapman, Anthony Corso, Andrew Davies, Nikhil Garg, Angela Gaylard, Robert Heilmayr, Hannah Kerner, Konstantin Klemmer, Vipin Kumar, Lester Mackey, Claire Monteleoni, Paul Moorcroft, Jonathan Palmer, Andrew Perrault, David Thau, Milind Tambe</li>
<li>for: 这份白皮书总结了在哈佛大学计算社会中心主办的AI助成决策工作坊上的演讲和讨论，旨在提出保护生态系统的开放研究问题，以及需要人工智能解决的保护挑战。</li>
<li>methods: 这份白皮书总结了工作坊上的讲座和讨论，并提出了一些开放研究问题，例如资源分配、规划和干预的算法化决策方法，以及如何应用这些方法解决生态系统的保护挑战。</li>
<li>results: 白皮书认为，AI助成决策方法可以帮助解决生态系统的保护挑战，但是还需要进一步的研究和开发，以确保这些方法能够应用到实际的保护场景中。<details>
<summary>Abstract</summary>
In this white paper, we synthesize key points made during presentations and discussions from the AI-Assisted Decision Making for Conservation workshop, hosted by the Center for Research on Computation and Society at Harvard University on October 20-21, 2022. We identify key open research questions in resource allocation, planning, and interventions for biodiversity conservation, highlighting conservation challenges that not only require AI solutions, but also require novel methodological advances. In addition to providing a summary of the workshop talks and discussions, we hope this document serves as a call-to-action to orient the expansion of algorithmic decision-making approaches to prioritize real-world conservation challenges, through collaborative efforts of ecologists, conservation decision-makers, and AI researchers.
</details>
<details>
<summary>摘要</summary>
在这份白皮书中，我们对哈佛大学计算社会中心在2022年10月20-21日举行的AI助成决策为保护生态系统工作shop的演讲和讨论进行了总结和分析。我们确定了保护生态系统的开放研究问题，包括资源分配、规划和干预，并指出了需要AI解决方案的保护挑战。此外，我们希望通过协作的方式，让生态学家、保护决策者和AI研究人员共同努力，推动算法决策方法的扩展，以便更好地应对实际生态系统保护挑战。
</details></li>
</ul>
<hr>
<h2 id="A-mixed-policy-to-improve-performance-of-language-models-on-math-problems"><a href="#A-mixed-policy-to-improve-performance-of-language-models-on-math-problems" class="headerlink" title="A mixed policy to improve performance of language models on math problems"></a>A mixed policy to improve performance of language models on math problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08767">http://arxiv.org/abs/2307.08767</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vividitytech/math_lm_rl">https://github.com/vividitytech/math_lm_rl</a></li>
<li>paper_authors: Gang Chen</li>
<li>for: 解决 math 问题时，语言模型通常采用采样策略来预测下一个词的概率。但这会导致 math 问题的解决结果不准确。为了解决这个问题，我们提出了一种混合策略探索方法，使用 reinforcement learning 来解决 math 问题。</li>
<li>methods: 我们提出了一种两级 токен探索策略，其中一级是概率采样，二级是决定性选择下一个tokен的最高分选择策略。具体来说，抽象层策略会根据概率采样决定下一个tokен是操作符或操作数，而第二级策略则是在决定性下选择下一个tokен的最高分选择。</li>
<li>results: 我们在 GSM8K 数据集上使用 GPT-2 模型进行测试，并达到了 более $2%$ 的性能提升。我们的实现可以在 GitHub 上找到：<a target="_blank" rel="noopener" href="https://github.com/vividitytech/math_lm_rl%E3%80%82">https://github.com/vividitytech/math_lm_rl。</a><details>
<summary>Abstract</summary>
When to solve math problems, most language models take a sampling strategy to predict next word according conditional probabilities. In the math reasoning step, it may generate wrong answer. Considering math problems are deterministic, we propose a mixed policy exploration approach to solve math problems with reinforcement learning. In peculiar, we propose a two level token exploration policy: the abstract level explores next token with probability and the second level is deterministic. Specifically, the abstract level policy will decide whether the token is operator or operand with probability sampling, while the second level is deterministic to select next token with the highest score in a greedy way. We test our method on GSM8K dataset with GPT-2 model, and demonstrate more than $2\%$ performance gain. Our implementation is available at https://github.com/vividitytech/math_lm_rl.
</details>
<details>
<summary>摘要</summary>
当解决数学问题时，大多数语言模型采用采样策略来预测下一个词的 conditional probabilities。在数学逻辑步骤中，它可能生成错误答案。考虑到数学问题是deterministic的，我们提出了混合策略探索方法来解决数学问题使用强化学习。具体来说，我们提出了两级токен探索策略：第一级探索下一个词的概率，第二级是决定性的选择下一个词的最高分。 Specifically, the first-level policy will decide whether the token is an operator or operand with probability sampling, while the second-level policy is deterministic to select the next token with the highest score in a greedy way. 我们在GSM8K dataset上使用GPT-2模型测试了我们的方法，并证明了 más de $2\%$ 的性能提升。我们的实现可以在https://github.com/vividitytech/math_lm_rl上找到。
</details></li>
</ul>
<hr>
<h2 id="Quality-Assessment-of-Photoplethysmography-Signals-For-Cardiovascular-Biomarkers-Monitoring-Using-Wearable-Devices"><a href="#Quality-Assessment-of-Photoplethysmography-Signals-For-Cardiovascular-Biomarkers-Monitoring-Using-Wearable-Devices" class="headerlink" title="Quality Assessment of Photoplethysmography Signals For Cardiovascular Biomarkers Monitoring Using Wearable Devices"></a>Quality Assessment of Photoplethysmography Signals For Cardiovascular Biomarkers Monitoring Using Wearable Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08766">http://arxiv.org/abs/2307.08766</a></li>
<li>repo_url: None</li>
<li>paper_authors: Felipe M. Dias, Marcelo A. F. Toledo, Diego A. C. Cardenas, Douglas A. Almeida, Filipe A. C. Oliveira, Estela Ribeiro, Jose E. Krieger, Marco A. Gutierrez</li>
<li>for: 这项研究用于评估非侵入式光谱学技术（PPG）的可靠性和准确性，以及开发远程、非侵入式和连续式测量设备。</li>
<li>methods: 该研究使用了27个统计特征从PPG信号中训练机器学习模型，包括梯度提升（XGBoost和CatBoost）和随机森林（RF）算法。</li>
<li>results: 研究发现，使用XGBoost、CatBoost和RF算法可以达到94.4、95.6和95.0的敏感度（Se）、正确预测率（PPV）和F1分数（F1），均高于当前文献报道的值。<details>
<summary>Abstract</summary>
Photoplethysmography (PPG) is a non-invasive technology that measures changes in blood volume in the microvascular bed of tissue. It is commonly used in medical devices such as pulse oximeters and wrist worn heart rate monitors to monitor cardiovascular hemodynamics. PPG allows for the assessment of parameters (e.g., heart rate, pulse waveform, and peripheral perfusion) that can indicate conditions such as vasoconstriction or vasodilation, and provides information about microvascular blood flow, making it a valuable tool for monitoring cardiovascular health. However, PPG is subject to a number of sources of variations that can impact its accuracy and reliability, especially when using a wearable device for continuous monitoring, such as motion artifacts, skin pigmentation, and vasomotion. In this study, we extracted 27 statistical features from the PPG signal for training machine-learning models based on gradient boosting (XGBoost and CatBoost) and Random Forest (RF) algorithms to assess quality of PPG signals that were labeled as good or poor quality. We used the PPG time series from a publicly available dataset and evaluated the algorithm s performance using Sensitivity (Se), Positive Predicted Value (PPV), and F1-score (F1) metrics. Our model achieved Se, PPV, and F1-score of 94.4, 95.6, and 95.0 for XGBoost, 94.7, 95.9, and 95.3 for CatBoost, and 93.7, 91.3 and 92.5 for RF, respectively. Our findings are comparable to state-of-the-art reported in the literature but using a much simpler model, indicating that ML models are promising for developing remote, non-invasive, and continuous measurement devices.
</details>
<details>
<summary>摘要</summary>
photoplethysmography (PPG) 是一种非侵入式技术，用于测量血液量变化在微血管床中的变化。它通常用于医疗器械中，如搏动计和胳膊上的心率测量仪器，以监测Cardiovascular 血液动力学。 PPG 允许评估一些参数（如心率、搏动波形和 péripheral perfusion），这些参数可能指示 condition 如血管收缩或血管 расширение，并提供关于 microvascular 血液流的信息，使其成为监测 Cardiovascular 健康的有用工具。然而，PPG 受到一些来源的变化的影响，这些变化可能影响其准确性和可靠性，特别是在使用可携带式设备进行连续监测时。在这种研究中，我们从 PPG 信号中提取了27个统计特征，用于训练机器学习模型，包括梯度拟合（XGBoost 和 CatBoost）和Random Forest（RF）算法。我们使用公共可用的数据集中的 PPG 时间序列，并评估算法的性能使用敏感度（Se）、正确预测值（PPV）和 F1 得分（F1）指标。我们的模型实现了 Se、PPV 和 F1 分别为 94.4、95.6 和 95.0，94.7、95.9 和 95.3，93.7、91.3 和 92.5。我们的发现与文献中的状态艺术相比，但使用 much simpler 模型，表明机器学习模型是开发 remote、非侵入式、连续测量设备的有力的选择。
</details></li>
</ul>
<hr>
<h2 id="Fast-model-inference-and-training-on-board-of-Satellites"><a href="#Fast-model-inference-and-training-on-board-of-Satellites" class="headerlink" title="Fast model inference and training on-board of Satellites"></a>Fast model inference and training on-board of Satellites</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08700">http://arxiv.org/abs/2307.08700</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/previtus/ravaen-unibap-dorbit">https://github.com/previtus/ravaen-unibap-dorbit</a></li>
<li>paper_authors: Vít Růžička, Gonzalo Mateo-García, Chris Bridges, Chris Brunskill, Cormac Purcell, Nicolas Longépé, Andrew Markham</li>
<li>for: 本研究旨在实现在 cubeSat 上部署了多任务模型，并在 satellite 上进行机器学习模型的训练。</li>
<li>methods: 本研究使用了一个轻量级基础模型叫做 RaVAEn，它可以将小图像组成缩排的维度向量，并可以支援多个下游任务。</li>
<li>results: 本研究获得了在 cubeSat 上部署 RaVAEn 模型，并在 satellite 上进行了快速的几何训练。 encoding 时间为 0.110 秒，并且可以在 satellite 上进行快速的数据预测和决策。<details>
<summary>Abstract</summary>
Artificial intelligence onboard satellites has the potential to reduce data transmission requirements, enable real-time decision-making and collaboration within constellations. This study deploys a lightweight foundational model called RaVAEn on D-Orbit's ION SCV004 satellite. RaVAEn is a variational auto-encoder (VAE) that generates compressed latent vectors from small image tiles, enabling several downstream tasks. In this work we demonstrate the reliable use of RaVAEn onboard a satellite, achieving an encoding time of 0.110s for tiles of a 4.8x4.8 km$^2$ area. In addition, we showcase fast few-shot training onboard a satellite using the latent representation of data. We compare the deployment of the model on the on-board CPU and on the available Myriad vision processing unit (VPU) accelerator. To our knowledge, this work shows for the first time the deployment of a multi-task model on-board a CubeSat and the on-board training of a machine learning model.
</details>
<details>
<summary>摘要</summary>
人工智能在卫星上有潜力减少数据传输要求、实时决策和卫星群 collaboration 。本研究部署了一个轻量级基础模型called RaVAEn 在D-Orbit的 ION SCV004卫星上。RaVAEn 是一种变量自动编码器（VAE），通过生成压缩的幂量向量来实现多个下游任务。在这个研究中，我们成功地在卫星上使用 RaVAEn，编码时间为 0.110 秒，对一个 4.8x4.8 km$^2$ 的区域进行编码。此外，我们还展示了在卫星上快速培训 few-shot 的 latent 表示。我们比较了在卫星上 CPU 和可用的 Myriad 视觉处理器（VPU）加速器上部署模型的性能。根据我们所知，这是第一次在 CubeSat 上部署多任务模型，以及在卫星上培训机器学习模型。
</details></li>
</ul>
<hr>
<h2 id="Pair-then-Relation-Pair-Net-for-Panoptic-Scene-Graph-Generation"><a href="#Pair-then-Relation-Pair-Net-for-Panoptic-Scene-Graph-Generation" class="headerlink" title="Pair then Relation: Pair-Net for Panoptic Scene Graph Generation"></a>Pair then Relation: Pair-Net for Panoptic Scene Graph Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08699">http://arxiv.org/abs/2307.08699</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/king159/pair-net">https://github.com/king159/pair-net</a></li>
<li>paper_authors: Jinghao Wang, Zhengyu Wen, Xiangtai Li, Zujin Guo, Jingkang Yang, Ziwei Liu</li>
<li>for: 本研究旨在提出一种新的基线方法来解决Scene Graph Generation（SGG）中的Panoptic Scene Graph（PSG）问题，以创造更加全面的场景图表示。</li>
<li>methods: 我们首先进行了深入分析，发现当前PSG方法中最大的瓶颈在于对每个对象之间的对应关系的缺失。基于这一点，我们提出了一种新的框架：Pair then Relation（Pair-Net），它使用一个Pair Proposal Network（PPN）来学习和筛选对象之间的笔直关系。</li>
<li>results: 我们通过了广泛的ablation和分析，证明了我们的方法可以大幅提高现有基线的性能。尤其是，我们的方法在PSG benchmark上实现了新的州际纪录，与PSGFormer相比增加了超过10%的绝对提升。代码可以在<a target="_blank" rel="noopener" href="https://github.com/king159/Pair-Net%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/king159/Pair-Net上获取。</a><details>
<summary>Abstract</summary>
Panoptic Scene Graph (PSG) is a challenging task in Scene Graph Generation (SGG) that aims to create a more comprehensive scene graph representation using panoptic segmentation instead of boxes. Compared to SGG, PSG has several challenging problems: pixel-level segment outputs and full relationship exploration (It also considers thing and stuff relation). Thus, current PSG methods have limited performance, which hinders downstream tasks or applications. The goal of this work aims to design a novel and strong baseline for PSG. To achieve that, we first conduct an in-depth analysis to identify the bottleneck of the current PSG models, finding that inter-object pair-wise recall is a crucial factor that was ignored by previous PSG methods. Based on this and the recent query-based frameworks, we present a novel framework: Pair then Relation (Pair-Net), which uses a Pair Proposal Network (PPN) to learn and filter sparse pair-wise relationships between subjects and objects. Moreover, we also observed the sparse nature of object pairs for both Motivated by this, we design a lightweight Matrix Learner within the PPN, which directly learn pair-wised relationships for pair proposal generation. Through extensive ablation and analysis, our approach significantly improves upon leveraging the segmenter solid baseline. Notably, our method achieves new state-of-the-art results on the PSG benchmark, with over 10\% absolute gains compared to PSGFormer. The code of this paper is publicly available at https://github.com/king159/Pair-Net.
</details>
<details>
<summary>摘要</summary>
panographic scene graph (PSG) 是Scene Graph Generation (SGG) 中一个挑战性的任务，旨在通过�annoying segmentation而非盒子来创建更加全面的场景图表示。相比SGG，PSG具有一些挑战性的问题：像素级别的分割输出和全面关系探索（同时考虑thing和stuff关系）。因此，当前PSG方法的性能有限，这阻碍了下游任务或应用。本文的目标是设计一个新的和强大的PSG基eline。为达到这一目标，我们首先进行了深入分析，并发现了当前PSG模型的瓶颈在于对象间对之间的recall问题。基于这一点和最近的查询基础框架，我们提出了一个新的框架：Pair then Relation（Pair-Net），它使用一个Pair Proposal Network（PPN）来学习和筛选对象间的笔 Edwards，并且对于对象对的笔 Edwards进行了灵活的学习。此外，我们还发现了对象对的笔 Edwards是稀疏的，因此我们设计了一个轻量级的矩阵学习器，直接学习对象间的笔 Edwards。经过广泛的拟合和分析，我们的方法在使用 segmenter 固定基线上显著提高了PSG的性能，并达到了PSGFormer 的新的国际纪录。代码可以在 <https://github.com/king159/Pair-Net> 中获取。
</details></li>
</ul>
<hr>
<h2 id="COLLIE-Systematic-Construction-of-Constrained-Text-Generation-Tasks"><a href="#COLLIE-Systematic-Construction-of-Constrained-Text-Generation-Tasks" class="headerlink" title="COLLIE: Systematic Construction of Constrained Text Generation Tasks"></a>COLLIE: Systematic Construction of Constrained Text Generation Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08689">http://arxiv.org/abs/2307.08689</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/princeton-nlp/Collie">https://github.com/princeton-nlp/Collie</a></li>
<li>paper_authors: Shunyu Yao, Howard Chen, Austin W. Hanjie, Runzhe Yang, Karthik Narasimhan</li>
<li>For:  This paper is written for those interested in natural language processing and the development of constrained text generation systems.* Methods:  The paper presents a grammar-based framework called COLLIE, which allows for the specification of rich and compositional constraints for diverse generation levels and modeling challenges. The framework includes tools for automatic extraction of task instances given a constraint structure and a raw text corpus.* Results:  The paper compiles a dataset called COLLIE-v1, which includes 2080 instances with 13 constraint structures, and performs systematic experiments with five state-of-the-art instruction-tuned language models to analyze their performances and reveal shortcomings.<details>
<summary>Abstract</summary>
Text generation under constraints have seen increasing interests in natural language processing, especially with the rapidly improving capabilities of large language models. However, existing benchmarks for constrained generation usually focus on fixed constraint types (e.g.,generate a sentence containing certain words) that have proved to be easy for state-of-the-art models like GPT-4. We present COLLIE, a grammar-based framework that allows the specification of rich, compositional constraints with diverse generation levels (word, sentence, paragraph, passage) and modeling challenges (e.g.,language understanding, logical reasoning, counting, semantic planning). We also develop tools for automatic extraction of task instances given a constraint structure and a raw text corpus. Using COLLIE, we compile the COLLIE-v1 dataset with 2080 instances comprising 13 constraint structures. We perform systematic experiments across five state-of-the-art instruction-tuned language models and analyze their performances to reveal shortcomings. COLLIE is designed to be extensible and lightweight, and we hope the community finds it useful to develop more complex constraints and evaluations in the future.
</details>
<details>
<summary>摘要</summary>
To address this limitation, we present COLLIE, a grammar-based framework that enables the specification of rich and compositional constraints at various generation levels (word, sentence, paragraph, passage) and with diverse modeling challenges (e.g., language understanding, logical reasoning, counting, semantic planning). We also develop tools for automatically extracting task instances from a constraint structure and a raw text corpus.Using COLLIE, we compiled the COLLIE-v1 dataset consisting of 2080 instances with 13 constraint structures. We conducted systematic experiments on five state-of-the-art instruction-tuned language models and analyzed their performances to reveal their shortcomings.COLLIE is designed to be extensible and lightweight, and we hope that the community will find it useful to develop more complex constraints and evaluations in the future.
</details></li>
</ul>
<hr>
<h2 id="Do-Models-Explain-Themselves-Counterfactual-Simulatability-of-Natural-Language-Explanations"><a href="#Do-Models-Explain-Themselves-Counterfactual-Simulatability-of-Natural-Language-Explanations" class="headerlink" title="Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations"></a>Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08678">http://arxiv.org/abs/2307.08678</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanda Chen, Ruiqi Zhong, Narutatsu Ri, Chen Zhao, He He, Jacob Steinhardt, Zhou Yu, Kathleen McKeown</li>
<li>for: 本研究旨在评估大语言模型（LLM）的自我解释能力，以及LLM可以帮助人类构建模型处理不同输入的心理模型。</li>
<li>methods: 我们提出了两种基于对反事实可靠性的评估指标：准确性和通用性。我们使用自动生成的反事实来评估现状的State-of-the-art LLMs（如GPT-4）在多步骤事实理解和奖励模型任务中的性能。</li>
<li>results: 我们发现LLM的解释准确性较低，并且准确性与可能性无关。因此，不仅通过人类批准（如RLHF）优化是不够的。<details>
<summary>Abstract</summary>
Large language models (LLMs) are trained to imitate humans to explain human decisions. However, do LLMs explain themselves? Can they help humans build mental models of how LLMs process different inputs? To answer these questions, we propose to evaluate $\textbf{counterfactual simulatability}$ of natural language explanations: whether an explanation can enable humans to precisely infer the model's outputs on diverse counterfactuals of the explained input. For example, if a model answers "yes" to the input question "Can eagles fly?" with the explanation "all birds can fly", then humans would infer from the explanation that it would also answer "yes" to the counterfactual input "Can penguins fly?". If the explanation is precise, then the model's answer should match humans' expectations.   We implemented two metrics based on counterfactual simulatability: precision and generality. We generated diverse counterfactuals automatically using LLMs. We then used these metrics to evaluate state-of-the-art LLMs (e.g., GPT-4) on two tasks: multi-hop factual reasoning and reward modeling. We found that LLM's explanations have low precision and that precision does not correlate with plausibility. Therefore, naively optimizing human approvals (e.g., RLHF) may not be a sufficient solution.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）通常被训练以模仿人类来解释人类决策。然而， LLM 是否能够解释自己？可以 LLM 帮助人类建立模型处理不同输入的心理模型吗？为了回答这些问题，我们提议评估 $\textbf{对假实际可能性}$ 的自然语言解释：是否可以通过解释来准确地预测模型对不同的对假输入的输出。例如，如果模型对问题 "鸟可以飞吗?" 的输入提供了解释 "所有鸟类都可以飞"，那么人类就可以从解释中推断出模型对 "鸟不能飞" 的对假输入的输出是什么。如果解释准确，那么模型的输出应该与人类的预期匹配。我们实现了两个基于对假实际可能性的度量：准确性和通用性。我们使用 LLM 自动生成了多种对假输入。然后，我们使用这两个度量来评估现状最佳的 LLM (例如 GPT-4) 在多步真实逻辑和奖励模型两个任务上的表现。我们发现 LLM 的解释准确性较低，并且准确性与可能性之间没有直接的相关性。因此，不仅通过人类批准（例如 RLHF）来优化模型可能不够。
</details></li>
</ul>
<hr>
<h2 id="TableGPT-Towards-Unifying-Tables-Nature-Language-and-Commands-into-One-GPT"><a href="#TableGPT-Towards-Unifying-Tables-Nature-Language-and-Commands-into-One-GPT" class="headerlink" title="TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT"></a>TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08674">http://arxiv.org/abs/2307.08674</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liangyu Zha, Junlin Zhou, Liyao Li, Rui Wang, Qingyi Huang, Saisai Yang, Jing Yuan, Changbao Su, Xiang Li, Aofeng Su, Tao Zhang, Chen Zhou, Kaizhe Shou, Miao Wang, Wufang Zhu, Guoshan Lu, Chao Ye, Yali Ye, Wentao Ye, Yiming Zhang, Xinglong Deng, Jie Xu, Haobo Wang, Gang Chen, Junbo Zhao</li>
<li>For: TableGPT is a unified fine-tuned framework that enables large language models (LLMs) to understand and operate on tables using external functional commands, allowing for seamless interaction with tabular data and enabling a wide range of functionalities such as question answering, data manipulation, data visualization, analysis report generation, and automated prediction.* Methods: TableGPT uses a novel concept of global tabular representations, which empowers LLMs to gain a comprehensive understanding of the entire table beyond meta-information. It jointly trains LLMs on both table and text modalities, achieving a deep understanding of tabular data and the ability to perform complex operations on tables through chain-of-command instructions.* Results: TableGPT offers several advantages, including being a self-contained system rather than relying on external API interfaces, supporting efficient data process flow, query rejection (when appropriate), and private deployment, enabling faster domain data fine-tuning and ensuring data privacy. These features enhance the framework’s adaptability to specific use cases.<details>
<summary>Abstract</summary>
Tables are prevalent in real-world databases, requiring significant time and effort for humans to analyze and manipulate. The advancements in large language models (LLMs) have made it possible to interact with tables using natural language input, bringing this capability closer to reality. In this paper, we present TableGPT, a unified fine-tuned framework that enables LLMs to understand and operate on tables using external functional commands. It introduces the capability to seamlessly interact with tables, enabling a wide range of functionalities such as question answering, data manipulation (e.g., insert, delete, query, and modify operations), data visualization, analysis report generation, and automated prediction. TableGPT aims to provide convenience and accessibility to users by empowering them to effortlessly leverage tabular data. At the core of TableGPT lies the novel concept of global tabular representations, which empowers LLMs to gain a comprehensive understanding of the entire table beyond meta-information. By jointly training LLMs on both table and text modalities, TableGPT achieves a deep understanding of tabular data and the ability to perform complex operations on tables through chain-of-command instructions. Importantly, TableGPT offers the advantage of being a self-contained system rather than relying on external API interfaces. Moreover, it supports efficient data process flow, query rejection (when appropriate) and private deployment, enabling faster domain data fine-tuning and ensuring data privacy, which enhances the framework's adaptability to specific use cases.
</details>
<details>
<summary>摘要</summary>
tables are prevalent in real-world databases, requiring significant time and effort for humans to analyze and manipulate. The advancements in large language models (LLMs) have made it possible to interact with tables using natural language input, bringing this capability closer to reality. In this paper, we present TableGPT, a unified fine-tuned framework that enables LLMs to understand and operate on tables using external functional commands. It introduces the capability to seamlessly interact with tables, enabling a wide range of functionalities such as question answering, data manipulation (e.g., insert, delete, query, and modify operations), data visualization, analysis report generation, and automated prediction. TableGPT aims to provide convenience and accessibility to users by empowering them to effortlessly leverage tabular data. At the core of TableGPT lies the novel concept of global tabular representations, which empowers LLMs to gain a comprehensive understanding of the entire table beyond meta-information. By jointly training LLMs on both table and text modalities, TableGPT achieves a deep understanding of tabular data and the ability to perform complex operations on tables through chain-of-command instructions. Importantly, TableGPT offers the advantage of being a self-contained system rather than relying on external API interfaces. Moreover, it supports efficient data process flow, query rejection (when appropriate) and private deployment, enabling faster domain data fine-tuning and ensuring data privacy, which enhances the framework's adaptability to specific use cases.
</details></li>
</ul>
<hr>
<h2 id="Quaternion-Convolutional-Neural-Networks-Current-Advances-and-Future-Directions"><a href="#Quaternion-Convolutional-Neural-Networks-Current-Advances-and-Future-Directions" class="headerlink" title="Quaternion Convolutional Neural Networks: Current Advances and Future Directions"></a>Quaternion Convolutional Neural Networks: Current Advances and Future Directions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08663">http://arxiv.org/abs/2307.08663</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gerardo Altamirano-Gomez, Carlos Gershenson</li>
<li>for: 本研究的目的是探讨量子值 convolutional neural network (QCNN) 的开发和应用。</li>
<li>methods: 本研究使用了现有的 QCNN 模型，并进行了系统性的分析和评估。</li>
<li>results: 研究发现，使用 QCNN 可以 achieve 同等或更好的性能，并且具有更少的参数。此外，QCNN 还可以捕捉到更多的信息，因此可以在各种应用中提供更好的性能。<details>
<summary>Abstract</summary>
Since their first applications, Convolutional Neural Networks (CNNs) have solved problems that have advanced the state-of-the-art in several domains. CNNs represent information using real numbers. Despite encouraging results, theoretical analysis shows that representations such as hyper-complex numbers can achieve richer representational capacities than real numbers, and that Hamilton products can capture intrinsic interchannel relationships. Moreover, in the last few years, experimental research has shown that Quaternion-Valued CNNs (QCNNs) can achieve similar performance with fewer parameters than their real-valued counterparts. This paper condenses research in the development of QCNNs from its very beginnings. We propose a conceptual organization of current trends and analyze the main building blocks used in the design of QCNN models. Based on this conceptual organization, we propose future directions of research.
</details>
<details>
<summary>摘要</summary>
自它们的首次应用，卷积神经网络（CNN）已经解决了许多领域的问题，提高了状态艺术。CNN表示信息使用实数。尽管获得了激励的结果，理论分析显示，幂复数可以实现更丰富的表达能力，而汉密尔顿产品可以捕捉内在的通道关系。此外，最近几年的实验室研究表明，四元值CNN（QCNN）可以与其实数对应的模型相比，使用 fewer 参数达到相似的性能。本文将从QCNN的开发的起源进行抽象，我们提出了现有趋势的概念组织，并分析了QCNN模型的主要构建块。基于这个概念组织，我们提出了未来的研究方向。
</details></li>
</ul>
<hr>
<h2 id="Hyperparameter-Tuning-Cookbook-A-guide-for-scikit-learn-PyTorch-river-and-spotPython"><a href="#Hyperparameter-Tuning-Cookbook-A-guide-for-scikit-learn-PyTorch-river-and-spotPython" class="headerlink" title="Hyperparameter Tuning Cookbook: A guide for scikit-learn, PyTorch, river, and spotPython"></a>Hyperparameter Tuning Cookbook: A guide for scikit-learn, PyTorch, river, and spotPython</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10262">http://arxiv.org/abs/2307.10262</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sequential-parameter-optimization/spotpython">https://github.com/sequential-parameter-optimization/spotpython</a></li>
<li>paper_authors: Thomas Bartz-Beielstein</li>
<li>for: 本文提供了一份完整的hyperparameter tuning指南，使用spotPython进行scikit-learn、PyTorch和river模型的优化。</li>
<li>methods: 本文使用spotPython的代理模型基本优化过程，并将hyperparameter tuning应用于sklearn模型 such as Support Vector Classification、Random Forests、Gradient Boosting (XGB)和K-nearest neighbors (KNN) 等，以及river中的Hoeffding Adaptive Tree Regressor。</li>
<li>results: 本文通过实践和步骤解释，为使用Python进行hyperparameter tuning提供了一个实用的开始点。特点包括Tensorboard、PyTorch Lightning、spotPython和river之间的交互，以及PyTorch和PyTorch Lightning训练工作流程的集成。<details>
<summary>Abstract</summary>
This document provides a comprehensive guide to hyperparameter tuning using spotPython for scikit-learn, PyTorch, and river. The first part introduces spotPython's surrogate model-based optimization process, while the second part focuses on hyperparameter tuning. Several case studies are presented, including hyperparameter tuning for sklearn models such as Support Vector Classification, Random Forests, Gradient Boosting (XGB), and K-nearest neighbors (KNN), as well as a Hoeffding Adaptive Tree Regressor from river. The integration of spotPython into the PyTorch and PyTorch Lightning training workflow is also discussed. With a hands-on approach and step-by-step explanations, this cookbook serves as a practical starting point for anyone interested in hyperparameter tuning with Python. Highlights include the interplay between Tensorboard, PyTorch Lightning, spotPython, and river. This publication is under development, with updates available on the corresponding webpage.
</details>
<details>
<summary>摘要</summary>
这份文档提供了使用 spotPython 进行 scikit-learn、PyTorch 和 river 模型hyperparameter tuning的全面指南。第一部分介绍了 spotPython 的代理模型基于优化过程，而第二部分则专注于 hyperparameter tuning。文档包含了多个案例研究，包括 scikit-learn 模型支持向量分类、Random Forests、Gradient Boosting (XGB) 和 K-nearest neighbors (KNN) 等模型的 hyperparameter tuning，以及来自 river 的 Hoeffding Adaptive Tree Regressor。文档还讲解了将 spotPython integrated into PyTorch 和 PyTorch Lightning 训练工作流程。以一种实践的方式和步骤说明，这本 cookbook 作为 Python 中 hyperparameter tuning 的实践入门点。文档在开发中，更新信息可以在相关 webpage 上获得。
</details></li>
</ul>
<hr>
<h2 id="Glamour-muscles-why-having-a-body-is-not-what-it-means-to-be-embodied"><a href="#Glamour-muscles-why-having-a-body-is-not-what-it-means-to-be-embodied" class="headerlink" title="Glamour muscles: why having a body is not what it means to be embodied"></a>Glamour muscles: why having a body is not what it means to be embodied</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08598">http://arxiv.org/abs/2307.08598</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shawn L. Beaulieu, Sam Kriegman</li>
<li>for: 提高智能机器的能力</li>
<li>methods: 使用embodiment方法</li>
<li>results: 生成更高级的智能工具<details>
<summary>Abstract</summary>
Embodiment has recently enjoyed renewed consideration as a means to amplify the faculties of smart machines. Proponents of embodiment seem to imply that optimizing for movement in physical space promotes something more than the acquisition of niche capabilities for solving problems in physical space. However, there is nothing in principle which should so distinguish the problem of action selection in physical space from the problem of action selection in more abstract spaces, like that of language. Rather, what makes embodiment persuasive as a means toward higher intelligence is that it promises to capture, but does not actually realize, contingent facts about certain bodies (living intelligence) and the patterns of activity associated with them. These include an active resistance to annihilation and revisable constraints on the processes that make the world intelligible. To be theoretically or practically useful beyond the creation of niche tools, we argue that "embodiment" cannot be the trivial fact of a body, nor its movement through space, but the perpetual negotiation of the function, design, and integrity of that body$\unicode{x2013}$that is, to participate in what it means to $\textit{constitute}$ a given body. It follows that computer programs which are strictly incapable of traversing physical space might, under the right conditions, be more embodied than a walking, talking robot.
</details>
<details>
<summary>摘要</summary>
现在，人工智能的实体化受到了新的重视。支持者们认为，通过 Physical space 中的运动优化智能机器的能力。然而，没有任何原理可以解释 why 在 Physical space 中的行动选择问题和其他更抽象的空间，如语言空间中的行动选择问题之间存在差异。实际上，embodiment 的吸引力来自于它承诺能够捕捉，但并没有实现，certain bodies （生命智能）和其活动模式之间的相互关系。这些关系包括活体抵抗灭亡和可修改的世界认知过程。如果旨在超越创建 nich 工具的理论或实践用途，我们认为embodiment 不能是身体的杂志性，也不能是运动在空间中的杂志性，而是通过参与到一个身体的定义、设计和完整性的协商来实现。从这个意义上来看，一个不能在 Physical space 中移动的计算机程序可能在某些条件下比一个步行、说话的 робот更加embodied。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/18/cs.AI_2023_07_18/" data-id="cloimip44000vs488dhk000zz" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_07_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/18/cs.CL_2023_07_18/" class="article-date">
  <time datetime="2023-07-18T11:00:00.000Z" itemprop="datePublished">2023-07-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/18/cs.CL_2023_07_18/">cs.CL - 2023-07-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Multi-Modal-Discussion-Transformer-Integrating-Text-Images-and-Graph-Transformers-to-Detect-Hate-Speech-on-Social-Media"><a href="#Multi-Modal-Discussion-Transformer-Integrating-Text-Images-and-Graph-Transformers-to-Detect-Hate-Speech-on-Social-Media" class="headerlink" title="Multi-Modal Discussion Transformer: Integrating Text, Images and Graph Transformers to Detect Hate Speech on Social Media"></a>Multi-Modal Discussion Transformer: Integrating Text, Images and Graph Transformers to Detect Hate Speech on Social Media</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09312">http://arxiv.org/abs/2307.09312</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liamhebert/multimodaldiscussiontransformer">https://github.com/liamhebert/multimodaldiscussiontransformer</a></li>
<li>paper_authors: Liam Hebert, Gaurav Sahu, Nanda Kishore Sreenivas, Lukasz Golab, Robin Cohen</li>
<li>for: 本研究旨在探讨在在线社交网络中推断仇恨言语的多模态graph transformer模型。</li>
<li>methods: 该模型基于文本和图像的共同分析，使用图transformer来捕捉整个讨论的上下文关系，并通过杂交层来结合文本和图像嵌入。</li>
<li>results: 对于基elines进行比较，我们发现我们的模型在推断仇恨言语方面表现出色，并进行了广泛的ablation研究。<details>
<summary>Abstract</summary>
We present the Multi-Modal Discussion Transformer (mDT), a novel multi-modal graph-based transformer model for detecting hate speech in online social networks. In contrast to traditional text-only methods, our approach to labelling a comment as hate speech centers around the holistic analysis of text and images. This is done by leveraging graph transformers to capture the contextual relationships in the entire discussion that surrounds a comment, with interwoven fusion layers to combine text and image embeddings instead of processing different modalities separately. We compare the performance of our model to baselines that only process text; we also conduct extensive ablation studies. We conclude with future work for multimodal solutions to deliver social value in online contexts, arguing that capturing a holistic view of a conversation greatly advances the effort to detect anti-social behavior.
</details>
<details>
<summary>摘要</summary>
我们介绍了多模态讨论变换器（mDT），一种新的多模态图形基于变换器模型，用于在社交网络上探测仇恨言论。与传统的文本只方法不同，我们的方法将注意点在整个讨论环境中心于评论，而不是仅仅是单独处理评论的文本。我们利用图transformers来捕捉讨论中的上下文关系，并使用杂交层来合并文本和图像嵌入。我们与基准方法进行比较，并进行了广泛的剥夺研究。我们认为，捕捉整个对话的全景视图可以大幅提高探测反社会行为的努力。
</details></li>
</ul>
<hr>
<h2 id="Mutual-Reinforcement-Effects-in-Japanese-Sentence-Classification-and-Named-Entity-Recognition-Tasks"><a href="#Mutual-Reinforcement-Effects-in-Japanese-Sentence-Classification-and-Named-Entity-Recognition-Tasks" class="headerlink" title="Mutual Reinforcement Effects in Japanese Sentence Classification and Named Entity Recognition Tasks"></a>Mutual Reinforcement Effects in Japanese Sentence Classification and Named Entity Recognition Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10291">http://arxiv.org/abs/2307.10291</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengguang Gan, Qinghao Zhang, Tatsunori Mori</li>
<li>for: 本研究旨在探讨 sentence classification 和 named entity recognition 的traditional segmentation方法之间的复杂交互关系，以及这两个信息提取子任务之间的互相强制效应。</li>
<li>methods: 本研究提出了一种 Sentence Classification and Named Entity Recognition Multi-task (SCNM) approach，combines Sentence Classification (SC) 和 named entity recognition (NER)。我们还开发了一个 Sentence-to-Label Generation (SLG) 框架，并使用了一个生成模型来生成 SC-标签、NER-标签和相关的文本段落。</li>
<li>results: 我们的结果显示，在 SCNM 中，SC 精度提高了1.13个点，NER 精度提高了1.06个点，并且使用 Constraint Mechanism (CM) 可以提高生成的格式精度。此外，我们还在单独的 SC 任务上实现了 SLG 框架，其性能比基准值更高。在 few-shot learning 实验中，SLG 框架也表现出了更好的性能。<details>
<summary>Abstract</summary>
Information extraction(IE) is a crucial subfield within natural language processing. However, for the traditionally segmented approach to sentence classification and Named Entity Recognition, the intricate interactions between these individual subtasks remain largely uninvestigated. In this study, we propose an integrative analysis, converging sentence classification with Named Entity Recognition, with the objective to unveil and comprehend the mutual reinforcement effect within these two information extraction subtasks. To achieve this, we introduce a Sentence Classification and Named Entity Recognition Multi-task (SCNM) approach that combines Sentence Classification (SC) and Named Entity Recognition (NER). We develop a Sentence-to-Label Generation (SLG) framework for SCNM and construct a Wikipedia dataset containing both SC and NER. Using a format converter, we unify input formats and employ a generative model to generate SC-labels, NER-labels, and associated text segments. We propose a Constraint Mechanism (CM) to improve generated format accuracy. Our results show SC accuracy increased by 1.13 points and NER by 1.06 points in SCNM compared to standalone tasks, with CM raising format accuracy from 63.61 to 100. The findings indicate mutual reinforcement effects between SC and NER, and integration enhances both tasks' performance. We additionally implemented the SLG framework on single SC task. It yielded superior accuracies compared to the baseline on two distinct Japanese SC datasets. Notably, in the experiment of few-shot learning, SLG framework shows much better performance than fine-tune method. These empirical findings contribute additional evidence to affirm the efficacy of the SLG framework.
</details>
<details>
<summary>摘要</summary>
信息提取（IE）是自然语言处理的重要子领域。然而，传统上分割的方法 для句子分类和名实体识别，两个个人任务之间的复杂互动还没有得到了充分的研究。在这一study中，我们提议了一种集成分析，将句子分类与名实体识别集成在一起，以探索和理解这两个信息提取任务之间的互相强化效应。为此，我们提出了一种句子分类和名实体识别多任务（SCNM）方法，将句子分类（SC）和名实体识别（NER）相结合。我们开发了一个句子标签生成（SLG）框架 для SCNM，并使用WIKIPEDIA数据集来构建SC和NER的 dataset。使用一种格式转换器，我们将输入格式统一化，并使用生成模型生成SC标签、NER标签和相关的文本段。我们提出了一种约束机制（CM），以提高生成的格式准确性。我们的结果表明，SCNM比单独任务的SC和NER准确率提高1.13个点和1.06个点，并且CM可以提高格式准确性从63.61%提升到100%。这些实验结果表明，SC和NER之间存在互相强化效应，集成可以提高两个任务的性能。此外，我们还应用SLG框架于单个SC任务，其表现比基eline更高，特别是在几何学学习中。这些实验证据为SLG框架的可效性提供了更多的证据。
</details></li>
</ul>
<hr>
<h2 id="Linearized-Relative-Positional-Encoding"><a href="#Linearized-Relative-Positional-Encoding" class="headerlink" title="Linearized Relative Positional Encoding"></a>Linearized Relative Positional Encoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09270">http://arxiv.org/abs/2307.09270</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aliutkus/spe">https://github.com/aliutkus/spe</a></li>
<li>paper_authors: Zhen Qin, Weixuan Sun, Kaiyue Lu, Hui Deng, Dongxu Li, Xiaodong Han, Yuchao Dai, Lingpeng Kong, Yiran Zhong</li>
<li>for: 这个论文是为了研究linear transformer中的相对位置编码方法的设计原理。</li>
<li>methods: 这篇论文使用了一系列现有的linear relative positional encoding方法，并提出了一种基于单位变换的linear relative positional encoding算法家族。</li>
<li>results: 对于语言模型、文本分类和图像分类等应用，LRPE比现有方法更高效，并且可以推导出更多的相对位置编码方法。<details>
<summary>Abstract</summary>
Relative positional encoding is widely used in vanilla and linear transformers to represent positional information. However, existing encoding methods of a vanilla transformer are not always directly applicable to a linear transformer, because the latter requires a decomposition of the query and key representations into separate kernel functions. Nevertheless, principles for designing encoding methods suitable for linear transformers remain understudied. In this work, we put together a variety of existing linear relative positional encoding approaches under a canonical form and further propose a family of linear relative positional encoding algorithms via unitary transformation. Our formulation leads to a principled framework that can be used to develop new relative positional encoding methods that preserve linear space-time complexity. Equipped with different models, the proposed linearized relative positional encoding (LRPE) family derives effective encoding for various applications. Experiments show that compared with existing methods, LRPE achieves state-of-the-art performance in language modeling, text classification, and image classification. Meanwhile, it emphasizes a general paradigm for designing broadly more relative positional encoding methods that are applicable to linear transformers. The code is available at https://github.com/OpenNLPLab/Lrpe.
</details>
<details>
<summary>摘要</summary>
“相对位置编码广泛应用于简单和线性变换器中，以表示位置信息。然而，现有的变换器编码方法不一定直接适用于线性变换器，因为后者需要对查询和关键表示的分解为分立的核函数。然而，适用于线性变换器的编码方法的原则尚未得到充分研究。在这项工作中，我们将现有的线性相对位置编码方法集成到一个共同形式下，并提出一家线性相对位置编码算法via单位变换。我们的表述导致一种原理性的框架，可以用于开发新的相对位置编码方法，保持线性空间时间复杂度。具有不同的模型，我们提出的线性化相对位置编码（LRPE）家族得到了有效的编码，并在语言模型、文本分类和图像分类等应用中达到了状态之arte的性能。同时，我们强调了一种通用的 paradigma для设计更多的相对位置编码方法，适用于线性 transformers。代码可以在https://github.com/OpenNLPLab/Lrpe 上下载。”
</details></li>
</ul>
<hr>
<h2 id="Text-vectorization-via-transformer-based-language-models-and-n-gram-perplexities"><a href="#Text-vectorization-via-transformer-based-language-models-and-n-gram-perplexities" class="headerlink" title="Text vectorization via transformer-based language models and n-gram perplexities"></a>Text vectorization via transformer-based language models and n-gram perplexities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09255">http://arxiv.org/abs/2307.09255</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mihailo Škorić</li>
<li>For: This paper aims to address the limitations of using scalar perplexity as a measure of text quality, and instead proposes a new method based on vector values that take into account the probability distribution of individual tokens within the input.* Methods: The proposed method uses n-gram perplexities to calculate the relative perplexity of each text token, and combines these values into a single vector representing the input. This approach allows for a more nuanced assessment of text quality, taking into account the probability distribution of individual tokens as well as their overall probability.* Results: The authors evaluate the effectiveness of their proposed method using several experiments, and show that it outperforms traditional scalar perplexity measures in terms of accurately assessing text quality. They also demonstrate the applicability of their method to a variety of natural language processing tasks, including language modeling and text classification.<details>
<summary>Abstract</summary>
As the probability (and thus perplexity) of a text is calculated based on the product of the probabilities of individual tokens, it may happen that one unlikely token significantly reduces the probability (i.e., increase the perplexity) of some otherwise highly probable input, while potentially representing a simple typographical error. Also, given that perplexity is a scalar value that refers to the entire input, information about the probability distribution within it is lost in the calculation (a relatively good text that has one unlikely token and another text in which each token is equally likely they can have the same perplexity value), especially for longer texts. As an alternative to scalar perplexity this research proposes a simple algorithm used to calculate vector values based on n-gram perplexities within the input. Such representations consider the previously mentioned aspects, and instead of a unique value, the relative perplexity of each text token is calculated, and these values are combined into a single vector representing the input.
</details>
<details>
<summary>摘要</summary>
随着文本中每个Token的概率产生产品，可能出现一个不太可能的Token会减少整体概率（即增加plexity），而这可能只是一个简单的字母输入错误。此外，由于plexity是一个Scalar值，它对整个输入的概率分布信息失去了信息（两个相对较好的文本，每个Token的概率相同，可能有同样的plexity值），尤其是 для longer texts。作为一种 alternativescalar perplexity，这些研究提出了一种简单的算法，用于计算基于n-gram perplexity的输入 vector值。这些表示器考虑了上述因素，而不是单个值，每个文本Token的概率值被计算，并将这些值组合成一个表示输入的单个vectors。
</details></li>
</ul>
<hr>
<h2 id="PAC-Neural-Prediction-Set-Learning-to-Quantify-the-Uncertainty-of-Generative-Language-Models"><a href="#PAC-Neural-Prediction-Set-Learning-to-Quantify-the-Uncertainty-of-Generative-Language-Models" class="headerlink" title="PAC Neural Prediction Set Learning to Quantify the Uncertainty of Generative Language Models"></a>PAC Neural Prediction Set Learning to Quantify the Uncertainty of Generative Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09254">http://arxiv.org/abs/2307.09254</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sangdon Park, Taesoo Kim</li>
<li>for: 本研究旨在提高模型的可靠性，通过不确定学习和模型评估来提高模型的可靠性。</li>
<li>methods: 本研究提出了一种基于神经网络 Parametric Neural Prediction Set（PNS）模型，可以为生成语言模型（GLM）中的不确定性进行精确评估，同时仍保持可靠性。</li>
<li>results: 对四种语言数据集和六种模型进行测试，结果显示，与标准基eline方法相比，本方法可以提高评估的不确定性精度，平均提高63%。<details>
<summary>Abstract</summary>
Uncertainty learning and quantification of models are crucial tasks to enhance the trustworthiness of the models. Importantly, the recent surge of generative language models (GLMs) emphasizes the need for reliable uncertainty quantification due to the concerns on generating hallucinated facts. In this paper, we propose to learn neural prediction set models that comes with the probably approximately correct (PAC) guarantee for quantifying the uncertainty of GLMs. Unlike existing prediction set models, which are parameterized by a scalar value, we propose to parameterize prediction sets via neural networks, which achieves more precise uncertainty quantification but still satisfies the PAC guarantee. We demonstrate the efficacy of our method on four types of language datasets and six types of models by showing that our method improves the quantified uncertainty by $63\%$ on average, compared to a standard baseline method.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本到简化中文。</SYS>>模型不确定性学习和量化是提高模型可靠性的关键任务。特别是最近的生成语言模型（GLMs）使得需要可靠的不确定量化，因为担心生成幻见的情况。在这篇论文中，我们提议通过神经网络来学习预测集模型，这些模型具有可靠的不确定量化保证（PAC），用于量化 GLMs 的不确定性。与现有的预测集模型不同，我们的模型通过神经网络来Parameterize预测集，实现更精确的不确定量化，仍满足 PAC 保证。我们在四种语言 dataset 和六种模型上进行了证明，并示出了我们的方法可以在平均上提高量化不确定性的比例达到 63%，相比标准基准方法。
</details></li>
</ul>
<hr>
<h2 id="Unveiling-Gender-Bias-in-Terms-of-Profession-Across-LLMs-Analyzing-and-Addressing-Sociological-Implications"><a href="#Unveiling-Gender-Bias-in-Terms-of-Profession-Across-LLMs-Analyzing-and-Addressing-Sociological-Implications" class="headerlink" title="Unveiling Gender Bias in Terms of Profession Across LLMs: Analyzing and Addressing Sociological Implications"></a>Unveiling Gender Bias in Terms of Profession Across LLMs: Analyzing and Addressing Sociological Implications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09162">http://arxiv.org/abs/2307.09162</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vishesh Thakur</li>
<li>for: 这种研究旨在分析大语言模型（LLM）中的性别偏见，尤其是GPT-2和GPT-3.5两种知名语言模型，以更好地理解其影响。</li>
<li>methods: 这项研究使用了文献综述、数据收集和处理、深入量化分析等方法，以评估LLM中的性别偏见。</li>
<li>results: 研究发现了 gendered word associations、语言使用和biased narratives在LLM中的存在，并讨论了这些现象的伦理含义和可能的社会影响。<details>
<summary>Abstract</summary>
Gender bias in artificial intelligence (AI) and natural language processing has garnered significant attention due to its potential impact on societal perceptions and biases. This research paper aims to analyze gender bias in Large Language Models (LLMs) with a focus on multiple comparisons between GPT-2 and GPT-3.5, some prominent language models, to better understand its implications. Through a comprehensive literature review, the study examines existing research on gender bias in AI language models and identifies gaps in the current knowledge. The methodology involves collecting and preprocessing data from GPT-2 and GPT-3.5, and employing in-depth quantitative analysis techniques to evaluate gender bias in the generated text. The findings shed light on gendered word associations, language usage, and biased narratives present in the outputs of these Large Language Models. The discussion explores the ethical implications of gender bias and its potential consequences on social perceptions and marginalized communities. Additionally, the paper presents strategies for reducing gender bias in LLMs, including algorithmic approaches and data augmentation techniques. The research highlights the importance of interdisciplinary collaborations and the role of sociological studies in mitigating gender bias in AI models. By addressing these issues, we can pave the way for more inclusive and unbiased AI systems that have a positive impact on society.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）和自然语言处理（NLP）中的性别偏见已经引起了社会的关注，因为它可能对社会观念和偏见产生影响。这篇研究论文的目的是分析LLMs中的性别偏见，以GPT-2和GPT-3.5两个著名的语言模型作为研究对象，以更好地了解其影响。通过对现有的AI语言模型性别偏见研究进行抽查，本研究发现了现有的知识空白。方法包括收集和处理GPT-2和GPT-3.5数据，并使用深入的量化分析技术来评估这些语言模型生成的文本中的性别偏见。发现结果指出了这些大语言模型生成的 gendered word associations、语言使用和偏执的 narraves 中的性别偏见。讨论探讨了性别偏见的伦理问题和可能对社会观念和边缘社群产生的影响。此外，论文还提出了减少LLMs中性别偏见的策略，包括算法方法和数据增强技术。研究强调了多学科合作和社会学研究的重要性，以消除性别偏见在AI模型中的问题，以便为社会带来更加包容和无偏见的AI系统。
</details></li>
</ul>
<hr>
<h2 id="Attention-over-pre-trained-Sentence-Embeddings-for-Long-Document-Classification"><a href="#Attention-over-pre-trained-Sentence-Embeddings-for-Long-Document-Classification" class="headerlink" title="Attention over pre-trained Sentence Embeddings for Long Document Classification"></a>Attention over pre-trained Sentence Embeddings for Long Document Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09084">http://arxiv.org/abs/2307.09084</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amine Abdaoui, Sourav Dutta</li>
<li>for: 本研究旨在提高 transformer 模型在长文本处理中的性能，解决 transformer 模型在长序列上的 quadratic attention 复杂性问题。</li>
<li>methods: 本研究使用 pre-trained sentence transformers 开始自己的含义性 embedding，然后通过一个小的注意层将它们组合，注意层的长度 linearly 增长于文档长度。</li>
<li>results: 研究获得了三个标准文档分类数据集上的竞争性result，与现有的 state-of-the-art 模型 using standard fine-tuning 相比， studied 方法 obtains 竞争性result，而且在冰transformer 下也可以获得更好的result。<details>
<summary>Abstract</summary>
Despite being the current de-facto models in most NLP tasks, transformers are often limited to short sequences due to their quadratic attention complexity on the number of tokens. Several attempts to address this issue were studied, either by reducing the cost of the self-attention computation or by modeling smaller sequences and combining them through a recurrence mechanism or using a new transformer model. In this paper, we suggest to take advantage of pre-trained sentence transformers to start from semantically meaningful embeddings of the individual sentences, and then combine them through a small attention layer that scales linearly with the document length. We report the results obtained by this simple architecture on three standard document classification datasets. When compared with the current state-of-the-art models using standard fine-tuning, the studied method obtains competitive results (even if there is no clear best model in this configuration). We also showcase that the studied architecture obtains better results when freezing the underlying transformers. A configuration that is useful when we need to avoid complete fine-tuning (e.g. when the same frozen transformer is shared by different applications). Finally, two additional experiments are provided to further evaluate the relevancy of the studied architecture over simpler baselines.
</details>
<details>
<summary>摘要</summary>
尽管现在大多数NLP任务中是使用trasnformer作为现实模型，但trasnformer通常只能处理短序列，因为它们的自我注意计算复杂度与序列长度成 quadratic关系。为了解决这个问题，一些研究将 concentrate 在减少自我注意计算成本或者使用回归机制或者新的trasnformer模型。在这篇论文中，我们建议利用预训练的句子trasnformer来获得含义rich的句子embeddings，然后通过一个小的注意层将它们相互组合，注意层的计算复杂度与文档长度成直线关系。我们在三个标准的文档分类数据集上进行了测试，与现有状态的艺术模型进行标准微调相比，我们的方法可以获得竞争力强的结果（即使没有明确的最佳模型）。我们还展示了在冻结下面trasnformer时，该方法可以获得更好的结果。这种配置是当我们需要避免完全微调（例如，当同一个冻结的trasnformer被分配给不同的应用程序）时 particualrly useful。最后，我们还提供了两个额外的实验，以进一步评估研究的建议。
</details></li>
</ul>
<hr>
<h2 id="Towards-a-Neural-Era-in-Dialogue-Management-for-Collaboration-A-Literature-Survey"><a href="#Towards-a-Neural-Era-in-Dialogue-Management-for-Collaboration-A-Literature-Survey" class="headerlink" title="Towards a Neural Era in Dialogue Management for Collaboration: A Literature Survey"></a>Towards a Neural Era in Dialogue Management for Collaboration: A Literature Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09021">http://arxiv.org/abs/2307.09021</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amogh Mannekote</li>
<li>for: 本文旨在探讨对话管理技术在协作对话系统中的应用，以便实现对话系统与人类在协作问题解决、创新探索和社交支持中的合作。</li>
<li>methods: 本文首先介绍了协作对话系统中对话管理的发展历史，从传统的手工和信息状态基础方法到基于人工智能规划的方法。然后，它转移到了当代数据驱动对话管理技术，强调将深度学习成功应用于协作上。</li>
<li>results: 本文分析了一些最近的 neural 方法在协作对话管理中的应用，探讨了当前领域的主要趋势。这篇文章希望能为未来对话管理技术的发展提供基础背景，特别是在对话系统社区开始接受大语言模型的情况下。<details>
<summary>Abstract</summary>
Dialogue-based human-AI collaboration can revolutionize collaborative problem-solving, creative exploration, and social support. To realize this goal, the development of automated agents proficient in skills such as negotiating, following instructions, establishing common ground, and progressing shared tasks is essential. This survey begins by reviewing the evolution of dialogue management paradigms in collaborative dialogue systems, from traditional handcrafted and information-state based methods to AI planning-inspired approaches. It then shifts focus to contemporary data-driven dialogue management techniques, which seek to transfer deep learning successes from form-filling and open-domain settings to collaborative contexts. The paper proceeds to analyze a selected set of recent works that apply neural approaches to collaborative dialogue management, spotlighting prevailing trends in the field. This survey hopes to provide foundational background for future advancements in collaborative dialogue management, particularly as the dialogue systems community continues to embrace the potential of large language models.
</details>
<details>
<summary>摘要</summary>
对话基于人工智能的合作可能会革命化协作问题解决、创新探索和社交支持。为实现这一目标，自动化代理人具备谈判、遵从指令、确立共同基础和共同任务进展的技能是非常重要的。这篇评论从合作对话系统的话语管理 парадиг进行了回顾，从传统的手工和信息状态基础方法到人工智能规划引用的方法。然后shift关注当今的数据驱动对话管理技术，它们希望通过将深度学习成功转移到开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中的形式填充和开放领域中
</details></li>
</ul>
<hr>
<h2 id="On-the-In-Effectiveness-of-Large-Language-Models-for-Chinese-Text-Correction"><a href="#On-the-In-Effectiveness-of-Large-Language-Models-for-Chinese-Text-Correction" class="headerlink" title="On the (In)Effectiveness of Large Language Models for Chinese Text Correction"></a>On the (In)Effectiveness of Large Language Models for Chinese Text Correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09007">http://arxiv.org/abs/2307.09007</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yinghui Li, Haojing Huang, Shirong Ma, Yong Jiang, Yangning Li, Feng Zhou, Hai-Tao Zheng, Qingyu Zhou</li>
<li>for: 这个论文主要是研究 chatGPT 在中文自然语言处理 tasks 中的表现，具体来说是在中文语法错误检测和中文拼写检测两个场景下。</li>
<li>methods: 作者使用了 chatGPT 作为基础模型，并进行了 fine-tuning 以适应中文语言处理任务。</li>
<li>results: 研究发现，chatGPT 在中文语法错误检测和中文拼写检测两个场景下的表现具有极高的水平，但也存在一些不满足的问题。<details>
<summary>Abstract</summary>
Recently, the development and progress of Large Language Models (LLMs) have amazed the entire Artificial Intelligence community. As an outstanding representative of LLMs and the foundation model that set off this wave of research on LLMs, ChatGPT has attracted more and more researchers to study its capabilities and performance on various downstream Natural Language Processing (NLP) tasks. While marveling at ChatGPT's incredible performance on kinds of tasks, we notice that ChatGPT also has excellent multilingual processing capabilities, such as Chinese. To explore the Chinese processing ability of ChatGPT, we focus on Chinese Text Correction, a fundamental and challenging Chinese NLP task. Specifically, we evaluate ChatGPT on the Chinese Grammatical Error Correction (CGEC) and Chinese Spelling Check (CSC) tasks, which are two main Chinese Text Correction scenarios. From extensive analyses and comparisons with previous state-of-the-art fine-tuned models, we empirically find that the ChatGPT currently has both amazing performance and unsatisfactory behavior for Chinese Text Correction. We believe our findings will promote the landing and application of LLMs in the Chinese NLP community.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)最近，大语言模型（LLMs）的发展和进步，让整个人工智能社区感到惊叹。作为LLMs的代表之一，以及这波研究的基础模型，ChatGPT已经吸引了越来越多的研究人员研究其能力和表现在不同的自然语言处理（NLP）任务上。而在观察ChatGPT的惊人表现的同时，我们也注意到ChatGPT在多种语言处理任务上表现出色，包括中文。为了探索ChatGPT在中文处理能力方面的可能性，我们专注于中文文法错误 corrections和中文拼写检查两个主要的中文文本修正enario。经过广泛的分析和对前一些State-of-the-art fine-tuned模型的比较，我们发现ChatGPT在中文文本修正方面具有惊人的表现和不满的行为。我们认为我们的发现将推动LLMs在中文NLP社区的应用。
</details></li>
</ul>
<hr>
<h2 id="Zero-shot-Domain-sensitive-Speech-Recognition-with-Prompt-conditioning-Fine-tuning"><a href="#Zero-shot-Domain-sensitive-Speech-Recognition-with-Prompt-conditioning-Fine-tuning" class="headerlink" title="Zero-shot Domain-sensitive Speech Recognition with Prompt-conditioning Fine-tuning"></a>Zero-shot Domain-sensitive Speech Recognition with Prompt-conditioning Fine-tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10274">http://arxiv.org/abs/2307.10274</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mtkresearch/clairaudience">https://github.com/mtkresearch/clairaudience</a></li>
<li>paper_authors: Feng-Ting Liao, Yung-Chieh Chan, Yi-Chang Chen, Chan-Jan Hsu, Da-shan Shiu</li>
<li>for: 这个论文旨在创建域特性敏感的语音识别模型，通过将文本域信息纳入模型生成过程来提高模型的表达准确性。</li>
<li>methods: 这个方法利用了一个预训练的端到端模型（Whisper），通过示例示例学习来让模型学习域特性。我们还扩展了这个方法，使其能够在文本 только fine-tuning 的情况下实现域特性和域适应。</li>
<li>results: 我们的模型在不同的域和提示上进行了评测，结果显示，模型在未经见过的数据集上可以实现 Word Error Rate（WER）下降达33%，而文本只 fine-tuning 的模型在医学对话数据集上可以达到最大的 WER 下降达29%。<details>
<summary>Abstract</summary>
In this work, we propose a method to create domain-sensitive speech recognition models that utilize textual domain information by conditioning its generation on a given text prompt. This is accomplished by fine-tuning a pre-trained, end-to-end model (Whisper) to learn from demonstrations with prompt examples. We show that this ability can be generalized to different domains and even various prompt contexts, with our model gaining a Word Error Rate (WER) reduction of up to 33% on unseen datasets from various domains, such as medical conversation, air traffic control communication, and financial meetings. Considering the limited availability of audio-transcript pair data, we further extend our method to text-only fine-tuning to achieve domain sensitivity as well as domain adaptation. We demonstrate that our text-only fine-tuned model can also attend to various prompt contexts, with the model reaching the most WER reduction of 29% on the medical conversation dataset.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们提出了一种方法，用于创建域特定的语音识别模型，该模型利用文本域信息，通过给定文本示例来定制其生成。我们通过练习示例来训练一个预训练的、端到端模型（Whisper），以便从示例中学习。我们发现这种能力可以泛化到不同的域和不同的提示上下文中，我们在不同领域的未seen数据上实现了Word Error Rate（WER）的减少，最高达33%。针对缺乏语音-转录对数据的限制，我们进一步扩展了我们的方法，以文本 solo 精度为基础，实现域特定性和域适应性。我们示示了我们的文本 solo 精度模型可以attend到不同的提示上下文，并在医学对话数据集上达到最高的WER减少29%。
</details></li>
</ul>
<hr>
<h2 id="AutoAlign-Fully-Automatic-and-Effective-Knowledge-Graph-Alignment-enabled-by-Large-Language-Models"><a href="#AutoAlign-Fully-Automatic-and-Effective-Knowledge-Graph-Alignment-enabled-by-Large-Language-Models" class="headerlink" title="AutoAlign: Fully Automatic and Effective Knowledge Graph Alignment enabled by Large Language Models"></a>AutoAlign: Fully Automatic and Effective Knowledge Graph Alignment enabled by Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11772">http://arxiv.org/abs/2307.11772</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rui Zhang, Yixin Su, Bayu Distiawan Trisedya, Xiaoyan Zhao, Min Yang, Hong Cheng, Jianzhong Qi</li>
<li>for:  automatic entity alignment between knowledge graphs (KGs)</li>
<li>methods:  constructs a predicate-proximity-graph with the help of large language models, computes entity embeddings using TransE, and shifts the two KGs’ entity embeddings into the same vector space based on attribute similarity</li>
<li>results:  improves the performance of entity alignment significantly compared to state-of-the-art methods<details>
<summary>Abstract</summary>
The task of entity alignment between knowledge graphs (KGs) aims to identify every pair of entities from two different KGs that represent the same entity. Many machine learning-based methods have been proposed for this task. However, to our best knowledge, existing methods all require manually crafted seed alignments, which are expensive to obtain. In this paper, we propose the first fully automatic alignment method named AutoAlign, which does not require any manually crafted seed alignments. Specifically, for predicate embeddings, AutoAlign constructs a predicate-proximity-graph with the help of large language models to automatically capture the similarity between predicates across two KGs. For entity embeddings, AutoAlign first computes the entity embeddings of each KG independently using TransE, and then shifts the two KGs' entity embeddings into the same vector space by computing the similarity between entities based on their attributes. Thus, both predicate alignment and entity alignment can be done without manually crafted seed alignments. AutoAlign is not only fully automatic, but also highly effective. Experiments using real-world KGs show that AutoAlign improves the performance of entity alignment significantly compared to state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
《EntityAlignment》是一个知识 graphs（KGs）中的任务，旨在将每个来自不同KGs的实体对应到同一个实体。许多机器学习基于方法已经被提出来解决这个任务。然而，我们所知道的所有方法都需要手动制作的种子对Alignment，这是 expensive的。在这篇论文中，我们提出了第一个完全自动的对Alignment方法，即AutoAlign，不需要任何手动制作的种子对Alignment。为 predicate embeddings，AutoAlign使用大型自然语言模型来自动捕捉两个KGs中 predicate的相似性，并构建一个 predicate-proximity-graph。为 entity embeddings，AutoAlign先使用TransE计算每个KG的独立实体表示，然后通过计算实体 attribute 的相似性来将两个KGs的实体表示Shift到同一个vector space中。因此， predicate alignment和entity alignment都可以通过自动生成的种子对Alignment来完成，不需要手动制作种子对Alignment。AutoAlign不仅是完全自动的，还是非常有效的。在实际世界KGs上进行实验，AutoAlign signicicantly improvese the performance of entity alignment，比对 estado-of-the-art 方法更高。
</details></li>
</ul>
<hr>
<h2 id="Mitigating-Label-Bias-via-Decoupled-Confident-Learning"><a href="#Mitigating-Label-Bias-via-Decoupled-Confident-Learning" class="headerlink" title="Mitigating Label Bias via Decoupled Confident Learning"></a>Mitigating Label Bias via Decoupled Confident Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08945">http://arxiv.org/abs/2307.08945</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunyi Li, Maria De-Arteaga, Maytal Saar-Tsechansky</li>
<li>for:  Mitigating algorithmic bias and addressing label bias in training data.</li>
<li>methods:  Decoupled Confident Learning (DeCoLe) pruning method to identify and remove biased labels.</li>
<li>results:  Successfully identified biased labels and outperformed competing approaches in the context of hate speech detection.Here’s the full text in Simplified Chinese:</li>
<li>for: 本文旨在 Mitigating algorithmic bias 和Addressing label bias 在训练数据中。</li>
<li>methods: 本文提出了 Decoupled Confident Learning (DeCoLe) 的架构，通过特有的采集方法来发现和移除偏见标签。</li>
<li>results: 在 hate speech detection 中应用 DeCoLe，成功发现了偏见标签，并超过了竞争方法的性能。<details>
<summary>Abstract</summary>
Growing concerns regarding algorithmic fairness have led to a surge in methodologies to mitigate algorithmic bias. However, such methodologies largely assume that observed labels in training data are correct. This is problematic because bias in labels is pervasive across important domains, including healthcare, hiring, and content moderation. In particular, human-generated labels are prone to encoding societal biases. While the presence of labeling bias has been discussed conceptually, there is a lack of methodologies to address this problem. We propose a pruning method -- Decoupled Confident Learning (DeCoLe) -- specifically designed to mitigate label bias. After illustrating its performance on a synthetic dataset, we apply DeCoLe in the context of hate speech detection, where label bias has been recognized as an important challenge, and show that it successfully identifies biased labels and outperforms competing approaches.
</details>
<details>
<summary>摘要</summary>
众所周知的算法公平性问题已导致一系列方法来减少算法偏见。然而，这些方法假设训练数据中所观察到的标签是正确的。这是一个问题，因为标签偏见是重要领域，如医疗、招聘和内容审核中的普遍存在。人类生成的标签容易带有社会偏见。虽然标签偏见的存在已经被描述了，但是没有有效的方法来解决这个问题。我们提议一种剪除方法——分离确定学习（DeCoLe）——特意设计来减少标签偏见。在一个人工数据集上验证其性能后，我们在仇恨言语检测中应用DeCoLe，并显示它成功地识别偏见标签，并超过其他方法的表现。
</details></li>
</ul>
<hr>
<h2 id="NTK-approximating-MLP-Fusion-for-Efficient-Language-Model-Fine-tuning"><a href="#NTK-approximating-MLP-Fusion-for-Efficient-Language-Model-Fine-tuning" class="headerlink" title="NTK-approximating MLP Fusion for Efficient Language Model Fine-tuning"></a>NTK-approximating MLP Fusion for Efficient Language Model Fine-tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08941">http://arxiv.org/abs/2307.08941</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/weitianxin/mlp_fusion">https://github.com/weitianxin/mlp_fusion</a></li>
<li>paper_authors: Tianxin Wei, Zeming Guo, Yifan Chen, Jingrui He</li>
<li>for: 这篇论文的目的是提出一种可靠地将大型语言模型（PLM）转换为轻量级语言模型（MLP），并且在不需要大量训练数据和 computation 的情况下，实现 PLM 的 fine-tuning。</li>
<li>methods: 这篇论文使用了 neural tangent kernel（NTK）的想法，将多层感知器（MLP）组件看作是一个组件，并将其分成一定数量的中心点，然后将这些中心点 Restore 为一个轻量级的 MLP，并证明这个方法可以将 NTK 的 PLM 转换为轻量级 MLP。</li>
<li>results: 这篇论文的实验结果显示，使用 proposed method 可以实现 PLM fine-tuning 的目的，并且在自然语言理解（NLU）和自然语言生成（NLG）任务上都有很好的效果。<details>
<summary>Abstract</summary>
Fine-tuning a pre-trained language model (PLM) emerges as the predominant strategy in many natural language processing applications. However, even fine-tuning the PLMs and doing inference are expensive, especially on edge devices with low computing power. Some general approaches (e.g. quantization and distillation) have been widely studied to reduce the compute/memory of PLM fine-tuning, while very few one-shot compression techniques are explored. In this paper, we investigate the neural tangent kernel (NTK)--which reveals the gradient descent dynamics of neural networks--of the multilayer perceptrons (MLP) modules in a PLM and propose to coin a lightweight PLM through NTK-approximating MLP fusion. To achieve this, we reconsider the MLP as a bundle of sub-MLPs, and cluster them into a given number of centroids, which can then be restored as a compressed MLP and surprisingly shown to well approximate the NTK of the original PLM. Extensive experiments of PLM fine-tuning on both natural language understanding (NLU) and generation (NLG) tasks are provided to verify the effectiveness of the proposed method MLP fusion. Our code is available at https://github.com/weitianxin/MLP_Fusion.
</details>
<details>
<summary>摘要</summary>
通用抽象语言模型（PLM）的精度调整被认为是许多自然语言处理应用中的主流策略。然而，即使进行PLM的精度调整和推理都是昂贵的，特别是在edge设备上，这些设备具有较低的计算力。一些通用的方法（如量化和精炼）已经广泛研究，以减少PLM精度调整的计算/存储量。然而，对于一键压缩技术，尚未得到充分的研究。在这篇论文中，我们 investigate了多层感知器（MLP）模块在PLM中的神经凝结阵（NTK），并提议使用NTK-近似MLP融合来创造轻量级PLM。为实现这一目标，我们将MLP视为一个包含多个子MLP的Bundle，然后将它们分为一定数量的中心点，并将这些中心点还原为一个压缩后的MLP，并Surprisingly Shown to well approximate the NTK of the original PLM。我们提供了对PLM精度调整的NLU和NLG任务的广泛实验来验证提议的效果。代码可以在https://github.com/weitianxin/MLP_Fusion上找到。
</details></li>
</ul>
<hr>
<h2 id="Teach-model-to-answer-questions-after-comprehending-the-document"><a href="#Teach-model-to-answer-questions-after-comprehending-the-document" class="headerlink" title="Teach model to answer questions after comprehending the document"></a>Teach model to answer questions after comprehending the document</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08931">http://arxiv.org/abs/2307.08931</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruiqing Sun, Ping Jian</li>
<li>for: 提高多选机器阅读理解（MRC）任务的表现，使模型更好地理解文本。</li>
<li>methods: 提出了一种两stage知识塑造方法，将MRC任务分为两个阶段，使模型更好地理解文本。</li>
<li>results: 实验结果显示，当student模型采用我们的方法时，对MRC任务的表现有显著改善，证明了方法的效果。<details>
<summary>Abstract</summary>
Multi-choice Machine Reading Comprehension (MRC) is a challenging extension of Natural Language Processing (NLP) that requires the ability to comprehend the semantics and logical relationships between entities in a given text. The MRC task has traditionally been viewed as a process of answering questions based on the given text. This single-stage approach has often led the network to concentrate on generating the correct answer, potentially neglecting the comprehension of the text itself. As a result, many prevalent models have faced challenges in performing well on this task when dealing with longer texts. In this paper, we propose a two-stage knowledge distillation method that teaches the model to better comprehend the document by dividing the MRC task into two separate stages. Our experimental results show that the student model, when equipped with our method, achieves significant improvements, demonstrating the effectiveness of our method.
</details>
<details>
<summary>摘要</summary>
多选机器阅读理解（MRC）是自然语言处理（NLP）的一个挑战性扩展，需要模型理解文本中实体之间的 semantics 和逻辑关系。传统上，MRC 任务被视为Answer questions based on the given text的一个过程。这种单个阶段approach 经常导致网络偏重于生成正确的答案，可能忽略文本本身的理解。因此，许多常见模型在处理 longer texts 时会遇到问题。在这篇论文中，我们提出了一种两个阶段知识填充方法，该方法将 MRC 任务分解为两个独立的阶段。我们的实验结果表明，当Student模型搭配我们的方法时，它们在MRC任务上表现出了显著改善，这demonstrates the effectiveness of our method。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-Perform-Diagnostic-Reasoning"><a href="#Large-Language-Models-Perform-Diagnostic-Reasoning" class="headerlink" title="Large Language Models Perform Diagnostic Reasoning"></a>Large Language Models Perform Diagnostic Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08922">http://arxiv.org/abs/2307.08922</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nlplab-best-team/diagnostic-reasoning">https://github.com/nlplab-best-team/diagnostic-reasoning</a></li>
<li>paper_authors: Cheng-Kuang Wu, Wei-Lin Chen, Hsin-Hsi Chen</li>
<li>for: 这 paper 的目的是探讨自动诊断任务中的幂等思维（Chain-of-Thought，CoT）提问的扩展。</li>
<li>methods: 该 paper 使用的方法是基于医生的底层思维过程，提出了诊断理解Chain-of-Thought（DR-CoT）。</li>
<li>results: 实验结果表明，只使用通用文本库训练的大语言模型，并使用两个 DR-CoT 示例，可以提高自动诊断的准确率15%，并在域外设置中达到了18%的差异。这些结果表明，通过适当的提问，可以在大语言模型中激发专家知识的推理。<details>
<summary>Abstract</summary>
We explore the extension of chain-of-thought (CoT) prompting to medical reasoning for the task of automatic diagnosis. Motivated by doctors' underlying reasoning process, we present Diagnostic-Reasoning CoT (DR-CoT). Empirical results demonstrate that by simply prompting large language models trained only on general text corpus with two DR-CoT exemplars, the diagnostic accuracy improves by 15% comparing to standard prompting. Moreover, the gap reaches a pronounced 18% in out-domain settings. Our findings suggest expert-knowledge reasoning in large language models can be elicited through proper promptings.
</details>
<details>
<summary>摘要</summary>
我们探索了思维链（CoT）提示的扩展到医学理解，以提高自动诊断的精度。受医生的深层次思维过程 inspirits，我们提出了诊断思维链（DR-CoT）。我们的实验结果表明，只需通过对通用文本库进行训练，并使用两个DR-CoT示例来提示大语言模型，可以提高自动诊断的准确率15%，并在 OUT-DOMAIN  Settings 中提高了18%。我们的发现表明，通过合适的提示，大语言模型中的专家知识 reasoning 可以被诱导出来。
</details></li>
</ul>
<hr>
<h2 id="An-Integrated-NPL-Approach-to-Sentiment-Analysis-in-Satisfaction-Surveys"><a href="#An-Integrated-NPL-Approach-to-Sentiment-Analysis-in-Satisfaction-Surveys" class="headerlink" title="An Integrated NPL Approach to Sentiment Analysis in Satisfaction Surveys"></a>An Integrated NPL Approach to Sentiment Analysis in Satisfaction Surveys</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11771">http://arxiv.org/abs/2307.11771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Edson B. Pinto-Luque</li>
<li>For: The paper aims to apply an integrated approach to natural language processing (NLP) to satisfaction surveys in order to understand and extract relevant information from survey responses, analyze feelings, and identify recurring word patterns.* Methods: The paper will use NLP techniques such as emotional polarity detection, response classification into positive, negative, or neutral categories, and opinion mining to highlight participants’ opinions. The analysis of word patterns in satisfaction survey responses will also be conducted using NLP.* Results: The paper will obtain results that can be used to identify areas for improvement, understand respondents’ preferences, and make strategic decisions based on analysis to improve respondent satisfaction. The results will provide a deeper understanding of feelings, opinions, and themes and trends present in respondents’ responses.<details>
<summary>Abstract</summary>
The research project aims to apply an integrated approach to natural language processing NLP to satisfaction surveys. It will focus on understanding and extracting relevant information from survey responses, analyzing feelings, and identifying recurring word patterns. NLP techniques will be used to determine emotional polarity, classify responses into positive, negative, or neutral categories, and use opinion mining to highlight participants opinions. This approach will help identify the most relevant aspects for participants and understand their opinions in relation to those specific aspects. A key component of the research project will be the analysis of word patterns in satisfaction survey responses using NPL. This analysis will provide a deeper understanding of feelings, opinions, and themes and trends present in respondents responses. The results obtained from this approach can be used to identify areas for improvement, understand respondents preferences, and make strategic decisions based on analysis to improve respondent satisfaction.
</details>
<details>
<summary>摘要</summary>
这个研究项目旨在应用综合的自然语言处理（NLP）技术来满意调查。它将关注从调查答案中提取有关信息，分析情感和识别反复出现的词语模式。通过NLP技术来确定情感方向，将答案分类为正面、负面或中性等类别，并使用意见挖掘来强调参与者的意见。这种方法可以帮助确定参与者最关心的方面，理解他们对这些方面的看法，并提供改进参与者满意度的信息。在这个研究项目中，word pattern在满意调查答案中的分析将为我们提供更深刻的情感、意见和问题趋势的理解。这些结果可以用来确定改进参与者满意度的方法，了解参与者的偏好，并基于分析来做战略决策。
</details></li>
</ul>
<hr>
<h2 id="Comparative-Performance-Evaluation-of-Large-Language-Models-for-Extracting-Molecular-Interactions-and-Pathway-Knowledge"><a href="#Comparative-Performance-Evaluation-of-Large-Language-Models-for-Extracting-Molecular-Interactions-and-Pathway-Knowledge" class="headerlink" title="Comparative Performance Evaluation of Large Language Models for Extracting Molecular Interactions and Pathway Knowledge"></a>Comparative Performance Evaluation of Large Language Models for Extracting Molecular Interactions and Pathway Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08813">http://arxiv.org/abs/2307.08813</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/boxorange/bioie-llm">https://github.com/boxorange/bioie-llm</a></li>
<li>paper_authors: Gilchan Park, Byung-Jun Yoon, Xihaier Luo, Vanessa López-Marrero, Patrick Johnstone, Shinjae Yoo, Francis J. Alexander</li>
<li>for: 本研究旨在使用大型自然语言模型提取生物系统中protein互动和信号通路知识，以解决现有数据库的不完整性和维护困难。</li>
<li>methods: 本研究使用了不同的大型自然语言模型进行蛋白互动、信号通路和基因调节关系的识别任务。</li>
<li>results: 研究发现了不同模型在不同任务中的表现，并提供了可重复的评价指标。未来可能性和剩下的挑战也得到了讨论。<details>
<summary>Abstract</summary>
Understanding protein interactions and pathway knowledge is crucial for unraveling the complexities of living systems and investigating the underlying mechanisms of biological functions and complex diseases. While existing databases provide curated biological data from literature and other sources, they are often incomplete and their maintenance is labor-intensive, necessitating alternative approaches. In this study, we propose to harness the capabilities of large language models to address these issues by automatically extracting such knowledge from the relevant scientific literature. Toward this goal, in this work, we investigate the effectiveness of different large language models in tasks that involve recognizing protein interactions, pathways, and gene regulatory relations. We thoroughly evaluate the performance of various models, highlight the significant findings, and discuss both the future opportunities and the remaining challenges associated with this approach. The code and data are available at: https://github.com/boxorange/BioIE-LLM
</details>
<details>
<summary>摘要</summary>
理解蛋白相互作用和生物路径知识是生命系统复杂性的关键，可以帮助我们理解生物功能和复杂疾病的下面机理。现有的数据库提供了经过 curación的生物数据，但这些数据库通常是不完整的，维护困难重，需要新的方法。在这种情况下，我们提议利用大型自然语言模型来解决这些问题，自动从相关科学文献中提取生物知识。为达到这个目标，我们在这项工作中评估了不同的大型自然语言模型在识别蛋白相互作用、生物路径和基因调节关系等任务中的效果。我们全面评估了各模型的表现，把重要发现提高到位，并讨论了这种方法的未来机遇和剩下的挑战。代码和数据可以在：https://github.com/boxorange/BioIE-LLM 获取。
</details></li>
</ul>
<hr>
<h2 id="AlpaGasus-Training-A-Better-Alpaca-with-Fewer-Data"><a href="#AlpaGasus-Training-A-Better-Alpaca-with-Fewer-Data" class="headerlink" title="AlpaGasus: Training A Better Alpaca with Fewer Data"></a>AlpaGasus: Training A Better Alpaca with Fewer Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08701">http://arxiv.org/abs/2307.08701</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, Hongxia Jin<br>for: 这篇论文的目的是提出一种简单有效的数据选择策略，以提高大型语言模型（LLMs）在 instrucion-finetuning（IFT）过程中的性能。methods: 该策略基于使用一个强大的语言模型（如ChatGPT）来自动识别和除去低质量数据。results: 该策略可以提高 LLMs 的 instrucion-following 能力，并且可以快速减少训练时间。在多个测试集上，AlpaGasus 表现比原始 Alpaca 更好，并且与其教师模型（Text-Davinci-003）的性能相似。<details>
<summary>Abstract</summary>
Large language models~(LLMs) obtain instruction-following capability through instruction-finetuning (IFT) on supervised instruction/response data. However, widely used IFT datasets (e.g., Alpaca's 52k data) surprisingly contain many low-quality instances with incorrect or irrelevant responses, which are misleading and detrimental to IFT. In this paper, we propose a simple and effective data selection strategy that automatically identifies and removes low-quality data using a strong LLM (e.g., ChatGPT). To this end, we introduce AlpaGasus, which is finetuned on only 9k high-quality data filtered from the 52k Alpaca data. AlpaGasus significantly outperforms the original Alpaca as evaluated by GPT-4 on multiple test sets and its 13B variant matches $>90\%$ performance of its teacher LLM (i.e., Text-Davinci-003) on test tasks. It also provides 5.7x faster training, reducing the training time for a 7B variant from 80 minutes (for Alpaca) to 14 minutes \footnote{We apply IFT for the same number of epochs as Alpaca(7B) but on fewer data, using 4$\times$NVIDIA A100 (80GB) GPUs and following the original Alpaca setting and hyperparameters.}. Overall, AlpaGasus demonstrates a novel data-centric IFT paradigm that can be generally applied to instruction-tuning data, leading to faster training and better instruction-following models. Our project page is available at: \url{https://lichang-chen.github.io/AlpaGasus/}.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）通过指令精度训练（IFT）来获得指令遵从能力。然而，广泛使用的 IFT 数据集（例如 Alpaca 的 52k 数据）意外地包含了许多低质量的实例，其中的回答是错误或无关的，这会诱导模型学习 incorrect 的指令遵从能力。在这篇论文中，我们提出了一种简单而有效的数据选择策略，使用强大的 LLM（例如 ChatGPT）自动标识并移除低质量数据。为此，我们引入 AlpaGasus，它是在只有 9k 高质量数据上进行了精度训练。AlpaGasus 与原始 Alpaca 进行比较，在多个测试集上表现出色，并且其 13B 变体与 teacher LLM（i.e., Text-Davinci-003）在测试任务上的性能相似。它还提供了5.7倍快的训练时间，从原始 Alpaca 的 80 分钟减少到 14 分钟。总的来说，AlpaGasus 展示了一种新的数据中心的 IFT 模式，可以通过更快的训练和更高质量的指令遵从模型来提高指令遵从能力。我们的项目页面可以在以下链接中找到：https://lichang-chen.github.io/AlpaGasus/。
</details></li>
</ul>
<hr>
<h2 id="Multilingual-Speech-to-Speech-Translation-into-Multiple-Target-Languages"><a href="#Multilingual-Speech-to-Speech-Translation-into-Multiple-Target-Languages" class="headerlink" title="Multilingual Speech-to-Speech Translation into Multiple Target Languages"></a>Multilingual Speech-to-Speech Translation into Multiple Target Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08655">http://arxiv.org/abs/2307.08655</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongyu Gong, Ning Dong, Sravya Popuri, Vedanuj Goswami, Ann Lee, Juan Pino</li>
<li>for: 本文旨在探讨多种语言间的speech-to-speech翻译（S2ST）技术，以实现不同语言之间的口语沟通。</li>
<li>methods: 本文提出了首先支持多种目标语言的多语言S2ST模型，基于直接S2ST的核心组件：speech-to-unit（S2U）和 vocoder。S2U的多语言扩展为speech-to-masked-unit（S2MU），用于减少语言干扰。 vocoder 则通过语言嵌入和auxiliary损失来学习多语言特征。</li>
<li>results: 在标准翻译测试集上，我们的提议的多语言模型比双语模型在英语到16种目标语言的翻译中表现出色。<details>
<summary>Abstract</summary>
Speech-to-speech translation (S2ST) enables spoken communication between people talking in different languages. Despite a few studies on multilingual S2ST, their focus is the multilinguality on the source side, i.e., the translation from multiple source languages to one target language. We present the first work on multilingual S2ST supporting multiple target languages. Leveraging recent advance in direct S2ST with speech-to-unit and vocoder, we equip these key components with multilingual capability. Speech-to-masked-unit (S2MU) is the multilingual extension of S2U, which applies masking to units which don't belong to the given target language to reduce the language interference. We also propose multilingual vocoder which is trained with language embedding and the auxiliary loss of language identification. On benchmark translation testsets, our proposed multilingual model shows superior performance than bilingual models in the translation from English into $16$ target languages.
</details>
<details>
<summary>摘要</summary>
听说到听翻译（S2ST）可以实现不同语言的人们之间的口头交流。虽有一些关于多语言S2ST的研究，但它们的研究重点在多语言源语言到一个目标语言的翻译方面。我们发表了多语言S2ST的首个研究，利用最新的直接S2ST技术，对关键组件进行多语言化。听说到压制单元（S2MU）是多语言扩展的S2U，它对不属于给定目标语言的单元进行掩蔽，以降低语言干扰。我们还提议多语言 vocoder，通过语言嵌入和 aux 损失来训练。在标准翻译测试集上，我们的提议的多语言模型在英语到 $16$ 个目标语言的翻译中表现出色，比双语模型更高。
</details></li>
</ul>
<hr>
<h2 id="Retentive-Network-A-Successor-to-Transformer-for-Large-Language-Models"><a href="#Retentive-Network-A-Successor-to-Transformer-for-Large-Language-Models" class="headerlink" title="Retentive Network: A Successor to Transformer for Large Language Models"></a>Retentive Network: A Successor to Transformer for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08621">http://arxiv.org/abs/2307.08621</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/unilm">https://github.com/microsoft/unilm</a></li>
<li>paper_authors: Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, Furu Wei</li>
<li>for: 这篇论文是为了提出一种基于大语言模型的基础架构Retentive Network（RetNet），同时实现培训并行、低成本推理和良好性能。</li>
<li>methods: 论文使用了理论 derivation来连接回归和注意力，并提出了保持机制来实现序列模型，该机制支持三种计算模式：并行、循环和分割循环。</li>
<li>results: 实验结果表明，RetNet在语言模型中实现了有利的扩展结果，并且可以实现并行培训、低成本推理和高效推理。<details>
<summary>Abstract</summary>
In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost $O(1)$ inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference. The intriguing properties make RetNet a strong successor to Transformer for large language models. Code will be available at https://aka.ms/retnet.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们提出了吸引网络（RetNet）作为大语言模型的基础架构，同时实现了训练并行、低成本推理和良好的性能。我们理论上 derivated了回快和注意力之间的连接。然后我们提出了保留机制，用于序列模型，该机制支持三种计算方法，即并行、循环和块循环。 Specifically，并行表示允许训练并行。循环表示可以在 $O(1)$ 推理成本下进行快速推理，这会提高解码速度、延迟和GPU内存，而无需牺牲性能。块循环表示可以高效地处理长序列模型，每个块都可以并行地编码，而循环 SUMmarize 每个块。实验结果表明，RetNet在语言模型方面实现了有利扩展、并行训练、低成本部署和高效推理。这些特有性使得RetNet成为Transformer的强力继承者。代码将在 https://aka.ms/retnet 上提供。
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Diffusion-Segmentation-Model-for-Object-Segmentation-from-Manipulation-Instructions"><a href="#Multimodal-Diffusion-Segmentation-Model-for-Object-Segmentation-from-Manipulation-Instructions" class="headerlink" title="Multimodal Diffusion Segmentation Model for Object Segmentation from Manipulation Instructions"></a>Multimodal Diffusion Segmentation Model for Object Segmentation from Manipulation Instructions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08597">http://arxiv.org/abs/2307.08597</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yui Iioka, Yu Yoshida, Yuiga Wada, Shumpei Hatanaka, Komei Sugiura</li>
<li>for: 本研究旨在开发一个能够理解自然语言指令（例如“去生活室里取得最近的毯子与广播艺术画作）”并生成这个目标日常物品的分割 маска。这个任务具有三个挑战：一、理解指令中多个物品的引用表达；二、预测句子中的目标词；三、生成像素精度的分割 маска而不是 bounding box。过往的语言基于分割方法有时会遮掩无关区域，导致分割不精确。在本文中，我们提出了多Modal Diffusion Segmentation Model（MDSM），它在第一阶段生成分割 маска，然后在第二阶段进行精确化。我们引入了交叉模式平行特征提取机制，并将扩展散度概率模型以应对交叉模式特征。</li>
<li>methods: 我们提出了 Multimodal Diffusion Segmentation Model（MDSM），它包括以下几个部分：在第一阶段，我们使用交叉模式平行特征提取机制来生成分割 маска；在第二阶段，我们使用扩展散度概率模型进行精确化。</li>
<li>results: 我们在这篇研究中获得了与基准方法相比的大幅提升（+10.13 mean IoU），证明了 MDSM 的效果。<details>
<summary>Abstract</summary>
In this study, we aim to develop a model that comprehends a natural language instruction (e.g., "Go to the living room and get the nearest pillow to the radio art on the wall") and generates a segmentation mask for the target everyday object. The task is challenging because it requires (1) the understanding of the referring expressions for multiple objects in the instruction, (2) the prediction of the target phrase of the sentence among the multiple phrases, and (3) the generation of pixel-wise segmentation masks rather than bounding boxes. Studies have been conducted on languagebased segmentation methods; however, they sometimes mask irrelevant regions for complex sentences. In this paper, we propose the Multimodal Diffusion Segmentation Model (MDSM), which generates a mask in the first stage and refines it in the second stage. We introduce a crossmodal parallel feature extraction mechanism and extend diffusion probabilistic models to handle crossmodal features. To validate our model, we built a new dataset based on the well-known Matterport3D and REVERIE datasets. This dataset consists of instructions with complex referring expressions accompanied by real indoor environmental images that feature various target objects, in addition to pixel-wise segmentation masks. The performance of MDSM surpassed that of the baseline method by a large margin of +10.13 mean IoU.
</details>
<details>
<summary>摘要</summary>
在这个研究中，我们目标是开发一个模型，可以理解自然语言指令（例如，“去生活厅获得最近的柔毂到墙上的广播艺术）”并生成目标日常物体的分割面积。这个任务具有三个挑战：一是理解指令中多个对象的引用表达，二是预测句子中的目标短语，三是生成像素级分割面积而不是边框框。尝试过语言基于分割方法，但它们有时会覆盖无关区域，对于复杂的句子来说。在这篇论文中，我们提出了多Modal扩散分割模型（MDSM），它在第一个阶段生成分割面积，然后在第二个阶段进行精细调整。我们引入了相关的并行特征提取机制，并扩展了扩散概率模型以处理相关特征。为验证我们的模型，我们创建了基于Matterport3D和REVERIE dataset的新 dataset，这个dataset包括具有复杂引用表达的指令，并且拥有真实的室内环境图像和多种目标物体的像素级分割面积。我们的模型在比较基准方法时，表现出了大幅提升的mean IoU值+10.13。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/18/cs.CL_2023_07_18/" data-id="cloimip65007ls488dc8k5wrh" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_07_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/18/cs.LG_2023_07_18/" class="article-date">
  <time datetime="2023-07-18T10:00:00.000Z" itemprop="datePublished">2023-07-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/18/cs.LG_2023_07_18/">cs.LG - 2023-07-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Enhancing-Pattern-Classification-in-Support-Vector-Machines-through-Matrix-Formulation"><a href="#Enhancing-Pattern-Classification-in-Support-Vector-Machines-through-Matrix-Formulation" class="headerlink" title="Enhancing Pattern Classification in Support Vector Machines through Matrix Formulation"></a>Enhancing Pattern Classification in Support Vector Machines through Matrix Formulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09372">http://arxiv.org/abs/2307.09372</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sambhav Jain Reshma Rastogi</li>
<li>for: 本研究 paper 的目的是提出一种矩阵形式的支持向量机器 (Matrix SVM)，以解决现有 SVM 模型在多类和多标签 Setting 中的限制。</li>
<li>methods: 本研究使用 Accelerated Gradient Descent 方法在 dual 中进行优化，以提高解决 Matrix-SVM 问题的效率。</li>
<li>results: 实验结果表明，Matrix SVM 在多标签和多类数据集上可以 достичь更高的时间效果，同时保持与 Binary Relevance SVM 相同的结果 Waterfall 。此外，矩阵形式还提供了一些透彻的意见和优势，可能不会在传统的 vector-based notation 中显示出来。<details>
<summary>Abstract</summary>
Support Vector Machines (SVM) have gathered significant acclaim as classifiers due to their successful implementation of Statistical Learning Theory. However, in the context of multiclass and multilabel settings, the reliance on vector-based formulations in existing SVM-based models poses limitations regarding flexibility and ease of incorporating additional terms to handle specific challenges. To overcome these limitations, our research paper focuses on introducing a matrix formulation for SVM that effectively addresses these constraints. By employing the Accelerated Gradient Descent method in the dual, we notably enhance the efficiency of solving the Matrix-SVM problem. Experimental evaluations on multilabel and multiclass datasets demonstrate that Matrix SVM achieves superior time efficacy while delivering similar results to Binary Relevance SVM.   Moreover, our matrix formulation unveils crucial insights and advantages that may not be readily apparent in traditional vector-based notations. We emphasize that numerous multilabel models can be viewed as extensions of SVM, with customised modifications to meet specific requirements. The matrix formulation presented in this paper establishes a solid foundation for developing more sophisticated models capable of effectively addressing the distinctive challenges encountered in multilabel learning.
</details>
<details>
<summary>摘要</summary>
支持向量机 (SVM) 在分类方面受到了广泛的赞誉，因为它们成功地应用了统计学学习理论。然而，在多类和多标签设置下，现有的 SVM 基本模型的向量化表述带来了灵活性和特定挑战处理的局限性。为了突破这些限制，我们的研究论文关注在 introducing 矩阵表述方法来解决这些问题。在 dual 中使用加速 gradient descent 方法，我们可以 notable 提高矩阵-SVM 问题的解决效率。实验评估在多标签和多类 datasets 上表明，矩阵 SVM 可以很快地完成任务，同时与 binary relevance SVM 的结果相似。此外，我们的矩阵表述还揭示了一些不太明显的优点和意义，它们可能不会在传统的向量化notation中得到表达。我们强调，许多多标签模型可以被视为 SVM 的扩展，通过自定义修改来满足特定的需求。矩阵表述在这篇论文中建立了一个坚实的基础，用于开发更加复杂的模型，以更好地Addressing 多标签学习中的特殊挑战。
</details></li>
</ul>
<hr>
<h2 id="Explanation-Guided-Fair-Federated-Learning-for-Transparent-6G-RAN-Slicing"><a href="#Explanation-Guided-Fair-Federated-Learning-for-Transparent-6G-RAN-Slicing" class="headerlink" title="Explanation-Guided Fair Federated Learning for Transparent 6G RAN Slicing"></a>Explanation-Guided Fair Federated Learning for Transparent 6G RAN Slicing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09494">http://arxiv.org/abs/2307.09494</a></li>
<li>repo_url: None</li>
<li>paper_authors: Swastika Roy, Hatim Chergui, Christos Verikoukis</li>
<li>for: 这个论文主要目标是建立在6G网络自动化中的透明性和可信度，通过使用可解释人工智能（XAI）技术来帮助建立AI黑obox中的信任。</li>
<li>methods: 这篇论文使用了closed-loop自动化和解释导向学习（EGL）的方法，并采用Jensen-Shannon（JS）差分来评估模型的解释。</li>
<li>results: 实验结果表明，提出的EGFL-JS方案可以提高了6G网络中RAN掉 Package的损失概率预测的可靠性和公平性，相比之下其他基于文献的基elines的性能提高了 более50%，并且提高了Recall metric的评价分。<details>
<summary>Abstract</summary>
Future zero-touch artificial intelligence (AI)-driven 6G network automation requires building trust in the AI black boxes via explainable artificial intelligence (XAI), where it is expected that AI faithfulness would be a quantifiable service-level agreement (SLA) metric along with telecommunications key performance indicators (KPIs). This entails exploiting the XAI outputs to generate transparent and unbiased deep neural networks (DNNs). Motivated by closed-loop (CL) automation and explanation-guided learning (EGL), we design an explanation-guided federated learning (EGFL) scheme to ensure trustworthy predictions by exploiting the model explanation emanating from XAI strategies during the training run time via Jensen-Shannon (JS) divergence. Specifically, we predict per-slice RAN dropped traffic probability to exemplify the proposed concept while respecting fairness goals formulated in terms of the recall metric which is included as a constraint in the optimization task. Finally, the comprehensiveness score is adopted to measure and validate the faithfulness of the explanations quantitatively. Simulation results show that the proposed EGFL-JS scheme has achieved more than $50\%$ increase in terms of comprehensiveness compared to different baselines from the literature, especially the variant EGFL-KL that is based on the Kullback-Leibler Divergence. It has also improved the recall score with more than $25\%$ relatively to unconstrained-EGFL.
</details>
<details>
<summary>摘要</summary>
未来零点touch的人工智能（AI）驱动的6G网络自动化需要建立AI黑盒子中的信任，通过可解释人工智能（XAI），其中AI的忠诚性将被视为服务级别协议（SLA）度量标准和电信键性表现指标（KPI）。这意味着利用XAI输出生成透明和不偏的深度神经网络（DNNs）。驱动closed-loop（CL）自动化和解释导向学习（EGL），我们设计了一种解释导向联合学习（EGFL）方案，以确保可靠的预测，通过在训练过程中使用XAI策略生成的模型解释。例如，我们预测了无线网络承载层损失报告概率，以 illustrate the proposed concept，并且遵循公平性目标表示为Recall度量，并包含在优化任务中。最后，我们采用了completeness分数来评估和验证解释的 faithfulness 量化。实验结果显示，我们的EGFL-JS方案在比较多个基elines之前获得了超过50%的提高，特别是与基于Kullback-Leibler分配的EGFL-KL变体相比，以及不受限制的EGFL。此外，EGFL-JS还提高了Recall分数，相比未受限制的EGFL，提高了超过25%。
</details></li>
</ul>
<hr>
<h2 id="Sparse-Gaussian-Graphical-Models-with-Discrete-Optimization-Computational-and-Statistical-Perspectives"><a href="#Sparse-Gaussian-Graphical-Models-with-Discrete-Optimization-Computational-and-Statistical-Perspectives" class="headerlink" title="Sparse Gaussian Graphical Models with Discrete Optimization: Computational and Statistical Perspectives"></a>Sparse Gaussian Graphical Models with Discrete Optimization: Computational and Statistical Perspectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09366">http://arxiv.org/abs/2307.09366</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kayhan Behdin, Wenyu Chen, Rahul Mazumder</li>
<li>For: The paper aims to estimate the inverse covariance matrix of a multivariate Gaussian distribution, assuming it is sparse.* Methods: The proposed method, called GraphL0BnB, is based on an $\ell_0$-penalized version of the pseudolikelihood function and uses a custom nonlinear branch-and-bound framework to solve the resulting mixed integer program.* Results: The paper reports numerical experiments on real and synthetic datasets that demonstrate the effectiveness of GraphL0BnB in solving the problem to near-optimality, even for large problem instances with $p &#x3D; 10^4$ variables. The paper also compares the performance of GraphL0BnB with various state-of-the-art approaches.Here are the three points in Simplified Chinese:* For: 本文目标是估计多变量 Gaussian 分布下的对角矩阵，假设它是稀疏的。* Methods: 提议的方法是基于 $\ell_0$ 约束的 Pseudolikelihood 函数，使用自定义的非线性分支和约束搜索 Framework 解决 resulting 混合整数程序。* Results: 文章报告了使用真实和 sintetic 数据进行的数值实验，表明 GraphL0BnB 可以准确地解决问题，即使 пробле 的规模很大，例如 $p &#x3D; 10^4$ 变量。文章还比较了 GraphL0BnB 与不同的现有方法的性能。<details>
<summary>Abstract</summary>
We consider the problem of learning a sparse graph underlying an undirected Gaussian graphical model, a key problem in statistical machine learning. Given $n$ samples from a multivariate Gaussian distribution with $p$ variables, the goal is to estimate the $p \times p$ inverse covariance matrix (aka precision matrix), assuming it is sparse (i.e., has a few nonzero entries). We propose GraphL0BnB, a new estimator based on an $\ell_0$-penalized version of the pseudolikelihood function, while most earlier approaches are based on the $\ell_1$-relaxation. Our estimator can be formulated as a convex mixed integer program (MIP) which can be difficult to compute at scale using off-the-shelf commercial solvers. To solve the MIP, we propose a custom nonlinear branch-and-bound (BnB) framework that solves node relaxations with tailored first-order methods. As a by-product of our BnB framework, we propose large-scale solvers for obtaining good primal solutions that are of independent interest. We derive novel statistical guarantees (estimation and variable selection) for our estimator and discuss how our approach improves upon existing estimators. Our numerical experiments on real/synthetic datasets suggest that our method can solve, to near-optimality, problem instances with $p = 10^4$ -- corresponding to a symmetric matrix of size $p \times p$ with $p^2/2$ binary variables. We demonstrate the usefulness of GraphL0BnB versus various state-of-the-art approaches on a range of datasets.
</details>
<details>
<summary>摘要</summary>
我们考虑一个学习简单Graph的问题，即在无向 Gaussian 统计模型中学习一个简单的 inverse covariance 矩阵（即精度矩阵）。假设这个矩阵只有几个非零元素。我们提出了一个新的估计器，即 GraphL0BnB，它基于一个 $\ell_0$-检测的扩展版 pseudolikelihood 函数。大多数先前的方法则是基于 $\ell_1$-缓和。我们的估计器可以表示为一个内部为整数的混合整数程式（MIP），它可能需要大量的计算资源使用现成的商业解决方案。为解决 MIP，我们提出了一个自定义的非线性分支与缓解（BnB）框架，它可以解决节点缓和使用特制的首项方法。作为我们的 BnB 框架的一个副产物，我们提出了一些大规模的解决方案，可以实现良好的原始解。我们 derive novel 的 statistically  garantuees（估计和变数选择） для我们的估计器，并讨论了我们的方法与先前的方法之间的优化。我们的数据实验表明，我们的方法可以对实际数据进行几乎优质的估计，并且可以解决具有 $p = 10^4$ 的问题，即一个对应的矩阵中的 $p^2/2$ 个二进制变数。我们还证明了我们的方法在各种数据集上的优化。
</details></li>
</ul>
<hr>
<h2 id="An-Evaluation-of-Zero-Cost-Proxies-–-from-Neural-Architecture-Performance-to-Model-Robustness"><a href="#An-Evaluation-of-Zero-Cost-Proxies-–-from-Neural-Architecture-Performance-to-Model-Robustness" class="headerlink" title="An Evaluation of Zero-Cost Proxies – from Neural Architecture Performance to Model Robustness"></a>An Evaluation of Zero-Cost Proxies – from Neural Architecture Performance to Model Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09365">http://arxiv.org/abs/2307.09365</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jovita Lukasik, Michael Moeller, Margret Keuper</li>
<li>for: 本文研究zero-cost proxy的应用在 neural architecture search 中，特别是在robustness和clean accuracy之间的joint search。</li>
<li>methods: 本文使用现有的zero-cost proxy来预测模型的性能，并分析这些proxy的特征重要性。</li>
<li>results: 本文发现，单个proxy预测robustness的任务相对更加困难，需要考虑多个proxy来预测模型的robustness。同时，clean accuracy可以由单个proxy进行预测。<details>
<summary>Abstract</summary>
Zero-cost proxies are nowadays frequently studied and used to search for neural architectures. They show an impressive ability to predict the performance of architectures by making use of their untrained weights. These techniques allow for immense search speed-ups. So far the joint search for well-performing and robust architectures has received much less attention in the field of NAS. Therefore, the main focus of zero-cost proxies is the clean accuracy of architectures, whereas the model robustness should play an evenly important part. In this paper, we analyze the ability of common zero-cost proxies to serve as performance predictors for robustness in the popular NAS-Bench-201 search space. We are interested in the single prediction task for robustness and the joint multi-objective of clean and robust accuracy. We further analyze the feature importance of the proxies and show that predicting the robustness makes the prediction task from existing zero-cost proxies more challenging. As a result, the joint consideration of several proxies becomes necessary to predict a model's robustness while the clean accuracy can be regressed from a single such feature.
</details>
<details>
<summary>摘要</summary>
现在，零成本代理常常被研究和使用来搜索神经网络架构。它们能够很好地预测架构的性能，只使用未训练的权重。这些技术可以大大减少搜索速度。迄今为止，搜索well-performing和Robust Architecture在神经网络搜索领域中得到了相对较少的关注。因此，零成本代理的主要焦点是神经网络的净精度，而模型的稳定性应该具有相同的重要性。在这篇论文中，我们分析了常见的零成本代理是否能够用来预测模型的稳定性在NAS-Bench-201搜索空间中。我们对单个预测任务和多目标任务（净精度和稳定性）进行分析。我们还分析了代理的特征重要性，并发现预测稳定性使得预测任务更加困难。因此，需要结合多个代理来预测模型的稳定性，而净精度可以从单个特征进行回归。
</details></li>
</ul>
<hr>
<h2 id="MOCA-Self-supervised-Representation-Learning-by-Predicting-Masked-Online-Codebook-Assignments"><a href="#MOCA-Self-supervised-Representation-Learning-by-Predicting-Masked-Online-Codebook-Assignments" class="headerlink" title="MOCA: Self-supervised Representation Learning by Predicting Masked Online Codebook Assignments"></a>MOCA: Self-supervised Representation Learning by Predicting Masked Online Codebook Assignments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09361">http://arxiv.org/abs/2307.09361</a></li>
<li>repo_url: None</li>
<li>paper_authors: Spyros Gidaris, Andrei Bursuc, Oriane Simeoni, Antonin Vobecky, Nikos Komodakis, Matthieu Cord, Patrick Pérez</li>
<li>for: 降低视Transformer网络的贪吃需求，使用自监学习来减少大量完全标注数据的需求。</li>
<li>methods: 提出了一种单Stage和独立的方法MOCA，通过使用高级特征定义的面积预测任务来捕捉具有良好上下文理解性和图像变化不变性的两种自监学习方法。</li>
<li>results: 在低投入设定下实现新的状态纪录Results，并在多种评估协议中显示出了强大的实验性能，训练时间至少3倍 быстреeder than先前的方法。<details>
<summary>Abstract</summary>
Self-supervised learning can be used for mitigating the greedy needs of Vision Transformer networks for very large fully-annotated datasets. Different classes of self-supervised learning offer representations with either good contextual reasoning properties, e.g., using masked image modeling strategies, or invariance to image perturbations, e.g., with contrastive methods. In this work, we propose a single-stage and standalone method, MOCA, which unifies both desired properties using novel mask-and-predict objectives defined with high-level features (instead of pixel-level details). Moreover, we show how to effectively employ both learning paradigms in a synergistic and computation-efficient way. Doing so, we achieve new state-of-the-art results on low-shot settings and strong experimental results in various evaluation protocols with a training that is at least 3 times faster than prior methods.
</details>
<details>
<summary>摘要</summary>
自我监督学习可以用于减轻视力转换网络的贪吃需求，需要很大的完全标注数据集。不同类型的自我监督学习可以提供具有不同特性的表示，如使用遮盲图像模型策略获得良好的上下文理解性，或者使用对比方法获得图像变化的抗变异性。在这项工作中，我们提出了一种单阶段、独立的方法MOCA，它通过定义高级特征的新式遮盲预测目标来 объединить这两种愿望的特性。此外，我们还示了如何有效地使用这两种学习方法，以实现 computation-efficient 的synergistic效果。通过这种方法，我们在低投入设定下 achieve new state-of-the-art 的结果，并在多种评估协议中获得了强大的实验结果，并且训练时间至少3倍 быстреeder than priori方法。
</details></li>
</ul>
<hr>
<h2 id="Using-the-IBM-Analog-In-Memory-Hardware-Acceleration-Kit-for-Neural-Network-Training-and-Inference"><a href="#Using-the-IBM-Analog-In-Memory-Hardware-Acceleration-Kit-for-Neural-Network-Training-and-Inference" class="headerlink" title="Using the IBM Analog In-Memory Hardware Acceleration Kit for Neural Network Training and Inference"></a>Using the IBM Analog In-Memory Hardware Acceleration Kit for Neural Network Training and Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09357">http://arxiv.org/abs/2307.09357</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/IBM/aihwkit">https://github.com/IBM/aihwkit</a></li>
<li>paper_authors: Manuel Le Gallo, Corey Lammie, Julian Buechel, Fabio Carta, Omobayode Fagbohungbe, Charles Mackin, Hsinyu Tsai, Vijay Narayanan, Abu Sebastian, Kaoutar El Maghraoui, Malte J. Rasch</li>
<li>for: 本文旨在介绍如何在Analog In-Memory Computing（AIMC）硬件上部署深度神经网络（DNN）推理和训练，以实现与数字计算相同的准确性。</li>
<li>methods: 本文使用IBM的Analog Hardware Acceleration Kit（AIHWKit）Python库来模拟DNN的推理和训练。AIHWKit提供了各种功能和最佳实践来进行推理和训练。</li>
<li>results: 本文提供了AIHWKit在推理和训练DNN时的性能分析和评估。此外，本文还介绍了Analog AI Cloud Composer，它提供了使用AIHWKit simulation platform的完全托管云环境的优势。<details>
<summary>Abstract</summary>
Analog In-Memory Computing (AIMC) is a promising approach to reduce the latency and energy consumption of Deep Neural Network (DNN) inference and training. However, the noisy and non-linear device characteristics, and the non-ideal peripheral circuitry in AIMC chips, require adapting DNNs to be deployed on such hardware to achieve equivalent accuracy to digital computing. In this tutorial, we provide a deep dive into how such adaptations can be achieved and evaluated using the recently released IBM Analog Hardware Acceleration Kit (AIHWKit), freely available at https://github.com/IBM/aihwkit. The AIHWKit is a Python library that simulates inference and training of DNNs using AIMC. We present an in-depth description of the AIHWKit design, functionality, and best practices to properly perform inference and training. We also present an overview of the Analog AI Cloud Composer, that provides the benefits of using the AIHWKit simulation platform in a fully managed cloud setting. Finally, we show examples on how users can expand and customize AIHWKit for their own needs. This tutorial is accompanied by comprehensive Jupyter Notebook code examples that can be run using AIHWKit, which can be downloaded from https://github.com/IBM/aihwkit/tree/master/notebooks/tutorial.
</details>
<details>
<summary>摘要</summary>
智能存储计算（AIMC）是一种有前途的方法，以减少深度神经网络（DNN）的延迟和能耗。然而，设备特性和周围电路在AIMC芯片上是不准确的，需要适应DNN来实现相同的准确率。在这个教程中，我们将提供深入的解释如何实现和评估这些适应，以及使用IBM的分析硬件加速器包（AIHWKit）进行 simulate inference和训练。AIHWKit是一个基于Python的库，可以模拟DNN的推理和训练 using AIMC。我们将提供AIHWKit的设计、功能和最佳实践，以及使用Analog AI Cloud Composer在云环境中实现相应的优势。最后，我们将展示用户如何扩展和自定义AIHWKit。这个教程的详细 Jupyter Notebook 代码示例可以从https://github.com/IBM/aihwkit/tree/master/notebooks/tutorial下载。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Select-SAT-Encodings-for-Pseudo-Boolean-and-Linear-Integer-Constraints"><a href="#Learning-to-Select-SAT-Encodings-for-Pseudo-Boolean-and-Linear-Integer-Constraints" class="headerlink" title="Learning to Select SAT Encodings for Pseudo-Boolean and Linear Integer Constraints"></a>Learning to Select SAT Encodings for Pseudo-Boolean and Linear Integer Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09342">http://arxiv.org/abs/2307.09342</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/felixvuo/lease-data">https://github.com/felixvuo/lease-data</a></li>
<li>paper_authors: Felix Ulrich-Oltean, Peter Nightingale, James Alfred Walker</li>
<li>for: 解决复杂的满足和优化问题</li>
<li>methods: 使用超级vised机器学习方法选择编码</li>
<li>results: 比AutoFolio好，可以选择不同类型的问题编码<details>
<summary>Abstract</summary>
Many constraint satisfaction and optimisation problems can be solved effectively by encoding them as instances of the Boolean Satisfiability problem (SAT). However, even the simplest types of constraints have many encodings in the literature with widely varying performance, and the problem of selecting suitable encodings for a given problem instance is not trivial. We explore the problem of selecting encodings for pseudo-Boolean and linear constraints using a supervised machine learning approach. We show that it is possible to select encodings effectively using a standard set of features for constraint problems; however we obtain better performance with a new set of features specifically designed for the pseudo-Boolean and linear constraints. In fact, we achieve good results when selecting encodings for unseen problem classes. Our results compare favourably to AutoFolio when using the same feature set. We discuss the relative importance of instance features to the task of selecting the best encodings, and compare several variations of the machine learning method.
</details>
<details>
<summary>摘要</summary>
许多约束满足和优化问题可以有效地通过编码为布尔满足问题（SAT）解决。然而，最简单的约束类型还有许多编码方法在文献中，这些编码方法之间的性能差异很大，选择适合的编码方法 для给定问题实例是一个不容易的问题。我们使用监督式机器学习方法来选择编码方法，并证明可以使用标准的约束问题特征集来选择编码方法，但是使用专门为 Pseudo-Boolean 和线性约束设计的新特征集可以获得更好的性能。我们在使用同一集特征时与 AutoFolio 进行比较，并讨论实例特征对选择最佳编码方法的重要性。我们还比较了几种机器学习方法的变种。
</details></li>
</ul>
<hr>
<h2 id="Towards-Automated-Semantic-Segmentation-in-Mammography-Images"><a href="#Towards-Automated-Semantic-Segmentation-in-Mammography-Images" class="headerlink" title="Towards Automated Semantic Segmentation in Mammography Images"></a>Towards Automated Semantic Segmentation in Mammography Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10296">http://arxiv.org/abs/2307.10296</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cesar A. Sierra-Franco, Jan Hurtado, Victor de A. Thomaz, Leonardo C. da Cruz, Santiago V. Silva, Alberto B. Raposo</li>
<li>for: 检测非可触护乳腺癌，提供诊断和评估图像质量的机会。</li>
<li>methods: 使用深度学习框架自动 segmenting 乳腺、肌肉、肉细胞和脂肪组织的边界。</li>
<li>results: 在多种不同的框架和图像 dataset 下，实现了准确的 segmentation 性能，表明该框架可以在临床实践中整合。<details>
<summary>Abstract</summary>
Mammography images are widely used to detect non-palpable breast lesions or nodules, preventing cancer and providing the opportunity to plan interventions when necessary. The identification of some structures of interest is essential to make a diagnosis and evaluate image adequacy. Thus, computer-aided detection systems can be helpful in assisting medical interpretation by automatically segmenting these landmark structures. In this paper, we propose a deep learning-based framework for the segmentation of the nipple, the pectoral muscle, the fibroglandular tissue, and the fatty tissue on standard-view mammography images. We introduce a large private segmentation dataset and extensive experiments considering different deep-learning model architectures. Our experiments demonstrate accurate segmentation performance on variate and challenging cases, showing that this framework can be integrated into clinical practice.
</details>
<details>
<summary>摘要</summary>
乳影像广泛用于检测不可触感乳腺癌病或肿块，预防癌病并提供诊断和治疗计划时的机会。确定一些关键结构的标识是诊断和评估影像质量的关键。因此，计算机助成检测系统可以帮助医疗解释人员自动分割关键结构。在这篇论文中，我们提出了基于深度学习的框架，用于标识乳膜、肌肉、肉絮组织和脂肪组织在标准视图乳影像中的自动分割。我们提供了大量私有分割数据集和广泛的实验，考虑了不同的深度学习模型架构。我们的实验结果表明，这种框架在多种和复杂的案例中具有高准确性，这表明该框架可以在临床实践中集成。
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Field-Dependencies-for-Learning-on-Categorical-Data"><a href="#Exploiting-Field-Dependencies-for-Learning-on-Categorical-Data" class="headerlink" title="Exploiting Field Dependencies for Learning on Categorical Data"></a>Exploiting Field Dependencies for Learning on Categorical Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09321">http://arxiv.org/abs/2307.09321</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/csiro-robotics/mdl">https://github.com/csiro-robotics/mdl</a></li>
<li>paper_authors: Zhibin Li, Piotr Koniusz, Lu Zhang, Daniel Edward Pagendam, Peyman Moghadam</li>
<li>for: 学习 categorical 数据中的依赖关系，以提高模型的准确率和稳定性。</li>
<li>methods: 提出了一种新的方法，通过学习全局字段依赖矩阵，然后在实例级别使用不同的权重（即本地依赖模型）来提高字段间的模型。</li>
<li>results: 在六个popular数据集上比较了多种现有方法，并达到了更高的准确率和稳定性。详细的ablation study提供了更多的内容。<details>
<summary>Abstract</summary>
Traditional approaches for learning on categorical data underexploit the dependencies between columns (\aka fields) in a dataset because they rely on the embedding of data points driven alone by the classification/regression loss. In contrast, we propose a novel method for learning on categorical data with the goal of exploiting dependencies between fields. Instead of modelling statistics of features globally (i.e., by the covariance matrix of features), we learn a global field dependency matrix that captures dependencies between fields and then we refine the global field dependency matrix at the instance-wise level with different weights (so-called local dependency modelling) w.r.t. each field to improve the modelling of the field dependencies. Our algorithm exploits the meta-learning paradigm, i.e., the dependency matrices are refined in the inner loop of the meta-learning algorithm without the use of labels, whereas the outer loop intertwines the updates of the embedding matrix (the matrix performing projection) and global dependency matrix in a supervised fashion (with the use of labels). Our method is simple yet it outperforms several state-of-the-art methods on six popular dataset benchmarks. Detailed ablation studies provide additional insights into our method.
</details>
<details>
<summary>摘要</summary>
传统方法学习 categorical 数据会忽略数据集中列（即字段）之间的依赖关系，因为它们基于单独的分类/回归损失来驱动数据点的嵌入。相比之下，我们提出了一种新的方法，旨在利用数据集中列之间的依赖关系。而不是通过特征的全局统计来模型特征（即特征的covariance矩阵），我们学习一个全局字段依赖矩阵，然后在每个实例级别使用不同的权重（即本地依赖模型）来改进字段依赖的模型。我们的算法利用了元学习 парадиг，即依赖矩阵在内Loop中被反复更新，而外Loop则在有标签的情况下，将插入矩阵的更新和全局依赖矩阵的更新相互交互。我们的方法简单，但它在六个流行的数据集benchmark上表现出色，并且我们进行了详细的拟合分析，以提供更多的准确性。
</details></li>
</ul>
<hr>
<h2 id="Biomaker-CA-a-Biome-Maker-project-using-Cellular-Automata"><a href="#Biomaker-CA-a-Biome-Maker-project-using-Cellular-Automata" class="headerlink" title="Biomaker CA: a Biome Maker project using Cellular Automata"></a>Biomaker CA: a Biome Maker project using Cellular Automata</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09320">http://arxiv.org/abs/2307.09320</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ettore Randazzo, Alexander Mordvintsev</li>
<li>for: 这个论文是关于使用细胞自动机（CA）模拟生物体的生长和进化的研究。</li>
<li>methods: 这个研究使用了Python JAX框架对2D网格上的CA规则进行并行计算，并提供了不同的环境和物理法则，以及不同的模型架构和 мутаagen策略。</li>
<li>results: 研究人员通过模拟不同的环境和物理法则，证明了植物代理可以在缺乏营养的环境中生长、存活、繁殖和演化，并且可以通过用户交互式进行进化。<details>
<summary>Abstract</summary>
We introduce Biomaker CA: a Biome Maker project using Cellular Automata (CA). In Biomaker CA, morphogenesis is a first class citizen and small seeds need to grow into plant-like organisms to survive in a nutrient starved environment and eventually reproduce with variation so that a biome survives for long timelines. We simulate complex biomes by means of CA rules in 2D grids and parallelize all of its computation on GPUs through the Python JAX framework. We show how this project allows for several different kinds of environments and laws of 'physics', alongside different model architectures and mutation strategies. We further analyze some configurations to show how plant agents can grow, survive, reproduce, and evolve, forming stable and unstable biomes. We then demonstrate how one can meta-evolve models to survive in a harsh environment either through end-to-end meta-evolution or by a more surgical and efficient approach, called Petri dish meta-evolution. Finally, we show how to perform interactive evolution, where the user decides how to evolve a plant model interactively and then deploys it in a larger environment. We open source Biomaker CA at: https://tinyurl.com/2x8yu34s .
</details>
<details>
<summary>摘要</summary>
我们介绍生物创造器 CA：一个基因组织（CA）的生物创造项目。在生物创造器 CA 中，形态形成是一等公民，小种子需要在营养不足的环境中增长成植物如果生存，并最终繁殖，具有多样性，以确保生物群落长期存活。我们使用2D网格上的 CA 规则来模拟复杂的生态系统，并通过 Python JAX 框架在 GPU 上分布式计算。我们显示了这个项目可以支援多种环境和物理法则，以及不同的模型架构和突变策略。我们进一步分析了一些配置，说明植物代表如何在营养不足的环境中增长、存活、繁殖和演化，形成稳定和不稳定的生态系统。我们还示出了如何使用终端进化来在严峻环境中存活，或者使用更精确和高效的方法，即 Petri dish 进化。最后，我们显示了如何进行互动演化，让用户可以在互动式的方式下演化植物模型，然后将其部署到更大的环境中。我们开源了生物创造器 CA，请参考以下连结：https://tinyurl.com/2x8yu34s。
</details></li>
</ul>
<hr>
<h2 id="Multi-Modal-Discussion-Transformer-Integrating-Text-Images-and-Graph-Transformers-to-Detect-Hate-Speech-on-Social-Media"><a href="#Multi-Modal-Discussion-Transformer-Integrating-Text-Images-and-Graph-Transformers-to-Detect-Hate-Speech-on-Social-Media" class="headerlink" title="Multi-Modal Discussion Transformer: Integrating Text, Images and Graph Transformers to Detect Hate Speech on Social Media"></a>Multi-Modal Discussion Transformer: Integrating Text, Images and Graph Transformers to Detect Hate Speech on Social Media</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09312">http://arxiv.org/abs/2307.09312</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liamhebert/multimodaldiscussiontransformer">https://github.com/liamhebert/multimodaldiscussiontransformer</a></li>
<li>paper_authors: Liam Hebert, Gaurav Sahu, Nanda Kishore Sreenivas, Lukasz Golab, Robin Cohen</li>
<li>for: 本研究的目的是开发一种基于多模态图表示的 hate speech 检测模型，以捕捉在在线社交网络中的谩骂语言。</li>
<li>methods: 该模型使用图transformer来捕捉整个讨论的上下文关系，并使用杂合层将文本和图像嵌入组合以取代单modal处理。</li>
<li>results: 对于基elines进行比较，我们发现我们的模型在检测 hate speech 方面的性能明显提高了，并进行了广泛的ablation研究。Translation:</li>
<li>for: The purpose of this study is to develop a hate speech detection model based on multimodal graph representation, to capture anti-social behavior in online social networks.</li>
<li>methods: The model uses graph transformers to capture the contextual relationships in the entire discussion, and combines text and image embeddings using fusion layers instead of processing them separately.</li>
<li>results: Compared to baselines, our model shows significantly improved performance in detecting hate speech, and we conducted extensive ablation studies.<details>
<summary>Abstract</summary>
We present the Multi-Modal Discussion Transformer (mDT), a novel multi-modal graph-based transformer model for detecting hate speech in online social networks. In contrast to traditional text-only methods, our approach to labelling a comment as hate speech centers around the holistic analysis of text and images. This is done by leveraging graph transformers to capture the contextual relationships in the entire discussion that surrounds a comment, with interwoven fusion layers to combine text and image embeddings instead of processing different modalities separately. We compare the performance of our model to baselines that only process text; we also conduct extensive ablation studies. We conclude with future work for multimodal solutions to deliver social value in online contexts, arguing that capturing a holistic view of a conversation greatly advances the effort to detect anti-social behavior.
</details>
<details>
<summary>摘要</summary>
我们介绍了多模态讨论变换器（mDT），一种新的多模态图基于变换器模型，用于在社交网络上探测 hate speech。与传统的文本Only方法不同，我们的方法是根据对评论的全面分析，包括文本和图像。我们利用图transformer来捕捉讨论中的上下文关系，并通过交叠卷积层将文本和图像嵌入拼接在一起，而不是分开处理不同的模式。我们与基线对比，并进行了广泛的剖析研究。我们 conclude 未来的多模态解决方案可以为在线上提供社会价值，因为捕捉讨论的全面视图可以帮助探测反社会行为。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Differentiation-for-Inverse-Problems-with-Applications-in-Quantum-Transport"><a href="#Automatic-Differentiation-for-Inverse-Problems-with-Applications-in-Quantum-Transport" class="headerlink" title="Automatic Differentiation for Inverse Problems with Applications in Quantum Transport"></a>Automatic Differentiation for Inverse Problems with Applications in Quantum Transport</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09311">http://arxiv.org/abs/2307.09311</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ivan Williams, Eric Polizzi</li>
<li>for:  inverse quantum transport problem</li>
<li>methods: neural solver and differentiable simulation</li>
<li>results: engineering continuous transmission properties and current-voltage characteristics<details>
<summary>Abstract</summary>
A neural solver and differentiable simulation of the quantum transmitting boundary model is presented for the inverse quantum transport problem. The neural solver is used to engineer continuous transmission properties and the differentiable simulation is used to engineer current-voltage characteristics.
</details>
<details>
<summary>摘要</summary>
neural 算法和可导的量子传输边界模型是用于反向量 calculus 问题的解决方案。 neural 算法用于引入连续传输性质，而可导的 simulate 用于引入电流-电压特性。
</details></li>
</ul>
<hr>
<h2 id="EigenTrajectory-Low-Rank-Descriptors-for-Multi-Modal-Trajectory-Forecasting"><a href="#EigenTrajectory-Low-Rank-Descriptors-for-Multi-Modal-Trajectory-Forecasting" class="headerlink" title="EigenTrajectory: Low-Rank Descriptors for Multi-Modal Trajectory Forecasting"></a>EigenTrajectory: Low-Rank Descriptors for Multi-Modal Trajectory Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09306">http://arxiv.org/abs/2307.09306</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/inhwanbae/eigentrajectory">https://github.com/inhwanbae/eigentrajectory</a></li>
<li>paper_authors: Inhwan Bae, Jean Oh, Hae-Gon Jeon</li>
<li>For: 本研究旨在提高人行道预测的精度和可靠性，通过使用新的轨迹描述符来减少轨迹的维度。* Methods: 我们使用一种新的轨迹描述符，将人行道径变换为一个紧凑的 $\mathbb{ET}$ 空间，然后使用现有的轨迹预测模型进行预测。此外，我们还提出了一种基于轨迹锚点的修正方法，以覆盖所有可能的未来。* Results: 我们的实验结果表明，使用我们的 EigenTrajectory 预测器可以显著提高现有轨迹预测模型的预测精度和可靠性，这表明我们的描述符适用于表示行人行为。代码可以在 <a target="_blank" rel="noopener" href="https://github.com/inhwanbae/EigenTrajectory">https://github.com/inhwanbae/EigenTrajectory</a> 上下载。<details>
<summary>Abstract</summary>
Capturing high-dimensional social interactions and feasible futures is essential for predicting trajectories. To address this complex nature, several attempts have been devoted to reducing the dimensionality of the output variables via parametric curve fitting such as the B\'ezier curve and B-spline function. However, these functions, which originate in computer graphics fields, are not suitable to account for socially acceptable human dynamics. In this paper, we present EigenTrajectory ($\mathbb{ET}$), a trajectory prediction approach that uses a novel trajectory descriptor to form a compact space, known here as $\mathbb{ET}$ space, in place of Euclidean space, for representing pedestrian movements. We first reduce the complexity of the trajectory descriptor via a low-rank approximation. We transform the pedestrians' history paths into our $\mathbb{ET}$ space represented by spatio-temporal principle components, and feed them into off-the-shelf trajectory forecasting models. The inputs and outputs of the models as well as social interactions are all gathered and aggregated in the corresponding $\mathbb{ET}$ space. Lastly, we propose a trajectory anchor-based refinement method to cover all possible futures in the proposed $\mathbb{ET}$ space. Extensive experiments demonstrate that our EigenTrajectory predictor can significantly improve both the prediction accuracy and reliability of existing trajectory forecasting models on public benchmarks, indicating that the proposed descriptor is suited to represent pedestrian behaviors. Code is publicly available at https://github.com/inhwanbae/EigenTrajectory .
</details>
<details>
<summary>摘要</summary>
Capturing high-dimensional social interactions and feasible futures is essential for predicting trajectories. To address this complex nature, several attempts have been devoted to reducing the dimensionality of the output variables via parametric curve fitting such as the B\'ezier curve and B-spline function. However, these functions, which originate in computer graphics fields, are not suitable to account for socially acceptable human dynamics. In this paper, we present EigenTrajectory ($\mathbb{ET}$), a trajectory prediction approach that uses a novel trajectory descriptor to form a compact space, known here as $\mathbb{ET}$ space, in place of Euclidean space, for representing pedestrian movements. We first reduce the complexity of the trajectory descriptor via a low-rank approximation. We transform the pedestrians' history paths into our $\mathbb{ET}$ space represented by spatio-temporal principle components, and feed them into off-the-shelf trajectory forecasting models. The inputs and outputs of the models as well as social interactions are all gathered and aggregated in the corresponding $\mathbb{ET}$ space. Lastly, we propose a trajectory anchor-based refinement method to cover all possible futures in the proposed $\mathbb{ET}$ space. Extensive experiments demonstrate that our EigenTrajectory predictor can significantly improve both the prediction accuracy and reliability of existing trajectory forecasting models on public benchmarks, indicating that the proposed descriptor is suited to represent pedestrian behaviors. Code is publicly available at https://github.com/inhwanbae/EigenTrajectory .
</details></li>
</ul>
<hr>
<h2 id="Conformal-prediction-under-ambiguous-ground-truth"><a href="#Conformal-prediction-under-ambiguous-ground-truth" class="headerlink" title="Conformal prediction under ambiguous ground truth"></a>Conformal prediction under ambiguous ground truth</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09302">http://arxiv.org/abs/2307.09302</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Stutz, Abhijit Guha Roy, Tatiana Matejovicova, Patricia Strachan, Ali Taylan Cemgil, Arnaud Doucet</li>
<li>for: 这个论文的目的是提出一种基于不确定标签的整形预测方法，以便在不具备准确标签的情况下进行不确定性评估。</li>
<li>methods: 该方法基于一种approximentation of the underlying posterior distribution of labels given inputs，以便处理不具备准确标签的情况。</li>
<li>results: 在synthetic和实际 dataset上，该方法可以准确地预测输入样本的标签，并且可以正确地评估输入样本的不确定性。在一个dermatology例子中，该方法可以成功地预测皮肤状况的标签。<details>
<summary>Abstract</summary>
In safety-critical classification tasks, conformal prediction allows to perform rigorous uncertainty quantification by providing confidence sets including the true class with a user-specified probability. This generally assumes the availability of a held-out calibration set with access to ground truth labels. Unfortunately, in many domains, such labels are difficult to obtain and usually approximated by aggregating expert opinions. In fact, this holds true for almost all datasets, including well-known ones such as CIFAR and ImageNet. Applying conformal prediction using such labels underestimates uncertainty. Indeed, when expert opinions are not resolvable, there is inherent ambiguity present in the labels. That is, we do not have ``crisp'', definitive ground truth labels and this uncertainty should be taken into account during calibration. In this paper, we develop a conformal prediction framework for such ambiguous ground truth settings which relies on an approximation of the underlying posterior distribution of labels given inputs. We demonstrate our methodology on synthetic and real datasets, including a case study of skin condition classification in dermatology.
</details>
<details>
<summary>摘要</summary>
在安全关键分类任务中，协形预测可以进行严格的不确定性评估，提供包含真实类别的信任集，用户可以指定概率。通常假设有一个保留 calibration set 可以获得真实标签。然而，在许多领域，这些标签很难获得，通常通过专家意见的汇总来估计。实际上，这是大多数数据集中的情况，包括常见的 CIFAR 和 ImageNet。在这些标签不确定的情况下，使用协形预测会下降 uncertainty。事实上，当专家意见不能分解时，存在 inherent  ambiguity 在标签中。即，我们没有 "卷积" 、definitive 的地面实标签，这种uncertainty 应该在 calibration 阶段被考虑。在这篇论文中，我们开发了一种协形预测框架，用于这种 ambiguous 地面实标签设置，基于输入的后期分布。我们在 sintetic 和实际数据集上进行了示例研究，包括皮肤状况分类在皮肤科学中。
</details></li>
</ul>
<hr>
<h2 id="FlexiAST-Flexibility-is-What-AST-Needs"><a href="#FlexiAST-Flexibility-is-What-AST-Needs" class="headerlink" title="FlexiAST: Flexibility is What AST Needs"></a>FlexiAST: Flexibility is What AST Needs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09286">http://arxiv.org/abs/2307.09286</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/JiuFengSC/FlexiAST_INTERSPEECH23">https://github.com/JiuFengSC/FlexiAST_INTERSPEECH23</a></li>
<li>paper_authors: Jiu Feng, Mehmet Hamza Erol, Joon Son Chung, Arda Senocak</li>
<li>for: 提高Audio Spectrogram Transformer（AST）模型在不同补丁大小下的性能。</li>
<li>methods: 提出一种基于随机补丁大小选择和补丁重量resize的训练方法，使标准AST模型在推理阶段能够适应不同补丁大小。</li>
<li>results: 实验表明，FlexiAST模型在不同补丁大小下保持类似的性能，而无需进行architecture变更。<details>
<summary>Abstract</summary>
The objective of this work is to give patch-size flexibility to Audio Spectrogram Transformers (AST). Recent advancements in ASTs have shown superior performance in various audio-based tasks. However, the performance of standard ASTs degrades drastically when evaluated using different patch sizes from that used during training. As a result, AST models are typically re-trained to accommodate changes in patch sizes. To overcome this limitation, this paper proposes a training procedure to provide flexibility to standard AST models without architectural changes, allowing them to work with various patch sizes at the inference stage - FlexiAST. This proposed training approach simply utilizes random patch size selection and resizing of patch and positional embedding weights. Our experiments show that FlexiAST gives similar performance to standard AST models while maintaining its evaluation ability at various patch sizes on different datasets for audio classification tasks.
</details>
<details>
<summary>摘要</summary>
“这个工作的目标是给Audio Spectrogram Transformer（AST）提供材质大小灵活性。现代AST的进步已经在不同的音频任务中展示出色的表现。然而，标准AST的表现会在不同的材质大小下逐渐下降，导致AST模型需要重新训练来适应材质大小的变化。为了解决这个限制，这篇文章提出了一种不需要建构更改的训练方法，可以让标准AST模型在测试阶段运行各种材质大小。这个提案使用随机选择材质大小和重复材质大小的位置嵌入对应。我们的实验表明，FlexiAST可以与标准AST模型的表现相似，并且在不同的材质大小下保持评估能力。”Note that Simplified Chinese is a romanization of Chinese, and the actual Chinese text may be written differently.
</details></li>
</ul>
<hr>
<h2 id="End-to-End-Neural-Network-Training-for-Hyperbox-Based-Classification"><a href="#End-to-End-Neural-Network-Training-for-Hyperbox-Based-Classification" class="headerlink" title="End-to-End Neural Network Training for Hyperbox-Based Classification"></a>End-to-End Neural Network Training for Hyperbox-Based Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09269">http://arxiv.org/abs/2307.09269</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mlde-ms/hypernn">https://github.com/mlde-ms/hypernn</a></li>
<li>paper_authors: Denis Mayr Lima Martins, Christian Lülf, Fabian Gieseke</li>
<li>for: 这篇论文是为了提出一个新的、可微分的核心框架，以便实现高效地对大量数据进行分类。</li>
<li>methods: 这篇论文使用了神经网络，并将核心框架转换为可微分的形式，以便实现更高效的训练。</li>
<li>results: 这篇论文的结果显示，使用这个新的核心框架和训练方法可以获得更好的分类结果，并且训练时间更短。<details>
<summary>Abstract</summary>
Hyperbox-based classification has been seen as a promising technique in which decisions on the data are represented as a series of orthogonal, multidimensional boxes (i.e., hyperboxes) that are often interpretable and human-readable. However, existing methods are no longer capable of efficiently handling the increasing volume of data many application domains face nowadays. We address this gap by proposing a novel, fully differentiable framework for hyperbox-based classification via neural networks. In contrast to previous work, our hyperbox models can be efficiently trained in an end-to-end fashion, which leads to significantly reduced training times and superior classification results.
</details>
<details>
<summary>摘要</summary>
互 box 基本的分类技术已被视为一种有前途的技术，在这种技术中，数据的决策被表示为一系列的正交、多维度的盒子（即互 box），这些盒子通常是可读的和人类可读的。然而，现有的方法不再能够有效地处理现在许多应用领域面临的数据量的增加。我们解决这个问题 by proposing a novel, fully differentiable framework for hyperbox-based classification via neural networks. 在我们的方法中，互 box 模型可以在端到端的方式进行高效地训练，这导致了训练时间的减少和分类结果的提高。
</details></li>
</ul>
<hr>
<h2 id="Mobility-Aware-Joint-User-Scheduling-and-Resource-Allocation-for-Low-Latency-Federated-Learning"><a href="#Mobility-Aware-Joint-User-Scheduling-and-Resource-Allocation-for-Low-Latency-Federated-Learning" class="headerlink" title="Mobility-Aware Joint User Scheduling and Resource Allocation for Low Latency Federated Learning"></a>Mobility-Aware Joint User Scheduling and Resource Allocation for Low Latency Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09263">http://arxiv.org/abs/2307.09263</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kecheng Fan, Wen Chen, Jun Li, Xiumei Deng, Xuefeng Han, Ming Ding</li>
<li>for: 这个论文的目的是提出一个实用的机器学习方法，以解决在联盟学习（Federated Learning，FL）中用户移动导致训练效能下降的问题。</li>
<li>methods: 这个论文使用了一个实际的用户移动模型，并提出了一个用户排程和资源分配方法，以减少训练延迟时间，并且考虑了对于用户移动的影响。</li>
<li>results:  simulations results show that the proposed algorithm achieves better performance than the state-of-the-art baselines, and a certain level of user mobility could improve training performance.<details>
<summary>Abstract</summary>
As an efficient distributed machine learning approach, Federated learning (FL) can obtain a shared model by iterative local model training at the user side and global model aggregating at the central server side, thereby protecting privacy of users. Mobile users in FL systems typically communicate with base stations (BSs) via wireless channels, where training performance could be degraded due to unreliable access caused by user mobility. However, existing work only investigates a static scenario or random initialization of user locations, which fail to capture mobility in real-world networks. To tackle this issue, we propose a practical model for user mobility in FL across multiple BSs, and develop a user scheduling and resource allocation method to minimize the training delay with constrained communication resources. Specifically, we first formulate an optimization problem with user mobility that jointly considers user selection, BS assignment to users, and bandwidth allocation to minimize the latency in each communication round. This optimization problem turned out to be NP-hard and we proposed a delay-aware greedy search algorithm (DAGSA) to solve it. Simulation results show that the proposed algorithm achieves better performance than the state-of-the-art baselines and a certain level of user mobility could improve training performance.
</details>
<details>
<summary>摘要</summary>
为了实现高效的分布式机器学习方法，联邦学习（FL）可以在用户端进行轮循式地本地模型训练和中央服务器端进行全球模型汇总，以保护用户隐私。在FL系统中，移动用户通常通过无线通信通道与基站（BS）进行交互，但是训练性能可能受到用户移动导致的不可预测访问所增加的干扰。现有研究仅考虑静止场景或随机初始化用户位置，未能捕捉实际网络中的移动。为解决这个问题，我们提出了实际的用户移动模型在FL中，并开发了一种用户调度和资源分配方法，以最小化通信资源的培训延迟。具体来说，我们首先将用户移动引入到联邦学习中的优化问题中，并jointly考虑用户选择、用户分配到BS以及带宽分配，以最小化每次通信圈中的延迟。这个优化问题被证明是NP困难的，我们提出了延迟意识搜索算法（DAGSA）解决它。实验结果表明，我们的算法在比较状态前的基elines上表现得更好，并且一定程度的用户移动可以提高训练性能。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Topological-Feature-via-Persistent-Homology-Filtration-Learning-for-Point-Clouds"><a href="#Adaptive-Topological-Feature-via-Persistent-Homology-Filtration-Learning-for-Point-Clouds" class="headerlink" title="Adaptive Topological Feature via Persistent Homology: Filtration Learning for Point Clouds"></a>Adaptive Topological Feature via Persistent Homology: Filtration Learning for Point Clouds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09259">http://arxiv.org/abs/2307.09259</a></li>
<li>repo_url: None</li>
<li>paper_authors: Naoki Nishikawa, Yuichi Ike, Kenji Yamanishi</li>
<li>for: 提高机器学习点云处理精度，应用于形态识别和材料科学等领域。</li>
<li>methods: 使用神经网络学习自适应滤波，以保证 persistent homology 的同质性。</li>
<li>results: 在多个分类任务中表现出色，证明了我们的框架的可行性。Here’s the full text in Simplified Chinese:</li>
<li>for: 本研究旨在提高机器学习点云处理精度，应用于形态识别和材料科学等领域。</li>
<li>methods: 我们提出了一种基于神经网络的自适应滤波方法，以保证 persistent homology 的同质性。</li>
<li>results: 我们在多个分类任务中进行了实验，结果表明我们的框架在这些任务中表现出色，证明了我们的方法的可行性。<details>
<summary>Abstract</summary>
Machine learning for point clouds has been attracting much attention, with many applications in various fields, such as shape recognition and material science. To enhance the accuracy of such machine learning methods, it is known to be effective to incorporate global topological features, which are typically extracted by persistent homology. In the calculation of persistent homology for a point cloud, we need to choose a filtration for the point clouds, an increasing sequence of spaces. Because the performance of machine learning methods combined with persistent homology is highly affected by the choice of a filtration, we need to tune it depending on data and tasks. In this paper, we propose a framework that learns a filtration adaptively with the use of neural networks. In order to make the resulting persistent homology isometry-invariant, we develop a neural network architecture with such invariance. Additionally, we theoretically show a finite-dimensional approximation result that justifies our architecture. Experimental results demonstrated the efficacy of our framework in several classification tasks.
</details>
<details>
<summary>摘要</summary>
In this paper, we propose a framework that learns a filtration adaptively using neural networks. To ensure the resulting persistent homology isometry-invariant, we develop a neural network architecture with such invariance. Additionally, we provide a finite-dimensional approximation result that justifies our architecture. Experimental results show the effectiveness of our framework in several classification tasks.Here's the translation in Simplified Chinese:机器学习 для点云已经吸引了很多关注，并在各种领域上有广泛的应用，如形状识别和材料科学。为了提高机器学习方法的准确性，通常需要包含全局拓扑特征，通常通过不变 homology 来提取。在计算不变 homology 中，我们需要选择一个筛选器，是一个增长序列的空间。由于选择筛选器的性能会高度影响机器学习方法的性能，因此需要根据数据和任务进行调整。在这篇论文中，我们提出了一种框架，通过使用神经网络来自适应地学习筛选器。为确保结果的不变 homology 尺度 invariants，我们开发了一种具有不变性的神经网络架构。此外，我们也提供了一个数学上的有限维approximation 结果，证明了我们的架构的正确性。实验结果表明，我们的框架在多个分类任务中表现出色。
</details></li>
</ul>
<hr>
<h2 id="PAC-Neural-Prediction-Set-Learning-to-Quantify-the-Uncertainty-of-Generative-Language-Models"><a href="#PAC-Neural-Prediction-Set-Learning-to-Quantify-the-Uncertainty-of-Generative-Language-Models" class="headerlink" title="PAC Neural Prediction Set Learning to Quantify the Uncertainty of Generative Language Models"></a>PAC Neural Prediction Set Learning to Quantify the Uncertainty of Generative Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09254">http://arxiv.org/abs/2307.09254</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sangdon Park, Taesoo Kim</li>
<li>for: 提高模型的可靠性和信任worthiness</li>
<li>methods: 使用神经网络 parameterized prediction set models，实现更精准的uncertainty quantification，并且满足 probably approximately correct (PAC) 保证</li>
<li>results: 在四种语言数据集和六种模型上，比基准方法提高quantified uncertainty的精度$63%$的平均值<details>
<summary>Abstract</summary>
Uncertainty learning and quantification of models are crucial tasks to enhance the trustworthiness of the models. Importantly, the recent surge of generative language models (GLMs) emphasizes the need for reliable uncertainty quantification due to the concerns on generating hallucinated facts. In this paper, we propose to learn neural prediction set models that comes with the probably approximately correct (PAC) guarantee for quantifying the uncertainty of GLMs. Unlike existing prediction set models, which are parameterized by a scalar value, we propose to parameterize prediction sets via neural networks, which achieves more precise uncertainty quantification but still satisfies the PAC guarantee. We demonstrate the efficacy of our method on four types of language datasets and six types of models by showing that our method improves the quantified uncertainty by $63\%$ on average, compared to a standard baseline method.
</details>
<details>
<summary>摘要</summary>
<<SYS>>对不确定性学习和模型评估是重要任务，以提高模型的可靠性。尤其是最近几年的生成语言模型（GLMs），它们的不确定性评估问题得到了更多的关注，因为它们可能会生成假信息。在这篇论文中，我们提议使用神经网络来学习预测集模型，这些模型具有可靠的不确定性评估保证（PAC），并且可以更 precisely 评估 GLMs 的不确定性。 unlike 现有的预测集模型，我们的模型参数化使用神经网络，这使得我们可以更好地评估 GLMs 的不确定性。我们在四种语言 dataset 和六种模型上进行了实验，并证明我们的方法可以提高评估不确定性的精度，相比标准基准方法，提高了63%的平均值。Note: " Probably approximately correct" (PAC) is a theoretical guarantee that the output of a machine learning model is likely to be close to the true output, with a certain level of confidence. In this context, the PAC guarantee is used to ensure that the uncertainty estimates produced by the model are reliable and accurate.
</details></li>
</ul>
<hr>
<h2 id="UniTabE-Pretraining-a-Unified-Tabular-Encoder-for-Heterogeneous-Tabular-Data"><a href="#UniTabE-Pretraining-a-Unified-Tabular-Encoder-for-Heterogeneous-Tabular-Data" class="headerlink" title="UniTabE: Pretraining a Unified Tabular Encoder for Heterogeneous Tabular Data"></a>UniTabE: Pretraining a Unified Tabular Encoder for Heterogeneous Tabular Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09249">http://arxiv.org/abs/2307.09249</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yazheng Yang, Yuqi Wang, Guang Liu, Ledell Wu, Qi Liu</li>
<li>for: 本研究旨在推广自然语言处理（NLP）中的预训练方法，应用于表格数据，以提高表格数据分析的Semantic Representation。</li>
<li>methods: 本研究使用了UniTabE方法，该方法基于表格元素模块（TabUnit）和Transformer编码器，可以适应不同的表格结构。此外，模型还支持预训练和Finetuning，通过自由形式的提示。</li>
<li>results: 实验结果显示，UniTabE方法在多个benchmark dataset上表现出色，超过了多个基eline模型。这说明UniTabE方法可以有效地提高表格数据的Semantic Representation，为表格数据分析带来 significiant progress.<details>
<summary>Abstract</summary>
Recent advancements in Natural Language Processing (NLP) have witnessed the groundbreaking impact of pretrained models, yielding impressive outcomes across various tasks. This study seeks to extend the power of pretraining methodologies to tabular data, a domain traditionally overlooked, yet inherently challenging due to the plethora of table schemas intrinsic to different tasks. The primary research questions underpinning this work revolve around the adaptation to heterogeneous table structures, the establishment of a universal pretraining protocol for tabular data, the generalizability and transferability of learned knowledge across tasks, the adaptation to diverse downstream applications, and the incorporation of incremental columns over time. In response to these challenges, we introduce UniTabE, a pioneering method designed to process tables in a uniform manner, devoid of constraints imposed by specific table structures. UniTabE's core concept relies on representing each basic table element with a module, termed TabUnit. This is subsequently followed by a Transformer encoder to refine the representation. Moreover, our model is designed to facilitate pretraining and finetuning through the utilization of free-form prompts. In order to implement the pretraining phase, we curated an expansive tabular dataset comprising approximately 13 billion samples, meticulously gathered from the Kaggle platform. Rigorous experimental testing and analyses were performed under a myriad of scenarios to validate the effectiveness of our methodology. The experimental results demonstrate UniTabE's superior performance against several baseline models across a multitude of benchmark datasets. This, therefore, underscores UniTabE's potential to significantly enhance the semantic representation of tabular data, thereby marking a significant stride in the field of tabular data analysis.
</details>
<details>
<summary>摘要</summary>
近年的自然语言处理（NLP）技术发展，启示出革命性的影响，在多种任务上取得了卓越的成绩。这项研究旨在扩展预训练方法的应用范围，推广到表格数据领域，这是传统上受过忽略的领域，但具有各种表格结构的强大挑战。本研究的主要问题包括适应不同表格结构、建立通用预训练协议、学习知识的通用性和跨任务传递性、适应多种下游应用、以及逐渐增加的列的支持。为解决这些挑战，我们提出了UniTabE方法，用于统一处理表格数据，不受特定表格结构的限制。UniTabE的核心思想是将每个基本表格元素表示为Module，称为TabUnit，然后使用Transformer编码器进行细化表示。此外，我们的模型设计能够方便预训练和finetuning，通过使用自由形式的提示。为进行预训练阶段，我们精心收集了约130亿个样本的大量表格数据，从Kaggle平台上精心收集。通过多种情况下的严格实验和分析，我们证明UniTabE方法在多个benchmark数据集上的表现优于多个基eline模型。这一结果 therefore表明UniTabE具有提高表格数据semantic表示的潜在能力，从而为表格数据分析带来重要的进步。
</details></li>
</ul>
<hr>
<h2 id="Application-of-BERT-in-Wind-Power-Forecasting-Teletraan’s-Solution-in-Baidu-KDD-Cup-2022"><a href="#Application-of-BERT-in-Wind-Power-Forecasting-Teletraan’s-Solution-in-Baidu-KDD-Cup-2022" class="headerlink" title="Application of BERT in Wind Power Forecasting-Teletraan’s Solution in Baidu KDD Cup 2022"></a>Application of BERT in Wind Power Forecasting-Teletraan’s Solution in Baidu KDD Cup 2022</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09248">http://arxiv.org/abs/2307.09248</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/longxingtan/kdd2022-baidu">https://github.com/longxingtan/kdd2022-baidu</a></li>
<li>paper_authors: Longxing Tan, Hongying Yue</li>
<li>for: 预测风力电力系统的可靠性和可持续发展</li>
<li>methods: 使用BERT模型和日均异常值补做来预测风力电力系统的输出</li>
<li>results: 在Baidu KDD Cup 2022中获得第三名，代表着模型的可靠性和精度Here’s a more detailed explanation of each point:</li>
<li>for: The paper is written for the purpose of improving the reliability and sustainability of wind power systems by using a BERT model and daily fluctuation post-processing to make accurate predictions.</li>
<li>methods: The paper uses the BERT model, which is a type of deep learning model that has shown great success in natural language processing tasks, to predict the output of wind power systems. Additionally, the authors add daily fluctuation to the predicted results through post-processing to make the predictions more accurate and in line with daily periodicity.</li>
<li>results: The authors achieved third place out of 2490 teams in the Baidu KDD Cup 2022, which demonstrates the effectiveness and accuracy of their proposed method.<details>
<summary>Abstract</summary>
Nowadays, wind energy has drawn increasing attention as its important role in carbon neutrality and sustainable development. When wind power is integrated into the power grid, precise forecasting is necessary for the sustainability and security of the system. However, the unpredictable nature and long sequence prediction make it especially challenging. In this technical report, we introduce the BERT model applied for Baidu KDD Cup 2022, and the daily fluctuation is added by post-processing to make the predicted results in line with daily periodicity. Our solution achieves 3rd place of 2490 teams. The code is released athttps://github.com/LongxingTan/KDD2022-Baidu
</details>
<details>
<summary>摘要</summary>
现在，风能资源已经吸引了越来越多的关注，因为它在碳中和可持续发展中扮演着重要的角色。当风力发电机与电力网络集成时，准确预测成为了系统可持续性和安全性的重要因素。然而，风力预测具有不可预测性和长时间序列预测的特点，使得预测变得特别困难。在这份技术报告中，我们介绍了BERT模型在Baidu KDD杯2022中的应用，并通过后处理来添加日律性，使预测结果与日律性保持一致。我们的解决方案在2490个团队中获得第三名，代码在https://github.com/LongxingTan/KDD2022-Baidu上发布。
</details></li>
</ul>
<hr>
<h2 id="Towards-Sustainable-Deep-Learning-for-Multi-Label-Classification-on-NILM"><a href="#Towards-Sustainable-Deep-Learning-for-Multi-Label-Classification-on-NILM" class="headerlink" title="Towards Sustainable Deep Learning for Multi-Label Classification on NILM"></a>Towards Sustainable Deep Learning for Multi-Label Classification on NILM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09244">http://arxiv.org/abs/2307.09244</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anže Pirnat, Blaž Bertalanič, Gregor Cerar, Mihael Mohorčič, Carolina Fortuna</li>
<li>For: The paper is written for the purpose of improving the computation and energy efficiency of deep learning (DL) models for non-intrusive load monitoring (NILM) classification.* Methods: The paper proposes a novel DL model for enhanced multi-label classification of NILM, which is designed to reduce computational and energy demands during training and operation.* Results: The proposed model achieves on average approximately 8 percentage points in performance improvement compared to the state-of-the-art, while reducing the carbon footprint by more than 23%.Here’s the Chinese translation of the three key information points:* For: 本文是为了提高深度学习（DL）模型的计算和能源效率，用于非侵入式电力监测（NILM）分类。* Methods: 本文提出了一种新的DL模型，用于改进NILM多标签分类，以降低训练和运行中的计算和能源需求。* Results: 提议的模型与状态艺术比较，在REFIT和UK-DALE数据集上测试时， average提高了约8%的性能，同时减少了碳脚印的23%以上。<details>
<summary>Abstract</summary>
Non-intrusive load monitoring (NILM) is the process of obtaining appliance-level data from a single metering point, measuring total electricity consumption of a household or a business. Appliance-level data can be directly used for demand response applications and energy management systems as well as for awareness raising and motivation for improvements in energy efficiency and reduction in the carbon footprint. Recently, classical machine learning and deep learning (DL) techniques became very popular and proved as highly effective for NILM classification, but with the growing complexity these methods are faced with significant computational and energy demands during both their training and operation. In this paper, we introduce a novel DL model aimed at enhanced multi-label classification of NILM with improved computation and energy efficiency. We also propose a testing methodology for comparison of different models using data synthesized from the measurement datasets so as to better represent real-world scenarios. Compared to the state-of-the-art, the proposed model has its carbon footprint reduced by more than 23% while providing on average approximately 8 percentage points in performance improvement when testing on data derived from REFIT and UK-DALE datasets.
</details>
<details>
<summary>摘要</summary>
非侵入式电力监测（NILM）是指从单个计量点获取家用电器或商业用电器的具体数据，计算总电力消耗量。家用电器或商业用电器的具体数据可以直接用于需求应答应用和能源管理系统，以及提高能源效率和减少碳足迹。在最近的几年中，传统的机器学习和深度学习（DL）技术在NILM类型分类中变得非常流行，但是随着模型的复杂度的增加，它们面临着显著的计算和能源投入问题。在本文中，我们介绍了一种新的深度学习模型，旨在提高多标签分类的NILM性能。我们还提出了一种测试方法ологи，用于对不同模型进行比较，以更好地 simulate real-world scenarios。相比之前的状态艺术，我们的提案模型可以减少碳脚印的23%以上，并在REFIT和UK-DALE数据集上测试平均提高8个百分点的性能。
</details></li>
</ul>
<hr>
<h2 id="Fusing-Hand-and-Body-Skeletons-for-Human-Action-Recognition-in-Assembly"><a href="#Fusing-Hand-and-Body-Skeletons-for-Human-Action-Recognition-in-Assembly" class="headerlink" title="Fusing Hand and Body Skeletons for Human Action Recognition in Assembly"></a>Fusing Hand and Body Skeletons for Human Action Recognition in Assembly</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09238">http://arxiv.org/abs/2307.09238</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dustin Aganian, Mona Köhler, Benedict Stephan, Markus Eisenbach, Horst-Michael Gross</li>
<li>for: 这篇论文主要是为了提高人机合作的效果，使用机器人在制造过程中协助人类完成Assembly任务。</li>
<li>methods: 该方法使用较为简单的人体骨架，并与高级别的手套骨架结合，使用CNN和转换器来提高人体动作识别率。</li>
<li>results: 该方法在Assembly场景中的人体动作识别率得到了提高，可以帮助机器人更好地协助人类完成Assembly任务。<details>
<summary>Abstract</summary>
As collaborative robots (cobots) continue to gain popularity in industrial manufacturing, effective human-robot collaboration becomes crucial. Cobots should be able to recognize human actions to assist with assembly tasks and act autonomously. To achieve this, skeleton-based approaches are often used due to their ability to generalize across various people and environments. Although body skeleton approaches are widely used for action recognition, they may not be accurate enough for assembly actions where the worker's fingers and hands play a significant role. To address this limitation, we propose a method in which less detailed body skeletons are combined with highly detailed hand skeletons. We investigate CNNs and transformers, the latter of which are particularly adept at extracting and combining important information from both skeleton types using attention. This paper demonstrates the effectiveness of our proposed approach in enhancing action recognition in assembly scenarios.
</details>
<details>
<summary>摘要</summary>
随着协同机器人（COBOT）在工业生产领域的普及，人机合作的效果变得越来越重要。COBOT应该能够认识人类动作，协助Assembly任务，并且可以自动行动。为达到这个目标，skeleton-based方法经常用于人机合作，因为它们可以在不同的人和环境中广泛普适。虽然body skeleton方法广泛用于动作识别，但它们可能无法准确地识别Assembly动作，这是因为工人的手指和手臂在这些动作中扮演着重要的角色。为解决这个限制，我们提议一种方法，即将较为简单的body skeleton与高级细节的手skeleton结合在一起。我们 investigate CNNs和transformers，后者尤其适合从skeleton类型中提取和组合重要信息，使用注意力。本文证明我们提议的方法可以在Assembly场景中提高动作识别的效果。
</details></li>
</ul>
<hr>
<h2 id="Detecting-Throat-Cancer-from-Speech-Signals-Using-Machine-Learning-A-Reproducible-Literature-Review"><a href="#Detecting-Throat-Cancer-from-Speech-Signals-Using-Machine-Learning-A-Reproducible-Literature-Review" class="headerlink" title="Detecting Throat Cancer from Speech Signals Using Machine Learning: A Reproducible Literature Review"></a>Detecting Throat Cancer from Speech Signals Using Machine Learning: A Reproducible Literature Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09230">http://arxiv.org/abs/2307.09230</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mary Paterson, James Moor, Luisa Cutillo</li>
<li>for: 这个研究是对现有文献中的嗓腔癌检测使用机器学习和人工智能的论文进行探讨。</li>
<li>methods: 这些论文使用的方法包括神经网络，并且大多数使用神经网络进行实现。Audio中的多种特征被提取，mel-frequency cepstral coefficients最常用。</li>
<li>results: 我们使用转移学习在多类问题上进行分类，并实现了53.54%的涂抹率，83.14%的敏感率和64.00%的特征率。我们的分类器与同一个数据集上的结果相似。<details>
<summary>Abstract</summary>
In this work we perform a scoping review of the current literature on the detection of throat cancer from speech recordings using machine learning and artificial intelligence. We find 22 papers within this area and discuss their methods and results. We split these papers into two groups - nine performing binary classification, and 13 performing multi-class classification. The papers present a range of methods with neural networks being most commonly implemented. Many features are also extracted from the audio before classification, with the most common bring mel-frequency cepstral coefficients. None of the papers found in this search have associated code repositories and as such are not reproducible. Therefore, we create a publicly available code repository of our own classifiers. We use transfer learning on a multi-class problem, classifying three pathologies and healthy controls. Using this technique we achieve an unweighted average recall of 53.54%, sensitivity of 83.14%, and specificity of 64.00%. We compare our classifiers with the results obtained on the same dataset and find similar results.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们进行了评估当前文献中用于喉部癌诊断从语音记录中使用机器学习和人工智能的研究。我们找到了22篇相关文献，并讨论了它们的方法和结果。我们将这些文献分为了两组：9篇为二分类，13篇为多类分类。文献中最常用的方法是神经网络。大多数文献在语音分类之前将各种特征提取出来，最常用的是MEL-frequency cepstral coefficients。我们发现所有搜索到的文献都没有关联代码库，因此无法重现。因此，我们创建了一个公共可用的代码库。我们使用传输学习解决多类问题，分类三种疾病和健康控制。使用这种技术，我们获得了无权重平均回归率53.54%，感知率83.14%和特征率64.00%。我们比较了我们的分类器与同一个数据集上的结果，发现结果类似。
</details></li>
</ul>
<hr>
<h2 id="How-Many-Neurons-Does-it-Take-to-Approximate-the-Maximum"><a href="#How-Many-Neurons-Does-it-Take-to-Approximate-the-Maximum" class="headerlink" title="How Many Neurons Does it Take to Approximate the Maximum?"></a>How Many Neurons Does it Take to Approximate the Maximum?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09212">http://arxiv.org/abs/2307.09212</a></li>
<li>repo_url: None</li>
<li>paper_authors: Itay Safran, Daniel Reichman, Paul Valiant</li>
<li>for: 本研究旨在探讨一个神经网络可以近似最大函数的大小，在最基本的近似情况下，即使用$L_2$范数，对于连续分布，并使用ReLU激活函数。</li>
<li>methods: 我们提供了新的下界和上界，以评估不同深度的神经网络所需的宽度，以及一个depth $\mathcal{O}(\log(\log(d)))$和宽度 $\mathcal{O}(d)$的建构，可以高效地近似最大函数。</li>
<li>results: 我们的结果显示，在depth 2和depth 3网络之间存在新的深度分离，以及depth 3和depth 5网络之间的深度分离。此外，我们还提供了一个depth $\mathcal{O}(\log(\log(d)))$和宽度 $\mathcal{O}(d)$的建构，可以高效地近似最大函数，远远超过了最好已知的深度 bound。<details>
<summary>Abstract</summary>
We study the size of a neural network needed to approximate the maximum function over $d$ inputs, in the most basic setting of approximating with respect to the $L_2$ norm, for continuous distributions, for a network that uses ReLU activations. We provide new lower and upper bounds on the width required for approximation across various depths. Our results establish new depth separations between depth 2 and 3, and depth 3 and 5 networks, as well as providing a depth $\mathcal{O}(\log(\log(d)))$ and width $\mathcal{O}(d)$ construction which approximates the maximum function, significantly improving upon the depth requirements of the best previously known bounds for networks with linearly-bounded width. Our depth separation results are facilitated by a new lower bound for depth 2 networks approximating the maximum function over the uniform distribution, assuming an exponential upper bound on the size of the weights. Furthermore, we are able to use this depth 2 lower bound to provide tight bounds on the number of neurons needed to approximate the maximum by a depth 3 network. Our lower bounds are of potentially broad interest as they apply to the widely studied and used \emph{max} function, in contrast to many previous results that base their bounds on specially constructed or pathological functions and distributions.
</details>
<details>
<summary>摘要</summary>
我们研究了一个神经网络需要来近似最大函数的大小，在最基本的设定下，即使用$L_2$ нор的近似，并且考虑到连续分布下的情况。我们提供了新的下界和上界，以及对不同深度的数据分布。我们的结果建立了新的深度分隔，包括深度2和3之间的分隔，以及深度3和5之间的分隔。此外，我们还提供了一个depth $\mathcal{O}(\log(\log(d)))$和宽度 $\mathcal{O}(d)$ 的建构，可以将最大函数近似，与之前最好的下界对于线性宽度的网络具有重要的改进。我们的深度分隔结果受到一个新的深度2网络近似最大函数的下界，假设Weight的大小是线性的。此外，我们还可以使用这个深度2下界，提供了对深度3网络近似最大函数的精确的 neuron 数量。我们的下界具有广泛的应用可能性，因为它们应用到了广泛研究和使用的\emph{max}函数，不同于许多先前的结果，它们基于特殊 constructed 或是 Pathological 函数和分布。
</details></li>
</ul>
<hr>
<h2 id="Automated-Ableism-An-Exploration-of-Explicit-Disability-Biases-in-Sentiment-and-Toxicity-Analysis-Models"><a href="#Automated-Ableism-An-Exploration-of-Explicit-Disability-Biases-in-Sentiment-and-Toxicity-Analysis-Models" class="headerlink" title="Automated Ableism: An Exploration of Explicit Disability Biases in Sentiment and Toxicity Analysis Models"></a>Automated Ableism: An Exploration of Explicit Disability Biases in Sentiment and Toxicity Analysis Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09209">http://arxiv.org/abs/2307.09209</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pranav Narayanan Venkit, Mukund Srinath, Shomir Wilson</li>
<li>for: 本研究旨在探讨 sentiment analysis 和攻击干预模型在探测人际障碍 (PWD) 的表现时是否存在显著的偏见。</li>
<li>methods: 我们使用了 Perturbation Sensitivity Analysis 检测探测 Twitter 和 Reddit 社交媒体平台上关于 PWD 的对话，以获得实际社交场景中如何传播残障偏见的信息。然后，我们创建了 \textit{Bias Identification Test in Sentiment} (BITS)  корпуス，以量化任何 sentiment analysis 和攻击干预模型中的直接残障偏见。</li>
<li>results: 我们的研究发现，这些open AIaaS sentiment analysis 工具（包括 TextBlob、VADER、Google Cloud Natural Language API 和 DistilBERT）以及两个攻击干预模型（包括 two versions of Toxic-BERT）都存在显著的直接残障偏见。<details>
<summary>Abstract</summary>
We analyze sentiment analysis and toxicity detection models to detect the presence of explicit bias against people with disability (PWD). We employ the bias identification framework of Perturbation Sensitivity Analysis to examine conversations related to PWD on social media platforms, specifically Twitter and Reddit, in order to gain insight into how disability bias is disseminated in real-world social settings. We then create the \textit{Bias Identification Test in Sentiment} (BITS) corpus to quantify explicit disability bias in any sentiment analysis and toxicity detection models. Our study utilizes BITS to uncover significant biases in four open AIaaS (AI as a Service) sentiment analysis tools, namely TextBlob, VADER, Google Cloud Natural Language API, DistilBERT and two toxicity detection models, namely two versions of Toxic-BERT. Our findings indicate that all of these models exhibit statistically significant explicit bias against PWD.
</details>
<details>
<summary>摘要</summary>
我们对偏见检测和负面情绪检测模型进行分析，以检测对人障（PWD）的直接偏见。我们使用扰动敏感性分析框架来分析社交媒体平台上关于PWD的对话，以获得实际社会中对障碍偏见的准确情况。然后，我们创建了《偏见标准测试集》（BITS）来衡量任何情感分析和负面情绪检测模型中的直接障碍偏见。我们的研究使用BITS来揭示四个开放的 AIaaS（人工智能 как服务）情感分析工具——TextBlob、VADER、Google Cloud Natural Language API和DistilBERT——以及两个负面情绪检测模型——两个版本的 Toxic-BERT——中的明显偏见。我们的发现表明，这些模型都存在 statistically significant的直接障碍偏见。
</details></li>
</ul>
<hr>
<h2 id="Context-Conditional-Navigation-with-a-Learning-Based-Terrain-and-Robot-Aware-Dynamics-Model"><a href="#Context-Conditional-Navigation-with-a-Learning-Based-Terrain-and-Robot-Aware-Dynamics-Model" class="headerlink" title="Context-Conditional Navigation with a Learning-Based Terrain- and Robot-Aware Dynamics Model"></a>Context-Conditional Navigation with a Learning-Based Terrain- and Robot-Aware Dynamics Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09206">http://arxiv.org/abs/2307.09206</a></li>
<li>repo_url: None</li>
<li>paper_authors: Suresh Guttikonda, Jan Achterhold, Haolong Li, Joschka Boedecker, Joerg Stueckler</li>
<li>For: 本研究目的是开发一种能够适应不同环境和机器人属性变化的自主导航方法。* Methods: 本研究使用了基于神经过程的meta-学前向动力学模型，以适应不同地形和机器人动力学变化。* Results: 实验表明，提出的模型在长期轨迹预测任务中的预测误差较低，而且在自主规划控制任务中也能够更好地规划控制路径。<details>
<summary>Abstract</summary>
In autonomous navigation settings, several quantities can be subject to variations. Terrain properties such as friction coefficients may vary over time depending on the location of the robot. Also, the dynamics of the robot may change due to, e.g., different payloads, changing the system's mass, or wear and tear, changing actuator gains or joint friction. An autonomous agent should thus be able to adapt to such variations. In this paper, we develop a novel probabilistic, terrain- and robot-aware forward dynamics model, termed TRADYN, which is able to adapt to the above-mentioned variations. It builds on recent advances in meta-learning forward dynamics models based on Neural Processes. We evaluate our method in a simulated 2D navigation setting with a unicycle-like robot and different terrain layouts with spatially varying friction coefficients. In our experiments, the proposed model exhibits lower prediction error for the task of long-horizon trajectory prediction, compared to non-adaptive ablation models. We also evaluate our model on the downstream task of navigation planning, which demonstrates improved performance in planning control-efficient paths by taking robot and terrain properties into account.
</details>
<details>
<summary>摘要</summary>
在自主导航设置下，许多量值可能会受到变化。地形特性，如摩擦系数，随时间的变化可能会影响机器人的位置。此外，机器人的动力学也可能会发生变化，例如不同的负荷、系统质量的变化、 actuator  gain 或 JOINT 摩擦的变化。因此，一个自主智能体应该能够适应这些变化。在这篇论文中，我们开发了一种新的probabilistic，地形和机器人意识的前瞻动力学模型，称为 TRADYN，它能够适应以上所 mention 的变化。它基于最近的前瞻动力学模型基于神经过程的meta-学进步。我们在一个模拟的2D导航设置中使用了一种unicycle-like 机器人和不同的地形布局，并对其进行了评估。在我们的实验中，提议的模型在长期轨迹预测任务中表现出较低的预测错误，相比非适应模型。此外，我们还评估了我们的模型在导航规划任务中的表现，其表现出了改进的控制效率的规划路径。
</details></li>
</ul>
<hr>
<h2 id="Learning-Dynamic-Attribute-factored-World-Models-for-Efficient-Multi-object-Reinforcement-Learning"><a href="#Learning-Dynamic-Attribute-factored-World-Models-for-Efficient-Multi-object-Reinforcement-Learning" class="headerlink" title="Learning Dynamic Attribute-factored World Models for Efficient Multi-object Reinforcement Learning"></a>Learning Dynamic Attribute-factored World Models for Efficient Multi-object Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09205">http://arxiv.org/abs/2307.09205</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fan Feng, Sara Magliacane</li>
<li>for: 这个论文的目的是提高强化学习任务中agent的扩展性和可重复性，使其能够在不同的物体和属性下进行学习和执行任务。</li>
<li>methods: 这个论文使用了对象中心表示学习来提取视觉输入中的物体，并将其分类为不同的类别。然后，对每个类别的物体，学习一个类模板图，描述了这种物体的动力和奖励如何因属性分解。还学习了对象之间的互动模式图，描述了不同类别的物体之间的互动。通过这些图和动态互动图，学习出一个策略，可以在新环境中直接应用。</li>
<li>results: 在三个标准 dataset上测试了这个框架，并证明了它在未seen的物体、属性和潜在参数下进行扩展和可重复性的任务时表现出色，以及在组合已知任务时的表现也是比较好的。<details>
<summary>Abstract</summary>
In many reinforcement learning tasks, the agent has to learn to interact with many objects of different types and generalize to unseen combinations and numbers of objects. Often a task is a composition of previously learned tasks (e.g. block stacking). These are examples of compositional generalization, in which we compose object-centric representations to solve complex tasks. Recent works have shown the benefits of object-factored representations and hierarchical abstractions for improving sample efficiency in these settings. On the other hand, these methods do not fully exploit the benefits of factorization in terms of object attributes. In this paper, we address this opportunity and introduce the Dynamic Attribute FacTored RL (DAFT-RL) framework. In DAFT-RL, we leverage object-centric representation learning to extract objects from visual inputs. We learn to classify them in classes and infer their latent parameters. For each class of object, we learn a class template graph that describes how the dynamics and reward of an object of this class factorize according to its attributes. We also learn an interaction pattern graph that describes how objects of different classes interact with each other at the attribute level. Through these graphs and a dynamic interaction graph that models the interactions between objects, we can learn a policy that can then be directly applied in a new environment by just estimating the interactions and latent parameters. We evaluate DAFT-RL in three benchmark datasets and show our framework outperforms the state-of-the-art in generalizing across unseen objects with varying attributes and latent parameters, as well as in the composition of previously learned tasks.
</details>
<details>
<summary>摘要</summary>
在许多强化学习任务中，机器人需要学习与多种不同类型的物体交互，并泛化到未经见过的组合和数量。经常情况下，任务是一个各种已经学习过的任务的组合（例如堆叠块）。这些任务是物体中心的泛化，在这些情况下，我们可以通过物体中心的表示学习来解决复杂任务。在这篇论文中，我们提出了动态特征划分RL（DAFT-RL）框架。在DAFT-RL中，我们利用物体中心的表示学习来提取视觉输入中的物体。我们可以将它们分类为类别，并且从属特征参数的推断。对于每个类型的物体，我们学习一个类型图，该图描述了物体的动力学和奖励因为其特征的分解。我们还学习了不同类型物体之间的交互图，该图描述了物体之间的特征级别交互。通过这些图和动态交互图，我们可以学习一个策略，该策略可以在新环境中直接应用，只需要估计交互和隐藏参数。我们在三个标准数据集上进行了评估，并证明了我们的框架在未经见过的物体特征和隐藏参数的泛化，以及在组合已经学习过的任务中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-for-Computationally-Constrained-Heterogeneous-Devices-A-Survey"><a href="#Federated-Learning-for-Computationally-Constrained-Heterogeneous-Devices-A-Survey" class="headerlink" title="Federated Learning for Computationally-Constrained Heterogeneous Devices: A Survey"></a>Federated Learning for Computationally-Constrained Heterogeneous Devices: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09182">http://arxiv.org/abs/2307.09182</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kilian Pfeiffer, Martin Rapp, Ramin Khalili, Jörg Henkel</li>
<li>for: 提高用户隐私和减少中心服务器的负担，实现在设备上进行神经网络训练。</li>
<li>methods: 联邦学习（Federated Learning）技术，通过共享设备之间的知识，保持用户隐私，同时提高模型精度。</li>
<li>results: 在具有多种设备的不同硬件和软件环境中，联邦学习技术面临着多种差异和挑战，需要采取多种策略来减少这些差异，以提高模型精度和可靠性。<details>
<summary>Abstract</summary>
With an increasing number of smart devices like internet of things (IoT) devices deployed in the field, offloadingtraining of neural networks (NNs) to a central server becomes more and more infeasible. Recent efforts toimprove users' privacy have led to on-device learning emerging as an alternative. However, a model trainedonly on a single device, using only local data, is unlikely to reach a high accuracy. Federated learning (FL)has been introduced as a solution, offering a privacy-preserving trade-off between communication overheadand model accuracy by sharing knowledge between devices but disclosing the devices' private data. Theapplicability and the benefit of applying baseline FL are, however, limited in many relevant use cases dueto the heterogeneity present in such environments. In this survey, we outline the heterogeneity challengesFL has to overcome to be widely applicable in real-world applications. We especially focus on the aspect ofcomputation heterogeneity among the participating devices and provide a comprehensive overview of recentworks on heterogeneity-aware FL. We discuss two groups: works that adapt the NN architecture and worksthat approach heterogeneity on a system level, covering Federated Averaging (FedAvg), distillation, and splitlearning-based approaches, as well as synchronous and asynchronous aggregation schemes.
</details>
<details>
<summary>摘要</summary>
随着智能设备的数量不断增加，如物联网（IoT）设备，将神经网络（NN）的训练卷积到中央服务器上成为了不可能的。随后，为了保护用户隐私，在设备上进行学习（On-Device Learning）已经成为了一个可行的选择。然而，基于单个设备和本地数据进行训练的模型很难达到高精度。为了解决这个问题，联邦学习（Federated Learning，FL）已经被提出，它可以在保护设备私钥数据的同时，通过共享设备之间的知识，实现私钥数据不泄露的高精度模型训练。然而，FL在许多实际应用场景中的可应用性和优势受到了多种多样性的限制。在这篇简述中，我们描述了FL面临的多样性挑战，特别是设备计算能力的多样性，并提供了一个全面的最新研究综述。我们将分为两组：一组是改进神经网络架构的方法，另一组是在系统层面进行多样性处理的方法，包括联邦平均（FedAvg）、液化、分布式学习等方法，以及同步和异步聚合方案。
</details></li>
</ul>
<hr>
<h2 id="ECSIC-Epipolar-Cross-Attention-for-Stereo-Image-Compression"><a href="#ECSIC-Epipolar-Cross-Attention-for-Stereo-Image-Compression" class="headerlink" title="ECSIC: Epipolar Cross Attention for Stereo Image Compression"></a>ECSIC: Epipolar Cross Attention for Stereo Image Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10284">http://arxiv.org/abs/2307.10284</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthias Wödlinger, Jan Kotera, Manuel Keglevic, Jan Xu, Robert Sablatnig</li>
<li>for: 这个论文是为了提出一种新的学习基于方法，用于压缩立体图像。</li>
<li>methods: 该方法利用了两个 Stero Context 模块和一个 Stero Cross Attention（SCA）模块来同时压缩左右图像。SCA模块在相对应的epipolar线上进行了交叉注意力，并在平行进行处理。</li>
<li>results: 对比其他方法，ECSIC 在 Cityscapes 和 InStereo2k 两个 популяр的立体图像数据集上达到了最佳性能，同时允许快速编码和解码，非常适合实时应用。<details>
<summary>Abstract</summary>
In this paper, we present ECSIC, a novel learned method for stereo image compression. Our proposed method compresses the left and right images in a joint manner by exploiting the mutual information between the images of the stereo image pair using a novel stereo cross attention (SCA) module and two stereo context modules. The SCA module performs cross-attention restricted to the corresponding epipolar lines of the two images and processes them in parallel. The stereo context modules improve the entropy estimation of the second encoded image by using the first image as a context. We conduct an extensive ablation study demonstrating the effectiveness of the proposed modules and a comprehensive quantitative and qualitative comparison with existing methods. ECSIC achieves state-of-the-art performance among stereo image compression models on the two popular stereo image datasets Cityscapes and InStereo2k while allowing for fast encoding and decoding, making it highly practical for real-time applications.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的学习方法 для顺帧图像压缩，称为ECSIC。我们的提议方法将左右图像压缩在一起，通过利用顺帧图像对的相互信息来进行压缩。我们使用了一种新的顺帧相关注意力（SCA）模块和两个顺帧上下文模块来实现这一目标。SCA模块在相应的轴线上进行交叉注意力限制，并在平行进行处理。两个顺帧上下文模块使得第二个编码图像的Entropy估计得到改善，通过使用第一个图像作为 Context。我们进行了广泛的缺省研究，并对现有方法进行了全面的量化和质量比较。ECSIC在Cityscapes和InStereo2k两个流行的顺帧图像数据集上实现了顺帧图像压缩模型的状态机器，同时具有快速编码和解码功能，因此在实时应用中非常实用。
</details></li>
</ul>
<hr>
<h2 id="Towards-Trustworthy-Dataset-Distillation"><a href="#Towards-Trustworthy-Dataset-Distillation" class="headerlink" title="Towards Trustworthy Dataset Distillation"></a>Towards Trustworthy Dataset Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09165">http://arxiv.org/abs/2307.09165</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shijie Ma, Fei Zhu, Zhen Cheng, Xu-Yao Zhang<br>for: Trustworthy Dataset Distillation (TrustDD) aims to reduce training costs and enhance the trustworthiness of deep learning models in real-world applications by distilling both in-distribution (InD) samples and outliers.methods: The proposed method utilizes dataset distillation (DD) to condenses large datasets into tiny synthetic datasets, and introduces Pseudo-Outlier Exposure (POE) to generate pseudo-outliers and enhance OOD detection.results: Comprehensive experiments on various settings demonstrate the effectiveness of TrustDD, and the proposed POE surpasses state-of-the-art method Outlier Exposure (OE). TrustDD is more trustworthy and applicable to real open-world scenarios compared to preceding DD methods.Here’s the simplified Chinese version:for: TrustDD 目的是提高深度学习模型在实际应用中的可靠性和训练效率，通过将大量数据筛选到简单的Synthetic dataset中。methods: TrustDD 使用 dataset distillation (DD) 将大量数据筛选到简单的 Synthetic dataset 中，并引入 Pseudo-Outlier Exposure (POE) 生成 Pseudo-outlier 并提高 OOD 检测。results: 各种设置的 comprehensive 实验表明 TrustDD 的有效性，并且提出的 POE 超越了state-of-the-art 方法 Outlier Exposure (OE)。TrustDD 比前一代 DD 方法更可靠和适用于实际开放世界应用场景。<details>
<summary>Abstract</summary>
Efficiency and trustworthiness are two eternal pursuits when applying deep learning in real-world applications. With regard to efficiency, dataset distillation (DD) endeavors to reduce training costs by distilling the large dataset into a tiny synthetic dataset. However, existing methods merely concentrate on in-distribution (InD) classification in a closed-world setting, disregarding out-of-distribution (OOD) samples. On the other hand, OOD detection aims to enhance models' trustworthiness, which is always inefficiently achieved in full-data settings. For the first time, we simultaneously consider both issues and propose a novel paradigm called Trustworthy Dataset Distillation (TrustDD). By distilling both InD samples and outliers, the condensed datasets are capable to train models competent in both InD classification and OOD detection. To alleviate the requirement of real outlier data and make OOD detection more practical, we further propose to corrupt InD samples to generate pseudo-outliers and introduce Pseudo-Outlier Exposure (POE). Comprehensive experiments on various settings demonstrate the effectiveness of TrustDD, and the proposed POE surpasses state-of-the-art method Outlier Exposure (OE). Compared with the preceding DD, TrustDD is more trustworthy and applicable to real open-world scenarios. Our code will be publicly available.
</details>
<details>
<summary>摘要</summary>
“效率和可靠性是深度学习应用实际场景中的两大永恒追求。在这个领域，数据集缩写（DD）尝试通过缩写大数据集为一个小型的合成数据集来减少训练成本。然而，现有方法仅关注在关闭世界设定下的内部分布（InD）类别，忽略了外部分布（OOD）样本。然而，OOD检测的目的是增强模型的可靠性，这通常在全数据设定下是不fficient的。为了解决这些问题，我们同时考虑了这两个问题，并提出了一种新的思路called Trustworthy Dataset Distillation（TrustDD）。通过缩写InD样本和异常样本，缩写后的数据集可以训练能够在InD类别和OOD检测中具备竞争力。为了避免实际异常数据的需求和使OOD检测更实用，我们进一步提出了 Pseudo-Outlier Exposure（POE）。我们对不同的设定进行了广泛的实验，并证明了 TrustDD 的有效性，而我们提出的 POE 超过了现有的 Outlier Exposure（OE）方法。相比之下，TrustDD 更加可靠和适用于真实的开放世界场景。我们的代码将在公共可用。”
</details></li>
</ul>
<hr>
<h2 id="MVA2023-Small-Object-Detection-Challenge-for-Spotting-Birds-Dataset-Methods-and-Results"><a href="#MVA2023-Small-Object-Detection-Challenge-for-Spotting-Birds-Dataset-Methods-and-Results" class="headerlink" title="MVA2023 Small Object Detection Challenge for Spotting Birds: Dataset, Methods, and Results"></a>MVA2023 Small Object Detection Challenge for Spotting Birds: Dataset, Methods, and Results</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09143">http://arxiv.org/abs/2307.09143</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iim-ttij/mva2023smallobjectdetection4spottingbirds">https://github.com/iim-ttij/mva2023smallobjectdetection4spottingbirds</a></li>
<li>paper_authors: Yuki Kondo, Norimichi Ukita, Takayuki Yamaguchi, Hao-Yu Hou, Mu-Yi Shen, Chia-Chi Hsu, En-Ming Huang, Yu-Chen Huang, Yu-Cheng Xia, Chien-Yao Wang, Chun-Yi Lee, Da Huo, Marc A. Kastner, Tingwei Liu, Yasutomo Kawanishi, Takatsugu Hirayama, Takahiro Komamizu, Ichiro Ide, Yosuke Shinya, Xinyao Liu, Guang Liang, Syusuke Yasui</li>
<li>for: 本研究旨在提出一个新的小物体检测数据集，以便进行远程小物体检测的实际应用。</li>
<li>methods: 本文提出了一种新的小物体检测方法，并在223名参与者的挑战中评测了其效果。</li>
<li>results: 研究发现，使用这种新方法可以在远程小物体检测中获得优秀的效果，并且提供了一个大量的小物体检测数据集和基eline代码以便进一步研究。<details>
<summary>Abstract</summary>
Small Object Detection (SOD) is an important machine vision topic because (i) a variety of real-world applications require object detection for distant objects and (ii) SOD is a challenging task due to the noisy, blurred, and less-informative image appearances of small objects. This paper proposes a new SOD dataset consisting of 39,070 images including 137,121 bird instances, which is called the Small Object Detection for Spotting Birds (SOD4SB) dataset. The detail of the challenge with the SOD4SB dataset is introduced in this paper. In total, 223 participants joined this challenge. This paper briefly introduces the award-winning methods. The dataset, the baseline code, and the website for evaluation on the public testset are publicly available.
</details>
<details>
<summary>摘要</summary>
小物体检测（SOD）是机器视觉领域的重要话题，因为（i）许多现实世界应用需要对远距离的物体进行检测，以及（ii）SOD是一项复杂的任务，因为小物体的图像表现具有噪声、模糊和不具有很多信息。这篇论文提出了一个新的SOD数据集，包括39,070张图像和137,121只鸟类实例，称为Small Object Detection for Spotting Birds（SOD4SB）数据集。本文介绍了SOD4SB数据集的挑战。总共有223名参与者参加了这个挑战。本文 briefly introduce了获奖方法。数据集、基线代码和评估网站对公共测试集进行评估是公共可用的。
</details></li>
</ul>
<hr>
<h2 id="Characterization-of-partial-wetting-by-CMAS-droplets-using-multiphase-many-body-dissipative-particle-dynamics-and-data-driven-discovery-based-on-PINNs"><a href="#Characterization-of-partial-wetting-by-CMAS-droplets-using-multiphase-many-body-dissipative-particle-dynamics-and-data-driven-discovery-based-on-PINNs" class="headerlink" title="Characterization of partial wetting by CMAS droplets using multiphase many-body dissipative particle dynamics and data-driven discovery based on PINNs"></a>Characterization of partial wetting by CMAS droplets using multiphase many-body dissipative particle dynamics and data-driven discovery based on PINNs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09142">http://arxiv.org/abs/2307.09142</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elham Kiyani, Mahdi Kooshkbaghi, Khemraj Shukla, Rahul Babu Koneru, Zhen Li, Luis Bravo, Anindya Ghoshal, George Em Karniadakis, Mikko Karttunen</li>
<li>For: 这研究探讨了高熔率的CMAS液滴在不同初始尺寸和稳定接触角下的湿润动态。* Methods: 这研究使用多相多体积耗散动力学（mDPD）模拟，研究CMAS液滴的湿润动态。使用Physics-Informed Neural Network（PINN）框架确定湿润半径行为。使用符号回归来表示关系函数。* Results: 研究发现了CMAS液滴的湿润半径行为，并使用Bayesian PINNs（B-PINNs）评估和量化相关参数的不确定性。这研究将湿润动态模拟和机器学习技术结合，为高温应用提供了创新解决方案。<details>
<summary>Abstract</summary>
The molten sand, a mixture of calcia, magnesia, alumina, and silicate, known as CMAS, is characterized by its high viscosity, density, and surface tension. The unique properties of CMAS make it a challenging material to deal with in high-temperature applications, requiring innovative solutions and materials to prevent its buildup and damage to critical equipment. Here, we use multiphase many-body dissipative particle dynamics (mDPD) simulations to study the wetting dynamics of highly viscous molten CMAS droplets. The simulations are performed in three dimensions, with varying initial droplet sizes and equilibrium contact angles. We propose a coarse parametric ordinary differential equation (ODE) that captures the spreading radius behavior of the CMAS droplets. The ODE parameters are then identified based on the Physics-Informed Neural Network (PINN) framework. Subsequently, the closed form dependency of parameter values found by PINN on the initial radii and contact angles are given using symbolic regression. Finally, we employ Bayesian PINNs (B-PINNs) to assess and quantify the uncertainty associated with the discovered parameters. In brief, this study provides insight into spreading dynamics of CMAS droplets by fusing simple parametric ODE modeling and state-of-the-art machine learning techniques.
</details>
<details>
<summary>摘要</summary>
熔融砂粒材料（CMAS），它是含 calcium、magnesia、alumina 和 silicate 的混合物，具有高粘度、密度和表面张力。由于 CMAS 的特有性，在高温应用中处理它是一项挑战，需要创新的解决方案和材料来避免它的堆积和设备损害。在这里，我们使用多相多体积排斥凝聚 dynamics（mDPD）仿真来研究高粘度熔融 CMAS 液滴的湿润动力学。仿真在三维空间中进行，初始液滴尺寸和均衡接触角度进行变化。我们提出了一个粗略的常数参数方程（ODE），捕捉液滴的扩散半径行为。ODE 参数的标准值 Subsequently, the closed form dependency of parameter values found by PINN on the initial radii and contact angles are given using symbolic regression. Finally, we employ Bayesian PINNs (B-PINNs) to assess and quantify the uncertainty associated with the discovered parameters. In brief, this study provides insight into spreading dynamics of CMAS droplets by fusing simple parametric ODE modeling and state-of-the-art machine learning techniques.
</details></li>
</ul>
<hr>
<h2 id="Mining-of-Single-Class-by-Active-Learning-for-Semantic-Segmentation"><a href="#Mining-of-Single-Class-by-Active-Learning-for-Semantic-Segmentation" class="headerlink" title="Mining of Single-Class by Active Learning for Semantic Segmentation"></a>Mining of Single-Class by Active Learning for Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09109">http://arxiv.org/abs/2307.09109</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hugues Lambert, Emma Slade</li>
<li>for: 本研究的目的是提出一种基于深度优化学习的活动学习策略，以提高特定类型的模型训练效果。</li>
<li>methods: 本研究使用的方法是基于深度优化学习的 MiSiCAL 方法，它可以通过量精度相关性来建立高性能的模型训练集。 MiSiCAL 方法不需要重新训练目标模型多次，因此适用于大批量训练。</li>
<li>results: 研究结果表明，MiSiCAL 方法能够在 COCO10k 数据集上 OUTPERFORM 随机策略的 150 个类型，而最强的基线方法只能 OUTPERFORM 随机策略的 101 个类型。<details>
<summary>Abstract</summary>
Several Active Learning (AL) policies require retraining a target model several times in order to identify the most informative samples and rarely offer the option to focus on the acquisition of samples from underrepresented classes. Here the Mining of Single-Class by Active Learning (MiSiCAL) paradigm is introduced where an AL policy is constructed through deep reinforcement learning and exploits quantity-accuracy correlations to build datasets on which high-performance models can be trained with regards to specific classes. MiSiCAL is especially helpful in the case of very large batch sizes since it does not require repeated model training sessions as is common in other AL methods. This is thanks to its ability to exploit fixed representations of the candidate data points. We find that MiSiCAL is able to outperform a random policy on 150 out of 171 COCO10k classes, while the strongest baseline only outperforms random on 101 classes.
</details>
<details>
<summary>摘要</summary>
几种活动学习（AL）策略需要重新训练目标模型多次以确定最有用的样本并rarely提供针对少 представohn classes 的集中着注意力选择样本的选择。在这里，我们介绍了一种名为MINING SINGLE-CLASS BY ACTIVE LEARNING（MiSiCAL）的策略，通过深度强化学习构建了一个AL策略，利用量精度相关性来建立高性能模型可以在特定类上训练。MiSiCAL在大批量时 particuarily helpful，因为它不需要重复的模型训练会议。这是因为它可以利用固定表示的候选数据点。我们发现MiSiCAL可以在150个COCO10k类中超过随机策略，而最强基eline只能在101个类中超过随机策略。
</details></li>
</ul>
<hr>
<h2 id="Non-stationary-Delayed-Combinatorial-Semi-Bandit-with-Causally-Related-Rewards"><a href="#Non-stationary-Delayed-Combinatorial-Semi-Bandit-with-Causally-Related-Rewards" class="headerlink" title="Non-stationary Delayed Combinatorial Semi-Bandit with Causally Related Rewards"></a>Non-stationary Delayed Combinatorial Semi-Bandit with Causally Related Rewards</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09093">http://arxiv.org/abs/2307.09093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saeed Ghoorchian, Setareh Maghsudi</li>
<li>for: The paper is written for solving the problem of sequential decision-making under uncertainty with long feedback delays, particularly in non-stationary environments with structural dependencies amongst the reward distributions.</li>
<li>methods: The paper proposes a policy that learns the causal relations between the arms using a stationary structural equation model, and utilizes this knowledge to optimize the decision-making while adapting to drifts.</li>
<li>results: The paper proves a regret bound for the performance of the proposed algorithm, and evaluates the method via numerical analysis using synthetic and real-world datasets to detect the regions that contribute the most to the spread of Covid-19 in Italy.<details>
<summary>Abstract</summary>
Sequential decision-making under uncertainty is often associated with long feedback delays. Such delays degrade the performance of the learning agent in identifying a subset of arms with the optimal collective reward in the long run. This problem becomes significantly challenging in a non-stationary environment with structural dependencies amongst the reward distributions associated with the arms. Therefore, besides adapting to delays and environmental changes, learning the causal relations alleviates the adverse effects of feedback delay on the decision-making process. We formalize the described setting as a non-stationary and delayed combinatorial semi-bandit problem with causally related rewards. We model the causal relations by a directed graph in a stationary structural equation model. The agent maximizes the long-term average payoff, defined as a linear function of the base arms' rewards. We develop a policy that learns the structural dependencies from delayed feedback and utilizes that to optimize the decision-making while adapting to drifts. We prove a regret bound for the performance of the proposed algorithm. Besides, we evaluate our method via numerical analysis using synthetic and real-world datasets to detect the regions that contribute the most to the spread of Covid-19 in Italy.
</details>
<details>
<summary>摘要</summary>
纷纷决策下面存在长时间反馈延迟，这会导致学习代理人的表现下降，无法在长期内确定优化收益的子集。在非站点环境中，抽象依赖关系的赏金分布难以预测，这使得问题变得更加挑战。因此，除了适应延迟和环境变化之外，学习 causal 关系可以减轻延迟的负面影响。我们将此设定形式化为非站点和延迟 combinatorial 半带抽象问题，使用 causally 相关的赏金分布模型。我们的代理人通过延迟反馈学习 structural 相关性，并使用这些相关性来优化决策。我们证明了我们提出的算法的 regret  bound。此外，我们通过NumPy和实际数据进行数学分析，以便检测意大利COVID-19 的扩散区域。
</details></li>
</ul>
<hr>
<h2 id="A-Federated-learning-model-for-Electric-Energy-management-using-Blockchain-Technology"><a href="#A-Federated-learning-model-for-Electric-Energy-management-using-Blockchain-Technology" class="headerlink" title="A Federated learning model for Electric Energy management using Blockchain Technology"></a>A Federated learning model for Electric Energy management using Blockchain Technology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09080">http://arxiv.org/abs/2307.09080</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Shoaib Farooq, Azeen Ahmed Hayat</li>
<li>For: The paper aims to address energy shortfall and electricity load shedding in developing countries by improving energy management and increasing the use of renewable energy sources.* Methods: The paper proposes the use of federated learning and blockchain technology to forecast energy requirements and ensure transparency, traceability, and security in energy transactions between prosumers and consumers.* Results: The experiment results show that renewable energy sources have produced better and comparable results to other non-renewable energy resources.Here’s the simplified Chinese text for the three points:* For: 这篇论文目的是解决发展中国家的能源短缺和电力卸载问题，通过改善能源管理和使用可再生能源。* Methods: 论文提议使用联邦学习和区块链技术，对消费者和生产者之间的能源交易进行透明度、跟踪性和安全性的保障。* Results: 实验结果表明，可再生能源资源比其他非可再生能源资源更好和相当。<details>
<summary>Abstract</summary>
Energy shortfall and electricity load shedding are the main problems for developing countries. The main causes are lack of management in the energy sector and the use of non-renewable energy sources. The improved energy management and use of renewable sources can be significant to resolve energy crisis. It is necessary to increase the use of renewable energy sources (RESs) to meet the increasing energy demand due to high prices of fossil-fuel based energy. Federated learning (FL) is the most emerging technique in the field of artificial intelligence. Federated learning helps to generate global model at server side by ensemble locally trained models at remote edges sites while preserving data privacy. The global model used to predict energy demand to satisfy the needs of consumers. In this article, we have proposed Blockchain based safe distributed ledger technology for transaction of data between prosumer and consumer to ensure their transparency, traceability and security. Furthermore, we have also proposed a Federated learning model to forecast the energy requirements of consumer and prosumer. Moreover, Blockchain has been used to store excess energy data from prosumer for better management of energy between prosumer and grid. Lastly, the experiment results revealed that renewable energy sources have produced better and comparable results to other non-renewable energy resources.
</details>
<details>
<summary>摘要</summary>
发展中国家面临着能源短缺和电力卸载危机，主要原因是能源部门的管理不足和使用非可再生能源。通过改善能源管理和使用可再生能源，可以有效解决能源危机。随着化石燃料基本能源价格的上涨，使用可再生能源成为了解决能源危机的重要手段。最新的技术之一是联邦学习（FL），它可以在远程的边缘设备上 ensemble 本地训练的模型，而不需要将数据传输到服务器端，以保护数据隐私。这个全球模型可以预测消费者的能源需求，并且可以使用可再生能源来满足消费者的需求。本文提出了基于区块链的安全分布式笔记录技术，用于在消费者和生产者之间传输数据，以确保数据的透明度、可追溯性和安全性。此外，我们还提出了基于联邦学习的能源需求预测模型，以便更好地预测消费者和生产者的能源需求。此外，使用区块链存储生产者的剩余能源数据，以便更好地管理能源的协调。实验结果表明，可再生能源可以生产更好的和相对比较好的结果，相比于其他非可再生能源资源。
</details></li>
</ul>
<hr>
<h2 id="DiTTO-Diffusion-inspired-Temporal-Transformer-Operator"><a href="#DiTTO-Diffusion-inspired-Temporal-Transformer-Operator" class="headerlink" title="DiTTO: Diffusion-inspired Temporal Transformer Operator"></a>DiTTO: Diffusion-inspired Temporal Transformer Operator</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09072">http://arxiv.org/abs/2307.09072</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oded Ovadia, Eli Turkel, Adar Kahana, George Em Karniadakis</li>
<li>for: 用于解决时间取值积分方程（PDE）。</li>
<li>methods: 使用数据驱动的操作学习方法，不需要时间排序。</li>
<li>results: 在多维度的 burgers 方程、navier-stokes 方程和声波方程上达到了状态机器人精度。 Additionally, the method can perform zero-shot super-resolution in time.Here’s the full text in Simplified Chinese:</li>
<li>for: 本文用于解决时间取值积分方程（PDE）。</li>
<li>methods: 本文提出了一种基于操作学习的数据驱动方法，不需要时间排序。</li>
<li>results: 在多维度的 burgers 方程、navier-stokes 方程和声波方程上，本方法达到了状态机器人精度。此外，方法还可以实现零试验超分辨率。<details>
<summary>Abstract</summary>
Solving partial differential equations (PDEs) using a data-driven approach has become increasingly common. The recent development of the operator learning paradigm has enabled the solution of a broader range of PDE-related problems. We propose an operator learning method to solve time-dependent PDEs continuously in time without needing any temporal discretization. The proposed approach, named DiTTO, is inspired by latent diffusion models. While diffusion models are usually used in generative artificial intelligence tasks, their time-conditioning mechanism is extremely useful for PDEs. The diffusion-inspired framework is combined with elements from the Transformer architecture to improve its capabilities.   We demonstrate the effectiveness of the new approach on a wide variety of PDEs in multiple dimensions, namely the 1-D Burgers' equation, 2-D Navier-Stokes equations, and the acoustic wave equation in 2-D and 3-D. DiTTO achieves state-of-the-art results in terms of accuracy for these problems. We also present a method to improve the performance of DiTTO by using fast sampling concepts from diffusion models. Finally, we show that DiTTO can accurately perform zero-shot super-resolution in time.
</details>
<details>
<summary>摘要</summary>
解决部分梯度方程（PDEs）使用数据驱动方法已成为日益普遍。最近的运算学学 paradigm的发展已使得解决更广泛的 PDE 相关问题变得可能。我们提议一种运算学学方法，名为 DiTTO，可以连续地在时间上解决时间依赖的 PDE。该方法灵感于潜在扩散模型，而扩散模型通常用于生成人工智能任务。扩散启发的框架与 transformer 架构的元素相结合，以提高其能力。我们在多维 PDE 上进行了广泛的实验，包括一维拜尔斯坦方程、二维奈尔-斯托克方程以及二维和三维的声波方程。DiTTO 在这些问题上达到了最新的精度标准。此外，我们还提出了使用快速抽样概念从扩散模型来提高 DiTTO 的性能的方法。最后，我们展示了 DiTTO 可以准确地进行零 shot 超分辨率在时间上。
</details></li>
</ul>
<hr>
<h2 id="Evaluate-Fine-tuning-Strategies-for-Fetal-Head-Ultrasound-Image-Segmentation-with-U-Net"><a href="#Evaluate-Fine-tuning-Strategies-for-Fetal-Head-Ultrasound-Image-Segmentation-with-U-Net" class="headerlink" title="Evaluate Fine-tuning Strategies for Fetal Head Ultrasound Image Segmentation with U-Net"></a>Evaluate Fine-tuning Strategies for Fetal Head Ultrasound Image Segmentation with U-Net</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09067">http://arxiv.org/abs/2307.09067</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/13204942/ft_methods_for_fetal_head_segmentation">https://github.com/13204942/ft_methods_for_fetal_head_segmentation</a></li>
<li>paper_authors: Fangyijie Wang, Guénolé Silvestre, Kathleen M. Curran</li>
<li>for: 这篇研究的目的是提高妊娠期间胎头圆周盘 circumference（HC）的测量效率，使用扩展学习（Transfer Learning，TL）方法来改善医疗生物米etry的精度。</li>
<li>methods: 本研究使用了潜在神经网络（Convolutional Neural Network，CNN）模型，并使用了轻量级的 MobileNet 作为数据库的数据库。</li>
<li>results: 研究发现，使用 Transfer Learning 方法可以将胎头像的数据库训练为 U-Net 网络，并且可以实现高度的准确性，仅需要有限的训练时间和资源。此外，本研究还发现，使用 Transfer Learning 方法可以实现较小的模型大小，并且可以提高模型的稳定性和可靠性。<details>
<summary>Abstract</summary>
Fetal head segmentation is a crucial step in measuring the fetal head circumference (HC) during gestation, an important biometric in obstetrics for monitoring fetal growth. However, manual biometry generation is time-consuming and results in inconsistent accuracy. To address this issue, convolutional neural network (CNN) models have been utilized to improve the efficiency of medical biometry. But training a CNN network from scratch is a challenging task, we proposed a Transfer Learning (TL) method. Our approach involves fine-tuning (FT) a U-Net network with a lightweight MobileNet as the encoder to perform segmentation on a set of fetal head ultrasound (US) images with limited effort. This method addresses the challenges associated with training a CNN network from scratch. It suggests that our proposed FT strategy yields segmentation performance that is comparable when trained with a reduced number of parameters by 85.8%. And our proposed FT strategy outperforms other strategies with smaller trainable parameter sizes below 4.4 million. Thus, we contend that it can serve as a dependable FT approach for reducing the size of models in medical image analysis. Our key findings highlight the importance of the balance between model performance and size in developing Artificial Intelligence (AI) applications by TL methods. Code is available at https://github.com/13204942/FT_Methods_for_Fetal_Head_Segmentation.
</details>
<details>
<summary>摘要</summary>
Gestational fetal head circumference (HC) measurement is crucial, and manual biometry is time-consuming and prone to errors. To address this, convolutional neural networks (CNNs) have been used for biometry. However, training a CNN from scratch is challenging. To solve this, we proposed a transfer learning (TL) method. Our approach involves fine-tuning a U-Net network with a lightweight MobileNet as the encoder to perform segmentation on a set of fetal head ultrasound (US) images with minimal effort. This method addresses the challenges of training a CNN from scratch and shows that our proposed FT strategy achieves segmentation performance comparable to training with a reduced number of parameters by 85.8%. Our proposed FT strategy also outperforms other strategies with smaller trainable parameter sizes below 4.4 million. Therefore, we suggest that our FT approach is a reliable method for reducing the size of models in medical image analysis. Our findings highlight the importance of balancing model performance and size in developing artificial intelligence (AI) applications using TL methods. 针对妊娠期胎头径的量度是至关重要，但是手动测量是时间费时且精度不稳定。为解决这个问题，人工神经网络（CNN）已经被应用于生物метría。然而，从零开始训练CNN是一项具有挑战性的任务。为解决这个问题，我们提出了传输学习（TL）方法。我们的方法涉及到了精细调整U-Net网络的 MobileNet 作为编码器，以实现对一组妊娠期胎头ultrasound（US）图像进行分割，即使是很少的努力。这种方法解决了训练CNN从零开始的挑战，并表明了我们的FT策略可以与减少参数数量的85.8%相比，实现分割性能。此外，我们的FT策略还超过了其他具有更小的可学习参数数量的策略。因此，我们建议这种FT方法可以作为医疗图像分析中减小模型的可靠方法。我们的关键发现指出了在通过TL方法开发人工智能应用程序时，模型性能和模型大小之间的平衡是非常重要的。
</details></li>
</ul>
<hr>
<h2 id="Learning-Adaptive-Neighborhoods-for-Graph-Neural-Networks"><a href="#Learning-Adaptive-Neighborhoods-for-Graph-Neural-Networks" class="headerlink" title="Learning Adaptive Neighborhoods for Graph Neural Networks"></a>Learning Adaptive Neighborhoods for Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09065">http://arxiv.org/abs/2307.09065</a></li>
<li>repo_url: None</li>
<li>paper_authors: Avishkar Saha, Oscar Mendez, Chris Russell, Richard Bowden</li>
<li>for: 实现终端学习于 гра组织资料上，但许多工作假设已知 graph 结构。当输入 graph 是噪音或无法取得时，一种方法是建构或学习 latent graph 结构。这些方法通常固定 graph 结构中每个 node 的度量，这是不佳的。相反，我们提出了一个 novel end-to-end differentiable graph generator，可以建构 graph 结构，每个 node 可以选择它的邻居和大小。</li>
<li>methods: 我们提出了一个 novel end-to-end differentiable graph generator，可以建构 graph 结构，每个 node 可以选择它的邻居和大小。</li>
<li>results: 我们将我们的模组 integrate 到 trajectory prediction, point cloud classification 和 node classification pipeline 中，实现了与其他 structure-learning 方法相比的提高精度，在各种数据集和 GCN 背景下。<details>
<summary>Abstract</summary>
Graph convolutional networks (GCNs) enable end-to-end learning on graph structured data. However, many works assume a given graph structure. When the input graph is noisy or unavailable, one approach is to construct or learn a latent graph structure. These methods typically fix the choice of node degree for the entire graph, which is suboptimal. Instead, we propose a novel end-to-end differentiable graph generator which builds graph topologies where each node selects both its neighborhood and its size. Our module can be readily integrated into existing pipelines involving graph convolution operations, replacing the predetermined or existing adjacency matrix with one that is learned, and optimized, as part of the general objective. As such it is applicable to any GCN. We integrate our module into trajectory prediction, point cloud classification and node classification pipelines resulting in improved accuracy over other structure-learning methods across a wide range of datasets and GCN backbones.
</details>
<details>
<summary>摘要</summary>
格点卷积网络（GCN）可以实现端到端学习在格式化数据上。然而，许多工作假设给定的图结构。当输入图是噪音或不可用时，一种方法是构建或学习隐藏的图结构。这些方法通常固定整个图的节点度，这是不优化的。相反，我们提出了一种新的终端可微的图生成器，它可以在每个节点选择其邻居和大小。我们的模块可以轻松地与现有的图卷积操作相结合，将预先确定或现有的相互作用矩阵 replaced with一个学习和优化的矩阵，并成为任何GCN的一部分。因此，它是可应用的。我们将我们的模块集成到了路径预测、点云分类和节点分类管道中，从而在各种数据集和GCN脊梁上实现了与其他结构学习方法相比较高的准确率。
</details></li>
</ul>
<hr>
<h2 id="Extreme-heatwave-sampling-and-prediction-with-analog-Markov-chain-and-comparisons-with-deep-learning"><a href="#Extreme-heatwave-sampling-and-prediction-with-analog-Markov-chain-and-comparisons-with-deep-learning" class="headerlink" title="Extreme heatwave sampling and prediction with analog Markov chain and comparisons with deep learning"></a>Extreme heatwave sampling and prediction with analog Markov chain and comparisons with deep learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09060">http://arxiv.org/abs/2307.09060</a></li>
<li>repo_url: None</li>
<li>paper_authors: George Miloshevich, Dario Lucente, Pascal Yiou, Freddy Bouchet</li>
<li>For: The paper aims to develop a data-driven emulator, called stochastic weather generator (SWG), to estimate the probabilities of prolonged heatwaves in France and Scandinavia.* Methods: The SWG emulator uses the method of analogs of circulation, which is combined with temperature and soil moisture as predictor fields. The emulator is trained on an intermediate complexity climate model run, and the performance is evaluated using proper score appropriate for rare events. Dimensionality reduction techniques are applied to accelerate the computation of analogs.* Results: The probabilistic prediction achieved with SWG is compared with the one achieved with Convolutional Neural Network (CNN). The SWG emulator trained on 80 years of data is capable of estimating extreme return times of order of thousands of years for heatwaves longer than several days more precisely than the fit based on generalised extreme value distribution. The quality of its synthetic extreme teleconnection patterns obtained with stochastic weather generator is studied, and two examples of such synthetic teleconnection patterns for heatwaves in France and Scandinavia are provided.<details>
<summary>Abstract</summary>
We present a data-driven emulator, stochastic weather generator (SWG), suitable for estimating probabilities of prolonged heatwaves in France and Scandinavia. This emulator is based on the method of analogs of circulation to which we add temperature and soil moisture as predictor fields. We train the emulator on an intermediate complexity climate model run and show that it is capable of predicting conditional probabilities (forecasting) of heatwaves out of sample. Special attention is payed that this prediction is evaluated using proper score appropriate for rare events. To accelerate the computation of analogs dimensionality reduction techniques are applied and the performance is evaluated. The probabilistic prediction achieved with SWG is compared with the one achieved with   Convolutional Neural Network (CNN). With the availability of hundreds of years of training data CNNs perform better at the task of probabilistic prediction. In addition, we show that the SWG emulator trained on 80 years of data is capable of estimating extreme return times of order of thousands of years for heatwaves longer than several days more precisely than the fit based on generalised extreme value distribution. Finally, the quality of its synthetic extreme teleconnection patterns obtained with stochastic weather generator is studied. We showcase two examples of such synthetic teleconnection patterns for heatwaves in France and Scandinavia that compare favorably to the very long climate model control run.
</details>
<details>
<summary>摘要</summary>
我们介绍了一个数据驱动的模拟器，随机天气生成器（SWG），用于估计法国和斯堪的纳维亚地区的持续高温事件的可能性。这个模拟器基于流体动力学方法，并添加温度和土壤湿度作为预测字段。我们使用一个中间复杂度气候模型的训练来训练这个模拟器，并显示它可以预测基于样本外的条件概率（预测）高温事件。特别是，我们使用合适的评价函数来评估这种预测，以确保对罕见事件进行正确的评估。为加速计算流体的维度减少技术是应用于 analogs，并评估其性能。我们还比较了使用 convolutional neural network（CNN）进行probabilistic预测的性能。通过使用数百年的训练数据，CNN在这项任务上表现更好。此外，我们发现使用80年的数据训练SWG模拟器可以更正精确地估计高温事件持续时间长于数天的极端返回时间，与基于总体极值分布的预测相比。最后，我们研究了SWG模拟器生成的 sintethic极端 теле连接模式的质量。我们展示了法国和斯堪的纳维亚两个高温事件的 sintethic tele连接模式，与非常长气候模型控制运行相比，表现良好。
</details></li>
</ul>
<hr>
<h2 id="Deep-learning-for-unsupervised-domain-adaptation-in-medical-imaging-Recent-advancements-and-future-perspectives"><a href="#Deep-learning-for-unsupervised-domain-adaptation-in-medical-imaging-Recent-advancements-and-future-perspectives" class="headerlink" title="Deep learning for unsupervised domain adaptation in medical imaging: Recent advancements and future perspectives"></a>Deep learning for unsupervised domain adaptation in medical imaging: Recent advancements and future perspectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01265">http://arxiv.org/abs/2308.01265</a></li>
<li>repo_url: None</li>
<li>paper_authors: Suruchi Kumari, Pravendra Singh</li>
<li>for: 本文主要探讨了医学成像领域中最新的深度学习领域适应（Unsupervised Domain Adaptation，UDA）技术，以及它们在各种医学成像任务中的应用。</li>
<li>methods: 本文分析了医学成像领域中最新的UDA方法，包括特征对应、图像翻译、自我超vision和分解表示方法等多种方法。</li>
<li>results: 本文对各种UDA方法进行了技术分析和评估，并将其分为六个类别，包括图像分类、生物marks检测、肿瘤识别、脑成像分析、肠胃成像分析等多种任务。<details>
<summary>Abstract</summary>
Deep learning has demonstrated remarkable performance across various tasks in medical imaging. However, these approaches primarily focus on supervised learning, assuming that the training and testing data are drawn from the same distribution. Unfortunately, this assumption may not always hold true in practice. To address these issues, unsupervised domain adaptation (UDA) techniques have been developed to transfer knowledge from a labeled domain to a related but unlabeled domain. In recent years, significant advancements have been made in UDA, resulting in a wide range of methodologies, including feature alignment, image translation, self-supervision, and disentangled representation methods, among others. In this paper, we provide a comprehensive literature review of recent deep UDA approaches in medical imaging from a technical perspective. Specifically, we categorize current UDA research in medical imaging into six groups and further divide them into finer subcategories based on the different tasks they perform. We also discuss the respective datasets used in the studies to assess the divergence between the different domains. Finally, we discuss emerging areas and provide insights and discussions on future research directions to conclude this survey.
</details>
<details>
<summary>摘要</summary>
深度学习在医疗影像领域已经表现出惊人的表现。然而，这些方法主要偏向于有监督学习，假设训练和测试数据来自同一个分布。然而，这种假设可能不 siempre 成立。为 Address 这些问题，无监督领域适应（UDA）技术被开发出来，以传递来自标注域的知识到相关 yet unlabeled 域。在过去几年中，UDA 领域内有了大量的进展，包括特征对齐、图像翻译、自我监督和分解表示方法等。在本文中，我们提供了医疗影像领域的深度 UDA 方法的全面文献回顾，具体来说，我们将当前 UDA 研究分为六个组，并将它们进一步分为不同任务的子类别。我们还讨论了不同研究使用的数据集，以评估不同领域之间的差异。最后，我们提出了未来研究的前景和意见，以结束本文的报告。
</details></li>
</ul>
<hr>
<h2 id="Globally-solving-the-Gromov-Wasserstein-problem-for-point-clouds-in-low-dimensional-Euclidean-spaces"><a href="#Globally-solving-the-Gromov-Wasserstein-problem-for-point-clouds-in-low-dimensional-Euclidean-spaces" class="headerlink" title="Globally solving the Gromov-Wasserstein problem for point clouds in low dimensional Euclidean spaces"></a>Globally solving the Gromov-Wasserstein problem for point clouds in low dimensional Euclidean spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09057">http://arxiv.org/abs/2307.09057</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martin Ryner, Jan Kronqvist, Johan Karlsson</li>
<li>for:  Computes the Gromov-Wasserstein problem between two sets of points in low dimensional spaces, to quantify the similarity between two formations or shapes.</li>
<li>methods:  Reformulates the Quadratic Assignment Problem (QAP) as an optimization problem with a low-dimensional domain, leveraging the fact that the problem can be expressed as a concave quadratic optimization problem with low rank.</li>
<li>results:  Scales well with the number of points and can be used to find the global solution for large-scale problems with thousands of points.<details>
<summary>Abstract</summary>
This paper presents a framework for computing the Gromov-Wasserstein problem between two sets of points in low dimensional spaces, where the discrepancy is the squared Euclidean norm. The Gromov-Wasserstein problem is a generalization of the optimal transport problem that finds the assignment between two sets preserving pairwise distances as much as possible. This can be used to quantify the similarity between two formations or shapes, a common problem in AI and machine learning. The problem can be formulated as a Quadratic Assignment Problem (QAP), which is in general computationally intractable even for small problems. Our framework addresses this challenge by reformulating the QAP as an optimization problem with a low-dimensional domain, leveraging the fact that the problem can be expressed as a concave quadratic optimization problem with low rank. The method scales well with the number of points, and it can be used to find the global solution for large-scale problems with thousands of points. We compare the computational complexity of our approach with state-of-the-art methods on synthetic problems and apply it to a near-symmetrical problem which is of particular interest in computational biology.
</details>
<details>
<summary>摘要</summary>
However, the Gromov-Wasserstein problem is computationally intractable, even for small problems, and is typically formulated as a Quadratic Assignment Problem (QAP). Our framework addresses this challenge by reformulating the QAP as an optimization problem with a low-dimensional domain, leveraging the fact that the problem can be expressed as a concave quadratic optimization problem with low rank.The proposed method scales well with the number of points and can be used to find the global solution for large-scale problems with thousands of points. We compare the computational complexity of our approach with state-of-the-art methods on synthetic problems and apply it to a near-symmetrical problem of particular interest in computational biology.
</details></li>
</ul>
<hr>
<h2 id="Outlier-Robust-Tensor-Low-Rank-Representation-for-Data-Clustering"><a href="#Outlier-Robust-Tensor-Low-Rank-Representation-for-Data-Clustering" class="headerlink" title="Outlier-Robust Tensor Low-Rank Representation for Data Clustering"></a>Outlier-Robust Tensor Low-Rank Representation for Data Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09055">http://arxiv.org/abs/2307.09055</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tong Wu</li>
<li>for: 提取受损张量数据中的异常点和分 clustering</li>
<li>methods: 基于张量特征值分解（t-SVD）的异常点检测和张量数据分 clustering</li>
<li>results: 可以 preciselly 恢复受损张量数据的行空间和检测异常点，并且可以 hanlde missing 数据情况。<details>
<summary>Abstract</summary>
Low-rank tensor analysis has received widespread attention with many practical applications. However, the tensor data are often contaminated by outliers or sample-specific corruptions. How to recover the tensor data that are corrupted by outliers and perform data clustering remains a challenging problem. This paper develops an outlier-robust tensor low-rank representation (OR-TLRR) method for simultaneous outlier detection and tensor data clustering based on the tensor singular value decomposition (t-SVD) algebraic framework. It is motivated by the recently proposed tensor-tensor product induced by invertible linear transforms that satisfy certain conditions. For tensor observations with arbitrary outlier corruptions, OR-TLRR has provable performance guarantee for exactly recovering the row space of clean data and detecting outliers under mild conditions. Moreover, an extension of OR-TLRR is also proposed to handle the case when parts of the data are missing. Finally, extensive experimental results on both synthetic and real data demonstrate the effectiveness of the proposed algorithms.
</details>
<details>
<summary>摘要</summary>
低级张量分析已经广泛受到关注，有很多实际应用。然而，张量数据经常受到异常值或样本特定的损害。如何修复受损的张量数据，并对其进行分类仍然是一个困难的问题。这篇论文开发了一种对异常值敏感的张量低级表示（OR-TLRR）方法，用于同时检测异常值和张量数据的分类。它是基于张量单值分解（t-SVD）的代数框架的。对于受到 произвольными异常损害的张量观察数据，OR-TLRR有可证明的性能保证，可以准确地恢复干净数据的列空间和检测异常值，只要异常值的干扰程度不太大。此外，论文还提出了处理缺失数据的扩展方法。最后，论文的实验结果表明了提议的算法的效果。
</details></li>
</ul>
<hr>
<h2 id="qecGPT-decoding-Quantum-Error-correcting-Codes-with-Generative-Pre-trained-Transformers"><a href="#qecGPT-decoding-Quantum-Error-correcting-Codes-with-Generative-Pre-trained-Transformers" class="headerlink" title="qecGPT: decoding Quantum Error-correcting Codes with Generative Pre-trained Transformers"></a>qecGPT: decoding Quantum Error-correcting Codes with Generative Pre-trained Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09025">http://arxiv.org/abs/2307.09025</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chy-i/qecgpt">https://github.com/chy-i/qecgpt</a></li>
<li>paper_authors: Hanyan Cao, Feng Pan, Yijia Wang, Pan Zhang</li>
<li>for: 提出了一个通用框架 для解码量子错误修正码，使用生成模型。</li>
<li>methods: 使用自然语言处理技术，特别是Transformers，学习逻辑运算和症状的联合概率。</li>
<li>results: 可以高效地计算逻辑运算的可能性，并直接生成最有可能的逻辑运算结果，计算复杂度为 $\mathcal O(2k)$，比传统最大可能性decoding算法要好。<details>
<summary>Abstract</summary>
We propose a general framework for decoding quantum error-correcting codes with generative modeling. The model utilizes autoregressive neural networks, specifically Transformers, to learn the joint probability of logical operators and syndromes. This training is in an unsupervised way, without the need for labeled training data, and is thus referred to as pre-training. After the pre-training, the model can efficiently compute the likelihood of logical operators for any given syndrome, using maximum likelihood decoding. It can directly generate the most-likely logical operators with computational complexity $\mathcal O(2k)$ in the number of logical qubits $k$, which is significantly better than the conventional maximum likelihood decoding algorithms that require $\mathcal O(4^k)$ computation. Based on the pre-trained model, we further propose refinement to achieve more accurately the likelihood of logical operators for a given syndrome by directly sampling the stabilizer operators. We perform numerical experiments on stabilizer codes with small code distances, using both depolarizing error models and error models with correlated noise. The results show that our approach provides significantly better decoding accuracy than the minimum weight perfect matching and belief-propagation-based algorithms. Our framework is general and can be applied to any error model and quantum codes with different topologies such as surface codes and quantum LDPC codes. Furthermore, it leverages the parallelization capabilities of GPUs, enabling simultaneous decoding of a large number of syndromes. Our approach sheds light on the efficient and accurate decoding of quantum error-correcting codes using generative artificial intelligence and modern computational power.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="U-shaped-Transformer-Retain-High-Frequency-Context-in-Time-Series-Analysis"><a href="#U-shaped-Transformer-Retain-High-Frequency-Context-in-Time-Series-Analysis" class="headerlink" title="U-shaped Transformer: Retain High Frequency Context in Time Series Analysis"></a>U-shaped Transformer: Retain High Frequency Context in Time Series Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09019">http://arxiv.org/abs/2307.09019</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qingkui Chen, Yiqin Zhang</li>
<li>for: 本研究旨在增进时间序列预测领域中的 neural network 性能，通过综合利用 transformer 和 MLP 两种网络结构。</li>
<li>methods: 本研究采用了 skip-layer 连接和 patch merge 和 split 操作，以提高 transformer 的低频特征表示能力，并使用更大的数据集来充分利用 transformer 背景。</li>
<li>results: 实验结果表明，模型在多个数据集上表现出了高水平的性能，而且比 traditional transformer 更加高效。<details>
<summary>Abstract</summary>
Time series prediction plays a crucial role in various industrial fields. In recent years, neural networks with a transformer backbone have achieved remarkable success in many domains, including computer vision and NLP. In time series analysis domain, some studies have suggested that even the simplest MLP networks outperform advanced transformer-based networks on time series forecast tasks. However, we believe these findings indicate there to be low-rank properties in time series sequences. In this paper, we consider the low-pass characteristics of transformers and try to incorporate the advantages of MLP. We adopt skip-layer connections inspired by Unet into traditional transformer backbone, thus preserving high-frequency context from input to output, namely U-shaped Transformer. We introduce patch merge and split operation to extract features with different scales and use larger datasets to fully make use of the transformer backbone. Our experiments demonstrate that the model performs at an advanced level across multiple datasets with relatively low cost.
</details>
<details>
<summary>摘要</summary>
时间序列预测在各个产业领域发挥重要作用。近年来，基于 transformer 结构的神经网络在各个领域，如计算机视觉和自然语言处理，取得了很大成功。然而，一些研究表明，简单的多层感知网络可以在时间序列预测任务上超越高级 transformer 基于网络。我们认为这些发现反映了时间序列序列中的低级特性。在这篇论文中，我们考虑了 transformer 的低通Characteristics 和 MLP 网络的优点，并将它们结合在一起。我们采用了 skip-layer 连接，以保持输入到输出的高频上下文，即 U-shaped Transformer。我们还引入 patch 合并和拆分操作，以提取不同缩放的特征，并使用更大的数据集，以充分利用 transformer 脊梁。我们的实验表明，模型在多个数据集上表现出了高水平的性能，而且Relative low cost。
</details></li>
</ul>
<hr>
<h2 id="Multimodal-LLMs-for-health-grounded-in-individual-specific-data"><a href="#Multimodal-LLMs-for-health-grounded-in-individual-specific-data" class="headerlink" title="Multimodal LLMs for health grounded in individual-specific data"></a>Multimodal LLMs for health grounded in individual-specific data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09018">http://arxiv.org/abs/2307.09018</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anastasiya Belyaeva, Justin Cosentino, Farhad Hormozdiari, Krish Eswaran, Shravya Shetty, Greg Corrado, Andrew Carroll, Cory Y. McLean, Nicholas A. Furlotte</li>
<li>for: 这篇研究的目的是为了创建能够处理多种资料模式的大语言模型（LLMs），以解决各种领域中的问题，包括健康领域。</li>
<li>methods: 这篇研究使用了一个名为HeLM（Health Large Language Model for Multimodal Understanding）的框架，它可以将高维度的医疗资料与LLMs集成，以估计个人疾病风险。HeLM使用了一个Encoder来转换资料模式，将复杂的资料模式转换为LLM的token embedding空间，并将简单的资料模式转换为文本。</li>
<li>results: 根据UK Biobank的数据，HeLM可以有效地使用民生和医疗资料，以及高维度时间序列数据估计疾病风险。例如，HeLM在结合表格和呼吸图数据模式时，可以获得预测病例0.75的AUROC，比仅使用表格数据模式时的0.49要高。总的来说，HeLM在选择的八个二分类问题中，都能够超越或与класичного机器学习方法相等。此外，研究还评估了这个模型的扩展性和对个人健康和快速诊断的应用。<details>
<summary>Abstract</summary>
Foundation large language models (LLMs) have shown an impressive ability to solve tasks across a wide range of fields including health. To effectively solve personalized health tasks, LLMs need the ability to ingest a diversity of data modalities that are relevant to an individual's health status. In this paper, we take a step towards creating multimodal LLMs for health that are grounded in individual-specific data by developing a framework (HeLM: Health Large Language Model for Multimodal Understanding) that enables LLMs to use high-dimensional clinical modalities to estimate underlying disease risk. HeLM encodes complex data modalities by learning an encoder that maps them into the LLM's token embedding space and for simple modalities like tabular data by serializing the data into text. Using data from the UK Biobank, we show that HeLM can effectively use demographic and clinical features in addition to high-dimensional time-series data to estimate disease risk. For example, HeLM achieves an AUROC of 0.75 for asthma prediction when combining tabular and spirogram data modalities compared with 0.49 when only using tabular data. Overall, we find that HeLM outperforms or performs at parity with classical machine learning approaches across a selection of eight binary traits. Furthermore, we investigate the downstream uses of this model such as its generalizability to out-of-distribution traits and its ability to power conversations around individual health and wellness.
</details>
<details>
<summary>摘要</summary>
基于大语言模型（LLM）的研究表明，LLM可以在各种领域解决问题，包括健康领域。为了在个人化医疗方面有效地解决问题，LLM需要能够处理个人健康状况相关的多种数据类型。在这篇论文中，我们开发了一个框架（HeLM：健康大语言模型 для多模态理解），帮助LLM使用个人化数据来估计疾病风险。HeLM将复杂的数据类型编码为将其映射到LLM的符号空间中，而简单的数据类型则通过将数据序列化为文本来实现。使用UK Biobank数据，我们显示了HeLM可以有效地使用人口和临床特征以及高维时序数据来估计疾病风险。例如，当 combining 表格和气流数据模式时，HeLM的 AUC 为 0.75，而只使用表格数据时的 AUC 为 0.49。总的来说，我们发现 HeLM 在选择的八个二分类特征上表现出色，并且与传统机器学习方法相当或超过其表现。此外，我们还 investigate HeLM 的下游应用，包括其对非标型特征的普适性和在个人医疗和健康谈话中的应用能力。
</details></li>
</ul>
<hr>
<h2 id="PLiNIO-A-User-Friendly-Library-of-Gradient-based-Methods-for-Complexity-aware-DNN-Optimization"><a href="#PLiNIO-A-User-Friendly-Library-of-Gradient-based-Methods-for-Complexity-aware-DNN-Optimization" class="headerlink" title="PLiNIO: A User-Friendly Library of Gradient-based Methods for Complexity-aware DNN Optimization"></a>PLiNIO: A User-Friendly Library of Gradient-based Methods for Complexity-aware DNN Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09488">http://arxiv.org/abs/2307.09488</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniele Jahier Pagliari, Matteo Risso, Beatrice Alessandra Motetti, Alessio Burrello</li>
<li>for: 这篇论文主要是为了提供一个开源的深度神经网络设计自动化库（PLiNIO），来搭配各种渐渐变小的优化技术，以提高深度神经网络在紧缩边缘设备上的效能。</li>
<li>methods: 这篇论文使用了许多现代的深度神经网络设计自动化技术，包括预测精度估计、优化搜索、阶层优化、卷积优化等，并将这些技术集成到一个开源库中，提供了一个易用的用户界面。</li>
<li>results: 根据实验结果，PLiNIO可以实现优化深度神经网络的体积和精度，并且可以实现大约94.34%的内存减少，却只有&lt;1%的精度损失比基eline架构。<details>
<summary>Abstract</summary>
Accurate yet efficient Deep Neural Networks (DNNs) are in high demand, especially for applications that require their execution on constrained edge devices. Finding such DNNs in a reasonable time for new applications requires automated optimization pipelines since the huge space of hyper-parameter combinations is impossible to explore extensively by hand. In this work, we propose PLiNIO, an open-source library implementing a comprehensive set of state-of-the-art DNN design automation techniques, all based on lightweight gradient-based optimization, under a unified and user-friendly interface. With experiments on several edge-relevant tasks, we show that combining the various optimizations available in PLiNIO leads to rich sets of solutions that Pareto-dominate the considered baselines in terms of accuracy vs model size. Noteworthy, PLiNIO achieves up to 94.34% memory reduction for a <1% accuracy drop compared to a baseline architecture.
</details>
<details>
<summary>摘要</summary>
高效减少内存的深度神经网络（DNN）在边缘设备上的应用越来越受欢迎，特别是在有限的边缘设备上运行。手动搜索大量的超参数组合是不可能的，因此需要自动优化管道。在这种情况下，我们提出PLiNIO，一个开源库，实现了现代DNN设计自动化技术的总集，所有基于轻量级的梯度基于优化，通过统一和易用的界面进行实现。经过一些边缘相关任务的实验，我们发现，PLiNIO中的多种优化技术的组合可以生成高精度和轻量级的解决方案，与考虑的基准架构相比，PLiNIO可以实现94.34%的内存减少，仅带来<1%的准确率下降。
</details></li>
</ul>
<hr>
<h2 id="How-is-ChatGPT’s-behavior-changing-over-time"><a href="#How-is-ChatGPT’s-behavior-changing-over-time" class="headerlink" title="How is ChatGPT’s behavior changing over time?"></a>How is ChatGPT’s behavior changing over time?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09009">http://arxiv.org/abs/2307.09009</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lchen001/llmdrift">https://github.com/lchen001/llmdrift</a></li>
<li>paper_authors: Lingjiao Chen, Matei Zaharia, James Zou</li>
<li>for: 评估 GPT-3.5 和 GPT-4 两个大语言模型在不同时间点上的变化。</li>
<li>methods: 使用多种多样化任务评估 GPT-3.5 和 GPT-4 在不同时间点上的表现。</li>
<li>results: 发现 GPT-4 和 GPT-3.5 在不同时间点上的表现和行为可能会有很大的变化，如 prime vs. composite numbers 识别 task 中 GPT-4 (3月2023) 的表现比 GPT-4 (6月2023) 更好，但是 GPT-3.5 在6月的表现更好于3月。<details>
<summary>Abstract</summary>
GPT-3.5 and GPT-4 are the two most widely used large language model (LLM) services. However, when and how these models are updated over time is opaque. Here, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on several diverse tasks: 1) math problems, 2) sensitive/dangerous questions, 3) opinion surveys, 4) multi-hop knowledge-intensive questions, 5) generating code, 6) US Medical License tests, and 7) visual reasoning. We find that the performance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time. For example, GPT-4 (March 2023) was reasonable at identifying prime vs. composite numbers (84% accuracy) but GPT-4 (June 2023) was poor on these same questions (51% accuracy). This is partly explained by a drop in GPT-4's amenity to follow chain-of-thought prompting. Interestingly, GPT-3.5 was much better in June than in March in this task. GPT-4 became less willing to answer sensitive questions and opinion survey questions in June than in March. GPT-4 performed better at multi-hop questions in June than in March, while GPT-3.5's performance dropped on this task. Both GPT-4 and GPT-3.5 had more formatting mistakes in code generation in June than in March. Overall, our findings show that the behavior of the "same" LLM service can change substantially in a relatively short amount of time, highlighting the need for continuous monitoring of LLMs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="OxfordVGG-Submission-to-the-EGO4D-AV-Transcription-Challenge"><a href="#OxfordVGG-Submission-to-the-EGO4D-AV-Transcription-Challenge" class="headerlink" title="OxfordVGG Submission to the EGO4D AV Transcription Challenge"></a>OxfordVGG Submission to the EGO4D AV Transcription Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09006">http://arxiv.org/abs/2307.09006</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/m-bain/whisperx">https://github.com/m-bain/whisperx</a></li>
<li>paper_authors: Jaesung Huh, Max Bain, Andrew Zisserman</li>
<li>for: 本研究报告提供了2023年EGO4D音频视觉自动语音识别挑战（AV-ASR）中oxfordvgg团队的技术细节。</li>
<li>methods: 本研究使用了WhisperX系统，用于高效处理长形audio的语音识别，并且使用了两个公开可用的文本标准化器。</li>
<li>results: 本研究在挑战测试集上取得56.0%的字误率（WER），排名第一名在排行板上。所有基eline代码和模型可以在<a target="_blank" rel="noopener" href="https://github.com/m-bain/whisperX%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/m-bain/whisperX上获取。</a><details>
<summary>Abstract</summary>
This report presents the technical details of our submission on the EGO4D Audio-Visual (AV) Automatic Speech Recognition Challenge 2023 from the OxfordVGG team. We present WhisperX, a system for efficient speech transcription of long-form audio with word-level time alignment, along with two text normalisers which are publicly available. Our final submission obtained 56.0% of the Word Error Rate (WER) on the challenge test set, ranked 1st on the leaderboard. All baseline codes and models are available on https://github.com/m-bain/whisperX.
</details>
<details>
<summary>摘要</summary>
这份报告介绍了我们在EGO4D Audio-Visual（AV）自动话语识别挑战2023中的提交技术细节，来自于牛津VGG团队。我们提出了一种名为WhisperX的高效语音转文本系统，以及两种公共可用的文本Normalizer。我们的最终提交在挑战测试集上达到56.0%的字SError率，排名第一名。所有基线代码和模型可以在https://github.com/m-bain/whisperX上下载。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Zero-shot-Domain-sensitive-Speech-Recognition-with-Prompt-conditioning-Fine-tuning"><a href="#Zero-shot-Domain-sensitive-Speech-Recognition-with-Prompt-conditioning-Fine-tuning" class="headerlink" title="Zero-shot Domain-sensitive Speech Recognition with Prompt-conditioning Fine-tuning"></a>Zero-shot Domain-sensitive Speech Recognition with Prompt-conditioning Fine-tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10274">http://arxiv.org/abs/2307.10274</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mtkresearch/clairaudience">https://github.com/mtkresearch/clairaudience</a></li>
<li>paper_authors: Feng-Ting Liao, Yung-Chieh Chan, Yi-Chang Chen, Chan-Jan Hsu, Da-shan Shiu</li>
<li>for: 这 paper 是为了创建域专注的语音识别模型，利用文本域信息进行 Conditioning 生成。</li>
<li>methods: 这 paper 使用了精心调整的预训练、端到端模型（Whisper），通过示例示出学习域特定的示例来学习。</li>
<li>results: 这 paper 表明这种能力可以在不同的域和不同的示例上进行泛化，模型在未经见过的数据集上 achieve Word Error Rate (WER) 减少达 33%，并且通过文本Only fine-tuning 来实现域敏感和域适应。<details>
<summary>Abstract</summary>
In this work, we propose a method to create domain-sensitive speech recognition models that utilize textual domain information by conditioning its generation on a given text prompt. This is accomplished by fine-tuning a pre-trained, end-to-end model (Whisper) to learn from demonstrations with prompt examples. We show that this ability can be generalized to different domains and even various prompt contexts, with our model gaining a Word Error Rate (WER) reduction of up to 33% on unseen datasets from various domains, such as medical conversation, air traffic control communication, and financial meetings. Considering the limited availability of audio-transcript pair data, we further extend our method to text-only fine-tuning to achieve domain sensitivity as well as domain adaptation. We demonstrate that our text-only fine-tuned model can also attend to various prompt contexts, with the model reaching the most WER reduction of 29% on the medical conversation dataset.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们提出了一种方法，用于创建域特定的语音识别模型，该模型利用文本域信息进行conditioning。这是通过练化一个预训练的、端到端模型（Whisper）来学习示例示唆。我们表明这种能力可以泛化到不同域和不同提示上下文中，我们的模型在未seen datasets上 achieved Word Error Rate（WER）下降达33%。考虑到语音-讲本数据的有限可用性，我们进一步扩展了我们的方法，使其能够在文本只 fine-tuning中实现域敏感性和域适应性。我们示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示
</details></li>
</ul>
<hr>
<h2 id="Oracle-Efficient-Online-Multicalibration-and-Omniprediction"><a href="#Oracle-Efficient-Online-Multicalibration-and-Omniprediction" class="headerlink" title="Oracle Efficient Online Multicalibration and Omniprediction"></a>Oracle Efficient Online Multicalibration and Omniprediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08999">http://arxiv.org/abs/2307.08999</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sumegha Garg, Christopher Jung, Omer Reingold, Aaron Roth</li>
<li>for: 这种研究的目的是为了研究在在线对抗 Setting中的 omniprediction 算法，以及其与多calibration 的关系。</li>
<li>methods: 这种研究使用了多calibration 和 omniprediction 两种概念，以及一些学习理论的概念。</li>
<li>results: 这种研究得到了一种新的在线多calibration 算法，可以在无限 benchmark 类 $F$ 中进行定义，并且是 oracle 有效的（即对于任何类 $F$, 算法可以转化为一种有效的减少 regret 学习算法）。此外，这种算法还可以在 linear functions 类 $F$ 中进行有效的实现。此外，这种研究还提供了 upper 和 lower bounds，用于评估这种算法的性能。<details>
<summary>Abstract</summary>
A recent line of work has shown a surprising connection between multicalibration, a multi-group fairness notion, and omniprediction, a learning paradigm that provides simultaneous loss minimization guarantees for a large family of loss functions. Prior work studies omniprediction in the batch setting. We initiate the study of omniprediction in the online adversarial setting. Although there exist algorithms for obtaining notions of multicalibration in the online adversarial setting, unlike batch algorithms, they work only for small finite classes of benchmark functions $F$, because they require enumerating every function $f \in F$ at every round. In contrast, omniprediction is most interesting for learning theoretic hypothesis classes $F$, which are generally continuously large.   We develop a new online multicalibration algorithm that is well defined for infinite benchmark classes $F$, and is oracle efficient (i.e. for any class $F$, the algorithm has the form of an efficient reduction to a no-regret learning algorithm for $F$). The result is the first efficient online omnipredictor -- an oracle efficient prediction algorithm that can be used to simultaneously obtain no regret guarantees to all Lipschitz convex loss functions. For the class $F$ of linear functions, we show how to make our algorithm efficient in the worst case. Also, we show upper and lower bounds on the extent to which our rates can be improved: our oracle efficient algorithm actually promises a stronger guarantee called swap-omniprediction, and we prove a lower bound showing that obtaining $O(\sqrt{T})$ bounds for swap-omniprediction is impossible in the online setting. On the other hand, we give a (non-oracle efficient) algorithm which can obtain the optimal $O(\sqrt{T})$ omniprediction bounds without going through multicalibration, giving an information theoretic separation between these two solution concepts.
</details>
<details>
<summary>摘要</summary>
最近的研究表明了 Multicalibration 和 Omniprediction 之间的意外联系，Multicalibration 是一种多组公平性概念，Omniprediction 是一种学习 парадиг，可以同时提供多种损失函数下的极小损失 garantías。先前的研究主要关注 Omniprediction 在批处理 setting 中。我们开始研究 Omniprediction 在在线对抗 setting 中。 existing 算法可以在在线对抗 setting 中获得 Multicalibration 的概念，但它们只适用于小型固定的 benchmark function 集合 $F$，因为它们需要在每个轮次中列出所有函数 $f \in F$。与此相比，Omniprediction 更加 interesseting，因为它可以学习理论上的假设类 $F$，这些类通常是无限大的。我们开发了一个新的在线 Multicalibration 算法，可以对无限 benchmark function 集合 $F$进行定义，并且是oracle efficient（即对于任何类 $F$，算法有形式的减少到一个不失业的学习算法 для $F$）。结果是首个 oracle efficient 的在线 omnipredictor，可以同时提供对所有 Lipschitz 几何损失函数的 no-regret  garantías。对于 linear function 集合 $F$，我们说明了如何使我们的算法高效。此外，我们还提供了上下 bounds 的证明，其中我们的 oracle efficient 算法实际上承诺了更强的 guarantee called swap-omniprediction，并且我们证明了在在线 setting 中，不可能在 $O(\sqrt{T})$ 级别上获得 swap-omniprediction。相反，我们提供了一个（非oracle efficient）算法，可以在无需 Multicalibration 的情况下获得最佳 $O(\sqrt{T})$  omniprediction  bound。这提供了一种信息理论上的分离，证明了这两个解决方案之间的不同。
</details></li>
</ul>
<hr>
<h2 id="GraphCL-DTA-a-graph-contrastive-learning-with-molecular-semantics-for-drug-target-binding-affinity-prediction"><a href="#GraphCL-DTA-a-graph-contrastive-learning-with-molecular-semantics-for-drug-target-binding-affinity-prediction" class="headerlink" title="GraphCL-DTA: a graph contrastive learning with molecular semantics for drug-target binding affinity prediction"></a>GraphCL-DTA: a graph contrastive learning with molecular semantics for drug-target binding affinity prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08989">http://arxiv.org/abs/2307.08989</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinxing Yang, Genke Yang, Jian Chu</li>
<li>for: 预测药物与Target分子之间的吸附积分，以便在药物探索的初期阶段快速地评估新药的可能性。</li>
<li>methods: 我们提出了一种基于分子图的对比学习框架——GraphCL-DTA，通过这种框架，学习药物的分子图表示，保持分子图的 semantics。此外，我们还设计了一种新的损失函数，可以直接调整药物和目标表示的均匀性。</li>
<li>results: 我们在两个真实数据集（KIBA和Davis）上验证了GraphCL-DTA的效果，结果显示GraphCL-DTA在这些数据集上表现出色，较之前的状态艺模型而言，具有更高的准确率和更好的可靠性。<details>
<summary>Abstract</summary>
Drug-target binding affinity prediction plays an important role in the early stages of drug discovery, which can infer the strength of interactions between new drugs and new targets. However, the performance of previous computational models is limited by the following drawbacks. The learning of drug representation relies only on supervised data, without taking into account the information contained in the molecular graph itself. Moreover, most previous studies tended to design complicated representation learning module, while uniformity, which is used to measure representation quality, is ignored. In this study, we propose GraphCL-DTA, a graph contrastive learning with molecular semantics for drug-target binding affinity prediction. In GraphCL-DTA, we design a graph contrastive learning framework for molecular graphs to learn drug representations, so that the semantics of molecular graphs are preserved. Through this graph contrastive framework, a more essential and effective drug representation can be learned without additional supervised data. Next, we design a new loss function that can be directly used to smoothly adjust the uniformity of drug and target representations. By directly optimizing the uniformity of representations, the representation quality of drugs and targets can be improved. The effectiveness of the above innovative elements is verified on two real datasets, KIBA and Davis. The excellent performance of GraphCL-DTA on the above datasets suggests its superiority to the state-of-the-art model.
</details>
<details>
<summary>摘要</summary>
药Target绑定亲和力预测在药物发现的早期阶段具有重要的作用，可以推断新药和新目标之间的亲和力强度。然而，先前的计算模型的性能受以下缺点限制。学习药物表示 rely only on supervised data, without considering the information contained in the molecular graph itself. In addition, most previous studies have designed complicated representation learning modules, while the uniformity of the representation quality is ignored. In this study, we propose GraphCL-DTA, a graph contrastive learning with molecular semantics for drug-target binding affinity prediction. In GraphCL-DTA, we design a graph contrastive learning framework for molecular graphs to learn drug representations, so that the semantics of molecular graphs are preserved. Through this graph contrastive framework, a more essential and effective drug representation can be learned without additional supervised data. Next, we design a new loss function that can be directly used to smoothly adjust the uniformity of drug and target representations. By directly optimizing the uniformity of representations, the representation quality of drugs and targets can be improved. The effectiveness of the above innovative elements is verified on two real datasets, KIBA and Davis. The excellent performance of GraphCL-DTA on the above datasets suggests its superiority to the state-of-the-art model.
</details></li>
</ul>
<hr>
<h2 id="Neural-Network-Pruning-as-Spectrum-Preserving-Process"><a href="#Neural-Network-Pruning-as-Spectrum-Preserving-Process" class="headerlink" title="Neural Network Pruning as Spectrum Preserving Process"></a>Neural Network Pruning as Spectrum Preserving Process</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08982">http://arxiv.org/abs/2307.08982</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shibo Yao, Dantong Yu, Ioannis Koutis</li>
<li>for: 本研究旨在提出一种基于矩阵 спектル学习的神经网络减量方法，以提高神经网络在边缘设备上的运行效率。</li>
<li>methods: 本文使用矩阵 спектル学习来分析神经网络的训练过程，并提出一种基于矩阵减量的神经网络减量算法。</li>
<li>results: 实验结果表明，该算法可以更好地保留神经网络的重要参数，并提高神经网络在边缘设备上的运行效率。<details>
<summary>Abstract</summary>
Neural networks have achieved remarkable performance in various application domains. Nevertheless, a large number of weights in pre-trained deep neural networks prohibit them from being deployed on smartphones and embedded systems. It is highly desirable to obtain lightweight versions of neural networks for inference in edge devices. Many cost-effective approaches were proposed to prune dense and convolutional layers that are common in deep neural networks and dominant in the parameter space. However, a unified theoretical foundation for the problem mostly is missing. In this paper, we identify the close connection between matrix spectrum learning and neural network training for dense and convolutional layers and argue that weight pruning is essentially a matrix sparsification process to preserve the spectrum. Based on the analysis, we also propose a matrix sparsification algorithm tailored for neural network pruning that yields better pruning result. We carefully design and conduct experiments to support our arguments. Hence we provide a consolidated viewpoint for neural network pruning and enhance the interpretability of deep neural networks by identifying and preserving the critical neural weights.
</details>
<details>
<summary>摘要</summary>
In this paper, we explore the close connection between matrix spectrum learning and neural network training for dense and convolutional layers, and argue that weight pruning is essentially a matrix sparsification process to preserve the spectrum. Based on this analysis, we propose a matrix sparsification algorithm tailored for neural network pruning that yields better pruning results.We carefully design and conduct experiments to support our arguments, providing a consolidated viewpoint for neural network pruning and enhancing the interpretability of deep neural networks by identifying and preserving the critical neural weights.
</details></li>
</ul>
<hr>
<h2 id="A-Unifying-Framework-for-Differentially-Private-Sums-under-Continual-Observation"><a href="#A-Unifying-Framework-for-Differentially-Private-Sums-under-Continual-Observation" class="headerlink" title="A Unifying Framework for Differentially Private Sums under Continual Observation"></a>A Unifying Framework for Differentially Private Sums under Continual Observation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08970">http://arxiv.org/abs/2307.08970</a></li>
<li>repo_url: None</li>
<li>paper_authors: Monika Henzinger, Jalaj Upadhyay, Sarvagya Upadhyay</li>
<li>for: 本研究考虑了维护具有不同权重的汇总数据的权衡私钥性问题，即在不断观察时维护 differentially private 的汇总数据。</li>
<li>methods: 我们提出了一种通用框架和有效的算法来解决这个问题，其适用于任何足够平滑的函数。我们的算法是首个不具有多项式误差的权衡私钥性汇总数据算法。</li>
<li>results: 我们的算法可以在不断观察时维护 differentially private 的汇总数据，并且可以 precisley  recover continual counting 问题中的误差（ Henzinger et al., SODA 2023）。我们的算法基于因子化机制，其误差取决于underlying matrix的 $\gamma_2$ 和 $\gamma_F$ 范数。我们给出了一个可构造的证明，证明了 $\gamma_2$ 和 $\gamma_F$ 范数的上界和下界。这是首次不同于所有非零元素都是相同的lower-triangular矩阵下的非тиrivial下界。<details>
<summary>Abstract</summary>
We study the problem of maintaining a differentially private decaying sum under continual observation. We give a unifying framework and an efficient algorithm for this problem for \emph{any sufficiently smooth} function. Our algorithm is the first differentially private algorithm that does not have a multiplicative error for polynomially-decaying weights. Our algorithm improves on all prior works on differentially private decaying sums under continual observation and recovers exactly the additive error for the special case of continual counting from Henzinger et al. (SODA 2023) as a corollary.   Our algorithm is a variant of the factorization mechanism whose error depends on the $\gamma_2$ and $\gamma_F$ norm of the underlying matrix. We give a constructive proof for an almost exact upper bound on the $\gamma_2$ and $\gamma_F$ norm and an almost tight lower bound on the $\gamma_2$ norm for a large class of lower-triangular matrices. This is the first non-trivial lower bound for lower-triangular matrices whose non-zero entries are not all the same. It includes matrices for all continual decaying sums problems, resulting in an upper bound on the additive error of any differentially private decaying sums algorithm under continual observation.   We also explore some implications of our result in discrepancy theory and operator algebra. Given the importance of the $\gamma_2$ norm in computer science and the extensive work in mathematics, we believe our result will have further applications.
</details>
<details>
<summary>摘要</summary>
我们研究维护具有泛化隐私衰减的总和问题在不断观察下。我们提供一个统一的框架和高效的算法来解决这个问题，该算法适用于任何足够光滑的函数。我们的算法是第一个不具有多项式误差的泛化隐私总和算法。我们的算法超越了所有以前的泛化隐私总和算法，并在特殊情况下 recover exactly  Henzinger et al.（SODA 2023）中的添加误差。我们的算法是一种因子化机制的变体，其误差取决于underlying矩阵的$\gamma_2$和$\gamma_F$范数。我们提供了一种可构造的证明，证明了一个非常接近的上界和下界，其中下界适用于一类lower-triangular矩阵。这是第一个不同于所有非零非同样的非零元素的下三角矩阵的下界。它包括所有不断观察下的总和问题矩阵，从而得到了任何泛化隐私总和算法的添加误差的Upper bound。我们还探讨了我们结果在不同误差理论和运算代数方面的应用。由于计算机科学中的$\gamma_2$范数的重要性以及数学领域的广泛工作，我们认为我们的结果将有更多应用。
</details></li>
</ul>
<hr>
<h2 id="AutoAlign-Fully-Automatic-and-Effective-Knowledge-Graph-Alignment-enabled-by-Large-Language-Models"><a href="#AutoAlign-Fully-Automatic-and-Effective-Knowledge-Graph-Alignment-enabled-by-Large-Language-Models" class="headerlink" title="AutoAlign: Fully Automatic and Effective Knowledge Graph Alignment enabled by Large Language Models"></a>AutoAlign: Fully Automatic and Effective Knowledge Graph Alignment enabled by Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11772">http://arxiv.org/abs/2307.11772</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rui Zhang, Yixin Su, Bayu Distiawan Trisedya, Xiaoyan Zhao, Min Yang, Hong Cheng, Jianzhong Qi</li>
<li>For: The paper is written for the task of entity alignment between knowledge graphs (KGs), specifically proposing a fully automatic method that does not require manually crafted seed alignments.* Methods: The method proposed in the paper is called AutoAlign, which uses predicate embeddings and entity embeddings to align entities between two KGs. Specifically, AutoAlign constructs a predicate-proximity-graph with the help of large language models to automatically capture the similarity between predicates across two KGs, and shifts the two KGs’ entity embeddings into the same vector space by computing the similarity between entities based on their attributes.* Results: The paper reports that AutoAlign improves the performance of entity alignment significantly compared to state-of-the-art methods, as demonstrated through experiments using real-world KGs.<details>
<summary>Abstract</summary>
The task of entity alignment between knowledge graphs (KGs) aims to identify every pair of entities from two different KGs that represent the same entity. Many machine learning-based methods have been proposed for this task. However, to our best knowledge, existing methods all require manually crafted seed alignments, which are expensive to obtain. In this paper, we propose the first fully automatic alignment method named AutoAlign, which does not require any manually crafted seed alignments. Specifically, for predicate embeddings, AutoAlign constructs a predicate-proximity-graph with the help of large language models to automatically capture the similarity between predicates across two KGs. For entity embeddings, AutoAlign first computes the entity embeddings of each KG independently using TransE, and then shifts the two KGs' entity embeddings into the same vector space by computing the similarity between entities based on their attributes. Thus, both predicate alignment and entity alignment can be done without manually crafted seed alignments. AutoAlign is not only fully automatic, but also highly effective. Experiments using real-world KGs show that AutoAlign improves the performance of entity alignment significantly compared to state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
知识 graphs (KGs) 的实体对应问题的任务是将两个不同 KGs 中的实体对应到同一个实体。许多机器学习基于方法已经被提出来解决这个问题。然而，我们所知道的是，现有的方法都需要手动制作的种子对应，这是贵重的。在这篇论文中，我们提出了第一个完全自动对应方法，名为 AutoAlign，不需要任何手动制作的种子对应。特别是，对于 predicate 嵌入，AutoAlign 使用大型自然语言模型来自动捕捉两个 KGs 中 predicate 之间的相似性。对于实体嵌入，AutoAlign 先使用 TransE 来独立计算每个 KG 中的实体嵌入，然后通过计算实体之间的属性相似性来将两个 KGs 的实体嵌入Shift到同一个向量空间中。因此， predicate 对应和实体对应都可以不需要手动制作种子对应。AutoAlign 不仅是完全自动的，还非常有效。使用实际世界 KGs 的实验表明，AutoAlign 在对entity alignment进行比对state-of-the-art方法时有显著改进。
</details></li>
</ul>
<hr>
<h2 id="Landscape-Surrogate-Learning-Decision-Losses-for-Mathematical-Optimization-Under-Partial-Information"><a href="#Landscape-Surrogate-Learning-Decision-Losses-for-Mathematical-Optimization-Under-Partial-Information" class="headerlink" title="Landscape Surrogate: Learning Decision Losses for Mathematical Optimization Under Partial Information"></a>Landscape Surrogate: Learning Decision Losses for Mathematical Optimization Under Partial Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08964">http://arxiv.org/abs/2307.08964</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/lancer">https://github.com/facebookresearch/lancer</a></li>
<li>paper_authors: Arman Zharmagambetov, Brandon Amos, Aaron Ferber, Taoan Huang, Bistra Dilkina, Yuandong Tian</li>
<li>for: 优化问题中的部分观察或通用优化器表现不佳，学习一个优化器 $\mathbf{g}$ 来解决这些问题，可以快速加速优化过程，并且可以利用过去经验。</li>
<li>methods: 使用一个可学习的地形代理 $M$ 来取代 $f\circ \mathbf{g}$，这个地形代理可以更快速地计算，提供稠密和平滑的梯度，可以泛化到未看到的优化问题，并通过分布式优化来高效地学习。</li>
<li>results: 在 sintetic 问题和实际问题上测试了我们的方法，比如短路和多维零链包，和股票配置优化，与状态 искусственный基线相比，我们的方法可以达到相同或更高的目标值，同时减少了对 $\mathbf{g}$ 的调用数量。特别是，我们的方法在高维ensional computationally expensive 问题上表现出色。<details>
<summary>Abstract</summary>
Recent works in learning-integrated optimization have shown promise in settings where the optimization problem is only partially observed or where general-purpose optimizers perform poorly without expert tuning. By learning an optimizer $\mathbf{g}$ to tackle these challenging problems with $f$ as the objective, the optimization process can be substantially accelerated by leveraging past experience. The optimizer can be trained with supervision from known optimal solutions or implicitly by optimizing the compound function $f\circ \mathbf{g}$. The implicit approach may not require optimal solutions as labels and is capable of handling problem uncertainty; however, it is slow to train and deploy due to frequent calls to optimizer $\mathbf{g}$ during both training and testing. The training is further challenged by sparse gradients of $\mathbf{g}$, especially for combinatorial solvers. To address these challenges, we propose using a smooth and learnable Landscape Surrogate $M$ as a replacement for $f\circ \mathbf{g}$. This surrogate, learnable by neural networks, can be computed faster than the solver $\mathbf{g}$, provides dense and smooth gradients during training, can generalize to unseen optimization problems, and is efficiently learned via alternating optimization. We test our approach on both synthetic problems, including shortest path and multidimensional knapsack, and real-world problems such as portfolio optimization, achieving comparable or superior objective values compared to state-of-the-art baselines while reducing the number of calls to $\mathbf{g}$. Notably, our approach outperforms existing methods for computationally expensive high-dimensional problems.
</details>
<details>
<summary>摘要</summary>
现代学习整合优化技术已经在部分 observable 的优化问题或通用优化器无需专家调整时表现出了搭配性。通过学习一个优化器 $\mathbf{g}$，以便在 $f$ 作为目标函数下解决这些复杂的问题，优化过程可以大幅加速。可以通过知识到的优化解决方案或间接地将 $f\circ \mathbf{g}$ 作为整合函数来训练优化器。 indirect approach 可能不需要优化解决方案作为标签，并且可以处理问题不确定性，但是它在训练和部署时间较慢，因为在训练和测试中频繁地调用优化器 $\mathbf{g}$。训练还面临着 $\mathbf{g}$ 的稀疏梯度问题，特别是对 combinatorial 解决器。为解决这些挑战，我们提出使用一个缓和可学习的景观准则 $M$，作为 $f\circ \mathbf{g}$ 的替代品。这个准则可以通过神经网络学习，在训练时间更快，提供稠密和平滑的梯度，可以泛化到未见优化问题，并通过相关优化来有效地学习。我们在 sintetic 问题和实际问题上进行了测试，包括短路和多维锦包，并取得了与状态艺术基准相同或更高的目标值，同时减少了对 $\mathbf{g}$ 的调用数量。特别是，我们的方法在高维计算成本高的问题上表现出色。
</details></li>
</ul>
<hr>
<h2 id="REX-Rapid-Exploration-and-eXploitation-for-AI-Agents"><a href="#REX-Rapid-Exploration-and-eXploitation-for-AI-Agents" class="headerlink" title="REX: Rapid Exploration and eXploitation for AI Agents"></a>REX: Rapid Exploration and eXploitation for AI Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08962">http://arxiv.org/abs/2307.08962</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rithesh Murthy, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Le Xue, Weiran Yao, Yihao Feng, Zeyuan Chen, Akash Gokul, Devansh Arpit, Ran Xu, Phil Mui, Huan Wang, Caiming Xiong, Silvio Savarese</li>
<li>for: 提高AI代理的快速探索和尝试能力，解决现有AutoGPT风格技术的缺陷，如偏重精确描述的决策和缺乏有系统的尝试和失败处理方式。</li>
<li>methods: 提出一种增强的Rapid Exploration and eXploitation（REX）方法，通过添加奖励层和基于Upper Confidence Bound（UCB）的概念，使AI代理性能更加稳定和高效。REX方法不需要模型细化，可以利用日志数据，并与现有基础模型协作无缝。</li>
<li>results:  Comparative analysis表明，使用REX方法可以与现有方法（如Chain-of-Thoughts（CoT）和Reasoning viA Planning（RAP））相比，在一些情况下甚至超越其表现，同时具有显著减少执行时间的优点，提高了在多样化场景下的实际应用性。<details>
<summary>Abstract</summary>
In this paper, we propose an enhanced approach for Rapid Exploration and eXploitation for AI Agents called REX. Existing AutoGPT-style techniques have inherent limitations, such as a heavy reliance on precise descriptions for decision-making, and the lack of a systematic approach to leverage try-and-fail procedures akin to traditional Reinforcement Learning (RL). REX introduces an additional layer of rewards and integrates concepts similar to Upper Confidence Bound (UCB) scores, leading to more robust and efficient AI agent performance. This approach has the advantage of enabling the utilization of offline behaviors from logs and allowing seamless integration with existing foundation models while it does not require any model fine-tuning. Through comparative analysis with existing methods such as Chain-of-Thoughts(CoT) and Reasoning viA Planning(RAP), REX-based methods demonstrate comparable performance and, in certain cases, even surpass the results achieved by these existing techniques. Notably, REX-based methods exhibit remarkable reductions in execution time, enhancing their practical applicability across a diverse set of scenarios.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种改进后的快速探索和努力（Rapid Exploration and eXploitation，REX）方法，用于AI代理。现有的AutoGPT样式技术存在一定的限制，如准确描述的重要性和RL的缺乏系统化适应。REX增加了一层奖励，并将Upper Confidence Bound（UCB）类概念纳入方法中，从而使AI代理表现更加稳定和高效。这种方法具有使用日志中的假日行为，无需模型细化而可以与现有基础模型集成，并且在比较分析中与现有方法（Chain-of-Thoughts（CoT）和Reasoning viA Planning（RAP）） demonstrate了相似或者甚至超过其表现。尤其是，REX基本方法在执行时间方面具有显著的减少，使其在多样化的情况下更加实用。
</details></li>
</ul>
<hr>
<h2 id="Discretization-based-ensemble-model-for-robust-learning-in-IoT"><a href="#Discretization-based-ensemble-model-for-robust-learning-in-IoT" class="headerlink" title="Discretization-based ensemble model for robust learning in IoT"></a>Discretization-based ensemble model for robust learning in IoT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08955">http://arxiv.org/abs/2307.08955</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anahita Namvar, Chandra Thapa, Salil S. Kanhere</li>
<li>for: 提高 IoT 设备识别模型的安全性，抵御黑盒和白盒攻击。</li>
<li>methods:  integrate discretization techniques and ensemble methods to improve the robustness of machine learning models for IoT device identification.</li>
<li>results: 提高了 ML 模型对 IoT 设备识别的可靠性和安全性，抵御了黑盒和白盒攻击。<details>
<summary>Abstract</summary>
IoT device identification is the process of recognizing and verifying connected IoT devices to the network. This is an essential process for ensuring that only authorized devices can access the network, and it is necessary for network management and maintenance. In recent years, machine learning models have been used widely for automating the process of identifying devices in the network. However, these models are vulnerable to adversarial attacks that can compromise their accuracy and effectiveness. To better secure device identification models, discretization techniques enable reduction in the sensitivity of machine learning models to adversarial attacks contributing to the stability and reliability of the model. On the other hand, Ensemble methods combine multiple heterogeneous models to reduce the impact of remaining noise or errors in the model. Therefore, in this paper, we integrate discretization techniques and ensemble methods and examine it on model robustness against adversarial attacks. In other words, we propose a discretization-based ensemble stacking technique to improve the security of our ML models. We evaluate the performance of different ML-based IoT device identification models against white box and black box attacks using a real-world dataset comprised of network traffic from 28 IoT devices. We demonstrate that the proposed method enables robustness to the models for IoT device identification.
</details>
<details>
<summary>摘要</summary>
互联网物联网设备识别是将连接到网络的互联网设备识别和验证的过程。这是确保只允许授权的设备访问网络的 essencial 过程，并对网络管理和维护是必需的。在过去几年中，机器学习模型广泛用于自动化网络中设备识别的过程。然而，这些模型容易受到恶意攻击的影响，这可能会降低其精度和有效性。为了更好地安全设备识别模型，精度技术可以减少机器学习模型对恶意攻击的敏感度，从而提高模型的稳定性和可靠性。此外，组合多种不同的模型可以减少剩下的噪音或错误的影响。因此，在这篇论文中，我们将精度技术和组合方法结合使用，并对其在模型对恶意攻击的Robustness进行评估。即我们提出了一种基于精度的集成堆叠技术，以提高互联网设备识别模型的安全性。我们使用了一个实际的网络流量数据集，包含28个互联网设备的网络流量，对不同的机器学习基于互联网设备识别模型进行了白盒和黑盒攻击的评估。我们的结果表明，我们的提议的方法可以提高互联网设备识别模型的Robustness。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-infused-Deep-Learning-Enables-Interpretable-Landslide-Forecasting"><a href="#Knowledge-infused-Deep-Learning-Enables-Interpretable-Landslide-Forecasting" class="headerlink" title="Knowledge-infused Deep Learning Enables Interpretable Landslide Forecasting"></a>Knowledge-infused Deep Learning Enables Interpretable Landslide Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08951">http://arxiv.org/abs/2307.08951</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengjing Ma, Gang Mei</li>
<li>for: 预测山崩发展和失败的可能性是一项复杂的任务，因为它们受到许多内部和外部因素的影响。</li>
<li>methods: 这篇文章使用了一种名为LFIT的转换器基本深度学习网络，该网络可以学习非线性关系，并且具有可读性和多源数据处理能力。</li>
<li>results: 文章表明，通过结合先前知识，可以提高整体山崩预测，并且可以捕捉不同地区的山崩行为和时间模式。通过使用塑形变形观测数据，文章验证了该方法的可靠性和可读性。<details>
<summary>Abstract</summary>
Forecasting how landslides will evolve over time or whether they will fail is a challenging task due to a variety of factors, both internal and external. Despite their considerable potential to address these challenges, deep learning techniques lack interpretability, undermining the credibility of the forecasts they produce. The recent development of transformer-based deep learning offers untapped possibilities for forecasting landslides with unprecedented interpretability and nonlinear feature learning capabilities. Here, we present a deep learning pipeline that is capable of predicting landslide behavior holistically, which employs a transformer-based network called LFIT to learn complex nonlinear relationships from prior knowledge and multiple source data, identifying the most relevant variables, and demonstrating a comprehensive understanding of landslide evolution and temporal patterns. By integrating prior knowledge, we provide improvement in holistic landslide forecasting, enabling us to capture diverse responses to various influencing factors in different local landslide areas. Using deformation observations as proxies for measuring the kinetics of landslides, we validate our approach by training models to forecast reservoir landslides in the Three Gorges Reservoir and creeping landslides on the Tibetan Plateau. When prior knowledge is incorporated, we show that interpretable landslide forecasting effectively identifies influential factors across various landslides. It further elucidates how local areas respond to these factors, making landslide behavior and trends more interpretable and predictable. The findings from this study will contribute to understanding landslide behavior in a new way and make the proposed approach applicable to other complex disasters influenced by internal and external factors in the future.
</details>
<details>
<summary>摘要</summary>
预测滑坡的发展趋势或是否会失败是一项复杂的任务，因为它们受到多种内部和外部因素的影响。 DESPITE THEIR POTENTIAL TO ADDRESS THESE CHALLENGES, deep learning techniques lack interpretability, which undermines the credibility of the forecasts they produce.  However, the recent development of transformer-based deep learning offers untapped possibilities for forecasting landslides with unprecedented interpretability and nonlinear feature learning capabilities. 在这种情况下，我们提出了一个深度学习管道，可以捕捉滑坡的行为的整体特征，该管道使用名为LFIT的变换器基于网络，可以学习复杂的非线性关系，并且可以确定最重要的变量。通过结合先前知识，我们提供了改进的总体滑坡预测方法，可以捕捉不同的滑坡区域响应不同的外部因素的多样化响应。使用塑形观察作为滑坡动力的代理，我们验证了我们的方法，通过在三峡水库和藏北高原的滑坡中训练模型，预测滑坡的发展趋势。当嵌入先前知识时，我们显示出可解释的滑坡预测方法可以准确地确定影响滑坡的因素，并且可以解释不同的滑坡区域如何响应这些因素，使滑坡行为和趋势更加可解释和预测。这些发现将在未来对其他复杂的自然灾害，受到内部和外部因素影响的灾害中应用。
</details></li>
</ul>
<hr>
<h2 id="Alioth-A-Machine-Learning-Based-Interference-Aware-Performance-Monitor-for-Multi-Tenancy-Applications-in-Public-Cloud"><a href="#Alioth-A-Machine-Learning-Based-Interference-Aware-Performance-Monitor-for-Multi-Tenancy-Applications-in-Public-Cloud" class="headerlink" title="Alioth: A Machine Learning Based Interference-Aware Performance Monitor for Multi-Tenancy Applications in Public Cloud"></a>Alioth: A Machine Learning Based Interference-Aware Performance Monitor for Multi-Tenancy Applications in Public Cloud</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08949">http://arxiv.org/abs/2307.08949</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sthowling/alioth">https://github.com/sthowling/alioth</a></li>
<li>paper_authors: Tianyao Shi, Yingxuan Yang, Yunlong Cheng, Xiaofeng Gao, Zhen Fang, Yongqiang Yang<br>for: This paper aims to monitor the performance degradation of cloud applications in public clouds caused by co-location interference.methods: The proposed method, Alioth, uses a novel machine learning framework that includes interference generators, denoising auto-encoders, domain adaptation neural networks, and SHAP explainers to monitor performance degradation.results: Alioth achieves an average mean absolute error of 5.29% offline and 10.8% when testing on applications unseen in the training stage, outperforming baseline methods. It also demonstrates robustness in signaling quality-of-service violation under dynamicity.<details>
<summary>Abstract</summary>
Multi-tenancy in public clouds may lead to co-location interference on shared resources, which possibly results in performance degradation of cloud applications. Cloud providers want to know when such events happen and how serious the degradation is, to perform interference-aware migrations and alleviate the problem. However, virtual machines (VM) in Infrastructure-as-a-Service public clouds are black-boxes to providers, where application-level performance information cannot be acquired. This makes performance monitoring intensely challenging as cloud providers can only rely on low-level metrics such as CPU usage and hardware counters.   We propose a novel machine learning framework, Alioth, to monitor the performance degradation of cloud applications. To feed the data-hungry models, we first elaborate interference generators and conduct comprehensive co-location experiments on a testbed to build Alioth-dataset which reflects the complexity and dynamicity in real-world scenarios. Then we construct Alioth by (1) augmenting features via recovering low-level metrics under no interference using denoising auto-encoders, (2) devising a transfer learning model based on domain adaptation neural network to make models generalize on test cases unseen in offline training, and (3) developing a SHAP explainer to automate feature selection and enhance model interpretability. Experiments show that Alioth achieves an average mean absolute error of 5.29% offline and 10.8% when testing on applications unseen in the training stage, outperforming the baseline methods. Alioth is also robust in signaling quality-of-service violation under dynamicity. Finally, we demonstrate a possible application of Alioth's interpretability, providing insights to benefit the decision-making of cloud operators. The dataset and code of Alioth have been released on GitHub.
</details>
<details>
<summary>摘要</summary>
多重租户在公共云上可能导致共享资源上的干扰，从而导致云应用程序的性能下降。云提供商希望在这些事件发生时知道其严重程度，以进行干扰意识的迁移和缓解问题。然而，基础设施协议（IaaS）云公共云中的虚拟机（VM）对提供商是黑洞，无法获得应用层性能信息。这使得性能监测变得非常困难，cloud提供商只能依靠低级别指标，如CPU使用率和硬件计数器。我们提出了一种新的机器学习框架，称为Alioth，用于监测云应用程序的性能下降。为了充实数据鲁棒的模型，我们首先 elaborated interference generators和在testbed上进行了广泛的共享实验，以建立Alioth-dataset，该集合反映了实际场景中的复杂性和动态性。然后，我们构建了Alioth，包括以下三个主要组成部分：1. 通过使用降噪自适应神经网络恢复低级别指标，扩展特征。2. 基于域 adapted神经网络模型，以便模型在测试 случаeschannel unseen in offline training中 generalize。3. 开发 SHAP解释器，以自动选择特征和提高模型解释性。实验表明，Alioth在线上和离线上的平均绝对误差为5.29%和10.8% respectively，比基eline方法优化。此外，Alioth在动态环境下也具有可靠的质量服务预测能力。最后，我们示出了Alioth的解释性可以为云运维师提供有价值的决策指导。Alioth-dataset和相关代码已经在GitHub上发布。
</details></li>
</ul>
<hr>
<h2 id="Mitigating-Label-Bias-via-Decoupled-Confident-Learning"><a href="#Mitigating-Label-Bias-via-Decoupled-Confident-Learning" class="headerlink" title="Mitigating Label Bias via Decoupled Confident Learning"></a>Mitigating Label Bias via Decoupled Confident Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08945">http://arxiv.org/abs/2307.08945</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunyi Li, Maria De-Arteaga, Maytal Saar-Tsechansky</li>
<li>for: 本研究旨在提出一种特点是适应标签偏见的分类方法，以便在涉及重要领域中减少算法偏见。</li>
<li>methods: 本研究提出了一种名为分离信心学习（DeCoLe）的遮盾方法，该方法可以减少标签偏见的影响。</li>
<li>results: 在一个Synthetic数据集上测试了DeCoLe方法，结果显示其能够成功地检测出偏见标签，并且在仇恨言语识别任务中超过其他方法表现。<details>
<summary>Abstract</summary>
Growing concerns regarding algorithmic fairness have led to a surge in methodologies to mitigate algorithmic bias. However, such methodologies largely assume that observed labels in training data are correct. This is problematic because bias in labels is pervasive across important domains, including healthcare, hiring, and content moderation. In particular, human-generated labels are prone to encoding societal biases. While the presence of labeling bias has been discussed conceptually, there is a lack of methodologies to address this problem. We propose a pruning method -- Decoupled Confident Learning (DeCoLe) -- specifically designed to mitigate label bias. After illustrating its performance on a synthetic dataset, we apply DeCoLe in the context of hate speech detection, where label bias has been recognized as an important challenge, and show that it successfully identifies biased labels and outperforms competing approaches.
</details>
<details>
<summary>摘要</summary>
algorithmic fairness的问题在不断增加，这导致了一系列方法来减少算法偏见。然而，这些方法假设训练数据中的标签是正确的。这是一个问题，因为标签中的偏见是广泛存在的，包括医疗、招聘和内容审核等重要领域。人类生成的标签容易带有社会偏见。虽然标签偏见的存在已经被讨论，但是没有有效的方法来解决这个问题。我们提出了一种剪裁方法——分离信任学习（DeCoLe），特意设计来减少标签偏见。我们在一个 sintetic 数据集上验证了 DeCoLe 的性能，然后在仇恨言语检测中应用了 DeCoLe，并证明它成功地标识了偏见标签，并超过了竞争方法的性能。
</details></li>
</ul>
<hr>
<h2 id="Siamese-Networks-for-Weakly-Supervised-Human-Activity-Recognition"><a href="#Siamese-Networks-for-Weakly-Supervised-Human-Activity-Recognition" class="headerlink" title="Siamese Networks for Weakly Supervised Human Activity Recognition"></a>Siamese Networks for Weakly Supervised Human Activity Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08944">http://arxiv.org/abs/2307.08944</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taoran Sheng, Manfred Huber</li>
<li>for: 这篇论文旨在应用深度学习于人体活动识别，但是训练深度神经网络需要大量的明确标注数据，这是困难的获得。</li>
<li>methods: 这篇论文提出了一种使用多个同构网络进行训练，只使用数据对的相似性信息来训练模型，从而生成一个可以作为各种各样的聚类算法的度量模型。</li>
<li>results: 论文在三个数据集上进行了评估，并证明了模型的效果iveness在分类和识别连续人体活动序列中。<details>
<summary>Abstract</summary>
Deep learning has been successfully applied to human activity recognition. However, training deep neural networks requires explicitly labeled data which is difficult to acquire. In this paper, we present a model with multiple siamese networks that are trained by using only the information about the similarity between pairs of data samples without knowing the explicit labels. The trained model maps the activity data samples into fixed size representation vectors such that the distance between the vectors in the representation space approximates the similarity of the data samples in the input space. Thus, the trained model can work as a metric for a wide range of different clustering algorithms. The training process minimizes a similarity loss function that forces the distance metric to be small for pairs of samples from the same kind of activity, and large for pairs of samples from different kinds of activities. We evaluate the model on three datasets to verify its effectiveness in segmentation and recognition of continuous human activity sequences.
</details>
<details>
<summary>摘要</summary>
深度学习已成功应用于人类活动识别。然而，训练深度神经网络需要明确标注的数据，具体来说是困难的获得。在这篇论文中，我们提出了一种使用多个同构网络进行训练，不需要明确标注数据。训练模型将活动数据样本映射到固定大小的表示向量中，使得表示空间中的距离 approximates 输入空间中的相似性。因此，训练模型可以作为各种不同的聚类算法的度量。训练过程中 minimizes 一个相似损失函数，该函数让距离度量在同类活动样本对应的情况下很小，并在不同类活动样本对应的情况下很大。我们在三个数据集上验证了模型的有效性，以确认其在连续人类活动序列的分割和识别方面的表现。
</details></li>
</ul>
<hr>
<h2 id="NTK-approximating-MLP-Fusion-for-Efficient-Language-Model-Fine-tuning"><a href="#NTK-approximating-MLP-Fusion-for-Efficient-Language-Model-Fine-tuning" class="headerlink" title="NTK-approximating MLP Fusion for Efficient Language Model Fine-tuning"></a>NTK-approximating MLP Fusion for Efficient Language Model Fine-tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08941">http://arxiv.org/abs/2307.08941</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/weitianxin/mlp_fusion">https://github.com/weitianxin/mlp_fusion</a></li>
<li>paper_authors: Tianxin Wei, Zeming Guo, Yifan Chen, Jingrui He</li>
<li>for: 这篇论文旨在提出一种通过NTK数据来实现简化预训练语言模型（PLM）的方法，以减少PLM的计算和内存需求。</li>
<li>methods: 本文使用NTK几何来检查PLM的多层感知器（MLP）模组，并提出一种通过将MLP装置为一些中心的组合来实现轻量级PLM的方法。</li>
<li>results: 实验结果显示，该方法可以实现PLM的简化，并在自然语言理解（NLU）和生成（NLG）任务上进行了有效的调整。<details>
<summary>Abstract</summary>
Fine-tuning a pre-trained language model (PLM) emerges as the predominant strategy in many natural language processing applications. However, even fine-tuning the PLMs and doing inference are expensive, especially on edge devices with low computing power. Some general approaches (e.g. quantization and distillation) have been widely studied to reduce the compute/memory of PLM fine-tuning, while very few one-shot compression techniques are explored. In this paper, we investigate the neural tangent kernel (NTK)--which reveals the gradient descent dynamics of neural networks--of the multilayer perceptrons (MLP) modules in a PLM and propose to coin a lightweight PLM through NTK-approximating MLP fusion. To achieve this, we reconsider the MLP as a bundle of sub-MLPs, and cluster them into a given number of centroids, which can then be restored as a compressed MLP and surprisingly shown to well approximate the NTK of the original PLM. Extensive experiments of PLM fine-tuning on both natural language understanding (NLU) and generation (NLG) tasks are provided to verify the effectiveness of the proposed method MLP fusion. Our code is available at https://github.com/weitianxin/MLP_Fusion.
</details>
<details>
<summary>摘要</summary>
大多数自然语言处理应用中出现了调整预训练语言模型（PLM）的方法。然而，即使是调整PLM和做出判断都是昂贵的，特别是在边缘设备上进行。一些通用的方法（如量化和液化）已经广泛研究以降低PLM调整的计算/存储量，而很少有一次性压缩技术被探索。在这篇论文中，我们研究了神经积分析（NTK）——描述神经网络的梯度下降动力学——的多层感知器（MLP）模块在PLM中，并提议通过NTK-近似MLP融合来创造轻量级PLM。为此，我们重新考虑MLP为一个分解成多个子MLP的Bundle，并将其分成一定数量的中心点，然后可以将其Restore为压缩MLP，并意外地发现可以良好地近似原PLM的NTK。我们提供了大量PLM精度调整NLU和NLG任务的实验来证明提议的方法的有效性。我们的代码可以在https://github.com/weitianxin/MLP_Fusion上找到。
</details></li>
</ul>
<hr>
<h2 id="Experimental-Security-Analysis-of-DNN-based-Adaptive-Cruise-Control-under-Context-Aware-Perception-Attacks"><a href="#Experimental-Security-Analysis-of-DNN-based-Adaptive-Cruise-Control-under-Context-Aware-Perception-Attacks" class="headerlink" title="Experimental Security Analysis of DNN-based Adaptive Cruise Control under Context-Aware Perception Attacks"></a>Experimental Security Analysis of DNN-based Adaptive Cruise Control under Context-Aware Perception Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08939">http://arxiv.org/abs/2307.08939</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xugui Zhou, Anqi Chen, Maxfield Kouzel, Haotian Ren, Morgan McCarty, Cristina Nita-Rotaru, Homa Alemzadeh</li>
<li>for: 评估深度神经网络（DNN）基于自适应巡航控制（ACC）系统的安全性，以防止恶意投毒攻击引起前方碰撞。</li>
<li>methods: 提出了一种结合知识驱动和数据驱动的方法，用于在攻击时选择最 kritical时刻，以及一种基于优化的方法来在运行时生成适应性的图像偏移。</li>
<li>results: 通过实验和实际驱动 simulator  Platform 和生产 ACC 系统，发现提案的攻击可以 achiev 142.9x 高的成功率，同时受到安全功能（如自动紧急刹车和前方碰撞预警）的干扰减少了89.6%。这种攻击 Robust 到实际世界因素和环境动态变化，同时能够避免被发现。这种研究提供了人类运行员和基本安全功能的抗攻击策略。<details>
<summary>Abstract</summary>
Adaptive Cruise Control (ACC) is a widely used driver assistance feature for maintaining desired speed and safe distance to the leading vehicles. This paper evaluates the security of the deep neural network (DNN) based ACC systems under stealthy perception attacks that strategically inject perturbations into camera data to cause forward collisions. We present a combined knowledge-and-data-driven approach to design a context-aware strategy for the selection of the most critical times for triggering the attacks and a novel optimization-based method for the adaptive generation of image perturbations at run-time. We evaluate the effectiveness of the proposed attack using an actual driving dataset and a realistic simulation platform with the control software from a production ACC system and a physical-world driving simulator while considering interventions by the driver and safety features such as Automatic Emergency Braking (AEB) and Forward Collision Warning (FCW). Experimental results show that the proposed attack achieves 142.9x higher success rate in causing accidents than random attacks and is mitigated 89.6% less by the safety features while being stealthy and robust to real-world factors and dynamic changes in the environment. This study provides insights into the role of human operators and basic safety interventions in preventing attacks.
</details>
<details>
<summary>摘要</summary>
这篇研究评估了基于深度神经网络（DNN）的自适应巡航控制（ACC）系统的安全性，以及这些系统对于隐藏式感知攻击的抵抗力。我们提出了一种结合知识驱动和数据驱动的方法，以选择最重要的时刻进行攻击，并且使用优化方法生成Run-time中的像素噪声。我们使用实际驾驶数据和真实的驾驶 simulate平台，考虑到驾驶员的干预和安全功能，例如自动紧急刹车（AEB）和前方冲击警示（FCW）。实验结果显示，我们的攻击成功率高于随机攻击的142.9倍，并且受到安全功能的抑制89.6%。此研究给出了人类驾驶员和基本安全功能的防御效果。
</details></li>
</ul>
<hr>
<h2 id="Multi-stage-Neural-Networks-Function-Approximator-of-Machine-Precision"><a href="#Multi-stage-Neural-Networks-Function-Approximator-of-Machine-Precision" class="headerlink" title="Multi-stage Neural Networks: Function Approximator of Machine Precision"></a>Multi-stage Neural Networks: Function Approximator of Machine Precision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08934">http://arxiv.org/abs/2307.08934</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongji Wang, Ching-Yao Lai</li>
<li>for: 这 paper 是为了提高神经网络在科学问题中的精度，并且使用多stage neural networks来mitigate spectral biases。</li>
<li>methods: 这 paper 使用了多stage neural networks，将训练过程分成不同的阶段，每个阶段使用一个新的网络来适应剩下的差异。</li>
<li>results: 这 paper 表明，使用多stage neural networks可以减少预测错误至O(10^{-16})水平，这是单个神经网络很难达到的精度。<details>
<summary>Abstract</summary>
Deep learning techniques are increasingly applied to scientific problems, where the precision of networks is crucial. Despite being deemed as universal function approximators, neural networks, in practice, struggle to reduce the prediction errors below $O(10^{-5})$ even with large network size and extended training iterations. To address this issue, we developed the multi-stage neural networks that divides the training process into different stages, with each stage using a new network that is optimized to fit the residue from the previous stage. Across successive stages, the residue magnitudes decreases substantially and follows an inverse power-law relationship with the residue frequencies. The multi-stage neural networks effectively mitigate the spectral biases associated with regular neural networks, enabling them to capture the high frequency feature of target functions. We demonstrate that the prediction error from the multi-stage training for both regression problems and physics-informed neural networks can nearly reach the machine-precision $O(10^{-16})$ of double-floating point within a finite number of iterations. Such levels of accuracy are rarely attainable using single neural networks alone.
</details>
<details>
<summary>摘要</summary>
深度学习技术在科学问题中越来越广泛应用，其精度的网络是关键。尽管被认为是通用函数近似器，实际上，神经网络在实践中难以降低预测错误 Below $O(10^{-5})$，即使使用大型网络和长时间训练轮次。为解决这问题，我们开发了多 stage 神经网络，将训练过程分解成不同的阶段，每个阶段使用新的网络，该网络是适应前一阶段剩余的。在 successive 阶段中，剩余大小减少了很多，并且与剩余频率关系为 inverse power-law 关系。多 stage 神经网络有效地 mitigate 神经网络的 спектраль偏好，使其能够捕捉目标函数的高频特征。我们示出，使用多 stage 训练，对于回归问题和物理学 Informed neural networks 的预测错误可以几乎达到机器精度 $O(10^{-16})$ 的水平，这些精度在单个神经网络alone 中 rarely 可以达到。
</details></li>
</ul>
<hr>
<h2 id="IxDRL-A-Novel-Explainable-Deep-Reinforcement-Learning-Toolkit-based-on-Analyses-of-Interestingness"><a href="#IxDRL-A-Novel-Explainable-Deep-Reinforcement-Learning-Toolkit-based-on-Analyses-of-Interestingness" class="headerlink" title="IxDRL: A Novel Explainable Deep Reinforcement Learning Toolkit based on Analyses of Interestingness"></a>IxDRL: A Novel Explainable Deep Reinforcement Learning Toolkit based on Analyses of Interestingness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08933">http://arxiv.org/abs/2307.08933</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sri-aic/23-xai-ixdrl-data">https://github.com/sri-aic/23-xai-ixdrl-data</a></li>
<li>paper_authors: Pedro Sequeira, Melinda Gervasio<br>for:The paper aims to provide a more explainable deep reinforcement learning (xDRL) framework to help human operators understand the competence of RL agents in complex decision-making tasks.methods:The proposed framework is based on interestingness analysis and is applicable to a wide range of RL algorithms, natively supporting the popular RLLib toolkit.results:The approach can identify agent behavior patterns and competency-controlling conditions, and the task elements mostly responsible for an agent’s competence, based on global and local analyses of interestingness. The framework provides agent designers with insights about RL agent competence, enabling more informed decisions about interventions, additional training, and other interactions in collaborative human-machine settings.<details>
<summary>Abstract</summary>
In recent years, advances in deep learning have resulted in a plethora of successes in the use of reinforcement learning (RL) to solve complex sequential decision tasks with high-dimensional inputs. However, existing systems lack the necessary mechanisms to provide humans with a holistic view of their competence, presenting an impediment to their adoption, particularly in critical applications where the decisions an agent makes can have significant consequences. Yet, existing RL-based systems are essentially competency-unaware in that they lack the necessary interpretation mechanisms to allow human operators to have an insightful, holistic view of their competency. Towards more explainable Deep RL (xDRL), we propose a new framework based on analyses of interestingness. Our tool provides various measures of RL agent competence stemming from interestingness analysis and is applicable to a wide range of RL algorithms, natively supporting the popular RLLib toolkit. We showcase the use of our framework by applying the proposed pipeline in a set of scenarios of varying complexity. We empirically assess the capability of the approach in identifying agent behavior patterns and competency-controlling conditions, and the task elements mostly responsible for an agent's competence, based on global and local analyses of interestingness. Overall, we show that our framework can provide agent designers with insights about RL agent competence, both their capabilities and limitations, enabling more informed decisions about interventions, additional training, and other interactions in collaborative human-machine settings.
</details>
<details>
<summary>摘要</summary>
To address this issue, we propose a new framework based on interestingness analysis to provide a more explainable Deep RL (xDRL) system. Our tool provides various measures of RL agent competence stemming from interestingness analysis and is applicable to a wide range of RL algorithms, natively supporting the popular RLLib toolkit.We demonstrate the use of our framework by applying it to a set of scenarios of varying complexity. We empirically assess the capability of the approach in identifying agent behavior patterns and competency-controlling conditions, as well as the task elements most responsible for an agent's competence, based on global and local analyses of interestingness.Our framework provides agent designers with insights into RL agent competence, including their capabilities and limitations, enabling more informed decisions about interventions, additional training, and other interactions in collaborative human-machine settings.
</details></li>
</ul>
<hr>
<h2 id="Submodular-Maximization-under-the-Intersection-of-Matroid-and-Knapsack-Constraints"><a href="#Submodular-Maximization-under-the-Intersection-of-Matroid-and-Knapsack-Constraints" class="headerlink" title="Submodular Maximization under the Intersection of Matroid and Knapsack Constraints"></a>Submodular Maximization under the Intersection of Matroid and Knapsack Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09487">http://arxiv.org/abs/2307.09487</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu-Ran Gu, Chao Bian, Chao Qian</li>
<li>for: 本文研究的目的是解决具有 intersection of $k$-matroid constraint和$m$-knapsack constraint的submodular maximization问题。</li>
<li>methods: 作者提出了一种名为SPROUT的新算法，通过将partial enumeration incorporated into the simultaneous greedy framework来解决该问题。</li>
<li>results: 作者证明了SPROUT可以在几乎polynomial时间内实现更好的approximation guarantee，并通过引入随机枚举和矫正技术，开发出了SPROUT++算法，在实践中具有更高的效率和相似的approximation guarantee。<details>
<summary>Abstract</summary>
Submodular maximization arises in many applications, and has attracted a lot of research attentions from various areas such as artificial intelligence, finance and operations research. Previous studies mainly consider only one kind of constraint, while many real-world problems often involve several constraints. In this paper, we consider the problem of submodular maximization under the intersection of two commonly used constraints, i.e., $k$-matroid constraint and $m$-knapsack constraint, and propose a new algorithm SPROUT by incorporating partial enumeration into the simultaneous greedy framework. We prove that SPROUT can achieve a polynomial-time approximation guarantee better than the state-of-the-art algorithms. Then, we introduce the random enumeration and smooth techniques into SPROUT to improve its efficiency, resulting in the SPROUT++ algorithm, which can keep a similar approximation guarantee. Experiments on the applications of movie recommendation and weighted max-cut demonstrate the superiority of SPROUT++ in practice.
</details>
<details>
<summary>摘要</summary>
<<SYS>>设<latex>k</latex>和<latex>m</latex>为两个正整数，我们考虑一个问题：在<latex>k</latex>-matroid约束和<latex>m</latex>-吸顺约束下，最大化submodular函数的问题。之前的研究主要考虑单一约束，而实际问题通常涉及多个约束。在本文中，我们提出了一种新的算法SPROUT，它通过同时干扰框架中的增量扩展来解决这个问题。我们证明了SPROUT可以在几乎真实时间内提供更好的近似度 garantia。然后，我们将随机枚举和缓和技术添加到SPROUT中，得到了SPROUT++算法，它可以保持相似的近似度 garantia。在电影推荐和权重最大枢纽问题的应用中，SPROUT++在实践中表现出了superiority。Note: The text has been translated using Google Translate, and may not be perfect.
</details></li>
</ul>
<hr>
<h2 id="On-the-fly-machine-learning-for-parametrization-of-the-effective-Hamiltonian"><a href="#On-the-fly-machine-learning-for-parametrization-of-the-effective-Hamiltonian" class="headerlink" title="On-the-fly machine learning for parametrization of the effective Hamiltonian"></a>On-the-fly machine learning for parametrization of the effective Hamiltonian</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08929">http://arxiv.org/abs/2307.08929</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingyue Ma, L. Bellaiche, Di Wu, Yurong Yang</li>
<li>for: 这研究旨在开发一种基于机器学习的启动式有效汉姆逻辑，用于预测和模拟 ferroelectrics 和 relaxor ferroelectrics 的性质。</li>
<li>methods: 这种方法使用 Bayesian 线性回归来 Parametrize 有效汉姆逻辑，在分子动力学实验中完成 Parametrization，并预测能量、力和压力以及它们的不确定性。当不确定性较大时，使用首要原理计算来重新训练参数。</li>
<li>results: 这种方法可以自动计算任何考虑系统的有效汉姆逻辑参数，包括复杂系统，而传统方法无法处理。用 BaTiO3 和 Pb(Sc,Ta)O3 作为示例，这种方法的准确性与传统首要原理 Parametrization 方法相当。<details>
<summary>Abstract</summary>
The first-principles-based effective Hamiltonian is widely used to predict and simulate the properties of ferroelectrics and relaxor ferroelectrics. However, the parametrization method of the effective Hamiltonian is complicated and hardly can resolve the systems with complex interactions and/or complex components. Here, we developed an on-the-fly machine learning approach to parametrize the effective Hamiltonian based on Bayesian linear regression. The parametrization is completed in molecular dynamics simulations, with the energy, forces and stress predicted at each step along with their uncertainties. First-principles calculations are executed when the uncertainties are large to retrain the parameters. This approach provides a universal and automatic way to compute the effective Hamiltonian parameters for any considered systems including complex systems which previous methods can not handle. BaTiO3 and Pb(Sc,Ta)O3 are taken as examples to show the accurateness of this approach comparing with conventional first-principles parametrization method.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用基本原理效 Hamiltonians 广泛预测和模拟 ferroelectrics 和 relaxor ferroelectrics 的性质。然而，基于效 Hamiltonians 的参数化方法复杂，难以处理复杂的交互和/或复杂的组分。我们在分子动力学实验中开发了一种在飞行中机器学习方法来参数化效 Hamiltonians，通过 Bayesian 线性回归来完成。在每步的分子动力学 simulate 中，能量、力和压力都预测了，同时预测了它们的不确定性。当不确定性大于一定程度时，我们使用首先原理计算来重新训练参数。这种方法提供了一种通用和自动的方法来计算效 Hamiltonians 参数，可以处理任何考虑的系统，包括复杂的系统，先前的方法无法处理。作为例子，我们选择了 BaTiO3 和 Pb(Sc,Ta)O3 来说明这种方法的准确性，与传统的首先原理参数化方法进行比较。
</details></li>
</ul>
<hr>
<h2 id="Federated-Large-Language-Model-A-Position-Paper"><a href="#Federated-Large-Language-Model-A-Position-Paper" class="headerlink" title="Federated Large Language Model: A Position Paper"></a>Federated Large Language Model: A Position Paper</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08925">http://arxiv.org/abs/2307.08925</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chaochao Chen, Xiaohua Feng, Jun Zhou, Jianwei Yin, Xiaolin Zheng</li>
<li>for: 这个研究旨在解决大规模语言模型（LLM）的开发问题，特别是在实际应用中遇到的挑战，例如公共领域数据的缺乏和维护private领域数据的隐私。</li>
<li>methods: 这个研究提出了一种称为“联邦式语言模型”（federated LLM）的技术，它包括三个主要的component，即联邦式语言模型预训练、联邦式语言模型细化和联邦式语言模型提示工程。每个component都有优点比传统LLM训练方法，并且提出了具体的工程策略来实现。</li>
<li>results: 这个研究获得了联邦式语言模型的优点，包括可以解决实际应用中的挑战，并且可以维护隐私和数据安全性。此外，研究也发现了联邦式语言模型在某些情况下可能会面临新的挑战和障碍。<details>
<summary>Abstract</summary>
Large scale language models (LLM) have received significant attention and found diverse applications across various domains, but their development encounters challenges in real-world scenarios. These challenges arise due to the scarcity of public domain data availability and the need to maintain privacy with respect to private domain data. To address these issues, federated learning (FL) has emerged as a promising technology that enables collaborative training of shared models while preserving decentralized data. We propose the concept of federated LLM, which comprises three key components, i.e., federated LLM pre-training, federated LLM fine-tuning, and federated LLM prompt engineering. For each component, we discuss its advantage over traditional LLM training methods and propose specific engineering strategies for implementation. Furthermore, we explore the novel challenges introduced by the integration of FL and LLM. We analyze existing solutions and identify potential obstacles faced by these solutions within the context of federated LLM.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Learning-to-Sample-Tasks-for-Meta-Learning"><a href="#Learning-to-Sample-Tasks-for-Meta-Learning" class="headerlink" title="Learning to Sample Tasks for Meta Learning"></a>Learning to Sample Tasks for Meta Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08924">http://arxiv.org/abs/2307.08924</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ZJLAB-AMMI/HS-OMRL">https://github.com/ZJLAB-AMMI/HS-OMRL</a></li>
<li>paper_authors: Jingyao Wang, Zeen Song, Xingzhe Su, Lingyu Si, Hongwei Dong, Wenwen Qiang, Changwen Zheng</li>
<li>for: 通过对各种元学习方法、任务采样器和少量学习任务进行实验，这篇论文得出了三个结论。首先，无法确保元学习模型性能的通用任务采样策略。其次，任务多样性可能导致模型在训练中 Either underfit 或 overfit。最后，模型的总结果受任务分化、任务熵和任务Difficulty的影响。</li>
<li>methods: 作者提出了一种名为 Adaptive Sampler (ASr) 的新任务采样器，该采样器可以根据任务分化、任务熵和任务Difficulty来采样任务。以便优化 ASr，作者提出了一种简单普适的元学习算法。</li>
<li>results: 许多实验证明了提出的 ASr 的有效性。<details>
<summary>Abstract</summary>
Through experiments on various meta-learning methods, task samplers, and few-shot learning tasks, this paper arrives at three conclusions. Firstly, there are no universal task sampling strategies to guarantee the performance of meta-learning models. Secondly, task diversity can cause the models to either underfit or overfit during training. Lastly, the generalization performance of the models are influenced by task divergence, task entropy, and task difficulty. In response to these findings, we propose a novel task sampler called Adaptive Sampler (ASr). ASr is a plug-and-play task sampler that takes task divergence, task entropy, and task difficulty to sample tasks. To optimize ASr, we rethink and propose a simple and general meta-learning algorithm. Finally, a large number of empirical experiments demonstrate the effectiveness of the proposed ASr.
</details>
<details>
<summary>摘要</summary>
通过多种元学习方法、任务采样策略和少量学习任务的实验，这篇论文得出了三个结论。首先，没有一种通用的任务采样策略可以保证元学习模型的性能。第二，任务多样性可以使模型在训练中出现下降或过度适应。最后，模型的总体性能受到任务分化、任务 entropy 和任务难度的影响。为了应对这些发现，我们提出了一种名为 Adaptive Sampler（ASr）的任务采样器。ASr 是一个插件和玩家的任务采样器，它根据任务分化、任务 entropy 和任务难度来采样任务。为了优化 ASr，我们提出了一种简单和通用的元学习算法。最后，大量的实验证明了我们提出的 ASr 的有效性。
</details></li>
</ul>
<hr>
<h2 id="Optimistic-Estimate-Uncovers-the-Potential-of-Nonlinear-Models"><a href="#Optimistic-Estimate-Uncovers-the-Potential-of-Nonlinear-Models" class="headerlink" title="Optimistic Estimate Uncovers the Potential of Nonlinear Models"></a>Optimistic Estimate Uncovers the Potential of Nonlinear Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08921">http://arxiv.org/abs/2307.08921</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yaoyu Zhang, Zhongwang Zhang, Leyang Zhang, Zhiwei Bai, Tao Luo, Zhi-Qin John Xu</li>
<li>for: 评估非线性模型最佳适应性表现的估计方法。</li>
<li>methods: 使用非线性模型，并且采用估计最小样本大小以达到目标函数的最佳适应性。</li>
<li>results: 对矩阵分解模型、深度模型和深度神经网络（DNN）进行了估计，并证明了这些模型在过参数化下的可适应性。此外，研究还发现了深度神经网络的两种特殊性：自由表达能力和成本表达能力。这两种特殊性提出了建议DNNS的建模设计原则：（一）不妨添加神经元和核函数；（二）限制神经元之间的连接。通过这种框架，我们预计在未来更深入理解如何和为什么许多非线性模型在实践中能够有效实现其潜在可能性。<details>
<summary>Abstract</summary>
We propose an optimistic estimate to evaluate the best possible fitting performance of nonlinear models. It yields an optimistic sample size that quantifies the smallest possible sample size to fit/recover a target function using a nonlinear model. We estimate the optimistic sample sizes for matrix factorization models, deep models, and deep neural networks (DNNs) with fully-connected or convolutional architecture. For each nonlinear model, our estimates predict a specific subset of targets that can be fitted at overparameterization, which are confirmed by our experiments. Our optimistic estimate reveals two special properties of the DNN models -- free expressiveness in width and costly expressiveness in connection. These properties suggest the following architecture design principles of DNNs: (i) feel free to add neurons/kernels; (ii) restrain from connecting neurons. Overall, our optimistic estimate theoretically unveils the vast potential of nonlinear models in fitting at overparameterization. Based on this framework, we anticipate gaining a deeper understanding of how and why numerous nonlinear models such as DNNs can effectively realize their potential in practice in the near future.
</details>
<details>
<summary>摘要</summary>
我们提出了一种优派估计来评估非线性模型的最佳适应性表现。它提供了一个最小的样本大小，用于评估目标函数使用非线性模型适应的可能性。我们对矩阵因子化模型、深度模型和深度神经网络（DNN）进行了估计，并证明了每种非线性模型的估计预测了一个特定的目标集可以在过参数化下适应。我们的估计还揭示了深度神经网络（DNN）模型的两种特殊性：自由表达能力和成本表达能力。这两种特殊性建议了深度神经网络（DNN）模型的建设设计原则：（i）自由添加神经元/核函数；（ii）限制神经元之间的连接。总的来说，我们的优派估计 theoretically 探明了非线性模型在过参数化下的潜在适应能力。基于这个框架，我们预计在未来将更深入地理解非线性模型在实践中如何有效地实现其潜在。
</details></li>
</ul>
<hr>
<h2 id="Continuous-Time-Reinforcement-Learning-New-Design-Algorithms-with-Theoretical-Insights-and-Performance-Guarantees"><a href="#Continuous-Time-Reinforcement-Learning-New-Design-Algorithms-with-Theoretical-Insights-and-Performance-Guarantees" class="headerlink" title="Continuous-Time Reinforcement Learning: New Design Algorithms with Theoretical Insights and Performance Guarantees"></a>Continuous-Time Reinforcement Learning: New Design Algorithms with Theoretical Insights and Performance Guarantees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08920">http://arxiv.org/abs/2307.08920</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brent A. Wallace, Jennie Si</li>
<li>for: 这个论文的目的是提出一种新的连续时间非线性优化控制方法，用于控制非线性系统。</li>
<li>methods: 这种方法基于分解physical system into smaller subproblems，并引入了一种新的刺激框架，以提高 persistency of excitation 和数值稳定性。</li>
<li>results: 这些算法可以提供 convergence 和关闭Loop稳定性保证，并在控制一个不稳定、非最小频段高速飞行器（HSV）上进行了示例应用。<details>
<summary>Abstract</summary>
Continuous-time nonlinear optimal control problems hold great promise in real-world applications. After decades of development, reinforcement learning (RL) has achieved some of the greatest successes as a general nonlinear control design method. However, a recent comprehensive analysis of state-of-the-art continuous-time RL (CT-RL) methods, namely, adaptive dynamic programming (ADP)-based CT-RL algorithms, reveals they face significant design challenges due to their complexity, numerical conditioning, and dimensional scaling issues. Despite advanced theoretical results, existing ADP CT-RL synthesis methods are inadequate in solving even small, academic problems. The goal of this work is thus to introduce a suite of new CT-RL algorithms for control of affine nonlinear systems. Our design approach relies on two important factors. First, our methods are applicable to physical systems that can be partitioned into smaller subproblems. This constructive consideration results in reduced dimensionality and greatly improved intuitiveness of design. Second, we introduce a new excitation framework to improve persistence of excitation (PE) and numerical conditioning performance via classical input/output insights. Such a design-centric approach is the first of its kind in the ADP CT-RL community. In this paper, we progressively introduce a suite of (decentralized) excitable integral reinforcement learning (EIRL) algorithms. We provide convergence and closed-loop stability guarantees, and we demonstrate these guarantees on a significant application problem of controlling an unstable, nonminimum phase hypersonic vehicle (HSV).
</details>
<details>
<summary>摘要</summary>
<<SYS>>translation into Simplified Chinese<</SYS>>连续时间非线性优化控制问题在实际应用中具有很大的推动力。在过去的几十年中，强化学习（RL）已经取得了一些非常成功的非线性控制设计方法。然而，一个最近的总体分析表明，使用ADP基于CT-RL算法的continuous-time RL（CT-RL）方法面临着复杂性、数值条件和维度增加问题。尽管有了先进的理论成果，现有的ADP CT-RL合成方法无法解决even small的学术问题。本工作的目标是引入一组新的CT-RL算法，用于控制非线性系统。我们的设计方法基于以下两个重要因素。首先，我们的方法适用于可以被分解成更小的子问题的物理系统。这种构建考虑导致维度减少和设计更加直观的问题。其次，我们引入了一个新的刺激框架，以提高持续刺激（PE）和数值条件性性能。这种设计中心的方法是CT-RL社区中的第一个。在这篇论文中，我们逐渐介绍了一组（分布式）刺激积分学习（EIRL）算法。我们提供了收敛和关闭Loop稳定性保证，并在一个重要应用问题中控制不稳定、非最小阶段速度飞行器（HSV）中进行了证明。
</details></li>
</ul>
<hr>
<h2 id="Accuracy-versus-time-frontiers-of-semi-supervised-and-self-supervised-learning-on-medical-images"><a href="#Accuracy-versus-time-frontiers-of-semi-supervised-and-self-supervised-learning-on-medical-images" class="headerlink" title="Accuracy versus time frontiers of semi-supervised and self-supervised learning on medical images"></a>Accuracy versus time frontiers of semi-supervised and self-supervised learning on medical images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08919">http://arxiv.org/abs/2307.08919</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tufts-ml/ssl-vs-ssl-benchmark">https://github.com/tufts-ml/ssl-vs-ssl-benchmark</a></li>
<li>paper_authors: Zhe Huang, Ruijie Jiang, Shuchin Aeron, Michael C. Hughes</li>
<li>for: 本研究的目的是为了解决资源受限、结果受关注的医学图像分类问题，通过采用自我超级vised学习和 semi-supervised learning方法，提高分类器的性能。</li>
<li>methods: 本研究使用了6种 semi-supervised 方法和5种自我超级vised学习方法，并与高品质标注数据作为基准进行比较。</li>
<li>results: 研究发现， MixMatch、SimCLR 和 BYOL 等方法在3种医学图像 datasets 上表现出色，并且可以在几个小时内达到优秀的性能。同时，通过选择适当的 hyperparameter 和进行较多的训练，可以获得进一步的提升。<details>
<summary>Abstract</summary>
For many applications of classifiers to medical images, a trustworthy label for each image can be difficult or expensive to obtain. In contrast, images without labels are more readily available. Two major research directions both promise that additional unlabeled data can improve classifier performance: self-supervised learning pretrains useful representations on unlabeled data only, then fine-tunes a classifier on these representations via the labeled set; semi-supervised learning directly trains a classifier on labeled and unlabeled data simultaneously. Recent methods from both directions have claimed significant gains on non-medical tasks, but do not systematically assess medical images and mostly compare only to methods in the same direction. This study contributes a carefully-designed benchmark to help answer a practitioner's key question: given a small labeled dataset and a limited budget of hours to spend on training, what gains from additional unlabeled images are possible and which methods best achieve them? Unlike previous benchmarks, ours uses realistic-sized validation sets to select hyperparameters, assesses runtime-performance tradeoffs, and bridges two research fields. By comparing 6 semi-supervised methods and 5 self-supervised methods to strong labeled-only baselines on 3 medical datasets with 30-1000 labels per class, we offer insights to resource-constrained, results-focused practitioners: MixMatch, SimCLR, and BYOL represent strong choices that were not surpassed by more recent methods. After much effort selecting hyperparameters on one dataset, we publish settings that enable strong methods to perform well on new medical tasks within a few hours, with further search over dozens of hours delivering modest additional gains.
</details>
<details>
<summary>摘要</summary>
für viele Anwendungen von Klassifikatoren auf medizinische Bilder kann ein zuverlässiger Label für jede Bilddatei schwierig oder teuer zu beschaffen sein. Im Gegensatz dazu sind Bilder ohne Labels mehr verfügbar. Zwei wichtige Forschungsrichtungen versprechen, dass zusätzliches unetikettiertes Datenmaterial die Leistung des Klassifikators verbessern kann: Selbstübergreifendes Lernen trainiert nützliche Representationen auf unetikettiertem Datenmaterial, then fine-tunes a classifier via the labeled set; semi-supervised Learning trainiert direkt einen Klassifikator auf etikettierten und unetikettierten Daten gleichzeitig. Recente Methoden beider Richtungen haben bedeutende Fortschritte bei nicht-medizinischen Aufgaben erzielt, aber systematisch die medizinischen Bilder nicht bewertet und sich nur mit Methoden in derselben Richtungen verglichen. This study contributes a carefully-designed Benchmark, um zu antworten auf eine wichtige Frage des Practitioners: given a small labeled dataset and a limited budget of hours to spend on training, what gains from additional unetikettiertes images are possible and which methods best achieve them? Im Gegensatz zu previous Benchmarks, ours uses realistic-sized validation sets to select hyperparameters, assesses runtime-performance tradeoffs, and bridges two research fields. By comparing 6 semi-supervised methods and 5 self-supervised methods to strong labeled-only baselines on 3 medical datasets with 30-1000 labels per class, we offer insights to resource-constrained, results-focused practitioners: MixMatch, SimCLR, and BYOL represent strong choices that were not surpassed by more recent methods. After much effort selecting hyperparameters on one dataset, we publish settings that enable strong methods to perform well on new medical tasks within a few hours, with further search over dozens of hours delivering modest additional gains.
</details></li>
</ul>
<hr>
<h2 id="Towards-the-Sparseness-of-Projection-Head-in-Self-Supervised-Learning"><a href="#Towards-the-Sparseness-of-Projection-Head-in-Self-Supervised-Learning" class="headerlink" title="Towards the Sparseness of Projection Head in Self-Supervised Learning"></a>Towards the Sparseness of Projection Head in Self-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08913">http://arxiv.org/abs/2307.08913</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeen Song, Xingzhe Su, Jingyao Wang, Wenwen Qiang, Changwen Zheng, Fuchun Sun</li>
<li>for: 提高自动学习（SSL）方法中的表示性能</li>
<li>methods: 使用对偶学习方法和理论分析，探讨投影头部件的内部机制和维度归一化现象之间的关系</li>
<li>results: 提出了一种假设，即只需要在数据批处理中最小化对偶损失时使用一部分特征；并通过对SSL方法进行论证，提出了一种名为SparseHead的规范项，可以减少投影头部件的稀疏性，从而提高SSL方法的表示性能。<details>
<summary>Abstract</summary>
In recent years, self-supervised learning (SSL) has emerged as a promising approach for extracting valuable representations from unlabeled data. One successful SSL method is contrastive learning, which aims to bring positive examples closer while pushing negative examples apart. Many current contrastive learning approaches utilize a parameterized projection head. Through a combination of empirical analysis and theoretical investigation, we provide insights into the internal mechanisms of the projection head and its relationship with the phenomenon of dimensional collapse. Our findings demonstrate that the projection head enhances the quality of representations by performing contrastive loss in a projected subspace. Therefore, we propose an assumption that only a subset of features is necessary when minimizing the contrastive loss of a mini-batch of data. Theoretical analysis further suggests that a sparse projection head can enhance generalization, leading us to introduce SparseHead - a regularization term that effectively constrains the sparsity of the projection head, and can be seamlessly integrated with any self-supervised learning (SSL) approaches. Our experimental results validate the effectiveness of SparseHead, demonstrating its ability to improve the performance of existing contrastive methods.
</details>
<details>
<summary>摘要</summary>
Note: The text has been translated into Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="Sharpness-Aware-Graph-Collaborative-Filtering"><a href="#Sharpness-Aware-Graph-Collaborative-Filtering" class="headerlink" title="Sharpness-Aware Graph Collaborative Filtering"></a>Sharpness-Aware Graph Collaborative Filtering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08910">http://arxiv.org/abs/2307.08910</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huiyuan Chen, Chin-Chia Michael Yeh, Yujie Fan, Yan Zheng, Junpeng Wang, Vivian Lai, Mahashweta Das, Hao Yang</li>
<li>for: 提高Graph Neural Networks（GNNs）在协同缓存中的表现。</li>
<li>methods: 提出了一种有效的训练方案{gSAM}，基于权重损失 landscape的平滑性来优化GNNs。</li>
<li>results: 实验结果表明，gSAM可以提高GNNs的表现。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have achieved impressive performance in collaborative filtering. However, GNNs tend to yield inferior performance when the distributions of training and test data are not aligned well. Also, training GNNs requires optimizing non-convex neural networks with an abundance of local and global minima, which may differ widely in their performance at test time. Thus, it is essential to choose the minima carefully. Here we propose an effective training schema, called {gSAM}, under the principle that the \textit{flatter} minima has a better generalization ability than the \textit{sharper} ones. To achieve this goal, gSAM regularizes the flatness of the weight loss landscape by forming a bi-level optimization: the outer problem conducts the standard model training while the inner problem helps the model jump out of the sharp minima. Experimental results show the superiority of our gSAM.
</details>
<details>
<summary>摘要</summary>
格raph神经网络（GNNs）在共同推荐中表现出色。然而，GNNs在训练和测试数据分布不匹配时表现不佳。此外，训练GNNs需要优化非核心积分神经网络，这些神经网络具有很多的本地和全局最小值，其测试时表现可能很不同。因此，选择最佳的最小值非常重要。我们提出了一种有效的训练方法，即{gSAM}，其基于“稍平”的最小值具有更好的泛化能力。为实现这个目标，gSAM在权重损失的折衔减中做出了二级优化：外部问题进行标准模型训练，而内部问题帮助模型离开锋利的最小值。实验结果显示了我们的gSAM的优越性。
</details></li>
</ul>
<hr>
<h2 id="Solving-multiphysics-based-inverse-problems-with-learned-surrogates-and-constraints"><a href="#Solving-multiphysics-based-inverse-problems-with-learned-surrogates-and-constraints" class="headerlink" title="Solving multiphysics-based inverse problems with learned surrogates and constraints"></a>Solving multiphysics-based inverse problems with learned surrogates and constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11099">http://arxiv.org/abs/2307.11099</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyi Yin, Rafael Orozco, Mathias Louboutin, Felix J. Herrmann</li>
<li>for: 这 paper 是用于解决地质碳存储监测中的多物理 inverse problem 的。</li>
<li>methods: 这 paper 使用了 computationally cheap 的 learned surrogates 和 learned constraints 来解决这些问题。</li>
<li>results: 这 paper 的结果表明，这种 combinaison 可以提高 fluid-flow 性能的减法，并且可以处理多模态数据，包括 well 测量和 active-source time-lapse seismic 数据。另外，这种方法还可以保持准确性，因为它使用了一个 trained deep neural network 来 constrain the model iterates。<details>
<summary>Abstract</summary>
Solving multiphysics-based inverse problems for geological carbon storage monitoring can be challenging when multimodal time-lapse data are expensive to collect and costly to simulate numerically. We overcome these challenges by combining computationally cheap learned surrogates with learned constraints. Not only does this combination lead to vastly improved inversions for the important fluid-flow property, permeability, it also provides a natural platform for inverting multimodal data including well measurements and active-source time-lapse seismic data. By adding a learned constraint, we arrive at a computationally feasible inversion approach that remains accurate. This is accomplished by including a trained deep neural network, known as a normalizing flow, which forces the model iterates to remain in-distribution, thereby safeguarding the accuracy of trained Fourier neural operators that act as surrogates for the computationally expensive multiphase flow simulations involving partial differential equation solves. By means of carefully selected experiments, centered around the problem of geological carbon storage, we demonstrate the efficacy of the proposed constrained optimization method on two different data modalities, namely time-lapse well and time-lapse seismic data. While permeability inversions from both these two modalities have their pluses and minuses, their joint inversion benefits from either, yielding valuable superior permeability inversions and CO2 plume predictions near, and far away, from the monitoring wells.
</details>
<details>
<summary>摘要</summary>
解决基于多物理学的逆 пробле 的地质碳存储监测可以是困难的，当 multimodal 时间差数据成本高昂， numerically 计算成本高昂时。我们利用计算成本低廉的学习的代理人与学习的约束结合，不仅能够大幅提高流体流动性的重要性质，孔隙性，还提供了自然的多模态数据逆向平台。通过添加学习的约束，我们实现了计算可行的逆向方法，保持精度。这是通过包含训练好的深度神经网络，即 нормализа流，使模型迭代器 remains 在distribution中，保证训练了Fourier神经网络作为多相流 simulations involving partial differential equation solves的计算昂贵的surrogate。通过选择精心的实验，以地质碳存储问题为中心，我们在时间差井和时间差地震数据两个不同的模式下展示了提案的受限优化方法的效果。虽然孔隙性逆向从这两个模式中有其优缺点，但是两者的共同逆向具有优势，为CO2泵预测和碳存储监测提供了有价值的Superior permeability inversions和预测。
</details></li>
</ul>
<hr>
<h2 id="Basal-Bolus-Advisor-for-Type-1-Diabetes-T1D-Patients-Using-Multi-Agent-Reinforcement-Learning-RL-Methodology"><a href="#Basal-Bolus-Advisor-for-Type-1-Diabetes-T1D-Patients-Using-Multi-Agent-Reinforcement-Learning-RL-Methodology" class="headerlink" title="Basal-Bolus Advisor for Type 1 Diabetes (T1D) Patients Using Multi-Agent Reinforcement Learning (RL) Methodology"></a>Basal-Bolus Advisor for Type 1 Diabetes (T1D) Patients Using Multi-Agent Reinforcement Learning (RL) Methodology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08897">http://arxiv.org/abs/2307.08897</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mehrad Jaloli, Marzia Cescon</li>
<li>for: 这种研究旨在开发一种基于多智能 reinforcement learning（RL）的个性化血糖控制方法，以改善ype 1  диабе尼（T1D）患者的血糖控制。</li>
<li>methods: 该方法使用一个关闭的循环系统，包括血糖代谢模型和多智能软actor-critic RL模型，作为基础-膳食帮手。</li>
<li>results: 研究结果表明，RL-基于的基础-膳食帮手可以有效改善血糖控制，降低血糖波动性，并增加血糖水平在目标范围内的时间。同时，RL方法可以有效预防低血糖事件，并减少高血糖事件。此外，RL方法还导致了对 convential 疗法相比，每天基础荷尔血糖剂的减少。这些发现表明RL方法可以在ype 1  диабе尼患者中实现更好的血糖控制，并减少严重高血糖的风险。<details>
<summary>Abstract</summary>
This paper presents a novel multi-agent reinforcement learning (RL) approach for personalized glucose control in individuals with type 1 diabetes (T1D). The method employs a closed-loop system consisting of a blood glucose (BG) metabolic model and a multi-agent soft actor-critic RL model acting as the basal-bolus advisor. Performance evaluation is conducted in three scenarios, comparing the RL agents to conventional therapy. Evaluation metrics include glucose levels (minimum, maximum, and mean), time spent in different BG ranges, and average daily bolus and basal insulin dosages. Results demonstrate that the RL-based basal-bolus advisor significantly improves glucose control, reducing glycemic variability and increasing time spent within the target range (70-180 mg/dL). Hypoglycemia events are effectively prevented, and severe hyperglycemia events are reduced. The RL approach also leads to a statistically significant reduction in average daily basal insulin dosage compared to conventional therapy. These findings highlight the effectiveness of the multi-agent RL approach in achieving better glucose control and mitigating the risk of severe hyperglycemia in individuals with T1D.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Evaluating-unsupervised-disentangled-representation-learning-for-genomic-discovery-and-disease-risk-prediction"><a href="#Evaluating-unsupervised-disentangled-representation-learning-for-genomic-discovery-and-disease-risk-prediction" class="headerlink" title="Evaluating unsupervised disentangled representation learning for genomic discovery and disease risk prediction"></a>Evaluating unsupervised disentangled representation learning for genomic discovery and disease risk prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08893">http://arxiv.org/abs/2307.08893</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taedong Yun</li>
<li>for: 这个论文主要是为了研究高维клиниче数据中的生物marks，以及使用深度学习技术进行遗传学研究。</li>
<li>methods: 这个论文使用了多种无监督学习方法，包括自动编码器、VAE、β-VAE和factorVAE，以学习分离的表示。</li>
<li>results: 研究发现，使用FactorVAE或β-VAE，可以提高遗传学研究中的结果，包括基因associes数量、heritability和多ifactorial风险分数的性能。factorVAE在不同的规则化参数值下表现良好，而β-VAE却受到规则化参数值的影响较大。<details>
<summary>Abstract</summary>
High-dimensional clinical data have become invaluable resources for genetic studies, due to their accessibility in biobank-scale datasets and the development of high performance modeling techniques especially using deep learning. Recent work has shown that low dimensional embeddings of these clinical data learned by variational autoencoders (VAE) can be used for genome-wide association studies and polygenic risk prediction. In this work, we consider multiple unsupervised learning methods for learning disentangled representations, namely autoencoders, VAE, beta-VAE, and FactorVAE, in the context of genetic association studies. Using spirograms from UK Biobank as a running example, we observed improvements in the number of genome-wide significant loci, heritability, and performance of polygenic risk scores for asthma and chronic obstructive pulmonary disease by using FactorVAE or beta-VAE, compared to standard VAE or non-variational autoencoders. FactorVAEs performed effectively across multiple values of the regularization hyperparameter, while beta-VAEs were much more sensitive to the hyperparameter values.
</details>
<details>
<summary>摘要</summary>
高维临床数据已成为生物银行规模数据集的不可或缺的资源，这主要归功于生物银行规模数据集的可用性和深度学习技术的发展。据研究表明，使用变量自动编码器（VAE）学习的低维度表示可以用于遗传相关研究和多ifactorial风险预测。在本工作中，我们考虑了多种无监督学习方法，包括 autoencoder、VAE、β-VAE 和 FactorVAE，在遗传相关研究中。使用 UK Biobank 的呼吸图为例，我们发现了使用 FactorVAE 或 β-VAE 而非标准 VAE 或非变量自动编码器后，对气喘病和肺部疾病的遗传相关性、heritability 和多ifactorial风险分数的改进。FactorVAE 在多个规则化超参数值上表现得更好，而 β-VAE 对超参数值的敏感性较高。
</details></li>
</ul>
<hr>
<h2 id="The-Predicted-Deletion-Dynamic-Model-Taking-Advantage-of-ML-Predictions-for-Free"><a href="#The-Predicted-Deletion-Dynamic-Model-Taking-Advantage-of-ML-Predictions-for-Free" class="headerlink" title="The Predicted-Deletion Dynamic Model: Taking Advantage of ML Predictions, for Free"></a>The Predicted-Deletion Dynamic Model: Taking Advantage of ML Predictions, for Free</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08890">http://arxiv.org/abs/2307.08890</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quanquan C. Liu, Vaidehi Srinivas</li>
<li>for: 这 paper 的目的是解决动态图中edge更新预测问题，提高动态算法的效率。</li>
<li>methods: 这 paper 使用了预测 deleting 的 Dynamic Model，并提出了一种基于这种模型的框架，可以将部分动态算法”升级”到完全动态Setting中，减少了更新时间的复杂性。</li>
<li>results: 这 paper 的算法在不同的问题上都能够实现更好的性能，具体来说，它们的平均更新时间与部分动态算法相似，而且在预测质量较高时，它们的性能与完全动态算法相当。<details>
<summary>Abstract</summary>
The main bottleneck in designing efficient dynamic algorithms is the unknown nature of the update sequence. In particular, there are some problems, like 3-vertex connectivity, planar digraph all pairs shortest paths, and others, where the separation in runtime between the best partially dynamic solutions and the best fully dynamic solutions is polynomial, sometimes even exponential.   In this paper, we formulate the predicted-deletion dynamic model, motivated by a recent line of empirical work about predicting edge updates in dynamic graphs. In this model, edges are inserted and deleted online, and when an edge is inserted, it is accompanied by a "prediction" of its deletion time. This models real world settings where services may have access to historical data or other information about an input and can subsequently use such information make predictions about user behavior. The model is also of theoretical interest, as it interpolates between the partially dynamic and fully dynamic settings, and provides a natural extension of the algorithms with predictions paradigm to the dynamic setting.   We give a novel framework for this model that "lifts" partially dynamic algorithms into the fully dynamic setting with little overhead. We use our framework to obtain improved efficiency bounds over the state-of-the-art dynamic algorithms for a variety of problems. In particular, we design algorithms that have amortized update time that scales with a partially dynamic algorithm, with high probability, when the predictions are of high quality. On the flip side, our algorithms do no worse than existing fully-dynamic algorithms when the predictions are of low quality. Furthermore, our algorithms exhibit a graceful trade-off between the two cases. Thus, we are able to take advantage of ML predictions asymptotically "for free.''
</details>
<details>
<summary>摘要</summary>
主要瓶颈在设计高效的动态算法上是未知的更新序列。特别是有些问题，如3个顶点连接性、平面图全对短路和其他问题，其运行时间差 между最佳半动态解决方案和最佳完全动态解决方案是多项式，有时甚至是指数。在这篇论文中，我们提出预测删除动态模型，因为一些现实世界中的服务可以通过历史数据或其他信息来预测用户行为。这种模型在理论上也很有 interess，因为它在半动态和完全动态之间进行 interpolating，并且为动态设定提供了自然的扩展。我们给出了一种新的框架，使得半动态算法在完全动态设定下具有较少的开销。我们使用这种框架，设计了一些算法，其更新时间平均与半动态算法相似，高probability 时，当预测质量高时。另一方面，我们的算法不比现有的完全动态算法差，当预测质量低时。此外，我们的算法具有温和的质量补偿，因此可以充分利用机器学习预测，“免费”。
</details></li>
</ul>
<hr>
<h2 id="Examining-the-Effects-of-Degree-Distribution-and-Homophily-in-Graph-Learning-Models"><a href="#Examining-the-Effects-of-Degree-Distribution-and-Homophily-in-Graph-Learning-Models" class="headerlink" title="Examining the Effects of Degree Distribution and Homophily in Graph Learning Models"></a>Examining the Effects of Degree Distribution and Homophily in Graph Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08881">http://arxiv.org/abs/2307.08881</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/google-research/graphworld">https://github.com/google-research/graphworld</a></li>
<li>paper_authors: Mustafa Yasir, John Palowitch, Anton Tsitsulin, Long Tran-Thanh, Bryan Perozzi</li>
<li>for: This paper aims to improve the evaluation of graph neural network (GNN) models by expanding the coverage of graph space within the GraphWorld framework.</li>
<li>methods: The paper uses three synthetic graph generators: the Stochastic Block Model (SBM), LFR, and CABAM. These generators are integrated into the GraphWorld framework to create more diverse populations of synthetic graphs for benchmarking GNN tasks.</li>
<li>results: The paper generates 300,000 graphs to benchmark 11 GNN models on a node classification task, and finds variations in GNN performance in response to homophily, degree distribution, and feature signal. The paper classifies GNN models based on their sensitivity to the new generators under these properties.Here’s the simplified Chinese text for the three information points:</li>
<li>for: 这篇论文目标是提高图神经网络（GNN）模型的评估，通过扩展图WORLD框架中的图空间覆盖。</li>
<li>methods: 这篇论文使用了三种 sintetic 图生成器：Stochastic Block Model（SBM）、LFR 和 CABAM。这些生成器被 integrate 到图WORLD框架中，以创造更多的多样化的 sintetic 图来 benchmark GNN 任务。</li>
<li>results: 这篇论文通过生成 300,000 个图，对 11 种 GNN 模型进行节点分类任务的评估，发现 GNN 性能响应于同类性、度分布和特征信号。 paper 根据这些特性将 GNN 模型分类为敏感性。<details>
<summary>Abstract</summary>
Despite a surge in interest in GNN development, homogeneity in benchmarking datasets still presents a fundamental issue to GNN research. GraphWorld is a recent solution which uses the Stochastic Block Model (SBM) to generate diverse populations of synthetic graphs for benchmarking any GNN task. Despite its success, the SBM imposed fundamental limitations on the kinds of graph structure GraphWorld could create.   In this work we examine how two additional synthetic graph generators can improve GraphWorld's evaluation; LFR, a well-established model in the graph clustering literature and CABAM, a recent adaptation of the Barabasi-Albert model tailored for GNN benchmarking. By integrating these generators, we significantly expand the coverage of graph space within the GraphWorld framework while preserving key graph properties observed in real-world networks. To demonstrate their effectiveness, we generate 300,000 graphs to benchmark 11 GNN models on a node classification task. We find GNN performance variations in response to homophily, degree distribution and feature signal. Based on these findings, we classify models by their sensitivity to the new generators under these properties. Additionally, we release the extensions made to GraphWorld on the GitHub repository, offering further evaluation of GNN performance on new graphs.
</details>
<details>
<summary>摘要</summary>
尽管GNN发展中的兴趣增长，仍然存在基本问题，即 benchmarking 数据集的同质性。GraphWorld 是一种最近的解决方案，使用 Stochastic Block Model（SBM）生成多样化的 synthetic graph 用于任何 GNN 任务的 benchmarking。尽管它成功，但 SBM 强制性限制 GraphWorld 可以创建的图结构类型。在这种工作中，我们检查了两种额外的 sintethic graph 生成器是如何提高 GraphWorld 的评估。LFR 是一种已有的图分群模型，CABAM 是一种对 Barabasi-Albert 模型的最近适应，专门用于 GNN  benchmarking。通过将这些生成器纳入 GraphWorld 框架，我们可以覆盖图空间的扩展，保持真实世界网络中观察到的关键图属性。为了证明其效果，我们生成了 300,000 个图用于对 11 种 GNN 模型进行节点分类任务的 benchmarking。我们发现 GNN 模型在同质性、度分布和特征信号下的性能变化。根据这些发现，我们将模型分为它们对新生成器下的性能响应。此外，我们在 GitHub 仓库中发布了 GraphWorld 的扩展，以便进一步评估 GNN 性能在新的图上。
</details></li>
</ul>
<hr>
<h2 id="Modular-Neural-Network-Approaches-for-Surgical-Image-Recognition"><a href="#Modular-Neural-Network-Approaches-for-Surgical-Image-Recognition" class="headerlink" title="Modular Neural Network Approaches for Surgical Image Recognition"></a>Modular Neural Network Approaches for Surgical Image Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08880">http://arxiv.org/abs/2307.08880</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nosseiba Ben Salem, Younes Bennani, Joseph Karkazan, Abir Barbara, Charles Dacheux, Thomas Gregory</li>
<li>for: 这个研究是为了提出一种基于深度学习的脑网络架构，以解决现代问题的复杂化和数据不足问题。</li>
<li>methods: 这个研究使用了自我训练的方法来解决数据不足问题，并且将问题分解为更简单的子 задачі，以提高模型的泛化和解释性。</li>
<li>results: 研究发现，使用模块学习方法可以提高分类性能，并且可以实现近乎完美的分类。另外，这种方法还可以提高数据分类的速度和可解释性。<details>
<summary>Abstract</summary>
Deep learning-based applications have seen a lot of success in recent years. Text, audio, image, and video have all been explored with great success using deep learning approaches. The use of convolutional neural networks (CNN) in computer vision, in particular, has yielded reliable results. In order to achieve these results, a large amount of data is required. However, the dataset cannot always be accessible. Moreover, annotating data can be difficult and time-consuming. Self-training is a semi-supervised approach that managed to alleviate this problem and achieve state-of-the-art performances. Theoretical analysis even proved that it may result in a better generalization than a normal classifier. Another problem neural networks can face is the increasing complexity of modern problems, requiring a high computational and storage cost. One way to mitigate this issue, a strategy that has been inspired by human cognition known as modular learning, can be employed. The principle of the approach is to decompose a complex problem into simpler sub-tasks. This approach has several advantages, including faster learning, better generalization, and enables interpretability.   In the first part of this paper, we introduce and evaluate different architectures of modular learning for Dorsal Capsulo-Scapholunate Septum (DCSS) instability classification. Our experiments have shown that modular learning improves performances compared to non-modular systems. Moreover, we found that weighted modular, that is to weight the output using the probabilities from the gating module, achieved an almost perfect classification. In the second part, we present our approach for data labeling and segmentation with self-training applied on shoulder arthroscopy images.
</details>
<details>
<summary>摘要</summary>
深度学习基于应用在过去几年中取得了很多成功。文本、音频、图像和视频都被使用深度学习方法进行了成功的探索。特别是在计算机视觉方面，卷积神经网络（CNN）的使用已经取得了可靠的结果。但是，获取数据的问题仍然存在。另外，标注数据可能会是困难的和耗时的。自学习是一种半监督学习方法，可以解决这个问题，并达到状态艺术的性能。理论分析还证明，它可能会在普通分类器之上取得更好的泛化性。另一个问题是现代问题的复杂性，需要高度的计算和存储成本。一种可以 mitigate这个问题的方法是模块学习。这种方法的原理是将复杂问题分解成更简单的子任务。这种方法有很多优点，包括更快的学习、更好的泛化和可读性。在本文的第一部分，我们介绍了不同的模块学习架构，并对DCSS不稳定性分类问题进行了评估。我们的实验结果表明，模块学习可以提高性能，而且使用权重模块可以达到几乎完美的分类。在第二部分，我们介绍了我们的自动标注和分割方法，使用自学习在肩镜像中进行了应用。
</details></li>
</ul>
<hr>
<h2 id="Disentangling-Node-Attributes-from-Graph-Topology-for-Improved-Generalizability-in-Link-Prediction"><a href="#Disentangling-Node-Attributes-from-Graph-Topology-for-Improved-Generalizability-in-Link-Prediction" class="headerlink" title="Disentangling Node Attributes from Graph Topology for Improved Generalizability in Link Prediction"></a>Disentangling Node Attributes from Graph Topology for Improved Generalizability in Link Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08877">http://arxiv.org/abs/2307.08877</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chatterjeeayan/upna">https://github.com/chatterjeeayan/upna</a></li>
<li>paper_authors: Ayan Chatterjee, Robin Walters, Giulia Menichetti, Tina Eliassi-Rad</li>
<li>for: 链接预测是图机器学习中关键的任务，具有广泛的应用。本文研究节点特征和图结构之间的互动，并证明在包含预训节点特征的情况下，链接预测模型的泛化能力得到提高。</li>
<li>methods: 我们提出的方法是UPNA（无监督节点特征预训），它解决了链接预测问题，学习一个函数，将两个节点特征作为输入，预测两节点之间的边的概率。与传统的图神经网络（GNN）不同，UPNA不会因为图中强制度分布的问题而陷入拟合偏见。</li>
<li>results: 我们的实验表明，UPNA可以在多种对比 datasets 上达到3X至34X的提高，超过当前的状态艺。此外，UPNA可以应用于多种对比学习任务，并与现有的链接预测模型集成，提高其泛化能力和图生成模型的强度。<details>
<summary>Abstract</summary>
Link prediction is a crucial task in graph machine learning with diverse applications. We explore the interplay between node attributes and graph topology and demonstrate that incorporating pre-trained node attributes improves the generalization power of link prediction models. Our proposed method, UPNA (Unsupervised Pre-training of Node Attributes), solves the inductive link prediction problem by learning a function that takes a pair of node attributes and predicts the probability of an edge, as opposed to Graph Neural Networks (GNN), which can be prone to topological shortcuts in graphs with power-law degree distribution. In this manner, UPNA learns a significant part of the latent graph generation mechanism since the learned function can be used to add incoming nodes to a growing graph. By leveraging pre-trained node attributes, we overcome observational bias and make meaningful predictions about unobserved nodes, surpassing state-of-the-art performance (3X to 34X improvement on benchmark datasets). UPNA can be applied to various pairwise learning tasks and integrated with existing link prediction models to enhance their generalizability and bolster graph generative models.
</details>
<details>
<summary>摘要</summary>
链接预测是图机器学习中关键的任务，具有广泛的应用。我们研究节点特征和图结构之间的互动，并证明在把预训练节点特征纳入模型中可以提高链接预测模型的通用能力。我们提出的方法是UPNA（无监督节点特征预训练），解决了链接预测问题，学习一个函数，用来预测两个节点之间的边的概率，而不是使用图神经网络（GNN），后者可能会因为图中具有强制的度分布而导致拓扑短 Circuit。这种方法可以学习图生成机制的一个重要部分，因为学习的函数可以用来添加新的入节点到生长中的图。通过利用预训练节点特征，我们超越观察偏见，可以对未观察的节点进行有意义的预测，超过了状态艺术性的表现（3X-34X提高在标准数据集上）。UPNA可以应用于多种对称学习任务，并可以与现有的链接预测模型集成，提高其通用性和图生成模型的鲁棒性。
</details></li>
</ul>
<hr>
<h2 id="Natural-Actor-Critic-for-Robust-Reinforcement-Learning-with-Function-Approximation"><a href="#Natural-Actor-Critic-for-Robust-Reinforcement-Learning-with-Function-Approximation" class="headerlink" title="Natural Actor-Critic for Robust Reinforcement Learning with Function Approximation"></a>Natural Actor-Critic for Robust Reinforcement Learning with Function Approximation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08875">http://arxiv.org/abs/2307.08875</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruida Zhou, Tao Liu, Min Cheng, Dileep Kalathil, P. R. Kumar, Chao Tian</li>
<li>for: 这个论文的目的是解决模型匹配问题，即在训练环境和测试环境之间的模型差异问题，以确定一个可靠性高的策略。</li>
<li>methods: 该论文提出了两种新的不确定集形式，一种基于双抽样，另一种基于积分概率度量。这两种不确定集形式使得大规模的Robust reinforcement learning（RL）变得可 tractable，即使只有训练环境。该论文还提出了一种 robust natural actor-critic（RNAC）方法，该方法包括新的不确定集形式和函数approximation。</li>
<li>results: 该论文的实验结果显示，RNAC方法可以在多个 MuJoCo 环境和一个实际世界的TurtleBot导航任务中提供良好的Robust性性能。<details>
<summary>Abstract</summary>
We study robust reinforcement learning (RL) with the goal of determining a well-performing policy that is robust against model mismatch between the training simulator and the testing environment. Previous policy-based robust RL algorithms mainly focus on the tabular setting under uncertainty sets that facilitate robust policy evaluation, but are no longer tractable when the number of states scales up. To this end, we propose two novel uncertainty set formulations, one based on double sampling and the other on an integral probability metric. Both make large-scale robust RL tractable even when one only has access to a simulator. We propose a robust natural actor-critic (RNAC) approach that incorporates the new uncertainty sets and employs function approximation. We provide finite-time convergence guarantees for the proposed RNAC algorithm to the optimal robust policy within the function approximation error. Finally, we demonstrate the robust performance of the policy learned by our proposed RNAC approach in multiple MuJoCo environments and a real-world TurtleBot navigation task.
</details>
<details>
<summary>摘要</summary>
我们研究了一种robust reinforcement learning（RL）方法，以确定在训练环境和测试环境之间存在模型匹配不良的情况下，可以达到良好的策略。之前的策略基于RL算法主要在表格设定下进行了不确定性集的使用，但是当状态数量增加时，这些算法就不再可行了。为此，我们提出了两种新的不确定性集形式，一种基于双抽样，另一种基于 интеграル概率度量。这两种形式使得大规模的RL问题变得可 tractable，即使只有训练环境的模型。我们提出了一种robust natural actor-critic（RNAC）方法，该方法包括新的不确定性集和函数近似。我们提供了finite-time converges guarantees，表明RNAC算法在函数近似误差下可以在有限时间内 converges到最佳robust策略。最后，我们在多个MuJoCo环境和一个实际世界TurtleBot导航任务中证明了我们提出的RNAC策略的robust性。
</details></li>
</ul>
<hr>
<h2 id="Latent-Space-Representations-of-Neural-Algorithmic-Reasoners"><a href="#Latent-Space-Representations-of-Neural-Algorithmic-Reasoners" class="headerlink" title="Latent Space Representations of Neural Algorithmic Reasoners"></a>Latent Space Representations of Neural Algorithmic Reasoners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08874">http://arxiv.org/abs/2307.08874</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mirjanic/nar-latent-spaces">https://github.com/mirjanic/nar-latent-spaces</a></li>
<li>paper_authors: Vladimir V. Mirjanić, Razvan Pascanu, Petar Veličković</li>
<li>for: 这个研究探讨了神经算法逻辑（NAR）领域中使用神经网络架构来可靠地捕捉经典计算方法的问题。</li>
<li>methods: 该研究使用图神经网络（GNN）架构，将输入编码成高维隐藏空间，并在执行算法时进行重复转换。</li>
<li>results: 研究发现GNN架构中的隐藏空间结构存在两种可能的失败模式：（1）loss of resolution，导致同样的值很难分辨；（2）无法处理训练期间未见到的值。提议使用softmax汇聚器和衰减隐藏空间来解决这两种问题，并证明这些改进可以在CLRS-30标准测试集上提高大多数算法的性能。<details>
<summary>Abstract</summary>
Neural Algorithmic Reasoning (NAR) is a research area focused on designing neural architectures that can reliably capture classical computation, usually by learning to execute algorithms. A typical approach is to rely on Graph Neural Network (GNN) architectures, which encode inputs in high-dimensional latent spaces that are repeatedly transformed during the execution of the algorithm. In this work we perform a detailed analysis of the structure of the latent space induced by the GNN when executing algorithms. We identify two possible failure modes: (i) loss of resolution, making it hard to distinguish similar values; (ii) inability to deal with values outside the range observed during training. We propose to solve the first issue by relying on a softmax aggregator, and propose to decay the latent space in order to deal with out-of-range values. We show that these changes lead to improvements on the majority of algorithms in the standard CLRS-30 benchmark when using the state-of-the-art Triplet-GMPNN processor. Our code is available at \href{https://github.com/mirjanic/nar-latent-spaces}{https://github.com/mirjanic/nar-latent-spaces}.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="An-Alternative-to-Variance-Gini-Deviation-for-Risk-averse-Policy-Gradient"><a href="#An-Alternative-to-Variance-Gini-Deviation-for-Risk-averse-Policy-Gradient" class="headerlink" title="An Alternative to Variance: Gini Deviation for Risk-averse Policy Gradient"></a>An Alternative to Variance: Gini Deviation for Risk-averse Policy Gradient</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08873">http://arxiv.org/abs/2307.08873</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yudong Luo, Guiliang Liu, Pascal Poupart, Yangchen Pan</li>
<li>for: 降低奖励学习中的风险偏好，即使是在数值上有些偏好。</li>
<li>methods: 使用新的风险度量——吉尼偏度，代替传统的归一化奖励方法。</li>
<li>results: 在具有明确的风险偏好的领域中，通过对吉尼偏度进行优化，实现高回报低风险的策略学习。其他方法在这些领域中很难学习一个合理的策略。<details>
<summary>Abstract</summary>
Restricting the variance of a policy's return is a popular choice in risk-averse Reinforcement Learning (RL) due to its clear mathematical definition and easy interpretability. Traditional methods directly restrict the total return variance. Recent methods restrict the per-step reward variance as a proxy. We thoroughly examine the limitations of these variance-based methods, such as sensitivity to numerical scale and hindering of policy learning, and propose to use an alternative risk measure, Gini deviation, as a substitute. We study various properties of this new risk measure and derive a policy gradient algorithm to minimize it. Empirical evaluation in domains where risk-aversion can be clearly defined, shows that our algorithm can mitigate the limitations of variance-based risk measures and achieves high return with low risk in terms of variance and Gini deviation when others fail to learn a reasonable policy.
</details>
<details>
<summary>摘要</summary>
限制策略回报的方差是现代学习控制（RL）中的一个受欢迎选择，因为它的数学定义具有明确的定义和易于理解的解释。传统方法直接限制总返回方差。最近的方法则限制每步奖励方差作为代理。我们详细检查了这些方差基于的方法的局限性，如数字化缩放的感度和策略学习妨碍，并提出一种新的风险度量，即吉尼度偏离，作为替代。我们研究了这种新的风险度量的多种性质，并 derivation 一种策略梯度算法来最小化它。实验表明，当其他策略不能学习合理的策略时，我们的算法可以减轻方差基于的风险度量的局限性，并在 variance 和吉尼度偏离方面实现高回报低风险。
</details></li>
</ul>
<hr>
<h2 id="Meta-Value-Learning-a-General-Framework-for-Learning-with-Learning-Awareness"><a href="#Meta-Value-Learning-a-General-Framework-for-Learning-with-Learning-Awareness" class="headerlink" title="Meta-Value Learning: a General Framework for Learning with Learning Awareness"></a>Meta-Value Learning: a General Framework for Learning with Learning Awareness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08863">http://arxiv.org/abs/2307.08863</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/metavaluelearning/metavaluelearning">https://github.com/metavaluelearning/metavaluelearning</a></li>
<li>paper_authors: Tim Cooijmans, Milad Aghajohari, Aaron Courville</li>
<li>For: 本 paper 的目的是解决多体系统中的梯度学习问题，因为梯度来自于一个第一个模型，这个模型不会考虑多体间学习过程的交互。* Methods: 本 paper extend了 LOLA 的想法，开发了一种全面的值基定义优化方法。这种方法的核心是一个我们称为元价值函数，它在每个 JOINT-policy 空间中为每个代理给出一个折抵负号的优化目标。我们 argue 这个梯度比原始目标更可靠，因为元价值函数来自于优化过程中的实际观察。* Results: 我们通过对 Logistic Game 和 Iterated Prisoner’s Dilemma 两个问题进行分析，显示了我们的方法的行为。<details>
<summary>Abstract</summary>
Gradient-based learning in multi-agent systems is difficult because the gradient derives from a first-order model which does not account for the interaction between agents' learning processes. LOLA (arXiv:1709.04326) accounts for this by differentiating through one step of optimization. We extend the ideas of LOLA and develop a fully-general value-based approach to optimization. At the core is a function we call the meta-value, which at each point in joint-policy space gives for each agent a discounted sum of its objective over future optimization steps. We argue that the gradient of the meta-value gives a more reliable improvement direction than the gradient of the original objective, because the meta-value derives from empirical observations of the effects of optimization. We show how the meta-value can be approximated by training a neural network to minimize TD error along optimization trajectories in which agents follow the gradient of the meta-value. We analyze the behavior of our method on the Logistic Game and on the Iterated Prisoner's Dilemma.
</details>
<details>
<summary>摘要</summary>
gradient-based learning in multi-agent systems 困难，因为梯度来自于一个第一阶模型，这个模型不考虑多个代理机器学习过程之间的互动。LOLA（arXiv:1709.04326）提出了一种解决方案，通过一步优化差分。我们在LOLA的基础上发展了一种完全普遍的价值基于方法，其核心是一个我们称为“元价值”的函数，每个代理机器在联合策略空间中的每个点处给每个代理机器一个折扣的未来优化步骤中的目标减少和。我们 argue that the gradient of the meta-value gives a more reliable improvement direction than the gradient of the original objective, because the meta-value derives from empirical observations of the effects of optimization. We show how the meta-value can be approximated by training a neural network to minimize TD error along optimization trajectories in which agents follow the gradient of the meta-value。我们分析了我们的方法在Logistic Game和Iterated Prisoner's Dilemma中的行为。
</details></li>
</ul>
<hr>
<h2 id="Curriculum-Learning-for-Graph-Neural-Networks-A-Multiview-Competence-based-Approach"><a href="#Curriculum-Learning-for-Graph-Neural-Networks-A-Multiview-Competence-based-Approach" class="headerlink" title="Curriculum Learning for Graph Neural Networks: A Multiview Competence-based Approach"></a>Curriculum Learning for Graph Neural Networks: A Multiview Competence-based Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08859">http://arxiv.org/abs/2307.08859</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/CLU-UML/MCCL">https://github.com/CLU-UML/MCCL</a></li>
<li>paper_authors: Nidhi Vakil, Hadi Amiri</li>
<li>for: 本研究旨在提出一种基于图复杂度形式和模型能力的新方法，以便在图神经网络训练中进行有效的课程学习。</li>
<li>methods: 该方法使用了一种调度方案，以确定有效的课程，并考虑了不同的图Difficulty标准和模型能力 durante el entrenamiento。</li>
<li>results: 实验结果表明，该方法可以在真实世界的链接预测和节点分类任务中提供更高的效果，比如使用多个图Difficulty标准和模型能力来评估模型的性能。<details>
<summary>Abstract</summary>
A curriculum is a planned sequence of learning materials and an effective one can make learning efficient and effective for both humans and machines. Recent studies developed effective data-driven curriculum learning approaches for training graph neural networks in language applications. However, existing curriculum learning approaches often employ a single criterion of difficulty in their training paradigms. In this paper, we propose a new perspective on curriculum learning by introducing a novel approach that builds on graph complexity formalisms (as difficulty criteria) and model competence during training. The model consists of a scheduling scheme which derives effective curricula by accounting for different views of sample difficulty and model competence during training. The proposed solution advances existing research in curriculum learning for graph neural networks with the ability to incorporate a fine-grained spectrum of graph difficulty criteria in their training paradigms. Experimental results on real-world link prediction and node classification tasks illustrate the effectiveness of the proposed approach.
</details>
<details>
<summary>摘要</summary>
一个课程是一个规划的学习材料序列，一个有效的课程可以使学习变得更加效率和有效。现代研究已经开发出了训练图型神经网络的数据驱动课程学习方法。然而，现有的课程学习方法通常使用单一的难度标准来训练。在这篇论文中，我们提出了一个新的观点，即基于图型复杂性形式（难度标准）和模型能力的课程学习方法。我们的方法包括一个时间表，它可以从训练中的不同角度来评估题目难度和模型能力，并从中 derivate 有效的课程。我们的解决方案超越了现有的课程学习研究，可以将图型难度标准细分为训练中的不同角度。实验结果显示，我们的方法在实际的连接预测和节点分类任务中具有优秀的效果。
</details></li>
</ul>
<hr>
<h2 id="An-Admissible-Shift-Consistent-Method-for-Recommender-Systems"><a href="#An-Admissible-Shift-Consistent-Method-for-Recommender-Systems" class="headerlink" title="An Admissible Shift-Consistent Method for Recommender Systems"></a>An Admissible Shift-Consistent Method for Recommender Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08857">http://arxiv.org/abs/2307.08857</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tung Nguyen, Jeffrey Uhlmann</li>
<li>for:  solves matrix&#x2F;tensor completion problems in the context of recommender systems</li>
<li>methods:  proposes a new constraint called shift-consistency, and provides a rigorous mathematical description of the method</li>
<li>results:  provably guarantees several key mathematical properties, including satisfaction of an admissibility criterion, fairness, and robustness<details>
<summary>Abstract</summary>
In this paper, we propose a new constraint, called shift-consistency, for solving matrix/tensor completion problems in the context of recommender systems. Our method provably guarantees several key mathematical properties: (1) satisfies a recently established admissibility criterion for recommender systems; (2) satisfies a definition of fairness that eliminates a specific class of potential opportunities for users to maliciously influence system recommendations; and (3) offers robustness by exploiting provable uniqueness of missing-value imputation. We provide a rigorous mathematical description of the method, including its generalization from matrix to tensor form to permit representation and exploitation of complex structural relationships among sets of user and product attributes. We argue that our analysis suggests a structured means for defining latent-space projections that can permit provable performance properties to be established for machine learning methods.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一个新的约束，称为偏移一致性，用于解决Matrix/Tensor completion问题在推荐系统中。我们的方法可以证明满足以下几个关键数学性质：（1）满足推荐系统中最近确立的适用性标准;（2）满足一种定义的公平性，以消除用户恶意影响推荐系统的可能性;（3）具有耐用性，通过利用缺失值填充的可证明唯一性来抗衡。我们提供了一个严格的数学描述，包括矩阵到多重形式的普遍化，以利用用户和产品特征之间的复杂结构关系。我们认为，我们的分析表明了一种结构化的方式，可以让 latent-space 投影具有可证明性能特性。
</details></li>
</ul>
<hr>
<h2 id="Autoregressive-Diffusion-Model-for-Graph-Generation"><a href="#Autoregressive-Diffusion-Model-for-Graph-Generation" class="headerlink" title="Autoregressive Diffusion Model for Graph Generation"></a>Autoregressive Diffusion Model for Graph Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08849">http://arxiv.org/abs/2307.08849</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lingkai Kong, Jiaming Cui, Haotian Sun, Yuchen Zhuang, B. Aditya Prakash, Chao Zhang</li>
<li>for: 本文提出了一种束缚基于模型 для图形生成。</li>
<li>methods: 该模型使用自适应扩散过程，直接在零域图空间中操作。在前向扩散过程中，我们设计了一个数据依赖的节点吸引排序网络，用于学习图的排序。在反向生成过程中，我们设计了一个减噪网络，用于高效地重建图。</li>
<li>results: 我们在六种不同的通用图数据集和两种分子数据集上进行了实验，结果显示，我们的模型可以与之前的状态地图形成比或更好，同时具有快速的生成速度。<details>
<summary>Abstract</summary>
Diffusion-based graph generative models have recently obtained promising results for graph generation. However, existing diffusion-based graph generative models are mostly one-shot generative models that apply Gaussian diffusion in the dequantized adjacency matrix space. Such a strategy can suffer from difficulty in model training, slow sampling speed, and incapability of incorporating constraints. We propose an \emph{autoregressive diffusion} model for graph generation. Unlike existing methods, we define a node-absorbing diffusion process that operates directly in the discrete graph space. For forward diffusion, we design a \emph{diffusion ordering network}, which learns a data-dependent node absorbing ordering from graph topology. For reverse generation, we design a \emph{denoising network} that uses the reverse node ordering to efficiently reconstruct the graph by predicting the node type of the new node and its edges with previously denoised nodes at a time. Based on the permutation invariance of graph, we show that the two networks can be jointly trained by optimizing a simple lower bound of data likelihood. Our experiments on six diverse generic graph datasets and two molecule datasets show that our model achieves better or comparable generation performance with previous state-of-the-art, and meanwhile enjoys fast generation speed.
</details>
<details>
<summary>摘要</summary>
Diffusion-based图生成模型在最近得到了优秀的结果，但现有的扩散基于的图生成模型多数是一次性的生成模型，它们在减量化的相对位图空间内应用扩散。这种策略可能会受到训练模型的困难，慢速的采样速度和约束的不具备。我们提出了一种“自适应扩散”模型，与现有方法不同，我们在离散图空间直接定义节点吸引扩散过程。对于前进扩散，我们设计了一个“扩散排序网络”，它学习从图ptopology得到数据依赖的节点吸引排序。对于逆生成，我们设计了一个“除噪网络”，它使用反向节点排序来高效地重建图， predicting the node type of the new node and its edges with previously denoised nodes at a time。基于图的幂等性，我们表明了这两个网络可以同时训练，通过优化数据可能性函数的简单下界来优化。我们在六种多样化的生成图据集和两个分子数据集上进行了实验，结果表明我们的模型可以与之前的状态时的性能相当或更好，同时具有快速的生成速度。
</details></li>
</ul>
<hr>
<h2 id="Privacy-preserving-patient-clustering-for-personalized-federated-learning"><a href="#Privacy-preserving-patient-clustering-for-personalized-federated-learning" class="headerlink" title="Privacy-preserving patient clustering for personalized federated learning"></a>Privacy-preserving patient clustering for personalized federated learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08847">http://arxiv.org/abs/2307.08847</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/g2lab/pcfbl">https://github.com/g2lab/pcfbl</a></li>
<li>paper_authors: Ahmed Elhussein, Gamze Gursoy</li>
<li>For: 这个研究旨在解决 Federated Learning (FL) 中 data 非同一性 Independent Distribution (non-IID) 问题，并提出 Privacy-preserving Community-Based Federated machine Learning (PCBFL) 框架，可以在不同医院中训练分组学习模型，并保护隐私。* Methods: PCBFL 使用 Secure Multiparty Computation (SMPC) 技术，可以安全地计算不同医院中病人的相似性分数，并使用 clustering 算法将病人分组。* Results: PCBFL 可以成功地将病人分组为低、中、高风险三种群体，并与传统和现有的 Clustered FL 框架进行比较，获得了平均 AUC 提升率4.3% 和 AUPRC 提升率7.8%。<details>
<summary>Abstract</summary>
Federated Learning (FL) is a machine learning framework that enables multiple organizations to train a model without sharing their data with a central server. However, it experiences significant performance degradation if the data is non-identically independently distributed (non-IID). This is a problem in medical settings, where variations in the patient population contribute significantly to distribution differences across hospitals. Personalized FL addresses this issue by accounting for site-specific distribution differences. Clustered FL, a Personalized FL variant, was used to address this problem by clustering patients into groups across hospitals and training separate models on each group. However, privacy concerns remained as a challenge as the clustering process requires exchange of patient-level information. This was previously solved by forming clusters using aggregated data, which led to inaccurate groups and performance degradation. In this study, we propose Privacy-preserving Community-Based Federated machine Learning (PCBFL), a novel Clustered FL framework that can cluster patients using patient-level data while protecting privacy. PCBFL uses Secure Multiparty Computation, a cryptographic technique, to securely calculate patient-level similarity scores across hospitals. We then evaluate PCBFL by training a federated mortality prediction model using 20 sites from the eICU dataset. We compare the performance gain from PCBFL against traditional and existing Clustered FL frameworks. Our results show that PCBFL successfully forms clinically meaningful cohorts of low, medium, and high-risk patients. PCBFL outperforms traditional and existing Clustered FL frameworks with an average AUC improvement of 4.3% and AUPRC improvement of 7.8%.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 是一种机器学习框架，允许多个组织共同训练模型，无需将数据分享到中央服务器。然而，如果数据不是非 identical independently distributed (non-IID)，FL 会经受显著性能下降。这是医疗设置中的问题，Variations in the patient population contribute significantly to distribution differences across hospitals。personalized FL 解决了这个问题，通过考虑各地点特定的分布差异。clustered FL，一种个人化 FL 变体，使用 clustering 方法将患者分组，并在每个组上训练 separating 模型。然而，隐私问题仍然成为挑战，因为 clustering 过程需要交换患者级别信息。这已经解决了通过使用聚合数据来组成 clusters，但这会导致不准确的组和性能下降。在本研究中，我们提出了隐私保护的社区基于 Federated 机器学习 (PCBFL)，一种新的 clustering FL 框架，可以在患者级别数据上 clustering 患者，同时保护隐私。PCBFL 使用 Secure Multiparty Computation，一种密码学技术，以安全地计算各地点患者相似度分数。我们然后评估 PCBFL，通过在 20 个 eICU 数据集中训练一个联邦 Mortality 预测模型。我们比较 PCBFL 的性能与传统和现有的 clustering FL 框架。我们的结果表明，PCBFL 成功划分了低、中、高风险患者的临床意义full cohort。PCBFL 与传统和现有的 clustering FL 框架相比，平均 AUC 提高4.3%，AUPRC 提高7.8%。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-Safe-Policy-Learning-with-Chance-Constrained-Optimization-Application-to-Military-Security-Assessment-during-the-Vietnam-War"><a href="#Bayesian-Safe-Policy-Learning-with-Chance-Constrained-Optimization-Application-to-Military-Security-Assessment-during-the-Vietnam-War" class="headerlink" title="Bayesian Safe Policy Learning with Chance Constrained Optimization: Application to Military Security Assessment during the Vietnam War"></a>Bayesian Safe Policy Learning with Chance Constrained Optimization: Application to Military Security Assessment during the Vietnam War</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08840">http://arxiv.org/abs/2307.08840</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeyang Jia, Eli Ben-Michael, Kosuke Imai</li>
<li>For: The paper aims to improve a security assessment algorithm used during the Vietnam War by using outcomes measured immediately after its introduction in late 1969.* Methods: The paper introduces the Average Conditional Risk (ACRisk) to quantify the risk of worse outcomes for subgroups of individual units, and a Bayesian policy learning framework to maximize the posterior expected value while controlling the ACRisk.* Results: The learned algorithm assesses most regions as more secure and emphasizes economic and political factors over military factors, compared to the actual algorithm used during the Vietnam War.Here are the three points in Simplified Chinese text:* For: 该文章目标是通过1969年底引入的出口来改进越南战争期间安全评估算法。* Methods: 该文章提出了 Conditional Risk (ACRisk) 来衡量各个单位 subgroup 的输出风险，以及 Bayesian 政策学习框架来控制 ACrisk 并最大化 posterior 期望值。* Results: 学习的算法认为大多数地区更安全，并且强调经济和政治因素比军事因素更重要，与实际使用的算法不同。<details>
<summary>Abstract</summary>
Algorithmic and data-driven decisions and recommendations are commonly used in high-stakes decision-making settings such as criminal justice, medicine, and public policy. We investigate whether it would have been possible to improve a security assessment algorithm employed during the Vietnam War, using outcomes measured immediately after its introduction in late 1969. This empirical application raises several methodological challenges that frequently arise in high-stakes algorithmic decision-making. First, before implementing a new algorithm, it is essential to characterize and control the risk of yielding worse outcomes than the existing algorithm. Second, the existing algorithm is deterministic, and learning a new algorithm requires transparent extrapolation. Third, the existing algorithm involves discrete decision tables that are common but difficult to optimize over.   To address these challenges, we introduce the Average Conditional Risk (ACRisk), which first quantifies the risk that a new algorithmic policy leads to worse outcomes for subgroups of individual units and then averages this over the distribution of subgroups. We also propose a Bayesian policy learning framework that maximizes the posterior expected value while controlling the posterior expected ACRisk. This framework separates the estimation of heterogeneous treatment effects from policy optimization, enabling flexible estimation of effects and optimization over complex policy classes. We characterize the resulting chance-constrained optimization problem as a constrained linear programming problem. Our analysis shows that compared to the actual algorithm used during the Vietnam War, the learned algorithm assesses most regions as more secure and emphasizes economic and political factors over military factors.
</details>
<details>
<summary>摘要</summary>
高科技和数据驱动的决策和建议在高风险决策场景中广泛应用，如刑事司法、医疗和公共政策。我们调查了越南战争期间使用的安全评估算法是否可以改进，使用实际实施后的1969年底的结果。这种实践中出现了一些高风险算法决策中常见的方法学挑战。首先，在实施新算法之前，需要评估和控制新算法可能导致差化的风险。第二，现有的算法是 deterministic，需要透明地推断新算法。第三，现有的算法包含精确的决策表，这些表difficult to optimize。为了解决这些挑战，我们引入了 Conditional Risk (ACRisk)，它首先评估新算法政策对各个单位的 subgroup 的风险差化，然后平均这些风险。我们还提出了 Bayesian 政策学习框架，该框架在控制 posterior 预期值时最大化预期值，并且可以灵活地估计影响和优化复杂的政策类型。我们将这种机会constrained optimization问题 characterized as a linear programming problem。我们的分析表明，相比 actual algorithm 使用 during the Vietnam War, the learned algorithm assesses most regions as more secure and emphasizes economic and political factors over military factors。
</details></li>
</ul>
<hr>
<h2 id="A-Meta-Learning-Based-Precoder-Optimization-Framework-for-Rate-Splitting-Multiple-Access"><a href="#A-Meta-Learning-Based-Precoder-Optimization-Framework-for-Rate-Splitting-Multiple-Access" class="headerlink" title="A Meta-Learning Based Precoder Optimization Framework for Rate-Splitting Multiple Access"></a>A Meta-Learning Based Precoder Optimization Framework for Rate-Splitting Multiple Access</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08822">http://arxiv.org/abs/2307.08822</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rafael Cerna Loli, Bruno Clerckx</li>
<li>for: 提出一种基于元学习的RSMA前处理优化框架，直接在无法知道整个通道状态情况的情况下优化RSMA前处理器。</li>
<li>methods: 利用含义过拟合的卷积神经网络来最大化显式均值总bitrate表达式，从而绕过需要其他训练数据的限制。</li>
<li>results: 数值结果显示，元学习基于的解决方案在中等规模场景下与传统前处理优化相当，在大规模场景下明显超越低复杂度前处理算法。<details>
<summary>Abstract</summary>
In this letter, we propose the use of a meta-learning based precoder optimization framework to directly optimize the Rate-Splitting Multiple Access (RSMA) precoders with partial Channel State Information at the Transmitter (CSIT). By exploiting the overfitting of the compact neural network to maximize the explicit Average Sum-Rate (ASR) expression, we effectively bypass the need for any other training data while minimizing the total running time. Numerical results reveal that the meta-learning based solution achieves similar ASR performance to conventional precoder optimization in medium-scale scenarios, and significantly outperforms sub-optimal low complexity precoder algorithms in the large-scale regime.
</details>
<details>
<summary>摘要</summary>
在这封信中，我们提议使用基于meta-学习的precoder优化框架来直接优化Rate-Splitting Multiple Access（RSMA）precoder，使用 transmitter （CSIT）中的部分通道状态信息。通过利用Compact Neural Network的过度适应来最大化显式Average Sum-Rate（ASR）表达，我们可以快速减少训练数据量，同时减少总耗时。 numerically 的结果表明，基于meta-学习的解决方案在中型情景下与传统precoder优化相似的ASR性能，而在大规模情景下明显超过低复杂度precoder算法。Here's the translation of the text into Traditional Chinese:在这封信中，我们提议使用基于meta-学习的precoder优化框架来直接优化Rate-Splitting Multiple Access（RSMA）precoder，使用传递器（CSIT）中的部分通道状态信息。通过利用Compact Neural Network的过度适应来最大化显式Average Sum-Rate（ASR）表达，我们可以快速减少训练数据量，同时减少总耗时。numerically 的结果显示，基于meta-学习的解决方案在中型情景下与传统precoder优化相似的ASR性能，而在大规模情景下明显超过低复杂度precoder算法。
</details></li>
</ul>
<hr>
<h2 id="Towards-Accelerating-Benders-Decomposition-via-Reinforcement-Learning-Surrogate-Models"><a href="#Towards-Accelerating-Benders-Decomposition-via-Reinforcement-Learning-Surrogate-Models" class="headerlink" title="Towards Accelerating Benders Decomposition via Reinforcement Learning Surrogate Models"></a>Towards Accelerating Benders Decomposition via Reinforcement Learning Surrogate Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08816">http://arxiv.org/abs/2307.08816</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephen Mak, Kyle Mana, Parisa Zehtabi, Michael Cashmore, Daniele Magazzeni, Manuela Veloso</li>
<li>for: 这篇论文是为了提出一种快速化Benders decomposition（BD）方法，以解决受不确定性影响的数值优化问题。</li>
<li>methods: 本论文使用的方法是BD方法，并利用一个代理模型来取代NP困难的整数主问题，以加速BD方法的执行。</li>
<li>results: 在实验中，这种加速BD方法可以让解决随机存储管理问题的时间提高30%，比其他加速BD实现方法更快。<details>
<summary>Abstract</summary>
Stochastic optimization (SO) attempts to offer optimal decisions in the presence of uncertainty. Often, the classical formulation of these problems becomes intractable due to (a) the number of scenarios required to capture the uncertainty and (b) the discrete nature of real-world planning problems. To overcome these tractability issues, practitioners turn to decomposition methods that divide the problem into smaller, more tractable sub-problems. The focal decomposition method of this paper is Benders decomposition (BD), which decomposes stochastic optimization problems on the basis of scenario independence. In this paper we propose a method of accelerating BD with the aid of a surrogate model in place of an NP-hard integer master problem. Through the acceleration method we observe 30% faster average convergence when compared to other accelerated BD implementations. We introduce a reinforcement learning agent as a surrogate and demonstrate how it can be used to solve a stochastic inventory management problem.
</details>
<details>
<summary>摘要</summary>
In this paper, we propose a method for accelerating Benders decomposition (BD), a popular decomposition method for stochastic optimization problems, using a surrogate model in place of the NP-hard integer master problem. Our method leverages a reinforcement learning agent as the surrogate and demonstrates its effectiveness in solving a stochastic inventory management problem. We observe an average convergence rate 30% faster than other accelerated BD implementations.Here is the text in Simplified Chinese: Stochastic optimization (SO) 尝试提供在不确定环境中的优化决策。然而， classical 的问题表述方式可能会变得不可求解，因为需要大量的enario来捕捉不确定性，以及实际规划问题的精度问题。为了解决这些 tractability 问题，专家们经常使用分解方法，将问题分解成更加可控的子问题。在这篇论文中，我们提出了使用代理模型加速 Benders 分解（BD）的方法。我们使用 reinforcement learning 代理来解决一个不确定存储管理问题。我们观察到，使用这种加速方法可以比其他加速BD实现的平均速度提高30%。
</details></li>
</ul>
<hr>
<h2 id="Comparative-Performance-Evaluation-of-Large-Language-Models-for-Extracting-Molecular-Interactions-and-Pathway-Knowledge"><a href="#Comparative-Performance-Evaluation-of-Large-Language-Models-for-Extracting-Molecular-Interactions-and-Pathway-Knowledge" class="headerlink" title="Comparative Performance Evaluation of Large Language Models for Extracting Molecular Interactions and Pathway Knowledge"></a>Comparative Performance Evaluation of Large Language Models for Extracting Molecular Interactions and Pathway Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08813">http://arxiv.org/abs/2307.08813</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/boxorange/bioie-llm">https://github.com/boxorange/bioie-llm</a></li>
<li>paper_authors: Gilchan Park, Byung-Jun Yoon, Xihaier Luo, Vanessa López-Marrero, Patrick Johnstone, Shinjae Yoo, Francis J. Alexander</li>
<li>for: 这项研究的目的是使用大型自然语言模型来自动从科学文献中提取蛋白质相互作用、蛋白质通路和基因调控关系的知识。</li>
<li>methods: 本研究使用了不同的大型自然语言模型来完成蛋白质相互作用、蛋白质通路和基因调控关系的识别任务。</li>
<li>results: 研究发现了不同的大型自然语言模型在完成这些任务时的效果，并提供了一些显著的发现和未来的机会，以及仍然存在的挑战。In English, it means:</li>
<li>for: The goal of this study is to use large language models to automatically extract knowledge of protein interactions, pathways, and gene regulatory relations from scientific literature.</li>
<li>methods: The study uses different large language models to complete tasks of recognizing protein interactions, pathways, and gene regulatory relations.</li>
<li>results: The study finds the effectiveness of different language models in completing these tasks, provides significant findings, and discusses future opportunities and remaining challenges.<details>
<summary>Abstract</summary>
Understanding protein interactions and pathway knowledge is crucial for unraveling the complexities of living systems and investigating the underlying mechanisms of biological functions and complex diseases. While existing databases provide curated biological data from literature and other sources, they are often incomplete and their maintenance is labor-intensive, necessitating alternative approaches. In this study, we propose to harness the capabilities of large language models to address these issues by automatically extracting such knowledge from the relevant scientific literature. Toward this goal, in this work, we investigate the effectiveness of different large language models in tasks that involve recognizing protein interactions, pathways, and gene regulatory relations. We thoroughly evaluate the performance of various models, highlight the significant findings, and discuss both the future opportunities and the remaining challenges associated with this approach. The code and data are available at: https://github.com/boxorange/BioIE-LLM
</details>
<details>
<summary>摘要</summary>
理解蛋白交互和生物路径知识是生物系统复杂性的关键，帮助我们探索生物功能和复杂疾病的基础机理。现有数据库提供了文献和其他来源中的生物数据，但这些数据库经常受到不完整性和维护劳动的限制，需要新的方法。在本研究中，我们利用大型自然语言模型来解决这些问题，自动从相关的科学文献中提取生物知识。为达到这个目标，我们在这篇论文中评估了不同的大型自然语言模型在蛋白交互、生物路径和蛋白质调控关系的识别任务中的效果。我们仔细评估了各模型的表现，披露了重要的发现，并讨论了这种方法的未来机会和仍然存在的挑战。代码和数据可以在 GitHub 上获取：<https://github.com/boxorange/BioIE-LLM>。
</details></li>
</ul>
<hr>
<h2 id="DeepMem-ML-Models-as-storage-channels-and-their-mis-applications"><a href="#DeepMem-ML-Models-as-storage-channels-and-their-mis-applications" class="headerlink" title="DeepMem: ML Models as storage channels and their (mis-)applications"></a>DeepMem: ML Models as storage channels and their (mis-)applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08811">http://arxiv.org/abs/2307.08811</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Abdullah Al Mamun, Quazi Mishkatul Alam, Erfan Shaigani, Pedram Zaree, Ihsen Alouani, Nael Abu-Ghazaleh</li>
<li>for: 本文提出了一种新的信息理论视角，视 ML 模型为一个存储通道，并研究了在这个存储通道上进行隐藏信息的存储和检测。</li>
<li>methods: 作者使用了一种黑盒访问方式，通过在训练时嵌入隐藏信息，并在部署后使用黑盒访问来检测和提取隐藏信息。</li>
<li>results: 作者分析了存储 primitives 和检测 primitives，并提出了一种基于 ML 特有的替换基于错误 correction 协议来提高存储 primitives 的可靠性。<details>
<summary>Abstract</summary>
Machine learning (ML) models are overparameterized to support generality and avoid overfitting. Prior works have shown that these additional parameters can be used for both malicious (e.g., hiding a model covertly within a trained model) and beneficial purposes (e.g., watermarking a model). In this paper, we propose a novel information theoretic perspective of the problem; we consider the ML model as a storage channel with a capacity that increases with overparameterization. Specifically, we consider a sender that embeds arbitrary information in the model at training time, which can be extracted by a receiver with a black-box access to the deployed model. We derive an upper bound on the capacity of the channel based on the number of available parameters. We then explore black-box write and read primitives that allow the attacker to: (i) store data in an optimized way within the model by augmenting the training data at the transmitter side, and (ii) to read it by querying the model after it is deployed. We also analyze the detectability of the writing primitive and consider a new version of the problem which takes information storage covertness into account. Specifically, to obtain storage covertness, we introduce a new constraint such that the data augmentation used for the write primitives minimizes the distribution shift with the initial (baseline task) distribution. This constraint introduces a level of "interference" with the initial task, thereby limiting the channel's effective capacity. Therefore, we develop optimizations to improve the capacity in this case, including a novel ML-specific substitution based error correction protocol. We believe that the proposed modeling of the problem offers new tools to better understand and mitigate potential vulnerabilities of ML, especially in the context of increasingly large models.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:机器学习（ML）模型通常过过参数化来支持通用性和避免过拟合。先前的研究表明，这些额外参数可以用于both malicious（如隐藏一个模型在训练过程中）和有益目的（如模型水印）。在这篇论文中，我们提出了一种新的信息理论视角，视ML模型为一个存储通道，其容量随参数的增加而增加。 Specifically，我们考虑一个发送者在训练时将自定义信息嵌入模型中，并且通过黑盒访问已部署模型来提取这些信息。我们得出了参数的容量的上限，并explore黑盒写和读 primitives，allowing the attacker to: (i) 在发送方 сторо面优化数据，以便在部署后通过模型进行读取，和 (ii) 通过访问部署后的模型来读取数据。我们还分析了写 primitives的检测性，并考虑了一个新的问题，即存储隐蔽性。 Specifically, to obtain storage covertness, we introduce a new constraint such that the data augmentation used for the write primitives minimizes the distribution shift with the initial (baseline task) distribution. This constraint introduces a level of "interference" with the initial task, thereby limiting the channel's effective capacity. Therefore, we develop optimizations to improve the capacity in this case, including a novel ML-specific substitution based error correction protocol. We believe that the proposed modeling of the problem offers new tools to better understand and mitigate potential vulnerabilities of ML, especially in the context of increasingly large models.Translated into Traditional Chinese:机器学习（ML）模型通常过过参数化来支持通用性和避免过拟合。先前的研究表明，这些额外参数可以用于both malicious（如隐藏一个模型在训练过程中）和有益目的（如模型水印）。在这篇论文中，我们提出了一个新的信息理论角度，视ML模型为一个存储通道，其容量随参数的增加而增加。 Specifically，我们考虑一个发送者在训练时将自定义信息嵌入模型中，并且透过黑盒访问已部署模型来提取这些信息。我们得出了参数的容量的上限，并explore黑盒写和读 primitives，allowing the attacker to: (i) 在发送方 сторо面优化数据，以便在部署后通过模型进行读取，和 (ii) 通过访问部署后的模型来读取数据。我们还分析了写 primitives的检测性，并考虑了一个新的问题，即存储隐蔽性。 Specifically, to obtain storage covertness, we introduce a new constraint such that the data augmentation used for the write primitives minimizes the distribution shift with the initial (baseline task) distribution. This constraint introduces a level of "interference" with the initial task, thereby limiting the channel's effective capacity. Therefore, we develop optimizations to improve the capacity in this case, including a novel ML-specific substitution based error correction protocol. We believe that the proposed modeling of the problem offers new tools to better understand and mitigate potential vulnerabilities of ML, especially in the context of increasingly large models.
</details></li>
</ul>
<hr>
<h2 id="Operator-Guidance-Informed-by-AI-Augmented-Simulations"><a href="#Operator-Guidance-Informed-by-AI-Augmented-Simulations" class="headerlink" title="Operator Guidance Informed by AI-Augmented Simulations"></a>Operator Guidance Informed by AI-Augmented Simulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08810">http://arxiv.org/abs/2307.08810</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samuel J. Edwards, Michael Levine</li>
<li>for: 这个论文是为了计算船舶响应统计数据的多优化、数据适应方法。</li>
<li>methods: 这个论文使用了Long Short-Term Memory（LSTM）神经网络，以及一个快速低精度的计算工具SimpleCode，以及一个更高精度的计算工具Large Amplitude Motion Program（LAMP）。</li>
<li>results: 研究发现，使用LSTM神经网络可以准确地估计船舶响应统计数据，并且可以在不同的海洋条件下提供高精度的结果。<details>
<summary>Abstract</summary>
This paper will present a multi-fidelity, data-adaptive approach with a Long Short-Term Memory (LSTM) neural network to estimate ship response statistics in bimodal, bidirectional seas. The study will employ a fast low-fidelity, volume-based tool SimpleCode and a higher-fidelity tool known as the Large Amplitude Motion Program (LAMP). SimpleCode and LAMP data were generated by common bi-modal, bi-directional sea conditions in the North Atlantic as training data. After training an LSTM network with LAMP ship motion response data, a sample route was traversed and randomly sampled historical weather was input into SimpleCode and the LSTM network, and compared against the higher fidelity results.
</details>
<details>
<summary>摘要</summary>
这篇论文将介绍一种多模精度、数据适应的方法，使用长期快速响应（LSTM）神经网络来估算船舶响应统计在双模态、双向海域中。这项研究将使用快速低精度的SimpleCode工具和高精度的Large Amplitude Motion Program（LAMP）工具。SimpleCode和LAMP数据都是通过共同的双模态、双向海域条件在北大西洋中生成的训练数据。 после训练LSTM网络使用LAMP船舶运动数据，一个示例路线被跨越，并将SimpleCode和LSTM网络输入历史气象数据，并与更高精度结果进行比较。
</details></li>
</ul>
<hr>
<h2 id="Local-or-Global-Selective-Knowledge-Assimilation-for-Federated-Learning-with-Limited-Labels"><a href="#Local-or-Global-Selective-Knowledge-Assimilation-for-Federated-Learning-with-Limited-Labels" class="headerlink" title="Local or Global: Selective Knowledge Assimilation for Federated Learning with Limited Labels"></a>Local or Global: Selective Knowledge Assimilation for Federated Learning with Limited Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08809">http://arxiv.org/abs/2307.08809</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yae Jee Cho, Gauri Joshi, Dimitrios Dimitriadis</li>
<li>for: 提高 federated learning 的效果，尤其是在 client 有限的标签数据的情况下。</li>
<li>methods: 提出 FedLabel 方法，使 client 可以选择本地或全局模型来pseudo-标签未标签数据，并通过全局-本地一致常量正则化来利用两个模型的知识。</li>
<li>results: 在 cross-device 和 cross-silo  Setting 中，FedLabel 比其他 semi-supervised FL 基线方法提高 $8$-$24%$，甚至超过了标准全部标签 FL 基线($100%$ 标签数据)，只使用 $5$-$20%$ 的标签数据。<details>
<summary>Abstract</summary>
Many existing FL methods assume clients with fully-labeled data, while in realistic settings, clients have limited labels due to the expensive and laborious process of labeling. Limited labeled local data of the clients often leads to their local model having poor generalization abilities to their larger unlabeled local data, such as having class-distribution mismatch with the unlabeled data. As a result, clients may instead look to benefit from the global model trained across clients to leverage their unlabeled data, but this also becomes difficult due to data heterogeneity across clients. In our work, we propose FedLabel where clients selectively choose the local or global model to pseudo-label their unlabeled data depending on which is more of an expert of the data. We further utilize both the local and global models' knowledge via global-local consistency regularization which minimizes the divergence between the two models' outputs when they have identical pseudo-labels for the unlabeled data. Unlike other semi-supervised FL baselines, our method does not require additional experts other than the local or global model, nor require additional parameters to be communicated. We also do not assume any server-labeled data or fully labeled clients. For both cross-device and cross-silo settings, we show that FedLabel outperforms other semi-supervised FL baselines by $8$-$24\%$, and even outperforms standard fully supervised FL baselines ($100\%$ labeled data) with only $5$-$20\%$ of labeled data.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Anomaly-Detection-with-Selective-Dictionary-Learning"><a href="#Anomaly-Detection-with-Selective-Dictionary-Learning" class="headerlink" title="Anomaly Detection with Selective Dictionary Learning"></a>Anomaly Detection with Selective Dictionary Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08807">http://arxiv.org/abs/2307.08807</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/denisilie94/pyod-dl">https://github.com/denisilie94/pyod-dl</a></li>
<li>paper_authors: Denis C. Ilie-Ablachim, Bogdan Dumitrescu</li>
<li>for: 本研究提出了基于词典学习（DL）和核心词典学习（KDL）的新型异常检测方法。</li>
<li>methods: 本研究使用了已知的DL和KDL算法，并将其改进为无监督的异常检测方法。此外，我们还提出了一种减少kernel版本（RKDL），用于解决大数据集问题。</li>
<li>results: 我们的算法在一个异常检测工具箱中引入，并与标准 referéncé результаты进行比较。<details>
<summary>Abstract</summary>
In this paper we present new methods of anomaly detection based on Dictionary Learning (DL) and Kernel Dictionary Learning (KDL). The main contribution consists in the adaption of known DL and KDL algorithms in the form of unsupervised methods, used for outlier detection. We propose a reduced kernel version (RKDL), which is useful for problems with large data sets, due to the large kernel matrix. We also improve the DL and RKDL methods by the use of a random selection of signals, which aims to eliminate the outliers from the training procedure. All our algorithms are introduced in an anomaly detection toolbox and are compared to standard benchmark results.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了基于字典学习（DL）和核函数字典学习（KDL）的新方法，用于异常检测。我们的主要贡献在于将知名的DL和KDL算法改进为无监督的方法，用于异常检测。我们还提出了一种减小核kernel版本（RKDL），适用于具有大数据集的问题，因为大 kernel 矩阵。此外，我们还使用随机选择的信号，以消除异常从训练过程中。我们的所有算法都是在异常检测工具箱中引入，并与标准 Referenz结果进行比较。Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese. If you prefer Traditional Chinese, please let me know and I can provide the translation in that version as well.
</details></li>
</ul>
<hr>
<h2 id="Towards-Automated-Design-of-Riboswitches"><a href="#Towards-Automated-Design-of-Riboswitches" class="headerlink" title="Towards Automated Design of Riboswitches"></a>Towards Automated Design of Riboswitches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08801">http://arxiv.org/abs/2307.08801</a></li>
<li>repo_url: None</li>
<li>paper_authors: Frederic Runge, Jörg K. H. Franke, Frank Hutter</li>
<li>for: 本研究旨在开发一种新的计算方法，用于降低扩ycz的筛选和选择成本，以提高核酸扩ycz的发现效率。</li>
<li>methods: 本研究使用了一种新的结构基于的设计方法，考虑了全球性和愿望的序列和结构特征。</li>
<li>results: 研究人员通过使用libLEARNA方法，成功地设计了茶苷核酸扩ycz库，包含30%更多的高质量独特候选者。<details>
<summary>Abstract</summary>
Experimental screening and selection pipelines for the discovery of novel riboswitches are expensive, time-consuming, and inefficient. Using computational methods to reduce the number of candidates for the screen could drastically decrease these costs. However, existing computational approaches do not fully satisfy all requirements for the design of such initial screening libraries. In this work, we present a new method, libLEARNA, capable of providing RNA focus libraries of diverse variable-length qualified candidates. Our novel structure-based design approach considers global properties as well as desired sequence and structure features. We demonstrate the benefits of our method by designing theophylline riboswitch libraries, following a previously published protocol, and yielding 30% more unique high-quality candidates.
</details>
<details>
<summary>摘要</summary>
现有的实验室检测和选择管道可能会带来高额的成本和时间开销，同时效率也不高。使用计算机方法来减少层次的候选人选择可能会带来极大的成本降低。然而，现有的计算方法并不完全满足初步层次检测图书馆的设计需求。在这个工作中，我们提出了一种新的方法，libLEARNA，可以提供多样化变长资格候选人库。我们的新的结构基于设计方法考虑了全局特性以及愿望的序列和结构特征。我们示出了我们的方法的优势，通过采用已发表的卡夫曼协议，设计了茶苷核酸抑制 riboswitch库，并且获得了30%更多的独特高质量候选人。
</details></li>
</ul>
<hr>
<h2 id="regulAS-A-Bioinformatics-Tool-for-the-Integrative-Analysis-of-Alternative-Splicing-Regulome-using-RNA-Seq-data"><a href="#regulAS-A-Bioinformatics-Tool-for-the-Integrative-Analysis-of-Alternative-Splicing-Regulome-using-RNA-Seq-data" class="headerlink" title="regulAS: A Bioinformatics Tool for the Integrative Analysis of Alternative Splicing Regulome using RNA-Seq data"></a>regulAS: A Bioinformatics Tool for the Integrative Analysis of Alternative Splicing Regulome using RNA-Seq data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08800">http://arxiv.org/abs/2307.08800</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/slipnitskaya/regulas">https://github.com/slipnitskaya/regulas</a></li>
<li>paper_authors: Sofya Lipnitskaya</li>
<li>for: regulAS is designed to support computational biology researchers in investigating regulatory mechanisms of splicing alterations in cancer and healthy human donors.</li>
<li>methods: regulAS uses integrative analysis of large-scale RNA-Seq data from TCGA and GTEx projects, with features such as RNA-Seq data retrieval, predictive modeling, and flexible reporting.</li>
<li>results: regulAS provides automated solutions for alternative splicing and cancer biology studies, enhancing efficiency, reproducibility, and customization of experimental design, with the extensibility to tailor the software to specific research needs.Here’s the same information in Simplified Chinese:</li>
<li>for: regulAS 是为 computation biology 研究人员提供一个支持工具，用于调查转录调节变化的规则机制，以及人体和癌症样本中的转录调节。</li>
<li>methods: regulAS 使用了大规模 RNA-Seq 数据，包括 TCGA 和 GTEx 项目，并提供了一些功能，如 RNA-Seq 数据检索、预测模型和灵活报告生成。</li>
<li>results: regulAS 提供了一个自动化的解决方案，用于研究转录调节和癌症生物学，提高了效率、可重复性和自定义实验设计的能力，同时允许研究人员根据自己的需求进行特定的自定义和扩展。<details>
<summary>Abstract</summary>
The regulAS software package is a bioinformatics tool designed to support computational biology researchers in investigating regulatory mechanisms of splicing alterations through integrative analysis of large-scale RNA-Seq data from cancer and healthy human donors, characterized by TCGA and GTEx projects. This technical report provides a comprehensive overview of regulAS, focusing on its core functionality, basic modules, experiment configuration, further extensibility and customisation.   The core functionality of regulAS enables the automation of computational experiments, efficient results storage and processing, and streamlined workflow management. Integrated basic modules extend regulAS with features such as RNA-Seq data retrieval from the public multi-omics UCSC Xena data repository, predictive modeling and feature ranking capabilities using the scikit-learn package, and flexible reporting generation for analysing gene expression profiles and relevant modulations of alternative splicing aberrations across tissues and cancer types. Experiment configuration is handled through YAML files with the Hydra and OmegaConf libraries, offering a user-friendly approach. Additionally, regulAS allows for the development and integration of custom modules to handle specialized tasks.   In conclusion, regulAS provides an automated solution for alternative splicing and cancer biology studies, enhancing efficiency, reproducibility, and customization of experimental design, while the extensibility of the pipeline enables researchers to further tailor the software package to their specific needs. Source code is available under the MIT license at https://github.com/slipnitskaya/regulAS.
</details>
<details>
<summary>摘要</summary>
regulAS 软件包是一款 bioinformatics 工具，用于支持生物计算研究人员在 investigate 蛋白水平修饰的调控机制方面进行集成分析大规模 RNA-Seq 数据。这份技术报告提供了 regulAS 的全面介绍，重点介绍其核心功能、基本模块、实验配置、进一步扩展和自定义。regulAS 的核心功能包括自动化计算实验、高效存储和处理结果，以及流程管理。 integrate 的基本模块包括从公共多元素 UCSC Xena 数据存储库中获取 RNA-Seq 数据、使用 scikit-learn 包进行预测模型和特征排名，以及自定义报告生成分析蛋白表达资料和相关的修饰异常现象 across 组织和癌种。实验配置通过 YAML 文件与 Hydra 和 OmegaConf 库进行处理，提供了一种用户友好的方法。此外，regulAS 还允许开发和集成特殊任务的自定义模块。总之，regulAS 提供了一个自动化的蛋白水平修饰和癌生物学研究的解决方案，提高了效率、可重复性和实验设计的自定义能力，同时 pipeline 的可扩展性允许研究人员根据自己的具体需求进行进一步的定制。源代码可以在 <https://github.com/slipnitskaya/regulAS> 获取，采用 MIT 许可证。
</details></li>
</ul>
<hr>
<h2 id="Reduced-Kernel-Dictionary-Learning"><a href="#Reduced-Kernel-Dictionary-Learning" class="headerlink" title="Reduced Kernel Dictionary Learning"></a>Reduced Kernel Dictionary Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08798">http://arxiv.org/abs/2307.08798</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/denisilie94/rkdl">https://github.com/denisilie94/rkdl</a></li>
<li>paper_authors: Denis C. Ilie-Ablachim, Bogdan Dumitrescu</li>
<li>for: 这篇论文是为了解决大量数据集时，常见的问题：核心矩阵的大小。</li>
<li>methods: 本文提出了一种新的方法，即使用训练精简表示法来生成减少大小的非线性表示。具体来说，我们使用梯度下降步骤来优化核kernel向量。</li>
<li>results: 我们通过三个数据集的实验显示，我们的方法可以提供更好的表示，即使使用一小数量的核kernel向量，同时也可以降低执行时间。<details>
<summary>Abstract</summary>
In this paper we present new algorithms for training reduced-size nonlinear representations in the Kernel Dictionary Learning (KDL) problem. Standard KDL has the drawback of a large size of the kernel matrix when the data set is large. There are several ways of reducing the kernel size, notably Nystr\"om sampling. We propose here a method more in the spirit of dictionary learning, where the kernel vectors are obtained with a trained sparse representation of the input signals. Moreover, we optimize directly the kernel vectors in the KDL process, using gradient descent steps. We show with three data sets that our algorithms are able to provide better representations, despite using a small number of kernel vectors, and also decrease the execution time with respect to KDL.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了新的算法用于在kernel Dictionary Learning（KDL）问题中训练减小非线性表示。标准KDL在数据集大时具有大kernel矩阵的缺点。有几种减小kernel大小的方法，其中一种是Nystr\"om sampling。我们在这里提出了一种更接近字典学习的方法，其中kernel вектор通过输入信号的训练稀缺表示获得。此外，我们直接在KDL过程中优化kernel вектор，使用梯度下降步骤。我们通过三个数据集的实验表明，我们的算法能够提供更好的表示，即使使用少量kernel вектор，同时也降低了与KDL的执行时间。
</details></li>
</ul>
<hr>
<h2 id="Classification-with-Incoherent-Kernel-Dictionary-Learning"><a href="#Classification-with-Incoherent-Kernel-Dictionary-Learning" class="headerlink" title="Classification with Incoherent Kernel Dictionary Learning"></a>Classification with Incoherent Kernel Dictionary Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08796">http://arxiv.org/abs/2307.08796</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/denisilie94/incoherent-kernel-dictionary-learning">https://github.com/denisilie94/incoherent-kernel-dictionary-learning</a></li>
<li>paper_authors: Denis C. Ilie-Ablachim, Bogdan Dumitrescu</li>
<li>for: 这个论文提出了一种基于字典学习（DL）的新的分类方法。</li>
<li>methods: 该方法使用了一种基于kernel的协方差DL，与标准线性版本的DL进行比较。此外，我们还提出了AK-SVD算法中的表示更新改进。</li>
<li>results: 我们对多个流行的分类问题数据库进行了测试，并得到了优秀的结果。<details>
<summary>Abstract</summary>
In this paper we present a new classification method based on Dictionary Learning (DL). The main contribution consists of a kernel version of incoherent DL, derived from its standard linear counterpart. We also propose an improvement of the AK-SVD algorithm concerning the representation update. Our algorithms are tested on several popular databases of classification problems.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种基于词典学习（DL）的新的分类方法。我们的主要贡献是基于 стандар的线性DL的kernel版本。此外，我们还提出了对AK-SVD算法的表示更新方法。我们的算法在一些流行的分类问题数据库上进行了测试。Here's a breakdown of the translation:* "In this paper" becomes "在这篇论文中"* "we present" becomes "我们提出"* "a new classification method" becomes "一种新的分类方法"* "based on Dictionary Learning (DL)" becomes "基于词典学习（DL）"* "The main contribution consists of" becomes "主要贡献是"* "a kernel version of incoherent DL" becomes "基于stanдар的线性DL的kernel版本"* "derived from its standard linear counterpart" becomes "从其标准线性对应部分 derivated"* "We also propose an improvement of the AK-SVD algorithm" becomes "此外，我们还提出了对AK-SVD算法的表示更新方法"* "concerning the representation update" becomes "关于表示更新"* "Our algorithms are tested on several popular databases of classification problems" becomes "我们的算法在一些流行的分类问题数据库上进行了测试"
</details></li>
</ul>
<hr>
<h2 id="Non-Stationary-Policy-Learning-for-Multi-Timescale-Multi-Agent-Reinforcement-Learning"><a href="#Non-Stationary-Policy-Learning-for-Multi-Timescale-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Non-Stationary Policy Learning for Multi-Timescale Multi-Agent Reinforcement Learning"></a>Non-Stationary Policy Learning for Multi-Timescale Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08794">http://arxiv.org/abs/2307.08794</a></li>
<li>repo_url: None</li>
<li>paper_authors: Patrick Emami, Xiangyu Zhang, David Biagioni, Ahmed S. Zamzam</li>
<li>for: This paper is written for learning non-stationary policies in multi-timescale multi-agent reinforcement learning (MARL) environments.</li>
<li>methods: The paper proposes a simple framework for learning non-stationary policies, using available information about agent timescales to define a periodic time encoding. The proposed algorithm uses phase-functioned neural networks to parameterize the actor and critic, providing an inductive bias for periodicity.</li>
<li>results: The paper demonstrates the effectiveness of the proposed framework in learning multi-timescale policies through simulations in a gridworld and building energy management environment.<details>
<summary>Abstract</summary>
In multi-timescale multi-agent reinforcement learning (MARL), agents interact across different timescales. In general, policies for time-dependent behaviors, such as those induced by multiple timescales, are non-stationary. Learning non-stationary policies is challenging and typically requires sophisticated or inefficient algorithms. Motivated by the prevalence of this control problem in real-world complex systems, we introduce a simple framework for learning non-stationary policies for multi-timescale MARL. Our approach uses available information about agent timescales to define a periodic time encoding. In detail, we theoretically demonstrate that the effects of non-stationarity introduced by multiple timescales can be learned by a periodic multi-agent policy. To learn such policies, we propose a policy gradient algorithm that parameterizes the actor and critic with phase-functioned neural networks, which provide an inductive bias for periodicity. The framework's ability to effectively learn multi-timescale policies is validated on a gridworld and building energy management environment.
</details>
<details>
<summary>摘要</summary>
在多时间步骤多代理人学习（MARL）中，代理人在不同的时间步骤之间互动。一般来说，由多个时间步骤引起的政策是非站立的。学习非站立政策具有挑战性，通常需要复杂或不fficient的算法。由实际世界中复杂系统中的控制问题的普遍性启发了我们，我们提出了一个简单的框架 для学习非站立政策。我们使用代理人的时间步骤信息来定义周期时间编码。在详细的演示中，我们证明了由多个时间步骤引起的非站立效果可以通过周期多代理人政策学习。为学习这种政策，我们提议使用phasic函数神经网络来参数化actor和critic，这些神经网络提供了周期性的偏好。我们的框架能够有效地学习多时间步骤的政策，并在格リッド世界和建筑能源管理环境中验证了其效果。
</details></li>
</ul>
<hr>
<h2 id="Quarl-A-Learning-Based-Quantum-Circuit-Optimizer"><a href="#Quarl-A-Learning-Based-Quantum-Circuit-Optimizer" class="headerlink" title="Quarl: A Learning-Based Quantum Circuit Optimizer"></a>Quarl: A Learning-Based Quantum Circuit Optimizer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10120">http://arxiv.org/abs/2307.10120</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zikun Li, Jinjun Peng, Yixuan Mei, Sina Lin, Yi Wu, Oded Padon, Zhihao Jia</li>
<li>for: 优化量子Circuit是一个具有很大搜索空间的函数等价Circuit的优化问题，需要应用变换来实现最终性能提高。这篇论文介绍了Quarl，一种基于学习的量子Circuit优化器。</li>
<li>methods: Quarl使用了复制学习（RL）来优化量子Circuit，但RL在量子Circuit优化中存在两个主要挑战：巨大和变化的行动空间，以及非均匀的状态表示。Quarl使用了一种新的神经网络架构和RL训练过程来解决这些问题。</li>
<li>results: 我们的评估显示，Quarl在大多数benchmark Circuit上显著超越了现有的Circuit优化器。另外，Quarl可以学习执行旋转合并，这是现有优化器中的一种复杂、非本地的循环优化。<details>
<summary>Abstract</summary>
Optimizing quantum circuits is challenging due to the very large search space of functionally equivalent circuits and the necessity of applying transformations that temporarily decrease performance to achieve a final performance improvement. This paper presents Quarl, a learning-based quantum circuit optimizer. Applying reinforcement learning (RL) to quantum circuit optimization raises two main challenges: the large and varying action space and the non-uniform state representation. Quarl addresses these issues with a novel neural architecture and RL-training procedure. Our neural architecture decomposes the action space into two parts and leverages graph neural networks in its state representation, both of which are guided by the intuition that optimization decisions can be mostly guided by local reasoning while allowing global circuit-wide reasoning. Our evaluation shows that Quarl significantly outperforms existing circuit optimizers on almost all benchmark circuits. Surprisingly, Quarl can learn to perform rotation merging, a complex, non-local circuit optimization implemented as a separate pass in existing optimizers.
</details>
<details>
<summary>摘要</summary>
优化量子Circuit是一项挑战性的任务，因为函数相同Circuit的搜索空间非常大，并且需要应用变换，暂时降低性能，以达到最终的性能提高。本文介绍Quarl，一种基于学习的量子Circuit优化器。在应用了反射学习（RL）到量子Circuit优化时，存在两个主要挑战：大和变化的动作空间，以及不均匀的状态表示。Quarl通过一种新的神经网络架构和RL训练过程来解决这些问题。我们的神经网络架构将动作空间分解成两个部分，并使用图 нейрон网络来表示状态，两者均受到了认为优化决策可以主要由本地逻辑引导，同时允许全局Circuit范围内的逻辑。我们的评估表明，Quarl在大多数测试Circuit上显著超越了现有的优化器。另外，Quarl还能学习执行旋转合并，这是一项复杂的、非本地Circuit优化，在现有的优化器中实现为单独的一个过程。
</details></li>
</ul>
<hr>
<h2 id="A-DPLL-T-Framework-for-Verifying-Deep-Neural-Networks"><a href="#A-DPLL-T-Framework-for-Verifying-Deep-Neural-Networks" class="headerlink" title="A DPLL(T) Framework for Verifying Deep Neural Networks"></a>A DPLL(T) Framework for Verifying Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10266">http://arxiv.org/abs/2307.10266</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dynaroars/neuralsat-solver">https://github.com/dynaroars/neuralsat-solver</a></li>
<li>paper_authors: Hai Duong, Linhan Li, ThanhVu Nguyen, Matthew Dwyer</li>
<li>for: 这个论文是为了提出一种新的深度神经网络验证方法，帮助检测和修复神经网络中的漏洞和攻击。</li>
<li>methods: 该方法基于DPLL(T)算法，包括冲突学习、抽象和理论解决，可以看作是一种基于SMT的神经网络验证框架。</li>
<li>results: 预liminary结果表明，NeuralSAT прототип与当前领导的状态之间具有竞争力。希望通过优化和工程化，NeuralSAT能够带来现代SAT&#x2F;SMT解决方案的力量和成功，并推动神经网络验证领域的发展。<details>
<summary>Abstract</summary>
Deep Neural Networks (DNNs) have emerged as an effective approach to tackling real-world problems. However, like human-written software, automatically-generated DNNs can have bugs and be attacked. This thus attracts many recent interests in developing effective and scalable DNN verification techniques and tools. In this work, we introduce a NeuralSAT, a new constraint solving approach to DNN verification. The design of NeuralSAT follows the DPLL(T) algorithm used modern SMT solving, which includes (conflict) clause learning, abstraction, and theory solving, and thus NeuralSAT can be considered as an SMT framework for DNNs. Preliminary results show that the NeuralSAT prototype is competitive to the state-of-the-art. We hope, with proper optimization and engineering, NeuralSAT will carry the power and success of modern SAT/SMT solvers to DNN verification. NeuralSAT is avaliable from: https://github.com/dynaroars/neuralsat-solver
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）已成为解决现实世界问题的有效方法。然而，如人工写的软件一样，自动生成的 DNN 也可能具有错误和攻击性。这引起了许多最近的关注，旨在开发有效和扩展性的 DNN 验证技术和工具。在这项工作中，我们介绍了一种名为 NeuralSAT 的新的约束解决方法。NeuralSAT 的设计基于现代 SMT 解决方法中的 DPLL(T) 算法，包括（冲突）条件学习、抽象和理论解决，因此 NeuralSAT 可以视为 DNN 的 SMT 框架。初步结果表明，NeuralSAT 原型在竞争力方面与现状保持紧密。我们希望，通过适当的优化和工程，NeuralSAT 能够将现代 SAT/SMT 解决方法的力量和成功带到 DNN 验证中。NeuralSAT 可以从以下地址获取：https://github.com/dynaroars/neuralsat-solver
</details></li>
</ul>
<hr>
<h2 id="A-mixed-policy-to-improve-performance-of-language-models-on-math-problems"><a href="#A-mixed-policy-to-improve-performance-of-language-models-on-math-problems" class="headerlink" title="A mixed policy to improve performance of language models on math problems"></a>A mixed policy to improve performance of language models on math problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08767">http://arxiv.org/abs/2307.08767</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vividitytech/math_lm_rl">https://github.com/vividitytech/math_lm_rl</a></li>
<li>paper_authors: Gang Chen</li>
<li>for: 解决 math 问题时，语言模型通常采用采样策略来预测下一个词的 conditional probabilities。但是，在 math 理解步骤中，这种方法可能会导致错误答案。因此，我们提出了一种混合策略探索方法，使用 reinforcement learning 解决 math 问题。</li>
<li>methods: 我们提出了一种两级 токен探索策略：抽象级别的策略会根据概率采样下一个 токен是操作符或操作数，而第二级是具有最高分的循环采集策略。</li>
<li>results: 我们在 GSM8K 数据集上测试了我们的方法，使用 GPT-2 模型，并证明了更高于 $2%$ 的性能提升。我们的实现可以在 <a target="_blank" rel="noopener" href="https://github.com/vividitytech/math_lm_rl">https://github.com/vividitytech/math_lm_rl</a> 上找到。<details>
<summary>Abstract</summary>
When to solve math problems, most language models take a sampling strategy to predict next word according conditional probabilities. In the math reasoning step, it may generate wrong answer. Considering math problems are deterministic, we propose a mixed policy exploration approach to solve math problems with reinforcement learning. In peculiar, we propose a two level token exploration policy: the abstract level explores next token with probability and the second level is deterministic. Specifically, the abstract level policy will decide whether the token is operator or operand with probability sampling, while the second level is deterministic to select next token with the highest score in a greedy way. We test our method on GSM8K dataset with GPT-2 model, and demonstrate more than $2\%$ performance gain. Our implementation is available at https://github.com/vividitytech/math_lm_rl.
</details>
<details>
<summary>摘要</summary>
当解决数学问题时，大多数语言模型采取采样策略来预测下一个词的条件概率。在数学逻辑步骤中，它可能生成错误答案。考虑到数学问题是deterministic的，我们提议一种混合策略探索方法来解决数学问题使用再征学习。具体来说，我们提议一个两级符号探索策略：第一级是概率采样，第二级是决定性选择下一个符号的最高分。 Specifically, the first-level policy will decide whether the token is an operator or an operand with probability sampling, while the second level is deterministic to select the next token with the highest score in a greedy way. We test our method on GSM8K dataset with GPT-2 model, and demonstrate more than 2% performance gain. Our implementation is available at <https://github.com/vividitytech/math_lm_rl>.
</details></li>
</ul>
<hr>
<h2 id="Quality-Assessment-of-Photoplethysmography-Signals-For-Cardiovascular-Biomarkers-Monitoring-Using-Wearable-Devices"><a href="#Quality-Assessment-of-Photoplethysmography-Signals-For-Cardiovascular-Biomarkers-Monitoring-Using-Wearable-Devices" class="headerlink" title="Quality Assessment of Photoplethysmography Signals For Cardiovascular Biomarkers Monitoring Using Wearable Devices"></a>Quality Assessment of Photoplethysmography Signals For Cardiovascular Biomarkers Monitoring Using Wearable Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08766">http://arxiv.org/abs/2307.08766</a></li>
<li>repo_url: None</li>
<li>paper_authors: Felipe M. Dias, Marcelo A. F. Toledo, Diego A. C. Cardenas, Douglas A. Almeida, Filipe A. C. Oliveira, Estela Ribeiro, Jose E. Krieger, Marco A. Gutierrez</li>
<li>for: 这个研究用于评估光学 Plethysmography (PPG) 信号质量，以提高血液征分和征识Cardiovascular 健康。</li>
<li>methods: 这个研究使用了机器学习算法（包括XGBoost、CatBoost和Random Forest）来训练27个统计特征从PPG信号中提取出高质量和低质量的PPG信号。</li>
<li>results: 研究发现，使用这些机器学习模型可以达到Se、PPV和F1-score的94.4、95.6和95.0，94.7、95.9和95.3，93.7、91.3和92.5，分别。这些结果与文献中的状态作准比较，表明机器学习模型可以用于开发远程、非侵入式和连续测量设备。<details>
<summary>Abstract</summary>
Photoplethysmography (PPG) is a non-invasive technology that measures changes in blood volume in the microvascular bed of tissue. It is commonly used in medical devices such as pulse oximeters and wrist worn heart rate monitors to monitor cardiovascular hemodynamics. PPG allows for the assessment of parameters (e.g., heart rate, pulse waveform, and peripheral perfusion) that can indicate conditions such as vasoconstriction or vasodilation, and provides information about microvascular blood flow, making it a valuable tool for monitoring cardiovascular health. However, PPG is subject to a number of sources of variations that can impact its accuracy and reliability, especially when using a wearable device for continuous monitoring, such as motion artifacts, skin pigmentation, and vasomotion. In this study, we extracted 27 statistical features from the PPG signal for training machine-learning models based on gradient boosting (XGBoost and CatBoost) and Random Forest (RF) algorithms to assess quality of PPG signals that were labeled as good or poor quality. We used the PPG time series from a publicly available dataset and evaluated the algorithm s performance using Sensitivity (Se), Positive Predicted Value (PPV), and F1-score (F1) metrics. Our model achieved Se, PPV, and F1-score of 94.4, 95.6, and 95.0 for XGBoost, 94.7, 95.9, and 95.3 for CatBoost, and 93.7, 91.3 and 92.5 for RF, respectively. Our findings are comparable to state-of-the-art reported in the literature but using a much simpler model, indicating that ML models are promising for developing remote, non-invasive, and continuous measurement devices.
</details>
<details>
<summary>摘要</summary>
photoplethysmography (PPG) 是一种不侵入性技术，用于测量血液Volume在微血管细胞床中的变化。它通常用于医疗器械 such as pulse oximeters和背部搭载的心率测量仪器来监测Cardiovascular hemodynamics。 PPG 可以评估参数（例如心率、脉冲形态和血液径流），这些参数可能会指示condition such as vasoconstriction or vasodilation，并提供关于微血管血液流动的信息，使其成为监测Cardiovascular health 的有用工具。然而，PPG 受到多种源的变化的影响，特别是在使用 wearable device 进行连续监测时，如运动 artifacts、皮肤颜色和血管运动。在这项研究中，我们从 PPG 信号中提取了27个统计特征，用于训练机器学习模型，包括梯度提升（XGBoost和CatBoost）和随机森（RF）算法。我们使用公共可用的 PPG 时间序列数据集，并使用 Se、Positive Predicted Value（PPV）和 F1-score（F1） metric 评估算法的性能。我们的模型实现了 Se、PPV 和 F1-score 的94.4、95.6和95.0，XGBoost 的94.7、95.9和95.3，CatBoost 的93.7、91.3和92.5，RF 的93.7、91.3和92.5。我们的发现与文献中的状态Of-the-art相似，但使用更简单的模型， indicating that ML models are promising for developing remote, non-invasive, and continuous measurement devices。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Application-of-Conditional-Normalizing-Flows-Stellar-Age-Inference-with-Gyrochronology"><a href="#A-Novel-Application-of-Conditional-Normalizing-Flows-Stellar-Age-Inference-with-Gyrochronology" class="headerlink" title="A Novel Application of Conditional Normalizing Flows: Stellar Age Inference with Gyrochronology"></a>A Novel Application of Conditional Normalizing Flows: Stellar Age Inference with Gyrochronology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08753">http://arxiv.org/abs/2307.08753</a></li>
<li>repo_url: None</li>
<li>paper_authors: Phil Van-Lane, Joshua S. Speagle, Stephanie Douglas</li>
<li>for: 用于推算低质量主序星的年龄</li>
<li>methods: 使用机器学习技术进行Conditional Normalizing Flows分析光谱数据</li>
<li>results: 实现了与文献值相符的年龄估算，并且提供了一种可靠的数据驱动的星系年龄推算方法<details>
<summary>Abstract</summary>
Stellar ages are critical building blocks of evolutionary models, but challenging to measure for low mass main sequence stars. An unexplored solution in this regime is the application of probabilistic machine learning methods to gyrochronology, a stellar dating technique that is uniquely well suited for these stars. While accurate analytical gyrochronological models have proven challenging to develop, here we apply conditional normalizing flows to photometric data from open star clusters, and demonstrate that a data-driven approach can constrain gyrochronological ages with a precision comparable to other standard techniques. We evaluate the flow results in the context of a Bayesian framework, and show that our inferred ages recover literature values well. This work demonstrates the potential of a probabilistic data-driven solution to widen the applicability of gyrochronological stellar dating.
</details>
<details>
<summary>摘要</summary>
星系年龄是进化模型的关键构建块，但低质量主序星测量具有挑战。未经探索的解决方案在这个领域是通过潜在机器学习方法进行gyrochronology，这是特别适用于这些星体的星年龄测量技术。虽然精确的分析gyrochronological模型具有挑战，但我们在光度测量数据上应用条件正常流，并示出了一种数据驱动的方法可以与其他标准技术相比的精度来限制gyrochronological年龄。我们在bayesian框架下评估流果，并发现我们的推断年龄与文献值很好地匹配。这种工作示出了潜在的数据驱动probabilistic解决方案可以扩展gyrochronological星年龄测量的应用范围。
</details></li>
</ul>
<hr>
<h2 id="Flow-Matching-in-Latent-Space"><a href="#Flow-Matching-in-Latent-Space" class="headerlink" title="Flow Matching in Latent Space"></a>Flow Matching in Latent Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08698">http://arxiv.org/abs/2307.08698</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vinairesearch/lfm">https://github.com/vinairesearch/lfm</a></li>
<li>paper_authors: Quan Dao, Hao Phung, Binh Nguyen, Anh Tran</li>
<li>for: train generative models with improved computational efficiency and scalability for high-resolution image synthesis</li>
<li>methods: apply flow matching in the latent spaces of pretrained autoencoders, integrate various conditions for conditional generation tasks</li>
<li>results: effective in both quantitative and qualitative results on various datasets, provide theoretical control of the Wasserstein-2 distance between the reconstructed latent flow distribution and true data distribution<details>
<summary>Abstract</summary>
Flow matching is a recent framework to train generative models that exhibits impressive empirical performance while being relatively easier to train compared with diffusion-based models. Despite its advantageous properties, prior methods still face the challenges of expensive computing and a large number of function evaluations of off-the-shelf solvers in the pixel space. Furthermore, although latent-based generative methods have shown great success in recent years, this particular model type remains underexplored in this area. In this work, we propose to apply flow matching in the latent spaces of pretrained autoencoders, which offers improved computational efficiency and scalability for high-resolution image synthesis. This enables flow-matching training on constrained computational resources while maintaining their quality and flexibility. Additionally, our work stands as a pioneering contribution in the integration of various conditions into flow matching for conditional generation tasks, including label-conditioned image generation, image inpainting, and semantic-to-image generation. Through extensive experiments, our approach demonstrates its effectiveness in both quantitative and qualitative results on various datasets, such as CelebA-HQ, FFHQ, LSUN Church & Bedroom, and ImageNet. We also provide a theoretical control of the Wasserstein-2 distance between the reconstructed latent flow distribution and true data distribution, showing it is upper-bounded by the latent flow matching objective. Our code will be available at https://github.com/VinAIResearch/LFM.git.
</details>
<details>
<summary>摘要</summary>
“流匹配”是一种最近的框架，用于训练生成模型，具有印象性的实验性能，而且训练更容易。然而，之前的方法仍面临computational expensive和大量的函数评估问题在像素空间中。另外，Latent-based生成方法在最近几年内表现出色，但这种模型类型在这个领域仍然未得到充分的探索。在这种工作中，我们提议在预训练 autoencoder 的 latent space 中应用流匹配，这可以提高计算效率和可扩展性，用于高分辨率图像生成。这种方法可以在受限的计算资源上进行流匹配训练，同时保持图像质量和灵活性。此外，我们的工作是在 flow matching 中 интеGRATION 多种条件的先驱性贡献，包括标签条件图像生成、图像填充和semantic-to-image生成。通过广泛的实验，我们的方法在多个数据集上表现出色，如 CelebA-HQ、FFHQ、LSUN Church & Bedroom 和 ImageNet。我们还提供了 Wasserstein-2 距离真实数据分布和重建 latent flow 分布的理论控制，证明它是上界。我们的代码将可以在 GitHub 上获得：https://github.com/VinAIResearch/LFM.git。
</details></li>
</ul>
<hr>
<h2 id="A-Multiobjective-Reinforcement-Learning-Framework-for-Microgrid-Energy-Management"><a href="#A-Multiobjective-Reinforcement-Learning-Framework-for-Microgrid-Energy-Management" class="headerlink" title="A Multiobjective Reinforcement Learning Framework for Microgrid Energy Management"></a>A Multiobjective Reinforcement Learning Framework for Microgrid Energy Management</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08692">http://arxiv.org/abs/2307.08692</a></li>
<li>repo_url: None</li>
<li>paper_authors: M. Vivienne Liu, Patrick M. Reed, David Gold, Garret Quist, C. Lindsay Anderson</li>
<li>for: 提供一种解决多目标冲突的微grid操作方法</li>
<li>methods: 利用外生信息和数据驱动学习来探索高维目标空间，找到多目标之间的补做</li>
<li>results: 比Status quo操作更高效，提供多样化、适应性和可解释的操作方法<details>
<summary>Abstract</summary>
The emergence of microgrids (MGs) has provided a promising solution for decarbonizing and decentralizing the power grid, mitigating the challenges posed by climate change. However, MG operations often involve considering multiple objectives that represent the interests of different stakeholders, leading to potentially complex conflicts. To tackle this issue, we propose a novel multi-objective reinforcement learning framework that explores the high-dimensional objective space and uncovers the tradeoffs between conflicting objectives. This framework leverages exogenous information and capitalizes on the data-driven nature of reinforcement learning, enabling the training of a parametric policy without the need for long-term forecasts or knowledge of the underlying uncertainty distribution. The trained policies exhibit diverse, adaptive, and coordinative behaviors with the added benefit of providing interpretable insights on the dynamics of their information use. We employ this framework on the Cornell University MG (CU-MG), which is a combined heat and power MG, to evaluate its effectiveness. The results demonstrate performance improvements in all objectives considered compared to the status quo operations and offer more flexibility in navigating complex operational tradeoffs.
</details>
<details>
<summary>摘要</summary>
随着微型电网（MG）的出现，为了解决气候变化所带来的挑战，提供了一个有前途的解决方案，即减少和分散电力网络。然而，MG的运营通常需要考虑多个目标，这些目标代表不同的利益相互之间的矛盾，这可能会导致复杂的冲突。为了解决这个问题，我们提出了一种新的多目标学习框架，该框架可以探索高维目标空间中的交叉关系，并揭示不同目标之间的负担变化。这种框架利用外生信息，并利用学习的数据驱动特性，可以在不需要长期预测或者知道下游不确定分布的情况下，训练一个参数化策略。训练出来的策略具有多样化、适应性和协调性，同时还提供了可解释的动态信息使用动态。我们在康奈尔大学微型电网（CU-MG）上使用这种框架进行评估，结果表明，相比Status quo操作，我们的方法可以提高所有考虑的目标的性能，并提供更多的操作复杂关系的灵活性。
</details></li>
</ul>
<hr>
<h2 id="FlashAttention-2-Faster-Attention-with-Better-Parallelism-and-Work-Partitioning"><a href="#FlashAttention-2-Faster-Attention-with-Better-Parallelism-and-Work-Partitioning" class="headerlink" title="FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"></a>FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08691">http://arxiv.org/abs/2307.08691</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dao-ailab/flash-attention">https://github.com/dao-ailab/flash-attention</a></li>
<li>paper_authors: Tri Dao</li>
<li>for: 提高Transformers的Sequence length scaling，以提高语言模型和高分辨率图像理解的性能，以及开启代码、音频和视频生成等新应用。</li>
<li>methods: 利用GPU内存层次结构，实现 linear 而不是 quadratic 的内存占用和运行时间减速，无需 aproximation。</li>
<li>results: 对比于优化baselines，FlashAttention-2可以达到2-4倍的运行时间减速，并且在A100 GPU上达到50-73%的理论最大FLOPs&#x2F;s，接近GEMM操作的效率。<details>
<summary>Abstract</summary>
Scaling Transformers to longer sequence lengths has been a major problem in the last several years, promising to improve performance in language modeling and high-resolution image understanding, as well as to unlock new applications in code, audio, and video generation. The attention layer is the main bottleneck in scaling to longer sequences, as its runtime and memory increase quadratically in the sequence length. FlashAttention exploits the asymmetric GPU memory hierarchy to bring significant memory saving (linear instead of quadratic) and runtime speedup (2-4$\times$ compared to optimized baselines), with no approximation. However, FlashAttention is still not nearly as fast as optimized matrix-multiply (GEMM) operations, reaching only 25-40\% of the theoretical maximum FLOPs/s. We observe that the inefficiency is due to suboptimal work partitioning between different thread blocks and warps on the GPU, causing either low-occupancy or unnecessary shared memory reads/writes. We propose FlashAttention-2, with better work partitioning to address these issues. In particular, we (1) tweak the algorithm to reduce the number of non-matmul FLOPs (2) parallelize the attention computation, even for a single head, across different thread blocks to increase occupancy, and (3) within each thread block, distribute the work between warps to reduce communication through shared memory. These yield around 2$\times$ speedup compared to FlashAttention, reaching 50-73\% of the theoretical maximum FLOPs/s on A100 and getting close to the efficiency of GEMM operations. We empirically validate that when used end-to-end to train GPT-style models, FlashAttention-2 reaches training speed of up to 225 TFLOPs/s per A100 GPU (72\% model FLOPs utilization).
</details>
<details>
<summary>摘要</summary>
缩放变换器在更长的序列长度上进行缩放是过去几年内的一个主要问题，承诺改进语言模型和高分辨率图像理解的性能，以及开启新的代码、音频和视频生成应用。注意层是缩放到更长序列的主要瓶颈，因为它的运行时间和内存增长为序列长度的平方。 FlashAttention 利用了 GPU 内存层次结构的非对称性，实现了重要的内存减少（线性而不是平方）和运行时间加速（2-4倍于优化基线），无需折衣。然而，FlashAttention 仍然不够快，只达到了25-40%的理论最大 FLOPs/s。我们发现，这是由不佳的工作分配导致的，包括在 GPU 中的不同线程块和批处理中的低占用或 Shared 内存中的不必要读写。我们提议 FlashAttention-2，它通过改进工作分配来解决这些问题。具体来说，我们（1）修改算法，减少非 matrix-multiply FLOPs;（2）在单个头上并行执行注意计算，以增加占用率;（3）在每个线程块中，分配工作 между批处理，以减少通信过 Shared 内存。这些提高了约 2 倍的速度，达到 50-73% 的理论最大 FLOPs/s 在 A100 上，与 GEMM 操作的效率相似。我们经验 validate 了，当用于 Train GPT-style 模型时，FlashAttention-2 的训练速度可达 225 TFLOPs/s per A100 GPU (72% 模型 FLOPs 利用率)。
</details></li>
</ul>
<hr>
<h2 id="COLLIE-Systematic-Construction-of-Constrained-Text-Generation-Tasks"><a href="#COLLIE-Systematic-Construction-of-Constrained-Text-Generation-Tasks" class="headerlink" title="COLLIE: Systematic Construction of Constrained Text Generation Tasks"></a>COLLIE: Systematic Construction of Constrained Text Generation Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08689">http://arxiv.org/abs/2307.08689</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/princeton-nlp/Collie">https://github.com/princeton-nlp/Collie</a></li>
<li>paper_authors: Shunyu Yao, Howard Chen, Austin W. Hanjie, Runzhe Yang, Karthik Narasimhan</li>
<li>for: 本研究旨在提供一种 grammar-based 框架，用于 specifying 复杂的、compositional 约束，以便在自然语言处理中进行 Text generation under constraints。</li>
<li>methods: 本研究使用了 grammar-based 框架 COLLIE，可以Specify 多种层次的约束（word、sentence、paragraph、passage）和模型挑战（语言理解、逻辑推理、计数、semantic planning）。此外，还开发了一些自动提取任务实例的工具，以便使用 COLLIE 进行数据生成。</li>
<li>results: 通过使用 COLLIE，研究人员编译了 COLLIE-v1 数据集，包含 2080 个任务实例，其中每个任务实例包含 13 种约束结构。通过对 five 种 instruction-tuned 语言模型进行系统性的实验和分析，发现这些模型在处理 COLLIE 数据集时存在缺陷。 COLLIE 框架设计为轻量级和可扩展，希望社区可以通过开发更复杂的约束和评价方法来进一步提高自然语言处理技术。<details>
<summary>Abstract</summary>
Text generation under constraints have seen increasing interests in natural language processing, especially with the rapidly improving capabilities of large language models. However, existing benchmarks for constrained generation usually focus on fixed constraint types (e.g.,generate a sentence containing certain words) that have proved to be easy for state-of-the-art models like GPT-4. We present COLLIE, a grammar-based framework that allows the specification of rich, compositional constraints with diverse generation levels (word, sentence, paragraph, passage) and modeling challenges (e.g.,language understanding, logical reasoning, counting, semantic planning). We also develop tools for automatic extraction of task instances given a constraint structure and a raw text corpus. Using COLLIE, we compile the COLLIE-v1 dataset with 2080 instances comprising 13 constraint structures. We perform systematic experiments across five state-of-the-art instruction-tuned language models and analyze their performances to reveal shortcomings. COLLIE is designed to be extensible and lightweight, and we hope the community finds it useful to develop more complex constraints and evaluations in the future.
</details>
<details>
<summary>摘要</summary>
文本生成 unter constraint 已经受到了自然语言处理领域的越来越多的关注，尤其是大语言模型的能力在不断提高。然而，现有的受制定生成标准通常围绕固定的约束类型（例如，生成包含某些词的句子），这些约束已经证明容易 для当前的模型 like GPT-4。我们提出了 COLLIE，一个基于语法的框架，允许指定Rich和多层次的 compositional 约束（单词、句子、段落、段落），以及模型挑战（例如，语言理解、逻辑推理、计数、semantic planning）。我们还开发了自动提取 task instance 的工具，基于约束结构和原始文本库。使用 COLLIE，我们编译了 COLLIE-v1 数据集，包含 2080 个实例，其中 13 种约束结构。我们在五种状态atracking 语言模型上进行了系统性的实验，并分析其性能，以揭示缺陷。COLLIE 是可扩展和轻量级的，我们希望社区能够在未来开发更复杂的约束和评价。
</details></li>
</ul>
<hr>
<h2 id="An-R-package-for-parametric-estimation-of-causal-effects"><a href="#An-R-package-for-parametric-estimation-of-causal-effects" class="headerlink" title="An R package for parametric estimation of causal effects"></a>An R package for parametric estimation of causal effects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08686">http://arxiv.org/abs/2307.08686</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joshua Wolff Anderson, Cyril Rakovski</li>
<li>for: 本文旨在介绍R包CausalModels，用于估计 causal effect。</li>
<li>methods: 本文使用了一些常见的统计方法，包括标准化、IP重み、G估计、结果回归、工具变量和投影匹配等。</li>
<li>results: 本文提供了一个简单和可访问的框架，可以在R中对不同的统计方法进行集成，用于估计 causal effect。Note: The above text is in Simplified Chinese.<details>
<summary>Abstract</summary>
This article explains the usage of R package CausalModels, which is publicly available on the Comprehensive R Archive Network. While packages are available for sufficiently estimating causal effects, there lacks a package that provides a collection of structural models using the conventional statistical approach developed by Hernan and Robins (2020). CausalModels addresses this deficiency of software in R concerning causal inference by offering tools for methods that account for biases in observational data without requiring extensive statistical knowledge. These methods should not be ignored and may be more appropriate or efficient in solving particular problems. While implementations of these statistical models are distributed among a number of causal packages, CausalModels introduces a simple and accessible framework for a consistent modeling pipeline among a variety of statistical methods for estimating causal effects in a single R package. It consists of common methods including standardization, IP weighting, G-estimation, outcome regression, instrumental variables and propensity matching.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Rubik’s-Cube-inspired-approach-to-Clifford-synthesis"><a href="#A-Rubik’s-Cube-inspired-approach-to-Clifford-synthesis" class="headerlink" title="A Rubik’s Cube inspired approach to Clifford synthesis"></a>A Rubik’s Cube inspired approach to Clifford synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08684">http://arxiv.org/abs/2307.08684</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gshartnett/rubiks-clifford-synthesis">https://github.com/gshartnett/rubiks-clifford-synthesis</a></li>
<li>paper_authors: Ning Bao, Gavin S. Hartnett</li>
<li>for: 解决Clifford元素的分解问题，即Clifford合成问题。</li>
<li>methods: 采用机器学习方法，基于距离标准的近似来实现Clifford合成。</li>
<li>results: 比现有算法更具有灵活性，可以适应特定设备的gate集、设备拓扑和gate精度。<details>
<summary>Abstract</summary>
The problem of decomposing an arbitrary Clifford element into a sequence of Clifford gates is known as Clifford synthesis. Drawing inspiration from similarities between this and the famous Rubik's Cube problem, we develop a machine learning approach for Clifford synthesis based on learning an approximation to the distance to the identity. This approach is probabilistic and computationally intensive. However, when a decomposition is successfully found, it often involves fewer gates than existing synthesis algorithms. Additionally, our approach is much more flexible than existing algorithms in that arbitrary gate sets, device topologies, and gate fidelities may incorporated, thus allowing for the approach to be tailored to a specific device.
</details>
<details>
<summary>摘要</summary>
“把任意的克利福德元素分解成克利福德门的序列是称为克利福德合成的问题。 Drawing inspiration from类似于这和著名的聂隐秘 куби cura问题，我们开发了基于学习距离Identidade的机器学习方法 для克利福德合成。 This approach是 probabilistic和 computationally intensive。 however，当一个分解成功时，它通常具有 fewer gates than existing synthesis algorithms。 In addition， our approach is much more flexible than existing algorithms in that arbitrary gate sets, device topologies, and gate fidelities may be incorporated， thus allowing for the approach to be tailored to a specific device.”Note that Simplified Chinese is the standard writing system used in mainland China, and it may be different from Traditional Chinese, which is used in Taiwan and other parts of the world.
</details></li>
</ul>
<hr>
<h2 id="Do-Models-Explain-Themselves-Counterfactual-Simulatability-of-Natural-Language-Explanations"><a href="#Do-Models-Explain-Themselves-Counterfactual-Simulatability-of-Natural-Language-Explanations" class="headerlink" title="Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations"></a>Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08678">http://arxiv.org/abs/2307.08678</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanda Chen, Ruiqi Zhong, Narutatsu Ri, Chen Zhao, He He, Jacob Steinhardt, Zhou Yu, Kathleen McKeown</li>
<li>for: 这个论文旨在研究大型自然语言模型（LLM）是否可以解释自己的决策过程。</li>
<li>methods: 作者提出了评估对natural language explanation的counterfactual simulatability，以测试LLM是否可以帮助人类构建模型处理不同输入的MENTAL MODEL。</li>
<li>results: 研究发现，LLM的解释具有低精度和不符合可能性，因此直接优化人类批准（例如RLHF）可能并不是 suficient solution。<details>
<summary>Abstract</summary>
Large language models (LLMs) are trained to imitate humans to explain human decisions. However, do LLMs explain themselves? Can they help humans build mental models of how LLMs process different inputs? To answer these questions, we propose to evaluate $\textbf{counterfactual simulatability}$ of natural language explanations: whether an explanation can enable humans to precisely infer the model's outputs on diverse counterfactuals of the explained input. For example, if a model answers "yes" to the input question "Can eagles fly?" with the explanation "all birds can fly", then humans would infer from the explanation that it would also answer "yes" to the counterfactual input "Can penguins fly?". If the explanation is precise, then the model's answer should match humans' expectations.   We implemented two metrics based on counterfactual simulatability: precision and generality. We generated diverse counterfactuals automatically using LLMs. We then used these metrics to evaluate state-of-the-art LLMs (e.g., GPT-4) on two tasks: multi-hop factual reasoning and reward modeling. We found that LLM's explanations have low precision and that precision does not correlate with plausibility. Therefore, naively optimizing human approvals (e.g., RLHF) may not be a sufficient solution.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）在训练时尝试模仿人类的决策，但是 LLM 是否能够解释自己的处理逻辑？可以使用 counterfactual simulatability 来评估 LLM 的解释能力。我们定义 counterfactual simulatability 为：一个解释是否能够帮助人类建立模型处理输入的精准模型。例如，如果一个模型对 input 问题 "Can eagles fly?" 的答案是 "yes"，并且提供解释 "all birds can fly"，那么人类就可以从解释中推断出模型对 counterfactual input "Can penguins fly?" 的答案是什么。如果解释准确，那么模型的答案应该与人类的预期相符。为了评估 LLM 的 counterfactual simulatability，我们提出了两种指标：精度和通用性。我们使用 LLM 自动生成了多个 counterfactual，然后使用这些指标来评估当前 state-of-the-art LLM （例如 GPT-4）在 multi-hop factual reasoning 和 reward modeling 两个任务上的表现。我们发现 LLM 的解释准确率很低，而且准确率与可能性无关。因此，直接优化人类的批准（例如 RLHF）可能并不是一个充分的解决方案。
</details></li>
</ul>
<hr>
<h2 id="TableGPT-Towards-Unifying-Tables-Nature-Language-and-Commands-into-One-GPT"><a href="#TableGPT-Towards-Unifying-Tables-Nature-Language-and-Commands-into-One-GPT" class="headerlink" title="TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT"></a>TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08674">http://arxiv.org/abs/2307.08674</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liangyu Zha, Junlin Zhou, Liyao Li, Rui Wang, Qingyi Huang, Saisai Yang, Jing Yuan, Changbao Su, Xiang Li, Aofeng Su, Tao Zhang, Chen Zhou, Kaizhe Shou, Miao Wang, Wufang Zhu, Guoshan Lu, Chao Ye, Yali Ye, Wentao Ye, Yiming Zhang, Xinglong Deng, Jie Xu, Haobo Wang, Gang Chen, Junbo Zhao</li>
<li>for: 论文旨在提供一个可以通过自然语言输入操作表格的框架，使用大语言模型（LLMs）来理解和处理表格。</li>
<li>methods: 该框架基于全新的全球表格表示方式，通过同时训练 LLMs 在表格和文本模式之间，以便它们能够深入理解表格数据并在指令链中执行复杂的操作。</li>
<li>results: TableGPT 可以提供简单易用的表格操作方式，包括问答、数据操作、数据可视化、分析报告生成和自动预测等，从而为用户提供更多的便利和访问ibilty。<details>
<summary>Abstract</summary>
Tables are prevalent in real-world databases, requiring significant time and effort for humans to analyze and manipulate. The advancements in large language models (LLMs) have made it possible to interact with tables using natural language input, bringing this capability closer to reality. In this paper, we present TableGPT, a unified fine-tuned framework that enables LLMs to understand and operate on tables using external functional commands. It introduces the capability to seamlessly interact with tables, enabling a wide range of functionalities such as question answering, data manipulation (e.g., insert, delete, query, and modify operations), data visualization, analysis report generation, and automated prediction. TableGPT aims to provide convenience and accessibility to users by empowering them to effortlessly leverage tabular data. At the core of TableGPT lies the novel concept of global tabular representations, which empowers LLMs to gain a comprehensive understanding of the entire table beyond meta-information. By jointly training LLMs on both table and text modalities, TableGPT achieves a deep understanding of tabular data and the ability to perform complex operations on tables through chain-of-command instructions. Importantly, TableGPT offers the advantage of being a self-contained system rather than relying on external API interfaces. Moreover, it supports efficient data process flow, query rejection (when appropriate) and private deployment, enabling faster domain data fine-tuning and ensuring data privacy, which enhances the framework's adaptability to specific use cases.
</details>
<details>
<summary>摘要</summary>
Tables are prevalent in real-world databases, requiring significant time and effort for humans to analyze and manipulate. The advancements in large language models (LLMs) have made it possible to interact with tables using natural language input, bringing this capability closer to reality. In this paper, we present TableGPT, a unified fine-tuned framework that enables LLMs to understand and operate on tables using external functional commands. It introduces the capability to seamlessly interact with tables, enabling a wide range of functionalities such as question answering, data manipulation (e.g., insert, delete, query, and modify operations), data visualization, analysis report generation, and automated prediction. TableGPT aims to provide convenience and accessibility to users by empowering them to effortlessly leverage tabular data. At the core of TableGPT lies the novel concept of global tabular representations, which empowers LLMs to gain a comprehensive understanding of the entire table beyond meta-information. By jointly training LLMs on both table and text modalities, TableGPT achieves a deep understanding of tabular data and the ability to perform complex operations on tables through chain-of-command instructions. Importantly, TableGPT offers the advantage of being a self-contained system rather than relying on external API interfaces. Moreover, it supports efficient data process flow, query rejection (when appropriate) and private deployment, enabling faster domain data fine-tuning and ensuring data privacy, which enhances the framework's adaptability to specific use cases.
</details></li>
</ul>
<hr>
<h2 id="CohortFinder-an-open-source-tool-for-data-driven-partitioning-of-biomedical-image-cohorts-to-yield-robust-machine-learning-models"><a href="#CohortFinder-an-open-source-tool-for-data-driven-partitioning-of-biomedical-image-cohorts-to-yield-robust-machine-learning-models" class="headerlink" title="CohortFinder: an open-source tool for data-driven partitioning of biomedical image cohorts to yield robust machine learning models"></a>CohortFinder: an open-source tool for data-driven partitioning of biomedical image cohorts to yield robust machine learning models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08673">http://arxiv.org/abs/2307.08673</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fan Fan, Georgia Martinez, Thomas Desilvio, John Shin, Yijiang Chen, Bangchen Wang, Takaya Ozeki, Maxime W. Lafarge, Viktor H. Koelzer, Laura Barisoni, Anant Madabhushi, Satish E. Viswanath, Andrew Janowczyk</li>
<li>for: 降低机器学习模型的泛化性下降，减少批处理影响</li>
<li>methods: 使用数据驱动的 cohort 分割方法来缓解批处理的影响</li>
<li>results: 在医疗影像处理任务中，使用 CohortFinder 可以提高机器学习模型的性能<details>
<summary>Abstract</summary>
Batch effects (BEs) refer to systematic technical differences in data collection unrelated to biological variations whose noise is shown to negatively impact machine learning (ML) model generalizability. Here we release CohortFinder, an open-source tool aimed at mitigating BEs via data-driven cohort partitioning. We demonstrate CohortFinder improves ML model performance in downstream medical image processing tasks. CohortFinder is freely available for download at cohortfinder.com.
</details>
<details>
<summary>摘要</summary>
批处效应（BE）指的是数据收集过程中的系统性技术差异，不 relacionados con variationes biológicas，这些噪声可能会负面影响机器学习（ML）模型的泛化性。我们现在发布了一个开源工具，即CohortFinder，用于缓解BE的影响。我们在医学图像处理任务中展示了CohortFinder可以提高ML模型的性能。CohortFinder可以免费下载于cohortfinder.com。
</details></li>
</ul>
<hr>
<h2 id="Neural-Image-Compression-Generalization-Robustness-and-Spectral-Biases"><a href="#Neural-Image-Compression-Generalization-Robustness-and-Spectral-Biases" class="headerlink" title="Neural Image Compression: Generalization, Robustness, and Spectral Biases"></a>Neural Image Compression: Generalization, Robustness, and Spectral Biases</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08657">http://arxiv.org/abs/2307.08657</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kelsey Lieberman, James Diffenderfer, Charles Godfrey, Bhavya Kailkhura</li>
<li>for: 评估 neural image compression (NIC) 模型在实际应用中的抗迁移性和一致性性能。</li>
<li>methods: 提供了一个完整的benchmark suite来评估图像压缩方法的out-of-distribution (OOD)性能，包括CLIC-C和Kodak-C两个 benchmark，并提出了基于 спектrum的检查工具来深入了解图像压缩方法引入的错误和OOD性能。</li>
<li>results: 对一种经典编码器和多种 NIC 变体进行了详细的性能比较，发现了一些挑战当前我们对 NIC 的强点和局限性的发现，并通过理论分析深入了解 NIC 的OOD性能和数据的spectral properties的关系。<details>
<summary>Abstract</summary>
Recent neural image compression (NIC) advances have produced models which are starting to outperform traditional codecs. While this has led to growing excitement about using NIC in real-world applications, the successful adoption of any machine learning system in the wild requires it to generalize (and be robust) to unseen distribution shifts at deployment. Unfortunately, current research lacks comprehensive datasets and informative tools to evaluate and understand NIC performance in real-world settings. To bridge this crucial gap, first, this paper presents a comprehensive benchmark suite to evaluate the out-of-distribution (OOD) performance of image compression methods. Specifically, we provide CLIC-C and Kodak-C by introducing 15 corruptions to popular CLIC and Kodak benchmarks. Next, we propose spectrally inspired inspection tools to gain deeper insight into errors introduced by image compression methods as well as their OOD performance. We then carry out a detailed performance comparison of a classical codec with several NIC variants, revealing intriguing findings that challenge our current understanding of the strengths and limitations of NIC. Finally, we corroborate our empirical findings with theoretical analysis, providing an in-depth view of the OOD performance of NIC and its dependence on the spectral properties of the data. Our benchmarks, spectral inspection tools, and findings provide a crucial bridge to the real-world adoption of NIC. We hope that our work will propel future efforts in designing robust and generalizable NIC methods. Code and data will be made available at https://github.com/klieberman/ood_nic.
</details>
<details>
<summary>摘要</summary>
Recent neural image compression (NIC) advances have produced models that are starting to outperform traditional codecs. While this has led to growing excitement about using NIC in real-world applications, the successful adoption of any machine learning system in the wild requires it to generalize (and be robust) to unseen distribution shifts at deployment. Unfortunately, current research lacks comprehensive datasets and informative tools to evaluate and understand NIC performance in real-world settings. To bridge this crucial gap, first, this paper presents a comprehensive benchmark suite to evaluate the out-of-distribution (OOD) performance of image compression methods. Specifically, we provide CLIC-C and Kodak-C by introducing 15 corruptions to popular CLIC and Kodak benchmarks. Next, we propose spectrally inspired inspection tools to gain deeper insight into errors introduced by image compression methods as well as their OOD performance. We then carry out a detailed performance comparison of a classical codec with several NIC variants, revealing intriguing findings that challenge our current understanding of the strengths and limitations of NIC. Finally, we corroborate our empirical findings with theoretical analysis, providing an in-depth view of the OOD performance of NIC and its dependence on the spectral properties of the data. Our benchmarks, spectral inspection tools, and findings provide a crucial bridge to the real-world adoption of NIC. We hope that our work will propel future efforts in designing robust and generalizable NIC methods. Code and data will be made available at https://github.com/klieberman/ood_nic.Here's the translation in Traditional Chinese:Recent neural image compression (NIC) advances have produced models that are starting to outperform traditional codecs. While this has led to growing excitement about using NIC in real-world applications, the successful adoption of any machine learning system in the wild requires it to generalize (and be robust) to unseen distribution shifts at deployment. Unfortunately, current research lacks comprehensive datasets and informative tools to evaluate and understand NIC performance in real-world settings. To bridge this crucial gap, first, this paper presents a comprehensive benchmark suite to evaluate the out-of-distribution (OOD) performance of image compression methods. Specifically, we provide CLIC-C and Kodak-C by introducing 15 corruptions to popular CLIC and Kodak benchmarks. Next, we propose spectrally inspired inspection tools to gain deeper insight into errors introduced by image compression methods as well as their OOD performance. We then carry out a detailed performance comparison of a classical codec with several NIC variants, revealing intriguing findings that challenge our current understanding of the strengths and limitations of NIC. Finally, we corroborate our empirical findings with theoretical analysis, providing an in-depth view of the OOD performance of NIC and its dependence on the spectral properties of the data. Our benchmarks, spectral inspection tools, and findings provide a crucial bridge to the real-world adoption of NIC. We hope that our work will propel future efforts in designing robust and generalizable NIC methods. Code and data will be made available at https://github.com/klieberman/ood_nic.
</details></li>
</ul>
<hr>
<h2 id="A-General-Framework-for-Learning-under-Corruption-Label-Noise-Attribute-Noise-and-Beyond"><a href="#A-General-Framework-for-Learning-under-Corruption-Label-Noise-Attribute-Noise-and-Beyond" class="headerlink" title="A General Framework for Learning under Corruption: Label Noise, Attribute Noise, and Beyond"></a>A General Framework for Learning under Corruption: Label Noise, Attribute Noise, and Beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08643">http://arxiv.org/abs/2307.08643</a></li>
<li>repo_url: None</li>
<li>paper_authors: Laura Iacovissi, Nan Lu, Robert C. Williamson</li>
<li>for: 本研究旨在系统地分析损害模型在分布水平上的影响，提供一个涵盖所有损害模型的通用框架，并研究损害对标准预测学习的影响。</li>
<li>methods: 本研究使用Markov kernel来形式地分析损害模型，并发现了 Label和特征上的复杂相互作用和依赖关系，这些关系通常被之前的研究所忽略。</li>
<li>results: 研究发现，损害对标准预测学习会导致 bayes 风险的变化，并提供了对不同损害实例的loss correction的理论分析。<details>
<summary>Abstract</summary>
Corruption is frequently observed in collected data and has been extensively studied in machine learning under different corruption models. Despite this, there remains a limited understanding of how these models relate such that a unified view of corruptions and their consequences on learning is still lacking. In this work, we formally analyze corruption models at the distribution level through a general, exhaustive framework based on Markov kernels. We highlight the existence of intricate joint and dependent corruptions on both labels and attributes, which are rarely touched by existing research. Further, we show how these corruptions affect standard supervised learning by analyzing the resulting changes in Bayes Risk. Our findings offer qualitative insights into the consequences of "more complex" corruptions on the learning problem, and provide a foundation for future quantitative comparisons. Applications of the framework include corruption-corrected learning, a subcase of which we study in this paper by theoretically analyzing loss correction with respect to different corruption instances.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>腐败是收集数据中常见的现象，在机器学习中也广泛研究了不同的腐败模型。尽管如此，我们对这些模型之间的关系仍然具有有限的理解，而且还缺乏一个总体的视角来描述这些腐败和它们对学习的影响。在这项工作中，我们使用Markov核来正式分析腐败模型的分布水平。我们发现了 labels和特征上的复杂联合腐败，这些腐败通常不受现有研究的关注。此外，我们还分析了这些腐败对标准指导学习的影响，并研究了由不同的腐败实例导致的损失修复。我们的发现可以提供对"更复杂"腐败对学习问题的影响的质量性理解，并为未来的量化比较提供基础。应用该框架包括腐败修正学习，我们在这篇论文中对这个子情况进行了理论分析。
</details></li>
</ul>
<hr>
<h2 id="LearnedSort-as-a-learning-augmented-SampleSort-Analysis-and-Parallelization"><a href="#LearnedSort-as-a-learning-augmented-SampleSort-Analysis-and-Parallelization" class="headerlink" title="LearnedSort as a learning-augmented SampleSort: Analysis and Parallelization"></a>LearnedSort as a learning-augmented SampleSort: Analysis and Parallelization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08637">http://arxiv.org/abs/2307.08637</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ivan Carvalho, Ramon Lawrence</li>
<li>for: 本文 analyze 和 parallelize LearnedSort algorithm，一种使用机器学习模型来实现排序的新算法。</li>
<li>methods: 本文使用了对predictions的分析， argue  dass LearnedSort 是一种learning-augmented SampleSort。</li>
<li>results: 对 synthetic 和实际 dataset 进行了 benchmark， parallel LearnedSort 比 IPS4o 和其他排序算法具有更高的并发性能。<details>
<summary>Abstract</summary>
This work analyzes and parallelizes LearnedSort, the novel algorithm that sorts using machine learning models based on the cumulative distribution function. LearnedSort is analyzed under the lens of algorithms with predictions, and it is argued that LearnedSort is a learning-augmented SampleSort. A parallel LearnedSort algorithm is developed combining LearnedSort with the state-of-the-art SampleSort implementation, IPS4o. Benchmarks on synthetic and real-world datasets demonstrate improved parallel performance for parallel LearnedSort compared to IPS4o and other sorting algorithms.
</details>
<details>
<summary>摘要</summary>
这个工作分析并平行化了 LearnedSort 算法，这是基于累累函数的机器学习模型来进行排序的新算法。 LearnedSort 被视为一种基于预测的算法，并且被证明是一种增强SampleSort的学习算法。我们开发了一种将 LearnedSort 与现有的 SampleSort 实现 IPS4o 结合的平行 LearnedSort 算法。对假数据和实际数据集进行了比较，结果显示了平行 LearnedSort 的性能提高 compared to IPS4o 和其他排序算法。
</details></li>
</ul>
<hr>
<h2 id="Retentive-Network-A-Successor-to-Transformer-for-Large-Language-Models"><a href="#Retentive-Network-A-Successor-to-Transformer-for-Large-Language-Models" class="headerlink" title="Retentive Network: A Successor to Transformer for Large Language Models"></a>Retentive Network: A Successor to Transformer for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08621">http://arxiv.org/abs/2307.08621</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/unilm">https://github.com/microsoft/unilm</a></li>
<li>paper_authors: Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, Furu Wei</li>
<li>for: This paper proposes a new architecture called Retentive Network (RetNet) for large language models, which simultaneously achieves training parallelism, low-cost inference, and good performance.</li>
<li>methods: The paper uses a retention mechanism for sequence modeling, which supports three computation paradigms: parallel, recurrent, and chunkwise recurrent. The parallel representation allows for training parallelism, while the recurrent representation enables low-cost $O(1)$ inference. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity.</li>
<li>results: The paper shows that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference. Experimental results on language modeling demonstrate the effectiveness of RetNet, making it a strong successor to Transformer for large language models.<details>
<summary>Abstract</summary>
In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost $O(1)$ inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference. The intriguing properties make RetNet a strong successor to Transformer for large language models. Code will be available at https://aka.ms/retnet.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们提议Retentive Network（RetNet）作为大语言模型的基础架构，同时实现培训并行、低成本推理和好性能。我们理论上 derivates了回忆和注意力之间的连接。然后我们提议了保留机制，用于序列模型化，该机制支持三种计算方式，即并行、循环和块级循环。Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost $O(1)$ inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks.实验结果表明，RetNet实现了有利扩展性、并行培训、低成本部署和高效推理。RetNet的特有性使其成为Transformer的强 successor for large language models。代码将提供在https://aka.ms/retnet.
</details></li>
</ul>
<hr>
<h2 id="Understanding-the-impacts-of-crop-diversification-in-the-context-of-climate-change-a-machine-learning-approach"><a href="#Understanding-the-impacts-of-crop-diversification-in-the-context-of-climate-change-a-machine-learning-approach" class="headerlink" title="Understanding the impacts of crop diversification in the context of climate change: a machine learning approach"></a>Understanding the impacts of crop diversification in the context of climate change: a machine learning approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08617">http://arxiv.org/abs/2307.08617</a></li>
<li>repo_url: None</li>
<li>paper_authors: Georgios Giannarakis, Ilias Tsoumas, Stelios Neophytides, Christiana Papoutsa, Charalampos Kontoes, Diofantos Hadjimitsis</li>
<li>for: 这个论文是为了研究农业可持续强化的方法，以及这些方法在气候变化的情况下的影响。</li>
<li>methods: 这篇论文使用了多种数据和机器学习方法来研究农业生产力的影响。</li>
<li>results: 论文发现，在更暖和干燥的气候下，多种作物杂 cultivation 能够提高农业生产力，平均提高了2.8%。这种效果与高温和低湿度有相互作用。<details>
<summary>Abstract</summary>
The concept of sustainable intensification in agriculture necessitates the implementation of management practices that prioritize sustainability without compromising productivity. However, the effects of such practices are known to depend on environmental conditions, and are therefore expected to change as a result of a changing climate. We study the impact of crop diversification on productivity in the context of climate change. We leverage heterogeneous Earth Observation data and contribute a data-driven approach based on causal machine learning for understanding how crop diversification impacts may change in the future. We apply this method to the country of Cyprus throughout a 4-year period. We find that, on average, crop diversification significantly benefited the net primary productivity of crops, increasing it by 2.8%. The effect generally synergized well with higher maximum temperatures and lower soil moistures. In a warmer and more drought-prone climate, we conclude that crop diversification exhibits promising adaptation potential and is thus a sensible policy choice with regards to agricultural productivity for present and future.
</details>
<details>
<summary>摘要</summary>
“减少农业的环境影响是一种把持可持续发展的概念，但这些实践的效果受环境因素的影响，因此随着气候变化而改变。我们研究了采用多种作物杂 planting 对产量的影响，并通过基于 causal machine learning 的数据驱动方法来理解这些影响可能在未来如何变化。我们在塞浦路斯国家范围内进行了4年的研究，发现，在平均来说，多种作物杂 planting 对农作物的 net primary productivity 产生了2.8%的增长。这种效果通常与高温和低湿度相关。在将来的气候变化中，我们认为多种作物杂 planting 具有良好的适应能力，因此是一种有理解的农业产量政策选择。”Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese used in mainland China and Singapore. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Temporal-and-Geographical-Analysis-of-Real-Economic-Activities-in-the-Bitcoin-Blockchain"><a href="#Temporal-and-Geographical-Analysis-of-Real-Economic-Activities-in-the-Bitcoin-Blockchain" class="headerlink" title="Temporal and Geographical Analysis of Real Economic Activities in the Bitcoin Blockchain"></a>Temporal and Geographical Analysis of Real Economic Activities in the Bitcoin Blockchain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08616">http://arxiv.org/abs/2307.08616</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rafael Ramos Tubino, Remy Cazabet, Natkamon Tovanich, Celine Robardet</li>
<li>for: The paper focuses on the real economic activity in the Bitcoin blockchain, specifically on transactions between retail users and their neighbors, rather than between organizations.</li>
<li>methods: The paper introduces a heuristic method to classify Bitcoin players into three main categories: Frequent Receivers (FR), Neighbors of FR, and Others.</li>
<li>results: Most real transactions involve Frequent Receivers, who represent a small fraction of the total value exchanged but a significant fraction of all payments, which raises concerns about the centralization of the Bitcoin ecosystem. Additionally, the paper conducts a weekly pattern analysis of activity to provide insights into the geographical location of Bitcoin users and to quantify the bias of a well-known dataset for actor identification.Here are the same information points in Simplified Chinese text:</li>
<li>for: 这篇论文关注比特币链上真实的经济活动，具体来说是对于个人用户的交易，而不是 между机构如市场、交易所或其他服务。</li>
<li>methods: 论文提出一种归纳方法，将比特币玩家分为三类：固定接收者（FR）、邻居 FR 和其他人。</li>
<li>results: 实际交易主要发生在固定接收者身上，占总交易值的小部分，但占所有支付的重要部分，这引发了中央化的担忧。论文还进行了每周活动模式分析，提供了比特币用户的地理位置信息，并且量化了一个常见的数据集的偏见。<details>
<summary>Abstract</summary>
We study the real economic activity in the Bitcoin blockchain that involves transactions from/to retail users rather than between organizations such as marketplaces, exchanges, or other services. We first introduce a heuristic method to classify Bitcoin players into three main categories: Frequent Receivers (FR), Neighbors of FR, and Others. We show that most real transactions involve Frequent Receivers, representing a small fraction of the total value exchanged according to the blockchain, but a significant fraction of all payments, raising concerns about the centralization of the Bitcoin ecosystem. We also conduct a weekly pattern analysis of activity, providing insights into the geographical location of Bitcoin users and allowing us to quantify the bias of a well-known dataset for actor identification.
</details>
<details>
<summary>摘要</summary>
我们研究比特币区块链上真正的经济活动，涉及到零售用户的交易而不是组织如市场、交易所或其他服务。我们首先提出一种启发法来分类比特币玩家为三个主要类别：固定接收者（FR）、邻居FR和其他人。我们表明，大多数真实交易发生在固定接收者身上，表示比特币总额中的一小部分，但是对所有支付都占有重要的比重，引发了中央化比特币生态系统的问题。我们还进行了每周活动模式分析，提供了比特币用户的地理位置信息，并使我们可以评估一个常见的数据集中截然的偏见。
</details></li>
</ul>
<hr>
<h2 id="Hyperparameter-Tuning-Cookbook-A-guide-for-scikit-learn-PyTorch-river-and-spotPython"><a href="#Hyperparameter-Tuning-Cookbook-A-guide-for-scikit-learn-PyTorch-river-and-spotPython" class="headerlink" title="Hyperparameter Tuning Cookbook: A guide for scikit-learn, PyTorch, river, and spotPython"></a>Hyperparameter Tuning Cookbook: A guide for scikit-learn, PyTorch, river, and spotPython</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10262">http://arxiv.org/abs/2307.10262</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sequential-parameter-optimization/spotpython">https://github.com/sequential-parameter-optimization/spotpython</a></li>
<li>paper_authors: Thomas Bartz-Beielstein</li>
<li>for: 本文提供了一个完整的 гипер参数优化指南，使用 spotPython 对 scikit-learn、PyTorch 和 river 进行优化。</li>
<li>methods: 本文使用 spotPython 的模拟模型基于优化过程，并详细介绍了 hyperparameter tuning 的过程。</li>
<li>results: 本文通过多个案例研究，包括 sklearn 模型 Support Vector Classification、Random Forests、Gradient Boosting (XGB) 和 K-nearest neighbors (KNN) 的 hyperparameter tuning，以及 river 中的 Hoeffding Adaptive Tree Regressor。  plus, the integration of spotPython into the PyTorch and PyTorch Lightning training workflow is also discussed.<details>
<summary>Abstract</summary>
This document provides a comprehensive guide to hyperparameter tuning using spotPython for scikit-learn, PyTorch, and river. The first part introduces spotPython's surrogate model-based optimization process, while the second part focuses on hyperparameter tuning. Several case studies are presented, including hyperparameter tuning for sklearn models such as Support Vector Classification, Random Forests, Gradient Boosting (XGB), and K-nearest neighbors (KNN), as well as a Hoeffding Adaptive Tree Regressor from river. The integration of spotPython into the PyTorch and PyTorch Lightning training workflow is also discussed. With a hands-on approach and step-by-step explanations, this cookbook serves as a practical starting point for anyone interested in hyperparameter tuning with Python. Highlights include the interplay between Tensorboard, PyTorch Lightning, spotPython, and river. This publication is under development, with updates available on the corresponding webpage.
</details>
<details>
<summary>摘要</summary>
这份文档提供了使用spotPython进行scikit-learn、PyTorch和river中的参数优化的全面指南。文档的首部介绍了spotPython的代理模型基于优化过程，而第二部分则专注于参数优化。文档包含了多个案例研究，包括scikit-learn模型如支持向量分类、随机森林、梯度折衔（XGB）和K最近邻（KNN）等，以及来自river的韦伯丁适应树回归模型。文档还讨论了spotPython在PyTorch和PyTorch Lightning训练工作流程中的集成。通过实践的方式和步骤说明，这本cookbook作为Python中参数优化的实践开始点，强调了Tensorboard、PyTorch Lightning、spotPython和river之间的互动。这份文档正在开发中，更新信息可以通过相应的网页获得。
</details></li>
</ul>
<hr>
<h2 id="Artificial-Intelligence-for-the-Electron-Ion-Collider-AI4EIC"><a href="#Artificial-Intelligence-for-the-Electron-Ion-Collider-AI4EIC" class="headerlink" title="Artificial Intelligence for the Electron Ion Collider (AI4EIC)"></a>Artificial Intelligence for the Electron Ion Collider (AI4EIC)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08593">http://arxiv.org/abs/2307.08593</a></li>
<li>repo_url: None</li>
<li>paper_authors: C. Allaire, R. Ammendola, E. -C. Aschenauer, M. Balandat, M. Battaglieri, J. Bernauer, M. Bondì, N. Branson, T. Britton, A. Butter, I. Chahrour, P. Chatagnon, E. Cisbani, E. W. Cline, S. Dash, C. Dean, W. Deconinck, A. Deshpande, M. Diefenthaler, R. Ent, C. Fanelli, M. Finger, M. Finger, Jr., E. Fol, S. Furletov, Y. Gao, J. Giroux, N. C. Gunawardhana Waduge, R. Harish, O. Hassan, P. L. Hegde, R. J. Hernández-Pinto, A. Hiller Blin, T. Horn, J. Huang, D. Jayakodige, B. Joo, M. Junaid, P. Karande, B. Kriesten, R. Kunnawalkam Elayavalli, M. Lin, F. Liu, S. Liuti, G. Matousek, M. McEneaney, D. McSpadden, T. Menzo, T. Miceli, V. Mikuni, R. Montgomery, B. Nachman, R. R. Nair, J. Niestroy, S. A. Ochoa Oregon, J. Oleniacz, J. D. Osborn, C. Paudel, C. Pecar, C. Peng, G. N. Perdue, W. Phelps, M. L. Purschke, K. Rajput, Y. Ren, D. F. Renteria-Estrada, D. Richford, B. J. Roy, D. Roy, N. Sato, T. Satogata, G. Sborlini, M. Schram, D. Shih, J. Singh, R. Singh, A. Siodmok, P. Stone, J. Stevens, L. Suarez, K. Suresh, A. -N. Tawfik, F. Torales Acosta, N. Tran, R. Trotta, F. J. Twagirayezu, R. Tyson, S. Volkova, A. Vossen, E. Walter, D. Whiteson, M. Williams, S. Wu, N. Zachariou, P. Zurita</li>
<li>for: The paper is written for the EIC community, discussing the potential applications of AI&#x2F;ML in the facility’s experiments and commissioning processes.</li>
<li>methods: The paper covers various R&amp;D projects and approaches currently being explored in the EIC community, including cutting-edge techniques from other experiments.</li>
<li>results: The paper provides an overview of the goals and strategies regarding AI&#x2F;ML in the EIC community, as well as the potential benefits and insights that can be gained from their application.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是为EIC社区写的，探讨了AI&#x2F;ML在该设施的实验和启动过程中的潜在应用。</li>
<li>methods: 论文涵盖了EIC社区当前在进行的多个R&amp;D项目和方法，包括其他实验中的前沿技术。</li>
<li>results: 论文提供了EIC社区对AI&#x2F;ML的应用 goals和策略的概述，以及通过其应用可以获得的优点和发现。<details>
<summary>Abstract</summary>
The Electron-Ion Collider (EIC), a state-of-the-art facility for studying the strong force, is expected to begin commissioning its first experiments in 2028. This is an opportune time for artificial intelligence (AI) to be included from the start at this facility and in all phases that lead up to the experiments. The second annual workshop organized by the AI4EIC working group, which recently took place, centered on exploring all current and prospective application areas of AI for the EIC. This workshop is not only beneficial for the EIC, but also provides valuable insights for the newly established ePIC collaboration at EIC. This paper summarizes the different activities and R&D projects covered across the sessions of the workshop and provides an overview of the goals, approaches and strategies regarding AI/ML in the EIC community, as well as cutting-edge techniques currently studied in other experiments.
</details>
<details>
<summary>摘要</summary>
电子离子碰撞器（EIC），一个最先进的强相互作用研究设施，预计在2028年开始首次实验启用。这是一个非常有利的时机，让人工智能（AI）从设施的开始就包括在内，并在所有实验阶段进行应用。第二年度的AI4EIC工作坊，由AI4EIC工作组组织，最近召开，主要是探讨CURRENT AND PROSPECTIVE APPLICATION AREAS OF AI FOR EIC。这个工作坊不仅对EIC有利，还为新成立的ePIC合作项目提供了宝贵的经验。本文将summarize工作坊的不同活动和R&D项目，提供EIC社区关于AI/ML的目标、方法和战略，以及目前在其他实验中研究的前沿技术。
</details></li>
</ul>
<hr>
<h2 id="Snapshot-Spectral-Clustering-–-a-costless-approach-to-deep-clustering-ensembles-generation"><a href="#Snapshot-Spectral-Clustering-–-a-costless-approach-to-deep-clustering-ensembles-generation" class="headerlink" title="Snapshot Spectral Clustering – a costless approach to deep clustering ensembles generation"></a>Snapshot Spectral Clustering – a costless approach to deep clustering ensembles generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08591">http://arxiv.org/abs/2307.08591</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam Piróg, Halina Kwaśnicka</li>
<li>for: 本研究旨在探讨将深度学习与聚类结合使用，以提高聚类结果的准确性和稳定性。</li>
<li>methods: 本研究提出了一种新的深度聚类协同方法（Snapshot Spectral Clustering），利用多个视角的数据生成多个深度学习模型，并将其组合以实现更高的聚类精度和稳定性。</li>
<li>results: 实验结果表明，Snapshot Spectral Clustering方法可以减少计算成本，同时提高聚类结果的准确性和稳定性，相比于传统的聚类方法和深度学习方法。<details>
<summary>Abstract</summary>
Despite tremendous advancements in Artificial Intelligence, learning from large sets of data in an unsupervised manner remains a significant challenge. Classical clustering algorithms often fail to discover complex dependencies in large datasets, especially considering sparse, high-dimensional spaces. However, deep learning techniques proved to be successful when dealing with large quantities of data, efficiently reducing their dimensionality without losing track of underlying information. Several interesting advancements have already been made to combine deep learning and clustering. Still, the idea of enhancing the clustering results by combining multiple views of the data generated by deep neural networks appears to be insufficiently explored yet. This paper aims to investigate this direction and bridge the gap between deep neural networks, clustering techniques and ensemble learning methods. To achieve this goal, we propose a novel deep clustering ensemble method - Snapshot Spectral Clustering, designed to maximize the gain from combining multiple data views while minimizing the computational costs of creating the ensemble. Comparative analysis and experiments described in this paper prove the proposed concept, while the conducted hyperparameter study provides a valuable intuition to follow when selecting proper values.
</details>
<details>
<summary>摘要</summary>
To achieve this goal, we propose a novel deep clustering ensemble method called Snapshot Spectral Clustering. This method is designed to maximize the gain from combining multiple data views while minimizing computational costs. Our comparative analysis and experiments show that the proposed method is effective, and a hyperparameter study provides valuable intuition for selecting appropriate values.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/18/cs.LG_2023_07_18/" data-id="cloimipab00lds4884tatewfl" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/18/eess.IV_2023_07_18/" class="article-date">
  <time datetime="2023-07-18T09:00:00.000Z" itemprop="datePublished">2023-07-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/18/eess.IV_2023_07_18/">eess.IV - 2023-07-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Regression-free-Blind-Image-Quality-Assessment"><a href="#Regression-free-Blind-Image-Quality-Assessment" class="headerlink" title="Regression-free Blind Image Quality Assessment"></a>Regression-free Blind Image Quality Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09279">http://arxiv.org/abs/2307.09279</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/XiaoqiWang/regression-free-iqa">https://github.com/XiaoqiWang/regression-free-iqa</a></li>
<li>paper_authors: Xiaoqi Wang, Jian Xiong, Hao Gao, Weisi Lin</li>
<li>for: 提高图像质量评估模型的准确性，避免因训练样本偏袋而导致的模型参数估计偏离 reality。</li>
<li>methods: 基于检索相似图像的快速准确评估方法，包括semantic-based classification（SC）模块和distortion-based classification（DC）模块。</li>
<li>results: 对四个标准数据库进行实验，研究发现该方法可以remarkably outperform当前最佳的 regression-based 模型。<details>
<summary>Abstract</summary>
Regression-based blind image quality assessment (IQA) models are susceptible to biased training samples, leading to a biased estimation of model parameters. To mitigate this issue, we propose a regression-free framework for image quality evaluation, which is founded upon retrieving similar instances by incorporating semantic and distortion features. The motivation behind this approach is rooted in the observation that the human visual system (HVS) has analogous visual responses to semantically similar image contents degraded by the same distortion. The proposed framework comprises two classification-based modules: semantic-based classification (SC) module and distortion-based classification (DC) module. Given a test image and an IQA database, the SC module retrieves multiple pristine images based on semantic similarity. The DC module then retrieves instances based on distortion similarity from the distorted images that correspond to each retrieved pristine image. Finally, the predicted quality score is derived by aggregating the subjective quality scores of multiple retrieved instances. Experimental results on four benchmark databases validate that the proposed model can remarkably outperform the state-of-the-art regression-based models.
</details>
<details>
<summary>摘要</summary>
“受训数据受损”问题导致抽象� returns blind图像质量评估（IQA）模型受损。为了解决这个问题，我们提出了一种不含回归的图像质量评估框架，基于检索相似实例。我们发现，人视系统（HVS）在Semantic� 相似的图像内容下具有相似的视觉响应，这成为我们的 Motivation。该框架包括两个分类模块：Semantic-based Classification（SC）模块和Distortion-based Classification（DC）模块。给定一个测试图像和IQA数据库，SC模块首先检索相似的整图，然后DC模块从相应的扭曲图像中检索具有相同扭曲的实例。最后，预测的质量分数由多个检索到的实例的主观质量分数进行汇总得来。实验结果表明，我们提出的模型可以很好地超越当前的回归型模型。
</details></li>
</ul>
<hr>
<h2 id="Soft-IntroVAE-for-Continuous-Latent-space-Image-Super-Resolution"><a href="#Soft-IntroVAE-for-Continuous-Latent-space-Image-Super-Resolution" class="headerlink" title="Soft-IntroVAE for Continuous Latent space Image Super-Resolution"></a>Soft-IntroVAE for Continuous Latent space Image Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09008">http://arxiv.org/abs/2307.09008</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhi-Song Liu, Zijia Wang, Zhen Jia</li>
<li>for: 这个研究是为了提出一个基于Variational AutoEncoder的连续图像超解析方法，以提供实用和灵活的图像扩展 для不同的显示器。</li>
<li>methods: 本研究使用了Local implicit image representation来将坐标和2D特征映射到隐藏空间中，并通过一种新的潜在空间对抗训练来实现照相实际的图像重建。</li>
<li>results: 研究人员透过量化和质感比较，证明了提案的Soft-introVAE-SR方法的效果，并且显示了其在对照噪声和实际图像超解析中的一般化能力。<details>
<summary>Abstract</summary>
Continuous image super-resolution (SR) recently receives a lot of attention from researchers, for its practical and flexible image scaling for various displays. Local implicit image representation is one of the methods that can map the coordinates and 2D features for latent space interpolation. Inspired by Variational AutoEncoder, we propose a Soft-introVAE for continuous latent space image super-resolution (SVAE-SR). A novel latent space adversarial training is achieved for photo-realistic image restoration. To further improve the quality, a positional encoding scheme is used to extend the original pixel coordinates by aggregating frequency information over the pixel areas. We show the effectiveness of the proposed SVAE-SR through quantitative and qualitative comparisons, and further, illustrate its generalization in denoising and real-image super-resolution.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。</SYS>>latest continuous image super-resolution (SR) technology has gained significant attention from researchers due to its practical and flexible image scaling capabilities for various displays. local implicit image representation is a method that can map coordinates and 2D features to latent space for interpolation. inspired by Variational AutoEncoder, we propose a Soft-introVAE for continuous latent space image super-resolution (SVAE-SR). a novel latent space adversarial training is achieved for photo-realistic image restoration. to further improve quality, a positional encoding scheme is used to extend the original pixel coordinates by aggregating frequency information over pixel areas. we demonstrate the effectiveness of the proposed SVAE-SR through quantitative and qualitative comparisons, and further illustrate its generalization in denoising and real-image super-resolution.Here's the translation in Traditional Chinese:<<SYS>>将文本翻译成简化中文。</SYS>>最新的连续图像超解析（SR）技术在研究人员中获得了很大的关注，因为它具有实用和 flexible 的图像扩展功能 для多种显示器。本地隐式图像表示是一种可以将坐标和2D特征映射到 latent space 中的方法，以便进行插值。受 Variational AutoEncoder 的启发，我们提议了 Soft-introVAE  для连续 latent space 图像超解析（SVAE-SR）。我们还实现了一种新的 latent space 反击训练，以达到真实图像 Restoration。为了进一步提高质量，我们使用了一个位置编码方案，将原始像素坐标与像素区域的频率信息聚合。我们显示了 SVAE-SR 的效果，通过量itative和质感比较，并进一步显示其扩展到干扰和真实图像超解析。
</details></li>
</ul>
<hr>
<h2 id="Frequency-mixed-Single-source-Domain-Generalization-for-Medical-Image-Segmentation"><a href="#Frequency-mixed-Single-source-Domain-Generalization-for-Medical-Image-Segmentation" class="headerlink" title="Frequency-mixed Single-source Domain Generalization for Medical Image Segmentation"></a>Frequency-mixed Single-source Domain Generalization for Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09005">http://arxiv.org/abs/2307.09005</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liamheng/non-iid_medical_image_segmentation">https://github.com/liamheng/non-iid_medical_image_segmentation</a></li>
<li>paper_authors: Heng Li, Haojin Li, Wei Zhao, Huazhu Fu, Xiuyun Su, Yan Hu, Jiang Liu</li>
<li>for: 提高医疗影像分类模型的普遍性，特别是当标注数据短缺时。</li>
<li>methods: 提出了一个叫做“频率混合单源领域普遍化法”（FreeSDG），利用不同频率的混合 Spectrum 来增强单源领域，同时运用自我监督来学习具有上下文感知的表示。</li>
<li>results: 实验结果显示，FreeSDG 比前一代方法更有效率，可以优化医疗影像分类模型的普遍性，特别是当标注数据短缺时。<details>
<summary>Abstract</summary>
The annotation scarcity of medical image segmentation poses challenges in collecting sufficient training data for deep learning models. Specifically, models trained on limited data may not generalize well to other unseen data domains, resulting in a domain shift issue. Consequently, domain generalization (DG) is developed to boost the performance of segmentation models on unseen domains. However, the DG setup requires multiple source domains, which impedes the efficient deployment of segmentation algorithms in clinical scenarios. To address this challenge and improve the segmentation model's generalizability, we propose a novel approach called the Frequency-mixed Single-source Domain Generalization method (FreeSDG). By analyzing the frequency's effect on domain discrepancy, FreeSDG leverages a mixed frequency spectrum to augment the single-source domain. Additionally, self-supervision is constructed in the domain augmentation to learn robust context-aware representations for the segmentation task. Experimental results on five datasets of three modalities demonstrate the effectiveness of the proposed algorithm. FreeSDG outperforms state-of-the-art methods and significantly improves the segmentation model's generalizability. Therefore, FreeSDG provides a promising solution for enhancing the generalization of medical image segmentation models, especially when annotated data is scarce. The code is available at https://github.com/liamheng/Non-IID_Medical_Image_Segmentation.
</details>
<details>
<summary>摘要</summary>
医学影像分割的标注缺乏问题使得深度学习模型的训练数据不够，这会导致模型在未见的数据域上不好地泛化。为了解决这个问题，域泛化（DG）技术被开发出来，以提高分割模型在未见的数据域上的性能。然而，DG设置需要多个源域，这阻碍了临床应用中的深度学习模型的有效部署。为了解决这个挑战并提高分割模型的泛化性，我们提出了一种新的方法：频率混合单源域泛化方法（FreeSDG）。通过分析频率对域差异的效果，FreeSDG利用混合频率谱来扩展单源域。此外，我们还构建了基于频率域的自我超vision来学习Context-aware表示。实验结果表明，FreeSDG方法可以高效地提高分割模型的泛化性。我们对五个数据集进行了五种modalities的实验，并证明FreeSDG方法可以与当前状态的方法相比，显著提高分割模型的泛化性。因此，FreeSDG方法提供了一种有效的解决医学影像分割模型的标注缺乏问题的方法，特别是当 annotated data scarce 时。代码可以在 <https://github.com/liamheng/Non-IID_Medical_Image_Segmentation> 上获取。
</details></li>
</ul>
<hr>
<h2 id="Learned-Scalable-Video-Coding-For-Humans-and-Machines"><a href="#Learned-Scalable-Video-Coding-For-Humans-and-Machines" class="headerlink" title="Learned Scalable Video Coding For Humans and Machines"></a>Learned Scalable Video Coding For Humans and Machines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08978">http://arxiv.org/abs/2307.08978</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hadi Hadizadeh, Ivan V. Bajić</li>
<li>for: 这个论文主要是为了支持自动视频分析，而不是人类视觉。</li>
<li>methods: 该论文使用了深度神经网络（DNN）来实现视频编码，并使用了 conditional coding 来提高压缩效果。</li>
<li>results: 实验结果表明，该系统在基层和优化层中都可以实现更好的压缩效果，并且可以在机器视觉任务和人类视觉任务之间进行可替换。<details>
<summary>Abstract</summary>
Video coding has traditionally been developed to support services such as video streaming, videoconferencing, digital TV, and so on. The main intent was to enable human viewing of the encoded content. However, with the advances in deep neural networks (DNNs), encoded video is increasingly being used for automatic video analytics performed by machines. In applications such as automatic traffic monitoring, analytics such as vehicle detection, tracking and counting, would run continuously, while human viewing could be required occasionally to review potential incidents. To support such applications, a new paradigm for video coding is needed that will facilitate efficient representation and compression of video for both machine and human use in a scalable manner. In this manuscript, we introduce the first end-to-end learnable video codec that supports a machine vision task in its base layer, while its enhancement layer supports input reconstruction for human viewing. The proposed system is constructed based on the concept of conditional coding to achieve better compression gains. Comprehensive experimental evaluations conducted on four standard video datasets demonstrate that our framework outperforms both state-of-the-art learned and conventional video codecs in its base layer, while maintaining comparable performance on the human vision task in its enhancement layer. We will provide the implementation of the proposed system at www.github.com upon completion of the review process.
</details>
<details>
<summary>摘要</summary>
视频编码传统上是为服务如视频流式、视频会议、数字电视等服务开发的。主要目的是为人类观看编码内容。然而，随着深度神经网络（DNN）的发展，编码的视频已经在机器自动分析中得到了广泛的应用。例如，在自动交通监测应用中，机器可以通过视频分析来探测、跟踪和计数交通车辆。在这些应用中，人类只需 occasionally 查看可能的意外事件。为支持这些应用，我们需要一种新的视频编码 paradigma，可以帮助高效地表示和压缩视频，以便同时支持机器和人类的使用。在这篇论文中，我们介绍了首个可学习的视频编码器，其基层支持机器视觉任务，而增强层支持人类视觉输入重建。我们的系统基于 conditional coding 的概念，以实现更好的压缩收益。我们在四个标准视频数据集上进行了广泛的实验评估，并证明了我们的框架在基层上比现状 learned 和 conventional 视频编码器更高效，而且在人类视觉任务的增强层中保持了相似的性能。我们将在 GitHub 上提供实现的 propose 系统。
</details></li>
</ul>
<hr>
<h2 id="Deep-Physics-Guided-Unrolling-Generalization-for-Compressed-Sensing"><a href="#Deep-Physics-Guided-Unrolling-Generalization-for-Compressed-Sensing" class="headerlink" title="Deep Physics-Guided Unrolling Generalization for Compressed Sensing"></a>Deep Physics-Guided Unrolling Generalization for Compressed Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08950">http://arxiv.org/abs/2307.08950</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/guaishou74851/prl">https://github.com/guaishou74851/prl</a></li>
<li>paper_authors: Bin Chen, Jiechong Song, Jingfen Xie, Jian Zhang</li>
<li>for: 这篇论文主要是为了提出一种高精度且可解释的图像重建方法，兼顾了模型驱动和数据驱动方法的优点，以解决 inverse imaging  зада题中的问题。</li>
<li>methods: 这篇论文提出了一种基于高维特征空间的Physics-guided unrolled recovery learning（PRL）框架，通过普通迭代法实现高精度的图像重建。此外，作者还提出了两种实现方式：PRL-PGD和PRL-RND。</li>
<li>results: 实验表明，PRL 网络比其他状态 искусственный方法具有显著的性能和效率优势，并且还有很大的应用前景，可以应用于其他 inverse imaging 问题或优化模型。<details>
<summary>Abstract</summary>
By absorbing the merits of both the model- and data-driven methods, deep physics-engaged learning scheme achieves high-accuracy and interpretable image reconstruction. It has attracted growing attention and become the mainstream for inverse imaging tasks. Focusing on the image compressed sensing (CS) problem, we find the intrinsic defect of this emerging paradigm, widely implemented by deep algorithm-unrolled networks, in which more plain iterations involving real physics will bring enormous computation cost and long inference time, hindering their practical application. A novel deep $\textbf{P}$hysics-guided un$\textbf{R}$olled recovery $\textbf{L}$earning ($\textbf{PRL}$) framework is proposed by generalizing the traditional iterative recovery model from image domain (ID) to the high-dimensional feature domain (FD). A compact multiscale unrolling architecture is then developed to enhance the network capacity and keep real-time inference speeds. Taking two different perspectives of optimization and range-nullspace decomposition, instead of building an algorithm-specific unrolled network, we provide two implementations: $\textbf{PRL-PGD}$ and $\textbf{PRL-RND}$. Experiments exhibit the significant performance and efficiency leading of PRL networks over other state-of-the-art methods with a large potential for further improvement and real application to other inverse imaging problems or optimization models.
</details>
<details>
<summary>摘要</summary>
通过吸收模型和数据驱动方法的优点，深度物理参与学习方案实现高精度和可解释的图像重建。它在反射图像任务中吸引了越来越多的关注，成为主流。但是，对于图像压缩感知（CS）问题，我们发现了深度算法拆箱网络广泛实施的内在缺陷：更多的简单迭代 iterations 会带来巨大的计算成本和长时间推理时间，限制其实际应用。为解决这个问题，我们提出了一种深度物理指导的解析学习（PRL）框架，通过将传统的迭代回归模型从图像域（ID）扩展到高维特征域（FD），提高网络容量和保持实时推理速度。此外，我们还开发了一种嵌入式多尺度拆箱架构，以增强网络的扩展性和灵活性。为了实现PRL网络的实现，我们提出了两种实现方法：PRL-PGD和PRL-RND。首先，我们使用权值迭代（PGD）方法来实现PRL网络的迭代过程，其中每个迭代都是一个简单的PGD过程。其次，我们使用几何范围零空间分解（RND）方法来实现PRL网络的解析过程，这种方法可以快速地解决图像的缺失信息。实验结果表明，PRL网络在反射图像任务中表现出色，与其他当前最佳方法相比，具有显著的性能和效率优势，同时还有很大的潜在提升空间和实际应用前景。
</details></li>
</ul>
<hr>
<h2 id="Image-Processing-Methods-Applied-to-Motion-Tracking-of-Nanomechanical-Buckling-on-SEM-Recordings"><a href="#Image-Processing-Methods-Applied-to-Motion-Tracking-of-Nanomechanical-Buckling-on-SEM-Recordings" class="headerlink" title="Image Processing Methods Applied to Motion Tracking of Nanomechanical Buckling on SEM Recordings"></a>Image Processing Methods Applied to Motion Tracking of Nanomechanical Buckling on SEM Recordings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08786">http://arxiv.org/abs/2307.08786</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ege Erdem, Berke Demiralp, Hadi S Pisheh, Peyman Firoozy, Ahmet Hakan Karakurt, M. Selim Hanay</li>
<li>for: 这个论文是为了解决扫描电子显微镜（SEM）记录的动态纳米电romechanical系统（NEMS）的问题，因为噪声引起的低帧率、不足的分辨率和由应用的电 potential所引起的模糊。</li>
<li>methods: 这个论文使用了一种基于物理系统的图像处理算法，用于跟踪NEMS结构在高噪声水平下的动态运动。该算法包括一个图像滤波器、两个数据滤波器和一个非线性回归模型，利用物理解决方案的预期形式。</li>
<li>results: 该算法可以跟踪NEMS的动态运动和捕捉了压缩力对矩形杆的弯曲强度的依赖关系。通过该算法，可以清晰地分解NEMS在SEM记录中的转换从间隙弯曲到内隙弯曲的过程。<details>
<summary>Abstract</summary>
The scanning electron microscope (SEM) recordings of dynamic nano-electromechanical systems (NEMS) are difficult to analyze due to the noise caused by low frame rate, insufficient resolution and blurriness induced by applied electric potentials. Here, we develop an image processing algorithm enhanced by the physics of the underlying system to track the motion of buckling NEMS structures in the presence of high noise levels. The algorithm is composed of an image filter, two data filters, and a nonlinear regression model, which utilizes the expected form of the physical solution. The method was applied to the recordings of a NEMS beam about 150 nm wide, undergoing intra-and inter-well post-buckling states with a transition rate of approximately 0.5 Hz. The algorithm can track the dynamical motion of the NEMS and capture the dependency of deflection amplitude on the compressive force on the beam. With the help of the proposed algorithm, the transition from inter-well to intra-well motion is clearly resolved for buckling NEMS imaged under SEM.
</details>
<details>
<summary>摘要</summary>
电子透镜记录动态纳米电romechanical系统（NEMS）具有较低的帧率、不够的分辨率和应用电压所引起的噪声，使得分析变得困难。在这种情况下，我们开发了一种基于系统物理学的图像处理算法，用于跟踪NEMS结构在高噪声水平下的动态运动。该算法包括一个图像滤波器、两个数据滤波器和一个非线性回归模型，其利用了系统物理学的预期解。该方法应用于一个宽约150nm的NEMS梁，在0.5Hz的过渡率下进行了内部和外部受压变换。该算法可以跟踪NEMS的动态运动和捕捉压缩力的影响于梁的折弯幅度。通过提posed algorithm，对buckling NEMS的图像进行了清晰的分辨和解决。
</details></li>
</ul>
<hr>
<h2 id="Implementation-of-a-perception-system-for-autonomous-vehicles-using-a-detection-segmentation-network-in-SoC-FPGA"><a href="#Implementation-of-a-perception-system-for-autonomous-vehicles-using-a-detection-segmentation-network-in-SoC-FPGA" class="headerlink" title="Implementation of a perception system for autonomous vehicles using a detection-segmentation network in SoC FPGA"></a>Implementation of a perception system for autonomous vehicles using a detection-segmentation network in SoC FPGA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08682">http://arxiv.org/abs/2307.08682</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vision-agh/mt_kria">https://github.com/vision-agh/mt_kria</a></li>
<li>paper_authors: Maciej Baczmanski, Mateusz Wasala, Tomasz Kryjak</li>
<li>for: 本研究旨在开发一种高效、实时、能效的自动驾驶感知控制系统，以满足不同道路条件下的障碍物识别和环境元素识别等功能要求。</li>
<li>methods: 本文使用MultiTaskV3检测分割网络作为感知系统的基础，并对其进行了适当的训练、量化和实现于AMD Xilinx Kria KV260 Vision AI嵌入式平台。通过这种设备，可以并行加速计算，同时减少能耗。</li>
<li>results: 实验结果显示，该系统在对象检测和图像分割方面具有高度准确性（mAP大于97%和mIoU大于90%），并且在实时性和能效性方面也具有优异表现。<details>
<summary>Abstract</summary>
Perception and control systems for autonomous vehicles are an active area of scientific and industrial research. These solutions should be characterised by high efficiency in recognising obstacles and other environmental elements in different road conditions, real-time capability, and energy efficiency. Achieving such functionality requires an appropriate algorithm and a suitable computing platform. In this paper, we have used the MultiTaskV3 detection-segmentation network as the basis for a perception system that can perform both functionalities within a single architecture. It was appropriately trained, quantised, and implemented on the AMD Xilinx Kria KV260 Vision AI embedded platform. By using this device, it was possible to parallelise and accelerate the computations. Furthermore, the whole system consumes relatively little power compared to a CPU-based implementation (an average of 5 watts, compared to the minimum of 55 watts for weaker CPUs, and the small size (119mm x 140mm x 36mm) of the platform allows it to be used in devices where the amount of space available is limited. It also achieves an accuracy higher than 97% of the mAP (mean average precision) for object detection and above 90% of the mIoU (mean intersection over union) for image segmentation. The article also details the design of the Mecanum wheel vehicle, which was used to test the proposed solution in a mock-up city.
</details>
<details>
<summary>摘要</summary>
自动驾驶车辆的感知和控制系统是科学技术和工业领域的活跃领域。这些解决方案应具备高效地认知障碍物和其他环境元素，实时性和能效性。实现这种功能需要适当的算法和适当的计算平台。在这篇论文中，我们使用了MultiTaskV3检测-分割网络作为感知系统的基础，可以同时完成这两个功能 within 一个架构。它被正确地训练、量化和在AMD Xilinx Kria KV260 Vision AI嵌入式平台上实现。通过使用这个设备，可以并行化和加速计算。此外，整个系统的功耗相对较少，只有5瓦特，比较弱的CPU实现的最低功耗55瓦特，而且平台的尺寸（119mm x 140mm x 36mm）也很小，可以在空间有限的设备中使用。它还实现了对 объек detection的准确率高于97%的mAP，以及对图像分割的准确率高于90%的mIoU。文章还详细介绍了使用的Mecanum轮胎车，该车在模拟城市中测试了提议的解决方案。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/18/eess.IV_2023_07_18/" data-id="cloimipgj011ss4884ckqcnw5" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_07_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/17/cs.SD_2023_07_17/" class="article-date">
  <time datetime="2023-07-17T15:00:00.000Z" itemprop="datePublished">2023-07-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/17/cs.SD_2023_07_17/">cs.SD - 2023-07-17</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="TST-Time-Sparse-Transducer-for-Automatic-Speech-Recognition"><a href="#TST-Time-Sparse-Transducer-for-Automatic-Speech-Recognition" class="headerlink" title="TST: Time-Sparse Transducer for Automatic Speech Recognition"></a>TST: Time-Sparse Transducer for Automatic Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08323">http://arxiv.org/abs/2307.08323</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaohui Zhang, Mangui Liang, Zhengkun Tian, Jiangyan Yi, Jianhua Tao</li>
<li>for: 这篇论文主要是为了解决端到端模型，特别是循环神经网络推导器（RNN-T）在语音识别中的长序处理问题。</li>
<li>methods: 作者提出了一个名为时间叠 sparse 的模型，内置了时间叠 Mechanism。这个 Mechanism 通过将时间解析为更低的时间解析获得了中间表示，然后使用权重平均算法融合这些表示，以生成简单的隐藏状态。</li>
<li>results: 实验结果显示，与 RNN-T 相比，时间叠 transducer 的字元错误率几乎相同，并且实时因子为原始的 50%。通过调整时间解析，时间叠 transducer 可以降低实时因子至原始的 16.54%，但是这需要付出一些精度的损失（4.94%）。<details>
<summary>Abstract</summary>
End-to-end model, especially Recurrent Neural Network Transducer (RNN-T), has achieved great success in speech recognition. However, transducer requires a great memory footprint and computing time when processing a long decoding sequence. To solve this problem, we propose a model named time-sparse transducer, which introduces a time-sparse mechanism into transducer. In this mechanism, we obtain the intermediate representations by reducing the time resolution of the hidden states. Then the weighted average algorithm is used to combine these representations into sparse hidden states followed by the decoder. All the experiments are conducted on a Mandarin dataset AISHELL-1. Compared with RNN-T, the character error rate of the time-sparse transducer is close to RNN-T and the real-time factor is 50.00% of the original. By adjusting the time resolution, the time-sparse transducer can also reduce the real-time factor to 16.54% of the original at the expense of a 4.94% loss of precision.
</details>
<details>
<summary>摘要</summary>
endpoint模型，特别是Recurrent Neural Network Transducer (RNN-T)，在语音识别中取得了很大成功。然而，抽取器需要较大的内存占用量和计算时间，特别是处理长度较长的解码序列。为解决这个问题，我们提出了一种模型，即时间稀缺抽取器（Time-Sparse Transducer）。在这种机制中，我们通过减少隐藏状态的时间分辨率来获得中间表示。然后，我们使用权重平均算法将这些表示与解码器结合。在使用AISHELL-1 dataset进行所有实验后，我们发现，相比RNN-T，时间稀缺抽取器的字符错误率几乎相同，实时因子为原始的50.00%。通过调整时间分辨率，时间稀缺抽取器还可以降低实时因子至原始的16.54%，同时付出了4.94%的精度损失。
</details></li>
</ul>
<hr>
<h2 id="ivrit-ai-A-Comprehensive-Dataset-of-Hebrew-Speech-for-AI-Research-and-Development"><a href="#ivrit-ai-A-Comprehensive-Dataset-of-Hebrew-Speech-for-AI-Research-and-Development" class="headerlink" title="ivrit.ai: A Comprehensive Dataset of Hebrew Speech for AI Research and Development"></a>ivrit.ai: A Comprehensive Dataset of Hebrew Speech for AI Research and Development</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08720">http://arxiv.org/abs/2307.08720</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yairl/ivrit.ai">https://github.com/yairl/ivrit.ai</a></li>
<li>paper_authors: Yanir Marmor, Kinneret Misgav, Yair Lifshitz</li>
<li>for: 提高希伯来语自动语音识别技术的研究和开发</li>
<li>methods: 使用了3,300小时的希伯来语音数据，包括1,000多个不同的说话人，并提供了不同的研究需求的三种数据形式：原始未处理的音频数据、后Voice Activity Detection的数据，以及部分转写的数据</li>
<li>results: 提供了一个大量的希伯来语音数据资源，可以免费使用，对研究人员、开发者和商业机构都是一个重要的资源，可以推进希伯来语言在人工智能技术中的发展<details>
<summary>Abstract</summary>
We introduce "ivrit.ai", a comprehensive Hebrew speech dataset, addressing the distinct lack of extensive, high-quality resources for advancing Automated Speech Recognition (ASR) technology in Hebrew. With over 3,300 speech hours and a over a thousand diverse speakers, ivrit.ai offers a substantial compilation of Hebrew speech across various contexts. It is delivered in three forms to cater to varying research needs: raw unprocessed audio; data post-Voice Activity Detection, and partially transcribed data. The dataset stands out for its legal accessibility, permitting use at no cost, thereby serving as a crucial resource for researchers, developers, and commercial entities. ivrit.ai opens up numerous applications, offering vast potential to enhance AI capabilities in Hebrew. Future efforts aim to expand ivrit.ai further, thereby advancing Hebrew's standing in AI research and technology.
</details>
<details>
<summary>摘要</summary>
我们介绍“ivrit.ai”，一个全面的希伯来语 speech dataset，填补了希伯来语自动语音识别（ASR）技术的缺乏丰富资源。该dataset包含了超过3,300小时的希伯来语 speech，来自超过1,000名多样化的说话者，覆盖了不同的场景。它提供了三种形式来满足不同的研究需求：原始的未处理音频数据，经过语音活动检测后的数据，以及部分转写的数据。该dataset的legal accessible，即可免费使用，因此成为了研究人员、开发者和商业机构的重要资源。“ivrit.ai”开启了许多应用程序，具有很大的潜力提高希伯来语AI技术。未来努力将ivrit.ai继续扩展，以提高希伯来语在AI研究和技术中的地位。
</details></li>
</ul>
<hr>
<h2 id="BASS-Block-wise-Adaptation-for-Speech-Summarization"><a href="#BASS-Block-wise-Adaptation-for-Speech-Summarization" class="headerlink" title="BASS: Block-wise Adaptation for Speech Summarization"></a>BASS: Block-wise Adaptation for Speech Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08217">http://arxiv.org/abs/2307.08217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roshan Sharma, Kenneth Zheng, Siddhant Arora, Shinji Watanabe, Rita Singh, Bhiksha Raj</li>
<li>for: 本研究旨在提高端到端speech summarization的性能，但现有模型受限于计算能力，因此通常只能使用宽度有限的输入序列进行训练。</li>
<li>methods: 本研究提出了一种逐块训练摘要模型的方法，通过分割输入序列进行批处理，以便在不同块之间传递语义上下文。</li>
<li>results: 实验结果表明，采用逐块训练方法可以提高ROUGE-L指标的表现，相比于 truncated input 基准值，提高了3个绝对点。<details>
<summary>Abstract</summary>
End-to-end speech summarization has been shown to improve performance over cascade baselines. However, such models are difficult to train on very large inputs (dozens of minutes or hours) owing to compute restrictions and are hence trained with truncated model inputs. Truncation leads to poorer models, and a solution to this problem rests in block-wise modeling, i.e., processing a portion of the input frames at a time. In this paper, we develop a method that allows one to train summarization models on very long sequences in an incremental manner. Speech summarization is realized as a streaming process, where hypothesis summaries are updated every block based on new acoustic information. We devise and test strategies to pass semantic context across the blocks. Experiments on the How2 dataset demonstrate that the proposed block-wise training method improves by 3 points absolute on ROUGE-L over a truncated input baseline.
</details>
<details>
<summary>摘要</summary>
听力摘要可以提高模型性能，但是训练在极长输入（多个分钟或小时）上受到计算限制，因此通常使用剪辑模型输入。剪辑会导致模型较差，我们的解决方案是使用块式模型，即在输入块中进行处理。在这篇论文中，我们开发了一种可以在极长序列上逐步训练摘要模型的方法。我们将听力摘要视为流动的过程，每个块基于新的听音信息更新假设摘要。我们还提出了将语义上下文传递到块中的策略。在How2 dataset上进行了实验，并证明了我们的块式训练方法可以与 truncated input 基准相比提高约3个绝对 ROUGE-L 分数。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Binary-Classification-Loss-For-Speaker-Verification"><a href="#Exploring-Binary-Classification-Loss-For-Speaker-Verification" class="headerlink" title="Exploring Binary Classification Loss For Speaker Verification"></a>Exploring Binary Classification Loss For Speaker Verification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08205">http://arxiv.org/abs/2307.08205</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hunterhuan/sphereface2_speaker_verification">https://github.com/hunterhuan/sphereface2_speaker_verification</a></li>
<li>paper_authors: Bing Han, Zhengyang Chen, Yanmin Qian</li>
<li>for: 这个论文旨在提高speaker verification任务中的表现，减少因为训练和评估频道的差异所导致的性能下降。</li>
<li>methods: 这个论文使用了多个二元分类器来训练speaker模型，而不是传统的多类别分类法，以提高表现的稳定性和精度。</li>
<li>results: 实验结果显示，SphereFace2方法可以优化speaker模型的表现，特别是在困难的实验中，并且可以与大margin fine-tuning策略相结合以获得更好的结果。此外，SphereFace2方法还表现出对于分类数据的耐读性，可以在半supervised训练 scenrio中实现更好的表现。<details>
<summary>Abstract</summary>
The mismatch between close-set training and open-set testing usually leads to significant performance degradation for speaker verification task. For existing loss functions, metric learning-based objectives depend strongly on searching effective pairs which might hinder further improvements. And popular multi-classification methods are usually observed with degradation when evaluated on unseen speakers. In this work, we introduce SphereFace2 framework which uses several binary classifiers to train the speaker model in a pair-wise manner instead of performing multi-classification. Benefiting from this learning paradigm, it can efficiently alleviate the gap between training and evaluation. Experiments conducted on Voxceleb show that the SphereFace2 outperforms other existing loss functions, especially on hard trials. Besides, large margin fine-tuning strategy is proven to be compatible with it for further improvements. Finally, SphereFace2 also shows its strong robustness to class-wise noisy labels which has the potential to be applied in the semi-supervised training scenario with inaccurate estimated pseudo labels. Codes are available in https://github.com/Hunterhuan/sphereface2_speaker_verification
</details>
<details>
<summary>摘要</summary>
通常情况下，靠近集训练和开集测试之间的差异会导致语音识别任务的性能下降。现有的损失函数和多类划分方法都会受到有效对的搜索的限制，而且在未seen speaker上测试时通常会出现下降。在这种情况下，我们引入了SphereFace2框架，它使用多个二分类器来训练说话者模型，而不是进行多类划分。这种学习模式可以有效地减少训练和评估之间的差距。在Voxceleb上进行的实验表明，SphereFace2可以比其他损失函数更高效地处理hard trial。此外，大margin精细调整策略与之相容，可以进一步提高性能。最后，SphereFace2还表明其强大的鲁棒性，可以在类别噪声标签的情况下进行 semi-supervised 训练。代码可以在https://github.com/Hunterhuan/sphereface2_speaker_verification 中找到。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/17/cs.SD_2023_07_17/" data-id="cloimipd600s0s4885bi0g7ps" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_07_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/17/eess.AS_2023_07_17/" class="article-date">
  <time datetime="2023-07-17T14:00:00.000Z" itemprop="datePublished">2023-07-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/17/eess.AS_2023_07_17/">eess.AS - 2023-07-17</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Dynamic-Kernel-Convolution-Network-with-Scene-dedicate-Training-for-Sound-Event-Localization-and-Detection"><a href="#Dynamic-Kernel-Convolution-Network-with-Scene-dedicate-Training-for-Sound-Event-Localization-and-Detection" class="headerlink" title="Dynamic Kernel Convolution Network with Scene-dedicate Training for Sound Event Localization and Detection"></a>Dynamic Kernel Convolution Network with Scene-dedicate Training for Sound Event Localization and Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08239">http://arxiv.org/abs/2307.08239</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siwei Huang, Jianfeng Chen, Jisheng Bai, Yafei Jia, Dongzhe Zhang</li>
<li>for: 这篇论文的目的是提出一种高效的声事件地理位置检测和检测系统，用于真实的空间声场。</li>
<li>methods: 该系统使用动态核心 convolution 模块来适应不同的感知范围，以及 SELDnet 和 EINv2 框架。此外，在训练阶段，还引入了两种场景专门的策略以提高系统在真实空间声场中的通用性。</li>
<li>results: 实验结果表明，提出的系统在 Sony-TAu 真实空间声场 dataset 上的表现出色，并超过了 fixes-kernel convolution SELD 系统。此外，该系统在 DCASE SELD 任务中获得了0.348的 SELD 分数，超过了 State-of-the-Art 方法。<details>
<summary>Abstract</summary>
DNN-based methods have shown high performance in sound event localization and detection(SELD). While in real spatial sound scenes, reverberation and the imbalanced presence of various sound events increase the complexity of the SELD task. In this paper, we propose an effective SELD system in real spatial scenes.In our approach, a dynamic kernel convolution module is introduced after the convolution blocks to adaptively model the channel-wise features with different receptive fields. Secondly, we incorporate the SELDnet and EINv2 framework into the proposed SELD system with multi-track ACCDOA. Moreover, two scene-dedicated strategies are introduced into the training stage to improve the generalization of the system in realistic spatial sound scenes. Finally, we apply data augmentation methods to extend the dataset using channel rotation, spatial data synthesis. Four joint metrics are used to evaluate the performance of the SELD system on the Sony-TAu Realistic Spatial Soundscapes 2022 dataset.Experimental results show that the proposed systems outperform the fixed-kernel convolution SELD systems. In addition, the proposed system achieved an SELD score of 0.348 in the DCASE SELD task and surpassed the SOTA methods.
</details>
<details>
<summary>摘要</summary>
使用 Deep Neural Network (DNN) 方法的 зву频事件localization和检测 (SELD) 表现非常高。然而，在实际空间声场中，吸收和各种声事件的不均衡存在，使得 SELD 任务变得更加复杂。在这篇论文中，我们提出了一个有效的 SELD 系统，用于实际空间声场中。我们的方法包括：1. 在卷积块后引入动态核心 convolution 模块，以自适应地处理不同感受场的通道特征。2. 将 SELDnet 和 EINv2 框架integrated 到我们的 SELD 系统中，并使用多轨迹 ACCDOA。3. 在训练阶段，引入了两种适应性战略，以提高系统在真实空间声场中的普适性。4. 使用数据扩展方法，将数据集扩展到更多的频率和空间数据。我们使用了四个联合评价指标来评价 SELD 系统在 Sony-TAu Realistic Spatial Soundscapes 2022 数据集上的性能。实验结果表明，我们的系统在固定核心 convolution SELD 系统上表现出色，并且在 DCASE SELD 任务中达到了最佳效果，超过了 State-of-the-Art 方法。
</details></li>
</ul>
<hr>
<h2 id="Adapting-Large-Language-Model-with-Speech-for-Fully-Formatted-End-to-End-Speech-Recognition"><a href="#Adapting-Large-Language-Model-with-Speech-for-Fully-Formatted-End-to-End-Speech-Recognition" class="headerlink" title="Adapting Large Language Model with Speech for Fully Formatted End-to-End Speech Recognition"></a>Adapting Large Language Model with Speech for Fully Formatted End-to-End Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08234">http://arxiv.org/abs/2307.08234</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/openai/whisper">https://github.com/openai/whisper</a></li>
<li>paper_authors: Shaoshi Ling, Yuxuan Hu, Shuangbei Qian, Guoli Ye, Yao Qian, Yifan Gong, Ed Lin, Michael Zeng</li>
<li>for: 这个论文的目的是提高端到端语音识别（E2E ASR）模型的性能。</li>
<li>methods: 这个论文使用了预训练的大型语言模型（LLMs）来改进E2E ASR模型的性能。</li>
<li>results: 该方法可以有效地利用预训练的LLMs来生成更易读的ASR转录。对于具有不同领域的完整E2E ASR转录任务，我们的模型可以超越强大的ASR模型，如Whisper，在识别错误率方面。<details>
<summary>Abstract</summary>
Most end-to-end (E2E) speech recognition models are composed of encoder and decoder blocks that perform acoustic and language modeling functions. Pretrained large language models (LLMs) have the potential to improve the performance of E2E ASR. However, integrating a pretrained language model into an E2E speech recognition model has shown limited benefits due to the mismatches between text-based LLMs and those used in E2E ASR. In this paper, we explore an alternative approach by adapting a pretrained LLMs to speech. Our experiments on fully-formatted E2E ASR transcription tasks across various domains demonstrate that our approach can effectively leverage the strengths of pretrained LLMs to produce more readable ASR transcriptions. Our model, which is based on the pretrained large language models with either an encoder-decoder or decoder-only structure, surpasses strong ASR models such as Whisper, in terms of recognition error rate, considering formats like punctuation and capitalization as well.
</details>
<details>
<summary>摘要</summary>
大多数端到端（E2E）语音识别模型由编码和解码块组成，这些块执行音频和语言模型功能。预训练大型语言模型（LLMs）有可能提高E2E ASR的性能。然而，将预训练语言模型与E2E语音识别模型结合使用显示有限的好处，这主要是因为文本基于的LLMs与E2E ASR中使用的模型之间存在差异。在这篇论文中，我们探讨一种替代方法，即适应预训练LLMs到语音。我们的实验表明，我们的方法可以有效地利用预训练LLMs的优势，生成更易读的ASR讯号。我们的模型基于预训练大型语言模型，可以是编码-解码结构或解码 только结构，在各个领域的完全格式E2E ASR转写任务中表现出色，胜过如喊叫等强大ASR模型。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/17/eess.AS_2023_07_17/" data-id="cloimipfc00ybs4886xgy0o6h" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/71/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/70/">70</a><a class="page-number" href="/page/71/">71</a><span class="page-number current">72</span><a class="page-number" href="/page/73/">73</a><a class="page-number" href="/page/74/">74</a><span class="space">&hellip;</span><a class="page-number" href="/page/84/">84</a><a class="extend next" rel="next" href="/page/73/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">116</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">56</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">112</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">62</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
