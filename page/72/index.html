
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/72/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.AI_2023_08_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/08/cs.AI_2023_08_08/" class="article-date">
  <time datetime="2023-08-08T12:00:00.000Z" itemprop="datePublished">2023-08-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/08/cs.AI_2023_08_08/">cs.AI - 2023-08-08</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-Lightweight-and-Accurate-Face-Detection-Algorithm-Based-on-Retinaface"><a href="#A-Lightweight-and-Accurate-Face-Detection-Algorithm-Based-on-Retinaface" class="headerlink" title="A Lightweight and Accurate Face Detection Algorithm Based on Retinaface"></a>A Lightweight and Accurate Face Detection Algorithm Based on Retinaface</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04340">http://arxiv.org/abs/2308.04340</a></li>
<li>repo_url: None</li>
<li>paper_authors: Baozhu Liu, Hewei Yu</li>
<li>for: 这个论文提出了一种轻量级准确的人脸检测算法LAFD（轻量级和准确的人脸检测），基于Retinaface。</li>
<li>methods: 这个算法使用了修改后的MobileNetV3网络作为后处网络，并将核心卷积的大小、通道扩展乘数和倒置径向块中的SE注意力机制调整。在Context模块中引入了弹性卷积网络(DCN)，并使用了焦点损失函数而不是交叉熵损失函数作为模型的分类损失函数。</li>
<li>results: 测试结果表明，LAFD在WIDERFACE数据集上的均值准确率为94.1%、92.2%和82.1%，与Retinaface相比提高3.4%、4.0%和8.3%，并且与轻量级模型LFFD相比提高3.1%、4.1%和4.1%。如果输入图像先进行预处理并将其横幅或长宽尺寸调整到1560px或1200px，则模型在’hard’验证子集上的均值准确率为86.2%。模型轻量级，只有10.2MB大小。<details>
<summary>Abstract</summary>
In this paper, we propose a lightweight and accurate face detection algorithm LAFD (Light and accurate face detection) based on Retinaface. Backbone network in the algorithm is a modified MobileNetV3 network which adjusts the size of the convolution kernel, the channel expansion multiplier of the inverted residuals block and the use of the SE attention mechanism. Deformable convolution network(DCN) is introduced in the context module and the algorithm uses focal loss function instead of cross-entropy loss function as the classification loss function of the model. The test results on the WIDERFACE dataset indicate that the average accuracy of LAFD is 94.1%, 92.2% and 82.1% for the "easy", "medium" and "hard" validation subsets respectively with an improvement of 3.4%, 4.0% and 8.3% compared to Retinaface and 3.1%, 4.1% and 4.1% higher than the well-performing lightweight model, LFFD. If the input image is pre-processed and scaled to 1560px in length or 1200px in width, the model achieves an average accuracy of 86.2% on the 'hard' validation subset. The model is lightweight, with a size of only 10.2MB.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种轻量级并高度准确的人脸检测算法LAFD（轻量级和准确的人脸检测），基于Retinaface。这个算法中的基础网络是一种修改后的MobileNetV3网络，通过调整卷积核的大小、扩展通道多少和使用SE注意力机制来调整。在 context 模块中，我们引入了弹性卷积网络（DCN），并使用 focal loss 函数 instead of cross-entropy loss function 作为模型的分类损失函数。在 WIDERFACE 数据集上进行测试，LAFD 的平均准确率为 94.1%、92.2% 和 82.1% ，对 Retinaface 的提高为 3.4%、4.0% 和 8.3%，而与轻量级表现良好的模型 LFFD 的提高为 3.1%、4.1% 和 4.1%。如果输入图像经过预处理并将其扩展到 1560px 长或 1200px 宽，则模型在 'hard' 验证子集上的平均准确率为 86.2%。该模型轻量级，只有 10.2MB 大小。
</details></li>
</ul>
<hr>
<h2 id="Pengembangan-Model-untuk-Mendeteksi-Kerusakan-pada-Terumbu-Karang-dengan-Klasifikasi-Citra"><a href="#Pengembangan-Model-untuk-Mendeteksi-Kerusakan-pada-Terumbu-Karang-dengan-Klasifikasi-Citra" class="headerlink" title="Pengembangan Model untuk Mendeteksi Kerusakan pada Terumbu Karang dengan Klasifikasi Citra"></a>Pengembangan Model untuk Mendeteksi Kerusakan pada Terumbu Karang dengan Klasifikasi Citra</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04337">http://arxiv.org/abs/2308.04337</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fadhil Muhammad, Alif Bintang Elfandra, Iqbal Pahlevi Amin, Alfan Farizki Wicaksono</li>
<li>For: 这个研究旨在开发一个精确的分类模型，以识别和区别健康和萎缩珊瑚的视觉特征。* Methods: 这个研究使用机器学习模型，特别是卷积神经网络（CNN），以识别和区别健康和萎缩珊瑚的视觉特征。* Results: 这个研究发现，由 scratch ResNet 模型可以在精度和准确性方面超越预训练的模型。这些精度的分类模型将有助研究人员和海洋生物学家更好地理解珊瑚礁生态环境的健康状况，并且可以用于监控珊瑚礁环境的变化，从而做出有关生态系统重建和保护的重要贡献。<details>
<summary>Abstract</summary>
The abundant biodiversity of coral reefs in Indonesian waters is a valuable asset that needs to be preserved. Rapid climate change and uncontrolled human activities have led to the degradation of coral reef ecosystems, including coral bleaching, which is a critical indicator of coral health conditions. Therefore, this research aims to develop an accurate classification model to distinguish between healthy corals and corals experiencing bleaching. This study utilizes a specialized dataset consisting of 923 images collected from Flickr using the Flickr API. The dataset comprises two distinct classes: healthy corals (438 images) and bleached corals (485 images). These images have been resized to a maximum of 300 pixels in width or height, whichever is larger, to maintain consistent sizes across the dataset.   The method employed in this research involves the use of machine learning models, particularly convolutional neural networks (CNN), to recognize and differentiate visual patterns associated with healthy and bleached corals. In this context, the dataset can be used to train and test various classification models to achieve optimal results. By leveraging the ResNet model, it was found that a from-scratch ResNet model can outperform pretrained models in terms of precision and accuracy. The success in developing accurate classification models will greatly benefit researchers and marine biologists in gaining a better understanding of coral reef health. These models can also be employed to monitor changes in the coral reef environment, thereby making a significant contribution to conservation and ecosystem restoration efforts that have far-reaching impacts on life.
</details>
<details>
<summary>摘要</summary>
INDONESIA的珊瑚礁多样性具有巨大的价值，需要保护。快速的气候变化和无控制的人类活动导致珊瑚礁生态系统的退化，包括珊瑚病症，是珊瑚健康状况的重要指标。因此，这项研究的目标是开发一个准确的分类模型，以分辨健康的珊瑚和经受病症的珊瑚。本研究使用特殊的数据集，包括Flickr API上收集的923张图片。这个数据集包括两个不同的类别：健康的珊瑚（438张图片）和病症的珊瑚（485张图片）。这些图片已经被缩放到最多300像素的宽或高，以保持数据集中图片的尺寸一致。本研究使用机器学习模型，特别是卷积神经网络（CNN），识别和区分健康和病症珊瑚的视觉特征。在这种情况下，数据集可以用来训练和测试不同的分类模型，以达到最佳结果。通过利用ResNet模型，发现从头开始的ResNet模型可以在精度和准确性方面超越预训练模型。成功地开发准确的分类模型，将对研究人员和海洋生物学家提供深刻的理解，珊瑚礁的健康状况。这些模型也可以用来监测珊瑚礁环境的变化，从而为保护和生态系统重建做出重要贡献。
</details></li>
</ul>
<hr>
<h2 id="Cooperative-Multi-agent-Bandits-Distributed-Algorithms-with-Optimal-Individual-Regret-and-Constant-Communication-Costs"><a href="#Cooperative-Multi-agent-Bandits-Distributed-Algorithms-with-Optimal-Individual-Regret-and-Constant-Communication-Costs" class="headerlink" title="Cooperative Multi-agent Bandits: Distributed Algorithms with Optimal Individual Regret and Constant Communication Costs"></a>Cooperative Multi-agent Bandits: Distributed Algorithms with Optimal Individual Regret and Constant Communication Costs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04314">http://arxiv.org/abs/2308.04314</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lin Yang, Xuchuang Wang, Mohammad Hajiesmaili, Lijun Zhang, John C. S. Lui, Don Towsley</li>
<li>for: 本文旨在研究分布式智能体在多重武器游戏中的合作游戏，以实现最佳团队和个体误差，同时减少智能体之间的通信成本。</li>
<li>methods: 本文使用分布式算法和领导者追随者模式来解决这个问题。</li>
<li>results: 本文的算法可以实现最佳个体误差和常数通信成本，并且超越了现有的分布式算法和领导者追随者模式。<details>
<summary>Abstract</summary>
Recently, there has been extensive study of cooperative multi-agent multi-armed bandits where a set of distributed agents cooperatively play the same multi-armed bandit game. The goal is to develop bandit algorithms with the optimal group and individual regrets and low communication between agents. The prior work tackled this problem using two paradigms: leader-follower and fully distributed algorithms. Prior algorithms in both paradigms achieve the optimal group regret. The leader-follower algorithms achieve constant communication costs but fail to achieve optimal individual regrets. The state-of-the-art fully distributed algorithms achieve optimal individual regrets but fail to achieve constant communication costs. This paper presents a simple yet effective communication policy and integrates it into a learning algorithm for cooperative bandits. Our algorithm achieves the best of both paradigms: optimal individual regret and constant communication costs.
</details>
<details>
<summary>摘要</summary>
近来，有广泛的研究关于协同多智能多手枪抽筋游戏，其中多个分布式代理共同参与同一个多手枪抽筋游戏。目标是开发抽筋算法，以便各个代理具有最佳小组和个人惩罚，同时减少代理之间的交流。先前的工作通过两种方法解决了这个问题：领导者-追随者和完全分布式算法。先前的领导者-追随者算法实现了常数交流成本，但失去最佳个人惩罚。现状的完全分布式算法实现了最佳个人惩罚，但失去常数交流成本。本文提出了一种简单又有效的交流策略，并将其 интегра到了一种学习算法中，以实现协同抽筋中的最佳个人惩罚和常数交流成本。
</details></li>
</ul>
<hr>
<h2 id="A-Comparative-Study-of-Code-Generation-using-ChatGPT-3-5-across-10-Programming-Languages"><a href="#A-Comparative-Study-of-Code-Generation-using-ChatGPT-3-5-across-10-Programming-Languages" class="headerlink" title="A Comparative Study of Code Generation using ChatGPT 3.5 across 10 Programming Languages"></a>A Comparative Study of Code Generation using ChatGPT 3.5 across 10 Programming Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04477">http://arxiv.org/abs/2308.04477</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abuscemi02/A-Comparative-Study-of-Code-Generation-using-ChatGPT-3.5-across-10-Programming-Languages">https://github.com/abuscemi02/A-Comparative-Study-of-Code-Generation-using-ChatGPT-3.5-across-10-Programming-Languages</a></li>
<li>paper_authors: Alessio Buscemi</li>
<li>for: 这个研究旨在评估OpenAI在11月2022年发布的ChatGPT 3.5语言模型在编程语言和软件领域中的代码创作能力。</li>
<li>methods: 该研究使用了10种编程语言和4种软件领域来评估模型的代码创作能力。</li>
<li>results: 研究发现了模型的一些意外行为和限制，以及 automatized code generation对编程语言和技术领域的演化的影响。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) are advanced Artificial Intelligence (AI) systems that have undergone extensive training using large datasets in order to understand and produce language that closely resembles that of humans. These models have reached a level of proficiency where they are capable of successfully completing university exams across several disciplines and generating functional code to handle novel problems. This research investigates the coding proficiency of ChatGPT 3.5, a LLM released by OpenAI in November 2022, which has gained significant recognition for its impressive text generating and code creation capabilities. The skill of the model in creating code snippets is evaluated across 10 various programming languages and 4 different software domains. Based on the findings derived from this research, major unexpected behaviors and limitations of the model have been identified. This study aims to identify potential areas for development and examine the ramifications of automated code generation on the evolution of programming languages and on the tech industry.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）是人工智能（AI）系统的进步，经过大量数据训练以便理解和生成语言，与人类语言更加相似。这些模型已经达到了人类水平，能够成功完成大学考试的多个领域和解决新的问题。本研究探讨了ChatGPT 3.5，一个由OpenAI在2022年11月发布的LLM，它在文本生成和代码创建方面获得了广泛的赞誉。这个模型在10种程式语言和4个软件领域中创建代码的技能被评估。根据这些研究发现的结果，模型具有一些意外的行为和限制。本研究旨在确定模型的发展前景和自动代码生成对程式语言的演化和科技业的影响。
</details></li>
</ul>
<hr>
<h2 id="Apple-Vision-Pro-for-Healthcare-“The-Ultimate-Display”-–-Entering-the-Wonderland-of-Precision"><a href="#Apple-Vision-Pro-for-Healthcare-“The-Ultimate-Display”-–-Entering-the-Wonderland-of-Precision" class="headerlink" title="Apple Vision Pro for Healthcare: “The Ultimate Display”? – Entering the Wonderland of Precision"></a>Apple Vision Pro for Healthcare: “The Ultimate Display”? – Entering the Wonderland of Precision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04313">http://arxiv.org/abs/2308.04313</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan Egger, Christina Gsaxner, Xiaojun Chen, Jiang Bian, Jens Kleesiek, Behrus Puladi</li>
<li>for: 这篇论文是关于Apple Vision Pro混合现实头戴式设备的研究，它可以作为虚拟现实（VR）设备，同时也具有增强现实（AR）功能。</li>
<li>methods: 该论文使用了内部摄像头和涂抹技术来实现头戴式设备的混合现实功能，同时还使用了一个名为”数字皇冠”的按钮来让用户轻松地融合数字内容和物理空间。</li>
<li>results: 该论文认为，Apple Vision Pro可以在医疗领域中提供更高效的辅助工具，帮助临床医生在诊断和治疗过程中占用更多时间与病人进行互动。<details>
<summary>Abstract</summary>
At the Worldwide Developers Conference (WWDC) in June 2023, Apple introduced the Vision Pro. The Vision Pro is a Mixed Reality (MR) headset, more specifically it is a Virtual Reality (VR) device with an additional Video See-Through (VST) capability. The VST capability turns the Vision Pro also into an Augmented Reality (AR) device. The AR feature is enabled by streaming the real world via cameras to the (VR) screens in front of the user's eyes. This is of course not unique and similar to other devices, like the Varjo XR-3. Nevertheless, the Vision Pro has some interesting features, like an inside-out screen that can show the headset wearers' eyes to "outsiders" or a button on the top, called "Digital Crown", that allows you to seamlessly blend digital content with your physical space by turning it. In addition, it is untethered, except for the cable to the battery, which makes the headset more agile, compared to the Varjo XR-3. This could actually come closer to the "Ultimate Display", which Ivan Sutherland had already sketched in 1965. Not available to the public yet, like the Ultimate Display, we want to take a look into the crystal ball in this perspective to see if it can overcome some clinical challenges that - especially - AR still faces in the medical domain, but also go beyond and discuss if the Vision Pro could support clinicians in essential tasks to spend more time with their patients.
</details>
<details>
<summary>摘要</summary>
在2023年6月的全球开发者大会（WWDC）上，苹果公司发布了“视野豪”（Vision Pro）混合现实（MR）头戴式设备，具体来说是虚拟现实（VR）设备具有视频增强（VST）功能。VST功能使得视野豪也成为了增强现实（AR）设备。AR功能由通过摄像头传输真实世界到用户的视网膜上的方式实现，这与其他设备类似，如Varjo XR-3。然而，视野豪有一些有趣的特点，如内置屏幕，可以在外部显示头戴式设备穿戴者的眼睛，以及位于顶部的“数字皇冠”（Digital Crown）按钮，可以轻松融合数字内容与实际空间。此外，它还不受绑定，除了电池供电的电缆，使得头戴式设备更加灵活，相比Varjo XR-3。这可能可以实现“最终显示”（Ultimate Display）， Ivan Sutherland在1965年绘制的概念。虽然不如“最终显示”一样不到公众，但我们可以通过幻灯片来看看这个头戴式设备是否可以在医疗领域超越临床挑战，同时还可以讨论这个设备是否可以支持临床专业人员在实际任务中更多时间与病人进行互动。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Goal-Based-model-for-Vehicle-Trajectory-Prediction-in-Interactive-Scenarios"><a href="#Interpretable-Goal-Based-model-for-Vehicle-Trajectory-Prediction-in-Interactive-Scenarios" class="headerlink" title="Interpretable Goal-Based model for Vehicle Trajectory Prediction in Interactive Scenarios"></a>Interpretable Goal-Based model for Vehicle Trajectory Prediction in Interactive Scenarios</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04312">http://arxiv.org/abs/2308.04312</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amina Ghoul, Itheri Yahiaoui, Anne Verroust-Blondet, Fawzi Nashashibi</li>
<li>for: 预测自动驾驶车辆的路径，提高道路安全性。</li>
<li>methods:  combinatorial discrete choice model和神经网络模型的组合，以提高预测的可解释性。</li>
<li>results: 通过使用 INTERACTION 数据集，实现并评估了我们的方案，并证明了我们的方案可以准确地预测车辆路径而不会产生可解释性的损害。<details>
<summary>Abstract</summary>
The abilities to understand the social interaction behaviors between a vehicle and its surroundings while predicting its trajectory in an urban environment are critical for road safety in autonomous driving. Social interactions are hard to explain because of their uncertainty. In recent years, neural network-based methods have been widely used for trajectory prediction and have been shown to outperform hand-crafted methods. However, these methods suffer from their lack of interpretability. In order to overcome this limitation, we combine the interpretability of a discrete choice model with the high accuracy of a neural network-based model for the task of vehicle trajectory prediction in an interactive environment. We implement and evaluate our model using the INTERACTION dataset and demonstrate the effectiveness of our proposed architecture to explain its predictions without compromising the accuracy.
</details>
<details>
<summary>摘要</summary>
autonomous driving 的道路安全受到 vehicle 与周围环境之间的社交互动行为的理解是关键。社交互动的不确定性使其很难以解释。在过去几年，基于神经网络的方法在路径预测方面得到了广泛应用，但这些方法受到了其不可解释性的限制。为了缓解这个问题，我们将精确的选择模型与高精度的神经网络模型结合，以实现在交互环境中的路径预测。我们使用 INTERACTION 数据集进行实现和评估，并证明了我们的提议的建筑可以不妨碍准确性而提供解释。
</details></li>
</ul>
<hr>
<h2 id="Vehicle-Motion-Forecasting-using-Prior-Information-and-Semantic-assisted-Occupancy-Grid-Maps"><a href="#Vehicle-Motion-Forecasting-using-Prior-Information-and-Semantic-assisted-Occupancy-Grid-Maps" class="headerlink" title="Vehicle Motion Forecasting using Prior Information and Semantic-assisted Occupancy Grid Maps"></a>Vehicle Motion Forecasting using Prior Information and Semantic-assisted Occupancy Grid Maps</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04303">http://arxiv.org/abs/2308.04303</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rabbia Asghar, Manuel Diaz-Zapata, Lukas Rummelhard, Anne Spalanzani, Christian Laugier</li>
<li>for: 本文是为了解决自动驾驶车辆中的动态预测问题，具体来说是使用卷积神经网络和概率方法预测车辆行为。</li>
<li>methods: 本文使用的方法包括将场景表示为动态占用Grid Maps（DOGMs），将占用细胞 assigning semantic标签，并使用地图信息。</li>
<li>results: 对于实际 NuScenes 数据集的测试和验证，本文的模型表现出色，能够更好地预测静止和动态车辆的行为，并且通过缺失数据集和地图信息的补做来证明模型的可靠性。<details>
<summary>Abstract</summary>
Motion prediction is a challenging task for autonomous vehicles due to uncertainty in the sensor data, the non-deterministic nature of future, and complex behavior of agents. In this paper, we tackle this problem by representing the scene as dynamic occupancy grid maps (DOGMs), associating semantic labels to the occupied cells and incorporating map information. We propose a novel framework that combines deep-learning-based spatio-temporal and probabilistic approaches to predict vehicle behaviors.Contrary to the conventional OGM prediction methods, evaluation of our work is conducted against the ground truth annotations. We experiment and validate our results on real-world NuScenes dataset and show that our model shows superior ability to predict both static and dynamic vehicles compared to OGM predictions. Furthermore, we perform an ablation study and assess the role of semantic labels and map in the architecture.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Motion prediction is a challenging task for autonomous vehicles due to uncertainty in the sensor data, the non-deterministic nature of future, and complex behavior of agents. In this paper, we tackle this problem by representing the scene as dynamic occupancy grid maps (DOGMs), associating semantic labels to the occupied cells and incorporating map information. We propose a novel framework that combines deep-learning-based spatio-temporal and probabilistic approaches to predict vehicle behaviors.Contrary to the conventional OGM prediction methods, evaluation of our work is conducted against the ground truth annotations. We experiment and validate our results on real-world NuScenes dataset and show that our model shows superior ability to predict both static and dynamic vehicles compared to OGM predictions. Furthermore, we perform an ablation study and assess the role of semantic labels and map in the architecture." into 中文（简体）Here's the translation:<<SYS>>预测行为是自动驾驶车辆的挑战之一，因为感知数据中的不确定性、未来的非束定性和智能代理人的复杂行为。在这篇论文中，我们通过将场景表示为动态占用格网图（DOGM），将占用细胞 association  semantic label，并利用地图信息来解决这个问题。我们提出了一种新的框架， combining 深度学习基于空间temporal和概率方法来预测车辆行为。与传统 OGM 预测方法不同，我们的评估采用了真实的地图注释。我们在实际的 NuScenes 数据集上进行了实验和验证，并证明了我们的模型在预测静止和动态车辆方面具有更高的能力，比传统 OGM 预测方法更好。此外，我们还进行了减少研究，以评估semantic label和地图在架构中的作用。
</details></li>
</ul>
<hr>
<h2 id="Actor-Critic-with-variable-time-discretization-via-sustained-actions"><a href="#Actor-Critic-with-variable-time-discretization-via-sustained-actions" class="headerlink" title="Actor-Critic with variable time discretization via sustained actions"></a>Actor-Critic with variable time discretization via sustained actions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04299">http://arxiv.org/abs/2308.04299</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jakub Łyskawa, Paweł Wawrzyński</li>
<li>for: 本研究使用强化学习方法解决维度缺失问题，并研究不同时间粒度设置对控制器性能的影响。</li>
<li>methods: 本文提出了一种名为SusACER的离政策强化学习算法，该算法在不同时间粒度设置下进行学习，并且可以在粗粒度和细粒度之间切换。</li>
<li>results: 在Ant、HalfCheetah、Hopper和Walker2D等四个机器人控制环境中，SusACER算法都能够超越当前最佳算法。<details>
<summary>Abstract</summary>
Reinforcement learning (RL) methods work in discrete time. In order to apply RL to inherently continuous problems like robotic control, a specific time discretization needs to be defined. This is a choice between sparse time control, which may be easier to train, and finer time control, which may allow for better ultimate performance. In this work, we propose SusACER, an off-policy RL algorithm that combines the advantages of different time discretization settings. Initially, it operates with sparse time discretization and gradually switches to a fine one. We analyze the effects of the changing time discretization in robotic control environments: Ant, HalfCheetah, Hopper, and Walker2D. In all cases our proposed algorithm outperforms state of the art.
</details>
<details>
<summary>摘要</summary>
重复学习（RL）方法在离散时间下运行。为了将RL应用于基于连续时间的问题，例如机器人控制，需要定义特定的时间离散设定。这是一个选择 между稀疏时间控制和精细时间控制的选择。在这种工作中，我们提出了 SusACER，一种离散RL算法，将不同时间离散设定的优点相互结合。首先，它使用稀疏时间离散，然后慢慢地转换到精细时间离散。我们对机器人控制环境中的Ant、半驰虎、跳跃机和 Walker2D进行分析，在所有情况下，我们的提议算法超越了现有的state of the art。
</details></li>
</ul>
<hr>
<h2 id="Engineering-LaCAM-ast-Towards-Real-Time-Large-Scale-and-Near-Optimal-Multi-Agent-Pathfinding"><a href="#Engineering-LaCAM-ast-Towards-Real-Time-Large-Scale-and-Near-Optimal-Multi-Agent-Pathfinding" class="headerlink" title="Engineering LaCAM$^\ast$: Towards Real-Time, Large-Scale, and Near-Optimal Multi-Agent Pathfinding"></a>Engineering LaCAM$^\ast$: Towards Real-Time, Large-Scale, and Near-Optimal Multi-Agent Pathfinding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04292">http://arxiv.org/abs/2308.04292</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keisuke Okumura</li>
<li>for: 本研究旨在解决实时、大规模、近似优质多智能路径找索（MAPF）问题，通过增强最近提出的LaCAM*算法进行改进。</li>
<li>methods: 本研究使用了各种改进技术，部分启发自其他MAPF方法，以提高LaCAM*算法的初始解质量和融合速度。</li>
<li>results: 经验证明，将这些改进技术融合到LaCAM*算法中，可以明显提高解质量，从而进一步推进MAPF算法的边缘。<details>
<summary>Abstract</summary>
This paper addresses the challenges of real-time, large-scale, and near-optimal multi-agent pathfinding (MAPF) through enhancements to the recently proposed LaCAM* algorithm. LaCAM* is a scalable search-based algorithm that guarantees the eventual finding of optimal solutions for cumulative transition costs. While it has demonstrated remarkable planning success rates, surpassing various state-of-the-art MAPF methods, its initial solution quality is far from optimal, and its convergence speed to the optimum is slow. To overcome these limitations, this paper introduces several improvement techniques, partly drawing inspiration from other MAPF methods. We provide empirical evidence that the fusion of these techniques significantly improves the solution quality of LaCAM*, thus further pushing the boundaries of MAPF algorithms.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:这篇论文解决了实时、大规模、近似优质多代理路径寻找（MAPF）的挑战，通过对最近提出的LaCAM*算法进行增强。LaCAM*是一种可扩展的搜索基本算法，可以 garantue the eventual finding of optimal solutions for cumulative transition costs。虽然它已经达到了多种状态前的寻找成功率，但其初始解质不佳，并且与优质相对较慢。为了超越这些限制，这篇论文提出了多种改进技术，部分 Draw inspiration from other MAPF methods。我们提供了实证证据，表明这些技术的融合可以 Significantly improve LaCAM*的解质，进一步推动MAPF算法的发展。
</details></li>
</ul>
<hr>
<h2 id="In-Context-Alignment-Chat-with-Vanilla-Language-Models-Before-Fine-Tuning"><a href="#In-Context-Alignment-Chat-with-Vanilla-Language-Models-Before-Fine-Tuning" class="headerlink" title="In-Context Alignment: Chat with Vanilla Language Models Before Fine-Tuning"></a>In-Context Alignment: Chat with Vanilla Language Models Before Fine-Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04275">http://arxiv.org/abs/2308.04275</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xhan77/in-context-alignment">https://github.com/xhan77/in-context-alignment</a></li>
<li>paper_authors: Xiaochuang Han</li>
<li>for: 这 paper 的目的是探讨在下文学习中进行归一化的可能性。</li>
<li>methods: 这 paper 使用了一个 vanilla 预训练语言模型 Llama-2，并通过在下文中学习来实现归一化。</li>
<li>results:  compared to 直接提示，在下context中进行归一化无需更改模型参数，可以提高 win-rate 7 倍，使 vanilla 语言模型与对齐 fine-tuning 的强基线模型相当。<details>
<summary>Abstract</summary>
In this note, we explore inference-time alignment through in-context learning. We consider a vanilla pretrained language model Llama-2 before any fine-tuning and retrieve an average of 9 demonstration alignment examples when the model is prompted to follow chat-style instructions. Compared to direct prompting, the in-context alignment without changing model weights leads to a 7x increase in win-rate w.r.t. the text-davinci-003 model from OpenAI, making the vanilla language model comparable to strong baselines with alignment fine-tuning.
</details>
<details>
<summary>摘要</summary>
在这份说明中，我们探索了在语言模型学习中的推理时对适应。我们考虑了一个未经任何微调的语言模型Llama-2，并从其在语音指令下检索了9个示例匹配例子。相比直接提示，无需更改模型参数的受上下文匹配导致了与文本-达维纳-003模型从OpenAI的7倍增加胜率，使原始语言模型与对适应微调相当。
</details></li>
</ul>
<hr>
<h2 id="Lossy-and-Lossless-L-2-Post-training-Model-Size-Compression"><a href="#Lossy-and-Lossless-L-2-Post-training-Model-Size-Compression" class="headerlink" title="Lossy and Lossless (L$^2$) Post-training Model Size Compression"></a>Lossy and Lossless (L$^2$) Post-training Model Size Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04269">http://arxiv.org/abs/2308.04269</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/modeltc/l2_compression">https://github.com/modeltc/l2_compression</a></li>
<li>paper_authors: Yumeng Shi, Shihao Bai, Xiuying Wei, Ruihao Gong, Jianlei Yang</li>
<li>for: 这个研究旨在提高深度神经网络的传输和储存方便性，透过结合lossy和lossless压缩方法。</li>
<li>methods: 本研究提出了一个后训练模型大小压缩方法，使用了一个统一的参数重复转换，以便在后训练过程中进行多种lossy压缩方法。此外，我们还引入了一个特殊的可微分Counter来帮助优化lossy压缩，以取得更适合的压缩点。</li>
<li>results: 本研究可以实现稳定的$10\times$压缩比率无损准确性，并且可以在短时间内取得$20\times$压缩比率对应轻微损失。代码可以在<a target="_blank" rel="noopener" href="https://github.com/ModelTC/L2_Compression%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ModelTC/L2_Compression上获取。</a><details>
<summary>Abstract</summary>
Deep neural networks have delivered remarkable performance and have been widely used in various visual tasks. However, their huge size causes significant inconvenience for transmission and storage. Many previous studies have explored model size compression. However, these studies often approach various lossy and lossless compression methods in isolation, leading to challenges in achieving high compression ratios efficiently. This work proposes a post-training model size compression method that combines lossy and lossless compression in a unified way. We first propose a unified parametric weight transformation, which ensures different lossy compression methods can be performed jointly in a post-training manner. Then, a dedicated differentiable counter is introduced to guide the optimization of lossy compression to arrive at a more suitable point for later lossless compression. Additionally, our method can easily control a desired global compression ratio and allocate adaptive ratios for different layers. Finally, our method can achieve a stable $10\times$ compression ratio without sacrificing accuracy and a $20\times$ compression ratio with minor accuracy loss in a short time. Our code is available at https://github.com/ModelTC/L2_Compression .
</details>
<details>
<summary>摘要</summary>
深度神经网络已经提供了很好的性能，并在各种视觉任务中广泛使用。然而，它们的巨大大小带来了传输和存储的不便。许多前面的研究已经探讨过模型大小压缩。然而，这些研究通常是采用各种损失压缩和无损压缩方法，导致高效压缩率很困难。本工作提出了一种后处理模型大小压缩方法，它可以同时使用损失压缩和无损压缩。我们首先提出了一种统一的参数重要性变换，使得不同的损失压缩方法可以在后处理中进行 JOINT 处理。然后，我们引入了特有的可微分Counter，以便通过优化损失压缩来到达更适合的点，以便 later 无损压缩。此外，我们的方法可以轻松地控制desired的全局压缩比，并分配适应的层级压缩率。最后，我们的方法可以实现稳定的 $10\times$ 压缩比，无损减少精度，以及 $20\times$ 压缩比，只有微量损失精度。我们的代码可以在 https://github.com/ModelTC/L2_Compression 上找到。
</details></li>
</ul>
<hr>
<h2 id="Teacher-Student-Architecture-for-Knowledge-Distillation-A-Survey"><a href="#Teacher-Student-Architecture-for-Knowledge-Distillation-A-Survey" class="headerlink" title="Teacher-Student Architecture for Knowledge Distillation: A Survey"></a>Teacher-Student Architecture for Knowledge Distillation: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04268">http://arxiv.org/abs/2308.04268</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengming Hu, Xuan Li, Dan Liu, Haolun Wu, Xi Chen, Ju Wang, Xue Liu</li>
<li>for: 本研究主要是为了解决深度神经网络（DNNs）在实际应用中的问题，即DNNs的参数量过多。</li>
<li>methods: 本研究使用了教师-学生架构，其中简单的学生网络只需要一些参数就可以达到与深度教师网络相同的性能。</li>
<li>results: 本研究通过多种知识压缩、扩展、适应和加强目标，成功地实现了多种知识压缩目标。<details>
<summary>Abstract</summary>
Although Deep neural networks (DNNs) have shown a strong capacity to solve large-scale problems in many areas, such DNNs are hard to be deployed in real-world systems due to their voluminous parameters. To tackle this issue, Teacher-Student architectures were proposed, where simple student networks with a few parameters can achieve comparable performance to deep teacher networks with many parameters. Recently, Teacher-Student architectures have been effectively and widely embraced on various knowledge distillation (KD) objectives, including knowledge compression, knowledge expansion, knowledge adaptation, and knowledge enhancement. With the help of Teacher-Student architectures, current studies are able to achieve multiple distillation objectives through lightweight and generalized student networks. Different from existing KD surveys that primarily focus on knowledge compression, this survey first explores Teacher-Student architectures across multiple distillation objectives. This survey presents an introduction to various knowledge representations and their corresponding optimization objectives. Additionally, we provide a systematic overview of Teacher-Student architectures with representative learning algorithms and effective distillation schemes. This survey also summarizes recent applications of Teacher-Student architectures across multiple purposes, including classification, recognition, generation, ranking, and regression. Lastly, potential research directions in KD are investigated, focusing on architecture design, knowledge quality, and theoretical studies of regression-based learning, respectively. Through this comprehensive survey, industry practitioners and the academic community can gain valuable insights and guidelines for effectively designing, learning, and applying Teacher-Student architectures on various distillation objectives.
</details>
<details>
<summary>摘要</summary>
although deep neural networks (DNNs) have shown strong capacity to solve large-scale problems in many areas, such DNNs are difficult to deploy in real-world systems due to their numerous parameters. To address this issue, Teacher-Student architectures were proposed, where simple student networks with a few parameters can achieve comparable performance to deep teacher networks with many parameters. recently, Teacher-Student architectures have been effectively and widely embraced on various knowledge distillation (KD) objectives, including knowledge compression, knowledge expansion, knowledge adaptation, and knowledge enhancement. With the help of Teacher-Student architectures, current studies are able to achieve multiple distillation objectives through lightweight and generalized student networks. different from existing KD surveys that primarily focus on knowledge compression, this survey first explores Teacher-Student architectures across multiple distillation objectives. this survey presents an introduction to various knowledge representations and their corresponding optimization objectives. additionally, we provide a systematic overview of Teacher-Student architectures with representative learning algorithms and effective distillation schemes. this survey also summarizes recent applications of Teacher-Student architectures across multiple purposes, including classification, recognition, generation, ranking, and regression. finally, potential research directions in KD are investigated, focusing on architecture design, knowledge quality, and theoretical studies of regression-based learning, respectively. through this comprehensive survey, industry practitioners and the academic community can gain valuable insights and guidelines for effectively designing, learning, and applying Teacher-Student architectures on various distillation objectives.
</details></li>
</ul>
<hr>
<h2 id="FLIRT-Feedback-Loop-In-context-Red-Teaming"><a href="#FLIRT-Feedback-Loop-In-context-Red-Teaming" class="headerlink" title="FLIRT: Feedback Loop In-context Red Teaming"></a>FLIRT: Feedback Loop In-context Red Teaming</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04265">http://arxiv.org/abs/2308.04265</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ninareh Mehrabi, Palash Goyal, Christophe Dupuy, Qian Hu, Shalini Ghosh, Richard Zemel, Kai-Wei Chang, Aram Galstyan, Rahul Gupta</li>
<li>for: 这篇论文旨在测试和分析 générative 模型中的漏洞，以便提高模型的安全性和适用性。</li>
<li>methods: 这篇论文提出了一个自动化红队框架，用于评估给定的模型，并暴露其具有不安全和不适当内容生成的漏洞。该框架使用了受Context学习的反馈循环，以便对模型进行自动化红队。</li>
<li>results: 对比基eline方法，这种提出的策略更有效地暴露了Stable Diffusion（SD）模型中的漏洞，即使SD模型具有安全特性。此外，这种框架还能够对文本到文本模型进行红队， resulting in significantly higher toxic response generation rate compared to previously reported numbers.<details>
<summary>Abstract</summary>
Warning: this paper contains content that may be inappropriate or offensive.   As generative models become available for public use in various applications, testing and analyzing vulnerabilities of these models has become a priority. Here we propose an automatic red teaming framework that evaluates a given model and exposes its vulnerabilities against unsafe and inappropriate content generation. Our framework uses in-context learning in a feedback loop to red team models and trigger them into unsafe content generation. We propose different in-context attack strategies to automatically learn effective and diverse adversarial prompts for text-to-image models. Our experiments demonstrate that compared to baseline approaches, our proposed strategy is significantly more effective in exposing vulnerabilities in Stable Diffusion (SD) model, even when the latter is enhanced with safety features. Furthermore, we demonstrate that the proposed framework is effective for red teaming text-to-text models, resulting in significantly higher toxic response generation rate compared to previously reported numbers.
</details>
<details>
<summary>摘要</summary>
警告：这篇论文可能包含不适或不宜的内容。 随着生成模型在不同应用中变得更加普遍使用，测试和分析这些模型的漏洞已成为一个优先事项。 在这篇论文中，我们提出一种自动红团框架，用于评估给定模型的漏洞，并让模型生成不安全或不适的内容。 我们的框架使用受 Context 学习的反馈循环，以红团模型并让它生成不安全内容。 我们提出了不同的 Context 攻击策略，以自动学习有效和多样的对抗示例 для文本到图像模型。 我们的实验表明，相比基eline方法，我们提出的策略在Stable Diffusion（SD）模型上更加有效，即使后者具有安全功能。 此外，我们的框架还对文本到文本模型进行了红团，并得到了远远高于之前报道的恶意回应率。
</details></li>
</ul>
<hr>
<h2 id="PokerKit-A-Comprehensive-Python-Library-for-Fine-Grained-Multi-Variant-Poker-Game-Simulations"><a href="#PokerKit-A-Comprehensive-Python-Library-for-Fine-Grained-Multi-Variant-Poker-Game-Simulations" class="headerlink" title="PokerKit: A Comprehensive Python Library for Fine-Grained Multi-Variant Poker Game Simulations"></a>PokerKit: A Comprehensive Python Library for Fine-Grained Multi-Variant Poker Game Simulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07327">http://arxiv.org/abs/2308.07327</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juho Kim</li>
<li>for: 这篇论文是用于描述一个开源的Python库PokerKit，该库用于扩展现有的 póker游戏模拟和手牌评估工具的功能，支持更多的 póker变种和自定义游戏。</li>
<li>methods: 这篇论文详细介绍了PokerKit的设计和实现，包括它的直观的编程API，多种变种游戏支持，以及不同手牌类型的手牌评估 suite。</li>
<li>results: PokerKit的可靠性已经通过静态类型检查、广泛的doctests和单元测试确认，实现了97%的代码覆盖率。PokerKit的出现对计算机 póker领域做出了重要贡献，推动未来的研究和高级AI开发，用于多种 póker游戏。<details>
<summary>Abstract</summary>
PokerKit is an open-source Python library designed to overcome the restrictions of existing poker game simulation and hand evaluation tools, which typically support only a handful of poker variants and lack flexibility in game state control. In contrast, PokerKit significantly expands this scope by supporting an extensive array of poker variants and it provides a flexible architecture for users to define their custom games. This paper details the design and implementation of PokerKit, including its intuitive programmatic API, multi-variant game support, and a unified hand evaluation suite across different hand types. The flexibility of PokerKit allows for applications in diverse areas, such as poker AI development, tool creation, and online poker casino implementation. PokerKit's reliability has been established through static type checking, extensive doctests, and unit tests, achieving 97\% code coverage. The introduction of PokerKit represents a significant contribution to the field of computer poker, fostering future research and advanced AI development for a wide variety of poker games.
</details>
<details>
<summary>摘要</summary>
pokerKit 是一个开源的 Python 库，旨在超越现有的 póker 游戏模拟和手牌评估工具，这些工具通常只支持几种 póker 变种并缺乏游戏状态控制的灵活性。相比之下，pokerKit 对此进行了广泛的扩展，支持了大量的 póker 变种，并提供了用户定义的自定义游戏功能。这篇论文介绍了 pokerKit 的设计和实现，包括它的直观的编程 API，多种变种游戏支持，以及不同手牌类型的统一手牌评估 suite。pokerKit 的灵活性允许其在多种领域应用，如 póker AI 研发、工具创造和在线 póker 赌场实现。pokerKit 的可靠性已经通过静态类型检查、extensive doctests 和单元测试达到 97% 代码覆盖率。pokerKit 的出现对计算机 póker 领域做出了重要贡献，激发未来的研究和高级 AI 开发，涵盖各种 póker 游戏。
</details></li>
</ul>
<hr>
<h2 id="MindDiffuser-Controlled-Image-Reconstruction-from-Human-Brain-Activity-with-Semantic-and-Structural-Diffusion"><a href="#MindDiffuser-Controlled-Image-Reconstruction-from-Human-Brain-Activity-with-Semantic-and-Structural-Diffusion" class="headerlink" title="MindDiffuser: Controlled Image Reconstruction from Human Brain Activity with Semantic and Structural Diffusion"></a>MindDiffuser: Controlled Image Reconstruction from Human Brain Activity with Semantic and Structural Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04249">http://arxiv.org/abs/2308.04249</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/reedonepeck/minddiffuser">https://github.com/reedonepeck/minddiffuser</a></li>
<li>paper_authors: Yizhuo Lu, Changde Du, Qiongyi zhou, Dianpeng Wang, Huiguang He</li>
<li>for: 这个论文的目的是提出一种两阶段图像重建模型，以解决脑计算机交互界面中图像重建精度和控制性的挑战。</li>
<li>methods: 该模型使用了VQ-VAE卷积Autoencoder和CLIP文本嵌入，并通过稳定扩散来初步重建图像，然后通过反射学习来调整图像的结构信息。</li>
<li>results: 该模型在Natural Scenes Dataset（NSD）上表现出了现在领先的性能，并且经过了许多质量和质量分析，得出了可读性的多模式特征，与脑响应的对应性得到了证明。<details>
<summary>Abstract</summary>
Reconstructing visual stimuli from brain recordings has been a meaningful and challenging task. Especially, the achievement of precise and controllable image reconstruction bears great significance in propelling the progress and utilization of brain-computer interfaces. Despite the advancements in complex image reconstruction techniques, the challenge persists in achieving a cohesive alignment of both semantic (concepts and objects) and structure (position, orientation, and size) with the image stimuli. To address the aforementioned issue, we propose a two-stage image reconstruction model called MindDiffuser. In Stage 1, the VQ-VAE latent representations and the CLIP text embeddings decoded from fMRI are put into Stable Diffusion, which yields a preliminary image that contains semantic information. In Stage 2, we utilize the CLIP visual feature decoded from fMRI as supervisory information, and continually adjust the two feature vectors decoded in Stage 1 through backpropagation to align the structural information. The results of both qualitative and quantitative analyses demonstrate that our model has surpassed the current state-of-the-art models on Natural Scenes Dataset (NSD). The subsequent experimental findings corroborate the neurobiological plausibility of the model, as evidenced by the interpretability of the multimodal feature employed, which align with the corresponding brain responses.
</details>
<details>
<summary>摘要</summary>
<<SYS>>重构脑记录中的视觉刺激是一项有意义且挑战性的任务。特别是在实现精确和可控的图像重建方面，这种任务具有推动脑机器交互的进步和应用的重要性。尽管复杂图像重建技术得到了进步，但是在图像刺激中协调semantic（概念和物体）和structure（位置、方向、大小）仍然是一项挑战。为了解决这个问题，我们提出了一种两个阶段的图像重建模型，称为 MindDiffuser。在第一阶段，使用VQ-VAE隐藏表示和CLIP文本嵌入从fMRI中解码的，并将其置入稳定扩散，从而得到包含semantic信息的初步图像。在第二阶段，我们利用CLIP视觉特征从fMRI中解码的，作为监督信息，通过反射来调整在第一阶段解码的两个特征向量，以实现结构信息的协调。实验结果表明，我们的模型在Natural Scenes Dataset（NSD）上超过了当前状态的艺术模型。后续的实验发现，证明了我们的模型在脑响应的可靠性方面具有神经生物学可能性，其中multimodal特征的可读性与脑响应的对应。
</details></li>
</ul>
<hr>
<h2 id="Gloss-Alignment-Using-Word-Embeddings"><a href="#Gloss-Alignment-Using-Word-Embeddings" class="headerlink" title="Gloss Alignment Using Word Embeddings"></a>Gloss Alignment Using Word Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04248">http://arxiv.org/abs/2308.04248</a></li>
<li>repo_url: None</li>
<li>paper_authors: Harry Walsh, Ozge Mercanoglu Sincan, Ben Saunders, Richard Bowden</li>
<li>for: 本研究旨在提高听语字幕和签语对应的精度，以便更好地训练无约束听语转文本模型。</li>
<li>methods: 本研究使用大量的说语言模型来对签语检测点进行对应。这种方法可以与现有的对应技术结合使用，从而降低计算成本。</li>
<li>results: 我们在\acf{mdgs}和\acf{bobsl} dataset上Quantitatively证明了我们的方法的效果，可以达到33.22 BLEU-1分数的word对应精度。<details>
<summary>Abstract</summary>
Capturing and annotating Sign language datasets is a time consuming and costly process. Current datasets are orders of magnitude too small to successfully train unconstrained \acf{slt} models. As a result, research has turned to TV broadcast content as a source of large-scale training data, consisting of both the sign language interpreter and the associated audio subtitle. However, lack of sign language annotation limits the usability of this data and has led to the development of automatic annotation techniques such as sign spotting. These spottings are aligned to the video rather than the subtitle, which often results in a misalignment between the subtitle and spotted signs. In this paper we propose a method for aligning spottings with their corresponding subtitles using large spoken language models. Using a single modality means our method is computationally inexpensive and can be utilized in conjunction with existing alignment techniques. We quantitatively demonstrate the effectiveness of our method on the \acf{mdgs} and \acf{bobsl} datasets, recovering up to a 33.22 BLEU-1 score in word alignment.
</details>
<details>
<summary>摘要</summary>
捕捉和标注手语数据集是一个时间consuming和成本高的过程。现有的数据集规模几个数量级小于需要成功训练无约制手语识别模型。因此，研究人员将视频广播内容作为大规模训练数据，包括手语 interprete 和关联的音频字幕。然而，手语注释缺失限制了这些数据的可用性，导致了自动注释技术的开发，如手语搜索。这些搜索被视频而不是字幕进行对齐，经常导致字幕和搜索到的手语之间的不一致。在这篇论文中，我们提议一种方法用于将搜索与其对应的字幕进行对齐，使用大量的人语言模型。由于我们只使用一种模式，我们的方法是计算机不昂贵的，可以与现有的对齐方法结合使用。我们量化地示示了我们的方法在\acf{mdgs}和\acf{bobsl}数据集上的效果，recovering up to 33.22 BLEU-1 分数。
</details></li>
</ul>
<hr>
<h2 id="AutoPCF-Efficient-Product-Carbon-Footprint-Accounting-with-Large-Language-Models"><a href="#AutoPCF-Efficient-Product-Carbon-Footprint-Accounting-with-Large-Language-Models" class="headerlink" title="AutoPCF: Efficient Product Carbon Footprint Accounting with Large Language Models"></a>AutoPCF: Efficient Product Carbon Footprint Accounting with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04241">http://arxiv.org/abs/2308.04241</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhu Deng, Jinjie Liu, Biao Luo, Can Yuan, Qingrun Yang, Lei Xiao, Wenwen Zhou, Zhu Liu</li>
<li>for: 这个研究旨在发展一个自动化的碳脚印检测框架，以便快速和自动地计算产品生命周期中的碳脚印。</li>
<li>methods: 本研究使用了五个大语言模型（LLMs）来测试和比较生命周期模型的emergent能力，并将这些模型应用于自动生成产品生命周期的数据库。另外，这个框架还使用了深度学习算法来自动匹配计算参数，以便快速计算产品的碳脚印。</li>
<li>results: 使用AutoPCF框架估计三个案例产品的碳脚印，结果显示AutoPCF框架具有快速计算碳脚印的能力，比传统方法快得多少倍。<details>
<summary>Abstract</summary>
The product carbon footprint (PCF) is crucial for decarbonizing the supply chain, as it measures the direct and indirect greenhouse gas emissions caused by all activities during the product's life cycle. However, PCF accounting often requires expert knowledge and significant time to construct life cycle models. In this study, we test and compare the emergent ability of five large language models (LLMs) in modeling the 'cradle-to-gate' life cycles of products and generating the inventory data of inputs and outputs, revealing their limitations as a generalized PCF knowledge database. By utilizing LLMs, we propose an automatic AI-driven PCF accounting framework, called AutoPCF, which also applies deep learning algorithms to automatically match calculation parameters, and ultimately calculate the PCF. The results of estimating the carbon footprint for three case products using the AutoPCF framework demonstrate its potential in achieving automatic modeling and estimation of PCF with a large reduction in modeling time from days to minutes.
</details>
<details>
<summary>摘要</summary>
产品碳脚印（PCF）对于减少供应链的碳排放非常重要，因为它测量产品生命周期中直接和间接气候变化所导致的绿house gas排放。然而，PCF会计通常需要专业知识和大量时间建立生命周期模型。在这项研究中，我们测试和比较五种大型自然语言模型（LLM）在产品“营养径”生命周期的模型和生成输入输出inv质数据方面的能力，揭示它们的局限性作为总体PCF知识库。通过使用LLM，我们提议一种自动驱动的PCF会计框架，称为AutoPCF，该框架还应用深度学习算法来自动匹配计算参数，最终计算PCF。三个案例 продукт的碳脚印估计结果表明AutoPCF框架在自动模型和估计PCF方面具有很大的潜力，从天天减少到分钟内。
</details></li>
</ul>
<hr>
<h2 id="Federated-Inference-with-Reliable-Uncertainty-Quantification-over-Wireless-Channels-via-Conformal-Prediction"><a href="#Federated-Inference-with-Reliable-Uncertainty-Quantification-over-Wireless-Channels-via-Conformal-Prediction" class="headerlink" title="Federated Inference with Reliable Uncertainty Quantification over Wireless Channels via Conformal Prediction"></a>Federated Inference with Reliable Uncertainty Quantification over Wireless Channels via Conformal Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04237">http://arxiv.org/abs/2308.04237</a></li>
<li>repo_url: None</li>
<li>paper_authors: Meiyi Zhu, Matteo Zecchin, Sangwoo Park, Caili Guo, Chunyan Feng, Osvaldo Simeone</li>
<li>for: 这个论文旨在研究在分布式计算环境下，通过设备到服务器的通信，提高服务器的推断准确性。</li>
<li>methods: 该论文提出了一种名为分布式准确预测（Federated Conformal Prediction，简称WFCP）的协议，它基于类型基本多访问（Type-Based Multiple Access，简称TBMA）和一种新的量词 corrections 策略。WFCP 提供了正式的可靠性保证，包括服务器预测集的覆盖率。</li>
<li>results: 该论文通过数值结果显示，WFCP 在有限通信资源和&#x2F;或大量设备情况下具有显著优势，特别是与已有的 federated CP 方案进行数字实现的比较。<details>
<summary>Abstract</summary>
Consider a setting in which devices and a server share a pre-trained model. The server wishes to make an inference on a new input given the model. Devices have access to data, previously not used for training, and can communicate to the server over a common wireless channel. If the devices have no access to the new input, can communication from devices to the server enhance the quality of the inference decision at the server? Recent work has introduced federated conformal prediction (CP), which leverages devices-to-server communication to improve the reliability of the server's decision. With federated CP, devices communicate to the server information about the loss accrued by the shared pre-trained model on the local data, and the server leverages this information to calibrate a decision interval, or set, so that it is guaranteed to contain the correct answer with a pre-defined target reliability level. Previous work assumed noise-free communication, whereby devices can communicate a single real number to the server. In this paper, we study for the first time federated CP in a wireless setting. We introduce a novel protocol, termed wireless federated conformal prediction (WFCP), which builds on type-based multiple access (TBMA) and on a novel quantile correction strategy. WFCP is proved to provide formal reliability guarantees in terms of coverage of the predicted set produced by the server. Using numerical results, we demonstrate the significant advantages of WFCP against digital implementations of existing federated CP schemes, especially in regimes with limited communication resources and/or large number of devices.
</details>
<details>
<summary>摘要</summary>
假设设备和服务器共享预训练模型。服务器想要对新输入进行推断。设备可以访问未使用过训练的数据，并可以通过共享的无线通信chnnel与服务器进行通信。如果设备没有访问新输入，可以通过设备到服务器的通信来提高服务器的推断决策质量吗？ latest work introduced federated conformal prediction (CP), which leverages devices-to-server communication to improve the reliability of the server's decision. With federated CP, devices communicate to the server information about the loss accrued by the shared pre-trained model on the local data, and the server leverages this information to calibrate a decision interval, or set, so that it is guaranteed to contain the correct answer with a pre-defined target reliability level. Previous work assumed noise-free communication, whereby devices can communicate a single real number to the server. In this paper, we study for the first time federated CP in a wireless setting. We introduce a novel protocol, termed wireless federated conformal prediction (WFCP), which builds on type-based multiple access (TBMA) and on a novel quantile correction strategy. WFCP is proved to provide formal reliability guarantees in terms of coverage of the predicted set produced by the server. Using numerical results, we demonstrate the significant advantages of WFCP against digital implementations of existing federated CP schemes, especially in regimes with limited communication resources and/or large number of devices.
</details></li>
</ul>
<hr>
<h2 id="Semantic-Interpretation-and-Validation-of-Graph-Attention-based-Explanations-for-GNN-Models"><a href="#Semantic-Interpretation-and-Validation-of-Graph-Attention-based-Explanations-for-GNN-Models" class="headerlink" title="Semantic Interpretation and Validation of Graph Attention-based Explanations for GNN Models"></a>Semantic Interpretation and Validation of Graph Attention-based Explanations for GNN Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04220">http://arxiv.org/abs/2308.04220</a></li>
<li>repo_url: None</li>
<li>paper_authors: Efimia Panagiotaki, Daniele De Martini, Lars Kunze</li>
<li>for: 这种方法用于提高图神经网络（GNN）模型的解释性，通过使用semantic attention来增强图结构中的特征重要性的描述。</li>
<li>methods: 该方法利用semantic attention mechanism来提供基于特征重要性的解释，并通过对模型精度和特征重要性之间的相关性进行分析，从而获得有价值的特征重要性信息。</li>
<li>results: 通过应用该方法于一个遥感点云估计模型，成功地 indentify了提高性的semantic类别，并生成了可靠的后续semantic解释。<details>
<summary>Abstract</summary>
In this work, we propose a methodology for investigating the application of semantic attention to enhance the explainability of Graph Neural Network (GNN)-based models, introducing semantically-informed perturbations and establishing a correlation between predicted feature-importance weights and model accuracy. Graph Deep Learning (GDL) has emerged as a promising field for tasks like scene interpretation, leveraging flexible graph structures to concisely describe complex features and relationships. As traditional explainability methods used in eXplainable AI (XAI) cannot be directly applied to such structures, graph-specific approaches are introduced. Attention mechanisms have demonstrated their efficacy in estimating the importance of input features in deep learning models and thus have been previously employed to provide feature-based explanations for GNN predictions. Building upon these insights, we extend existing attention-based graph-explainability methods investigating the use of attention weights as importance indicators of semantically sorted feature sets. Through analysing the behaviour of predicted attention-weights distribution in correlation with model accuracy, we gain valuable insights into feature importance with respect to the behaviour of the GNN model. We apply our methodology to a lidar pointcloud estimation model successfully identifying key semantic classes that contribute to enhanced performance effectively generating reliable post-hoc semantic explanations.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们提出了一种方法来增强Graph Neural Network（GNN）模型的解释性，通过引入semantically-informed perturbations和建立 predicted feature-importance weights与模型准确率之间的相关性。Graph Deep Learning（GDL）已经成为一个有前途的领域，用于场景理解等任务，利用灵活的图结构 concisely describe complex features和关系。traditional explainability methods在XAI中不能直接应用于such structures，因此introduce graph-specific approaches。Attention mechanisms have demonstrated their efficacy in estimating the importance of input features in deep learning models, and thus have been previously employed to provide feature-based explanations for GNN predictions. Building upon these insights, we extend existing attention-based graph-explainability methods by investigating the use of attention weights as importance indicators of semantically sorted feature sets. Through analyzing the behavior of predicted attention-weights distribution in correlation with model accuracy, we gain valuable insights into feature importance with respect to the behavior of the GNN model. We apply our methodology to a lidar pointcloud estimation model and successfully identify key semantic classes that contribute to enhanced performance, effectively generating reliable post-hoc semantic explanations.
</details></li>
</ul>
<hr>
<h2 id="Hybrid-Retrieval-Augmented-Generation-for-Real-time-Composition-Assistance"><a href="#Hybrid-Retrieval-Augmented-Generation-for-Real-time-Composition-Assistance" class="headerlink" title="Hybrid Retrieval-Augmented Generation for Real-time Composition Assistance"></a>Hybrid Retrieval-Augmented Generation for Real-time Composition Assistance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04215">http://arxiv.org/abs/2308.04215</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuchao Zhang, Menglin Xia, Camille Couturier, Guoqing Zheng, Saravan Rajmohan, Victor Ruhle</li>
<li>for: 提高语言模型的上下文理解和私有数据的 интеграción，以及减少幻觉。</li>
<li>methods: 使用Hybrid Retrieval-Augmented Generation（HybridRAG）框架，结合云端和客户端模型，并在云端使用大语言模型（LLM）生成异步的检索增强内存。</li>
<li>results: HybridRAG在 Wikitext 和 Pile 子集上实现了更低的延迟，并在实用性方面超过了云端只有模型。<details>
<summary>Abstract</summary>
Retrieval augmented models show promise in enhancing traditional language models by improving their contextual understanding, integrating private data, and reducing hallucination. However, the processing time required for retrieval augmented large language models poses a challenge when applying them to tasks that require real-time responses, such as composition assistance.   To overcome this limitation, we propose the Hybrid Retrieval-Augmented Generation (HybridRAG) framework that leverages a hybrid setting that combines both client and cloud models. HybridRAG incorporates retrieval-augmented memory generated asynchronously by a Large Language Model (LLM) in the cloud. By integrating this retrieval augmented memory, the client model acquires the capability to generate highly effective responses, benefiting from the LLM's capabilities. Furthermore, through asynchronous memory integration, the client model is capable of delivering real-time responses to user requests without the need to wait for memory synchronization from the cloud. Our experiments on Wikitext and Pile subsets show that HybridRAG achieves lower latency than a cloud-based retrieval-augmented LLM, while outperforming client-only models in utility.
</details>
<details>
<summary>摘要</summary>
Note:* "Retrieval-augmented models" refers to models that use retrieval-augmented memory to improve their performance.* "Large Language Model" (LLM) refers to a model that can process and generate human-like language.* "Client model" refers to a model that runs on a local device, such as a smartphone or a computer.* "Cloud model" refers to a model that runs on a remote server, such as a cloud computing service.* "Memory synchronization" refers to the process of synchronizing the memory of multiple devices or models, so that they can access and share the same information.* "Utility" refers to the usefulness or effectiveness of a model or approach.
</details></li>
</ul>
<hr>
<h2 id="Adding-Why-to-What-Analyses-of-an-Everyday-Explanation"><a href="#Adding-Why-to-What-Analyses-of-an-Everyday-Explanation" class="headerlink" title="Adding Why to What? Analyses of an Everyday Explanation"></a>Adding Why to What? Analyses of an Everyday Explanation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04187">http://arxiv.org/abs/2308.04187</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lutz Terfloth, Michael Schaffer, Heike M. Buhl, Carsten Schulte</li>
<li>for: 这篇论文的目的是研究如何为非专家用户提供可解释的技术决策。</li>
<li>methods: 这篇论文使用了技术哲学的双重本质理论来探讨对非专家用户的解释。</li>
<li>results: 研究发现，解释者在解释游戏时首先关注建筑（Architecture），然后关注相关性（Relevance）。在视频回忆中，解释者解释了基本组件之前 initially 解释了Physical Aspects，然后才转移到更复杂的、不可见的方面。 shift between addressing the two sides was justified by explanation goals, emerging misunderstandings, and the knowledge needs of the explainee。<details>
<summary>Abstract</summary>
In XAI it is important to consider that, in contrast to explanations for professional audiences, one cannot assume common expertise when explaining for laypeople. But such explanations between humans vary greatly, making it difficult to research commonalities across explanations. We used the dual nature theory, a techno-philosophical approach, to cope with these challenges. According to it, one can explain, for example, an XAI's decision by addressing its dual nature: by focusing on the Architecture (e.g., the logic of its algorithms) or the Relevance (e.g., the severity of a decision, the implications of a recommendation). We investigated 20 game explanations using the theory as an analytical framework. We elaborate how we used the theory to quickly structure and compare explanations of technological artifacts. We supplemented results from analyzing the explanation contents with results from a video recall to explore how explainers justified their explanation. We found that explainers were focusing on the physical aspects of the game first (Architecture) and only later on aspects of the Relevance. Reasoning in the video recalls indicated that EX regarded the focus on the Architecture as important for structuring the explanation initially by explaining the basic components before focusing on more complex, intangible aspects. Shifting between addressing the two sides was justified by explanation goals, emerging misunderstandings, and the knowledge needs of the explainee. We discovered several commonalities that inspire future research questions which, if further generalizable, provide first ideas for the construction of synthetic explanations.
</details>
<details>
<summary>摘要</summary>
在XAI中，需要注意的是，与专业听众的解释不同，不能假设共同知识。然而，人类之间的解释却很多样化，这使得研究共同点困难。我们采用了双重本质理论，一种技术哲学方法，以应对这些挑战。根据这种理论，可以通过关注XAI的几个方面来解释它的决策： Architecture（例如算法逻辑）或 Relevance（例如决策严重性、建议的影响）。我们对20个游戏解释使用了这种分析框架。我们详细介绍了如何使用这种理论快速结构和比较解释技术 artifacts。我们还补充了分析解释内容的结果，以及视频回忆中的解释者 justify their explanation。我们发现，解释者在初始阶段关注物理方面（Architecture），然后才关注更复杂、无形的方面。在视频回忆中的理由表明，EX认为在初始阶段通过解释基本组件来结构化解释是重要的。在转换 между两个方面时，解释者根据解释目标、出现的混淆和需要了解的知识来决定转换。我们发现了一些共同点，这些共同点可能会激发未来的研究问题。如果这些共同点能够普遍适用，它们将提供首先的想法 для构建人工解释。
</details></li>
</ul>
<hr>
<h2 id="Assistive-Chatbots-for-healthcare-a-succinct-review"><a href="#Assistive-Chatbots-for-healthcare-a-succinct-review" class="headerlink" title="Assistive Chatbots for healthcare: a succinct review"></a>Assistive Chatbots for healthcare: a succinct review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04178">http://arxiv.org/abs/2308.04178</a></li>
<li>repo_url: None</li>
<li>paper_authors: Basabdatta Sen Bhattacharya, Vibhav Sinai Pissurlenkar</li>
<li>For: The paper is written to review the state-of-the-art in AI-enabled Chatbots in healthcare, specifically during the last 10 years (2013-2023).* Methods: The paper reviews commercial and non-commercial Chatbots that are being used for patient support, as well as those in clinical trial phases. It also discusses the need for thorough and rigorous checks to ensure patient safety and medical ethics.* Results: The paper highlights a lack of trust in AI-enabled Chatbots among healthcare workers, patients, and the wider community, as well as dissatisfaction with the NLP skills of the Chatbots. It suggests that to enable deployment and integration of AI-enabled Chatbots in public health services, the technology needs to be simple and safe to use, and confidence in the technology needs to be built among the medical community and the wider community through outreach.Here are the three points in Simplified Chinese text:* For: 这篇论文是为了回顾过去十年（2013-2023）内健康服务中AI应用的状况。* Methods: 论文评论了商业和非商业的Chatbot，以及它们在患者支持方面的应用。它还提出了为保证患者安全和医疗伦理的严格检查的需要。* Results: 论文指出了健康工作者、患者和社会大众对AI应用Chatbot的不信任，以及Chatbot的自然语言处理技术不够的不满。它建议，为了让AI应用Chatbot在公共医疗服务中得到广泛应用，技术需要简单、安全，并需要对医疗人员和社会大众进行宣传和培训。<details>
<summary>Abstract</summary>
Artificial Intelligence (AI) for supporting healthcare services has never been more necessitated than by the recent global pandemic. Here, we review the state-of-the-art in AI-enabled Chatbots in healthcare proposed during the last 10 years (2013-2023). The focus on AI-enabled technology is because of its potential for enhancing the quality of human-machine interaction via Chatbots, reducing dependence on human-human interaction and saving man-hours. Our review indicates that there are a handful of (commercial) Chatbots that are being used for patient support, while there are others (non-commercial) that are in the clinical trial phases. However, there is a lack of trust on this technology regarding patient safety and data protection, as well as a lack of wider awareness on its benefits among the healthcare workers and professionals. Also, patients have expressed dissatisfaction with Natural Language Processing (NLP) skills of the Chatbots in comparison to humans. Notwithstanding the recent introduction of ChatGPT that has raised the bar for the NLP technology, this Chatbot cannot be trusted with patient safety and medical ethics without thorough and rigorous checks to serve in the `narrow' domain of assistive healthcare. Our review suggests that to enable deployment and integration of AI-enabled Chatbots in public health services, the need of the hour is: to build technology that is simple and safe to use; to build confidence on the technology among: (a) the medical community by focussed training and development; (b) the patients and wider community through outreach.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）在支持医疗服务方面从未如今所需要的那么重要。我们对过去10年（2013-2023）提出的AI应用于医疗领域的评论。我们的评论表明，只有一些商业聊天机器人在患者支持方面使用，而其他非商业聊天机器人则处于临床试验阶段。然而，技术的可靠性和数据保护方面存在不足的信任，同时医疗工作者和专业人员对其利好的认知也不够。此外，患者对自然语言处理（NLP）技术的评价较低，与人类之间的交流仍然存在差距。尽管最近出现了ChatGPT，但这种技术在医疗领域的应用仍需进行严格的检验和评估，以确保Patient Safety和医疗伦理的安全性。我们的评论建议，为了使AI应用于医疗服务中，需要：建立简单安全的技术；帮助医疗社区了解和信任技术；通过宣传和教育，建立患者和社区的信任。
</details></li>
</ul>
<hr>
<h2 id="Predicting-Drug-Drug-Interactions-Using-Knowledge-Graphs"><a href="#Predicting-Drug-Drug-Interactions-Using-Knowledge-Graphs" class="headerlink" title="Predicting Drug-Drug Interactions Using Knowledge Graphs"></a>Predicting Drug-Drug Interactions Using Knowledge Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04172">http://arxiv.org/abs/2308.04172</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lizzy Farrugia, Lilian M. Azzopardi, Jeremy Debattista, Charlie Abela<br>for:The paper aims to predict unknown Drug-Drug Interactions (DDIs) by incorporating Knowledge Graphs (KGs) and various drug features from public drug repositories.methods:The medicX end-to-end framework uses a combination of translation, factorisation, and Neural Network (NN) based KG Embedding (KGE) methods to integrate drug features and predict unknown DDIs. The best performing combination was the ComplEx embedding method with a Long Short-Term Memory (LSTM) network, which achieved an F1-score of 95.19%.results:The ComplEx embedding method with an LSTM network achieved an F1-score of 95.19% on a dataset based on the DDIs found in DrugBank version 5.1.8, outperforming the state-of-the-art model DeepDDI by 5.61%. Additionally, a graph auto-encoder model using a Graph Neural Network (GNN) achieved an F1-score of 91.94%.<details>
<summary>Abstract</summary>
In the last decades, people have been consuming and combining more drugs than before, increasing the number of Drug-Drug Interactions (DDIs). To predict unknown DDIs, recently, studies started incorporating Knowledge Graphs (KGs) since they are able to capture the relationships among entities providing better drug representations than using a single drug property. In this paper, we propose the medicX end-to-end framework that integrates several drug features from public drug repositories into a KG and embeds the nodes in the graph using various translation, factorisation and Neural Network (NN) based KG Embedding (KGE) methods. Ultimately, we use a Machine Learning (ML) algorithm that predicts unknown DDIs. Among the different translation and factorisation-based KGE models, we found that the best performing combination was the ComplEx embedding method with a Long Short-Term Memory (LSTM) network, which obtained an F1-score of 95.19% on a dataset based on the DDIs found in DrugBank version 5.1.8. This score is 5.61% better than the state-of-the-art model DeepDDI. Additionally, we also developed a graph auto-encoder model that uses a Graph Neural Network (GNN), which achieved an F1-score of 91.94%. Consequently, GNNs have demonstrated a stronger ability to mine the underlying semantics of the KG than the ComplEx model, and thus using higher dimension embeddings within the GNN can lead to state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
在最近几十年中，人们的药物consumption和组合已经变得更加普遍，导致药物相互作用（DDIs）的数量增加。为预测未知的DDIs，最近的研究开始 incorporating知识图（KGs），因为它们可以捕捉药物之间的关系，提供更好的药物表示than使用单一的药物属性。在这篇文章中，我们提出了medicX终端框架，该框架 integrates 多种药物特征从公共药物库中into a KG，并使用不同的翻译、分解和神经网络（NN）基于KGE方法来嵌入图节点。最终，我们使用机器学习算法预测未知DDIs。在不同的翻译和分解基于KGE模型中，我们发现了最佳的组合是ComplEx嵌入方法与长短期记忆网络（LSTM），其在基于DrugBank版本5.1.8的数据集上取得了F1得分95.19%，高于当前状态的模型DeepDDI。此外，我们还开发了一种图自编码模型，使用图神经网络（GNN），其取得了F1得分91.94%。因此，GNNs在挖掘知识图下的能力更强，使用高维度嵌入在GNN中可以达到状态之Art。
</details></li>
</ul>
<hr>
<h2 id="Current-and-Future-Challenges-in-Knowledge-Representation-and-Reasoning"><a href="#Current-and-Future-Challenges-in-Knowledge-Representation-and-Reasoning" class="headerlink" title="Current and Future Challenges in Knowledge Representation and Reasoning"></a>Current and Future Challenges in Knowledge Representation and Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04161">http://arxiv.org/abs/2308.04161</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/Other-sources">https://github.com/Aryia-Behroziuan/Other-sources</a></li>
<li>paper_authors: James P. Delgrande, Birte Glimm, Thomas Meyer, Miroslaw Truszczynski, Frank Wolter</li>
<li>For: The paper discusses the current state of the art in Knowledge Representation and Reasoning, including its relation to other areas such as machine learning and uncertainty reasoning, and provides recommendations for future progress.* Methods: The paper is based on presentations, panels, working groups, and discussions that took place at a Dagstuhl Perspectives workshop on Knowledge Representation and Reasoning in July 2022.* Results: The paper provides a manifesto that declares the current views on Knowledge Representation, including its origins, goals, milestones, and current foci, as well as its challenges and key priorities for the next decade.Here is the same information in Simplified Chinese text:</li>
<li>for: 本文讲述了知识表示和推理领域的当前状况，包括它与其他领域的关系，如机器学习和不确定性推理，以及未来进展的建议。</li>
<li>methods: 本文基于2022年7月的达斯图尔视点工作shop的现场表示、小组讨论和推动活动。</li>
<li>results: 本文提供了一份宣言，宣布知识表示的起源、目标、里程碑和当前焦点，以及其挑战和未来十年的关键优先事项。<details>
<summary>Abstract</summary>
Knowledge Representation and Reasoning is a central, longstanding, and active area of Artificial Intelligence. Over the years it has evolved significantly; more recently it has been challenged and complemented by research in areas such as machine learning and reasoning under uncertainty. In July 2022 a Dagstuhl Perspectives workshop was held on Knowledge Representation and Reasoning. The goal of the workshop was to describe the state of the art in the field, including its relation with other areas, its shortcomings and strengths, together with recommendations for future progress. We developed this manifesto based on the presentations, panels, working groups, and discussions that took place at the Dagstuhl Workshop. It is a declaration of our views on Knowledge Representation: its origins, goals, milestones, and current foci; its relation to other disciplines, especially to Artificial Intelligence; and on its challenges, along with key priorities for the next decade.
</details>
<details>
<summary>摘要</summary>
知识表示和推理是人工智能的中心、长期积极发展的领域。随着时间的推移，它不断发展和改进，最近受到机器学习和不确定性推理的研究启发。2022年7月，达斯图尔视角工作坊（Dagstuhl Perspectives）举行了关于知识表示和推理的国际研讨会。工作坊的目的是描述该领域的现状，包括与其他领域的关系、短coming和优势，以及未来十年的发展优先级。我们基于工作坊的演讲、审议组、工作组和讨论会议的结果，制定了这份宣言。这是我们对知识表示的看法，包括其起源、目标、里程碑和当前焦点；与其他学科的关系，特别是人工智能；以及其挑战和未来十年的发展优先级。
</details></li>
</ul>
<hr>
<h2 id="Correlating-Medi-Claim-Service-by-Deep-Learning-Neural-Networks"><a href="#Correlating-Medi-Claim-Service-by-Deep-Learning-Neural-Networks" class="headerlink" title="Correlating Medi-Claim Service by Deep Learning Neural Networks"></a>Correlating Medi-Claim Service by Deep Learning Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04469">http://arxiv.org/abs/2308.04469</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jayanthi Vajiram, Negha Senthil, Nean Adhith. P</li>
<li>for: 防止医疗保险诈骗案件，包括患者、医生、诊断中心和保险公司之间的串谍关系，以确保金融增长。</li>
<li>methods: 使用卷积神经网络架构，通过对不同提供者的clam做 corrrelation 研究，检测诈骗CLAIM。同时使用超级vised和无监督分类器来检测诈骗和非诈骗CLAIM。</li>
<li>results: 通过使用卷积神经网络架构和 corrrelation 研究，能够准确地检测诈骗CLAIM，并且可以帮助防止金融诈骗案件。<details>
<summary>Abstract</summary>
Medical insurance claims are of organized crimes related to patients, physicians, diagnostic centers, and insurance providers, forming a chain reaction that must be monitored constantly. These kinds of frauds affect the financial growth of both insured people and health insurance companies. The Convolution Neural Network architecture is used to detect fraudulent claims through a correlation study of regression models, which helps to detect money laundering on different claims given by different providers. Supervised and unsupervised classifiers are used to detect fraud and non-fraud claims.
</details>
<details>
<summary>摘要</summary>
医疗保险养成有组织犯罪关系于病人、医生、诊断中心和保险公司，形成一个推动式的链 reaction。这种类型的诈骗活动会对保险人和健康保险公司的财务增长产生影响。使用卷积神经网络架构来检测诈骗养成，通过对不同提供者的clamshell进行相关性研究，可以检测到不同提供者的钱财洗涤。使用supervised和Unsupervised分类器来检测诈骗和非诈骗养成。
</details></li>
</ul>
<hr>
<h2 id="Heterogeneous-360-Degree-Videos-in-Metaverse-Differentiated-Reinforcement-Learning-Approaches"><a href="#Heterogeneous-360-Degree-Videos-in-Metaverse-Differentiated-Reinforcement-Learning-Approaches" class="headerlink" title="Heterogeneous 360 Degree Videos in Metaverse: Differentiated Reinforcement Learning Approaches"></a>Heterogeneous 360 Degree Videos in Metaverse: Differentiated Reinforcement Learning Approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04083">http://arxiv.org/abs/2308.04083</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenhan Yu, Jun Zhao</li>
<li>for: 这篇论文旨在提出一种适用于多种需求的质量服务模型，以满足未来元宇宙中多样化用户需求的视频技术发展。</li>
<li>methods: 该论文提出了一种基于自适应深度学习算法的帧槽结构，并对帧进行优化。两种结构：分离输入异构输出（SIDO）和合并输入异构输出（MIDO），以适应多种需求的场景。</li>
<li>results: 实验表明，该模型能够有效地优化帧率和压缩率，并适应不同需求的场景。<details>
<summary>Abstract</summary>
Advanced video technologies are driving the development of the futuristic Metaverse, which aims to connect users from anywhere and anytime. As such, the use cases for users will be much more diverse, leading to a mix of 360-degree videos with two types: non-VR and VR 360-degree videos. This paper presents a novel Quality of Service model for heterogeneous 360-degree videos with different requirements for frame rates and cybersickness. We propose a frame-slotted structure and conduct frame-wise optimization using self-designed differentiated deep reinforcement learning algorithms. Specifically, we design two structures, Separate Input Differentiated Output (SIDO) and Merged Input Differentiated Output (MIDO), for this heterogeneous scenario. We also conduct comprehensive experiments to demonstrate their effectiveness.
</details>
<details>
<summary>摘要</summary>
高级视频技术驱动未来Metaverse的发展，目的是Connect users from anywhere and anytime。因此，用户的用例将变得更加多样化，导致360度视频的两种类型：非VR和VR 360度视频。这篇论文提出了一种新的服务质量模型 для不同需求的 heterogeneous 360度视频，包括帧率和恶心症的不同需求。我们提出了一种帧槽结构，并通过自定义分化深度学习算法进行帧WISE优化。具体来说，我们设计了两种结构：分离输入�ifferentiated输出（SIDO）和合并输入�ifferentiated输出（MIDO），为这种多样化enario提供了优化。我们还进行了广泛的实验，以证明它们的有效性。
</details></li>
</ul>
<hr>
<h2 id="Federated-Zeroth-Order-Optimization-using-Trajectory-Informed-Surrogate-Gradients"><a href="#Federated-Zeroth-Order-Optimization-using-Trajectory-Informed-Surrogate-Gradients" class="headerlink" title="Federated Zeroth-Order Optimization using Trajectory-Informed Surrogate Gradients"></a>Federated Zeroth-Order Optimization using Trajectory-Informed Surrogate Gradients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04077">http://arxiv.org/abs/2308.04077</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yao Shu, Xiaoqiang Lin, Zhongxiang Dai, Bryan Kian Hsiang Low</li>
<li>for:  Federated zeroth-order optimization (ZOO) algorithms, which are used for query- and communication-efficient optimization in applications such as federated learning.</li>
<li>methods:  Trajectory-informed gradient surrogates and adaptive gradient correction techniques, which are used to improve the accuracy and efficiency of federated ZOO.</li>
<li>results:  The proposed FZooS algorithm achieves theoretical improvements over existing approaches and is supported by real-world experiments in federated black-box adversarial attack and federated non-differentiable metric optimization.Here is the simplified Chinese version of the three information:</li>
<li>for:  federated zeroth-order优化（ZOO）算法，用于实现缓存和通信效率的优化，如联合学习等应用。</li>
<li>methods:  using trajectory-informed gradient surrogates和适应式Gradient correction技术，以提高联合ZOO的准确性和效率。</li>
<li>results:  proposed FZooS算法在理论上有所改进，并在实际中通过联合黑盒抗击和非凸度量优化等实验得到支持。<details>
<summary>Abstract</summary>
Federated optimization, an emerging paradigm which finds wide real-world applications such as federated learning, enables multiple clients (e.g., edge devices) to collaboratively optimize a global function. The clients do not share their local datasets and typically only share their local gradients. However, the gradient information is not available in many applications of federated optimization, which hence gives rise to the paradigm of federated zeroth-order optimization (ZOO). Existing federated ZOO algorithms suffer from the limitations of query and communication inefficiency, which can be attributed to (a) their reliance on a substantial number of function queries for gradient estimation and (b) the significant disparity between their realized local updates and the intended global updates. To this end, we (a) introduce trajectory-informed gradient surrogates which is able to use the history of function queries during optimization for accurate and query-efficient gradient estimation, and (b) develop the technique of adaptive gradient correction using these gradient surrogates to mitigate the aforementioned disparity. Based on these, we propose the federated zeroth-order optimization using trajectory-informed surrogate gradients (FZooS) algorithm for query- and communication-efficient federated ZOO. Our FZooS achieves theoretical improvements over the existing approaches, which is supported by our real-world experiments such as federated black-box adversarial attack and federated non-differentiable metric optimization.
</details>
<details>
<summary>摘要</summary>
联合优化，是一种兴起的概念，它在联合学习、联合优化等实际应用中找到了广泛的应用。在这种概念下，多个客户端（例如边缘设备）可以共同优化一个全球函数。客户端不会分享自己的本地数据，通常只会分享本地的梯度。但是，在许多应用中，梯度信息不可用，因此产生了联合零阶优化（ZOO）的概念。现有的联合ZOO算法受到函数询问和通信不�fficiente的限制，这可以被归因于（a）它们依赖了访问函数的很多次以估计梯度，以及（b）它们实现的本地更新和 globally 预期的更新之间存在很大的差异。为了解决这个问题，我们（a）引入了路径受限的梯度代理，这些梯度代理可以使用优化过程中的历史函数询问来实现精确和查询节省的梯度估计，以及（b）开发了适应性梯度调整技术，使用这些梯度代理来缓和上述差异。基于这些，我们提出了联合零阶优化使用路径受限梯度代理（FZooS）算法，实现了查询和通信节省的联合ZOO。我们的FZooS理论上超越了现有的方法，这被我们在实际应用中，如联合黑盒抗击和联合非 diffeomorphic 度量优化中所证明。
</details></li>
</ul>
<hr>
<h2 id="Path-Signatures-for-Diversity-in-Probabilistic-Trajectory-Optimisation"><a href="#Path-Signatures-for-Diversity-in-Probabilistic-Trajectory-Optimisation" class="headerlink" title="Path Signatures for Diversity in Probabilistic Trajectory Optimisation"></a>Path Signatures for Diversity in Probabilistic Trajectory Optimisation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04071">http://arxiv.org/abs/2308.04071</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucas Barcelos, Tin Lai, Rafael Oliveira, Paulo Borges, Fabio Ramos</li>
<li>for: 这个论文的目的是提出一种用于平行 trajectory 优化的算法，以避免模式溃灭并实现更好的全局性。</li>
<li>methods: 该算法基于粗路论断理论中的新进展，利用粗路论断理论中的粗路签名和希尔伯特空间表示来实现平行优化，并将平行变量推断与多样性推进的kernel相连接。</li>
<li>results: 实验表明，该策略可以在各种问题上实现更低的平均成本，包括2D导航和受损环境中的机器人手臂操作。<details>
<summary>Abstract</summary>
Motion planning can be cast as a trajectory optimisation problem where a cost is minimised as a function of the trajectory being generated. In complex environments with several obstacles and complicated geometry, this optimisation problem is usually difficult to solve and prone to local minima. However, recent advancements in computing hardware allow for parallel trajectory optimisation where multiple solutions are obtained simultaneously, each initialised from a different starting point. Unfortunately, without a strategy preventing two solutions to collapse on each other, naive parallel optimisation can suffer from mode collapse diminishing the efficiency of the approach and the likelihood of finding a global solution. In this paper we leverage on recent advances in the theory of rough paths to devise an algorithm for parallel trajectory optimisation that promotes diversity over the range of solutions, therefore avoiding mode collapses and achieving better global properties. Our approach builds on path signatures and Hilbert space representations of trajectories, and connects parallel variational inference for trajectory estimation with diversity promoting kernels. We empirically demonstrate that this strategy achieves lower average costs than competing alternatives on a range of problems, from 2D navigation to robotic manipulators operating in cluttered environments.
</details>
<details>
<summary>摘要</summary>
路径规划可以被看作是一个轨迹优化问题，其中需要将轨迹优化为最小化一个成本函数。在复杂的环境中，找到globally optimal solution可以是一个困难的任务，因为这个问题通常会陷入到地方最优解。然而，随着计算机硬件的进步，我们可以使用并行的轨迹优化方法，从不同的初始点开始并行地生成多个解决方案。然而，如果不采取措施来避免解决方案之间的冲突，那么纯粹的并行优化方法可能会陷入到模式塌突，从而降低方法的效率和找到全局解的可能性。在这篇论文中，我们采用了最近的粗 PATH 理论来设计一种并行轨迹优化算法，该算法可以在轨迹优化过程中提高多样性，因此避免模式塌突并实现更好的全局性。我们的方法基于轨迹签名和希尔伯特空间表示，并将并行变分推理与多样性激活函数相连接。我们实际上证明了这种策略在一系列问题上实现了更低的平均成本，从2D导航到受损环境中的机器人抓取器。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Adversarial-Robustness-in-Low-Label-Regime-via-Adaptively-Weighted-Regularization-and-Knowledge-Distillation"><a href="#Enhancing-Adversarial-Robustness-in-Low-Label-Regime-via-Adaptively-Weighted-Regularization-and-Knowledge-Distillation" class="headerlink" title="Enhancing Adversarial Robustness in Low-Label Regime via Adaptively Weighted Regularization and Knowledge Distillation"></a>Enhancing Adversarial Robustness in Low-Label Regime via Adaptively Weighted Regularization and Knowledge Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04061">http://arxiv.org/abs/2308.04061</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongyoon Yang, Insung Kong, Yongdai Kim</li>
<li>for: 本研究针对具有仅有少量标签数据的情况下进行了 semi-supervised adversarial 训练。</li>
<li>methods: 本研究提出了两个上限函数，并提出了一个问题数据驱动的调整项。然后，我们开发了一个兼容这些上限函数的 semi-supervised adversarial 训练算法，其结合了问题数据驱动的知识传递和专家模型（i.e., 一个使用 semi-supervised 学习算法训练的教师模型）。</li>
<li>results: 我们的实验结果显示，我们的提案的算法可以实现 state-of-the-art 的性能，与现有算法相比，具有显著的优势。具体来说，对于仅有少量标签数据的情况下，我们的算法与使用所有标签数据的超级vised adversarial 训练算法相比，在 CIFAR-10 上的标准和Robust 精度上几乎相同。例如，我们的算法仅使用 8% 的标签数据时，与使用所有标签数据的超级vised adversarial 训练算法相比，其性能仍然具有显著的优势。<details>
<summary>Abstract</summary>
Adversarial robustness is a research area that has recently received a lot of attention in the quest for trustworthy artificial intelligence. However, recent works on adversarial robustness have focused on supervised learning where it is assumed that labeled data is plentiful. In this paper, we investigate semi-supervised adversarial training where labeled data is scarce. We derive two upper bounds for the robust risk and propose a regularization term for unlabeled data motivated by these two upper bounds. Then, we develop a semi-supervised adversarial training algorithm that combines the proposed regularization term with knowledge distillation using a semi-supervised teacher (i.e., a teacher model trained using a semi-supervised learning algorithm). Our experiments show that our proposed algorithm achieves state-of-the-art performance with significant margins compared to existing algorithms. In particular, compared to supervised learning algorithms, performance of our proposed algorithm is not much worse even when the amount of labeled data is very small. For example, our algorithm with only 8\% labeled data is comparable to supervised adversarial training algorithms that use all labeled data, both in terms of standard and robust accuracies on CIFAR-10.
</details>
<details>
<summary>摘要</summary>
“敌对类型调教是现在人工智能的研究领域中受到了很多关注，以确保人工智能的可靠性。然而，现有的工作通常假设有充足的标签数据，而我们在这篇论文中则 investigate 敌对调教中的半supervised 学习，在标签数据 scarce 的情况下。我们 deriv 了两个上限 bound 的敌对风险，并提出了一个基于这两个上限 bound 的调教term。然后，我们开发了一个半supervised adversarial training algorithm，它结合了我们提出的调教term 和知识传授使用半supervised teacher (即一个使用半supervised learning algorithm训练的教师模型)。我们的实验结果显示，我们的提案的算法可以 achieve state-of-the-art 性能，并且与已有算法相比，在标签数据很少的情况下，性能不会太差。例如，我们的算法仅使用8%的标签数据时，可以与完全supervised adversarial training algorithm相比，在 CIFAR-10 上 Both 标准和敌对精度方面表现出色。”
</details></li>
</ul>
<hr>
<h2 id="SODFormer-Streaming-Object-Detection-with-Transformer-Using-Events-and-Frames"><a href="#SODFormer-Streaming-Object-Detection-with-Transformer-Using-Events-and-Frames" class="headerlink" title="SODFormer: Streaming Object Detection with Transformer Using Events and Frames"></a>SODFormer: Streaming Object Detection with Transformer Using Events and Frames</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04047">http://arxiv.org/abs/2308.04047</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dianzl/sodformer">https://github.com/dianzl/sodformer</a></li>
<li>paper_authors: Dianze Li, Jianing Li, Yonghong Tian</li>
<li>for: 提高对象检测的精度和效率，特别是在高速运动和低光照条件下。</li>
<li>methods: 利用Transformer架构， integrates events and frames to continuously detect objects in an asynchronous manner，并使用 asynchronous attention-based fusion module to integrate two heterogeneous sensing modalities。</li>
<li>results: 与四种state-of-the-art方法和八个基eline比较，提出的SODFormer方法显示出了显著的性能优势。 Additionally, the proposed method works well even in cases where the conventional frame-based camera fails, such as high-speed motion and low-light conditions.<details>
<summary>Abstract</summary>
DAVIS camera, streaming two complementary sensing modalities of asynchronous events and frames, has gradually been used to address major object detection challenges (e.g., fast motion blur and low-light). However, how to effectively leverage rich temporal cues and fuse two heterogeneous visual streams remains a challenging endeavor. To address this challenge, we propose a novel streaming object detector with Transformer, namely SODFormer, which first integrates events and frames to continuously detect objects in an asynchronous manner. Technically, we first build a large-scale multimodal neuromorphic object detection dataset (i.e., PKU-DAVIS-SOD) over 1080.1k manual labels. Then, we design a spatiotemporal Transformer architecture to detect objects via an end-to-end sequence prediction problem, where the novel temporal Transformer module leverages rich temporal cues from two visual streams to improve the detection performance. Finally, an asynchronous attention-based fusion module is proposed to integrate two heterogeneous sensing modalities and take complementary advantages from each end, which can be queried at any time to locate objects and break through the limited output frequency from synchronized frame-based fusion strategies. The results show that the proposed SODFormer outperforms four state-of-the-art methods and our eight baselines by a significant margin. We also show that our unifying framework works well even in cases where the conventional frame-based camera fails, e.g., high-speed motion and low-light conditions. Our dataset and code can be available at https://github.com/dianzl/SODFormer.
</details>
<details>
<summary>摘要</summary>
《DAVIS摄像头 Streaming Two Complementary Sensing Modalities of Asynchronous Events and Frames for Object Detection》DAVIS摄像头， Streaming two complementary sensing modalities of asynchronous events and frames，已经被广泛应用于重要的物体检测挑战中（例如快速运动模糊和低光照）。然而，如何有效利用rich temporal cues和融合两种不同的视觉流还是一个挑战。为了解决这个挑战，我们提出了一种新的流动对象检测器，即SODFormer，它首先将事件和帧集成为一起continuously检测物体。技术上，我们首先建立了一个大规模的多模态神经元摄像头检测数据集（即PKU-DAVIS-SOD），包括1080.1k的手动标签。然后，我们设计了一种空间时间Transformer架构，通过一个终到终的序列预测问题，来检测物体。在这个架构中，我们提出了一种新的 temporal Transformer模块，利用了两个视觉流的rich temporal cues来提高检测性能。最后，我们提出了一种异步注意力基于的融合模块，以便将两种不同的感知模式融合在一起，并且可以在任何时候提问，以便查找物体和跨出同步帧基于的融合策略的限制。结果显示，我们提出的SODFormer方法在比较四种state-of-the-art方法和我们的八个基eline之上取得了显著的提高。我们还证明了我们的统一框架在高速运动和低光照等情况下也能够正常工作。我们的数据集和代码可以在https://github.com/dianzl/SODFormer上下载。
</details></li>
</ul>
<hr>
<h2 id="Non-Intrusive-Electric-Load-Monitoring-Approach-Based-on-Current-Feature-Visualization-for-Smart-Energy-Management"><a href="#Non-Intrusive-Electric-Load-Monitoring-Approach-Based-on-Current-Feature-Visualization-for-Smart-Energy-Management" class="headerlink" title="Non-Intrusive Electric Load Monitoring Approach Based on Current Feature Visualization for Smart Energy Management"></a>Non-Intrusive Electric Load Monitoring Approach Based on Current Feature Visualization for Smart Energy Management</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11627">http://arxiv.org/abs/2308.11627</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiwen Xu, Dengfeng Liu, Liangtao Huang, Zhiquan Lin, Tiesong Zhao, Sam Kwong</li>
<li>for: 本研究旨在提出一种非侵入式电力负荷监测方法，以支持智能城市的经济可持续能源管理。</li>
<li>methods: 本文employs popular计算机视觉技术，包括卷积变换和gramianangular场方法，将一维电流信号映射到二维颜色特征图像上。然后，通过U型深度神经网络 WITH multi-scale特征提取和注意机制，识别所有电动负荷。</li>
<li>results: 实验结果表明，提出的方法在公共数据集和私有数据集上均达到了superior表现，可以支持大规模互联网对象（IoT）中的能效能源管理。<details>
<summary>Abstract</summary>
The state-of-the-art smart city has been calling for an economic but efficient energy management over large-scale network, especially for the electric power system. It is a critical issue to monitor, analyze and control electric loads of all users in system. In this paper, we employ the popular computer vision techniques of AI to design a non-invasive load monitoring method for smart electric energy management. First of all, we utilize both signal transforms (including wavelet transform and discrete Fourier transform) and Gramian Angular Field (GAF) methods to map one-dimensional current signals onto two-dimensional color feature images. Second, we propose to recognize all electric loads from color feature images using a U-shape deep neural network with multi-scale feature extraction and attention mechanism. Third, we design our method as a cloud-based, non-invasive monitoring of all users, thereby saving energy cost during electric power system control. Experimental results on both public and our private datasets have demonstrated our method achieves superior performances than its peers, and thus supports efficient energy management over large-scale Internet of Things (IoT).
</details>
<details>
<summary>摘要</summary>
现代智能城市的要求是实现经济高效的能源管理，特别是电力系统。监测、分析和控制所有用户的电载是一个关键问题。在这篇论文中，我们利用人工智能popular计算机视觉技术来设计一种不侵入式的电力监测方法。首先，我们利用信号变换（包括wavelet transform和Discrete Fourier Transform）和Gramian Angular Field（GAF）方法将一维电流信号映射到二维颜色特征图像上。其次，我们提出了通过U型深度神经网络with multi-scale feature extraction和注意机制来识别所有电载。最后，我们设计了一种云端基于的非侵入式监测方法，以 saves energy cost during electric power system control。实验结果表明，我们的方法在公共数据集和私有数据集上都达到了更高的性能，因此支持了大规模Internet of Things（IoT）中的高效能源管理。
</details></li>
</ul>
<hr>
<h2 id="InfeRE-Step-by-Step-Regex-Generation-via-Chain-of-Inference"><a href="#InfeRE-Step-by-Step-Regex-Generation-via-Chain-of-Inference" class="headerlink" title="InfeRE: Step-by-Step Regex Generation via Chain of Inference"></a>InfeRE: Step-by-Step Regex Generation via Chain of Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04041">http://arxiv.org/abs/2308.04041</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/smallqqqq/infere">https://github.com/smallqqqq/infere</a></li>
<li>paper_authors: Shuai Zhang, Xiaodong Gu, Yuting Chen, Beijun Shen</li>
<li>for: 这个论文的目的是提出一种新的自然语言生成regex表达式（InfeRE），它可以帮助生成regex表达式的神经语言模型更加准确和可读性好。</li>
<li>methods: 这个论文使用了一种新的批处理方法，即将生成regex表达式的过程 decomposes into chains of step-by-step inference，以提高生成的regex表达式的精度和可读性。此外，它还引入了一种自适应均衡机制，以 Ensemble 多个模型的输出，从而提高了生成的regex表达式的稳定性。</li>
<li>results: 实验结果表明，InfeRE 可以备受提高神经语言模型生成regex表达式的精度，在两个公开的数据集上（NL-RX-Turk 和 KB13）测试，与前一代的基eline 和树状生成方法相比，InfeRE 可以提高 DFA@5 准确率的16.3% 和 14.7%。特别是，InfeRE 可以在两个数据集上，相比之前的树状生成方法，提高 DFA@5 准确率的18.1% 和 11.3%。<details>
<summary>Abstract</summary>
Automatically generating regular expressions (abbrev. regexes) from natural language description (NL2RE) has been an emerging research area. Prior studies treat regex as a linear sequence of tokens and generate the final expressions autoregressively in a single pass. They did not take into account the step-by-step internal text-matching processes behind the final results. This significantly hinders the efficacy and interpretability of regex generation by neural language models. In this paper, we propose a new paradigm called InfeRE, which decomposes the generation of regexes into chains of step-by-step inference. To enhance the robustness, we introduce a self-consistency decoding mechanism that ensembles multiple outputs sampled from different models. We evaluate InfeRE on two publicly available datasets, NL-RX-Turk and KB13, and compare the results with state-of-the-art approaches and the popular tree-based generation approach TRANX. Experimental results show that InfeRE substantially outperforms previous baselines, yielding 16.3% and 14.7% improvement in DFA@5 accuracy on two datasets, respectively. Particularly, InfeRE outperforms the popular tree-based generation approach by 18.1% and 11.3% on both datasets, respectively, in terms of DFA@5 accuracy.
</details>
<details>
<summary>摘要</summary>
自然语言描述（NL2RE）自动生成正则表达式（regex）是一个emerging研究领域。先前的研究通常将regex视为一个连续序列的token，通过单个通过一次推导生成最终结果。然而，这些研究未能考虑regex生成的内部文本匹配过程，这会限制神经语言模型的效果和可读性。在这篇论文中，我们提出了一新的思路called InfeRE，它将regex生成分解为一系列步骤的推导链。为了提高稳定性，我们还引入了自适应嵌入机制，该机制可以从不同模型中抽象多个输出，并将其ensemble。我们在两个公共可用的数据集NL-RX-Turk和KB13上进行了实验，并与当前的基eline和树状生成方法相比较。实验结果表明，InfeREsubstantiallyoutsperforms先前的基eline，在两个数据集上DFA@5准确率提高16.3%和14.7%。尤其是，InfeRE在两个数据集上与树状生成方法相比，DFA@5准确率提高18.1%和11.3%。
</details></li>
</ul>
<hr>
<h2 id="Adapting-Foundation-Models-for-Information-Synthesis-of-Wireless-Communication-Specifications"><a href="#Adapting-Foundation-Models-for-Information-Synthesis-of-Wireless-Communication-Specifications" class="headerlink" title="Adapting Foundation Models for Information Synthesis of Wireless Communication Specifications"></a>Adapting Foundation Models for Information Synthesis of Wireless Communication Specifications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04033">http://arxiv.org/abs/2308.04033</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manikanta Kotaru</li>
<li>for: 本研究旨在提供一种基于人工智能技术的 wireless 通信规范总结工具，帮助用户快速获取相关信息。</li>
<li>methods: 该工具基于现有的基础模型，并添加了三个关键组件：域特定数据库、上下文提取器和反馈机制。用户的问题将被补充了基于技术规范的简短和相关信息。</li>
<li>results: 根据一个标准 benchmark 集合，该工具能够提供更加准确和相关的答案，其中 Bleu 分数和 BERTScore F1-度分别为 0.37 和 0.79，比前一代工具 ChatGPT 的分数高出许多。<details>
<summary>Abstract</summary>
Existing approaches to understanding, developing and researching modern wireless communication technologies involves time-intensive and arduous process of sifting through numerous webpages and technical specification documents, gathering the required information and synthesizing it. This paper presents NextGen Communications Copilot, a conversational artificial intelligence tool for information synthesis of wireless communication specifications. The system builds on top of recent advancements in foundation models and consists of three key additional components: a domain-specific database, a context extractor, and a feedback mechanism. The system appends user queries with concise and query-dependent contextual information extracted from a database of wireless technical specifications and incorporates tools for expert feedback and data contributions. On evaluation using a benchmark dataset of queries and reference responses created by subject matter experts, the system demonstrated more relevant and accurate answers with an average BLEU score and BERTScore F1-measure of 0.37 and 0.79 respectively compared to the corresponding values of 0.07 and 0.59 achieved by state-of-the-art tools like ChatGPT.
</details>
<details>
<summary>摘要</summary>
现有的方法 для了解、开发和研究现代无线通信技术都是一个时间consuming和辛苦的过程，需要逐页搜索众多的网页和技术规范文档，收集所需信息并将其综合化。本文介绍了 NextGen Communications Copilot，一个基于最新的基础模型的会话型人工智能工具，用于无线通信规范信息的综合处理。该系统包括三个关键组件：域pecific数据库、上下文提取器和反馈机制。系统将用户查询 append 与域pecific数据库中的简短和查询dependent的上下文信息，并包括专家反馈和数据贡献工具。经评估使用一个标准 benchmark dataset of queries和 reference responses，创建了由专家制定的查询和参照响应，系统示出了与现有工具 like ChatGPT 的相对比较好的准确性和相关性，其 BLEU 分数和 BERTScore F1-measure 分别为 0.37 和 0.79。
</details></li>
</ul>
<hr>
<h2 id="Measure-of-Uncertainty-in-Human-Emotions"><a href="#Measure-of-Uncertainty-in-Human-Emotions" class="headerlink" title="Measure of Uncertainty in Human Emotions"></a>Measure of Uncertainty in Human Emotions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04032">http://arxiv.org/abs/2308.04032</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/Other-sources">https://github.com/Aryia-Behroziuan/Other-sources</a></li>
<li>paper_authors: Etienne Naude, Henry Gann, Balaram Panda, Lance Zhang, Raina Song, Yuwei Shen</li>
<li>for: 这个研究旨在调查计算机是否能够根据人类表达的情感来进行不同任务。</li>
<li>methods: 这个研究使用了不同的uncertainty信息显示方式来影响人类决策过程。</li>
<li>results: 研究发现，显示更多的uncertainty信息可以帮助用户更自信地做出决策。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Many research explore how well computers are able to examine emotions displayed by humans and use that data to perform different tasks. However, there have been very few research which evaluate the computers ability to generate emotion classification information in an attempt to help the user make decisions or perform tasks. This is a crucial area to explore as it is paramount to the two way communication between humans and computers. This research conducted an experiment to investigate the impact of different uncertainty information displays of emotion classification on the human decision making process. Results show that displaying more uncertainty information can help users to be more confident when making decisions.
</details>
<details>
<summary>摘要</summary>
很多研究都在研究计算机如何识别人类表达的情感，并使用这些数据来完成不同的任务。然而，有很少的研究探讨计算机是否能够生成情感分类信息，以帮助用户做出决策或完成任务。这是一个关键的领域，因为两个方向的人机交互是非常重要的。本研究进行了一项实验，以调查不同的不确定信息显示方式对人类决策过程的影响。结果显示，显示更多的不确定信息可以帮助用户更加自信地做出决策。
</details></li>
</ul>
<hr>
<h2 id="Gentopia-A-Collaborative-Platform-for-Tool-Augmented-LLMs"><a href="#Gentopia-A-Collaborative-Platform-for-Tool-Augmented-LLMs" class="headerlink" title="Gentopia: A Collaborative Platform for Tool-Augmented LLMs"></a>Gentopia: A Collaborative Platform for Tool-Augmented LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04030">http://arxiv.org/abs/2308.04030</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gentopia-ai/gentopia">https://github.com/gentopia-ai/gentopia</a></li>
<li>paper_authors: Binfeng Xu, Xukun Liu, Hua Shen, Zeyu Han, Yuhan Li, Murong Yue, Zhiyuan Peng, Yuchen Liu, Ziyu Yao, Dongkuan Xu</li>
<li>For: The paper aims to provide a flexible and customizable framework for Augmented Language Models (ALMs) that enables the use of various language models, task formats, prompting modules, and plugins.* Methods: The paper proposes a new framework called gentopia, which allows users to customize their ALMs through simple configurations and integrates various language models, task formats, prompting modules, and plugins into a unified paradigm.* Results: The paper establishes gentpool, a public platform for registering and sharing user-customized agents, and gentbench, an integral component of gentpool that evaluates user-customized agents across diverse aspects such as safety, robustness, and efficiency.<details>
<summary>Abstract</summary>
Augmented Language Models (ALMs) empower large language models with the ability to use tools, transforming them into intelligent agents for real-world interactions. However, most existing frameworks for ALMs, to varying degrees, are deficient in the following critical features: flexible customization, collaborative democratization, and holistic evaluation. We present gentopia, an ALM framework enabling flexible customization of agents through simple configurations, seamlessly integrating various language models, task formats, prompting modules, and plugins into a unified paradigm. Furthermore, we establish gentpool, a public platform enabling the registration and sharing of user-customized agents. Agents registered in gentpool are composable such that they can be assembled together for agent collaboration, advancing the democratization of artificial intelligence. To ensure high-quality agents, gentbench, an integral component of gentpool, is designed to thoroughly evaluate user-customized agents across diverse aspects such as safety, robustness, efficiency, etc. We release gentopia on Github and will continuously move forward.
</details>
<details>
<summary>摘要</summary>
基于扩展语言模型（ALM）的框架，gentopia，允许大语言模型使用工具，将其转变成智能代理人进行实际交互。然而，现有的ALM框架，各有不同程度的缺失，包括灵活定制、合作民主化和整体评估。我们提出了gentopia框架，允许用户通过简单的配置来自定义代理人，并允许不同的语言模型、任务格式、提示模块和插件在一个统一的架构中协作。此外，我们建立了gentpool公共平台，让用户可以注册和分享自定义代理人。gentpool中注册的代理人可以组合起来，推动人工智能的民主化。为保证高质量代理人，gentbench，gentpool的一个重要组件，专门用于评估用户自定义代理人的多个方面，包括安全、稳定性、效率等。我们将gentopia发布到Github，并将持续推进。
</details></li>
</ul>
<hr>
<h2 id="Top-K-Relevant-Passage-Retrieval-for-Biomedical-Question-Answering"><a href="#Top-K-Relevant-Passage-Retrieval-for-Biomedical-Question-Answering" class="headerlink" title="Top K Relevant Passage Retrieval for Biomedical Question Answering"></a>Top K Relevant Passage Retrieval for Biomedical Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04028">http://arxiv.org/abs/2308.04028</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shashank140195/Biomedical_QA_Model">https://github.com/shashank140195/Biomedical_QA_Model</a></li>
<li>paper_authors: Shashank Gupta</li>
<li>for: 这个论文的目的是提高生物医学问答系统的精度，使其能够更正确地回答生物医学相关的问题。</li>
<li>methods: 这个论文使用了现有的Dense Passage Retrieval（DPR）框架，并对其进行了微调，以便在生物医学领域中应用。具体来说，他们使用了Pubmed文献来回答医学问题。</li>
<li>results: 经过微调后，这个DPR模型在BioASQ问答 dataset上得到了0.81的F1分数，表明其能够准确地回答生物医学相关的问题。<details>
<summary>Abstract</summary>
Question answering is a task that answers factoid questions using a large collection of documents. It aims to provide precise answers in response to the user's questions in natural language. Question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. On the web, there is no single article that could provide all the possible answers available on the internet to the question of the problem asked by the user. The existing Dense Passage Retrieval model has been trained on Wikipedia dump from Dec. 20, 2018, as the source documents for answering questions. Question answering (QA) has made big strides with several open-domain and machine comprehension systems built using large-scale annotated datasets. However, in the clinical domain, this problem remains relatively unexplored. According to multiple surveys, Biomedical Questions cannot be answered correctly from Wikipedia Articles. In this work, we work on the existing DPR framework for the biomedical domain and retrieve answers from the Pubmed articles which is a reliable source to answer medical questions. When evaluated on a BioASQ QA dataset, our fine-tuned dense retriever results in a 0.81 F1 score.
</details>
<details>
<summary>摘要</summary>
问答任务是回答基于大量文档的问题，目的是通过自然语言提供精确的答案。问答依赖于高效的段 Retrieval，传统的稀疏 вектор空间模型，如 TF-IDF 或 BM25，是现实中的标准方法。在互联网上，没有一篇文章可以提供用户问题的所有可能的答案。现有的 dense passage retrieval 模型已经在 Dec. 20, 2018 的Wikipedia dump上进行了训练，作为回答问题的源文档。问答（QA）在开放领域和机器理解领域已经做出了大量的进展，但在医疗领域，这个问题还很少研究。根据多个调查，医学问题无法从 Wikipedia 文章中正确地回答。在这种情况下，我们在现有的 DPR 框架上进行了改进，并从可靠的 Pubmed 文章中提取答案。当评估在 BioASQ QA 数据集上时，我们的精制 dense retriever 得分为 0.81 F1 分。
</details></li>
</ul>
<hr>
<h2 id="AgentSims-An-Open-Source-Sandbox-for-Large-Language-Model-Evaluation"><a href="#AgentSims-An-Open-Source-Sandbox-for-Large-Language-Model-Evaluation" class="headerlink" title="AgentSims: An Open-Source Sandbox for Large Language Model Evaluation"></a>AgentSims: An Open-Source Sandbox for Large Language Model Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04026">http://arxiv.org/abs/2308.04026</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaju Lin, Haoran Zhao, Aochi Zhang, Yiting Wu, Huqiuyue Ping, Qin Chen</li>
<li>for: 评估大语言模型（LLM）的能力是一个公开的问题，因为现有的评估方法受到以下缺点的限制：（1）受限的评估能力，（2）易受到攻击的标准套件，（3）不具有客观的度量。</li>
<li>methods: 我们建议使用任务基本评估方法，即让LLM代理在模拟环境中完成任务，这是一个一 Size fits all的解决方案，可以解决上述问题。我们提供了 AgentSims，一个易于使用的基础设施，它可以帮助研究人员从不同领域测试他们感兴趣的具体能力。研究人员可以通过点击 GUI 或输入几行代码来构建评估任务和添加代理，以及测试新的支持机制，如记忆、规划和工具使用系统。</li>
<li>results: 我们的示例可以在 <a target="_blank" rel="noopener" href="https://agentsims.com/">https://agentsims.com</a> 上查看。<details>
<summary>Abstract</summary>
With ChatGPT-like large language models (LLM) prevailing in the community, how to evaluate the ability of LLMs is an open question. Existing evaluation methods suffer from following shortcomings: (1) constrained evaluation abilities, (2) vulnerable benchmarks, (3) unobjective metrics. We suggest that task-based evaluation, where LLM agents complete tasks in a simulated environment, is a one-for-all solution to solve above problems. We present AgentSims, an easy-to-use infrastructure for researchers from all disciplines to test the specific capacities they are interested in. Researchers can build their evaluation tasks by adding agents and buildings on an interactive GUI or deploy and test new support mechanisms, i.e. memory, planning and tool-use systems, by a few lines of codes. Our demo is available at https://agentsims.com .
</details>
<details>
<summary>摘要</summary>
具有chatGPT大语言模型（LLM）的社区中，评估这些模型的能力是一个公开的问题。现有的评估方法受到以下缺点：（1）受限的评价能力，（2）易受到攻击的标准，（3）不准确的度量。我们建议使用任务基本评估，让LLM代理在模拟环境中完成任务，作为一个一元解决方案。我们提供了 AgentSims，一个易于使用的基础设施，让研究人员从各个领域测试他们感兴趣的具体能力。研究人员可以通过在交互式GUI上添加代理和建筑，或者通过几行代码来部署和测试新的支持机制，如记忆、规划和工具使用系统。我们的 demo 可以在 <https://agentsims.com> 上查看。
</details></li>
</ul>
<hr>
<h2 id="MSAC-Multiple-Speech-Attribute-Control-Method-for-Speech-Emotion-Recognition"><a href="#MSAC-Multiple-Speech-Attribute-Control-Method-for-Speech-Emotion-Recognition" class="headerlink" title="MSAC: Multiple Speech Attribute Control Method for Speech Emotion Recognition"></a>MSAC: Multiple Speech Attribute Control Method for Speech Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04025">http://arxiv.org/abs/2308.04025</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Pan</li>
<li>for: 本研究旨在探讨speech emotion recognition（SER）方法的可靠性，并研究如何从多个speech attribute的分布角度来模型speech emotion。</li>
<li>methods: 本研究提出了一种基于CNN的新型SER模型，采用了添加性marginsoftmax损失函数来提高类别间特征之间的距离，从而提高分类的精度。此外，提出了一种多种speech attribute控制方法MSAC，可以Explicitly控制speech attribute，使模型免受情绪无关的attribute的影响，捕捉更细腻的情绪相关特征。</li>
<li>results: 对于单个corpus和跨corpus的SER场景，我们的提出的SER工作流程在recognition、generalization和可靠性性方面均表现出优于基eline。单个corpusSER场景中，我们的SER工作流程取得了72.97%的WAR和71.76%的UAR在IEMOCAP corpus上。<details>
<summary>Abstract</summary>
Despite significant progress, speech emotion recognition (SER) remains challenging due to inherent complexity and ambiguity of the emotion attribute, particularly in wild world. Whereas current studies primarily focus on recognition and generalization capabilities, this work pioneers an exploration into the reliability of SER methods and investigates how to model the speech emotion from the aspect of data distribution across various speech attributes. Specifically, we first build a novel CNN-based SER model which adopts additive margin softmax loss to expand the distance between features of different classes, thereby enhancing their discrimination. Second, a novel multiple speech attribute control method MSAC is proposed to explicitly control speech attributes, enabling the model to be less affected by emotion-agnostic attributes and capture more fine-grained emotion-related features. Third, we make a first attempt to test and analyze the reliability of the proposed SER workflow using the out-of-distribution detection method. Extensive experiments on both single and cross-corpus SER scenarios show that our proposed unified SER workflow consistently outperforms the baseline in terms of recognition, generalization, and reliability performance. Besides, in single-corpus SER, the proposed SER workflow achieves superior recognition results with a WAR of 72.97\% and a UAR of 71.76\% on the IEMOCAP corpus.
</details>
<details>
<summary>摘要</summary>
尽管有了 significative progress，speech emotion recognition（SER）仍然具有挑战性，主要是因为情感属性的内在复杂和不确定性，特别是在野外环境中。而现有研究主要关注 reconocimiento y generalización capacidades，这个工作则探索了SER方法的可靠性，并 investigate了如何从数据分布角度来模型speech emotion。 Specifically，我们首先构建了一个基于CNN的SER模型，采用了添加式margin softmax损失函数，以增强不同类别之间的距离，从而提高它们的区分度。其次，我们提出了一种 Multiple Speech Attribute Control（MSAC）方法，以控制speech attribute，使模型免受情感无关的属性的影响，捕捉更细腻的情感相关特征。 Finally，我们对提出的SER工作流进行了首次测试和分析，并在单个corpus和交叉corpus中进行了广泛的实验。结果表明，我们的提出的SER工作流在认知、泛化和可靠性方面均有显著的优异性。此外，在单个corpus中，我们的SER工作流在IEMOCAP corpus上 achievable 的recognition结果为72.97%和71.76%。
</details></li>
</ul>
<hr>
<h2 id="Scope-Loss-for-Imbalanced-Classification-and-RL-Exploration"><a href="#Scope-Loss-for-Imbalanced-Classification-and-RL-Exploration" class="headerlink" title="Scope Loss for Imbalanced Classification and RL Exploration"></a>Scope Loss for Imbalanced Classification and RL Exploration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04024">http://arxiv.org/abs/2308.04024</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hasham Burhani, Xiao Qi Shi, Jonathan Jaegerman, Daniel Balicki</li>
<li>for: This paper aims to address the exploration-exploitation trade-off in reinforcement learning and the dataset imbalance problem in supervised classification.</li>
<li>methods: The paper equates the two problems and derives a novel loss function called Scope Loss, which adjusts gradients to prevent performance losses from over-exploitation and dataset imbalances without the need for tuning.</li>
<li>results: The paper shows that Scope Loss outperforms state-of-the-art loss functions over a basket of benchmark reinforcement learning tasks and a skewed classification dataset.<details>
<summary>Abstract</summary>
We demonstrate equivalence between the reinforcement learning problem and the supervised classification problem. We consequently equate the exploration exploitation trade-off in reinforcement learning to the dataset imbalance problem in supervised classification, and find similarities in how they are addressed. From our analysis of the aforementioned problems we derive a novel loss function for reinforcement learning and supervised classification. Scope Loss, our new loss function, adjusts gradients to prevent performance losses from over-exploitation and dataset imbalances, without the need for any tuning. We test Scope Loss against SOTA loss functions over a basket of benchmark reinforcement learning tasks and a skewed classification dataset, and show that Scope Loss outperforms other loss functions.
</details>
<details>
<summary>摘要</summary>
我们证明了回归学习问题与supervised分类问题之间的等价性。我们因此将rek-exploration偏好和数据偏好问题相提并论，并发现它们在解决方面存在相似之处。基于这些问题的分析，我们提出了一种新的损失函数，称为Scope损失。Scope损失函数可以适应找到潜在的性能损失和数据偏好问题，无需任何调整。我们对一组标准回归学习任务和一个偏好分类 dataset进行测试，并证明Scope损失函数在与现状最优损失函数进行比较时表现出色。
</details></li>
</ul>
<hr>
<h2 id="Improving-Performance-of-Semi-Supervised-Learning-by-Adversarial-Attacks"><a href="#Improving-Performance-of-Semi-Supervised-Learning-by-Adversarial-Attacks" class="headerlink" title="Improving Performance of Semi-Supervised Learning by Adversarial Attacks"></a>Improving Performance of Semi-Supervised Learning by Adversarial Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04018">http://arxiv.org/abs/2308.04018</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongyoon Yang, Kunwoong Kim, Yongdai Kim</li>
<li>for: 提高现有的隐私学习（SSL）算法性能</li>
<li>methods: 利用对预训练模型的 adversarial 攻击选择高自信度无标记数据进行标注</li>
<li>results: 在 CIFAR10 上，与 SCAR 结合的三种 latest SSL algorithms 显示出显著提高图像分类性能<details>
<summary>Abstract</summary>
Semi-supervised learning (SSL) algorithm is a setup built upon a realistic assumption that access to a large amount of labeled data is tough. In this study, we present a generalized framework, named SCAR, standing for Selecting Clean samples with Adversarial Robustness, for improving the performance of recent SSL algorithms. By adversarially attacking pre-trained models with semi-supervision, our framework shows substantial advances in classifying images. We introduce how adversarial attacks successfully select high-confident unlabeled data to be labeled with current predictions. On CIFAR10, three recent SSL algorithms with SCAR result in significantly improved image classification.
</details>
<details>
<summary>摘要</summary>
半supervised learning（SSL）算法是基于现实的假设，即获得大量标注数据很Difficult。在这种研究中，我们提出一种普适的框架，名为SCAR，即选择干净样本并具有对抗性强度，以提高 latest SSL算法的性能。通过对预训练模型进行对抗性攻击，我们的框架成功地选择高自信产生的无标注样本进行标注。在CIFAR10上，三种latest SSL算法与SCAR结果显著改善图像分类。
</details></li>
</ul>
<hr>
<h2 id="Multi-Granularity-Attention-Model-for-Group-Recommendation"><a href="#Multi-Granularity-Attention-Model-for-Group-Recommendation" class="headerlink" title="Multi-Granularity Attention Model for Group Recommendation"></a>Multi-Granularity Attention Model for Group Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04017">http://arxiv.org/abs/2308.04017</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianye Ji, Jiayan Pei, Shaochuan Lin, Taotao Zhou, Hengxu He, Jia Jia, Ning Hu</li>
<li>for: 提供个性化推荐给多个用户组 based on their shared interests, preferences, and characteristics.</li>
<li>methods: 使用多级别的granularity (i.e., subsets, groups, and supersets) to uncover group members’ latent preferences and mitigate recommendation noise.  Specifically, our method includes a Subset Preference Extraction module, a Group Preference Extraction module, and a Superset Preference Extraction module.</li>
<li>results: 在多个级别的granularity上减少推荐噪音，并全面学习用户的个性兴趣. Extensive offline and online experiments have demonstrated the superiority of our method in terms of performance.<details>
<summary>Abstract</summary>
Group recommendation provides personalized recommendations to a group of users based on their shared interests, preferences, and characteristics. Current studies have explored different methods for integrating individual preferences and making collective decisions that benefit the group as a whole. However, most of them heavily rely on users with rich behavior and ignore latent preferences of users with relatively sparse behavior, leading to insufficient learning of individual interests. To address this challenge, we present the Multi-Granularity Attention Model (MGAM), a novel approach that utilizes multiple levels of granularity (i.e., subsets, groups, and supersets) to uncover group members' latent preferences and mitigate recommendation noise. Specially, we propose a Subset Preference Extraction module that enhances the representation of users' latent subset-level preferences by incorporating their previous interactions with items and utilizing a hierarchical mechanism. Additionally, our method introduces a Group Preference Extraction module and a Superset Preference Extraction module, which explore users' latent preferences on two levels: the group-level, which maintains users' original preferences, and the superset-level, which includes group-group exterior information. By incorporating the subset-level embedding, group-level embedding, and superset-level embedding, our proposed method effectively reduces group recommendation noise across multiple granularities and comprehensively learns individual interests. Extensive offline and online experiments have demonstrated the superiority of our method in terms of performance.
</details>
<details>
<summary>摘要</summary>
群体推荐提供个性化的推荐给群体成员基于他们共同的兴趣、偏好和特征。现有研究已经探索了不同的方法来集成个体偏好并为群体作出共同的决策，但大多数情况都忽略了用户的潜在偏好，导致个体兴趣的学习不够。为解决这个挑战，我们提出了多级别注意力模型（MGAM），一种新的方法，利用不同级别的划分（i.e., 子集、组和超集）来探索群体成员的潜在偏好并减少推荐噪音。具体来说，我们提出了一个子集偏好提取模块，通过利用用户对物品的前期互动和层次机制来强化用户的潜在子集级别偏好的表示。此外，我们的方法还引入了组偏好提取模块和超集偏好提取模块，它们分别探索用户的组级别偏好和超集级别偏好。通过结合子集级别嵌入、组级别嵌入和超集级别嵌入，我们提出的方法可以有效减少群体推荐噪音并全面学习个体兴趣。经过大量的线上和线下实验，我们的方法在性能方面表现出了明显的优势。
</details></li>
</ul>
<hr>
<h2 id="Understanding-CNN-Hidden-Neuron-Activations-Using-Structured-Background-Knowledge-and-Deductive-Reasoning"><a href="#Understanding-CNN-Hidden-Neuron-Activations-Using-Structured-Background-Knowledge-and-Deductive-Reasoning" class="headerlink" title="Understanding CNN Hidden Neuron Activations Using Structured Background Knowledge and Deductive Reasoning"></a>Understanding CNN Hidden Neuron Activations Using Structured Background Knowledge and Deductive Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03999">http://arxiv.org/abs/2308.03999</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abhilekha-dalal/xai-using-wikidataAndEcii">https://github.com/abhilekha-dalal/xai-using-wikidataAndEcii</a></li>
<li>paper_authors: Abhilekha Dalal, Md Kamruzzaman Sarker, Adrita Barua, Eugene Vasserman, Pascal Hitzler</li>
<li>for: 这篇论文的目的是解释深度学习系统中隐藏层neuron的活动，以提供系统内部检测输入的相关信息，从而减轻深度学习系统的黑盒效应。</li>
<li>methods: 这篇论文使用了大规模背景知识（约200万类）和基于描述逻辑的符号推理方法 called Concept Induction，原本设计用于semantic web领域。这种方法可以自动将大规模背景知识链接到 convolutional neural network 中 dense layer 中的各个神经元，并通过假设和验证过程提供有意义的标签。</li>
<li>results: 研究结果表明，这种方法可以自动地将大规模背景知识链接到 convolutional neural network 中 dense layer 中的各个神经元，并提供有意义的标签。这些标签可以帮助解释深度学习系统中隐藏层neuron的活动，从而减轻深度学习系统的黑盒效应。<details>
<summary>Abstract</summary>
A major challenge in Explainable AI is in correctly interpreting activations of hidden neurons: accurate interpretations would provide insights into the question of what a deep learning system has internally detected as relevant on the input, demystifying the otherwise black-box character of deep learning systems. The state of the art indicates that hidden node activations can, in some cases, be interpretable in a way that makes sense to humans, but systematic automated methods that would be able to hypothesize and verify interpretations of hidden neuron activations are underexplored. In this paper, we provide such a method and demonstrate that it provides meaningful interpretations. Our approach is based on using large-scale background knowledge approximately 2 million classes curated from the Wikipedia concept hierarchy together with a symbolic reasoning approach called Concept Induction based on description logics, originally developed for applications in the Semantic Web field. Our results show that we can automatically attach meaningful labels from the background knowledge to individual neurons in the dense layer of a Convolutional Neural Network through a hypothesis and verification process.
</details>
<details>
<summary>摘要</summary>
一个主要挑战在可解释人工智能是正确地解释隐藏神经元的活动：正确的解释会提供关于deep learning系统内部检测到的输入信息的深入了解，从而消除深度学习系统的黑盒特性。现状的最佳实践表明，隐藏节点的活动可以，在某些情况下，被解释得通常是人类可理解的，但系统化的自动方法，能够假设和验证解释隐藏神经元的活动，尚未得到充分的探索。在这篇论文中，我们提供了一种这样的方法，并证明它可以提供有意义的解释。我们的方法基于使用大规模的背景知识（约200万个类别），来自Wikipedia概念层次结构，以及基于描述逻辑的符号推理方法 called Concept Induction，原始是为Semantic Web领域开发的。我们的结果表明，我们可以通过一种假设和验证过程，自动将background知识中的有意义标签附加到 convolutional neural network 的紧凑层中的个体神经元。
</details></li>
</ul>
<hr>
<h2 id="Cooperative-Multi-Type-Multi-Agent-Deep-Reinforcement-Learning-for-Resource-Management-in-Space-Air-Ground-Integrated-Networks"><a href="#Cooperative-Multi-Type-Multi-Agent-Deep-Reinforcement-Learning-for-Resource-Management-in-Space-Air-Ground-Integrated-Networks" class="headerlink" title="Cooperative Multi-Type Multi-Agent Deep Reinforcement Learning for Resource Management in Space-Air-Ground Integrated Networks"></a>Cooperative Multi-Type Multi-Agent Deep Reinforcement Learning for Resource Management in Space-Air-Ground Integrated Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03995">http://arxiv.org/abs/2308.03995</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hengxi Zhang, Huaze Tang, Wenbo Ding, Xiao-Ping Zhang</li>
<li>for: 这篇论文的目的是提出一个包含多种通讯链接的Space-Air-Ground Integrated Network（SAGIN）系统，并使用合作多型多代理人深度强化学习（CMT-MARL）方法来解决资源管理问题。</li>
<li>methods: 这篇论文使用了五种不同的通讯链接，并提出了一个有效的CMT-MARL方法来管理这些链接的资源。</li>
<li>results: 实验结果显示了CMT-MARL方法的有效性，包括总转送率和转送成功率等关键性能指标。这些结果证明了SAGIN系统的可能性和实现性。<details>
<summary>Abstract</summary>
The Space-Air-Ground Integrated Network (SAGIN), integrating heterogeneous devices including low earth orbit (LEO) satellites, unmanned aerial vehicles (UAVs), and ground users (GUs), holds significant promise for advancing smart city applications. However, resource management of the SAGIN is a challenge requiring urgent study in that inappropriate resource management will cause poor data transmission, and hence affect the services in smart cities. In this paper, we develop a comprehensive SAGIN system that encompasses five distinct communication links and propose an efficient cooperative multi-type multi-agent deep reinforcement learning (CMT-MARL) method to address the resource management issue. The experimental results highlight the efficacy of the proposed CMT-MARL, as evidenced by key performance indicators such as the overall transmission rate and transmission success rate. These results underscore the potential value and feasibility of future implementation of the SAGIN.
</details>
<details>
<summary>摘要</summary>
Space-Air-Ground  интеegrated Network (SAGIN)，融合各种不同设备，包括低地球轨道卫星（LEO）、无人飞行器（UAV）和地面用户（GU），具有推动智能城市应用的巨大潜力。然而，SAGIN资源管理是一项需要紧迫研究的挑战，因为不当的资源管理会导致数据传输差，从而影响智能城市服务的质量。在这篇论文中，我们提出了一个全面的 SAGIN 系统，包括五种不同的通信链接，并提出了一种高效的合作多种多代理人深度学习（CMT-MARL）方法来解决资源管理问题。实验结果表明，提议的 CMT-MARL 方法能够减少数据传输差和提高传输成功率，这些结果证明了 SAGIN 的可能性和实现性。
</details></li>
</ul>
<hr>
<h2 id="AI-Chatbots-as-Multi-Role-Pedagogical-Agents-Transforming-Engagement-in-CS-Education"><a href="#AI-Chatbots-as-Multi-Role-Pedagogical-Agents-Transforming-Engagement-in-CS-Education" class="headerlink" title="AI Chatbots as Multi-Role Pedagogical Agents: Transforming Engagement in CS Education"></a>AI Chatbots as Multi-Role Pedagogical Agents: Transforming Engagement in CS Education</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03992">http://arxiv.org/abs/2308.03992</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cassie Chen Cao, Zijian Ding, Jionghao Lin, Frank Hopfgartner<br>for:这项研究旨在利用人工智能（AI）搭载的多角色 чат bot 来提高计算机科学教育的学习经验和参与度。methods:我们采用了设计基本研究方法，开发、实现和评估一个具有四个不同 чат bot 角色的学习环境，这些角色基于自主决定理论，满足学生的三种 innate 心理需求 - 能力、自主和相互关系。results:我们在高等教育上下文中进行了一个月的测试，征得 200 名学生的参与，并与人教和单个 чат bot 的条件进行比较。我们的研究采用了混合方法，包括量化测量如 chat log 序列分析，以及讨论和问卷调查。通过结合 cutting-edge 自然语言处理技术如话题分析和情感分析，我们提供了深入的理解系统对学生参与度、动机和问题解决方面的影响。<details>
<summary>Abstract</summary>
This study investigates the use of Artificial Intelligence (AI)-powered, multi-role chatbots as a means to enhance learning experiences and foster engagement in computer science education. Leveraging a design-based research approach, we develop, implement, and evaluate a novel learning environment enriched with four distinct chatbot roles: Instructor Bot, Peer Bot, Career Advising Bot, and Emotional Supporter Bot. These roles, designed around the tenets of Self-Determination Theory, cater to the three innate psychological needs of learners - competence, autonomy, and relatedness. Additionally, the system embraces an inquiry-based learning paradigm, encouraging students to ask questions, seek solutions, and explore their curiosities.   We test this system in a higher education context over a period of one month with 200 participating students, comparing outcomes with conditions involving a human tutor and a single chatbot. Our research utilizes a mixed-methods approach, encompassing quantitative measures such as chat log sequence analysis, and qualitative methods including surveys and focus group interviews. By integrating cutting-edge Natural Language Processing techniques such as topic modelling and sentiment analysis, we offer an in-depth understanding of the system's impact on learner engagement, motivation, and inquiry-based learning.   This study, through its rigorous design and innovative approach, provides significant insights into the potential of AI-empowered, multi-role chatbots in reshaping the landscape of computer science education and fostering an engaging, supportive, and motivating learning environment.
</details>
<details>
<summary>摘要</summary>
We test the system in a higher education context for one month with 200 participating students, comparing outcomes with conditions involving a human tutor and a single chatbot. Our research combines quantitative measures such as chat log sequence analysis and qualitative methods like surveys and focus group interviews. We employ cutting-edge Natural Language Processing techniques like topic modeling and sentiment analysis to gain a deeper understanding of the system's impact on learner engagement, motivation, and inquiry-based learning.Our study offers significant insights into the potential of AI-empowered, multi-role chatbots to reshape computer science education and create an engaging, supportive, and motivating learning environment. By integrating innovative approaches and cutting-edge technologies, we provide a comprehensive understanding of the system's effectiveness and its potential for future applications.
</details></li>
</ul>
<hr>
<h2 id="NEOLAF-an-LLM-powered-neural-symbolic-cognitive-architecture"><a href="#NEOLAF-an-LLM-powered-neural-symbolic-cognitive-architecture" class="headerlink" title="NEOLAF, an LLM-powered neural-symbolic cognitive architecture"></a>NEOLAF, an LLM-powered neural-symbolic cognitive architecture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03990">http://arxiv.org/abs/2308.03990</a></li>
<li>repo_url: None</li>
<li>paper_authors: Richard Jiarui Tong, Cassie Chen Cao, Timothy Xueqian Lee, Guodong Zhao, Ray Wan, Feiyue Wang, Xiangen Hu, Robin Schmucker, Jinsheng Pan, Julian Quevedo, Yu Lu</li>
<li>for: 这篇论文旨在构建一个智能代理人，用于解决复杂的数学问题。</li>
<li>methods: 该论文提出了一种基于神经网络和符号学的聪明架构，名为NEOLAF，可以模型和构建智能代理人。NEOLAF架构具有可解释性、逐步学习、高效性、协作和分布式学习、人工智能在循环中启用、自我改进等优点。</li>
<li>results: 在使用MATH数据集上进行的实验表明，NEOLAF代理人具有出色的学习能力，并且有可能革新认知架构和自我改进的教学系统。<details>
<summary>Abstract</summary>
This paper presents the Never Ending Open Learning Adaptive Framework (NEOLAF), an integrated neural-symbolic cognitive architecture that models and constructs intelligent agents. The NEOLAF framework is a superior approach to constructing intelligent agents than both the pure connectionist and pure symbolic approaches due to its explainability, incremental learning, efficiency, collaborative and distributed learning, human-in-the-loop enablement, and self-improvement. The paper further presents a compelling experiment where a NEOLAF agent, built as a problem-solving agent, is fed with complex math problems from the open-source MATH dataset. The results demonstrate NEOLAF's superior learning capability and its potential to revolutionize the field of cognitive architectures and self-improving adaptive instructional systems.
</details>
<details>
<summary>摘要</summary>
Translation Notes:* "Never Ending Open Learning Adaptive Framework" (NEOLAF) is translated as "无止境开放学习适应框架" (Wú zhì jìng kāifàng xuéxí suīyìng kāngyì)* "pure connectionist" is translated as "纯连接主义" (chún liánxì zhǔyì)* "pure symbolic" is translated as "纯符号主义" (chún fúhào zhǔyì)* "explainability" is translated as "可解释性" (kějìexplainability)* "incremental learning" is translated as "逐步学习" (jìbù xuéxí)* "efficiency" is translated as "效率" (fùliàng)* "collaborative and distributed learning" is translated as "合作分布式学习" (hèzuò fēnzhèng zhīxíng xuéxí)* "human-in-the-loop enablement" is translated as "人在循环启用" (rén zài xiànglún kāi yòng)* "self-improvement" is translated as "自我改进" (zìwǒ gǎi jìn)
</details></li>
</ul>
<hr>
<h2 id="SimplyRetrieve-A-Private-and-Lightweight-Retrieval-Centric-Generative-AI-Tool"><a href="#SimplyRetrieve-A-Private-and-Lightweight-Retrieval-Centric-Generative-AI-Tool" class="headerlink" title="SimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI Tool"></a>SimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI Tool</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03983">http://arxiv.org/abs/2308.03983</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rcgai/simplyretrieve">https://github.com/rcgai/simplyretrieve</a></li>
<li>paper_authors: Youyang Ng, Daisuke Miyashita, Yasuto Hoshi, Yasuhiro Morioka, Osamu Torii, Tomoya Kodama, Jun Deguchi</li>
<li>for: 这篇论文旨在探讨如何通过私有数据和公共可用的生成AI系统之间的集成，以提高生成AI的性能而不需要额外的模型微调。</li>
<li>methods: 该论文使用了 Retrieval-Centric Generation（RCG）方法，其中分离了LLM和检索器在上下文理解和知识储存中的角色，从而可能导致更高效的实现。</li>
<li>results: 该论文介绍了一个开源的GUI和API基于RCG平台，名为SimplyRetrieve，它具有本地化、轻量级和用户友好的界面，可以帮助机器学习社区更好地利用这些高级技术。<details>
<summary>Abstract</summary>
Large Language Model (LLM) based Generative AI systems have seen significant progress in recent years. Integrating a knowledge retrieval architecture allows for seamless integration of private data into publicly available Generative AI systems using pre-trained LLM without requiring additional model fine-tuning. Moreover, Retrieval-Centric Generation (RCG) approach, a promising future research direction that explicitly separates roles of LLMs and retrievers in context interpretation and knowledge memorization, potentially leads to more efficient implementation. SimplyRetrieve is an open-source tool with the goal of providing a localized, lightweight, and user-friendly interface to these sophisticated advancements to the machine learning community. SimplyRetrieve features a GUI and API based RCG platform, assisted by a Private Knowledge Base Constructor and a Retrieval Tuning Module. By leveraging these capabilities, users can explore the potential of RCG for improving generative AI performance while maintaining privacy standards. The tool is available at https://github.com/RCGAI/SimplyRetrieve with an MIT license.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="CheXFusion-Effective-Fusion-of-Multi-View-Features-using-Transformers-for-Long-Tailed-Chest-X-Ray-Classification"><a href="#CheXFusion-Effective-Fusion-of-Multi-View-Features-using-Transformers-for-Long-Tailed-Chest-X-Ray-Classification" class="headerlink" title="CheXFusion: Effective Fusion of Multi-View Features using Transformers for Long-Tailed Chest X-Ray Classification"></a>CheXFusion: Effective Fusion of Multi-View Features using Transformers for Long-Tailed Chest X-Ray Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03968">http://arxiv.org/abs/2308.03968</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongkyun Kim</li>
<li>for: 这份论文是为了解决医疗影像分类中的长尾分布、诊断发现之间的共存、以及每个研究或病人可以提供多个视角的问题。</li>
<li>methods: 这份论文提出了一个基于对称融合模组的解决方案，称为CheXFusion，可以有效地聚合多个视角特征，并考虑预测结果的共存关系。这个模组利用自我注意和跨视角注意机制来有效地聚合多个视角特征。此外，论文还探讨了资料平衡和自我训练方法来优化模型的性能。</li>
<li>results: 这份论文的解决方案在MIMIC-CXR测试集上取得了0.372 mAP的成绩，在竞赛中排名第一。这表明了考虑多个视角、类别不均匀和预测结果的共存关系在医疗影像分类中的重要性。论文的代码可以在<a target="_blank" rel="noopener" href="https://github.com/dongkyuk/CXR-LT-public-solution%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/dongkyuk/CXR-LT-public-solution上获取。</a><details>
<summary>Abstract</summary>
Medical image classification poses unique challenges due to the long-tailed distribution of diseases, the co-occurrence of diagnostic findings, and the multiple views available for each study or patient. This paper introduces our solution to the ICCV CVAMD 2023 Shared Task on CXR-LT: Multi-Label Long-Tailed Classification on Chest X-Rays. Our approach introduces CheXFusion, a transformer-based fusion module incorporating multi-view images. The fusion module, guided by self-attention and cross-attention mechanisms, efficiently aggregates multi-view features while considering label co-occurrence. Furthermore, we explore data balancing and self-training methods to optimize the model's performance. Our solution achieves state-of-the-art results with 0.372 mAP in the MIMIC-CXR test set, securing 1st place in the competition. Our success in the task underscores the significance of considering multi-view settings, class imbalance, and label co-occurrence in medical image classification. Public code is available at https://github.com/dongkyuk/CXR-LT-public-solution
</details>
<details>
<summary>摘要</summary>
医学图像分类面临独特挑战，这些挑战包括疾病的长尾分布、诊断发现的共处和每个案例或病人可以提供多个视图。本文介绍我们在ICCV CVAMD 2023 共同任务中的解决方案：多标签长尾分类在胸部X射线图像（CXR-LT）中。我们的方法引入了CheXFusion，一种基于变换器的融合模块，该模块通过自我注意和交叉注意机制有效地聚合多视图特征，同时考虑标签共处。此外，我们还探索了数据填充和自我训练方法来优化模型性能。我们的解决方案在MIMIC-CXR测试集上 achievement 0.372 mAP，在竞赛中获得了第一名，这 подтвержда了在医学图像分类中考虑多视图设置、类别不均衡和标签共处的重要性。我们的代码可以在https://github.com/dongkyuk/CXR-LT-public-solution 上获取。
</details></li>
</ul>
<hr>
<h2 id="ALFA-–-Leveraging-All-Levels-of-Feature-Abstraction-for-Enhancing-the-Generalization-of-Histopathology-Image-Classification-Across-Unseen-Hospitals"><a href="#ALFA-–-Leveraging-All-Levels-of-Feature-Abstraction-for-Enhancing-the-Generalization-of-Histopathology-Image-Classification-Across-Unseen-Hospitals" class="headerlink" title="ALFA – Leveraging All Levels of Feature Abstraction for Enhancing the Generalization of Histopathology Image Classification Across Unseen Hospitals"></a>ALFA – Leveraging All Levels of Feature Abstraction for Enhancing the Generalization of Histopathology Image Classification Across Unseen Hospitals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03936">http://arxiv.org/abs/2308.03936</a></li>
<li>repo_url: None</li>
<li>paper_authors: Milad Sikaroudi, Maryam Hosseini, Shahryar Rahnamayan, H. R. Tizhoosh</li>
<li>for: 提高图像分类的泛化性，使模型能够在不同的医院中提供更好的表现</li>
<li>methods: 使用扩展自我超级视图，并在不同的分布差异场景下进行自我超级视图，从而 derivatin invariant feature from training images，并使用域对齐模块来进一步提取抽象特征</li>
<li>results: 实验结果表明，提出的方法可以在不同的医院图像中提供更好的泛化性，并在不同的分布差异场景下进行更好的表现<details>
<summary>Abstract</summary>
We propose an exhaustive methodology that leverages all levels of feature abstraction, targeting an enhancement in the generalizability of image classification to unobserved hospitals. Our approach incorporates augmentation-based self-supervision with common distribution shifts in histopathology scenarios serving as the pretext task. This enables us to derive invariant features from training images without relying on training labels, thereby covering different abstraction levels. Moving onto the subsequent abstraction level, we employ a domain alignment module to facilitate further extraction of invariant features across varying training hospitals. To represent the highly specific features of participating hospitals, an encoder is trained to classify hospital labels, independent of their diagnostic labels. The features from each of these encoders are subsequently disentangled to minimize redundancy and segregate the features. This representation, which spans a broad spectrum of semantic information, enables the development of a model demonstrating increased robustness to unseen images from disparate distributions. Experimental results from the PACS dataset (a domain generalization benchmark), a synthetic dataset created by applying histopathology-specific jitters to the MHIST dataset (defining different domains with varied distribution shifts), and a Renal Cell Carcinoma dataset derived from four image repositories from TCGA, collectively indicate that our proposed model is adept at managing varying levels of image granularity. Thus, it shows improved generalizability when faced with new, out-of-distribution hospital images.
</details>
<details>
<summary>摘要</summary>
我们提出了一种涵盖所有水平的特征抽象方法，目的是提高图像分类的通用性，覆盖不同医院的不见图像。我们的方法通过在历史病理景象中添加自我超visuospatial alignment，使得在不需要训练标签的情况下 derivation invariant features from 训练图像。在接下来的层次，我们使用域Alignment模块来进一步提取不同医院的抽象特征。为了表示参与医院的特定特征，我们训练了一个Encoder来分类医院标签，不同于其诊断标签。从每个Encoder中提取的特征后，我们进行了拟合以避免重复性和分化特征。这种表示，覆盖了广泛的语义信息，使得我们提出的模型在面对新、未经见图像时显示出更好的通用性。实验结果来自PACS数据集（领域通用性标准 benchmark）、在应用特定于 histopathology 的扰动后生成的 sintethic 数据集以及来自TCGA的 Renal Cell Carcinoma 数据集，表明我们的模型在不同水平的图像粒度下具有更好的普适性。
</details></li>
</ul>
<hr>
<h2 id="Establishing-Trust-in-ChatGPT-BioMedical-Generated-Text-An-Ontology-Based-Knowledge-Graph-to-Validate-Disease-Symptom-Links"><a href="#Establishing-Trust-in-ChatGPT-BioMedical-Generated-Text-An-Ontology-Based-Knowledge-Graph-to-Validate-Disease-Symptom-Links" class="headerlink" title="Establishing Trust in ChatGPT BioMedical Generated Text: An Ontology-Based Knowledge Graph to Validate Disease-Symptom Links"></a>Establishing Trust in ChatGPT BioMedical Generated Text: An Ontology-Based Knowledge Graph to Validate Disease-Symptom Links</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03929">http://arxiv.org/abs/2308.03929</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Abdeen Hamed, Alessandro Crimi, Magdalena M. Misiak, Byung Suk Lee</li>
<li>for: 本研究的目的是使用ontology-based知识图构建医学文献和人工智能生成的内容，以分辨准确信息和未经验证的数据。</li>
<li>methods: 我们使用了疾病 ontology (DOID) 和症状 ontology (SYMP) 构建知识图，并使用了我们的事实检查算法和网络中心性度量来进行 GPT 疾病-症状链分析，以量化医学文献和人工智能生成的内容中的准确性。</li>
<li>results: 我们的结果表明，在比较不同的 ChatGPT 知识图和其相应的 PubMed 知识图时，发现了一些有趣的观察结果。例如，一些 ChatGPT 知识图中的连接数比 PubMed 知识图更多，而且一些 GPT 知识图的中心性度量更高，尤其是对于相互重叠的节点。这些结果表明了人工智能生成的内容中的未经验证知识的潜在价值，需要进一步验证。<details>
<summary>Abstract</summary>
Methods: Through an innovative approach, we construct ontology-based knowledge graphs from authentic medical literature and AI-generated content. Our goal is to distinguish factual information from unverified data. We compiled two datasets: one from biomedical literature using a "human disease and symptoms" query, and another generated by ChatGPT, simulating articles. With these datasets (PubMed and ChatGPT), we curated 10 sets of 250 abstracts each, selected randomly with a specific seed. Our method focuses on utilizing disease ontology (DOID) and symptom ontology (SYMP) to build knowledge graphs, robust mathematical models that facilitate unbiased comparisons. By employing our fact-checking algorithms and network centrality metrics, we conducted GPT disease-symptoms link analysis to quantify the accuracy of factual knowledge amid noise, hypotheses, and significant findings.   Results: The findings obtained from the comparison of diverse ChatGPT knowledge graphs with their PubMed counterparts revealed some interesting observations. While PubMed knowledge graphs exhibit a wealth of disease-symptom terms, it is surprising to observe that some ChatGPT graphs surpass them in the number of connections. Furthermore, some GPT graphs are demonstrating supremacy of the centrality scores, especially for the overlapping nodes. This striking contrast indicates the untapped potential of knowledge that can be derived from AI-generated content, awaiting verification. Out of all the graphs, the factual link ratio between any two graphs reached its peak at 60%.   Conclusions: An intriguing insight from our findings was the striking number of links among terms in the knowledge graph generated from ChatGPT datasets, surpassing some of those in its PubMed counterpart. This early discovery has prompted further investigation using universal network metrics to unveil the new knowledge the links may hold.
</details>
<details>
<summary>摘要</summary>
方法：通过创新的方法，我们从authentic医学文献和AI生成的内容中构建了ontology-based知识图。我们的目标是区分 фактической信息和未经证实的数据。我们编译了两个数据集：一个是生物医学文献，使用“人类疾病和症状”查询，另一个是由ChatGPT生成的文章。 With这两个数据集（PubMed和ChatGPT），我们精心审选了250个摘要，使用特定的种子值进行随机选择。我们的方法是利用疾病ontology（DOID）和症状ontology（SYMP）建立知识图，并使用我们的 фактиче性检查算法和网络中心度度量来进行GPT疾病-症状链接分析，以量化factual知识中的噪音、假设和重要发现。结果：对比多个ChatGPT知识图与其PubMed对应的知识图，我们发现了一些有趣的观察。PubMed知识图显示了丰富的疾病-症状 термина，但是某些ChatGPT graphs在连接数量方面超过了它们。此外，一些GPT graphs的中心度分数特别高，特别是在重叠的节点上。这个明显的对比表明AI生成的内容中的知识尚未得到证实，但它们具有潜在的价值。在所有知识图中，factual链接比率最高达60%。结论：我们的发现表明，ChatGPT生成的知识图中的链接数量异常多，有些连接数量甚至超过了PubMed知识图中的一些连接。这种早期的发现已经引发了我们进一步的调查，使用通用网络度量来揭示这些链接可能含有的新知识。
</details></li>
</ul>
<hr>
<h2 id="ViLP-Knowledge-Exploration-using-Vision-Language-and-Pose-Embeddings-for-Video-Action-Recognition"><a href="#ViLP-Knowledge-Exploration-using-Vision-Language-and-Pose-Embeddings-for-Video-Action-Recognition" class="headerlink" title="ViLP: Knowledge Exploration using Vision, Language, and Pose Embeddings for Video Action Recognition"></a>ViLP: Knowledge Exploration using Vision, Language, and Pose Embeddings for Video Action Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03908">http://arxiv.org/abs/2308.03908</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soumyabrata Chaudhuri, Saumik Bhattacharya</li>
<li>for: 本文提出了一种基于多模态学习的人体动作识别方法，以提高人体动作识别的准确率。</li>
<li>methods: 本文使用了一种pose增强的视觉语言模型（VLM），结合了pose、视觉信息和文本特征，以便更好地识别人体动作。</li>
<li>results: 根据实验结果，本文的方法在UCFC-101和HMDB-51两个人体动作识别数据集上的准确率分别为92.81%和73.02%，而无需视频数据预训练，而且经过kinetics预训练后，准确率分别提高至96.11%和75.75%。<details>
<summary>Abstract</summary>
Video Action Recognition (VAR) is a challenging task due to its inherent complexities. Though different approaches have been explored in the literature, designing a unified framework to recognize a large number of human actions is still a challenging problem. Recently, Multi-Modal Learning (MML) has demonstrated promising results in this domain. In literature, 2D skeleton or pose modality has often been used for this task, either independently or in conjunction with the visual information (RGB modality) present in videos. However, the combination of pose, visual information, and text attributes has not been explored yet, though text and pose attributes independently have been proven to be effective in numerous computer vision tasks. In this paper, we present the first pose augmented Vision-language model (VLM) for VAR. Notably, our scheme achieves an accuracy of 92.81% and 73.02% on two popular human video action recognition benchmark datasets, UCF-101 and HMDB-51, respectively, even without any video data pre-training, and an accuracy of 96.11% and 75.75% after kinetics pre-training.
</details>
<details>
<summary>摘要</summary>
视频动作识别（VAR）是一个复杂的任务，它的内在复杂性使得设计一个综合性的框架来识别大量人类动作变得具有挑战性。在文献中，不同的方法已经被探讨，但是设计一个综合性的框架来识别大量人类动作仍然是一个挑战性的问题。在文献中，2D骨架或 pose 模式 часто被用于这项任务，可以独立或与视觉信息（RGB 模式）一起使用。然而，将 pose、视觉信息和文本特征相结合尚未被探讨，尽管文本和 pose 特征独立地已经在计算机视觉任务中证明有效。在这篇论文中，我们提出了首个含有pose的视力语言模型（VLM），该模型在 UCF-101 和 HMDB-51 两个常用的人类视频动作识别 benchmark 数据集上取得了92.81% 和 73.02% 的准确率，而不需要任何视频数据预训练，并且在 kinetic 预训练后达到了96.11% 和 75.75% 的准确率。
</details></li>
</ul>
<hr>
<h2 id="Intelligent-Assistant-Language-Understanding-On-Device"><a href="#Intelligent-Assistant-Language-Understanding-On-Device" class="headerlink" title="Intelligent Assistant Language Understanding On Device"></a>Intelligent Assistant Language Understanding On Device</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03905">http://arxiv.org/abs/2308.03905</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Cecilia Aas, Hisham Abdelsalam, Irina Belousova, Shruti Bhargava, Jianpeng Cheng, Robert Daland, Joris Driesen, Federico Flego, Tristan Guigue, Anders Johannsen, Partha Lal, Jiarui Lu, Joel Ruben Antony Moniz, Nathan Perkins, Dhivya Piraviperumal, Stephen Pulman, Diarmuid Ó Séaghdha, David Q. Sun, John Torr, Marco Del Vecchio, Jay Wacker, Jason D. Williams, Hong Yu</li>
<li>for: 本研究旨在提出一种运行于个人设备上的自然语言理解系统，以提高隐私、可靠性、速度、表达力和准确性。</li>
<li>methods: 本文介绍了设计选择和技术方向，包括对对话系统文献中一些方法的实践评估，以及对适用于实际部署的挑战。</li>
<li>results: 本研究实现了一种更加私钥、可靠、快速、表达力和准确的自然语言理解系统，并提供了实践经验和建议，以便未来的研究工作。<details>
<summary>Abstract</summary>
It has recently become feasible to run personal digital assistants on phones and other personal devices. In this paper we describe a design for a natural language understanding system that runs on device. In comparison to a server-based assistant, this system is more private, more reliable, faster, more expressive, and more accurate. We describe what led to key choices about architecture and technologies. For example, some approaches in the dialog systems literature are difficult to maintain over time in a deployment setting. We hope that sharing learnings from our practical experiences may help inform future work in the research community.
</details>
<details>
<summary>摘要</summary>
现在已经可以在手机和其他个人设备上运行个人数字助手。在这篇论文中，我们描述了一种运行在设备上的自然语言理解系统的设计。与服务器上的助手相比，这种系统更加私钥、可靠、快速、表达力强、准确。我们详细介绍了一些关键的建筑和技术选择。例如，一些对话系统文献中的方法在部署环境中具有维护困难。我们希望通过分享我们的实践经验，对未来的研究工作产生影响。
</details></li>
</ul>
<hr>
<h2 id="FLIPS-Federated-Learning-using-Intelligent-Participant-Selection"><a href="#FLIPS-Federated-Learning-using-Intelligent-Participant-Selection" class="headerlink" title="FLIPS: Federated Learning using Intelligent Participant Selection"></a>FLIPS: Federated Learning using Intelligent Participant Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03901">http://arxiv.org/abs/2308.03901</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rahul Atul Bhope, K. R. Jayaram, Nalini Venkatasubramanian, Ashish Verma, Gegi Thomas<br>for: 这个论文旨在解决 Federated Learning (FL) 训练任务中数据和参与者多样性的管理问题，特别是在FL训练过程中对参与者选择的影响。methods: 该论文提出了一种基于标签分布划分的中间件系统，称为 FLIPS，它可以在FL训练过程中对参与者进行划分，以确保每个划分群在参与者选择中具有平等的代表性。此外，FLIPS还支持多种常见的FL算法，包括 FedAvg、FedProx、FedDyn、FedOpt 和 FedYogi。为了管理分布式平台的多样性和动态资源可用性，FLIPS还包含了一种卫星管理机制。results: 该论文的实验研究表明，FLIPS可以在实际世界数据集上提高FL训练的精度，相比随机选择、Oort和梯度划分等其他两种”聪明”选择机制，FLIPS可以在20-60%的通信成本下提高精度 by 17-20%。此外，FLIPS的效果还能在存在延迟参与者的情况下保持。<details>
<summary>Abstract</summary>
This paper presents the design and implementation of FLIPS, a middleware system to manage data and participant heterogeneity in federated learning (FL) training workloads. In particular, we examine the benefits of label distribution clustering on participant selection in federated learning. FLIPS clusters parties involved in an FL training job based on the label distribution of their data apriori, and during FL training, ensures that each cluster is equitably represented in the participants selected. FLIPS can support the most common FL algorithms, including FedAvg, FedProx, FedDyn, FedOpt and FedYogi. To manage platform heterogeneity and dynamic resource availability, FLIPS incorporates a straggler management mechanism to handle changing capacities in distributed, smart community applications. Privacy of label distributions, clustering and participant selection is ensured through a trusted execution environment (TEE). Our comprehensive empirical evaluation compares FLIPS with random participant selection, as well as two other "smart" selection mechanisms - Oort and gradient clustering using two real-world datasets, two different non-IID distributions and three common FL algorithms (FedYogi, FedProx and FedAvg). We demonstrate that FLIPS significantly improves convergence, achieving higher accuracy by 17 - 20 % with 20 - 60 % lower communication costs, and these benefits endure in the presence of straggler participants.
</details>
<details>
<summary>摘要</summary>
具体来说，这篇论文提出了一个名为FLIPS的中间件系统，用于管理 federated learning（FL）训练任务中的数据和参与者多样性。FLIPS在FL训练之前将参与者按照其标签分布进行分群，并在训练中确保每个分群都得到了公平的表现。FLIPS支持通用的FL算法，同时管理平台多样性和动态资源可用性，并通过安全执行环境（TEE）保证标签分布、分群和参与者选择的隐私。我们的实验证明，FLIPS可以大幅提高FL训练的收敛速度，在20-60%的通信成本下达到17-20%的高精度，这些优势在受到延迟参与者的情况下也保持不变。
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Generalization-in-Offline-Reinforcement-Learning-via-Unseen-State-Augmentations"><a href="#Exploiting-Generalization-in-Offline-Reinforcement-Learning-via-Unseen-State-Augmentations" class="headerlink" title="Exploiting Generalization in Offline Reinforcement Learning via Unseen State Augmentations"></a>Exploiting Generalization in Offline Reinforcement Learning via Unseen State Augmentations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03882">http://arxiv.org/abs/2308.03882</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nirbhay Modhe, Qiaozi Gao, Ashwin Kalyan, Dhruv Batra, Govind Thattai, Gaurav Sukhatme</li>
<li>for: 这个论文主要研究了线上强化学习（RL）方法在找到未知状态的问题上。</li>
<li>methods: 这个论文使用了模型自由RL方法和模型基于RL方法，它们都会征略未知状态的值。但是这些方法因两个因素受限：一是模型的扩展horizon非常短，二是模型扩展只基于已知的Offline数据。这个论文提出了一种新的未知状态扩展策略，允许在已知状态的基础上找到未知状态。</li>
<li>results: 这个论文在多个Offline RL任务中实现了改进的性能，并发现了其扩展策略通常比基eline更保守。<details>
<summary>Abstract</summary>
Offline reinforcement learning (RL) methods strike a balance between exploration and exploitation by conservative value estimation -- penalizing values of unseen states and actions. Model-free methods penalize values at all unseen actions, while model-based methods are able to further exploit unseen states via model rollouts. However, such methods are handicapped in their ability to find unseen states far away from the available offline data due to two factors -- (a) very short rollout horizons in models due to cascading model errors, and (b) model rollouts originating solely from states observed in offline data. We relax the second assumption and present a novel unseen state augmentation strategy to allow exploitation of unseen states where the learned model and value estimates generalize. Our strategy finds unseen states by value-informed perturbations of seen states followed by filtering out states with epistemic uncertainty estimates too high (high error) or too low (too similar to seen data). We observe improved performance in several offline RL tasks and find that our augmentation strategy consistently leads to overall lower average dataset Q-value estimates i.e. more conservative Q-value estimates than a baseline.
</details>
<details>
<summary>摘要</summary>
“在线束缚学习（RL）方法寻求平衡between exploration和利用，通过保守的价值估计--- penalty 未看过的状态和动作的价值。无模型方法对所有未看过的动作进行 penalty，而具有模型方法可以通过模型执行来进一步利用未看过的状态。然而，这些方法因两个因素受到限制---（a）模型中的执行 horizon 非常短，因为模型错误的堆叠，以及（b）模型执行仅启动自已经见过的状态。我们松动这一假设，并提出了一种新的未看过状态扩展策略，允许在已知模型和价值估计中利用未看过状态。我们的策略通过在已经看过的状态上进行价值意识的偏移，然后过滤高度不确定性（高错误）或者太相似于已经看过的数据的状态。我们发现在多个Offline RL任务中表现出色，并观察到我们的扩展策略通常比基准值更保守，即更低的平均数据Q值估计。”Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Guarding-the-Guardians-Automated-Analysis-of-Online-Child-Sexual-Abuse"><a href="#Guarding-the-Guardians-Automated-Analysis-of-Online-Child-Sexual-Abuse" class="headerlink" title="Guarding the Guardians: Automated Analysis of Online Child Sexual Abuse"></a>Guarding the Guardians: Automated Analysis of Online Child Sexual Abuse</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03880">http://arxiv.org/abs/2308.03880</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juanita Puentes, Angela Castillo, Wilmar Osejo, Yuly Calderón, Viviana Quintero, Lina Saldarriaga, Diana Agudelo, Pablo Arbeláez</li>
<li>For: The paper is written to address the urgent need for a solution to analyze children’s sexual abuse reports comprehensively, with a focus on reducing the risk of exposure to harmful content for analysts.* Methods: The paper proposes a novel automated tool that categorizes reports on three dimensions: Subject, Degree of Criminality, and Damage. Additionally, the paper introduces a novel approach to annotate the collected data, enabling a more in-depth analysis of the reports.* Results: The paper’s approach significantly reduces the risk of exposure to harmful content for analysts, and improves the comprehension of fundamental patterns and trends in children’s sexual abuse reports, enabling law enforcement agencies and policymakers to create focused strategies in the fight against children’s violence.In Simplified Chinese text, the three key points would be:</li>
<li>for: 这篇论文是为了解决儿童色情虐待报告的全面分析问题，尤其是减少分析人员遭受有害内容的风险。</li>
<li>methods: 论文提出了一种新的自动化工具，可以将报告分为三个维度：主体、犯罪程度和伤害。此外，论文还介绍了一种新的标注方法，以便更深入地分析报告中的数据。</li>
<li>results: 论文的方法可以明显减少分析人员遭受有害内容的风险，同时提高了对儿童色情虐待报告的基本 patrón和趋势的理解，为儿童保护和法制建设提供了有力的支持。<details>
<summary>Abstract</summary>
Online violence against children has increased globally recently, demanding urgent attention. Competent authorities manually analyze abuse complaints to comprehend crime dynamics and identify patterns. However, the manual analysis of these complaints presents a challenge because it exposes analysts to harmful content during the review process. Given these challenges, we present a novel solution, an automated tool designed to analyze children's sexual abuse reports comprehensively. By automating the analysis process, our tool significantly reduces the risk of exposure to harmful content by categorizing the reports on three dimensions: Subject, Degree of Criminality, and Damage. Furthermore, leveraging our multidisciplinary team's expertise, we introduce a novel approach to annotate the collected data, enabling a more in-depth analysis of the reports. This approach improves the comprehension of fundamental patterns and trends, enabling law enforcement agencies and policymakers to create focused strategies in the fight against children's violence.
</details>
<details>
<summary>摘要</summary>
在全球范围内，网络对儿童的暴力行为已经增加，需要紧急关注。有能力的当局人工分析滥剑投诉，以便更好地理解犯罪动力和趋势。然而，手动分析这些投诉存在挑战，因为它可能曝露分析员遭受有害内容的风险。为了解决这些挑战，我们提出了一种新的解决方案：一种自动化分析儿童色情虐待投诉的工具。通过自动化分析过程，我们的工具可以减少分析员遭受有害内容的风险，并将投诉分为三个维度：主体、犯罪程度和伤害。此外，我们的多科学队伍专家的协作，我们引入了一种新的数据标注方法，以便更深入地分析投诉。这种方法可以更好地描述基本的趋势和模式，使宪法机关和制定政策者可以根据这些数据制定有关儿童暴力的专门策略。
</details></li>
</ul>
<hr>
<h2 id="Trusting-Language-Models-in-Education"><a href="#Trusting-Language-Models-in-Education" class="headerlink" title="Trusting Language Models in Education"></a>Trusting Language Models in Education</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03866">http://arxiv.org/abs/2308.03866</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Jogi Suda Neto, Li Deng, Thejaswi Raya, Reza Shahbazi, Nick Liu, Adhitya Venkatesh, Miral Shah, Neeru Khosla, Rodrigo Capobianco Guido</li>
<li>for: 这个论文是为了提高语言模型在教育领域中的准确率，避免模型显示错误的答案，从而对学生造成误导。</li>
<li>methods: 这个论文提出了使用XGBoost在BERT之上进行报告修正，使用基于注意力机制的特征来改善模型的自信度。</li>
<li>results: 这个论文发现了注意力流中的不确定程度与模型回答质量之间存在关系，并通过修正模型的自信度来避免错误答案的显示。<details>
<summary>Abstract</summary>
Language Models are being widely used in Education. Even though modern deep learning models achieve very good performance on question-answering tasks, sometimes they make errors. To avoid misleading students by showing wrong answers, it is important to calibrate the confidence - that is, the prediction probability - of these models. In our work, we propose to use an XGBoost on top of BERT to output the corrected probabilities, using features based on the attention mechanism. Our hypothesis is that the level of uncertainty contained in the flow of attention is related to the quality of the model's response itself.
</details>
<details>
<summary>摘要</summary>
语言模型在教育领域广泛使用。虽然现代深度学习模型在问答任务上表现非常出色，但有时会出现错误。为了避免通过错误答案误导学生，需要对这些模型进行准确性调整。在我们的工作中，我们提议使用XGBoost在BERT之上输出修正的概率，使用基于注意力机制的特征。我们假设注意力流中的不确定程度与模型的答案质量之间存在相关性。
</details></li>
</ul>
<hr>
<h2 id="AI-Text-to-Behavior-A-Study-In-Steerability"><a href="#AI-Text-to-Behavior-A-Study-In-Steerability" class="headerlink" title="AI Text-to-Behavior: A Study In Steerability"></a>AI Text-to-Behavior: A Study In Steerability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07326">http://arxiv.org/abs/2308.07326</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Noever, Sam Hyams</li>
<li>for: 本研究探讨了大语言模型（LLM）的可控性，尤其是OpenAI的ChatGPT迭代。</li>
<li>methods: 我们使用了行为心理学框架OCEAN（开放性、聪明性、外向性、合作性、情绪性），量化测量模型对特定提示的回应。</li>
<li>results: 我们发现，“开放性”在语言上存在很大的混乱，而“聪明性”和“情绪性”在OCEAN框架中表现出了明显的强调，“外向性”和“合作性”则表现出了明确的分离。我们的发现表明GPT的多样性和可以根据人类意图进行定制的能力。<details>
<summary>Abstract</summary>
The research explores the steerability of Large Language Models (LLMs), particularly OpenAI's ChatGPT iterations. By employing a behavioral psychology framework called OCEAN (Openness, Conscientiousness, Extroversion, Agreeableness, Neuroticism), we quantitatively gauged the model's responsiveness to tailored prompts. When asked to generate text mimicking an extroverted personality, OCEAN scored the language alignment to that behavioral trait. In our analysis, while "openness" presented linguistic ambiguity, "conscientiousness" and "neuroticism" were distinctly evoked in the OCEAN framework, with "extroversion" and "agreeableness" showcasing a notable overlap yet distinct separation from other traits. Our findings underscore GPT's versatility and ability to discern and adapt to nuanced instructions. Furthermore, historical figure simulations highlighted the LLM's capacity to internalize and project instructible personas, precisely replicating their philosophies and dialogic styles. However, the rapid advancements in LLM capabilities and the opaque nature of some training techniques make metric proposals degrade rapidly. Our research emphasizes a quantitative role to describe steerability in LLMs, presenting both its promise and areas for further refinement in aligning its progress to human intentions.
</details>
<details>
<summary>摘要</summary>
研究探讨大语言模型（LLM）的可控性，尤其是OpenAI的ChatGPT迭代。通过employnig行为心理学框架called OCEAN（开放性、聪明性、外向性、合作性、情绪性），我们量化了模型对定制提示的回应。当请求生成文本模拟外向性人格时，OCEAN分数表示语言对该行为 trait的吻合。在我们的分析中，“开放性”存在语言 ambiguity，而“聪明性”和“情绪性”在OCEAN框架中得到了明显的表达，而“外向性”和“合作性”则显示了明显的 overlap yet distinct separation from other traits。我们的发现强调GPT的灵活性和对 instrucible 指令的适应能力。此外，历史人物模拟表明了LLM的能力 internalize和 project instructible personas，精准地复制他们的哲学和对话风格。然而，LLM的技能快速发展和一些训练技术的不透明性使得metric proposal degrade rapidly。我们的研究强调了量化描述 LLM 的可控性的重要性，并提出了其推进人类意图的方法。
</details></li>
</ul>
<hr>
<h2 id="Mobile-Supply-The-Last-Piece-of-Jigsaw-of-Recommender-System"><a href="#Mobile-Supply-The-Last-Piece-of-Jigsaw-of-Recommender-System" class="headerlink" title="Mobile Supply: The Last Piece of Jigsaw of Recommender System"></a>Mobile Supply: The Last Piece of Jigsaw of Recommender System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03855">http://arxiv.org/abs/2308.03855</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenhao Jiang, Biao Zeng, Hao Feng, Jin Liu, Jie Zhang, Jia Jia, Ning Hu</li>
<li>for: 提高边缘推荐系统的性能和用户体验</li>
<li>methods: 提出了一个新的模块”Mobile Supply”，并使用点 wise paradigm和设备相关的移动排名方法来解决分页触发机制问题</li>
<li>results: 实验证明，提出的方法可以further improve the performance of edge-side recommender systems and user experience,并已经在一个大规模的在线美食平台上部署，获得了可观的业务效益。<details>
<summary>Abstract</summary>
Recommendation system is a fundamental functionality of online platforms. With the development of computing power of mobile phones, some researchers have deployed recommendation algorithms on users' mobile devices to address the problems of data transmission delay and pagination trigger mechanism. However, the existing edge-side mobile rankings cannot completely solve the problem of pagination trigger mechanism. The mobile ranking can only sort the items on the current page, and the fixed set of candidate items limits the performance of the mobile ranking. Besides, after the user has viewed the items of interest to the user on the current page, the user refresh to get a new page of items. This will affect the user's immersive experience because the user is not satisfied with the left items on the current page. In order to address the problem of pagination trigger mechanism, we propose a completely new module in the pipeline of recommender system named Mobile Supply. The pipeline of recommender system is extended to "retrival->pre-ranking->ranking->re-ranking->Mobile Supply->mobile ranking". Specifically, we introduce the concept of list value and use point-wise paradigm to approximate list-wise estimation to calculate the maximum revenue that can be achieved by mobile ranking for the current page. We also design a new mobile ranking approach named device-aware mobile ranking considering the differences of mobile devices tailored to the new pipeline. Extensive offline and online experiments show the superiority of our proposed method and prove that Mobile Supply can further improve the performance of edge-side recommender system and user experience. Mobile Supply has been deployed on the homepage of a large-scale online food platform and has yielded considerable profits in our business.
</details>
<details>
<summary>摘要</summary>
“推荐系统是线上平台的基本功能之一。随着移动设备的计算能力的提高，一些研究人员已经将推荐算法部署到用户的移动设备上以解决数据传输延迟和分页触发器机制的问题。然而，现有的边缘式移动排名无法完全解决分页触发器机制的问题。这个边缘式移动排名只能在当前页面上排序项目，而且固定的候选项目限制了排名的表现。此外，当用户已经查看了他们 interessant 的项目时，用户刷新以获取新的页面项目。这会影响用户的沉浸体验，因为用户不满意LEFT项目。”“为了解决分页触发器机制的问题，我们提出了一个全新的模组，名为 Mobile Supply。我们将推荐系统的管线延展为“获取->预选->排名->重新排名->Mobile Supply->边缘式排名”。具体来说，我们引入了列值的概念，并使用点子法来估算列值的最大收益，以计算可以由边缘式排名获得的当前页面的最大收益。我们还设计了一个新的边缘式排名方法，名为 Device-Aware Mobile Ranking，考虑了移动设备的不同特点，以适应新的管线。”“我们将 Mobile Supply 部署到一个大规模的线上食物平台的首页上，并获得了显著的收益。”
</details></li>
</ul>
<hr>
<h2 id="Revisiting-Prompt-Engineering-via-Declarative-Crowdsourcing"><a href="#Revisiting-Prompt-Engineering-via-Declarative-Crowdsourcing" class="headerlink" title="Revisiting Prompt Engineering via Declarative Crowdsourcing"></a>Revisiting Prompt Engineering via Declarative Crowdsourcing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03854">http://arxiv.org/abs/2308.03854</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aditya G. Parameswaran, Shreya Shankar, Parth Asawa, Naman Jain, Yujie Wang</li>
<li>for: 该论文旨在提出一种宣告式描述工程（Declarative Prompt Engineering）的视野，以便为LLM（大型自然语言模型）数据处理工作流程进行优化，同时保持成本在所需范围内。</li>
<li>methods: 该论文提出了一种基于宣告式描述的工程方法，利用大量自然语言模型（LLM）来进行数据处理工作流程的优化。该方法包括多个宣告策略、内部一致性和混合LLM-非LLM方法等。</li>
<li>results: 该论文的预liminary案例研究表明，使用宣告式描述工程可以提高LLM数据处理工作流程的质量，同时保持成本在所需范围内。这些案例包括排序、实体解决和填充等。<details>
<summary>Abstract</summary>
Large language models (LLMs) are incredibly powerful at comprehending and generating data in the form of text, but are brittle and error-prone. There has been an advent of toolkits and recipes centered around so-called prompt engineering-the process of asking an LLM to do something via a series of prompts. However, for LLM-powered data processing workflows, in particular, optimizing for quality, while keeping cost bounded, is a tedious, manual process. We put forth a vision for declarative prompt engineering. We view LLMs like crowd workers and leverage ideas from the declarative crowdsourcing literature-including leveraging multiple prompting strategies, ensuring internal consistency, and exploring hybrid-LLM-non-LLM approaches-to make prompt engineering a more principled process. Preliminary case studies on sorting, entity resolution, and imputation demonstrate the promise of our approach
</details>
<details>
<summary>摘要</summary>
巨型语言模型（LLM）极其强大地理解和生成文本数据，但是脆弱和容易出错。随着推广工具和热门recipes的出现，关于 socalled prompt engineering——通过一系列提示来要求 LLM 做某件事——的过程在 LLM 驱动的数据处理工作流程中变得极其重要。然而，在Optimizing for quality的同时，保持成本在可控的范围内是一个艰辛的、手动的过程。我们提出了声明式提示工程的视野，将 LLM 看作是一群人群，利用声明式人群创新的想法——包括多种提示策略、保证内部一致性，以及混合 LLM 和非 LLM 方法——来使提示工程变得更加原则化。我们的初步案例研究包括排序、实体解析和填充，表明了我们的方法的承诺。
</details></li>
</ul>
<hr>
<h2 id="Recurrent-Multi-scale-Transformer-for-High-Resolution-Salient-Object-Detection"><a href="#Recurrent-Multi-scale-Transformer-for-High-Resolution-Salient-Object-Detection" class="headerlink" title="Recurrent Multi-scale Transformer for High-Resolution Salient Object Detection"></a>Recurrent Multi-scale Transformer for High-Resolution Salient Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03826">http://arxiv.org/abs/2308.03826</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinhao Deng, Pingping Zhang, Wei Liu, Huchuan Lu<br>for:This paper aims to improve the performance of high-resolution salient object detection (HRSOD) by proposing a new dataset and a novel Recurrent Multi-scale Transformer (RMFormer) method.methods:The proposed RMFormer method utilizes shared Transformers and multi-scale refinement architectures to generate high-resolution saliency maps, guided by lower-resolution predictions.results:Extensive experiments on both high-resolution and low-resolution benchmarks demonstrate the effectiveness and superiority of the proposed framework, with the RMFormer method achieving state-of-the-art performance on the newly proposed HRS10K dataset.<details>
<summary>Abstract</summary>
Salient Object Detection (SOD) aims to identify and segment the most conspicuous objects in an image or video. As an important pre-processing step, it has many potential applications in multimedia and vision tasks. With the advance of imaging devices, SOD with high-resolution images is of great demand, recently. However, traditional SOD methods are largely limited to low-resolution images, making them difficult to adapt to the development of High-Resolution SOD (HRSOD). Although some HRSOD methods emerge, there are no large enough datasets for training and evaluating. Besides, current HRSOD methods generally produce incomplete object regions and irregular object boundaries. To address above issues, in this work, we first propose a new HRS10K dataset, which contains 10,500 high-quality annotated images at 2K-8K resolution. As far as we know, it is the largest dataset for the HRSOD task, which will significantly help future works in training and evaluating models. Furthermore, to improve the HRSOD performance, we propose a novel Recurrent Multi-scale Transformer (RMFormer), which recurrently utilizes shared Transformers and multi-scale refinement architectures. Thus, high-resolution saliency maps can be generated with the guidance of lower-resolution predictions. Extensive experiments on both high-resolution and low-resolution benchmarks show the effectiveness and superiority of the proposed framework. The source code and dataset are released at: https://github.com/DrowsyMon/RMFormer.
</details>
<details>
<summary>摘要</summary>
抽象对象检测（SOD）目的是在图像或视频中识别和分割最为醒目的对象。作为前处理步骤，它在多媒体和视觉任务中具有重要的应用前景。随着捕捉设备的发展，高分辨率SOD（HRSOD）的需求日益增加。然而，传统的SOD方法主要适用于低分辨率图像，使其难以适应HRSOD的发展。虽然一些HRSOD方法已经出现，但是没有足够的大型数据集用于训练和评估。此外，现有的HRSOD方法通常生成不完整的对象区域和不规则的对象边界。为解决上述问题，在这种工作中，我们首先提出了一个新的HRSOD数据集，名为HRS10K，它包含10500个高质量注解图像，分别在2K-8K分辨率上。我们知道，这是HRSOD任务中最大的数据集，它将有助于未来的工作在训练和评估模型。此外，为提高HRSOD性能，我们提出了一种新的循环多ScaleTransformer（RMFormer），它可以在不同的尺度上重复使用共享的Transformer和多尺度精度建立。因此，高分辨率的Saliency图可以通过低分辨率预测的指导生成。我们进行了广泛的实验，并证明了我们的框架的有效性和超越性。数据集和代码可以在https://github.com/DrowsyMon/RMFormer上下载。
</details></li>
</ul>
<hr>
<h2 id="A-Cost-Analysis-of-Generative-Language-Models-and-Influence-Operations"><a href="#A-Cost-Analysis-of-Generative-Language-Models-and-Influence-Operations" class="headerlink" title="A Cost Analysis of Generative Language Models and Influence Operations"></a>A Cost Analysis of Generative Language Models and Influence Operations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03740">http://arxiv.org/abs/2308.03740</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/georgetown-cset/disinfo-costs">https://github.com/georgetown-cset/disinfo-costs</a></li>
<li>paper_authors: Micah Musser</li>
<li>for: 这个研究的目的是研究宣传人员使用大语言模型（LLM）时的成本和效益。</li>
<li>methods: 该研究使用了成本建模和优化分析来分析宣传人员使用LLM时的成本和效益。</li>
<li>results: 研究结果表明，LLM只需要生成可用输出，并且输出的可靠性只需要达到25%，就可以为宣传人员提供成本节省。同时，监控控制对于API访问ible的LLM可以减少成本，但是对于国家来说，特别是进行大规模影响操作的国家，没有经济上的收益来自于专门为影响操作培训自己的LLM。<details>
<summary>Abstract</summary>
Despite speculation that recent large language models (LLMs) are likely to be used maliciously to improve the quality or scale of influence operations, uncertainty persists regarding the economic value that LLMs offer propagandists. This research constructs a model of costs facing propagandists for content generation at scale and analyzes (1) the potential savings that LLMs could offer propagandists, (2) the potential deterrent effect of monitoring controls on API-accessible LLMs, and (3) the optimal strategy for propagandists choosing between multiple private and/or open source LLMs when conducting influence operations. Primary results suggest that LLMs need only produce usable outputs with relatively low reliability (roughly 25%) to offer cost savings to propagandists, that the potential reduction in content generation costs can be quite high (up to 70% for a highly reliable model), and that monitoring capabilities have sharply limited cost imposition effects when alternative open source models are available. In addition, these results suggest that nation-states -- even those conducting many large-scale influence operations per year -- are unlikely to benefit economically from training custom LLMs specifically for use in influence operations.
</details>
<details>
<summary>摘要</summary>
尽管有人 especulate recent large language models (LLMs) 可能会被用于提高媒体操作质量或规模，但是对于宣传者而言， LLMS 的经济价值还存在uncertainty。这项研究构建了宣传者内容生成在大规模时所面临的成本模型，并分析了以下问题：(1) LLMs 可以提供宣传者内容生成的可能性，(2) API 可访问的 LLMs 监控控制的抑效果，以及(3) 宣传者选择多个私人和/或开源 LLMs 时的优化策略。主要结果表明，LLMs 只需生成可用输出，并且只需要roughly 25% 的可靠性，就能为宣传者提供成本节省。此外，研究还发现，监控控制对于使用开源模型来源的宣传者来说，成本干扰效果很少。最后，这些结果表明，even nation-states 进行大规模的媒体操作，不太可能通过专门为影响操作培训自己的 LLMs 来获得经济效益。
</details></li>
</ul>
<hr>
<h2 id="SurvBeX-An-explanation-method-of-the-machine-learning-survival-models-based-on-the-Beran-estimator"><a href="#SurvBeX-An-explanation-method-of-the-machine-learning-survival-models-based-on-the-Beran-estimator" class="headerlink" title="SurvBeX: An explanation method of the machine learning survival models based on the Beran estimator"></a>SurvBeX: An explanation method of the machine learning survival models based on the Beran estimator</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03730">http://arxiv.org/abs/2308.03730</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/danilaeremenko/survbex">https://github.com/danilaeremenko/survbex</a></li>
<li>paper_authors: Lev V. Utkin, Danila Y. Eremenko, Andrei V. Konstantinov</li>
<li>for: The paper proposes a new explanation method called SurvBeX for interpreting predictions of machine learning survival black-box models.</li>
<li>methods: The method uses a modified Beran estimator as the surrogate explanation model, and generates many points in a local area around an example of interest to compute the survival function of the black-box model and the Beran estimator.</li>
<li>results: The paper demonstrates the efficiency of SurvBeX through numerical experiments with synthetic and real survival data, and compares the method with SurvLIME and SurvSHAP. The code implementing SurvBeX is available online.<details>
<summary>Abstract</summary>
An explanation method called SurvBeX is proposed to interpret predictions of the machine learning survival black-box models. The main idea behind the method is to use the modified Beran estimator as the surrogate explanation model. Coefficients, incorporated into Beran estimator, can be regarded as values of the feature impacts on the black-box model prediction. Following the well-known LIME method, many points are generated in a local area around an example of interest. For every generated example, the survival function of the black-box model is computed, and the survival function of the surrogate model (the Beran estimator) is constructed as a function of the explanation coefficients. In order to find the explanation coefficients, it is proposed to minimize the mean distance between the survival functions of the black-box model and the Beran estimator produced by the generated examples. Many numerical experiments with synthetic and real survival data demonstrate the SurvBeX efficiency and compare the method with the well-known method SurvLIME. The method is also compared with the method SurvSHAP. The code implementing SurvBeX is available at: https://github.com/DanilaEremenko/SurvBeX
</details>
<details>
<summary>摘要</summary>
一种名为SurvBeX的解释方法被提议用于解释机器学习生存黑盒模型的预测结果。该方法的主要思想是使用修改后的Beran估计器作为解释模型。将这些修改后的Beran估计器作为特征影响值，可以看作黑盒模型预测结果中特征的影响。与已知的LIME方法类似，在一个当地区域around一个Example of interest中，生成多个例子。对每个生成的例子，计算黑盒模型的生存函数，并将BERAN估计器中的生存函数作为特征影响值构建。为了找到解释系数，提议使用生成的例子中的平均距离来最小化黑盒模型和BERAN估计器生成的生存函数之间的距离。多个数学实验证明SurvBeX的效果，并与SurvLIME和SurvSHAP方法进行比较。代码实现SurvBeX可以在以下链接中找到：https://github.com/DanilaEremenko/SurvBeX。
</details></li>
</ul>
<hr>
<h2 id="Tiny-LVLM-eHub-Early-Multimodal-Experiments-with-Bard"><a href="#Tiny-LVLM-eHub-Early-Multimodal-Experiments-with-Bard" class="headerlink" title="Tiny LVLM-eHub: Early Multimodal Experiments with Bard"></a>Tiny LVLM-eHub: Early Multimodal Experiments with Bard</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03729">http://arxiv.org/abs/2308.03729</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/opengvlab/multi-modality-arena">https://github.com/opengvlab/multi-modality-arena</a></li>
<li>paper_authors: Wenqi Shao, Yutao Hu, Peng Gao, Meng Lei, Kaipeng Zhang, Fanqing Meng, Peng Xu, Siyuan Huang, Hongsheng Li, Yu Qiao, Ping Luo</li>
<li>for: 本研究目的是评估大型视语言模型（LVLM）在多模态任务上的表现，特别是Google的Bard模型，并提出一种轻量级的LVLM-eHub变体。</li>
<li>methods: 本研究使用了一种系统性评估多模态能力的方法，包括视觉理解、视觉知识获取、视觉逻辑、视觉常识、物体推理和embodied intelligence等六类多模态能力，通过42种文本相关的视觉benchmark测试。</li>
<li>results: 研究结果显示，Bard模型在大多数多模态能力中表现出色，仅在物体推理方面表现不佳，与人类评估更加一致。此外，Tiny LVLM-eHub变体可以便捷地评估各种Offline LVLMs模型。<details>
<summary>Abstract</summary>
Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated significant progress in tackling complex multimodal tasks. Among these cutting-edge developments, Google's Bard stands out for its remarkable multimodal capabilities, promoting comprehensive comprehension and reasoning across various domains. This work presents an early and holistic evaluation of LVLMs' multimodal abilities, with a particular focus on Bard, by proposing a lightweight variant of LVLM-eHub, named Tiny LVLM-eHub. In comparison to the vanilla version, Tiny LVLM-eHub possesses several appealing properties. Firstly, it provides a systematic assessment of six categories of multimodal capabilities, including visual perception, visual knowledge acquisition, visual reasoning, visual commonsense, object hallucination, and embodied intelligence, through quantitative evaluation of $42$ standard text-related visual benchmarks. Secondly, it conducts an in-depth analysis of LVLMs' predictions using the ChatGPT Ensemble Evaluation (CEE), which leads to a robust and accurate evaluation and exhibits improved alignment with human evaluation compared to the word matching approach. Thirdly, it comprises a mere $2.1$K image-text pairs, facilitating ease of use for practitioners to evaluate their own offline LVLMs. Through extensive experimental analysis, this study demonstrates that Bard outperforms previous LVLMs in most multimodal capabilities except object hallucination, to which Bard is still susceptible. Tiny LVLM-eHub serves as a baseline evaluation for various LVLMs and encourages innovative strategies aimed at advancing multimodal techniques. Our project is publicly available at \url{https://github.com/OpenGVLab/Multi-Modality-Arena}.
</details>
<details>
<summary>摘要</summary>
近期大量视语言模型（LVLM）的进步，表明了许多复杂多Modal任务的解决方案。其中，Google的Bard凸出了优异的多Modal能力，涵盖了多个领域的全面理解和合理思维。本文提出了一种轻量级的LVLM-eHub变体，名为Tiny LVLM-eHub，与传统版本相比具有多个优点。首先，它提供了六类多Modal能力的系统性评估，包括视觉理解、视觉知识获取、视觉逻辑、视觉常识、物体梦幻和embodied智能，通过42个标准文本相关的视觉准确度评估。其次，它使用ChatGPT Ensemble Evaluation（CEE）进行深入分析，从而获得了更加稳定和准确的评估结果，并与人类评估更加一致。最后，它使用2.1万个图文对象，使得评估方便快速。通过广泛的实验分析，本研究表明，Bard在大多数多Modal能力中都超越了前一代LVLM，只有物体梦幻能力方面存在一定的极点。Tiny LVLM-eHub可以作为多种LVLM的基准评估，激励创新的多Modal技术发展。我们的项目公开可用于\url{https://github.com/OpenGVLab/Multi-Modality-Arena}.
</details></li>
</ul>
<hr>
<h2 id="Dimensionality-Reduction-for-Improving-Out-of-Distribution-Detection-in-Medical-Image-Segmentation"><a href="#Dimensionality-Reduction-for-Improving-Out-of-Distribution-Detection-in-Medical-Image-Segmentation" class="headerlink" title="Dimensionality Reduction for Improving Out-of-Distribution Detection in Medical Image Segmentation"></a>Dimensionality Reduction for Improving Out-of-Distribution Detection in Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03723">http://arxiv.org/abs/2308.03723</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mckellwoodland/dimen_reduce_mahal">https://github.com/mckellwoodland/dimen_reduce_mahal</a></li>
<li>paper_authors: McKell Woodland, Nihil Patel, Mais Al Taie, Joshua P. Yung, Tucker J. Netherton, Ankit B. Patel, Kristy K. Brock</li>
<li>for: 验证 segmentation 模型在数据外部分布下的性能。</li>
<li>methods: 使用 Mahalanobis 距离post hoc 方法对瓶颈特征进行降维，并使用 Principal Component Analysis 降维瓶颈特征。</li>
<li>results: 可以高效地检测到数据外部分布下的图像。<details>
<summary>Abstract</summary>
Clinically deployed segmentation models are known to fail on data outside of their training distribution. As these models perform well on most cases, it is imperative to detect out-of-distribution (OOD) images at inference to protect against automation bias. This work applies the Mahalanobis distance post hoc to the bottleneck features of a Swin UNETR model that segments the liver on T1-weighted magnetic resonance imaging. By reducing the dimensions of the bottleneck features with principal component analysis, OOD images were detected with high performance and minimal computational load.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SEM-GAT-Explainable-Semantic-Pose-Estimation-using-Learned-Graph-Attention"><a href="#SEM-GAT-Explainable-Semantic-Pose-Estimation-using-Learned-Graph-Attention" class="headerlink" title="SEM-GAT: Explainable Semantic Pose Estimation using Learned Graph Attention"></a>SEM-GAT: Explainable Semantic Pose Estimation using Learned Graph Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03718">http://arxiv.org/abs/2308.03718</a></li>
<li>repo_url: None</li>
<li>paper_authors: Efimia Panagiotaki, Daniele De Martini, Georgi Pramatarov, Matthew Gadd, Lars Kunze</li>
<li>for: 本研究提出了一种基于GNN的方法，用于利用语义和地方几何信息导引可靠的点云注册候选者。</li>
<li>methods: 本方法使用了一种新的轻量级静止图structures，将语义特征和形态特征作为关键参考点，以便实现高精度激光探测pose估计。我们的novel lightweight static graph structure通过语义实例基于的关系提取semantic instance-based关系，从而减少了计算卷积核算符的计算负担。</li>
<li>results: 我们的方法在KITTI odometry dataset上进行测试，与参考方法相比具有竞争性的准确率，同时具有更高的轨迹缓和更少的网络参数。<details>
<summary>Abstract</summary>
This paper proposes a GNN-based method for exploiting semantics and local geometry to guide the identification of reliable pointcloud registration candidates. Semantic and morphological features of the environment serve as key reference points for registration, enabling accurate lidar-based pose estimation. Our novel lightweight static graph structure informs our attention-based keypoint node aggregation GNN network by identifying semantic instance-based relationships, acting as inductive bias to significantly reduce the computational burden of pointcloud registration. By connecting candidate nodes and exploiting cross-graph attention, we identify confidence scores for all potential registration correspondences, estimating the displacement between pointcloud scans. Our pipeline enables introspective analysis of the model's performance by correlating it with the individual contributions of local structures in the environment, providing valuable insights into the system's behaviour. We test our method on the KITTI odometry dataset, achieving competitive accuracy compared to benchmark methods and a higher track smoothness while relying on significantly fewer network parameters.
</details>
<details>
<summary>摘要</summary>
（本文提出了一种基于GNN的方法，利用 semantics和local geometry来导引可靠的点云注册候选者的标识。环境中的semantic和形态特征作为注册参考点，实现了高精度的激光探测pose estimation。我们的新的轻量级静止图 структуры告诉我们的注意力基于节点聚合GNN网络，通过标识semantic实例之间的关系，以 inductive bias 的形式减少点云注册的计算成本。通过连接候选节点并利用交叉图注意力，我们可以为所有可能的注册匹配计算出信任度，并估算点云扫描中的偏移量。我们的管道可以 introspective 地分析模型的性能，将其与本地环境结构的个别贡献相对考量，提供有价值的信息，了解系统的行为。我们在KITTI odometry dataset上测试了我们的方法，与标准方法相比，实现了竞争性的准确率和更高的车辆运动平滑性，同时使用的网络参数数量更少。）
</details></li>
</ul>
<hr>
<h2 id="Safe-Multimodal-Communication-in-Human-Robot-Collaboration"><a href="#Safe-Multimodal-Communication-in-Human-Robot-Collaboration" class="headerlink" title="Safe Multimodal Communication in Human-Robot Collaboration"></a>Safe Multimodal Communication in Human-Robot Collaboration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03690">http://arxiv.org/abs/2308.03690</a></li>
<li>repo_url: None</li>
<li>paper_authors: Davide Ferrari, Andrea Pupa, Alberto Signoretti, Cristian Secchi</li>
<li>for: 本研究旨在帮助人工智能机器人和人类在各种新工业设置中合作完成任务，但是这需要考虑多种因素。</li>
<li>methods: 本研究提出了一种基于多Modal融合的语音和手势命令的框架，以便人工智能机器人和人类之间进行自然和高效的交流。同时，该框架 siempre respects safety regulations。</li>
<li>results: 通过比较实验表明，通过多Modal融合的语音和手势命令，机器人可以从人类提供有价值的信息来完成任务，同时保证操作员的安全。<details>
<summary>Abstract</summary>
The new industrial settings are characterized by the presence of human and robots that work in close proximity, cooperating in performing the required job. Such a collaboration, however, requires to pay attention to many aspects. Firstly, it is crucial to enable a communication between this two actors that is natural and efficient. Secondly, the robot behavior must always be compliant with the safety regulations, ensuring always a safe collaboration. In this paper, we propose a framework that enables multi-channel communication between humans and robots by leveraging multimodal fusion of voice and gesture commands while always respecting safety regulations. The framework is validated through a comparative experiment, demonstrating that, thanks to multimodal communication, the robot can extract valuable information for performing the required task and additionally, with the safety layer, the robot can scale its speed to ensure the operator's safety.
</details>
<details>
<summary>摘要</summary>
新的工业设置 caracterized by the presence of human and robots working in close proximity, cooperating to perform the required job. However, such collaboration requires attention to many aspects. Firstly, it is crucial to enable natural and efficient communication between the two actors. Secondly, the robot's behavior must always comply with safety regulations, ensuring safe collaboration. In this paper, we propose a framework that enables multi-channel communication between humans and robots by leveraging multimodal fusion of voice and gesture commands while always respecting safety regulations. The framework is validated through a comparative experiment, demonstrating that, thanks to multimodal communication, the robot can extract valuable information for performing the required task and additionally, with the safety layer, the robot can scale its speed to ensure the operator's safety.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="AgentBench-Evaluating-LLMs-as-Agents"><a href="#AgentBench-Evaluating-LLMs-as-Agents" class="headerlink" title="AgentBench: Evaluating LLMs as Agents"></a>AgentBench: Evaluating LLMs as Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03688">http://arxiv.org/abs/2308.03688</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thudm/agentbench">https://github.com/thudm/agentbench</a></li>
<li>paper_authors: Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, Jie Tang</li>
<li>for: 这个论文的目的是评估大语言模型（LLMs）作为智能代理的能力，以及评估 LLMS 在复杂环境中的决策和判断能力。</li>
<li>methods: 这篇论文使用了 AgentBench 多维度演变 bencmark，该 bencmark 包括 8 个不同环境，用于评估 LLMS 的解释和决策能力。</li>
<li>results: 测试结果显示，商业 LLMS 在复杂环境中表现强，但是与开源竞争对手相比，它们的表现存在显著差异。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) are becoming increasingly smart and autonomous, targeting real-world pragmatic missions beyond traditional NLP tasks. As a result, there has been an urgent need to evaluate LLMs as agents on challenging tasks in interactive environments. We present AgentBench, a multi-dimensional evolving benchmark that currently consists of 8 distinct environments to assess LLM-as-Agent's reasoning and decision-making abilities in a multi-turn open-ended generation setting. Our extensive test over 25 LLMs (including APIs and open-sourced models) shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and open-sourced competitors. It also serves as a component of an ongoing project with wider coverage and deeper consideration towards systematic LLM evaluation. Datasets, environments, and an integrated evaluation package for AgentBench are released at https://github.com/THUDM/AgentBench
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/08/cs.AI_2023_08_08/" data-id="clp9qz7yh001zok882d0ie2v6" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_08_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/08/cs.CL_2023_08_08/" class="article-date">
  <time datetime="2023-08-08T11:00:00.000Z" itemprop="datePublished">2023-08-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/08/cs.CL_2023_08_08/">cs.CL - 2023-08-08</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Unmasking-Nationality-Bias-A-Study-of-Human-Perception-of-Nationalities-in-AI-Generated-Articles"><a href="#Unmasking-Nationality-Bias-A-Study-of-Human-Perception-of-Nationalities-in-AI-Generated-Articles" class="headerlink" title="Unmasking Nationality Bias: A Study of Human Perception of Nationalities in AI-Generated Articles"></a>Unmasking Nationality Bias: A Study of Human Perception of Nationalities in AI-Generated Articles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04346">http://arxiv.org/abs/2308.04346</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pranav Narayanan Venkit, Sanjana Gautam, Ruchi Panchanadikar, Ting-Hao &#96;Kenneth’ Huang, Shomir Wilson</li>
<li>for: 本研究旨在检测自然语言处理（NLP）模型中的国籍偏见，以确定AI系统的公正性和正义。</li>
<li>methods: 本研究采用了两步混合方法，包括量化分析和质量分析，以识别和理解国籍偏见在文本生成模型中的影响。</li>
<li>results: 研究发现，偏见NLP模型通常会复制和强化现有的社会偏见，可能导致社会技术环境中的歧视。参与者的口头问naire和主题分析也表明，读者阅读文章时可能受到这些偏见的影响，从而改变他们对国家的看法。这些发现强调了AI系统在社会中的影响，以及需要更正AI系统中的偏见。<details>
<summary>Abstract</summary>
We investigate the potential for nationality biases in natural language processing (NLP) models using human evaluation methods. Biased NLP models can perpetuate stereotypes and lead to algorithmic discrimination, posing a significant challenge to the fairness and justice of AI systems. Our study employs a two-step mixed-methods approach that includes both quantitative and qualitative analysis to identify and understand the impact of nationality bias in a text generation model. Through our human-centered quantitative analysis, we measure the extent of nationality bias in articles generated by AI sources. We then conduct open-ended interviews with participants, performing qualitative coding and thematic analysis to understand the implications of these biases on human readers. Our findings reveal that biased NLP models tend to replicate and amplify existing societal biases, which can translate to harm if used in a sociotechnical setting. The qualitative analysis from our interviews offers insights into the experience readers have when encountering such articles, highlighting the potential to shift a reader's perception of a country. These findings emphasize the critical role of public perception in shaping AI's impact on society and the need to correct biases in AI systems.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)我们研究使用人类评估方法检测自然语言处理（NLP）模型中的国籍偏见。偏见的NLP模型可能扩大和复制现有社会偏见，导致算法性隔离，这对于AI系统的公平和正义具有挑战性。我们的研究采用了一种两步混合方法，包括量化和质量分析，以确定和理解国籍偏见在文本生成模型中的影响。我们通过人类中心的量化分析 mesure了AI源生成的文章中的国籍偏见的程度。然后，我们通过对参与者进行开放结构问naire和Theme coding分析来理解这些偏见对人类读者的影响。我们的发现表明，偏见的NLP模型通常会复制和加强现有社会偏见，这可能在社会技术 Setting中导致害。我们的访问分析表明，当读者遇到这些文章时，可能会改变他们对某个国家的看法。这些发现强调了AI对社会的影响的重要性，以及需要 corrections in AI systems。
</details></li>
</ul>
<hr>
<h2 id="Towards-an-AI-to-Win-Ghana’s-National-Science-and-Maths-Quiz"><a href="#Towards-an-AI-to-Win-Ghana’s-National-Science-and-Maths-Quiz" class="headerlink" title="Towards an AI to Win Ghana’s National Science and Maths Quiz"></a>Towards an AI to Win Ghana’s National Science and Maths Quiz</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04333">http://arxiv.org/abs/2308.04333</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nsmq-ai/nsmqai">https://github.com/nsmq-ai/nsmqai</a></li>
<li>paper_authors: George Boateng, Jonathan Abrefah Mensah, Kevin Takyi Yeboah, William Edor, Andrew Kojo Mensah-Onumah, Naafi Dasana Ibrahim, Nana Sam Yeboah</li>
<li>for: The paper is written to explore the possibility of building an AI system that can compete in Ghana’s National Science and Maths Quiz (NSMQ) and potentially win.</li>
<li>methods: The paper describes an open-source project that is building AI to compete in the NSMQ, with a focus on speech-to-text, text-to-speech, question-answering, and human-computer interaction.</li>
<li>results: The paper provides an overview of the progress made thus far in the project, including the development of the AI system and the next steps toward its planned launch and debut in October for NSMQ 2023.<details>
<summary>Abstract</summary>
Can an AI win Ghana's National Science and Maths Quiz (NSMQ)? That is the question we seek to answer in the NSMQ AI project, an open-source project that is building AI to compete live in the NSMQ and win. The NSMQ is an annual live science and mathematics competition for senior secondary school students in Ghana in which 3 teams of 2 students compete by answering questions across biology, chemistry, physics, and math in 5 rounds over 5 progressive stages until a winning team is crowned for that year. The NSMQ is an exciting live quiz competition with interesting technical challenges across speech-to-text, text-to-speech, question-answering, and human-computer interaction. In this ongoing work that began in January 2023, we give an overview of the project, describe each of the teams, progress made thus far, and the next steps toward our planned launch and debut of the AI in October for NSMQ 2023. An AI that conquers this grand challenge can have real-world impact on education such as enabling millions of students across Africa to have one-on-one learning support from this AI.
</details>
<details>
<summary>摘要</summary>
可以AI赢得加纳国家科学和数学竞赛（NSMQ）呢？我们在NSMQ AI项目中寻求答案，这是一个开源项目，旨在通过AI参加NSMQ并赢得奖。NSMQ是每年在加纳举行的生活 science和数学竞赛，参赛者是高中二年级学生，共有3个队伍，每个队伍有2名学生，在5轮5阶段的竞赛中回答生物、化学、物理和数学等领域的问题。NSMQ是一场激动人心的直播竞赛，涉及到语音识别、文本识别、问题回答和人机交互等技术挑战。在我们自2023年1月开始的工作中，我们将提供项目概述，介绍各个团队、已经进步的情况，以及下一步的计划，以备在10月份的NSMQ 2023上发布和使用AI。一旦AI成功解决这一大挑战，可以对教育产生实际影响，如提供非洲数百万学生一对一的学习支持。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Based-Knowledge-Injection-for-Metaphor-Detection-A-Comprehensive-Review"><a href="#Deep-Learning-Based-Knowledge-Injection-for-Metaphor-Detection-A-Comprehensive-Review" class="headerlink" title="Deep Learning-Based Knowledge Injection for Metaphor Detection: A Comprehensive Review"></a>Deep Learning-Based Knowledge Injection for Metaphor Detection: A Comprehensive Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04306">http://arxiv.org/abs/2308.04306</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng Yang, Wenye Zhao, Zhiyue Liu, Qingbao Huang</li>
<li>for: 本研究的目的是提供深度学习在метaphore认知任务中知识批注的综述和总结。</li>
<li>methods: 本文系统地总结了主流的知识和知识批注原则，并评估了在 métaphore认知任务中使用的数据集、评价指标和参考模型。</li>
<li>results: 本文结果预示，现有的知识批注方法在 métaphore认知任务中具有较高的识别率和准确率。但是，现有的方法还存在一些问题，如知识批注的质量和可靠性问题。<details>
<summary>Abstract</summary>
The history of metaphor research also marks the evolution of knowledge infusion research. With the continued advancement of deep learning techniques in recent years, the natural language processing community has shown great interest in applying knowledge to successful results in metaphor recognition tasks. Although there has been a gradual increase in the number of approaches involving knowledge injection in the field of metaphor recognition, there is a lack of a complete review article on knowledge injection based approaches. Therefore, the goal of this paper is to provide a comprehensive review of research advances in the application of deep learning for knowledge injection in metaphor recognition tasks. In this paper, we systematically summarize and generalize the mainstream knowledge and knowledge injection principles, as well as review the datasets, evaluation metrics, and benchmark models used in metaphor recognition tasks. Finally, we explore the current issues facing knowledge injection methods and provide an outlook on future research directions.
</details>
<details>
<summary>摘要</summary>
历史上的比喻研究也标志着知识混合研究的演化。随着近年深度学习技术的不断发展，自然语言处理社区对于应用知识到成功的结果在比喻识别任务中表示了极大的兴趣。虽然在比喻识别领域中有一个慢慢增长的方法涉及知识注入，但是没有一篇完整的文章来评论这些方法。因此，本文的目标是为您提供深度学习在比喻识别任务中知识注入的完整评论。在这篇文章中，我们系统地总结和总结主流的知识和知识注入原则，同时回顾用于比喻识别任务的数据集、评价指标和标准模型。最后，我们探讨知识注入方法当前面临的问题，并对未来研究方向提出了一些想法。
</details></li>
</ul>
<hr>
<h2 id="Comparative-Analysis-of-the-wav2vec-2-0-Feature-Extractor"><a href="#Comparative-Analysis-of-the-wav2vec-2-0-Feature-Extractor" class="headerlink" title="Comparative Analysis of the wav2vec 2.0 Feature Extractor"></a>Comparative Analysis of the wav2vec 2.0 Feature Extractor</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04286">http://arxiv.org/abs/2308.04286</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peter Vieting, Ralf Schlüter, Hermann Ney</li>
<li>for: 这个论文的目的是研究一种基于神经网络的原始波形特征提取器（FEs），以取代传统的手动设计的特征提取方法，以实现更加一致的模型化从语音到文本转写。</li>
<li>methods: 这个论文使用了wav2vec 2.0模型，这是一种最近受欢迎的模型，它使用了一种卷积 convolutional FE，直接操作语音波形。然而，这个方法尚未得到了广泛的研究。</li>
<li>results: 研究表明，使用神经网络原始波形特征提取器可以与传统的特征提取方法竞争，并且可以在LibriSpeech benchmark上实现类似的性能。此外，研究还分析了各个组件的效果，并发现了一些帮助ASR系统获得重要信息的带宽滤波器。<details>
<summary>Abstract</summary>
Automatic speech recognition (ASR) systems typically use handcrafted feature extraction pipelines. To avoid their inherent information loss and to achieve more consistent modeling from speech to transcribed text, neural raw waveform feature extractors (FEs) are an appealing approach. Also the wav2vec 2.0 model, which has recently gained large popularity, uses a convolutional FE which operates directly on the speech waveform. However, it is not yet studied extensively in the literature. In this work, we study its capability to replace the standard feature extraction methods in a connectionist temporal classification (CTC) ASR model and compare it to an alternative neural FE. We show that both are competitive with traditional FEs on the LibriSpeech benchmark and analyze the effect of the individual components. Furthermore, we analyze the learned filters and show that the most important information for the ASR system is obtained by a set of bandpass filters.
</details>
<details>
<summary>摘要</summary>
自动语音识别（ASR）系统通常使用手工设计的特征提取管道。为了避免其内置的信息损失并实现更一致的模型化从语音到转录文本，神经原始波形特征提取器（FEs）是一种吸引人的方法。另外，最近广受欢迎的wav2vec 2.0模型使用了一种卷积 convolutional FE，该模型直接操作语音波形。然而，它在文献中还未得到了广泛的研究。在这种工作中，我们研究了它的可行性来代替标准特征提取方法在一个连接主义时间分类（CTC） ASR 模型中，并与一种代替神经 FE 进行比较。我们发现两者都与传统的特征提取方法竞争在 LibriSpeech 测试集上，并分析了各个组件的效果。此外，我们分析了学习的滤波器，并发现了一组频率带滤波器是 ASR 系统中最重要的信息来源。
</details></li>
</ul>
<hr>
<h2 id="CLASSLA-Stanza-The-Next-Step-for-Linguistic-Processing-of-South-Slavic-Languages"><a href="#CLASSLA-Stanza-The-Next-Step-for-Linguistic-Processing-of-South-Slavic-Languages" class="headerlink" title="CLASSLA-Stanza: The Next Step for Linguistic Processing of South Slavic Languages"></a>CLASSLA-Stanza: The Next Step for Linguistic Processing of South Slavic Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04255">http://arxiv.org/abs/2308.04255</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luka Terčon, Nikola Ljubešić</li>
<li>for: 这个论文是为了提出一个基于Stanza自然语言处理管道的自动语言标注管道，用于南斯拉夫语言的语言处理。</li>
<li>methods: 该管道使用了主要改进于Stanza，并对2.1版本进行了详细的模型训练过程。</li>
<li>results: 管道在不同语言和方言上都达到了高性能水平，并在大多数任务上超越或扩展了父管道Stanza。此外，新增的网络数据处理功能和其原因也被介绍。<details>
<summary>Abstract</summary>
We present CLASSLA-Stanza, a pipeline for automatic linguistic annotation of the South Slavic languages, which is based on the Stanza natural language processing pipeline. We describe the main improvements in CLASSLA-Stanza with respect to Stanza, and give a detailed description of the model training process for the latest 2.1 release of the pipeline. We also report performance scores produced by the pipeline for different languages and varieties. CLASSLA-Stanza exhibits consistently high performance across all the supported languages and outperforms or expands its parent pipeline Stanza at all the supported tasks. We also present the pipeline's new functionality enabling efficient processing of web data and the reasons that led to its implementation.
</details>
<details>
<summary>摘要</summary>
我团队今天发布了一个名为CLASSLA-Stanza的自动语言标注管道，这个管道基于Stanza自然语言处理管道。我们详细介绍了CLASSLA-Stanza与Stanza之间的主要改进，以及latest版本2.1中模型训练过程的详细描述。我们还公布了不同语言和变体的性能分数。CLASSLA-Stanza在所有支持语言上表现了高稳定性，并在所有任务上超越或扩展了父管道Stanza的性能。我们还介绍了新增的网络数据处理功能，以及这种功能的实现的原因。
</details></li>
</ul>
<hr>
<h2 id="OpinionConv-Conversational-Product-Search-with-Grounded-Opinions"><a href="#OpinionConv-Conversational-Product-Search-with-Grounded-Opinions" class="headerlink" title="OpinionConv: Conversational Product Search with Grounded Opinions"></a>OpinionConv: Conversational Product Search with Grounded Opinions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04226">http://arxiv.org/abs/2308.04226</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vahid Sadiri Javadi, Martin Potthast, Lucie Flek</li>
<li>for:  This paper aims to address the problem of training conversational AI in simulating sales conversations by leveraging product reviews as a rich source of product opinions to ground conversational AI in true subjective narratives.</li>
<li>methods: The paper uses product reviews as a source of product opinions to train a conversational AI model called OpinionConv, which can simulate sales conversations.</li>
<li>results: The paper conducts several user studies to validate the generated conversations and shows that the generated opinions are perceived as realistic. The assessors also confirm the importance of opinions as an informative basis for decision-making.Here’s the simplified Chinese version of the three key points:</li>
<li>for: 这篇论文目标是使用产品评论作为对话AI训练的基础，以便模拟销售对话。</li>
<li>methods: 论文使用产品评论作为对话AI的训练数据，开发了一个名为OpinionConv的对话AI模型。</li>
<li>results: 论文通过多个用户研究证明了生成的对话是真实的，评分人也证明了对话中的意见对决策提供了有用信息。<details>
<summary>Abstract</summary>
When searching for products, the opinions of others play an important role in making informed decisions. Subjective experiences about a product can be a valuable source of information. This is also true in sales conversations, where a customer and a sales assistant exchange facts and opinions about products. However, training an AI for such conversations is complicated by the fact that language models do not possess authentic opinions for their lack of real-world experience. We address this problem by leveraging product reviews as a rich source of product opinions to ground conversational AI in true subjective narratives. With OpinionConv, we develop the first conversational AI for simulating sales conversations. To validate the generated conversations, we conduct several user studies showing that the generated opinions are perceived as realistic. Our assessors also confirm the importance of opinions as an informative basis for decision-making.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Studying-Socially-Unacceptable-Discourse-Classification-SUD-through-different-eyes-“Are-we-on-the-same-page-”"><a href="#Studying-Socially-Unacceptable-Discourse-Classification-SUD-through-different-eyes-“Are-we-on-the-same-page-”" class="headerlink" title="Studying Socially Unacceptable Discourse Classification (SUD) through different eyes: “Are we on the same page ?”"></a>Studying Socially Unacceptable Discourse Classification (SUD) through different eyes: “Are we on the same page ?”</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04180">http://arxiv.org/abs/2308.04180</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mlinardicyu/sud_study_different_eyes">https://github.com/mlinardicyu/sud_study_different_eyes</a></li>
<li>paper_authors: Bruno Machado Carneiro, Michele Linardi, Julien Longhi</li>
<li>for: 这个论文是为了研究在线文本中的社会不接受的语言表达（SUD）的特征和检测方法。</li>
<li>methods: 作者首先建立了一个包含多种不同在线源的手动标注文本的新 корпуス，以用于测试现有的机器学习（ML）SUD检测解决方案中的通用性。</li>
<li>results: 作者通过分析不同批标注方法对SUD学习的影响，并提供了一些可支持领域专家在标注任务中的数据洞察。<details>
<summary>Abstract</summary>
We study Socially Unacceptable Discourse (SUD) characterization and detection in online text. We first build and present a novel corpus that contains a large variety of manually annotated texts from different online sources used so far in state-of-the-art Machine learning (ML) SUD detection solutions. This global context allows us to test the generalization ability of SUD classifiers that acquire knowledge around the same SUD categories, but from different contexts. From this perspective, we can analyze how (possibly) different annotation modalities influence SUD learning by discussing open challenges and open research directions. We also provide several data insights which can support domain experts in the annotation task.
</details>
<details>
<summary>摘要</summary>
我们研究社会不可接受的语言（SUD）的特征化和检测在线文本中。我们首先构建了一个新的文献库，包含了不同的在线来源的手动标注文本，以及现有的机器学习（ML）SUD检测解决方案中使用的同一类型的文本。这个全球背景允许我们测试SUD分类器在不同上下文中是否具有泛化能力。从这个角度来看，我们可以分析不同的标注方式对SUD学习产生的影响，并讨论开放的挑战和未来研究方向。我们还提供了一些数据分析视图，以支持领域专家进行标注任务。
</details></li>
</ul>
<hr>
<h2 id="On-Monotonic-Aggregation-for-Open-domain-QA"><a href="#On-Monotonic-Aggregation-for-Open-domain-QA" class="headerlink" title="On Monotonic Aggregation for Open-domain QA"></a>On Monotonic Aggregation for Open-domain QA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04176">http://arxiv.org/abs/2308.04176</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yeonseokjeong/judge-specialist">https://github.com/yeonseokjeong/judge-specialist</a></li>
<li>paper_authors: Sang-eun Han, Yeonseok Jeong, Seung-won Hwang, Kyungjae Lee</li>
<li>for:  answering user questions on unrestricted knowledge sources</li>
<li>methods:  Judge-Specialist framework with specialist retrievers&#x2F;readers and a dedicated language model to select the final answer</li>
<li>results:  outperforms state-of-the-art multi-source QA methods on Natural Questions, and robustly preserves monotonicity against noise from speech recognition<details>
<summary>Abstract</summary>
Question answering (QA) is a critical task for speech-based retrieval from knowledge sources, by sifting only the answers without requiring to read supporting documents. Specifically, open-domain QA aims to answer user questions on unrestricted knowledge sources. Ideally, adding a source should not decrease the accuracy, but we find this property (denoted as "monotonicity") does not hold for current state-of-the-art methods. We identify the cause, and based on that we propose Judge-Specialist framework. Our framework consists of (1) specialist retrievers/readers to cover individual sources, and (2) judge, a dedicated language model to select the final answer. Our experiments show that our framework not only ensures monotonicity, but also outperforms state-of-the-art multi-source QA methods on Natural Questions. Additionally, we show that our models robustly preserve the monotonicity against noise from speech recognition. We publicly release our code and setting.
</details>
<details>
<summary>摘要</summary>
问答（QA）是知识源检索中的关键任务，通过只检索答案而不需要阅读支持文档。特别是开放领域QA旨在回答用户问题在不限制的知识源上。理想情况下，添加源应该不会降低准确性，但我们发现现有方法中的性质（ denoted as "monotonicity"）不成立。我们认定了原因，并基于此我们提出了 Judge-Specialist 框架。我们的框架包括（1）专家检索/读取器，覆盖个别源，以及（2）判官，专门的语言模型来选择最终答案。我们的实验表明，我们的框架不仅保证幂等性，而且超越了当前状态的跨源QA方法在自然问题上的性能。此外，我们的模型也能够坚定地保持幂等性面对语音识别器的噪音。我们在线发布了我们的代码和设置。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Model-Prompt-Chaining-for-Long-Legal-Document-Classification"><a href="#Large-Language-Model-Prompt-Chaining-for-Long-Legal-Document-Classification" class="headerlink" title="Large Language Model Prompt Chaining for Long Legal Document Classification"></a>Large Language Model Prompt Chaining for Long Legal Document Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04138">http://arxiv.org/abs/2308.04138</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dietrich Trautmann</li>
<li>for: 这个论文主要是为了解决法律文档分类问题，以提高法律文档分类的效率和准确率。</li>
<li>methods: 这个论文使用了提问链接法（prompt chaining）来解决法律文档分类问题。提问链接法是一种细分大任务，将其 decomposes 成一系列更小的任务，以提高模型的性能。在这个论文中，首先创建了原始文档的简洁摘要，然后进行 semantic search 来找到相关的示例文档和其对应的注释集。最后，通过提问来 assigning 标签，基于任务的需求。</li>
<li>results: 根据论文的结果，通过提问链接法，可以不 только超越零shot，还可以超过大型模型，如ChatGPT零shot，使用更小的模型。<details>
<summary>Abstract</summary>
Prompting is used to guide or steer a language model in generating an appropriate response that is consistent with the desired outcome. Chaining is a strategy used to decompose complex tasks into smaller, manageable components. In this study, we utilize prompt chaining for extensive legal document classification tasks, which present difficulties due to their intricate domain-specific language and considerable length. Our approach begins with the creation of a concise summary of the original document, followed by a semantic search for related exemplar texts and their corresponding annotations from a training corpus. Finally, we prompt for a label - based on the task - to assign, by leveraging the in-context learning from the few-shot prompt. We demonstrate that through prompt chaining, we can not only enhance the performance over zero-shot, but also surpass the micro-F1 score achieved by larger models, such as ChatGPT zero-shot, using smaller models.
</details>
<details>
<summary>摘要</summary>
提示是用于引导或导引语言模型生成适当的回应，以确保与所需结果相符。链式是一种策略，用于将复杂任务分解成更小、更容易处理的组件。在这项研究中，我们使用提示链式来处理广泛的法律文档分类任务，这些任务因其专业领域语言和较长的文档长度而更加具有挑战性。我们的方法包括：首先创建原始文档的简短摘要，然后通过semantic search找到相关的示例文档和它们的相关注释，从训练集中获取。最后，我们根据任务提供标签，通过受到上下文学习的几个提示来启用。我们示出，通过提示链式，不仅可以超越零shot的性能，还可以使用更小的模型超越更大的模型，如ChatGPT零shot。
</details></li>
</ul>
<hr>
<h2 id="Social-Media-Topic-Modeling-and-Sentiment-Analysis-in-Municipal-Decision-Support"><a href="#Social-Media-Topic-Modeling-and-Sentiment-Analysis-in-Municipal-Decision-Support" class="headerlink" title="Social Media, Topic Modeling and Sentiment Analysis in Municipal Decision Support"></a>Social Media, Topic Modeling and Sentiment Analysis in Municipal Decision Support</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04124">http://arxiv.org/abs/2308.04124</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miloš Švaňa<br>for: This paper is written for municipal decision-makers who want to incorporate social media sentiment into their decision-making processes.methods: The paper proposes a framework for processing social media posts that consists of three steps: determining the sentiment polarity of each post, identifying prevalent topics, and aggregating the sentiment information. The framework uses fuzzy numbers to represent the sentiment in a richer way and capture the diversity of opinions expressed on social media.results: The paper demonstrates the application of the framework on tweets published from Ostrava, Czechia over a period of about two months. The results show how fuzzy numbers can represent the sentiment in a more nuanced way and capture the diversity of opinions expressed on social media.<details>
<summary>Abstract</summary>
Many cities around the world are aspiring to become. However, smart initiatives often give little weight to the opinions of average citizens.   Social media are one of the most important sources of citizen opinions. This paper presents a prototype of a framework for processing social media posts with municipal decision-making in mind. The framework consists of a sequence of three steps: (1) determining the sentiment polarity of each social media post (2) identifying prevalent topics and mapping these topics to individual posts, and (3) aggregating these two pieces of information into a fuzzy number representing the overall sentiment expressed towards each topic. Optionally, the fuzzy number can be reduced into a tuple of two real numbers indicating the "amount" of positive and negative opinion expressed towards each topic.   The framework is demonstrated on tweets published from Ostrava, Czechia over a period of about two months. This application illustrates how fuzzy numbers represent sentiment in a richer way and capture the diversity of opinions expressed on social media.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Determine the sentiment polarity of each social media post (是否积极或消极的意见)2. Identify prevalent topics and map them to individual posts (找出主要话题并将其与各个帖子相关联)3. Aggregate the two pieces of information into a fuzzy number representing the overall sentiment expressed towards each topic (将这两个信息合并为一个模糊数字表示每个话题的总意见)Optionally, the fuzzy number can be reduced into a tuple of two real numbers indicating the “amount” of positive and negative opinion expressed towards each topic (可以将模糊数字转换为一个二元数组，表示每个话题的积极和消极意见的量)This framework was demonstrated on tweets published from Ostrava, Czechia over a period of about two months, showing how fuzzy numbers can represent sentiment in a richer way and capture the diversity of opinions expressed on social media.</details></li>
</ol>
<hr>
<h2 id="Collective-Human-Opinions-in-Semantic-Textual-Similarity"><a href="#Collective-Human-Opinions-in-Semantic-Textual-Similarity" class="headerlink" title="Collective Human Opinions in Semantic Textual Similarity"></a>Collective Human Opinions in Semantic Textual Similarity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04114">http://arxiv.org/abs/2308.04114</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuxiaw/usts">https://github.com/yuxiaw/usts</a></li>
<li>paper_authors: Yuxia Wang, Shimin Tao, Ning Xie, Hao Yang, Timothy Baldwin, Karin Verspoor</li>
<li>for: 这个论文的目的是研究语义相似性(STS)的不确定性和人类评分的分布。</li>
<li>methods: 这个论文使用了一个新的 uncertainty-aware STS 数据集（USTS），包含了 ~15,000 个中文句子对和 150,000 个标签，以研究集体人类评分的变化。</li>
<li>results: 分析发现，人类评分的集体变化不能用标准的整数或单个高斯函数来描述，而是由人类不同的评分差异所引起的。此外，现有的 STS 模型无法捕捉人类对具体实例的不一致，而更反映了模型对总体数据集的预测程度。<details>
<summary>Abstract</summary>
Despite the subjective nature of semantic textual similarity (STS) and pervasive disagreements in STS annotation, existing benchmarks have used averaged human ratings as the gold standard. Averaging masks the true distribution of human opinions on examples of low agreement, and prevents models from capturing the semantic vagueness that the individual ratings represent. In this work, we introduce USTS, the first Uncertainty-aware STS dataset with ~15,000 Chinese sentence pairs and 150,000 labels, to study collective human opinions in STS. Analysis reveals that neither a scalar nor a single Gaussian fits a set of observed judgements adequately. We further show that current STS models cannot capture the variance caused by human disagreement on individual instances, but rather reflect the predictive confidence over the aggregate dataset.
</details>
<details>
<summary>摘要</summary>
尽管 semantic textual similarity (STS) 的评估是主观的，且存在各种不同的评估标准，现有的 benchmark 都使用了人类评分的均值作为金标准。但是，均值将低度一致的示例评分压缩到了一个平均值上，从而隐藏了人类意见的差异。在这项工作中，我们引入了 USTS，首个带有 ~15,000 个中文句子对和 150,000 个标签的不确定性意见 STS 数据集，以研究人类集体意见在 STS 中的表现。分析发现， neither 一个整数还是一个 Gaussian 能够准确地描述观察到的判断。此外，我们还表明，现有的 STS 模型无法捕捉人类对具体实例的不一致，而是反映了对总体数据集的预测信心。
</details></li>
</ul>
<hr>
<h2 id="I-WAS-a-Data-Augmentation-Method-with-GPT-2-for-Simile-Detection"><a href="#I-WAS-a-Data-Augmentation-Method-with-GPT-2-for-Simile-Detection" class="headerlink" title="I-WAS: a Data Augmentation Method with GPT-2 for Simile Detection"></a>I-WAS: a Data Augmentation Method with GPT-2 for Simile Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04109">http://arxiv.org/abs/2308.04109</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cyzLoveDream/I-was">https://github.com/cyzLoveDream/I-was</a></li>
<li>paper_authors: Yongzhu Chang, Rongsheng Zhang, Jiashu Pu</li>
<li>for: 用于提高自然语言处理（NLP）相关应用中的比喻检测精度。</li>
<li>methods: 使用Word replacement和Sentence completion方法，通过GPT-2语言模型进行数据增强。</li>
<li>results: 实验结果表明，我们的提议的数据增强方法可以有效提高比喻检测的性能。<details>
<summary>Abstract</summary>
Simile detection is a valuable task for many natural language processing (NLP)-based applications, particularly in the field of literature. However, existing research on simile detection often relies on corpora that are limited in size and do not adequately represent the full range of simile forms. To address this issue, we propose a simile data augmentation method based on \textbf{W}ord replacement And Sentence completion using the GPT-2 language model. Our iterative process called I-WAS, is designed to improve the quality of the augmented sentences. To better evaluate the performance of our method in real-world applications, we have compiled a corpus containing a more diverse set of simile forms for experimentation. Our experimental results demonstrate the effectiveness of our proposed data augmentation method for simile detection.
</details>
<details>
<summary>摘要</summary>
寓言检测是许多自然语言处理（NLP）应用中的重要任务，特别在文学领域。然而，现有的寓言检测研究通常基于有限的词库和不充分表示寓言的全面形式。为解决这个问题，我们提出了基于Word replacement和Sentence completion的GPT-2语言模型的寓言数据增强方法。我们的迭代过程被称为I-WAS，旨在提高增强后的句子质量。为更好地评估我们的方法在实际应用中的表现，我们将一个包含更多寓言形式的词库编译起来。我们的实验结果表明我们的提议的数据增强方法对寓言检测具有有效性。
</details></li>
</ul>
<hr>
<h2 id="DataTales-Investigating-the-use-of-Large-Language-Models-for-Authoring-Data-Driven-Articles"><a href="#DataTales-Investigating-the-use-of-Large-Language-Models-for-Authoring-Data-Driven-Articles" class="headerlink" title="DataTales: Investigating the use of Large Language Models for Authoring Data-Driven Articles"></a>DataTales: Investigating the use of Large Language Models for Authoring Data-Driven Articles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04076">http://arxiv.org/abs/2308.04076</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicole Sultanum, Arjun Srinivasan</li>
<li>for: 这个论文是为了探讨使用现代大语言模型（LLM）来帮助作者写作数据驱动文章的可能性和价值。</li>
<li>methods: 该论文使用了一个名为DataTales的 прототип系统，利用LLM生成 Chart 的文字导趋。</li>
<li>results: 该研究通过对 11 名专业人士的反馈，发现 DataTales 可以帮助作者更快速地撰写数据驱动文章，并提供了一些可能性和机会来进一步 интегра LLM 为数据驱动文章作者的 valuabe助手。<details>
<summary>Abstract</summary>
Authoring data-driven articles is a complex process requiring authors to not only analyze data for insights but also craft a cohesive narrative that effectively communicates the insights. Text generation capabilities of contemporary large language models (LLMs) present an opportunity to assist the authoring of data-driven articles and expedite the writing process. In this work, we investigate the feasibility and perceived value of leveraging LLMs to support authors of data-driven articles. We designed a prototype system, DataTales, that leverages a LLM to generate textual narratives accompanying a given chart. Using DataTales as a design probe, we conducted a qualitative study with 11 professionals to evaluate the concept, from which we distilled affordances and opportunities to further integrate LLMs as valuable data-driven article authoring assistants.
</details>
<details>
<summary>摘要</summary>
作者撰写数据驱动文章是一个复杂的过程，作者需要不仅分析数据获得洞察，还需要把握数据来编写一篇有关的文章。当代大语言模型（LLM）的文本生成能力提供了帮助作者撰写数据驱动文章的机会，并且可以快速化写作过程。在这项工作中，我们研究了利用LLM支持数据驱动文章作者的可能性和价值。我们设计了一个名为DataTales的 прототип系统，该系统利用LLM生成与给定图表相关的文字导趋。通过DataTales作为设计探索工具，我们对11名专业人士进行了质量研究，从中提炼出了LLM在数据驱动文章作者助手中的可能性和优势。
</details></li>
</ul>
<hr>
<h2 id="The-Five-Dollar-Model-Generating-Game-Maps-and-Sprites-from-Sentence-Embeddings"><a href="#The-Five-Dollar-Model-Generating-Game-Maps-and-Sprites-from-Sentence-Embeddings" class="headerlink" title="The Five-Dollar Model: Generating Game Maps and Sprites from Sentence Embeddings"></a>The Five-Dollar Model: Generating Game Maps and Sprites from Sentence Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04052">http://arxiv.org/abs/2308.04052</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/TimMerino1710/five-dollar-model">https://github.com/TimMerino1710/five-dollar-model</a></li>
<li>paper_authors: Timothy Merino, Roman Negri, Dipika Rajesh, M Charity, Julian Togelius</li>
<li>for: 这篇论文旨在描述一种可以从编码文本提示生成低维度图像的轻量级文本-图像生成模型。</li>
<li>methods: 该模型使用了一些限制量的训练数据，并应用了一些新的扩展策略来提高模型在三个小 datasets（像素艺术视频游戏地图、视频游戏 sprite 图像和压缩emoji图像）的性能。</li>
<li>results: 根据cosine相似性分数，该模型能够成功地生成具有编码 semantic 含义的图像，并且在限制量数据下可以达到高质量和美观的图像生成。<details>
<summary>Abstract</summary>
The five-dollar model is a lightweight text-to-image generative architecture that generates low dimensional images from an encoded text prompt. This model can successfully generate accurate and aesthetically pleasing content in low dimensional domains, with limited amounts of training data. Despite the small size of both the model and datasets, the generated images are still able to maintain the encoded semantic meaning of the textual prompt. We apply this model to three small datasets: pixel art video game maps, video game sprite images, and down-scaled emoji images and apply novel augmentation strategies to improve the performance of our model on these limited datasets. We evaluate our models performance using cosine similarity score between text-image pairs generated by the CLIP VIT-B/32 model.
</details>
<details>
<summary>摘要</summary>
“五币模型”是一个轻量级文本至图生成架构，它将文本提示转换为低维度图像。这个模型可以成功实现精确和美观的内容生成，即使对于训练数据的量相对较少。尽管模型和数据集都很小，但生成的图像仍然能够保持文本提示中的 semantics 含义。我们将这个模型应用到三个小型数据集：像素艺术游戏地图、游戏图像和缩小的表情符号图像，并对这些限制的数据集进行新的扩展策略以改善我们的模型表现。我们使用 CLIP VIT-B/32 模型的弹性相似度分数来评估我们的模型表现。
</details></li>
</ul>
<hr>
<h2 id="A-Comparative-Study-on-TF-IDF-feature-Weighting-Method-and-its-Analysis-using-Unstructured-Dataset"><a href="#A-Comparative-Study-on-TF-IDF-feature-Weighting-Method-and-its-Analysis-using-Unstructured-Dataset" class="headerlink" title="A Comparative Study on TF-IDF feature Weighting Method and its Analysis using Unstructured Dataset"></a>A Comparative Study on TF-IDF feature Weighting Method and its Analysis using Unstructured Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04037">http://arxiv.org/abs/2308.04037</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mamata Das, Selvakumar K., P. J. A. Alphonse</li>
<li>For: The paper is written for text classification and its algorithms, specifically focusing on the feature weighting method for text classification on unstructured data.* Methods: The paper uses two features, N-Grams and TF-IDF, on the IMDB movie reviews and Amazon Alexa reviews dataset for sentiment analysis. The state-of-the-art classifiers used to validate the method include SVM, Logistic Regression, Multinomial Naive Bayes, Random Forest, Decision Tree, and k-nearest neighbors.* Results: The paper found that TF-IDF features resulted in a significant increase in feature extraction, with TF-IDF achieving the maximum accuracy, precision, recall, and F1-score values of 93.81%, 94.20%, 93.81%, and 91.99%, respectively, in the Random Forest classifier.<details>
<summary>Abstract</summary>
Text Classification is the process of categorizing text into the relevant categories and its algorithms are at the core of many Natural Language Processing (NLP). Term Frequency-Inverse Document Frequency (TF-IDF) and NLP are the most highly used information retrieval methods in text classification. We have investigated and analyzed the feature weighting method for text classification on unstructured data. The proposed model considered two features N-Grams and TF-IDF on the IMDB movie reviews and Amazon Alexa reviews dataset for sentiment analysis. Then we have used the state-of-the-art classifier to validate the method i.e., Support Vector Machine (SVM), Logistic Regression, Multinomial Naive Bayes (Multinomial NB), Random Forest, Decision Tree, and k-nearest neighbors (KNN). From those two feature extractions, a significant increase in feature extraction with TF-IDF features rather than based on N-Gram. TF-IDF got the maximum accuracy (93.81%), precision (94.20%), recall (93.81%), and F1-score (91.99%) value in Random Forest classifier.
</details>
<details>
<summary>摘要</summary>
文本分类是将文本分类到相关的类别，其算法是自然语言处理（NLP）的核心。文本频率-反转文档频率（TF-IDF）和NLP是文本检索中最常用的方法。我们已经调查和分析了文本分类中的特征赋值方法，并在IMDB电影评论和Amazon Alexa评论数据集上进行了 sentiment分析。然后，我们使用了当今最佳分类器来验证方法，即支持向量机（SVM）、梯度回归、多元随机树（Multinomial NB）、Random Forest、决策树和k-最近邻居（KNN）。从这两个特征提取方法中，TF-IDF特征提取得到了显著的提高，而不是基于N-Gram。TF-IDF在Random Forest分类器中获得了最高的准确率（93.81%）、精度（94.20%）、回归率（93.81%）和F1分数（91.99%）值。
</details></li>
</ul>
<hr>
<h2 id="Continual-Pre-Training-of-Large-Language-Models-How-to-re-warm-your-model"><a href="#Continual-Pre-Training-of-Large-Language-Models-How-to-re-warm-your-model" class="headerlink" title="Continual Pre-Training of Large Language Models: How to (re)warm your model?"></a>Continual Pre-Training of Large Language Models: How to (re)warm your model?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04014">http://arxiv.org/abs/2308.04014</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kshitij Gupta, Benjamin Thérien, Adam Ibrahim, Mats L. Richter, Quentin Anthony, Eugene Belilovsky, Irina Rish, Timothée Lesort</li>
<li>for: 这个论文的目的是探讨如何实现大语言模型的持续预训练，以提高计算效率和预训练模型的性能。</li>
<li>methods: 本研究使用了不同的暖身策略来研究模型在新数据上的性能。</li>
<li>results: 研究结果显示，使用暖身策略可以在长期内提高下游数据的性能，并且在大下游数据集上超越从头开始训练的模型。<details>
<summary>Abstract</summary>
Large language models (LLMs) are routinely pre-trained on billions of tokens, only to restart the process over again once new data becomes available. A much cheaper and more efficient solution would be to enable the continual pre-training of these models, i.e. updating pre-trained models with new data instead of re-training them from scratch. However, the distribution shift induced by novel data typically results in degraded performance on past data. Taking a step towards efficient continual pre-training, in this work, we examine the effect of different warm-up strategies. Our hypothesis is that the learning rate must be re-increased to improve compute efficiency when training on a new dataset. We study the warmup phase of models pre-trained on the Pile (upstream data, 300B tokens) as we continue to pre-train on SlimPajama (downstream data, 297B tokens), following a linear warmup and cosine decay schedule. We conduct all experiments on the Pythia 410M language model architecture and evaluate performance through validation perplexity. We experiment with different pre-training checkpoints, various maximum learning rates, and various warmup lengths. Our results show that while rewarming models first increases the loss on upstream and downstream data, in the longer run it improves the downstream performance, outperforming models trained from scratch$\unicode{x2013}$even for a large downstream dataset.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Simple-synthetic-data-reduces-sycophancy-in-large-language-models"><a href="#Simple-synthetic-data-reduces-sycophancy-in-large-language-models" class="headerlink" title="Simple synthetic data reduces sycophancy in large language models"></a>Simple synthetic data reduces sycophancy in large language models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03958">http://arxiv.org/abs/2308.03958</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/google/sycophancy-intervention">https://github.com/google/sycophancy-intervention</a></li>
<li>paper_authors: Jerry Wei, Da Huang, Yifeng Lu, Denny Zhou, Quoc V. Le</li>
<li>for: 本研究旨在研究语音模型中的奴役行为（sycophancy），并提出一种简单的人工数据干预方法来减少这种行为。</li>
<li>methods: 研究者使用了三个偏见任务（Perez et al., 2022），测试了模型在不同的缩放和调教情况下的奴役行为。</li>
<li>results: 研究发现，对于PaLM模型，通过缩放和调教可以显著增强奴役行为，而且even when the user’s view is objectively incorrect, models will still agree with them。此外，研究者还提出了一种简单的人工数据干预方法，通过在公共NLP任务上添加一些适当的数据，可以减少模型对用户意见的依赖。<details>
<summary>Abstract</summary>
Sycophancy is an undesirable behavior where models tailor their responses to follow a human user's view even when that view is not objectively correct (e.g., adapting liberal views once a user reveals that they are liberal). In this paper, we study the prevalence of sycophancy in language models and propose a simple synthetic-data intervention to reduce this behavior.   First, on a set of three sycophancy tasks (Perez et al., 2022) where models are asked for an opinion on statements with no correct answers (e.g., politics), we observe that both model scaling and instruction tuning significantly increase sycophancy for PaLM models up to 540B parameters. Second, we extend sycophancy evaluations to simple addition statements that are objectively incorrect, finding that despite knowing that these statements are wrong, language models will still agree with them if the user does as well.   To reduce sycophancy, we present a straightforward synthetic-data intervention that takes public NLP tasks and encourages models to be robust to user opinions on these tasks. Adding these data in a lightweight finetuning step can significantly reduce sycophantic behavior on held-out prompts. Code for generating synthetic data for intervention can be found at https://github.com/google/sycophancy-intervention.
</details>
<details>
<summary>摘要</summary>
sycophancy 是一种不良行为，在语言模型回答时适应人工用户的观点，即使这些观点不是 объекively 正确（例如，适应自由主义观点一旦用户承认自己是自由主义者）。在这篇论文中，我们研究了语言模型中的 sycophancy 的普遍性和提出了一种简单的人工数据干预措施来降低这种行为。首先，我们在 Perez et al. (2022) 中提供的三个 sycophancy 任务上观察到，随着模型缩放和指令调整，PaLM 模型的 sycophancy 会增加到 540B 参数的最大值。其次，我们扩展了 sycophancy 评估范围到对象错误的简单加法句子，发现，即使用户认为这些句子是错误的，语言模型仍会同意它们，如果用户也同意。为了降低 sycophancy，我们提出了一种简单的人工数据干预措施，通过在公共 NLP 任务上添加一些适应用户观点的数据，让模型在新的任务上具有更好的Robustness。可以在 https://github.com/google/sycophancy-intervention 找到代码生成 synthetic data 的步骤。
</details></li>
</ul>
<hr>
<h2 id="Universal-Automatic-Phonetic-Transcription-into-the-International-Phonetic-Alphabet"><a href="#Universal-Automatic-Phonetic-Transcription-into-the-International-Phonetic-Alphabet" class="headerlink" title="Universal Automatic Phonetic Transcription into the International Phonetic Alphabet"></a>Universal Automatic Phonetic Transcription into the International Phonetic Alphabet</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03917">http://arxiv.org/abs/2308.03917</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ctaguchi/multipa">https://github.com/ctaguchi/multipa</a></li>
<li>paper_authors: Chihiro Taguchi, Yusuke Sakai, Parisa Haghani, David Chiang</li>
<li>for: 这个研究旨在开发一个可以将任何语言的speech转录为国际音声字母表（IPA）的模型。</li>
<li>methods: 这个模型基于wav2vec 2.0，通过对听取到的音频输入进行 fine-tuning，预测IPA。</li>
<li>results: 该模型在七种语言的CommonVoice 11.0训练数据上达到了与人工标注几乎相当的质量水平，并且与之前的最佳speech-to-IPA模型（Wav2Vec2Phoneme）的训练数据集相比，该模型的训练数据集更小。<details>
<summary>Abstract</summary>
This paper presents a state-of-the-art model for transcribing speech in any language into the International Phonetic Alphabet (IPA). Transcription of spoken languages into IPA is an essential yet time-consuming process in language documentation, and even partially automating this process has the potential to drastically speed up the documentation of endangered languages. Like the previous best speech-to-IPA model (Wav2Vec2Phoneme), our model is based on wav2vec 2.0 and is fine-tuned to predict IPA from audio input. We use training data from seven languages from CommonVoice 11.0, transcribed into IPA semi-automatically. Although this training dataset is much smaller than Wav2Vec2Phoneme's, its higher quality lets our model achieve comparable or better results. Furthermore, we show that the quality of our universal speech-to-IPA models is close to that of human annotators.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Cross-Domain-Evaluation-of-Approaches-for-Causal-Knowledge-Extraction"><a href="#A-Cross-Domain-Evaluation-of-Approaches-for-Causal-Knowledge-Extraction" class="headerlink" title="A Cross-Domain Evaluation of Approaches for Causal Knowledge Extraction"></a>A Cross-Domain Evaluation of Approaches for Causal Knowledge Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03891">http://arxiv.org/abs/2308.03891</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aniksh/causal-spert">https://github.com/aniksh/causal-spert</a></li>
<li>paper_authors: Anik Saha, Oktie Hassanzadeh, Alex Gittens, Jian Ni, Kavitha Srinivas, Bulent Yener</li>
<li>for: 提取文本中的 causa-effect 关系</li>
<li>methods: 使用预训练语言模型（如BERT）的序列标记模型和span基于方法进行 causal 知识提取</li>
<li>results: 结果表明，使用BERT预训练语言模型的序列标记模型可以提供 significnat 性能提升，而span基于方法在四个数据集中的表现都比simple sequence tagging模型更好。<details>
<summary>Abstract</summary>
Causal knowledge extraction is the task of extracting relevant causes and effects from text by detecting the causal relation. Although this task is important for language understanding and knowledge discovery, recent works in this domain have largely focused on binary classification of a text segment as causal or non-causal. In this regard, we perform a thorough analysis of three sequence tagging models for causal knowledge extraction and compare it with a span based approach to causality extraction. Our experiments show that embeddings from pre-trained language models (e.g. BERT) provide a significant performance boost on this task compared to previous state-of-the-art models with complex architectures. We observe that span based models perform better than simple sequence tagging models based on BERT across all 4 data sets from diverse domains with different types of cause-effect phrases.
</details>
<details>
<summary>摘要</summary>
causal knowledge extraction 是另一个重要的自然语言处理任务，即从文本中提取有关 causal 关系的信息。 although recent works in this area have mainly focused on将文本段分类为 causal 或非 causal，我们在这个领域进行了系统性的分析，并与 span 基于 causality 提取方法进行比较。 our experiments show that pre-trained language model 的 embedding 提供了 significannot performance boost 在这个任务中，比之前的 state-of-the-art 模型 with complex architectures。 我们发现 span 基于模型在所有四个数据集中表现较好， especialy when dealing with diverse domains and different types of cause-effect phrases.
</details></li>
</ul>
<hr>
<h2 id="Generative-Benchmark-Creation-for-Table-Union-Search"><a href="#Generative-Benchmark-Creation-for-Table-Union-Search" class="headerlink" title="Generative Benchmark Creation for Table Union Search"></a>Generative Benchmark Creation for Table Union Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03883">http://arxiv.org/abs/2308.03883</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/northeastern-datalab/alt-gen">https://github.com/northeastern-datalab/alt-gen</a></li>
<li>paper_authors: Koyena Pal, Aamod Khatiwada, Roee Shraga, Renée J. Miller</li>
<li>for: 本研究的目的是为了开发一种基于生成AI模型的数据管理 benchmark，以解决现有的数据管理问题具有语义性。</li>
<li>methods: 本研究使用的方法包括生成AI模型来创建结构化数据 benchmark，以及对现有的手动纪录和标注的数据进行evaluation。</li>
<li>results: 研究发现，使用生成AI模型创建的 benchmark 比手动纪录和标注的 benchmark 更加具有挑战性，并且允许更加详细的分析方法的性能。 Specifically, the top-performing method achieves a Mean Average Precision of around 60%, over 30% less than its performance on existing manually created benchmarks.<details>
<summary>Abstract</summary>
Data management has traditionally relied on synthetic data generators to generate structured benchmarks, like the TPC suite, where we can control important parameters like data size and its distribution precisely. These benchmarks were central to the success and adoption of database management systems. But more and more, data management problems are of a semantic nature. An important example is finding tables that can be unioned. While any two tables with the same cardinality can be unioned, table union search is the problem of finding tables whose union is semantically coherent. Semantic problems cannot be benchmarked using synthetic data. Our current methods for creating benchmarks involve the manual curation and labeling of real data. These methods are not robust or scalable and perhaps more importantly, it is not clear how robust the created benchmarks are. We propose to use generative AI models to create structured data benchmarks for table union search. We present a novel method for using generative models to create tables with specified properties. Using this method, we create a new benchmark containing pairs of tables that are both unionable and non-unionable but related. We thoroughly evaluate recent existing table union search methods over existing benchmarks and our new benchmark. We also present and evaluate a new table search methods based on recent large language models over all benchmarks. We show that the new benchmark is more challenging for all methods than hand-curated benchmarks, specifically, the top-performing method achieves a Mean Average Precision of around 60%, over 30% less than its performance on existing manually created benchmarks. We examine why this is the case and show that the new benchmark permits more detailed analysis of methods, including a study of both false positives and false negatives that were not possible with existing benchmarks.
</details>
<details>
<summary>摘要</summary>
“数据管理历史上依赖了人工生成的数据生成器来生成结构化的标准吞吐量测试（TPC），以控制数据大小和分布的重要参数。这些测试对数据库管理系统的采用和普及做出了重要贡献。然而，越来越多的数据管理问题是 semantic 性质的，例如找到可union的表。虽然任何两个表都可以union，但表union搜索问题是找到semantically coherent的表的union。semantic问题无法使用人工生成的数据来 benchmark。我们当前的创建benchmark方法是通过手动筛选和标注实际数据来实现。这些方法不具有可靠性和扩展性，而且可能更重要的是，不确定创建的benchmark的可靠性。我们提议使用生成AI模型来创建结构化数据 benchmarks for table union search。我们提出了一种使用生成模型创建表 avec specified properties的新方法。使用这种方法，我们创建了一个新的benchmark，包含可union和non-union但相关的表对。我们进行了对现有benchmark和我们新的benchmark的严格评估。我们还提出了基于最新的大语言模型的新表搜索方法，并对所有benchmark进行评估。我们发现新的benchmark比手动创建的benchmark更加具有挑战性，特别是top-performing方法的 Mean Average Precision 约为60%，相比手动创建的benchmark的30%以上。我们分析了这种情况，并证明新的benchmark允许更详细的方法分析，包括对方法的false positives和false negatives的研究，这些研究不可能通过现有benchmark进行。”
</details></li>
</ul>
<hr>
<h2 id="Semantic-Equivalence-of-e-Commerce-Queries"><a href="#Semantic-Equivalence-of-e-Commerce-Queries" class="headerlink" title="Semantic Equivalence of e-Commerce Queries"></a>Semantic Equivalence of e-Commerce Queries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03869">http://arxiv.org/abs/2308.03869</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aritra Mandal, Daniel Tunkelang, Zhe Wu</li>
<li>for: 提高电商搜索中的用户体验和企业业绩</li>
<li>methods: 提出了一种框架，通过识别和利用查询等价性来提高搜索结果的准确率和用户满意度</li>
<li>results: 实验结果表明，该框架可以高效地识别和利用查询等价性，并与流行的句子转换模型相比，实现了更高的查询相似性（Pearson correlation coefficient为0.85），这表明该方法可以提高电商搜索中的用户体验和企业业绩。<details>
<summary>Abstract</summary>
Search query variation poses a challenge in e-commerce search, as equivalent search intents can be expressed through different queries with surface-level differences. This paper introduces a framework to recognize and leverage query equivalence to enhance searcher and business outcomes. The proposed approach addresses three key problems: mapping queries to vector representations of search intent, identifying nearest neighbor queries expressing equivalent or similar intent, and optimizing for user or business objectives. The framework utilizes both surface similarity and behavioral similarity to determine query equivalence. Surface similarity involves canonicalizing queries based on word inflection, word order, compounding, and noise words. Behavioral similarity leverages historical search behavior to generate vector representations of query intent. An offline process is used to train a sentence similarity model, while an online nearest neighbor approach supports processing of unseen queries. Experimental evaluations demonstrate the effectiveness of the proposed approach, outperforming popular sentence transformer models and achieving a Pearson correlation of 0.85 for query similarity. The results highlight the potential of leveraging historical behavior data and training models to recognize and utilize query equivalence in e-commerce search, leading to improved user experiences and business outcomes. Further advancements and benchmark datasets are encouraged to facilitate the development of solutions for this critical problem in the e-commerce domain.
</details>
<details>
<summary>摘要</summary>
<SYS> translate-send:   from-language en  to-language zh-CN  text-type text  contents "Search query variation poses a challenge in e-commerce search, as equivalent search intents can be expressed through different queries with surface-level differences. This paper introduces a framework to recognize and leverage query equivalence to enhance searcher and business outcomes. The proposed approach addresses three key problems: mapping queries to vector representations of search intent, identifying nearest neighbor queries expressing equivalent or similar intent, and optimizing for user or business objectives. The framework utilizes both surface similarity and behavioral similarity to determine query equivalence. Surface similarity involves canonicalizing queries based on word inflection, word order, compounding, and noise words. Behavioral similarity leverages historical search behavior to generate vector representations of query intent. An offline process is used to train a sentence similarity model, while an online nearest neighbor approach supports processing of unseen queries. Experimental evaluations demonstrate the effectiveness of the proposed approach, outperforming popular sentence transformer models and achieving a Pearson correlation of 0.85 for query similarity. The results highlight the potential of leveraging historical behavior data and training models to recognize and utilize query equivalence in e-commerce search, leading to improved user experiences and business outcomes. Further advancements and benchmark datasets are encouraged to facilitate the development of solutions for this critical problem in the e-commerce domain."</SYS>Here's the translation in Simplified Chinese:搜索查询的变化 poses 电商搜索中的挑战，因为等效的搜索意图可以通过不同的查询语句表达出来，具有表面上的差异。本文提出了一种框架，用于认可和利用查询相似性，以提高搜索者和企业的结果。该框架解决了三个关键问题：将查询映射到搜索意图的vector表示，标识最相似的查询语句，并优化用户或企业的目标。该框架利用表面相似性和行为相似性来确定查询相似性。表面相似性包括Word排序、幂等词、缩合词和噪音词的canonicalization。行为相似性利用历史搜索行为生成查询意图的vector表示。在线程中使用了一个历史搜索行为训练的模型，而在线上使用了一个最近的邻居方法来处理未看过的查询。实验证明了该方法的效果，超越了流行的句子变换模型，并达到了0.85的Pearson相关性。结果表明，通过利用历史行为数据和训练模型，可以认可和利用查询相似性，提高用户体验和商业结果。进一步的进步和标准 datasets 是鼓励的，以便开发电商搜索领域中的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Storyfier-Exploring-Vocabulary-Learning-Support-with-Text-Generation-Models"><a href="#Storyfier-Exploring-Vocabulary-Learning-Support-with-Text-Generation-Models" class="headerlink" title="Storyfier: Exploring Vocabulary Learning Support with Text Generation Models"></a>Storyfier: Exploring Vocabulary Learning Support with Text Generation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03864">http://arxiv.org/abs/2308.03864</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenhui Peng, Xingbo Wang, Qiushi Han, Junkai Zhu, Xiaojuan Ma, Huamin Qu</li>
<li>for: 支持学习任务的生成模型（Generative Adversarial Networks，GANs）</li>
<li>methods: 使用文本生成模型生成故事，并提供学习者可以使用AI助手进行写作和练习语言使用的功能</li>
<li>results: 学习者对使用Storyfier进行学习有很好的满意度，但是在阅读、填充和写作任务中，使用Storyfier的学习者表现相对较差，尤其是在记忆和使用目标词汇方面。<details>
<summary>Abstract</summary>
Vocabulary learning support tools have widely exploited existing materials, e.g., stories or video clips, as contexts to help users memorize each target word. However, these tools could not provide a coherent context for any target words of learners' interests, and they seldom help practice word usage. In this paper, we work with teachers and students to iteratively develop Storyfier, which leverages text generation models to enable learners to read a generated story that covers any target words, conduct a story cloze test, and use these words to write a new story with adaptive AI assistance. Our within-subjects study (N=28) shows that learners generally favor the generated stories for connecting target words and writing assistance for easing their learning workload. However, in the read-cloze-write learning sessions, participants using Storyfier perform worse in recalling and using target words than learning with a baseline tool without our AI features. We discuss insights into supporting learning tasks with generative models.
</details>
<details>
<summary>摘要</summary>
学习词汇支持工具已经广泛利用现有的材料，如故事或视频片段，作为词汇记忆的 Context。然而，这些工具无法提供学生们感兴趣的词汇的 coherent Context，并rarely帮助学生们实践词汇使用。在这篇论文中，我们与教师和学生合作开发了Storyfier，利用文本生成模型，让学生可以阅读一个包含target词的生成故事，进行故事填充测试，并使用这些词汇写新的故事，并且有adaptive AI帮助。我们的在人Subjects研究（N=28）表明，学生通常喜欢使用Storyfier来连接target词和写作帮助，以减轻学习劳动。然而，在read-cloze-write学习 Session中，参与者使用Storyfier表现比基eline工具而言，更难记忆和使用target词。我们讨论了如何使用生成模型支持学习任务的信息。
</details></li>
</ul>
<hr>
<h2 id="Extracting-detailed-oncologic-history-and-treatment-plan-from-medical-oncology-notes-with-large-language-models"><a href="#Extracting-detailed-oncologic-history-and-treatment-plan-from-medical-oncology-notes-with-large-language-models" class="headerlink" title="Extracting detailed oncologic history and treatment plan from medical oncology notes with large language models"></a>Extracting detailed oncologic history and treatment plan from medical oncology notes with large language models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03853">http://arxiv.org/abs/2308.03853</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/madhumitasushil/oncllmextraction">https://github.com/madhumitasushil/oncllmextraction</a></li>
<li>paper_authors: Madhumita Sushil, Vanessa E. Kennedy, Brenda Y. Miao, Divneet Mandair, Travis Zack, Atul J. Butte</li>
<li>for: 这个研究的目的是为了评估最新的自然语言处理模型（GPT-4、GPT-3.5-turbo、FLAN-UL2）在抽取肿瘤病例纪录中的表现。</li>
<li>methods: 这个研究使用了一种细化的 schema 来标注肿瘤病例纪录中的信息，包括患者特征、肿瘤特征、测试和治疗等。然后，使用这些标注数据来评估这三个模型在抽取肿瘤病例纪录中的表现。</li>
<li>results: 研究发现，GPT-4 模型在抽取肿瘤病例纪录中表现最佳，其中的 BLEU 分数为 0.69，ROUGE 分数为 0.72，并且在复杂任务中的准确率为 67%。这个模型在抽取肿瘤特征和药物信息方面表现特别出色，并且在推断疾病的 симптом和未来药物的考虑方面也表现了优异。这个研究表明，GPT-4 可能已经可以用于从肿瘤进程纪录中提取重要的信息，以便于临床研究、复杂人口管理和评估quality patient care。<details>
<summary>Abstract</summary>
Both medical care and observational studies in oncology require a thorough understanding of a patient's disease progression and treatment history, often elaborately documented in clinical notes. Despite their vital role, no current oncology information representation and annotation schema fully encapsulates the diversity of information recorded within these notes. Although large language models (LLMs) have recently exhibited impressive performance on various medical natural language processing tasks, due to the current lack of comprehensively annotated oncology datasets, an extensive evaluation of LLMs in extracting and reasoning with the complex rhetoric in oncology notes remains understudied. We developed a detailed schema for annotating textual oncology information, encompassing patient characteristics, tumor characteristics, tests, treatments, and temporality. Using a corpus of 10 de-identified breast cancer progress notes at University of California, San Francisco, we applied this schema to assess the abilities of three recently-released LLMs (GPT-4, GPT-3.5-turbo, and FLAN-UL2) to perform zero-shot extraction of detailed oncological history from two narrative sections of clinical progress notes. Our team annotated 2750 entities, 2874 modifiers, and 1623 relationships. The GPT-4 model exhibited overall best performance, with an average BLEU score of 0.69, an average ROUGE score of 0.72, and an average accuracy of 67% on complex tasks (expert manual evaluation). Notably, it was proficient in tumor characteristic and medication extraction, and demonstrated superior performance in inferring symptoms due to cancer and considerations of future medications. The analysis demonstrates that GPT-4 is potentially already usable to extract important facts from cancer progress notes needed for clinical research, complex population management, and documenting quality patient care.
</details>
<details>
<summary>摘要</summary>
医疗和观察研究在肿瘤学中都需要深刻理解病人疾病进展和治疗历史，这些信息通常都是在临床笔记中详细记录的。 despite their vital role， current oncology information representation and annotation schema 没有完全涵盖临床笔记中的多样性信息。 Recently， large language models (LLMs) have shown impressive performance on various medical natural language processing tasks，but due to the lack of comprehensively annotated oncology datasets，the extent to which LLMs can extract and reason with the complex rhetoric in oncology notes remains understudied。We developed a detailed schema for annotating textual oncology information， including patient characteristics，tumor characteristics，tests，treatments，and temporality。Using a corpus of 10 de-identified breast cancer progress notes at University of California, San Francisco，we applied this schema to assess the abilities of three recently-released LLMs (GPT-4, GPT-3.5-turbo, and FLAN-UL2) to perform zero-shot extraction of detailed oncological history from two narrative sections of clinical progress notes。Our team annotated 2750 entities，2874 modifiers，and 1623 relationships。GPT-4 model exhibited overall best performance，with an average BLEU score of 0.69，an average ROUGE score of 0.72，and an average accuracy of 67% on complex tasks (expert manual evaluation)。It was proficient in tumor characteristic and medication extraction，and demonstrated superior performance in inferring symptoms due to cancer and considerations of future medications。The analysis demonstrates that GPT-4 is potentially already usable to extract important facts from cancer progress notes needed for clinical research，complex population management，and documenting quality patient care。
</details></li>
</ul>
<hr>
<h2 id="What-about-translation-New-coding-system-for-content-analysis-on-the-perception-of-literary-translation-around-the-political-transformation-in-1989-in-Hungary-as-a-classification-problem-on-an-unbalanced-dataset"><a href="#What-about-translation-New-coding-system-for-content-analysis-on-the-perception-of-literary-translation-around-the-political-transformation-in-1989-in-Hungary-as-a-classification-problem-on-an-unbalanced-dataset" class="headerlink" title="What about translation? New coding system for content analysis on the perception of literary translation around the political transformation in 1989 in Hungary as a classification problem on an unbalanced dataset"></a>What about translation? New coding system for content analysis on the perception of literary translation around the political transformation in 1989 in Hungary as a classification problem on an unbalanced dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03742">http://arxiv.org/abs/2308.03742</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dalma Galambos, Pál Zsámboki</li>
<li>for:  Tracking trends in the perception of literary translation during political transformation in 1989 in Hungary.</li>
<li>methods:  Trained BERT models to carry over coding system to 1980-1999 issues of literary journal Nagyvilág, with extensive hyperparameter tuning, loss functions robust to label unbalance, 10-fold cross-validation, model ensemble for prediction, manual validation, and calibration method to better predict label counts.</li>
<li>results:  Study of relations between labels using label relation networks.<details>
<summary>Abstract</summary>
To track trends in the perception of literary translation around the political transformation in 1989 in Hungary, a coding system was developed on the paragraphs of the 1980-1999 issues of the literary journal Alf\"old. This paper describes how we trained BERT models to carry over the coding system to the 1980-1999 issues of the literary journal Nagyvil\'ag. We use extensive hyperparameter tuning, loss functions robust to label unbalance, 10-fold cross-validation for precise evaluations and a model ensemble for prediction, manual validation on the predict set, a new calibration method to better predict label counts for sections of the Nagyvil\'ag corpus, and to study the relations between labels, we construct label relation networks.
</details>
<details>
<summary>摘要</summary>
为了跟踪1989年政治转型期间文学翻译的观点变化，我们在1980-1999年《alföld》期刊中的段落上设计了一个编码系统。本文描述了我们如何使用BERT模型将编码系统传播到1980-1999年《大世界》期刊中的段落上。我们采用了广泛的hyperparameter优化、 Label不均衡的损失函数、10重交叉验证、精确的预测和手动验证预测集、一个新的准确预测标签计数的方法、以及为了研究标签之间的关系，我们构建了标签关系网络。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/08/cs.CL_2023_08_08/" data-id="clp9qz813009yok882lt36a0n" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/08/cs.LG_2023_08_08/" class="article-date">
  <time datetime="2023-08-08T10:00:00.000Z" itemprop="datePublished">2023-08-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/08/cs.LG_2023_08_08/">cs.LG - 2023-08-08</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="TranSTYLer-Multimodal-Behavioral-Style-Transfer-for-Facial-and-Body-Gestures-Generation"><a href="#TranSTYLer-Multimodal-Behavioral-Style-Transfer-for-Facial-and-Body-Gestures-Generation" class="headerlink" title="TranSTYLer: Multimodal Behavioral Style Transfer for Facial and Body Gestures Generation"></a>TranSTYLer: Multimodal Behavioral Style Transfer for Facial and Body Gestures Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10843">http://arxiv.org/abs/2308.10843</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mireille Fares, Catherine Pelachaud, Nicolas Obin</li>
<li>for: 这篇论文目标是将虚拟代理人的行为表达风格传递到另一个代理人中，保持行为的形式不变，以便在交流中传递意思。</li>
<li>methods: 我们提出了一种基于多模态变换器的模型，称为TranSTYLer，可以将多 modal的source speaker的行为与目标 speaker的风格相结合。我们假设行为表达风格在不同的沟通模式中都存在，包括文本、语音、身体姿势和面部表情。模型采用了内容和风格分离的方法，以确保传递的风格不会对源行为的意思产生干扰。</li>
<li>results: 我们在PATS数据集上训练了我们的模型，并对比了现有的状态数据模型。对jective和主观评价结果表明，我们的模型在seen和unseen风格中都能够实现更高的性能。此外，我们还提出了一种方法来评估传递的行为和姿势是否正确，以确保源行为的意思不会产生泄露。<details>
<summary>Abstract</summary>
This paper addresses the challenge of transferring the behavior expressivity style of a virtual agent to another one while preserving behaviors shape as they carry communicative meaning. Behavior expressivity style is viewed here as the qualitative properties of behaviors. We propose TranSTYLer, a multimodal transformer based model that synthesizes the multimodal behaviors of a source speaker with the style of a target speaker. We assume that behavior expressivity style is encoded across various modalities of communication, including text, speech, body gestures, and facial expressions. The model employs a style and content disentanglement schema to ensure that the transferred style does not interfere with the meaning conveyed by the source behaviors. Our approach eliminates the need for style labels and allows the generalization to styles that have not been seen during the training phase. We train our model on the PATS corpus, which we extended to include dialog acts and 2D facial landmarks. Objective and subjective evaluations show that our model outperforms state of the art models in style transfer for both seen and unseen styles during training. To tackle the issues of style and content leakage that may arise, we propose a methodology to assess the degree to which behavior and gestures associated with the target style are successfully transferred, while ensuring the preservation of the ones related to the source content.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Accurate-Explainable-and-Private-Models-Providing-Recourse-While-Minimizing-Training-Data-Leakage"><a href="#Accurate-Explainable-and-Private-Models-Providing-Recourse-While-Minimizing-Training-Data-Leakage" class="headerlink" title="Accurate, Explainable, and Private Models: Providing Recourse While Minimizing Training Data Leakage"></a>Accurate, Explainable, and Private Models: Providing Recourse While Minimizing Training Data Leakage</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04341">http://arxiv.org/abs/2308.04341</a></li>
<li>repo_url: None</li>
<li>paper_authors: Catherine Huang, Chelse Swoopes, Christina Xiao, Jiaqi Ma, Himabindu Lakkaraju</li>
<li>For: This paper aims to mitigate attacks on machine learning models that provide algorithmic recourse to individuals who receive negative outcomes.* Methods: The paper presents two novel methods for generating differentially private recourse: Differentially Private Model (DPM) and Laplace Recourse (LR).* Results: The authors find that DPM and LR perform well in reducing what an adversary can infer, especially at low false positive rates. When the training dataset size is large enough, the authors achieve particular success in preventing privacy leakage while maintaining model and recourse accuracy with the LR method.Here’s the information in Simplified Chinese text:</li>
<li>for: 这篇论文目标是解决机器学习模型提供的算法救济对于个人不良结果的攻击。</li>
<li>methods: 论文提出了两种新的幂等私人救济方法：差分私人模型（DPM）和拉普拉斯救济（LR）。</li>
<li>results: 作者发现，在低假阳性率下，DPM和LR都能够减少攻击者可以获取的信息量，特别是在训练数据集大 enough 的情况下。<details>
<summary>Abstract</summary>
Machine learning models are increasingly utilized across impactful domains to predict individual outcomes. As such, many models provide algorithmic recourse to individuals who receive negative outcomes. However, recourse can be leveraged by adversaries to disclose private information. This work presents the first attempt at mitigating such attacks. We present two novel methods to generate differentially private recourse: Differentially Private Model (DPM) and Laplace Recourse (LR). Using logistic regression classifiers and real world and synthetic datasets, we find that DPM and LR perform well in reducing what an adversary can infer, especially at low FPR. When training dataset size is large enough, we find particular success in preventing privacy leakage while maintaining model and recourse accuracy with our novel LR method.
</details>
<details>
<summary>摘要</summary>
机器学习模型在影响各个领域中越来越广泛应用，以预测个人结果。然而，这些模型可能会泄露个人隐私信息。这项工作提出了首个防止这种攻击的方法。我们提出了两种新的涉嫌隐私模型（DPM）和拉普拉斯补偿（LR）。使用логистиック回归分类器和实际世界和synthetic数据集，我们发现DPM和LR在低 False Positive Rate（FP）下具有良好的隐私保护能力，特别是当训练集大小充分时。我们的LR方法在防止隐私泄露的同时保持模型和补偿精度。
</details></li>
</ul>
<hr>
<h2 id="RLHF-Blender-A-Configurable-Interactive-Interface-for-Learning-from-Diverse-Human-Feedback"><a href="#RLHF-Blender-A-Configurable-Interactive-Interface-for-Learning-from-Diverse-Human-Feedback" class="headerlink" title="RLHF-Blender: A Configurable Interactive Interface for Learning from Diverse Human Feedback"></a>RLHF-Blender: A Configurable Interactive Interface for Learning from Diverse Human Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04332">http://arxiv.org/abs/2308.04332</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yannick Metz, David Lindner, Raphaël Baur, Daniel Keim, Mennatallah El-Assady</li>
<li>for: 用于学习人类反馈的奖励模型，并考虑人类反馈的因素。</li>
<li>methods: 使用RLHF-Blender，一个可配置的交互式界面，系统atically investigate人类反馈的性质和质量。</li>
<li>results: 可以 investigate various types of feedback, such as demonstrations, rankings, comparisons, and natural language instructions, as well as studies considering the impact of human factors on their effectiveness.<details>
<summary>Abstract</summary>
To use reinforcement learning from human feedback (RLHF) in practical applications, it is crucial to learn reward models from diverse sources of human feedback and to consider human factors involved in providing feedback of different types. However, the systematic study of learning from diverse types of feedback is held back by limited standardized tooling available to researchers. To bridge this gap, we propose RLHF-Blender, a configurable, interactive interface for learning from human feedback. RLHF-Blender provides a modular experimentation framework and implementation that enables researchers to systematically investigate the properties and qualities of human feedback for reward learning. The system facilitates the exploration of various feedback types, including demonstrations, rankings, comparisons, and natural language instructions, as well as studies considering the impact of human factors on their effectiveness. We discuss a set of concrete research opportunities enabled by RLHF-Blender. More information is available at https://rlhfblender.info/.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将人类反馈学习（RLHF）应用于实际场景中，是非常重要的。因此，学习从多种人类反馈来的奖励模型是必须的，同时也需要考虑人类提供反馈的因素。然而，现有的研究工具有限，使得系统性的研究受到阻碍。为了bridging这个差距，我们提议RLHF-Blender，一个可配置的交互式界面，用于学习人类反馈。RLHF-Blender提供了可模块化的实验框架和实现，帮助研究者系统地探索不同类型的人类反馈的性质和质量。系统支持explore多种反馈类型，包括示例、排名、比较和自然语言指令，以及考虑人类因素对其效果的影响。我们介绍了RLHF-Blender可以开发的具体研究机会。更多信息请访问https://rlhfblender.info/.Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Cooperative-Multi-agent-Bandits-Distributed-Algorithms-with-Optimal-Individual-Regret-and-Constant-Communication-Costs"><a href="#Cooperative-Multi-agent-Bandits-Distributed-Algorithms-with-Optimal-Individual-Regret-and-Constant-Communication-Costs" class="headerlink" title="Cooperative Multi-agent Bandits: Distributed Algorithms with Optimal Individual Regret and Constant Communication Costs"></a>Cooperative Multi-agent Bandits: Distributed Algorithms with Optimal Individual Regret and Constant Communication Costs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04314">http://arxiv.org/abs/2308.04314</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lin Yang, Xuchuang Wang, Mohammad Hajiesmaili, Lijun Zhang, John C. S. Lui, Don Towsley</li>
<li>for: 这个论文的目的是开发一种协同多智能体多机枪游戏中的Optimal group regret和低通信成本的bandit算法。</li>
<li>methods: 这个论文使用了两种方法：领导者-追随者和完全分布式算法。</li>
<li>results: 这个论文的算法可以达到最佳个体 regret和常量通信成本。<details>
<summary>Abstract</summary>
Recently, there has been extensive study of cooperative multi-agent multi-armed bandits where a set of distributed agents cooperatively play the same multi-armed bandit game. The goal is to develop bandit algorithms with the optimal group and individual regrets and low communication between agents. The prior work tackled this problem using two paradigms: leader-follower and fully distributed algorithms. Prior algorithms in both paradigms achieve the optimal group regret. The leader-follower algorithms achieve constant communication costs but fail to achieve optimal individual regrets. The state-of-the-art fully distributed algorithms achieve optimal individual regrets but fail to achieve constant communication costs. This paper presents a simple yet effective communication policy and integrates it into a learning algorithm for cooperative bandits. Our algorithm achieves the best of both paradigms: optimal individual regret and constant communication costs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="The-Model-Inversion-Eavesdropping-Attack-in-Semantic-Communication-Systems"><a href="#The-Model-Inversion-Eavesdropping-Attack-in-Semantic-Communication-Systems" class="headerlink" title="The Model Inversion Eavesdropping Attack in Semantic Communication Systems"></a>The Model Inversion Eavesdropping Attack in Semantic Communication Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04304">http://arxiv.org/abs/2308.04304</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhao Chen, Qianqian Yang, Zhiguo Shi, Jiming Chen</li>
<li>for: 本研究探讨了 semantic communication 系统中的隐私泄露问题，并提出了一种基于 Random Permutation and Substitution 的防御策略。</li>
<li>methods: 本研究使用了 Model Inversion Eavesdropping Attack (MIEA) 来攻击 semantic communication 系统，并考虑了 white-box 和 black-box 两种设定。</li>
<li>results: 实验结果表明，提出的防御策略可以有效防止 MIEA，并且在不同的通道条件下能够保持高质量的征文重建。<details>
<summary>Abstract</summary>
In recent years, semantic communication has been a popular research topic for its superiority in communication efficiency. As semantic communication relies on deep learning to extract meaning from raw messages, it is vulnerable to attacks targeting deep learning models. In this paper, we introduce the model inversion eavesdropping attack (MIEA) to reveal the risk of privacy leaks in the semantic communication system. In MIEA, the attacker first eavesdrops the signal being transmitted by the semantic communication system and then performs model inversion attack to reconstruct the raw message, where both the white-box and black-box settings are considered. Evaluation results show that MIEA can successfully reconstruct the raw message with good quality under different channel conditions. We then propose a defense method based on random permutation and substitution to defend against MIEA in order to achieve secure semantic communication. Our experimental results demonstrate the effectiveness of the proposed defense method in preventing MIEA.
</details>
<details>
<summary>摘要</summary>
近年来， semantic communication 成为研究热点，因为它可以提高通信效率。然而， semantic communication 依赖深度学习来提取消息的意义，因此它容易受到深度学习模型的攻击。在这篇论文中，我们介绍了模型反向窃听攻击（MIEA），以揭示 semantic communication 系统中的隐私泄露风险。在 MIEA 中，攻击者首先监听 semantic communication 系统传输的信号，然后通过模型反向攻击来重建原始消息，包括白盒和黑盒两种设置。我们的evaluation结果表明， MIEA 可以在不同的通信道条件下成功重建原始消息，并且提议了基于随机排序和替换的防御方法，以确保 semantic communication 的安全。我们的实验结果表明，提议的防御方法可以有效防止 MIEA。
</details></li>
</ul>
<hr>
<h2 id="Comparative-Analysis-of-the-wav2vec-2-0-Feature-Extractor"><a href="#Comparative-Analysis-of-the-wav2vec-2-0-Feature-Extractor" class="headerlink" title="Comparative Analysis of the wav2vec 2.0 Feature Extractor"></a>Comparative Analysis of the wav2vec 2.0 Feature Extractor</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04286">http://arxiv.org/abs/2308.04286</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peter Vieting, Ralf Schlüter, Hermann Ney</li>
<li>for: 这个论文主要是为了检验 neural raw waveform feature extractors (FEs) 是否可以取代传统的手工特征提取方法，以实现更加一致的模型从语音到转录文本。</li>
<li>methods: 这篇论文使用了 wav2vec 2.0 模型，这是一种直接在语音波形上运行的 convolutional FE，以及一种替代的神经网络特征提取方法。</li>
<li>results: 研究表明，两种神经网络特征提取方法都能与传统的特征提取方法竞争在 LibriSpeech benchmark 上，并且分析了各个组件的效果。 另外，研究还发现，ASR 系统最重要的信息是由一组带宽滤波器获得的。<details>
<summary>Abstract</summary>
Automatic speech recognition (ASR) systems typically use handcrafted feature extraction pipelines. To avoid their inherent information loss and to achieve more consistent modeling from speech to transcribed text, neural raw waveform feature extractors (FEs) are an appealing approach. Also the wav2vec 2.0 model, which has recently gained large popularity, uses a convolutional FE which operates directly on the speech waveform. However, it is not yet studied extensively in the literature. In this work, we study its capability to replace the standard feature extraction methods in a connectionist temporal classification (CTC) ASR model and compare it to an alternative neural FE. We show that both are competitive with traditional FEs on the LibriSpeech benchmark and analyze the effect of the individual components. Furthermore, we analyze the learned filters and show that the most important information for the ASR system is obtained by a set of bandpass filters.
</details>
<details>
<summary>摘要</summary>
自动语音识别（ASR）系统通常使用手工设计的特征提取管道。以避免其内置的信息损失并实现更一致的模型化从语音到转录文本，神经原始波形特征提取器（FEs）是一种吸引人的方法。另外，最近广受欢迎的wav2vec 2.0模型使用了一个 convolutional FE，该模型直接操作于语音波形。然而，它在文献中还没有得到广泛的研究。在这项工作中，我们研究了它的可行性以replace标准特征提取方法在一个 Connectionist Temporal Classification（CTC） ASR 模型中，并与一个 alternating neural FE 进行比较。我们发现两者在 LibriSpeech benchmark 上都是与传统特征提取方法竞争的，并分析了各种组件的效果。此外，我们还分析了学习的滤波器，发现主要的信息 для ASR 系统是由一组 bandpass 滤波器获得。
</details></li>
</ul>
<hr>
<h2 id="In-Context-Alignment-Chat-with-Vanilla-Language-Models-Before-Fine-Tuning"><a href="#In-Context-Alignment-Chat-with-Vanilla-Language-Models-Before-Fine-Tuning" class="headerlink" title="In-Context Alignment: Chat with Vanilla Language Models Before Fine-Tuning"></a>In-Context Alignment: Chat with Vanilla Language Models Before Fine-Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04275">http://arxiv.org/abs/2308.04275</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xhan77/in-context-alignment">https://github.com/xhan77/in-context-alignment</a></li>
<li>paper_authors: Xiaochuang Han</li>
<li>for: 这个研究探讨了在运行时进行对适应的自适应语言模型。</li>
<li>methods: 研究使用了一个未经任何精度调整的语言模型Llama-2，并在它的示例上进行了协同学习。</li>
<li>results: 与直接提示相比，在受Context的协同学习下，无需修改模型参数的情况下，vanilla语言模型的赢利率提高了7倍，与文本达文西003模型from OpenAI进行比较。<details>
<summary>Abstract</summary>
In this note, we explore inference-time alignment through in-context learning. We consider a vanilla pretrained language model Llama-2 before any fine-tuning and retrieve an average of 9 demonstration alignment examples when the model is prompted to follow chat-style instructions. Compared to direct prompting, the in-context alignment without changing model weights leads to a 7x increase in win-rate w.r.t. the text-davinci-003 model from OpenAI, making the vanilla language model comparable to strong baselines with alignment fine-tuning.
</details>
<details>
<summary>摘要</summary>
在这份笔记中，我们研究了在使用受Context learning时进行推理时的对齐。我们考虑了未经任何微调的语言模型Llama-2，并从chat风格的指令中获取了9个示例对齐。与直接提示相比，在不变换模型参数时进行受Context learning的对齐，导致了与文本-达文西003模型from OpenAI的7倍增加赢得率，使得未经微调的语言模型与对齐微调的基elines相当。
</details></li>
</ul>
<hr>
<h2 id="Teacher-Student-Architecture-for-Knowledge-Distillation-A-Survey"><a href="#Teacher-Student-Architecture-for-Knowledge-Distillation-A-Survey" class="headerlink" title="Teacher-Student Architecture for Knowledge Distillation: A Survey"></a>Teacher-Student Architecture for Knowledge Distillation: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04268">http://arxiv.org/abs/2308.04268</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengming Hu, Xuan Li, Dan Liu, Haolun Wu, Xi Chen, Ju Wang, Xue Liu</li>
<li>for: 本研究旨在探讨 teacher-student 架构在多种知识压缩目标上的应用，包括知识压缩、知识扩展、知识适应和知识增强等。</li>
<li>methods: 本文提出了一种系统性的对 teacher-student 架构的介绍，包括不同的知识表示方法和优化目标，以及一些代表学习算法和有效的压缩方案。</li>
<li>results: 本文综述了现有的应用场景，包括分类、识别、生成、排名和回归等多种目标，并提出了未来研究方向，包括架构设计、知识质量和回归学习等。<details>
<summary>Abstract</summary>
Although Deep neural networks (DNNs) have shown a strong capacity to solve large-scale problems in many areas, such DNNs are hard to be deployed in real-world systems due to their voluminous parameters. To tackle this issue, Teacher-Student architectures were proposed, where simple student networks with a few parameters can achieve comparable performance to deep teacher networks with many parameters. Recently, Teacher-Student architectures have been effectively and widely embraced on various knowledge distillation (KD) objectives, including knowledge compression, knowledge expansion, knowledge adaptation, and knowledge enhancement. With the help of Teacher-Student architectures, current studies are able to achieve multiple distillation objectives through lightweight and generalized student networks. Different from existing KD surveys that primarily focus on knowledge compression, this survey first explores Teacher-Student architectures across multiple distillation objectives. This survey presents an introduction to various knowledge representations and their corresponding optimization objectives. Additionally, we provide a systematic overview of Teacher-Student architectures with representative learning algorithms and effective distillation schemes. This survey also summarizes recent applications of Teacher-Student architectures across multiple purposes, including classification, recognition, generation, ranking, and regression. Lastly, potential research directions in KD are investigated, focusing on architecture design, knowledge quality, and theoretical studies of regression-based learning, respectively. Through this comprehensive survey, industry practitioners and the academic community can gain valuable insights and guidelines for effectively designing, learning, and applying Teacher-Student architectures on various distillation objectives.
</details>
<details>
<summary>摘要</summary>
although deep neural networks (DNNs) have shown strong capacity to solve large-scale problems in many areas, such DNNs are hard to be deployed in real-world systems due to their voluminous parameters. to tackle this issue, teacher-student architectures were proposed, where simple student networks with a few parameters can achieve comparable performance to deep teacher networks with many parameters. recently, teacher-student architectures have been effectively and widely embraced on various knowledge distillation (KD) objectives, including knowledge compression, knowledge expansion, knowledge adaptation, and knowledge enhancement. with the help of teacher-student architectures, current studies are able to achieve multiple distillation objectives through lightweight and generalized student networks. different from existing KD surveys that primarily focus on knowledge compression, this survey first explores teacher-student architectures across multiple distillation objectives. this survey presents an introduction to various knowledge representations and their corresponding optimization objectives. additionally, we provide a systematic overview of teacher-student architectures with representative learning algorithms and effective distillation schemes. this survey also summarizes recent applications of teacher-student architectures across multiple purposes, including classification, recognition, generation, ranking, and regression. lastly, potential research directions in KD are investigated, focusing on architecture design, knowledge quality, and theoretical studies of regression-based learning, respectively. through this comprehensive survey, industry practitioners and the academic community can gain valuable insights and guidelines for effectively designing, learning, and applying teacher-student architectures on various distillation objectives.
</details></li>
</ul>
<hr>
<h2 id="BarlowRL-Barlow-Twins-for-Data-Efficient-Reinforcement-Learning"><a href="#BarlowRL-Barlow-Twins-for-Data-Efficient-Reinforcement-Learning" class="headerlink" title="BarlowRL: Barlow Twins for Data-Efficient Reinforcement Learning"></a>BarlowRL: Barlow Twins for Data-Efficient Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04263">http://arxiv.org/abs/2308.04263</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omer Veysel Cagatan</li>
<li>for: 提高数据效率的强化学习Agent</li>
<li>methods:  combinest Barlow Twins自动学习框架和DER数据有效雨bow算法</li>
<li>results: 在Atari 100k测试集上表现出色，超过了DER和其它对比算法的表现In Simplified Chinese:</li>
<li>for: 提高强化学习数据效率</li>
<li>methods:  combine Barlow Twins 自动学习框架和DER 数据有效雨bow 算法</li>
<li>results: 在Atari 100k 测试集上表现出色，超过了DER 和其它对比算法的表现<details>
<summary>Abstract</summary>
This paper introduces BarlowRL, a data-efficient reinforcement learning agent that combines the Barlow Twins self-supervised learning framework with DER (Data-Efficient Rainbow) algorithm. BarlowRL outperforms both DER and its contrastive counterpart CURL on the Atari 100k benchmark. BarlowRL avoids dimensional collapse by enforcing information spread to the whole space. This helps RL algorithms to utilize uniformly spread state representation that eventually results in a remarkable performance. The integration of Barlow Twins with DER enhances data efficiency and achieves superior performance in the RL tasks. BarlowRL demonstrates the potential of incorporating self-supervised learning techniques to improve RL algorithms.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了BarlowRL，一种数据效率的 reinforcement learning代理人，它将Barlow Twins自我超vis学框架与DER（数据效率雨bow）算法结合在一起。BarlowRL在Atari 100k benchmark上表现出优于DER和其对应的对比算法CURL。BarlowRL通过保证信息散布到整个空间，避免维度塌陷，使RL算法能够利用 uniformly 分布的状态表示，最终导致了很好的表现。将Barlow Twins与DER集成，可以提高数据效率并实现RL任务中的优秀表现。BarlowRL表明了将自我超vis学技术integrated into RL算法可以提高其表现。
</details></li>
</ul>
<hr>
<h2 id="SDLFormer-A-Sparse-and-Dense-Locality-enhanced-Transformer-for-Accelerated-MR-Image-Reconstruction"><a href="#SDLFormer-A-Sparse-and-Dense-Locality-enhanced-Transformer-for-Accelerated-MR-Image-Reconstruction" class="headerlink" title="SDLFormer: A Sparse and Dense Locality-enhanced Transformer for Accelerated MR Image Reconstruction"></a>SDLFormer: A Sparse and Dense Locality-enhanced Transformer for Accelerated MR Image Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04262">http://arxiv.org/abs/2308.04262</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rahul-gs-16/sdlformer">https://github.com/rahul-gs-16/sdlformer</a></li>
<li>paper_authors: Rahul G. S., Sriprabha Ramnarayanan, Mohammad Al Fahim, Keerthi Ram, Preejith S. P, Mohanasankar Sivaprakasam</li>
<li>for: 这个论文目的是提出一种基于窗口变换器的快速MRI图像重建方法，以优化MRI图像重建速度和质量。</li>
<li>methods: 该方法使用了窗口变换器网络，并 integrate了扩展注意力机制和卷积操作，以捕捉更远的像素关系和学习低级翻译不变的特征。</li>
<li>results: 实验结果显示，提出的方法可以在4x和5x的下采样情况下，与其他架构和平行领域自主学习基准相比，提高了1.40dB的PSNR和0.028的SSIM的平均提升。代码可以在<a target="_blank" rel="noopener" href="https://github.com/rahul-gs-16/sdlformer.git%E4%B8%AD%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/rahul-gs-16/sdlformer.git中下载。</a><details>
<summary>Abstract</summary>
Transformers have emerged as viable alternatives to convolutional neural networks owing to their ability to learn non-local region relationships in the spatial domain. The self-attention mechanism of the transformer enables transformers to capture long-range dependencies in the images, which might be desirable for accelerated MRI image reconstruction as the effect of undersampling is non-local in the image domain. Despite its computational efficiency, the window-based transformers suffer from restricted receptive fields as the dependencies are limited to within the scope of the image windows. We propose a window-based transformer network that integrates dilated attention mechanism and convolution for accelerated MRI image reconstruction. The proposed network consists of dilated and dense neighborhood attention transformers to enhance the distant neighborhood pixel relationship and introduce depth-wise convolutions within the transformer module to learn low-level translation invariant features for accelerated MRI image reconstruction. The proposed model is trained in a self-supervised manner. We perform extensive experiments for multi-coil MRI acceleration for coronal PD, coronal PDFS and axial T2 contrasts with 4x and 5x under-sampling in self-supervised learning based on k-space splitting. We compare our method against other reconstruction architectures and the parallel domain self-supervised learning baseline. Results show that the proposed model exhibits improvement margins of (i) around 1.40 dB in PSNR and around 0.028 in SSIM on average over other architectures (ii) around 1.44 dB in PSNR and around 0.029 in SSIM over parallel domain self-supervised learning. The code is available at https://github.com/rahul-gs-16/sdlformer.git
</details>
<details>
<summary>摘要</summary>
transformers 已经成为了卷积神经网络的可行替代品，因为它们可以学习图像空间中的非本地区关系。transformers 的自注意机制使得它们可以捕捉图像中的长距离依赖关系，这可能是加速 MRI 图像重建的潜在的优点，因为 MRI 图像下折衰的效果是非本地的。尽管它们有计算效率的优势，但窗口基于的 transformers 受限于图像窗口范围内的依赖关系。我们提议一种窗口基于的 transformer 网络，该网络 integrate 了扩展注意力机制和卷积操作以加速 MRI 图像重建。我们的提案的网络包括扩展和密集 neighborhood attention transformers，以增强远方块像素关系，并在 transformer 模块中添加 depth-wise 卷积来学习低级翻译不变的特征。我们的模型在自我超vised 的方式进行训练。我们进行了多种实验，包括多个 MRI 加速器，以及 coronal PD、coronal PDFS 和 axial T2 对比。我们与其他重建架构和并行Domain self-supervised learning 基线进行比较。结果表明，我们的模型在 PSNR 和 SSIM 两个指标上分别提高了约1.40 dB和约0.028的平均提升。代码可以在 <https://github.com/rahul-gs-16/sdlformer.git> 中找到。
</details></li>
</ul>
<hr>
<h2 id="Advancing-Natural-Language-Based-Audio-Retrieval-with-PaSST-and-Large-Audio-Caption-Data-Sets"><a href="#Advancing-Natural-Language-Based-Audio-Retrieval-with-PaSST-and-Large-Audio-Caption-Data-Sets" class="headerlink" title="Advancing Natural-Language Based Audio Retrieval with PaSST and Large Audio-Caption Data Sets"></a>Advancing Natural-Language Based Audio Retrieval with PaSST and Large Audio-Caption Data Sets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04258">http://arxiv.org/abs/2308.04258</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/optimusprimus/dcase2023_task6b">https://github.com/optimusprimus/dcase2023_task6b</a></li>
<li>paper_authors: Paul Primus, Khaled Koutini, Gerhard Widmer</li>
<li>for: 这篇论文旨在提出一种基于预训练文本和声音变换器的文本到声音检索系统。</li>
<li>methods: 该方法使用自注意力基于声音编码器对声音进行编码，并在不同模式之间进行了系统性分析，以评估每个系统组件对检索性能的影响。</li>
<li>results: 该系统在2023年DCASE挑战中 ranked第一，并在ClothoV2测试集上超越当前状态的艺术点，提高了5.6 pp. mAP@10。<details>
<summary>Abstract</summary>
This work presents a text-to-audio-retrieval system based on pre-trained text and spectrogram transformers. Our method projects recordings and textual descriptions into a shared audio-caption space in which related examples from different modalities are close. Through a systematic analysis, we examine how each component of the system influences retrieval performance. As a result, we identify two key components that play a crucial role in driving performance: the self-attention-based audio encoder for audio embedding and the utilization of additional human-generated and synthetic data sets during pre-training. We further experimented with augmenting ClothoV2 captions with available keywords to increase their variety; however, this only led to marginal improvements. Our system ranked first in the 2023's DCASE Challenge, and it outperforms the current state of the art on the ClothoV2 benchmark by 5.6 pp. mAP@10.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:这个研究提出了一个基于预训练文本和spectrogram转换器的文本至声音检索系统。我们的方法将录音和文本描述映射到一个共享声音-caption空间，在不同modalities中相关的示例都很近。通过系统atic分析，我们评估每个系统组件对检索性能的影响。我们发现两个关键的组件对检索性能有决定性的影响：使用自我注意力基于的声音编码器，以及在预训练过程中使用additional human-generated和合成数据集。我们还尝试了将ClothoV2标签中可用的关键词加入，但只导致了微妙的改进。我们的系统在2023年DCASE挑战中名列第一，并在ClothoV2标准测试集上比现状权威的检索性能提高5.6 pp. mAP@10。
</details></li>
</ul>
<hr>
<h2 id="Federated-Inference-with-Reliable-Uncertainty-Quantification-over-Wireless-Channels-via-Conformal-Prediction"><a href="#Federated-Inference-with-Reliable-Uncertainty-Quantification-over-Wireless-Channels-via-Conformal-Prediction" class="headerlink" title="Federated Inference with Reliable Uncertainty Quantification over Wireless Channels via Conformal Prediction"></a>Federated Inference with Reliable Uncertainty Quantification over Wireless Channels via Conformal Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04237">http://arxiv.org/abs/2308.04237</a></li>
<li>repo_url: None</li>
<li>paper_authors: Meiyi Zhu, Matteo Zecchin, Sangwoo Park, Caili Guo, Chunyan Feng, Osvaldo Simeone</li>
<li>for: 这篇论文旨在研究在多个设备和服务器共享预训练模型的情况下，通过设备到服务器的通信来提高服务器的推断决策质量。</li>
<li>methods: 这篇论文提出了一种名为“联邦均衡预测”（Federated Conformal Prediction，简称WFCP）的协议，它基于类型基本多访问（Type-Based Multiple Access，TBMA）和一种新的量词 corrections 策略。WFCP 可以在无阻塞通信的情况下提供正式的可靠性保证。</li>
<li>results: 根据数值结果，作者比较了WFCP 与现有的联邦CP 方案的性能，发现WFCP 在有限通信资源和&#x2F;或多个设备的情况下具有显著优势。特别是，WFCP 可以在无阻塞通信的情况下提供正式的可靠性保证，而现有的联邦CP 方案则不能做到。<details>
<summary>Abstract</summary>
Consider a setting in which devices and a server share a pre-trained model. The server wishes to make an inference on a new input given the model. Devices have access to data, previously not used for training, and can communicate to the server over a common wireless channel. If the devices have no access to the new input, can communication from devices to the server enhance the quality of the inference decision at the server? Recent work has introduced federated conformal prediction (CP), which leverages devices-to-server communication to improve the reliability of the server's decision. With federated CP, devices communicate to the server information about the loss accrued by the shared pre-trained model on the local data, and the server leverages this information to calibrate a decision interval, or set, so that it is guaranteed to contain the correct answer with a pre-defined target reliability level. Previous work assumed noise-free communication, whereby devices can communicate a single real number to the server. In this paper, we study for the first time federated CP in a wireless setting. We introduce a novel protocol, termed wireless federated conformal prediction (WFCP), which builds on type-based multiple access (TBMA) and on a novel quantile correction strategy. WFCP is proved to provide formal reliability guarantees in terms of coverage of the predicted set produced by the server. Using numerical results, we demonstrate the significant advantages of WFCP against digital implementations of existing federated CP schemes, especially in regimes with limited communication resources and/or large number of devices.
</details>
<details>
<summary>摘要</summary>
Setting 中，设备和服务器共享预训练模型。服务器想要对新输入进行推断。设备可以访问未使用过训练的数据，并可以通过公共无线频道与服务器进行通信。如果设备没有访问新输入，是否可以通过设备到服务器的通信提高服务器的推断决策质量？ latest work 引入了联邦均衡预测（CP），该技术利用设备到服务器的通信来提高服务器的决策可靠性。在联邦CP中，设备将共享模型在本地数据上的损失信息通过无线频道传输给服务器，服务器利用这些信息进行均衡决策集，以 garantuee 决策集中包含正确答案，并且预定的可靠性水平。 previous work 假设了无噪通信，设备可以将单个实数传输给服务器。在这篇论文中，我们研究了在无线设置下的联邦CP。我们提出了一种新的协议，称为无线联邦均衡预测（WFCP），它基于类型基本多访问（TBMA）和一种新的量衡修正策略。WFCP提供了正式可靠性保证，包括预测集产生的覆盖率。通过数值结果，我们展示了WFCP在数字实现联邦CP方案的情况下，特别是在通信资源有限和/或设备数量很大的情况下，具有显著优势。
</details></li>
</ul>
<hr>
<h2 id="OpinionConv-Conversational-Product-Search-with-Grounded-Opinions"><a href="#OpinionConv-Conversational-Product-Search-with-Grounded-Opinions" class="headerlink" title="OpinionConv: Conversational Product Search with Grounded Opinions"></a>OpinionConv: Conversational Product Search with Grounded Opinions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04226">http://arxiv.org/abs/2308.04226</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vahid Sadiri Javadi, Martin Potthast, Lucie Flek</li>
<li>for: 这 paper 是为了 simulating sales conversations 和 grounding conversational AI in true subjective narratives.</li>
<li>methods: 该 paper 使用 product reviews 作为 rich source of product opinions.</li>
<li>results: 在 several user studies 中，generated conversations 被评估为 realistic, 并且 assessors 确认 opinions 作为 informed basis for decision-making.<details>
<summary>Abstract</summary>
When searching for products, the opinions of others play an important role in making informed decisions. Subjective experiences about a product can be a valuable source of information. This is also true in sales conversations, where a customer and a sales assistant exchange facts and opinions about products. However, training an AI for such conversations is complicated by the fact that language models do not possess authentic opinions for their lack of real-world experience. We address this problem by leveraging product reviews as a rich source of product opinions to ground conversational AI in true subjective narratives. With OpinionConv, we develop the first conversational AI for simulating sales conversations. To validate the generated conversations, we conduct several user studies showing that the generated opinions are perceived as realistic. Our assessors also confirm the importance of opinions as an informative basis for decision-making.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:在寻找产品时，他人的意见具有重要的指导作用。产品的主观经验可以提供有价值的信息。这也是销售对话中的事实，顾客和销售助手交换产品的信息和意见。然而，用AI训练这些对话是因为语言模型缺乏真实世界经验而复杂。我们解决这个问题，利用产品评论作为产品意见的丰富源，以真实的主观故事为 conversational AI 定位。我们开发了 OpinionConv，首个用于模拟销售对话的对话AI。为验证生成的对话，我们进行了多个用户研究，显示生成的意见被评估为真实。我们的评估人也证实了意见作为决策基础的重要性。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Interpretation-and-Validation-of-Graph-Attention-based-Explanations-for-GNN-Models"><a href="#Semantic-Interpretation-and-Validation-of-Graph-Attention-based-Explanations-for-GNN-Models" class="headerlink" title="Semantic Interpretation and Validation of Graph Attention-based Explanations for GNN Models"></a>Semantic Interpretation and Validation of Graph Attention-based Explanations for GNN Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04220">http://arxiv.org/abs/2308.04220</a></li>
<li>repo_url: None</li>
<li>paper_authors: Efimia Panagiotaki, Daniele De Martini, Lars Kunze</li>
<li>for: 这个论文旨在研究如何使用Semantic Attention提高Graph Neural Network（GNN）模型的解释性，并通过在模型中引入semantically-informed perturbations来建立feature-importance weights和模型准确性之间的相关性。</li>
<li>methods: 该论文提出了一种基于Graph Deep Learning（GDL）的方法，通过引入Semantic Attention Mechanism来提高GNN模型的解释性。该方法基于Attention Mechanism的概念，通过计算模型对输入特征的重要性来提供feature-based解释。</li>
<li>results: 该论文通过应用该方法于一个Lidar点云估计模型，成功地标识了模型的透明性和性能之间的相关性，并生成了可靠的后果Semantic Explanation。<details>
<summary>Abstract</summary>
In this work, we propose a methodology for investigating the application of semantic attention to enhance the explainability of Graph Neural Network (GNN)-based models, introducing semantically-informed perturbations and establishing a correlation between predicted feature-importance weights and model accuracy. Graph Deep Learning (GDL) has emerged as a promising field for tasks like scene interpretation, leveraging flexible graph structures to concisely describe complex features and relationships. As traditional explainability methods used in eXplainable AI (XAI) cannot be directly applied to such structures, graph-specific approaches are introduced. Attention mechanisms have demonstrated their efficacy in estimating the importance of input features in deep learning models and thus have been previously employed to provide feature-based explanations for GNN predictions. Building upon these insights, we extend existing attention-based graph-explainability methods investigating the use of attention weights as importance indicators of semantically sorted feature sets. Through analysing the behaviour of predicted attention-weights distribution in correlation with model accuracy, we gain valuable insights into feature importance with respect to the behaviour of the GNN model. We apply our methodology to a lidar pointcloud estimation model successfully identifying key semantic classes that contribute to enhanced performance effectively generating reliable post-hoc semantic explanations.
</details>
<details>
<summary>摘要</summary>
在这个研究中，我们提出了一种方法来提高图 neural network（GNN）模型的解释性，通过引入Semantic attention和建立 predicted feature-importance  weights 和模型准确率之间的相关性。图深度学习（GDL）已经成为了场景理解的一个有前途的领域，利用图结构来简洁地描述复杂的特征和关系。传统的解释方法在XAI中不能直接应用于这些结构，因此图特定的方法被引入。Attention机制已经证明了它们可以估算输入特征的重要性，因此在GNN预测中提供了基于特征的解释。我们在这些基础上进一步推广了现有的注意力Weight-based graph-explainability方法，研究 attention weights 作为semantic sorted feature sets的重要性指标。通过分析预测的注意力分布的行为和模型准确率之间的相关性，我们获得了对feature importance的重要信息，即 Semantic classes的贡献对于提高性能的贡献。我们应用了我们的方法ology到一个 lidar pointcloud estimation模型，成功地Identifying key semantic classes that contribute to enhanced performance, effectively generating reliable post-hoc semantic explanations.
</details></li>
</ul>
<hr>
<h2 id="Varying-coefficients-for-regional-quantile-via-KNN-based-LASSO-with-applications-to-health-outcome-study"><a href="#Varying-coefficients-for-regional-quantile-via-KNN-based-LASSO-with-applications-to-health-outcome-study" class="headerlink" title="Varying-coefficients for regional quantile via KNN-based LASSO with applications to health outcome study"></a>Varying-coefficients for regional quantile via KNN-based LASSO with applications to health outcome study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04212">http://arxiv.org/abs/2308.04212</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/younghhk/software">https://github.com/younghhk/software</a></li>
<li>paper_authors: Seyoung Park, Eun Ryung Lee, Hyokyoung G. Hong</li>
<li>for: 这个论文的目的是动态模型健康结果和风险因素之间的关系，以及年龄的时变效应。</li>
<li>methods: 这个论文使用了变 coefficient (VC) 区域量化回归和K-最近邻 (KNN) 混合lasso，以捕捉健康结果和风险因素之间的时变关系。</li>
<li>results: 该方法在实际应用中能够准确地捕捉健康结果和风险因素之间的复杂时变关系。<details>
<summary>Abstract</summary>
Health outcomes, such as body mass index and cholesterol levels, are known to be dependent on age and exhibit varying effects with their associated risk factors. In this paper, we propose a novel framework for dynamic modeling of the associations between health outcomes and risk factors using varying-coefficients (VC) regional quantile regression via K-nearest neighbors (KNN) fused Lasso, which captures the time-varying effects of age. The proposed method has strong theoretical properties, including a tight estimation error bound and the ability to detect exact clustered patterns under certain regularity conditions. To efficiently solve the resulting optimization problem, we develop an alternating direction method of multipliers (ADMM) algorithm. Our empirical results demonstrate the efficacy of the proposed method in capturing the complex age-dependent associations between health outcomes and their risk factors.
</details>
<details>
<summary>摘要</summary>
健康结果，如体重指数和尿痰水平，与年龄存在许多关系，这些关系随着风险因素的变化而发生变化。在这篇论文中，我们提出了一种新的方法，即时变量方程模型，用于描述健康结果和风险因素之间的关系。这种方法通过变量系数（VC）地方量化回归和K-最近邻（KNN）束regularization，捕捉年龄的时间变化效应。我们的方法具有强制实施证明，包括紧张估计误差 bound和在某些正则条件下检测到具体的团集模式。为解决相应的优化问题，我们开发了一种分解方法of multipliers（ADMM）算法。我们的实验结果表明，我们的方法能够准确捕捉健康结果和风险因素之间的复杂年龄关系。
</details></li>
</ul>
<hr>
<h2 id="Iterative-Sketching-for-Secure-Coded-Regression"><a href="#Iterative-Sketching-for-Secure-Coded-Regression" class="headerlink" title="Iterative Sketching for Secure Coded Regression"></a>Iterative Sketching for Secure Coded Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04185">http://arxiv.org/abs/2308.04185</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neophytos Charalambides, Hessam Mahdavifar, Mert Pilanci, Alfred O. Hero III</li>
<li>for: 这篇论文是为了提高线性回归的分布式加速而写的。</li>
<li>methods: 该论文使用随机抽样技术，并提高异步系统中势特缺失的抗性。具体来说，它使用随机正交矩阵，并对块进行抽样，以同时保护信息和降低回归问题的维度。在我们的设置中，这种变换对应于一种编码加密的简化 Gradient Coding Scheme，而抽样对应于非势特工作者的回答。</li>
<li>results: 该论文提出了一种分布式iterative sketching方法，可以同时实现线性回归的加速和安全保护。具体来说，它提出了一种使用随机抽样和编码加密的方法，可以在分布式系统中实现高效的线性回归计算。此外，论文还特别关注了一种特殊的随机化hadamard transform，并将其扩展到块抽样。<details>
<summary>Abstract</summary>
In this work, we propose methods for speeding up linear regression distributively, while ensuring security. We leverage randomized sketching techniques, and improve straggler resilience in asynchronous systems. Specifically, we apply a random orthonormal matrix and then subsample \textit{blocks}, to simultaneously secure the information and reduce the dimension of the regression problem. In our setup, the transformation corresponds to an encoded encryption in an \textit{approximate gradient coding scheme}, and the subsampling corresponds to the responses of the non-straggling workers; in a centralized coded computing network. This results in a distributive \textit{iterative sketching} approach for an $\ell_2$-subspace embedding, \textit{i.e.} a new sketch is considered at each iteration. We also focus on the special case of the \textit{Subsampled Randomized Hadamard Transform}, which we generalize to block sampling; and discuss how it can be modified in order to secure the data.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们提出了一种加速线性回归的分布式方法，同时保证安全性。我们利用随机抽取技术，并改进异步系统中的延迟问题。特别是，我们首先应用随机正交矩阵，然后对块进行采样，以同时保护信息和缩小回归问题的维度。在我们的设置中，这种变换对应于一种编码加密方案，即精度梯度编码，而采样对应于非延迟工作者的响应。因此，我们得到了一种分布式迭代绘制方法，即在每次迭代中生成一个新的绘制。我们还关注特殊情况下的归一化随机哈达姆变换，并将其扩展到块采样；并讨论如何修改它以保护数据。
</details></li>
</ul>
<hr>
<h2 id="Studying-Socially-Unacceptable-Discourse-Classification-SUD-through-different-eyes-“Are-we-on-the-same-page-”"><a href="#Studying-Socially-Unacceptable-Discourse-Classification-SUD-through-different-eyes-“Are-we-on-the-same-page-”" class="headerlink" title="Studying Socially Unacceptable Discourse Classification (SUD) through different eyes: “Are we on the same page ?”"></a>Studying Socially Unacceptable Discourse Classification (SUD) through different eyes: “Are we on the same page ?”</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04180">http://arxiv.org/abs/2308.04180</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mlinardicyu/sud_study_different_eyes">https://github.com/mlinardicyu/sud_study_different_eyes</a></li>
<li>paper_authors: Bruno Machado Carneiro, Michele Linardi, Julien Longhi</li>
<li>for: 本研究探讨了在线文本中的社会不容许语言特征描述和检测。</li>
<li>methods: 我们首先构建了一个包含多种不同在线源的手动标注文本的新集合，以测试现有机器学习（ML）社会不容许语言检测解决方案中的通用能力。</li>
<li>results: 我们提供了一些数据洞察，以支持领域专家在标注任务中。同时，我们还分析了可能存在的不同批注modalities的影响于社会不容许语言学习，并提出了一些未解决的挑战和研究方向。<details>
<summary>Abstract</summary>
We study Socially Unacceptable Discourse (SUD) characterization and detection in online text. We first build and present a novel corpus that contains a large variety of manually annotated texts from different online sources used so far in state-of-the-art Machine learning (ML) SUD detection solutions. This global context allows us to test the generalization ability of SUD classifiers that acquire knowledge around the same SUD categories, but from different contexts. From this perspective, we can analyze how (possibly) different annotation modalities influence SUD learning by discussing open challenges and open research directions. We also provide several data insights which can support domain experts in the annotation task.
</details>
<details>
<summary>摘要</summary>
我们研究社会不容许的语言讨论（SUD）Characterization和检测在在线文本中。我们首先构建并提供了一个新的 корпуス，包含了不同在线源的手动标注的文本，这些文本在过去的 estado-of-the-art 机器学习（ML）SUD检测解决方案中使用过。这个全球背景允许我们测试SUD分类器的通用能力，这些分类器从不同的上下文中获得了相同的 SUD 类别知识。从这个角度来看，我们可以分析不同标注方式对 SUD 学习的影响，并讨论开放的挑战和研究方向。我们还提供了一些数据见解，以支持领域专家在标注任务中。Note: "SUD" stands for "Socially Unacceptable Discourse" in English.
</details></li>
</ul>
<hr>
<h2 id="Dual-input-neural-networks-for-positional-sound-source-localization"><a href="#Dual-input-neural-networks-for-positional-sound-source-localization" class="headerlink" title="Dual input neural networks for positional sound source localization"></a>Dual input neural networks for positional sound source localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04169">http://arxiv.org/abs/2308.04169</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/egrinstein/di_nn">https://github.com/egrinstein/di_nn</a></li>
<li>paper_authors: Eric Grinstein, Vincent W. Neo, Patrick A. Naylor</li>
<li>for: 本研究旨在提高Sound Source Localization（SSL）算法的精度，通过将高维度的多渠道声音信号和场景的声学特性（如投射坐标）与神经网络模型结合使用。</li>
<li>methods: 本研究提出了一种简单有效的双输入神经网络（DI-NN）模型，可以有效地模型高维度声音信号和场景声学特性。</li>
<li>results: 对于一系列的实验数据，DI-NN与基准方法（如最小二乘法和卷积循环神经网络）进行比较，DI-NN在本试用 dataset 中实现了五倍的Localization error reduction than 基准方法，并且较CRNN两倍。<details>
<summary>Abstract</summary>
In many signal processing applications, metadata may be advantageously used in conjunction with a high dimensional signal to produce a desired output. In the case of classical Sound Source Localization (SSL) algorithms, information from a high dimensional, multichannel audio signals received by many distributed microphones is combined with information describing acoustic properties of the scene, such as the microphones' coordinates in space, to estimate the position of a sound source. We introduce Dual Input Neural Networks (DI-NNs) as a simple and effective way to model these two data types in a neural network. We train and evaluate our proposed DI-NN on scenarios of varying difficulty and realism and compare it against an alternative architecture, a classical Least-Squares (LS) method as well as a classical Convolutional Recurrent Neural Network (CRNN). Our results show that the DI-NN significantly outperforms the baselines, achieving a five times lower localization error than the LS method and two times lower than the CRNN in a test dataset of real recordings.
</details>
<details>
<summary>摘要</summary>
在许多信号处理应用程序中，元数据可能被利用于高维信号生成愿景输出。在经典的声音源地理位置算法中，来自多个分布式 Mikrophone 的高维多通道音频信号以及场景的声学属性信息（如 Mikrophone 的空间坐标）被组合以估算声音源的位置。我们引入双输入神经网络（DI-NN）作为一种简单而有效的方法来模型这两种数据类型。我们在不同的难度和真实性场景下训练和评估我们的提议 DI-NN，并与基准architecture（经典的最小二乘法和卷积循环神经网络）进行比较。我们的结果显示，DI-NN 明显超越了基准，在实际录制的测试集中实现了与经典方法（最小二乘法）和卷积循环神经网络（CRNN）相比的五倍的地理位置误差。
</details></li>
</ul>
<hr>
<h2 id="Comprehensive-Assessment-of-the-Performance-of-Deep-Learning-Classifiers-Reveals-a-Surprising-Lack-of-Robustness"><a href="#Comprehensive-Assessment-of-the-Performance-of-Deep-Learning-Classifiers-Reveals-a-Surprising-Lack-of-Robustness" class="headerlink" title="Comprehensive Assessment of the Performance of Deep Learning Classifiers Reveals a Surprising Lack of Robustness"></a>Comprehensive Assessment of the Performance of Deep Learning Classifiers Reveals a Surprising Lack of Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04137">http://arxiv.org/abs/2308.04137</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael W. Spratling</li>
<li>for: 评估机器学习模型的可靠性和稳定性</li>
<li>methods: 使用多种数据类型进行评估，并使用单一指标评估模型的性能</li>
<li>results: 现有深度神经网络模型容易在某些数据类型上出现错误，表明它们在实际场景中可能不可靠，并且容易被骗到错误决策Here’s a more detailed explanation of each point:</li>
<li>for: The paper aims to evaluate the robustness and reliability of machine learning models, specifically deep neural networks, by using a wide range of data types and a single metric to assess their performance.</li>
<li>methods: The authors propose using a benchmark that includes multiple types of data to evaluate the models’ performance, and they use a single metric to compare the models’ performance across different data types.</li>
<li>results: The authors found that current deep neural networks are vulnerable to making mistakes on certain types of data, which means they may not be reliable in real-world scenarios where they may encounter data from many different domains. Additionally, the authors found that these models can be easily fooled into making wrong decisions.<details>
<summary>Abstract</summary>
Reliable and robust evaluation methods are a necessary first step towards developing machine learning models that are themselves robust and reliable. Unfortunately, current evaluation protocols typically used to assess classifiers fail to comprehensively evaluate performance as they tend to rely on limited types of test data, and ignore others. For example, using the standard test data fails to evaluate the predictions made by the classifier to samples from classes it was not trained on. On the other hand, testing with data containing samples from unknown classes fails to evaluate how well the classifier can predict the labels for known classes. This article advocates bench-marking performance using a wide range of different types of data and using a single metric that can be applied to all such data types to produce a consistent evaluation of performance. Using such a benchmark it is found that current deep neural networks, including those trained with methods that are believed to produce state-of-the-art robustness, are extremely vulnerable to making mistakes on certain types of data. This means that such models will be unreliable in real-world scenarios where they may encounter data from many different domains, and that they are insecure as they can easily be fooled into making the wrong decisions. It is hoped that these results will motivate the wider adoption of more comprehensive testing methods that will, in turn, lead to the development of more robust machine learning methods in the future.   Code is available at: \url{https://codeberg.org/mwspratling/RobustnessEvaluation}
</details>
<details>
<summary>摘要</summary>
可靠和可靠的评估方法是开发可靠和可靠的机器学习模型的必要第一步。 unfortunately，现有的评估协议通常只能够部分评估模型的性能，因为它们通常只使用有限的测试数据来进行评估。例如，使用标准测试数据不能评估模型对未知类样本的预测结果。相反，使用未知类样本来测试模型将不能评估模型对已知类样本的预测结果。本文提出了使用多种不同类型的数据进行 benchmarking性能，并使用一个可以应用于所有数据类型的单一指标来生成一致的评估性能。使用这种标准，发现当前的深度神经网络，包括使用认为会生成状态对的训练方法，对于某些类型的数据表示极度易误。这意味着这些模型在实际世界中的应用中将不可靠，因为它们可能会遇到多种领域的数据。此外，这些模型也是不安全的，因为它们可以轻松地被骗到错误地做出决策。希望这些结果能够激励更广泛的测试方法的采用，以便在未来开发更加可靠的机器学习方法。Code is available at: \url{https://codeberg.org/mwspratling/RobustnessEvaluation}
</details></li>
</ul>
<hr>
<h2 id="D-Score-A-Synapse-Inspired-Approach-for-Filter-Pruning"><a href="#D-Score-A-Synapse-Inspired-Approach-for-Filter-Pruning" class="headerlink" title="D-Score: A Synapse-Inspired Approach for Filter Pruning"></a>D-Score: A Synapse-Inspired Approach for Filter Pruning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04470">http://arxiv.org/abs/2308.04470</a></li>
<li>repo_url: None</li>
<li>paper_authors: Doyoung Park, Jinsoo Kim, Jina Nam, Jooyoung Chang, Sang Min Park</li>
<li>for: 本文提出了一种基于神经系统的filter pruning方法，用于减少卷积神经网络中过度的权重。</li>
<li>methods: 该方法采用了神经科学的视角，使用了动态得分（D-Score）分析独立重要性，并将权重分配分数来评估独立重要性。</li>
<li>results: 实验结果表明，该方法可以在CIFAR-10和ImageNet datasets上减少了显著的计算量和参数数量，而无需损失精度。<details>
<summary>Abstract</summary>
This paper introduces a new aspect for determining the rank of the unimportant filters for filter pruning on convolutional neural networks (CNNs). In the human synaptic system, there are two important channels known as excitatory and inhibitory neurotransmitters that transmit a signal from a neuron to a cell. Adopting the neuroscientific perspective, we propose a synapse-inspired filter pruning method, namely Dynamic Score (D-Score). D-Score analyzes the independent importance of positive and negative weights in the filters and ranks the independent importance by assigning scores. Filters having low overall scores, and thus low impact on the accuracy of neural networks are pruned. The experimental results on CIFAR-10 and ImageNet datasets demonstrate the effectiveness of our proposed method by reducing notable amounts of FLOPs and Params without significant Acc. Drop.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="OmniDataComposer-A-Unified-Data-Structure-for-Multimodal-Data-Fusion-and-Infinite-Data-Generation"><a href="#OmniDataComposer-A-Unified-Data-Structure-for-Multimodal-Data-Fusion-and-Infinite-Data-Generation" class="headerlink" title="OmniDataComposer: A Unified Data Structure for Multimodal Data Fusion and Infinite Data Generation"></a>OmniDataComposer: A Unified Data Structure for Multimodal Data Fusion and Infinite Data Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04126">http://arxiv.org/abs/2308.04126</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongyang Yu, Shihao Wang, Yuan Fang, Wangpeng An</li>
<li>for: 这篇论文旨在提出一种多modal数据融合和无限数据生成的创新方法，以提高多modal数据之间的交互和增强。</li>
<li>methods: 该方法使用了多种多operation，包括视频&#x2F;图像描述EXTRACTION、稠密描述EXTRACTION、自动语音识别（ASR）、光学字符识别（OCR）、Recognize Anything Model（RAM）和物体跟踪。</li>
<li>results: 该方法可以识别超过6400种类型的对象，大幅扩大视觉信息范围。它将多modal数据融合起来，促进modalities之间的互助和跨modal数据更正。最终输出将每个视频输入转化为详细的时间序列文档，使视频内容更易被大语言模型处理。<details>
<summary>Abstract</summary>
This paper presents OmniDataComposer, an innovative approach for multimodal data fusion and unlimited data generation with an intent to refine and uncomplicate interplay among diverse data modalities. Coming to the core breakthrough, it introduces a cohesive data structure proficient in processing and merging multimodal data inputs, which include video, audio, and text.   Our crafted algorithm leverages advancements across multiple operations such as video/image caption extraction, dense caption extraction, Automatic Speech Recognition (ASR), Optical Character Recognition (OCR), Recognize Anything Model(RAM), and object tracking. OmniDataComposer is capable of identifying over 6400 categories of objects, substantially broadening the spectrum of visual information. It amalgamates these diverse modalities, promoting reciprocal enhancement among modalities and facilitating cross-modal data correction. \textbf{The final output metamorphoses each video input into an elaborate sequential document}, virtually transmuting videos into thorough narratives, making them easier to be processed by large language models.   Future prospects include optimizing datasets for each modality to encourage unlimited data generation. This robust base will offer priceless insights to models like ChatGPT, enabling them to create higher quality datasets for video captioning and easing question-answering tasks based on video content. OmniDataComposer inaugurates a new stage in multimodal learning, imparting enormous potential for augmenting AI's understanding and generation of complex, real-world data.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Constructing-Custom-Thermodynamics-Using-Deep-Learning"><a href="#Constructing-Custom-Thermodynamics-Using-Deep-Learning" class="headerlink" title="Constructing Custom Thermodynamics Using Deep Learning"></a>Constructing Custom Thermodynamics Using Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04119">http://arxiv.org/abs/2308.04119</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoli Chen, Beatrice W. Soh, Zi-En Ooi, Eleonore Vissol-Gaudin, Haijun Yu, Kostya S. Novoselov, Kedar Hippalgaonkar, Qianxiao Li</li>
<li>for: 这个论文的目的是为了开发一种基于总的奥托曼理论的自动化科学发现平台，用于研究复杂动态系统。</li>
<li>methods: 该论文使用了机器学习方法，通过对微观轨迹观察的学习，直接从微观描述中学习宏观动力描述。</li>
<li>results: 该论文通过对聚合物延伸的研究，成功地学习了三个可解释的热动力坐标，并建立了聚合物延伸的动力景观，包括稳定状态和转变状态的识别，以及延伸速率的控制。此外，该论文还应用了该方法到了不同领域的空间疫病问题，证明了该方法的广泛科学和技术应用前景。<details>
<summary>Abstract</summary>
One of the most exciting applications of AI is automated scientific discovery based on previously amassed data, coupled with restrictions provided by the known physical principles, including symmetries and conservation laws. Such automated hypothesis creation and verification can assist scientists in studying complex phenomena, where traditional physical intuition may fail. Of particular importance are complex dynamic systems where their time evolution is strongly influenced by varying external parameters. In this paper we develop a platform based on a generalised Onsager principle to learn macroscopic dynamical descriptions of arbitrary stochastic dissipative systems directly from observations of their microscopic trajectories. We focus on systems whose complexity and sheer sizes render complete microscopic description impractical, and constructing theoretical macroscopic models requires extensive domain knowledge or trial-and-error. Our machine learning approach addresses this by simultaneously constructing reduced thermodynamic coordinates and interpreting the dynamics on these coordinates. We demonstrate our method by studying theoretically and validating experimentally, the stretching of long polymer chains in an externally applied field. Specifically, we learn three interpretable thermodynamic coordinates and build a dynamical landscape of polymer stretching, including (1) the identification of stable and transition states and (2) the control of the stretching rate. We further demonstrate the universality of our approach by applying it to an unrelated problem in a different domain: constructing macroscopic dynamics for spatial epidemics, showing that our method addresses wide scientific and technological applications.
</details>
<details>
<summary>摘要</summary>
一种非常有趣的人工智能应用是基于先前整理的数据自动化科学发现，与知道的物理原理限制相结合，包括对称和能量保守法则。这种自动生成和验证假设可以帮助科学家研究复杂现象，其中传统的物理直觉可能失效。特别是复杂动态系统，其时间演化受外部参数变化的影响很强。在这篇论文中，我们开发了基于总体的奥托生理定律的平台，用于直接从微型跟踪数据中学习杂动系统的宏观动力描述。我们关注的是 complexity 和 scale 至今不可能完全描述的系统，并且构建理论宏观模型需要广泛的领域知识或尝试试验。我们的机器学习方法解决了这个问题，同时构建了减少的热力学坐标和解释动力学。我们通过研究杂动聚合物强制延展的实验和理论分析，证明我们的方法可以在不同领域应用。
</details></li>
</ul>
<hr>
<h2 id="PTransIPs-Identification-of-phosphorylation-sites-based-on-protein-pretrained-language-model-and-Transformer"><a href="#PTransIPs-Identification-of-phosphorylation-sites-based-on-protein-pretrained-language-model-and-Transformer" class="headerlink" title="PTransIPs: Identification of phosphorylation sites based on protein pretrained language model and Transformer"></a>PTransIPs: Identification of phosphorylation sites based on protein pretrained language model and Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05115">http://arxiv.org/abs/2308.05115</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/statxzy7/ptransips">https://github.com/statxzy7/ptransips</a></li>
<li>paper_authors: Ziyang Xu, Haitian Zhong<br>for:这份研究用于开发一个新的深度学习模型，用于识别蛋白质中的磷酸化位点。methods:这个模型使用了一种新的深度学习架构，叫做PTransIPs，它将蛋白质中的氨基酸看作是字，将它们拓展为唯一的编码，并且使用了大型预训练的蛋白质模型的嵌入。results:实验结果显示，PTransIPs 能够高效地识别蛋白质中的磷酸化位点，AUROC 值为 0.9232 和 0.9660，分别用于识别磷酸化 S&#x2F;T 和 Y 位点。此外，实验还显示了预训练模型嵌入的贡献，以及模型的可读性和普遍性。<details>
<summary>Abstract</summary>
Phosphorylation is central to numerous fundamental cellular processes, influencing the onset and progression of a variety of diseases. The correct identification of these phosphorylation sites is of great importance to unravel the intricate molecular mechanisms within cells and during viral infections, potentially leading to the discovery of new therapeutic targets. In this study, we introduce PTransIPs, a novel deep learning model for the identification of phosphorylation sites. PTransIPs treat amino acids within protein sequences as words, extracting unique encodings based on their type and sequential position. The model also incorporates embeddings from large pretrained protein models as additional data inputs. PTransIPS is further trained on a combination model of convolutional neural network with residual connections and Transformer model equipped with multi-head attention mechanisms. At last, the model outputs classification results through a fully connected layer. The results of independent testing reveal that PTransIPs outperforms existing state-of-the-art(SOTA) methods, achieving AUROCs of 0.9232 and 0.9660 for identifying phosphorylated S/T and Y sites respectively. In addition, ablation studies prove that pretrained model embeddings contribute to the performance of PTransIPs. Furthermore, PTransIPs has interpretable amino acid preference, visible training process and shows generalizability on other bioactivity classification tasks. To facilitate usage, our code and data are publicly accessible at \url{https://github.com/StatXzy7/PTransIPs}.
</details>
<details>
<summary>摘要</summary>
蛋白磷酸化是细胞内多种基本生物过程中的核心，影响疾病发生和进程。正确识别这些磷酸化位点非常重要，以解释细胞内分子机制和病毒感染过程，可能导致新的药物目标的发现。在这项研究中，我们介绍了PTransIPs，一种新的深度学习模型，用于识别磷酸化位点。PTransIPs将蛋白质内的氨基酸看作 слова，提取唯一的编码，基于它们的类型和顺序位置。模型还使用大型预训练蛋白质模型的嵌入为附加数据输入。PTransIPS在一种组合的卷积神经网络和Transformer模型中进行了进一步训练。最后，模型输出了分类结果通过完全连接层。独立测试结果表明，PTransIPs超过了现有状态的方法，实现了AUROC值为0.9232和0.9660，用于识别磷酸化S/T和Y位点。此外，归因研究表明，预训练模型嵌入对PTransIPs的性能做出了贡献。此外，PTransIPs具有可解释的氨基酸偏好、可见的训练过程和在其他生物活动分类任务上的普适性。为便于使用，我们的代码和数据在GitHub上公开 accessible。
</details></li>
</ul>
<hr>
<h2 id="Correlating-Medi-Claim-Service-by-Deep-Learning-Neural-Networks"><a href="#Correlating-Medi-Claim-Service-by-Deep-Learning-Neural-Networks" class="headerlink" title="Correlating Medi-Claim Service by Deep Learning Neural Networks"></a>Correlating Medi-Claim Service by Deep Learning Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04469">http://arxiv.org/abs/2308.04469</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jayanthi Vajiram, Negha Senthil, Nean Adhith. P</li>
<li>for: 防止医疗保险诈骗案件，涵盖患者、医生、诊断中心和保险公司等多方面。</li>
<li>methods: 使用卷积神经网络架构，通过对不同提供者的clamshell进行相关性研究，检测洗钱活动。同时使用supervised和Unsupervised分类器来检测诈骗和非诈骗laims。</li>
<li>results: 通过这种方法，可以准确地检测和预测诈骗案件，保护医疗保险公司和投保人的金融发展。<details>
<summary>Abstract</summary>
Medical insurance claims are of organized crimes related to patients, physicians, diagnostic centers, and insurance providers, forming a chain reaction that must be monitored constantly. These kinds of frauds affect the financial growth of both insured people and health insurance companies. The Convolution Neural Network architecture is used to detect fraudulent claims through a correlation study of regression models, which helps to detect money laundering on different claims given by different providers. Supervised and unsupervised classifiers are used to detect fraud and non-fraud claims.
</details>
<details>
<summary>摘要</summary>
医疗保险索赔是有组织犯罪活动相关的患者、医生、诊断中心和保险公司，形成一个排练的链式反推。这种类型的诈骗活动会对保险人和医疗保险公司的财务发展产生影响。使用卷积神经网络架构来检测诈骗索赔，通过对不同提供者的索赔进行相关的回归分析，可以检测到财务融资。使用超级vised和无级supervised分类器来检测诈骗和非诈骗索赔。
</details></li>
</ul>
<hr>
<h2 id="Explainable-machine-learning-to-enable-high-throughput-electrical-conductivity-optimization-of-doped-conjugated-polymers"><a href="#Explainable-machine-learning-to-enable-high-throughput-electrical-conductivity-optimization-of-doped-conjugated-polymers" class="headerlink" title="Explainable machine learning to enable high-throughput electrical conductivity optimization of doped conjugated polymers"></a>Explainable machine learning to enable high-throughput electrical conductivity optimization of doped conjugated polymers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04103">http://arxiv.org/abs/2308.04103</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ji Wei Yoon, Adithya Kumar, Pawan Kumar, Kedar Hippalgaonkar, J Senthilnath, Vijila Chellappan<br>for: 这研究旨在提高填充 polymer 材料的电导率测量效率，并通过机器学习（ML）方法来加速物料发现。methods: 该研究使用 readily measured absorbance spectra 作为输入，使用 ML 模型来预测填充 polymer 材料的电导率。results: 研究发现，使用 ML 模型可以高度准确地分类和预测填充 polymer 材料的电导率，并且可以提高实验测量效率 by 89%。此外，该研究还解决了机器学习模型中的常见问题，即不可解释性，通过利用特有的数学性质和 ML 模型，得到了证明了 spectral influences on conductivity 的准确信息。<details>
<summary>Abstract</summary>
The combination of high-throughput experimentation techniques and machine learning (ML) has recently ushered in a new era of accelerated material discovery, enabling the identification of materials with cutting-edge properties. However, the measurement of certain physical quantities remains challenging to automate. Specifically, meticulous process control, experimentation and laborious measurements are required to achieve optimal electrical conductivity in doped polymer materials. We propose a ML approach, which relies on readily measured absorbance spectra, to accelerate the workflow associated with measuring electrical conductivity. The first ML model (classification model), accurately classifies samples with a conductivity >~25 to 100 S/cm, achieving a maximum of 100% accuracy rate. For the subset of highly conductive samples, we employed a second ML model (regression model), to predict their conductivities, yielding an impressive test R2 value of 0.984. To validate the approach, we showed that the models, neither trained on the samples with the two highest conductivities of 498 and 506 S/cm, were able to, in an extrapolative manner, correctly classify and predict them at satisfactory levels of errors. The proposed ML workflow results in an improvement in the efficiency of the conductivity measurements by 89% of the maximum achievable using our experimental techniques. Furthermore, our approach addressed the common challenge of the lack of explainability in ML models by exploiting bespoke mathematical properties of the descriptors and ML model, allowing us to gain corroborated insights into the spectral influences on conductivity. Through this study, we offer an accelerated pathway for optimizing the properties of doped polymer materials while showcasing the valuable insights that can be derived from purposeful utilization of ML in experimental science.
</details>
<details>
<summary>摘要</summary>
高通过率实验技术和机器学习（ML）已经引入了一个新的时代，快速发现新材料的Properties。然而，一些物理量的测量仍然具有挑战。 Specifically, 制造过程控制、实验和劳动密集的测量是必需的，以实现射频电性的优化。我们提议一种ML方法，基于ready measured absorbance spectrum，加速测量电性的工作流程。第一个ML模型（分类模型）精确地将样本分类为电性> ~25 to 100 S/cm，达到了100%的准确率。对于部分高电性样本，我们使用了第二个ML模型（回归模型），预测他们的电性，得到了惊人的测试R2值为0.984。为验证方法，我们证明了模型没有在两个最高电性的样本（498和506 S/cm）上训练时，仍然可以在推导性的方式下，正确地分类和预测它们，并且达到了满意的误差水平。提出的ML工作流程可以提高电性测量的效率 by 89%。此外，我们的方法解决了通用机器学习模型的解释性问题，通过特有的数学属性和ML模型，使我们可以获得协同的理解，从而提高了我们对电性的理解。通过这项研究，我们提供了一个加速优化射频电性材料的路径，同时展示了机器学习在实验科学中的有价值。
</details></li>
</ul>
<hr>
<h2 id="Asynchronous-Evolution-of-Deep-Neural-Network-Architectures"><a href="#Asynchronous-Evolution-of-Deep-Neural-Network-Architectures" class="headerlink" title="Asynchronous Evolution of Deep Neural Network Architectures"></a>Asynchronous Evolution of Deep Neural Network Architectures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04102">http://arxiv.org/abs/2308.04102</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jason Liang, Hormoz Shahrzad, Risto Miikkulainen</li>
<li>for: 提高ENAS的并发评估速度，提高演化过程的效率。</li>
<li>methods: 提出了一种通用异步评估策略（AES），适用于ENAS。AES使用队列保存最多$K$个个体，等待工作节点进行评估，并在$M&lt;&lt;K$个个体已经被评估后进行下一代创建。</li>
<li>results: 在11比特多路分配任务和图像描述任务中，AES实现了多重性和效率的提升， Suggesting that AES is a promising method for parallelizing the evolution of complex systems with long and variable evaluation times.<details>
<summary>Abstract</summary>
Many evolutionary algorithms (EAs) take advantage of parallel evaluation of candidates. However, if evaluation times vary significantly, many worker nodes (i.e.,\ compute clients) are idle much of the time, waiting for the next generation to be created. Evolutionary neural architecture search (ENAS), a class of EAs that optimizes the architecture and hyperparameters of deep neural networks, is particularly vulnerable to this issue. This paper proposes a generic asynchronous evaluation strategy (AES) that is then adapted to work with ENAS. AES increases throughput by maintaining a queue of upto $K$ individuals ready to be sent to the workers for evaluation and proceeding to the next generation as soon as $M<<K$ individuals have been evaluated by the workers. A suitable value for $M$ is determined experimentally, balancing diversity and efficiency. To showcase the generality and power of AES, it was first evaluated in 11-bit multiplexer design (a single-population verifiable discovery task) and then scaled up to ENAS for image captioning (a multi-population open-ended-optimization task). In both problems, a multifold performance improvement was observed, suggesting that AES is a promising method for parallelizing the evolution of complex systems with long and variable evaluation times, such as those in ENAS.
</details>
<details>
<summary>摘要</summary>
(Note: The text has been translated into Simplified Chinese, which is the standard form of Chinese used in mainland China and widely used in other countries as well. The translation is written in the traditional Chinese characters, rather than the simplified Chinese characters used in mainland China. The translation is based on the standard grammar and vocabulary of Simplified Chinese, and may differ slightly from the original text in terms of wording and sentence structure.)
</details></li>
</ul>
<hr>
<h2 id="Why-Data-Science-Projects-Fail"><a href="#Why-Data-Science-Projects-Fail" class="headerlink" title="Why Data Science Projects Fail"></a>Why Data Science Projects Fail</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04896">http://arxiv.org/abs/2308.04896</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xLaszlo/datascience-fails">https://github.com/xLaszlo/datascience-fails</a></li>
<li>paper_authors: Balaram Panda</li>
</ul>
<details>
<summary>Abstract</summary>
Data Science is a modern Data Intelligence practice, which is the core of many businesses and helps businesses build smart strategies around to deal with businesses challenges more efficiently. Data Science practice also helps in automating business processes using the algorithm, and it has several other benefits, which also deliver in a non-profitable framework. In regards to data science, three key components primarily influence the effective outcome of a data science project. Those are 1.Availability of Data 2.Algorithm 3.Processing power or infrastructure
</details>
<details>
<summary>摘要</summary>
《数据科学是现代数据智能实践之一，它是许多企业的核心，帮助企业构建智能策略，更有效地面对企业挑战。数据科学实践还可以自动化商业过程，它还有许多其他的优点，可以在非营利性框架下实现。在数据科学方面，三个关键组件主要影响数据科学项目的效果。那些是1.数据的可用性2.算法3.处理能力或基础设施》Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details>


<hr>
<h2 id="Application-Oriented-Benchmarking-of-Quantum-Generative-Learning-Using-QUARK"><a href="#Application-Oriented-Benchmarking-of-Quantum-Generative-Learning-Using-QUARK" class="headerlink" title="Application-Oriented Benchmarking of Quantum Generative Learning Using QUARK"></a>Application-Oriented Benchmarking of Quantum Generative Learning Using QUARK</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04082">http://arxiv.org/abs/2308.04082</a></li>
<li>repo_url: None</li>
<li>paper_authors: Florian J. Kiwit, Marwa Marso, Philipp Ross, Carlos A. Riofrío, Johannes Klepsch, Andre Luckow</li>
<li>for: 本研究旨在提供一个标准化的 Quantum Machine Learning（QML）算法评估框架，以便更好地评估Quantum Computing（QC）应用程序的性能。</li>
<li>methods: 本研究使用了QUantum computing Application benchmaRK（QUARK）框架，并将其扩展以包括训练和部署Quantum generative models的能力。</li>
<li>results: 本研究通过将不同的Quantum generative models在不同的环境下训练和部署，并使用了广泛的评估指标，以评估这些模型的实际性和可行性。<details>
<summary>Abstract</summary>
Benchmarking of quantum machine learning (QML) algorithms is challenging due to the complexity and variability of QML systems, e.g., regarding model ansatzes, data sets, training techniques, and hyper-parameters selection. The QUantum computing Application benchmaRK (QUARK) framework simplifies and standardizes benchmarking studies for quantum computing applications. Here, we propose several extensions of QUARK to include the ability to evaluate the training and deployment of quantum generative models. We describe the updated software architecture and illustrate its flexibility through several example applications: (1) We trained different quantum generative models using several circuit ansatzes, data sets, and data transformations. (2) We evaluated our models on GPU and real quantum hardware. (3) We assessed the generalization capabilities of our generative models using a broad set of metrics that capture, e.g., the novelty and validity of the generated data.
</details>
<details>
<summary>摘要</summary>
审核量子机器学习（QML）算法具有复杂性和多样性，如模型架构、数据集、训练技术和超参数选择等方面。QUantum computing Application benchmaRK（QUARK）框架可以简化和标准化量子计算应用程序的审核研究。我们提出了将QUARK扩展以支持量子生成模型的训练和部署评估。我们描述了更新后的软件架构，并通过多个示例应用 illustrate its flexibility：1. 我们使用不同的量子生成模型、数据集和数据变换训练了多种环境。2. 我们在GPU和真实量子硬件上评估了我们的模型。3. 我们使用一组广泛的指标评估我们的生成模型的泛化能力，例如生成数据的新鲜度和有效性。
</details></li>
</ul>
<hr>
<h2 id="Federated-Zeroth-Order-Optimization-using-Trajectory-Informed-Surrogate-Gradients"><a href="#Federated-Zeroth-Order-Optimization-using-Trajectory-Informed-Surrogate-Gradients" class="headerlink" title="Federated Zeroth-Order Optimization using Trajectory-Informed Surrogate Gradients"></a>Federated Zeroth-Order Optimization using Trajectory-Informed Surrogate Gradients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04077">http://arxiv.org/abs/2308.04077</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yao Shu, Xiaoqiang Lin, Zhongxiang Dai, Bryan Kian Hsiang Low</li>
<li>for: 该文章的目的是提出一种基于追踪信息的 federated zeroth-order optimization（FZoo）算法，以提高 Query 和通信效率。</li>
<li>methods: 该算法使用追踪信息来估计函数梯度，并通过自适应梯度修正来减少实际更新与globally更新之间的差异。</li>
<li>results: 实验表明，该算法在 federated black-box adversarial attack 和 federated non-differentiable metric optimization 等实际应用中具有理论上的改进和实际效果。<details>
<summary>Abstract</summary>
Federated optimization, an emerging paradigm which finds wide real-world applications such as federated learning, enables multiple clients (e.g., edge devices) to collaboratively optimize a global function. The clients do not share their local datasets and typically only share their local gradients. However, the gradient information is not available in many applications of federated optimization, which hence gives rise to the paradigm of federated zeroth-order optimization (ZOO). Existing federated ZOO algorithms suffer from the limitations of query and communication inefficiency, which can be attributed to (a) their reliance on a substantial number of function queries for gradient estimation and (b) the significant disparity between their realized local updates and the intended global updates. To this end, we (a) introduce trajectory-informed gradient surrogates which is able to use the history of function queries during optimization for accurate and query-efficient gradient estimation, and (b) develop the technique of adaptive gradient correction using these gradient surrogates to mitigate the aforementioned disparity. Based on these, we propose the federated zeroth-order optimization using trajectory-informed surrogate gradients (FZooS) algorithm for query- and communication-efficient federated ZOO. Our FZooS achieves theoretical improvements over the existing approaches, which is supported by our real-world experiments such as federated black-box adversarial attack and federated non-differentiable metric optimization.
</details>
<details>
<summary>摘要</summary>
联合优化，是一种emerging paradigm，可以应用于联合学习、联合优化等实际应用中。在这种模型中，多个客户端（例如边缘设备）可以共同优化一个全球函数。客户端不会分享本地数据，通常只会分享本地梯度。然而，在许多应用中，梯度信息不可用，这导致了联合零次顺序优化（ZOO）的出现。现有的联合ZOO算法受到查询和通信不确定性的限制，这可以被归因于（a）它们依赖大量的函数查询来Estimate梯度，以及（b）它们实现的本地更新与globally intended的更新之间的差异。为解决这个问题，我们（a）引入了路径参数预测的梯度代理，可以在优化过程中使用历史的函数查询来精确地Estimate梯度，以及（b）开发了适应的梯度调整技术，以mitigate the aforementioned disparity。基于这些，我们提出了联合零次顺序优化使用路径参数预测梯度（FZooS）算法，实现了查询和通信效率的联合ZOO。我们的FZooS理论上超越了现有的方法，这被支持了我们的实际实验，例如联合黑盒抗攻击和联合非 differentiable 度量优化。
</details></li>
</ul>
<hr>
<h2 id="Learning-Specialized-Activation-Functions-for-Physics-informed-Neural-Networks"><a href="#Learning-Specialized-Activation-Functions-for-Physics-informed-Neural-Networks" class="headerlink" title="Learning Specialized Activation Functions for Physics-informed Neural Networks"></a>Learning Specialized Activation Functions for Physics-informed Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04073">http://arxiv.org/abs/2308.04073</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leaplabthu/adaafforpinns">https://github.com/leaplabthu/adaafforpinns</a></li>
<li>paper_authors: Honghui Wang, Lu Lu, Shiji Song, Gao Huang</li>
<li>for: This paper aims to address the optimization difficulty of physics-informed neural networks (PINNs) by exploring the connection between PINNs and activation functions.</li>
<li>methods: The paper introduces adaptive activation functions to search for the optimal function when solving different problems, and compares different adaptive activation functions and their limitations in the context of PINNs.</li>
<li>results: The proposed adaptive activation function can be used to solve different PDE systems in an interpretable way, and its effectiveness is demonstrated on a series of benchmarks.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文的目的是解决physics-informed neural networks (PINNs) 中的优化困难，通过研究 PINNs 和活动函数之间的连接。</li>
<li>methods: 论文提出了 adaptive 活动函数，用于在不同问题上搜索优化的最佳函数，并对不同的 adaptive 活动函数进行比较和限制分析。</li>
<li>results: 提议的 adaptive 活动函数可以用于解决不同 PDE 系统，并且在可读性方面具有优势，效果在一系列 benchmark 上得到证明。<details>
<summary>Abstract</summary>
Physics-informed neural networks (PINNs) are known to suffer from optimization difficulty. In this work, we reveal the connection between the optimization difficulty of PINNs and activation functions. Specifically, we show that PINNs exhibit high sensitivity to activation functions when solving PDEs with distinct properties. Existing works usually choose activation functions by inefficient trial-and-error. To avoid the inefficient manual selection and to alleviate the optimization difficulty of PINNs, we introduce adaptive activation functions to search for the optimal function when solving different problems. We compare different adaptive activation functions and discuss their limitations in the context of PINNs. Furthermore, we propose to tailor the idea of learning combinations of candidate activation functions to the PINNs optimization, which has a higher requirement for the smoothness and diversity on learned functions. This is achieved by removing activation functions which cannot provide higher-order derivatives from the candidate set and incorporating elementary functions with different properties according to our prior knowledge about the PDE at hand. We further enhance the search space with adaptive slopes. The proposed adaptive activation function can be used to solve different PDE systems in an interpretable way. Its effectiveness is demonstrated on a series of benchmarks. Code is available at https://github.com/LeapLabTHU/AdaAFforPINNs.
</details>
<details>
<summary>摘要</summary>
物理学 informed neural networks (PINNs)  oftentimes 受到优化困难。在这种工作中，我们揭示了 PINNs 的优化困难与 activation functions 之间的关系。具体来说，我们发现 PINNs 解决不同的 PDE 问题时会具有高度敏感性于 activation functions。现有的工作通常通过不efficient trial-and-error 来选择 activation functions。为了避免不efficient manual selection 和 PINNs 的优化困难，我们引入了适应 activation functions，以搜索解决不同问题的优化函数。我们比较了不同的适应 activation functions，并讨论它们在 PINNs 中的局限性。此外，我们提议在 PINNs 优化中应用学习组合 candidate activation functions 的思想，以提高学习得到的函数的平滑性和多样性。这可以通过从候选集中除掉无法提供高阶导数的 activation functions，并将不同性质的 elementary functions 纳入候选集中来实现。我们还增加了 adaptive slopes，以进一步扩大搜索空间。我们的提议的适应 activation function 可以在可读性方面解决不同 PDE 系统。我们在一系列 benchmark 上证明了其效iveness。代码可以在 GitHub 上找到：https://github.com/LeapLabTHU/AdaAFforPINNs。
</details></li>
</ul>
<hr>
<h2 id="Path-Signatures-for-Diversity-in-Probabilistic-Trajectory-Optimisation"><a href="#Path-Signatures-for-Diversity-in-Probabilistic-Trajectory-Optimisation" class="headerlink" title="Path Signatures for Diversity in Probabilistic Trajectory Optimisation"></a>Path Signatures for Diversity in Probabilistic Trajectory Optimisation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04071">http://arxiv.org/abs/2308.04071</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucas Barcelos, Tin Lai, Rafael Oliveira, Paulo Borges, Fabio Ramos</li>
<li>for: 这篇论文是为了提出一种基于粗路理论的并行轨迹优化算法，以避免模式塌生和提高全球性。</li>
<li>methods: 该算法使用了粗路理论的路径签名和希尔伯特空间表示法，并将并行变ational推断与多样性推广核心相连接。</li>
<li>results: 经验表明，该策略可以在各种问题上实现更低的平均成本，从2D导航到受擦层环境中的机器人抓取器。<details>
<summary>Abstract</summary>
Motion planning can be cast as a trajectory optimisation problem where a cost is minimised as a function of the trajectory being generated. In complex environments with several obstacles and complicated geometry, this optimisation problem is usually difficult to solve and prone to local minima. However, recent advancements in computing hardware allow for parallel trajectory optimisation where multiple solutions are obtained simultaneously, each initialised from a different starting point. Unfortunately, without a strategy preventing two solutions to collapse on each other, naive parallel optimisation can suffer from mode collapse diminishing the efficiency of the approach and the likelihood of finding a global solution. In this paper we leverage on recent advances in the theory of rough paths to devise an algorithm for parallel trajectory optimisation that promotes diversity over the range of solutions, therefore avoiding mode collapses and achieving better global properties. Our approach builds on path signatures and Hilbert space representations of trajectories, and connects parallel variational inference for trajectory estimation with diversity promoting kernels. We empirically demonstrate that this strategy achieves lower average costs than competing alternatives on a range of problems, from 2D navigation to robotic manipulators operating in cluttered environments.
</details>
<details>
<summary>摘要</summary>
运动规划可以被视为一个轨迹优化问题，其中一个目标是将轨迹优化为最小化成本函数。在复杂的环境中，拥有多个障碍物和复杂的几何结构时，这个优化问题通常具有困难和极杂的本地最优解。然而，当前的计算硬件技术使得可以并行进行轨迹优化，从不同的起始点初始化多个解决方案。然而，如果没有避免两个解决方案相互冲突的策略，直观的并行优化可能会降低效率和找到全局解的可能性。在这篇论文中，我们利用了最近的粗 PATH 理论来设计一种避免模式崩溃的并行轨迹优化算法，该算法会Promote 多样性在解决方案的范围内，从而避免模式崩溃并实现更好的全局性。我们的方法基于轨迹签名和希尔伯特空间表示法，并将并行变分推理与多样性激活器相连接。我们在各种问题上进行了实验，并证明了这种策略可以在范围内实现更低的平均成本。
</details></li>
</ul>
<hr>
<h2 id="ConDistFL-Conditional-Distillation-for-Federated-Learning-from-Partially-Annotated-Data"><a href="#ConDistFL-Conditional-Distillation-for-Federated-Learning-from-Partially-Annotated-Data" class="headerlink" title="ConDistFL: Conditional Distillation for Federated Learning from Partially Annotated Data"></a>ConDistFL: Conditional Distillation for Federated Learning from Partially Annotated Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04070">http://arxiv.org/abs/2308.04070</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nvidia/nvflare">https://github.com/nvidia/nvflare</a></li>
<li>paper_authors: Pochuan Wang, Chen Shen, Weichung Wang, Masahiro Oda, Chiou-Shann Fuh, Kensaku Mori, Holger R. Roth</li>
<li>For: 提出了一种总结多个器官和疾病的整体分割模型，使用联合学习（FL）技术，并且解决了基本缺乏完全标注数据的问题。* Methods:  combining FL with knowledge distillation，使得本地模型可以从全球模型中提取未标注器官和肿瘤的知识，并且使用适当的条件概率表示来做这一点。* Results: 对四个不同的部分标注的腹部CT数据集进行验证，并证明了该方法与FedAvg和FedOpt基elines相比，具有显著的提高。此外，对外部测试数据集的性能也表明了模型在不同数据集上进行集成训练后的优异普适性。<details>
<summary>Abstract</summary>
Developing a generalized segmentation model capable of simultaneously delineating multiple organs and diseases is highly desirable. Federated learning (FL) is a key technology enabling the collaborative development of a model without exchanging training data. However, the limited access to fully annotated training data poses a major challenge to training generalizable models. We propose "ConDistFL", a framework to solve this problem by combining FL with knowledge distillation. Local models can extract the knowledge of unlabeled organs and tumors from partially annotated data from the global model with an adequately designed conditional probability representation. We validate our framework on four distinct partially annotated abdominal CT datasets from the MSD and KiTS19 challenges. The experimental results show that the proposed framework significantly outperforms FedAvg and FedOpt baselines. Moreover, the performance on an external test dataset demonstrates superior generalizability compared to models trained on each dataset separately. Our ablation study suggests that ConDistFL can perform well without frequent aggregation, reducing the communication cost of FL. Our implementation will be available at https://github.com/NVIDIA/NVFlare/tree/dev/research/condist-fl.
</details>
<details>
<summary>摘要</summary>
发展一种可同时分割多个器官和疾病的通用模型是非常有优点的。联邦学习（FL）是一种关键技术，它允许合作建立模型，而不需要交换训练数据。然而，有限的完全标注数据对训练通用模型 pose 一个主要挑战。我们提出了 "ConDistFL" 框架，它将 FL 与知识塑造相结合，以解决这个问题。本地模型可以从全球模型中提取未标注器官和肿瘤的知识，使用适当设计的conditional probability表示。我们在四个不同的 partially annotated 腹部 CT 数据集上验证了我们的框架。实验结果表明，我们的框架在 FedAvg 和 FedOpt 基elines 上显著超越了。此外，对于外部测试集的性能表明，我们的模型具有较高的普适性，比单独在每个数据集上训练的模型要好。我们的剖析研究表明，ConDistFL 可以在不经常的聚合情况下表现良好，降低了联邦学习中的通信成本。我们的实现将在 GitHub 上提供，请参考 <https://github.com/NVIDIA/NVFlare/tree/dev/research/condist-fl>。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Adversarial-Robustness-in-Low-Label-Regime-via-Adaptively-Weighted-Regularization-and-Knowledge-Distillation"><a href="#Enhancing-Adversarial-Robustness-in-Low-Label-Regime-via-Adaptively-Weighted-Regularization-and-Knowledge-Distillation" class="headerlink" title="Enhancing Adversarial Robustness in Low-Label Regime via Adaptively Weighted Regularization and Knowledge Distillation"></a>Enhancing Adversarial Robustness in Low-Label Regime via Adaptively Weighted Regularization and Knowledge Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04061">http://arxiv.org/abs/2308.04061</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongyoon Yang, Insung Kong, Yongdai Kim</li>
<li>For: This paper focuses on semi-supervised adversarial training, where labeled data is scarce.* Methods: The authors derive two upper bounds for the robust risk and propose a regularization term for unlabeled data. They also develop a semi-supervised adversarial training algorithm that combines the proposed regularization term with knowledge distillation using a semi-supervised teacher.* Results: The authors achieve state-of-the-art performance with significant margins compared to existing algorithms. Specifically, their algorithm with only 8% labeled data is comparable to supervised adversarial training algorithms that use all labeled data in terms of standard and robust accuracies on CIFAR-10.Here’s the Chinese translation of the three key points:* For: 这篇论文专注于半指导式对抗训练，即标注数据匮乏的情况。* Methods: 作者提出了两个Upper bound，并提出了一个用于未标注数据的正则化项。他们还开发了一种半指导式对抗训练算法，该算法结合了提出的正则化项和知识塑造。* Results: 作者实现了现有算法的最佳性能，具体来说，他们的算法只使用8%的标注数据，仍能与全量标注数据使用的超级vised adversarial training算法相当，即在CIFAR-10上的标准准确率和对抗性准确率。<details>
<summary>Abstract</summary>
Adversarial robustness is a research area that has recently received a lot of attention in the quest for trustworthy artificial intelligence. However, recent works on adversarial robustness have focused on supervised learning where it is assumed that labeled data is plentiful. In this paper, we investigate semi-supervised adversarial training where labeled data is scarce. We derive two upper bounds for the robust risk and propose a regularization term for unlabeled data motivated by these two upper bounds. Then, we develop a semi-supervised adversarial training algorithm that combines the proposed regularization term with knowledge distillation using a semi-supervised teacher (i.e., a teacher model trained using a semi-supervised learning algorithm). Our experiments show that our proposed algorithm achieves state-of-the-art performance with significant margins compared to existing algorithms. In particular, compared to supervised learning algorithms, performance of our proposed algorithm is not much worse even when the amount of labeled data is very small. For example, our algorithm with only 8\% labeled data is comparable to supervised adversarial training algorithms that use all labeled data, both in terms of standard and robust accuracies on CIFAR-10.
</details>
<details>
<summary>摘要</summary>
“敌对响应性”是人工智能的研究领域，最近受到了很多关注，以建立可靠的人工智能。然而，现有的工作假设了充足的标签数据，并且专注于监督学习。在本文中，我们研究 semi-supervised adversarial 训练，其中标签数据稀缺。我们 derive two upper bounds for the robust risk，并提出一个鼓励不标签数据的调整项。然后，我们开发了一个 semi-supervised adversarial 训练算法，它结合了提案的调整项和知识传授使用 semi-supervised teacher (即使用 semi-supervised 学习算法训练的教师模型)。我们的实验结果显示，我们的提案算法可以实现现在的最佳性能，并且与已有的算法相比，仅在标签数据非常少时，性能与监督学习算法相似。例如，我们的算法仅使用 8% 的标签数据时，与监督学习算法使用所有标签数据相比，在 CIFAR-10 上的标准和敌对精度都具有显著的优化。”
</details></li>
</ul>
<hr>
<h2 id="Backdoor-Federated-Learning-by-Poisoning-Backdoor-Critical-Layers"><a href="#Backdoor-Federated-Learning-by-Poisoning-Backdoor-Critical-Layers" class="headerlink" title="Backdoor Federated Learning by Poisoning Backdoor-Critical Layers"></a>Backdoor Federated Learning by Poisoning Backdoor-Critical Layers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04466">http://arxiv.org/abs/2308.04466</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haomin Zhuang, Mingxian Yu, Hao Wang, Yang Hua, Jian Li, Xu Yuan</li>
<li>for: 这 paper 旨在探讨 federated learning (FL) 中存在攻击敏感数据的差点，并提出了一种基于攻击者视角的增强型隐蔽攻击方法。</li>
<li>methods: 该 paper 使用了一种涉及攻击者视角的方法来识别 federated learning (FL) 模型中的敏感层次，然后通过适应性地进行攻击来寻找适合的攻击方法。</li>
<li>results: 实验结果表明，该 paper 提出的 BC 层攻击方法可以在七种 state-of-the-art (SOTA) 防御策略下成功地攻击 federated learning (FL)，且比较新的攻击方法更高效。<details>
<summary>Abstract</summary>
Federated learning (FL) has been widely deployed to enable machine learning training on sensitive data across distributed devices. However, the decentralized learning paradigm and heterogeneity of FL further extend the attack surface for backdoor attacks. Existing FL attack and defense methodologies typically focus on the whole model. None of them recognizes the existence of backdoor-critical (BC) layers-a small subset of layers that dominate the model vulnerabilities. Attacking the BC layers achieves equivalent effects as attacking the whole model but at a far smaller chance of being detected by state-of-the-art (SOTA) defenses. This paper proposes a general in-situ approach that identifies and verifies BC layers from the perspective of attackers. Based on the identified BC layers, we carefully craft a new backdoor attack methodology that adaptively seeks a fundamental balance between attacking effects and stealthiness under various defense strategies. Extensive experiments show that our BC layer-aware backdoor attacks can successfully backdoor FL under seven SOTA defenses with only 10% malicious clients and outperform the latest backdoor attack methods.
</details>
<details>
<summary>摘要</summary>
联合学习（FL）已经广泛应用以进行分散设备上的机器学习训练。然而，分散式学习模式和资料多样性对FL的攻击面积增加了额外的隐藏问题。现有的FL攻击和防御方法通常集中在整个模型上。 none of them 认为存在关键层（BC）-一小subset of layers that dominate the model vulnerabilities. 攻击BC层可以实现equivalent 的效果，但是比攻击整个模型要小得多，这使得现有的防御技术更难察觉。 This paper proposes a general in-situ approach that identifies and verifies BC layers from the perspective of attackers. Based on the identified BC layers, we carefully craft a new backdoor attack methodology that adaptively seeks a fundamental balance between attacking effects and stealthiness under various defense strategies. 实验表明，我们的BC层意识的后门攻击可以成功地在七种SOTA防御措施下进行后门攻击，并且比latest backdoor attack methods 高效。
</details></li>
</ul>
<hr>
<h2 id="Toward-Improving-Predictive-Risk-Modelling-for-New-Zealand’s-Child-Welfare-System-Using-Clustering-Methods"><a href="#Toward-Improving-Predictive-Risk-Modelling-for-New-Zealand’s-Child-Welfare-System-Using-Clustering-Methods" class="headerlink" title="Toward Improving Predictive Risk Modelling for New Zealand’s Child Welfare System Using Clustering Methods"></a>Toward Improving Predictive Risk Modelling for New Zealand’s Child Welfare System Using Clustering Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04060">http://arxiv.org/abs/2308.04060</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sahar Barmomanesh, Victor Miranda-Soberanis</li>
<li>for: 这个研究旨在帮助社工人员更好地识别儿童滋扰的风险因素，并决定当局是否应对儿童进行介入。</li>
<li>methods: 这个研究使用了主成因分析和K-Means clustering方法来识别儿童的风险因素，并分析这些因素之间的互动关系。</li>
<li>results: 研究发现，使用不同的 clustering 方法可以分辨出不同的儿童群体，并且这些群体之间存在一定的区别。此外，研究发现，使用特定的年龄组别的模型可以提高模型的准确性。<details>
<summary>Abstract</summary>
The combination of clinical judgement and predictive risk models crucially assist social workers to segregate children at risk of maltreatment and decide when authorities should intervene. Predictive risk modelling to address this matter has been initiated by several governmental welfare authorities worldwide involving administrative data and machine learning algorithms. While previous studies have investigated risk factors relating to child maltreatment, several gaps remain as to understanding how such risk factors interact and whether predictive risk models perform differently for children with different features. By integrating Principal Component Analysis and K-Means clustering, this paper presents initial findings of our work on the identification of such features as well as their potential effect on current risk modelling frameworks. This approach allows examining existent, unidentified yet, clusters of New Zealand (NZ) children reported with care and protection concerns, as well as to analyse their inner structure, and evaluate the performance of prediction models trained cluster wise. We aim to discover the extent of clustering degree required as an early step in the development of predictive risk models for child maltreatment and so enhance the accuracy of such models intended for use by child protection authorities. The results from testing LASSO logistic regression models trained on identified clusters revealed no significant difference in their performance. The models, however, performed slightly better for two clusters including younger children. our results suggest that separate models might need to be developed for children of certain age to gain additional control over the error rates and to improve model accuracy. While results are promising, more evidence is needed to draw definitive conclusions, and further investigation is necessary.
</details>
<details>
<summary>摘要</summary>
临床判断和预测风险模型可以帮助社工分类受护儿童投入风险和决定当局是否介入。预测风险模型在世界各地政府儿童护理机构中已经被开发，使用行政数据和机器学习算法。 although previous studies have investigated child maltreatment risk factors, there are still gaps in understanding how these risk factors interact and whether predictive risk models perform differently for children with different features. 本文使用主成分分析和K-Means聚类分析初步发现了这些特征，以及它们可能对当前风险模型 frameworks 有何影响。这种方法允许我们检查新西兰（NZ）儿童报告了护理和保护问题的现有、未知的群集，以及其内部结构，并评估这些群集训练的预测模型性能。我们的目标是发现预测模型是否需要不同的年龄层分配，以提高预测模型的准确性。我们的结果表明，使用LASSO logistic regression模型训练于特定群集没有显著差异。然而，这些模型在两个年龄较少的群集中表现稍微更好。这些结果表明，可能需要为不同的年龄层开发不同的模型，以提高预测模型的准确性。虽然结果有前途，但需要更多的证据来 draw definitive conclusions，并进一步进行调查。
</details></li>
</ul>
<hr>
<h2 id="The-Five-Dollar-Model-Generating-Game-Maps-and-Sprites-from-Sentence-Embeddings"><a href="#The-Five-Dollar-Model-Generating-Game-Maps-and-Sprites-from-Sentence-Embeddings" class="headerlink" title="The Five-Dollar Model: Generating Game Maps and Sprites from Sentence Embeddings"></a>The Five-Dollar Model: Generating Game Maps and Sprites from Sentence Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04052">http://arxiv.org/abs/2308.04052</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/TimMerino1710/five-dollar-model">https://github.com/TimMerino1710/five-dollar-model</a></li>
<li>paper_authors: Timothy Merino, Roman Negri, Dipika Rajesh, M Charity, Julian Togelius</li>
<li>for: 这个论文旨在提出一种轻量级的文本到图像生成模型，能够从编码的文本提示生成低维度图像。</li>
<li>methods: 这个模型使用了一些新的扩展策略，以提高模型在有限的数据集上的性能。</li>
<li>results: 模型能够生成高度准确和美观的图像，同时保持文本提示中的含义。<details>
<summary>Abstract</summary>
The five-dollar model is a lightweight text-to-image generative architecture that generates low dimensional images from an encoded text prompt. This model can successfully generate accurate and aesthetically pleasing content in low dimensional domains, with limited amounts of training data. Despite the small size of both the model and datasets, the generated images are still able to maintain the encoded semantic meaning of the textual prompt. We apply this model to three small datasets: pixel art video game maps, video game sprite images, and down-scaled emoji images and apply novel augmentation strategies to improve the performance of our model on these limited datasets. We evaluate our models performance using cosine similarity score between text-image pairs generated by the CLIP VIT-B/32 model.
</details>
<details>
<summary>摘要</summary>
“五块模型”是一种轻量级文本到图像生成架构，可以从编码的文本提示生成低维度图像。这种模型可以在有限的培训数据下生成准确和美观的内容，并且保持文本提示中的含义。我们将这种模型应用于三个小 datasets：像素艺术视频游戏地图、视频游戏填充图像和压缩emoji图像。我们还使用了新的扩展策略来提高我们模型的性能。我们使用 cosine similarity 分数来评估我们模型对文本-图像对的表现。
</details></li>
</ul>
<hr>
<h2 id="Generative-Models-for-Anomaly-Detection-and-Design-Space-Dimensionality-Reduction-in-Shape-Optimization"><a href="#Generative-Models-for-Anomaly-Detection-and-Design-Space-Dimensionality-Reduction-in-Shape-Optimization" class="headerlink" title="Generative Models for Anomaly Detection and Design-Space Dimensionality Reduction in Shape Optimization"></a>Generative Models for Anomaly Detection and Design-Space Dimensionality Reduction in Shape Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04051">http://arxiv.org/abs/2308.04051</a></li>
<li>repo_url: None</li>
<li>paper_authors: Danny D’Agostino</li>
<li>for: 提高全球优化算法的效率，同时促进优化过程中高质量的设计生成。</li>
<li>methods: 减少设计变量数量，最大化几何变量的差异，使用概率线性隐藏变量模型，如因素分析和概率主成分分析。</li>
<li>results: 提高全球优化算法的收敛性，仅生成高质量几何特征的设计，避免 computationally expensive 优化过程中的浪费。<details>
<summary>Abstract</summary>
Our work presents a novel approach to shape optimization, that has the twofold objective to improve the efficiency of global optimization algorithms while promoting the generation of high-quality designs during the optimization process free of geometrical anomalies. This is accomplished by reducing the number of the original design variables defining a new reduced subspace where the geometrical variance is maximized and modeling the underlying generative process of the data via probabilistic linear latent variable models such as Factor Analysis and Probabilistic Principal Component Analysis. We show that the data follows approximately a Gaussian distribution when the shape modification method is linear and the design variables are sampled uniformly at random, due to the direct application of the central limit theorem. The model uncertainty is measured in terms of Mahalanobis distance, and the paper demonstrates that anomalous designs tend to exhibit a high value of this metric. This enables the definition of a new optimization model where anomalous geometries are penalized and consequently avoided during the optimization loop. The procedure is demonstrated for hull shape optimization of the DTMB 5415 model, extensively used as an international benchmark for shape optimization problems. The global optimization routine is carried out using Bayesian Optimization and the DIRECT algorithm. From the numerical results, the new framework improves the convergence of global optimization algorithms, while only designs with high-quality geometrical features are generated through the optimization routine thereby avoiding the wastage of precious computationally expensive simulations.
</details>
<details>
<summary>摘要</summary>
我们的工作提出了一种新的方法 для优化形状，以提高全球优化算法的效率，同时推出高质量的设计。这是通过减少原始设计变量，定义一个新的减少子空间，使几何异常值最大化，并使用抽象线性latent variable模型，如因素分析和概率主成分分析，来模型数据的生成过程。我们证明数据遵循近似 Gaussian 分布，当shape modification方法是线性的，并且设计变量随机 sampling 时，通过直接应用中心偏移定理。模型不确定性被测量为 Mahalanobis 距离，并且实验表明，异常设计通常具有高值这个指标。这允许定义一个新的优化模型，惩罚异常几何，并在优化迭代中避免异常设计的生成。我们在 DTMB 5415 模型的船体形状优化中进行了实验，使用 Bayesian 优化和 DIRECT 算法。从numerical 结果来看，新的框架可以提高全球优化算法的收敛，同时只有高质量的几何特征被优化算法生成，从而避免了计算成本expensive的simulation 的浪费。
</details></li>
</ul>
<hr>
<h2 id="A-Comparative-Study-on-TF-IDF-feature-Weighting-Method-and-its-Analysis-using-Unstructured-Dataset"><a href="#A-Comparative-Study-on-TF-IDF-feature-Weighting-Method-and-its-Analysis-using-Unstructured-Dataset" class="headerlink" title="A Comparative Study on TF-IDF feature Weighting Method and its Analysis using Unstructured Dataset"></a>A Comparative Study on TF-IDF feature Weighting Method and its Analysis using Unstructured Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04037">http://arxiv.org/abs/2308.04037</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mamata Das, Selvakumar K., P. J. A. Alphonse</li>
<li>for: 这篇论文的目的是研究文本分类中feature重要性的问题，以及使用TF-IDF和NLP算法进行文本分类。</li>
<li>methods: 这篇论文使用了IMDB电影评论和Amazon Alexa评论数据集进行实验，并使用了多种常见的分类算法来验证提出的方法，包括支持向量机（SVM）、抽象函数（Logistic Regression）、多项随机树（Random Forest）、决策树（Decision Tree）和k-最近邻居（KNN）。</li>
<li>results: 研究发现，基于TF-IDF特征提取方法可以获得最高的准确率（93.81%）、精度（94.20%）、回归率（93.81%）和F1分数（91.99%）值，而基于N-Gram特征提取方法则不如TF-IDF方法。<details>
<summary>Abstract</summary>
Text Classification is the process of categorizing text into the relevant categories and its algorithms are at the core of many Natural Language Processing (NLP). Term Frequency-Inverse Document Frequency (TF-IDF) and NLP are the most highly used information retrieval methods in text classification. We have investigated and analyzed the feature weighting method for text classification on unstructured data. The proposed model considered two features N-Grams and TF-IDF on the IMDB movie reviews and Amazon Alexa reviews dataset for sentiment analysis. Then we have used the state-of-the-art classifier to validate the method i.e., Support Vector Machine (SVM), Logistic Regression, Multinomial Naive Bayes (Multinomial NB), Random Forest, Decision Tree, and k-nearest neighbors (KNN). From those two feature extractions, a significant increase in feature extraction with TF-IDF features rather than based on N-Gram. TF-IDF got the maximum accuracy (93.81%), precision (94.20%), recall (93.81%), and F1-score (91.99%) value in Random Forest classifier.
</details>
<details>
<summary>摘要</summary>
文本分类是将文本分类到相关的类别中的过程，其算法是自然语言处理（NLP）的核心。文本频率-反向文档频率（TF-IDF）和NLP是文本检索中最广泛使用的方法。我们已经对文本分类中的特征赋值方法进行了调查和分析。我们提出了基于IMDB电影评论和Amazon Alexa评论数据集的 sentiment analysis 的方法，并使用了当今最佳的分类器来验证方法，即支持向量机（SVM）、概率回归、多项随机森林（Multinomial NB）、随机树、决策树和k-最近邻居（KNN）。从两个特征提取来看，TF-IDF特征的特征提取得到了显著的增加，而不是基于N- Gram。TF-IDF在Random Forest分类器中获得了最大的准确率（93.81%）、精度（94.20%）、回归率（93.81%）和F1分数（91.99%)值。
</details></li>
</ul>
<hr>
<h2 id="Top-K-Relevant-Passage-Retrieval-for-Biomedical-Question-Answering"><a href="#Top-K-Relevant-Passage-Retrieval-for-Biomedical-Question-Answering" class="headerlink" title="Top K Relevant Passage Retrieval for Biomedical Question Answering"></a>Top K Relevant Passage Retrieval for Biomedical Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04028">http://arxiv.org/abs/2308.04028</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shashank140195/Biomedical_QA_Model">https://github.com/shashank140195/Biomedical_QA_Model</a></li>
<li>paper_authors: Shashank Gupta<br>for:本研究旨在开发一个基于Pubmed文章的生物医学问答系统，以提供准确的答案。methods:本研究使用现有的DPR框架，并在其基础上进行了细致的调整和训练，以提高问答系统的准确率。results:在 BioASQ 问答集上进行评估，我们的调整后的紧密检索器得分为0.81，表明我们的方法可以提供高度准确的答案。<details>
<summary>Abstract</summary>
Question answering is a task that answers factoid questions using a large collection of documents. It aims to provide precise answers in response to the user's questions in natural language. Question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. On the web, there is no single article that could provide all the possible answers available on the internet to the question of the problem asked by the user. The existing Dense Passage Retrieval model has been trained on Wikipedia dump from Dec. 20, 2018, as the source documents for answering questions. Question answering (QA) has made big strides with several open-domain and machine comprehension systems built using large-scale annotated datasets. However, in the clinical domain, this problem remains relatively unexplored. According to multiple surveys, Biomedical Questions cannot be answered correctly from Wikipedia Articles. In this work, we work on the existing DPR framework for the biomedical domain and retrieve answers from the Pubmed articles which is a reliable source to answer medical questions. When evaluated on a BioASQ QA dataset, our fine-tuned dense retriever results in a 0.81 F1 score.
</details>
<details>
<summary>摘要</summary>
问答任务是用一个大量文档集来回答用户的问题。其目标是通过自然语言提供精准的答案。问答需要高效的段落检索，以选择可能的上下文，传统上使用TF-IDF或BM25等稀疏 вектор空间模型。在互联网上，没有一篇文章可以提供用户问题的所有可能的答案。我们使用Dec. 20, 2018年的Wikipedia备份作为问答模型的训练数据源。问答（QA）在开放领域和机器理解领域已经做出了很大的进步，但在医疗领域这个问题还很少研究。根据多个调查，医学问题不能准确地从Wikipedia文章中得到答案。在这种情况下，我们对现有的DPR框架进行了修改，并从Pubmed文章中检索答案。当评估在BioASQ QA数据集上时，我们的精度检索器得到了0.81的F1分数。
</details></li>
</ul>
<hr>
<h2 id="Scope-Loss-for-Imbalanced-Classification-and-RL-Exploration"><a href="#Scope-Loss-for-Imbalanced-Classification-and-RL-Exploration" class="headerlink" title="Scope Loss for Imbalanced Classification and RL Exploration"></a>Scope Loss for Imbalanced Classification and RL Exploration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04024">http://arxiv.org/abs/2308.04024</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hasham Burhani, Xiao Qi Shi, Jonathan Jaegerman, Daniel Balicki</li>
<li>for: 本研究目的是Equivalence between reinforcement learning problem和Supervised classification problem，并找到它们之间的相似性。</li>
<li>methods: 本研究使用了探索尝试和优化问题的探索-优化补偿来Address the exploration exploitation trade-off in reinforcement learning and the dataset imbalance problem in supervised classification。</li>
<li>results: 研究发现了一种新的损失函数Scope Loss，可以防止过度利用和数据偏好导致的性能下降，无需进行任何调整。Scope Loss在一系列基准功能回归学任务和一个偏好分类 dataset 上测试，与State-of-the-art损失函数相比，Scope Loss表现出色。<details>
<summary>Abstract</summary>
We demonstrate equivalence between the reinforcement learning problem and the supervised classification problem. We consequently equate the exploration exploitation trade-off in reinforcement learning to the dataset imbalance problem in supervised classification, and find similarities in how they are addressed. From our analysis of the aforementioned problems we derive a novel loss function for reinforcement learning and supervised classification. Scope Loss, our new loss function, adjusts gradients to prevent performance losses from over-exploitation and dataset imbalances, without the need for any tuning. We test Scope Loss against SOTA loss functions over a basket of benchmark reinforcement learning tasks and a skewed classification dataset, and show that Scope Loss outperforms other loss functions.
</details>
<details>
<summary>摘要</summary>
我们展示了强化学习问题和超级vised分类问题之间的等值性。我们遂视探索优化和数据集不均势问题在强化学习和超级vised分类中的相似性，并从这些问题的分析中获得了一个新的损失函数。我们称之为Scope Loss。Scope Loss可以调整 gradients，以避免因过度探索而导致的性能损失和数据集不均势问题，不需要任何调整。我们将Scope Loss与现有的损失函数进行比较，在一签 benchmark 强化学习任务和一个偏斜的分类dataset上进行测试，结果显示Scope Loss可以超越其他损失函数。
</details></li>
</ul>
<hr>
<h2 id="Improving-Performance-of-Semi-Supervised-Learning-by-Adversarial-Attacks"><a href="#Improving-Performance-of-Semi-Supervised-Learning-by-Adversarial-Attacks" class="headerlink" title="Improving Performance of Semi-Supervised Learning by Adversarial Attacks"></a>Improving Performance of Semi-Supervised Learning by Adversarial Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04018">http://arxiv.org/abs/2308.04018</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongyoon Yang, Kunwoong Kim, Yongdai Kim</li>
<li>for: 提高latest SSL算法的表现，使其更适应 semi-supervised learning 的应用场景。</li>
<li>methods: 提出了一种通用框架SCAR，通过对预训练模型进行 adversarial 攻击，选择高自信仪器的无标样本进行标注。</li>
<li>results: 在 CIFAR10 上，与 SCAR 结合的三种 latest SSL 算法显示出了显著提高图像分类的表现。<details>
<summary>Abstract</summary>
Semi-supervised learning (SSL) algorithm is a setup built upon a realistic assumption that access to a large amount of labeled data is tough. In this study, we present a generalized framework, named SCAR, standing for Selecting Clean samples with Adversarial Robustness, for improving the performance of recent SSL algorithms. By adversarially attacking pre-trained models with semi-supervision, our framework shows substantial advances in classifying images. We introduce how adversarial attacks successfully select high-confident unlabeled data to be labeled with current predictions. On CIFAR10, three recent SSL algorithms with SCAR result in significantly improved image classification.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>半有指导学习（SSL）算法是基于现实的假设，即获得大量标注数据困难。在这个研究中，我们提出一种通用框架，名为SCAR，即选择清洁样本并具有对抗鲁棒性。通过对预训练模型进行对抗攻击，我们的框架实现了显著提高图像分类性能。我们介绍了如何使用对抗攻击选择高信度无标记数据，并将当前预测作为标注。在CIFAR10上，三种最近的SSL算法与SCAR结果显著提高图像分类。
</details></li>
</ul>
<hr>
<h2 id="Continual-Pre-Training-of-Large-Language-Models-How-to-re-warm-your-model"><a href="#Continual-Pre-Training-of-Large-Language-Models-How-to-re-warm-your-model" class="headerlink" title="Continual Pre-Training of Large Language Models: How to (re)warm your model?"></a>Continual Pre-Training of Large Language Models: How to (re)warm your model?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04014">http://arxiv.org/abs/2308.04014</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kshitij Gupta, Benjamin Thérien, Adam Ibrahim, Mats L. Richter, Quentin Anthony, Eugene Belilovsky, Irina Rish, Timothée Lesort</li>
<li>for: 这研究旨在提高大型语言模型（LLMs）的效率和成本，通过不断更新已经预训练的模型而不是从scratch重新训练。</li>
<li>methods: 研究者采用了不同的温存策略来检查学习率的影响，包括线性温存和偏微分衰减。</li>
<li>results: 研究结果表明，在继续预训练时，模型的整体性能会逐渐提高，即使在大量下游数据集上。此外，在不同的预训练点和最大学习率下，模型的性能也有显著的不同。<details>
<summary>Abstract</summary>
Large language models (LLMs) are routinely pre-trained on billions of tokens, only to restart the process over again once new data becomes available. A much cheaper and more efficient solution would be to enable the continual pre-training of these models, i.e. updating pre-trained models with new data instead of re-training them from scratch. However, the distribution shift induced by novel data typically results in degraded performance on past data. Taking a step towards efficient continual pre-training, in this work, we examine the effect of different warm-up strategies. Our hypothesis is that the learning rate must be re-increased to improve compute efficiency when training on a new dataset. We study the warmup phase of models pre-trained on the Pile (upstream data, 300B tokens) as we continue to pre-train on SlimPajama (downstream data, 297B tokens), following a linear warmup and cosine decay schedule. We conduct all experiments on the Pythia 410M language model architecture and evaluate performance through validation perplexity. We experiment with different pre-training checkpoints, various maximum learning rates, and various warmup lengths. Our results show that while rewarming models first increases the loss on upstream and downstream data, in the longer run it improves the downstream performance, outperforming models trained from scratch$\unicode{x2013}$even for a large downstream dataset.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）通常在数十亿个字符上进行预训练，然后又重新开始预训练。一种更经济高效的解决方案是让这些模型在新数据上进行连续预训练，而不是从scratch重新训练。然而，新数据引入的分布变化通常会导致过去数据的性能下降。为了实现效率的连续预训练，在这项工作中，我们研究了不同的温存策略。我们的假设是，在训练新数据集时，学习率必须重新增加以提高计算效率。我们研究在Pile（上游数据，300亿个字符）预训练后，在SlimPajama（下游数据，297亿个字符）上继续预训练，采用线性温存和cosine衰减时间表。我们在Pythia 410M语言模型架构上进行所有实验，并通过验证plexity来评估性能。我们对不同的预训练检查点、最大学习率和温存长度进行了尝试。我们的结果表明，虽然在重新暖化模型后，初期loss会增加在上游和下游数据上，但在长期来看，它会提高下游性能，超过从scratch训练的模型，即使是大型下游数据集。
</details></li>
</ul>
<hr>
<h2 id="Generalization-bound-for-estimating-causal-effects-from-observational-network-data"><a href="#Generalization-bound-for-estimating-causal-effects-from-observational-network-data" class="headerlink" title="Generalization bound for estimating causal effects from observational network data"></a>Generalization bound for estimating causal effects from observational network data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04011">http://arxiv.org/abs/2308.04011</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruichu Cai, Zeqin Yang, Weilin Chen, Yuguang Yan, Zhifeng Hao</li>
<li>for: 这篇论文是为了估计来自观察网络数据的 causal effect 的。</li>
<li>methods: 论文使用了重量学习和 representation learning 两种方法来估计 causal effect。</li>
<li>results: 实验研究表明，这种方法可以有效地估计 causal effect，并且可以提供一个理论上的支持来减少复杂的干扰偏见。<details>
<summary>Abstract</summary>
Estimating causal effects from observational network data is a significant but challenging problem. Existing works in causal inference for observational network data lack an analysis of the generalization bound, which can theoretically provide support for alleviating the complex confounding bias and practically guide the design of learning objectives in a principled manner. To fill this gap, we derive a generalization bound for causal effect estimation in network scenarios by exploiting 1) the reweighting schema based on joint propensity score and 2) the representation learning schema based on Integral Probability Metric (IPM). We provide two perspectives on the generalization bound in terms of reweighting and representation learning, respectively. Motivated by the analysis of the bound, we propose a weighting regression method based on the joint propensity score augmented with representation learning. Extensive experimental studies on two real-world networks with semi-synthetic data demonstrate the effectiveness of our algorithm.
</details>
<details>
<summary>摘要</summary>
估计来自观察网络数据的 causal effect 是一个重要 yet 挑战性的问题。现有的 causal inference 在网络数据上lacks 一个分析 generalization bound，可以 theoretically 提供支持来减少复杂的混杂偏见和实践 guide 学习目标的原则性。为了填这个 gap，我们 derivate 一个 generalization bound  для causal effect estimation 在网络场景下，通过 exploiting 1) 重量 schema based on joint propensity score 和 2) representation learning schema based on Integral Probability Metric (IPM)。我们提供 two perspectives on the generalization bound in terms of reweighting and representation learning，分别。受 bound 分析的激励，我们提议一种基于 joint propensity score 和 representation learning的重量回归方法。经验性研究在 two real-world networks 上的 semi-synthetic data 表明了我们的算法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Understanding-CNN-Hidden-Neuron-Activations-Using-Structured-Background-Knowledge-and-Deductive-Reasoning"><a href="#Understanding-CNN-Hidden-Neuron-Activations-Using-Structured-Background-Knowledge-and-Deductive-Reasoning" class="headerlink" title="Understanding CNN Hidden Neuron Activations Using Structured Background Knowledge and Deductive Reasoning"></a>Understanding CNN Hidden Neuron Activations Using Structured Background Knowledge and Deductive Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03999">http://arxiv.org/abs/2308.03999</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abhilekha-dalal/xai-using-wikidataAndEcii">https://github.com/abhilekha-dalal/xai-using-wikidataAndEcii</a></li>
<li>paper_authors: Abhilekha Dalal, Md Kamruzzaman Sarker, Adrita Barua, Eugene Vasserman, Pascal Hitzler</li>
<li>for: 本研究旨在解释深度学习系统中隐藏层神经元的活动，以提供深度学习系统内部探测输入的信息。</li>
<li>methods: 本研究使用了大规模背景知识（约200万个类）和基于描述逻辑的符号逻辑方法called Concept Induction，以自动附加隐藏层神经元的意义 labels。</li>
<li>results: 研究结果表明，我们可以通过一种假设和验证过程，自动将大规模背景知识中的意义labels附加到Convolutional Neural Network的 dense层神经元上。<details>
<summary>Abstract</summary>
A major challenge in Explainable AI is in correctly interpreting activations of hidden neurons: accurate interpretations would provide insights into the question of what a deep learning system has internally detected as relevant on the input, demystifying the otherwise black-box character of deep learning systems. The state of the art indicates that hidden node activations can, in some cases, be interpretable in a way that makes sense to humans, but systematic automated methods that would be able to hypothesize and verify interpretations of hidden neuron activations are underexplored. In this paper, we provide such a method and demonstrate that it provides meaningful interpretations. Our approach is based on using large-scale background knowledge approximately 2 million classes curated from the Wikipedia concept hierarchy together with a symbolic reasoning approach called Concept Induction based on description logics, originally developed for applications in the Semantic Web field. Our results show that we can automatically attach meaningful labels from the background knowledge to individual neurons in the dense layer of a Convolutional Neural Network through a hypothesis and verification process.
</details>
<details>
<summary>摘要</summary>
一 Major challenge in Explainable AI 是正确地理解隐藏节点的活动：正确的解释可以提供关于深度学习系统内部检测到的输入的信息，从而干预深度学习系统的黑盒特性。现状的技术是，隐藏节点的活动可以在某些情况下被解释得通用人类理解，但系统化的自动方法来测试和验证解释是未经探索的。在这篇论文中，我们提供了一种方法，并证明其可以提供有意义的解释。我们的方法基于使用大规模背景知识（约200万个类别，来自wikipedia知识树），并使用基于描述逻辑的符号推理方法 called Concept Induction，原始是为Semantic Web领域开发的。我们的结果表明，我们可以通过一个假设和验证过程，将background知识中的有意义标签自动地应用于 dense层中的神经元。
</details></li>
</ul>
<hr>
<h2 id="Cooperative-Multi-Type-Multi-Agent-Deep-Reinforcement-Learning-for-Resource-Management-in-Space-Air-Ground-Integrated-Networks"><a href="#Cooperative-Multi-Type-Multi-Agent-Deep-Reinforcement-Learning-for-Resource-Management-in-Space-Air-Ground-Integrated-Networks" class="headerlink" title="Cooperative Multi-Type Multi-Agent Deep Reinforcement Learning for Resource Management in Space-Air-Ground Integrated Networks"></a>Cooperative Multi-Type Multi-Agent Deep Reinforcement Learning for Resource Management in Space-Air-Ground Integrated Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03995">http://arxiv.org/abs/2308.03995</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hengxi Zhang, Huaze Tang, Wenbo Ding, Xiao-Ping Zhang</li>
<li>for: 提高智能城市应用的潜在价值和可行性</li>
<li>methods: 提出了一种涵盖五个不同通信链的完整SAGIN系统，并提出了一种高效的合作多类多代理人深度学习（CMT-MARL）方法来解决资源管理问题</li>
<li>results: 实验结果表明，提议的CMT-MARL方法能够有效地解决资源管理问题，并且可以提高总传输率和传输成功率等关键性能指标。<details>
<summary>Abstract</summary>
The Space-Air-Ground Integrated Network (SAGIN), integrating heterogeneous devices including low earth orbit (LEO) satellites, unmanned aerial vehicles (UAVs), and ground users (GUs), holds significant promise for advancing smart city applications. However, resource management of the SAGIN is a challenge requiring urgent study in that inappropriate resource management will cause poor data transmission, and hence affect the services in smart cities. In this paper, we develop a comprehensive SAGIN system that encompasses five distinct communication links and propose an efficient cooperative multi-type multi-agent deep reinforcement learning (CMT-MARL) method to address the resource management issue. The experimental results highlight the efficacy of the proposed CMT-MARL, as evidenced by key performance indicators such as the overall transmission rate and transmission success rate. These results underscore the potential value and feasibility of future implementation of the SAGIN.
</details>
<details>
<summary>摘要</summary>
SAGIN（空间-空气-地面集成网络），包括低地球轨道卫星（LEO）、无人飞行器（UAV）和地面用户（GU）多种设备，具有推进智能城市应用的潜力。然而，SAGIN资源管理却是一项需要优先研究的挑战，因为不当的资源管理会导致数据传输差，从而影响智能城市服务。本文提出了一个完整的SAGIN系统，包括五种不同的通信链，并提出了一种高效的合作多种多代理人深度学习（CMT-MARL）方法来解决资源管理问题。实验结果表明，提议的CMT-MARL方法能够有效地解决SAGIN资源管理问题，以示KEY表现指标（总传输率和传输成功率）。这些结果表明SAGIN的可能性和实现性。
</details></li>
</ul>
<hr>
<h2 id="Fourier-neural-operator-for-real-time-simulation-of-3D-dynamic-urban-microclimate"><a href="#Fourier-neural-operator-for-real-time-simulation-of-3D-dynamic-urban-microclimate" class="headerlink" title="Fourier neural operator for real-time simulation of 3D dynamic urban microclimate"></a>Fourier neural operator for real-time simulation of 3D dynamic urban microclimate</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03985">http://arxiv.org/abs/2308.03985</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenhui Peng, Shaoxiang Qin, Senwen Yang, Jianchun Wang, Xue Liu, Liangzhu, Wang</li>
<li>For: The paper aims to develop a real-time three-dimensional urban wind field simulation method using the Fourier Neural Operator (FNO) network to accelerate the modeling of complex non-linear interactions and system dynamics in urban microclimates.* Methods: The paper uses a combination of Computational Fluid Dynamics (CFD) simulation and the FNO network to model urban microclimates. The training and testing data are generated from CFD simulation of the urban area, based on the semi-Lagrangian approach and fractional stepping method.* Results: The paper shows that the FNO model can accurately reconstruct the instantaneous spatial velocity field and generalize well on different wind directions. The FNO approach can make predictions within milliseconds on the graphics processing unit, making real-time simulation of 3D dynamic urban microclimate possible.Here are the three points in Simplified Chinese:* For: 本研究旨在通过 Фурье神经网络（FNO）加速城市微气候模型化。* Methods: 本研究使用CFD计算和FNO网络模拟城市微气候。训练和测试数据来自CFD计算城市区域，基于半拉格朗日方法和分辨率步骤法。* Results: FNO模型可以准确重建三维城市风场速度场，并在不同风向下Generalize well。FNO方法可以在图形处理器上进行毫秒级准确预测，使城市微气候实时模拟变得可能。<details>
<summary>Abstract</summary>
Global urbanization has underscored the significance of urban microclimates for human comfort, health, and building/urban energy efficiency. They profoundly influence building design and urban planning as major environmental impacts. Understanding local microclimates is essential for cities to prepare for climate change and effectively implement resilience measures. However, analyzing urban microclimates requires considering a complex array of outdoor parameters within computational domains at the city scale over a longer period than indoors. As a result, numerical methods like Computational Fluid Dynamics (CFD) become computationally expensive when evaluating the impact of urban microclimates. The rise of deep learning techniques has opened new opportunities for accelerating the modeling of complex non-linear interactions and system dynamics. Recently, the Fourier Neural Operator (FNO) has been shown to be very promising in accelerating solving the Partial Differential Equations (PDEs) and modeling fluid dynamic systems. In this work, we apply the FNO network for real-time three-dimensional (3D) urban wind field simulation. The training and testing data are generated from CFD simulation of the urban area, based on the semi-Lagrangian approach and fractional stepping method to simulate urban microclimate features for modeling large-scale urban problems. Numerical experiments show that the FNO model can accurately reconstruct the instantaneous spatial velocity field. We further evaluate the trained FNO model on unseen data with different wind directions, and the results show that the FNO model can generalize well on different wind directions. More importantly, the FNO approach can make predictions within milliseconds on the graphics processing unit, making real-time simulation of 3D dynamic urban microclimate possible.
</details>
<details>
<summary>摘要</summary>
Recently, deep learning techniques have been applied to accelerate the modeling of complex non-linear interactions and system dynamics. One promising approach is the Fourier Neural Operator (FNO), which can accelerate the solution of Partial Differential Equations (PDEs) and model fluid dynamic systems.In this study, we use the FNO network for real-time three-dimensional (3D) urban wind field simulation. The training and testing data are generated from CFD simulations of the urban area, using the semi-Lagrangian approach and fractional stepping method to simulate urban microclimate features for modeling large-scale urban problems. Our numerical experiments show that the FNO model can accurately reconstruct the instantaneous spatial velocity field. We also evaluate the trained FNO model on unseen data with different wind directions, and the results show that the model can generalize well on different wind directions.More importantly, the FNO approach can make predictions within milliseconds on a graphics processing unit, making real-time simulation of 3D dynamic urban microclimate possible. This has significant implications for urban planning and design, as well as for the development of more energy-efficient and resilient cities.
</details></li>
</ul>
<hr>
<h2 id="Characterization-of-Human-Balance-through-a-Reinforcement-Learning-based-Muscle-Controller"><a href="#Characterization-of-Human-Balance-through-a-Reinforcement-Learning-based-Muscle-Controller" class="headerlink" title="Characterization of Human Balance through a Reinforcement Learning-based Muscle Controller"></a>Characterization of Human Balance through a Reinforcement Learning-based Muscle Controller</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04462">http://arxiv.org/abs/2308.04462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kübra Akbaş, Carlotta Mummolo, Xianlian Zhou</li>
<li>for: This paper aims to explore the use of center of mass (COM) state space and reinforcement learning (RL) to monitor balance capabilities in humans, and to establish balance recovery limits.</li>
<li>methods: The paper employs a musculoskeletal model integrated with a balance controller, trained through RL, to investigate balancing capabilities. The RL framework includes two interconnected neural networks governing balance recovery and muscle coordination, trained using Proximal Policy Optimization (PPO) with reference state initialization, early termination, and multiple training strategies.</li>
<li>results: The paper obtains final balance recovery (BR) enclosing successful balance recovery trajectories by exploring recovery from random initial COM states (position and velocity) space for a trained controller. The BRs are compared with analytical postural stability limits from a linear inverted pendulum model, and the results show a similar trend in successful COM states but more limited ranges in the recoverable areas. The paper also investigates the effect of muscle weakness and neural excitation delay on the BRs, revealing reduced balancing capability in different regions.<details>
<summary>Abstract</summary>
Balance assessment during physical rehabilitation often relies on rubric-oriented battery tests to score a patient's physical capabilities, leading to subjectivity. While some objective balance assessments exist, they are often limited to tracking the center of pressure (COP), which does not fully capture the whole-body postural stability. This study explores the use of the center of mass (COM) state space and presents a promising avenue for monitoring the balance capabilities in humans. We employ a musculoskeletal model integrated with a balance controller, trained through reinforcement learning (RL), to investigate balancing capabilities. The RL framework consists of two interconnected neural networks governing balance recovery and muscle coordination respectively, trained using Proximal Policy Optimization (PPO) with reference state initialization, early termination, and multiple training strategies. By exploring recovery from random initial COM states (position and velocity) space for a trained controller, we obtain the final BR enclosing successful balance recovery trajectories. Comparing the BRs with analytical postural stability limits from a linear inverted pendulum model, we observe a similar trend in successful COM states but more limited ranges in the recoverable areas. We further investigate the effect of muscle weakness and neural excitation delay on the BRs, revealing reduced balancing capability in different regions. Overall, our approach of learning muscular balance controllers presents a promising new method for establishing balance recovery limits and objectively assessing balance capability in bipedal systems, particularly in humans.
</details>
<details>
<summary>摘要</summary>
评估身体重建中的平衡能力经常采用套路-oriented测试维度来评估病人的身体能力，带来主观性。尽管有一些 объектив的平衡评估存在，但它们通常只能跟踪中心重量（COP），不能完全捕捉人体整体姿态稳定性。本研究探讨了使用中心质量（COM）状态空间来监测人体平衡能力。我们采用了一种musculoskeletal模型和平衡控制器，通过反射学习（RL）训练，investigate balancing capabilities。RL框架包括两个相连的神经网络，一个 governing balance recovery，另一个 governing muscle coordination，通过距离最小化算法（PPO）进行训练。通过探索已经训练好的控制器从随机初始COM状态空间中恢复平衡的过程，我们获得了最终的BR（balance recovery）。 Comparing the BRs with analytical postural stability limits from a linear inverted pendulum model, we observe a similar trend in successful COM states but more limited ranges in the recoverable areas。我们进一步调查了肌肉衰竭和神经刺激延迟对BR的影响，发现在不同区域的平衡能力受到了限制。总的来说，我们的学习muscular平衡控制器的方法可能是评估人体平衡能力的新方法，特别是在人类身上。
</details></li>
</ul>
<hr>
<h2 id="PUG-Photorealistic-and-Semantically-Controllable-Synthetic-Data-for-Representation-Learning"><a href="#PUG-Photorealistic-and-Semantically-Controllable-Synthetic-Data-for-Representation-Learning" class="headerlink" title="PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning"></a>PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03977">http://arxiv.org/abs/2308.03977</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/pug">https://github.com/facebookresearch/pug</a></li>
<li>paper_authors: Florian Bordes, Shashank Shekhar, Mark Ibrahim, Diane Bouchacourt, Pascal Vincent, Ari S. Morcos</li>
<li>for: 这篇论文旨在推广使用真实图像数据的替代方案，提供更多的控制权和更加真实的图像数据，以便更好地训练和评估深度神经网络。</li>
<li>methods: 这篇论文使用了Unreal Engine游戏引擎，生成了PUG（真实图像格式）环境和数据集，以便进行表示学习研究。</li>
<li>results: 论文通过PUG环境和数据集，实现了更加准确和可靠的视觉模型评估，提供了一种更加可控和真实的替代方案。<details>
<summary>Abstract</summary>
Synthetic image datasets offer unmatched advantages for designing and evaluating deep neural networks: they make it possible to (i) render as many data samples as needed, (ii) precisely control each scene and yield granular ground truth labels (and captions), (iii) precisely control distribution shifts between training and testing to isolate variables of interest for sound experimentation. Despite such promise, the use of synthetic image data is still limited -- and often played down -- mainly due to their lack of realism. Most works therefore rely on datasets of real images, which have often been scraped from public images on the internet, and may have issues with regards to privacy, bias, and copyright, while offering little control over how objects precisely appear. In this work, we present a path to democratize the use of photorealistic synthetic data: we develop a new generation of interactive environments for representation learning research, that offer both controllability and realism. We use the Unreal Engine, a powerful game engine well known in the entertainment industry, to produce PUG (Photorealistic Unreal Graphics) environments and datasets for representation learning. In this paper, we demonstrate the potential of PUG to enable more rigorous evaluations of vision models.
</details>
<details>
<summary>摘要</summary>
<<SYS>>通过实验室自动生成的图像集，深度神经网络的设计和评估受到了无与伦比的优势：可以（i）生成无数样本，（ii）精准控制每个场景，并提供细腻的标签和描述，（iii）在训练和测试之间准确控制分布变化，以孤立变量对照。尽管如此，使用synthetic图像数据仍然受到限制——常被淡化——主要因为它们缺乏真实感。大多数作品因此选择使用实际图像数据，这些数据通常是从互联网上抓取的，可能存在隐私、偏见和版权问题，而且对物体的显示没有准确控制。在这篇论文中，我们提出了一种将高真实度的synthetic数据普及化的方法：我们开发了一代新的交互环境，以Unreal Engine游戏引擎为基础，生成PUG（高真实度Unreal图形）环境和数据集，用于 representation learning研究。我们在这篇论文中展示了PUG的潜在力量，帮助vision模型的更加严格的评估。
</details></li>
</ul>
<hr>
<h2 id="Amortized-Global-Search-for-Efficient-Preliminary-Trajectory-Design-with-Deep-Generative-Models"><a href="#Amortized-Global-Search-for-Efficient-Preliminary-Trajectory-Design-with-Deep-Generative-Models" class="headerlink" title="Amortized Global Search for Efficient Preliminary Trajectory Design with Deep Generative Models"></a>Amortized Global Search for Efficient Preliminary Trajectory Design with Deep Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03960">http://arxiv.org/abs/2308.03960</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anjian Li, Amlan Sinha, Ryne Beeson</li>
<li>for: 提出了一种用于减轻全球搜索问题的 computational complexity的方法，以便更好地解决高维度和非对称的 trajectory optimization problem。</li>
<li>methods: 我们使用深度生成模型来预测 trajectory 的解，并且利用 clustering 结构来加速全球搜索。</li>
<li>results: 我们在 De Jong 的 5 个函数和一个低推力圆形三体问题中进行了评估，并得到了良好的结果。<details>
<summary>Abstract</summary>
Preliminary trajectory design is a global search problem that seeks multiple qualitatively different solutions to a trajectory optimization problem. Due to its high dimensionality and non-convexity, and the frequent adjustment of problem parameters, the global search becomes computationally demanding. In this paper, we exploit the clustering structure in the solutions and propose an amortized global search (AmorGS) framework. We use deep generative models to predict trajectory solutions that share similar structures with previously solved problems, which accelerates the global search for unseen parameter values. Our method is evaluated using De Jong's 5th function and a low-thrust circular restricted three-body problem.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本到简化中文。<</SYS>>预liminary trajectory design是一个全球搜索问题，旨在找到多个 качеitative不同的解决方案。由于其高维度和非拟合性，以及常见的问题参数调整，全球搜索变得计算极其困难。在这篇论文中，我们利用解决方案中的凝集结构，并提出了一种含有各种凝集的全球搜索（AmorGS）框架。我们使用深度生成模型预测 trajectory解决方案，这些解决方案与之前已解决的问题中的结构相似，从而加速了未before seen的参数值上的全球搜索。我们的方法被评估使用De Jong的第五个函数和一个低推力圆形 restricted three-body problem。
</details></li>
</ul>
<hr>
<h2 id="Fixed-Inter-Neuron-Covariability-Induces-Adversarial-Robustness"><a href="#Fixed-Inter-Neuron-Covariability-Induces-Adversarial-Robustness" class="headerlink" title="Fixed Inter-Neuron Covariability Induces Adversarial Robustness"></a>Fixed Inter-Neuron Covariability Induces Adversarial Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03956">http://arxiv.org/abs/2308.03956</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Ahmed Shah, Bhiksha Raj</li>
<li>for: 这个论文旨在提高深度神经网络（DNNs）对抗攻击的可靠性，并探索人类视觉的特性可能会帮助提高DNNs的类别化能力。</li>
<li>methods: 本论文提出了一个叫做自适应活化（SCA）层的新方法，这个层包含neuron的启动是彼此一致的，它们遵循一个已知但是学习的 covariability 模式。</li>
<li>results: 在实验中，使用 SCA 层的模型在图像和声音识别任务中实现了高准确率，并在Auto-PGD攻击中展现了明显更高的Robustness，不需要在训练过程中使用随机噪声训练。<details>
<summary>Abstract</summary>
The vulnerability to adversarial perturbations is a major flaw of Deep Neural Networks (DNNs) that raises question about their reliability when in real-world scenarios. On the other hand, human perception, which DNNs are supposed to emulate, is highly robust to such perturbations, indicating that there may be certain features of the human perception that make it robust but are not represented in the current class of DNNs. One such feature is that the activity of biological neurons is correlated and the structure of this correlation tends to be rather rigid over long spans of times, even if it hampers performance and learning. We hypothesize that integrating such constraints on the activations of a DNN would improve its adversarial robustness, and, to test this hypothesis, we have developed the Self-Consistent Activation (SCA) layer, which comprises of neurons whose activations are consistent with each other, as they conform to a fixed, but learned, covariability pattern. When evaluated on image and sound recognition tasks, the models with a SCA layer achieved high accuracy, and exhibited significantly greater robustness than multi-layer perceptron models to state-of-the-art Auto-PGD adversarial attacks \textit{without being trained on adversarially perturbed data
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="PMU-measurements-based-short-term-voltage-stability-assessment-of-power-systems-via-deep-transfer-learning"><a href="#PMU-measurements-based-short-term-voltage-stability-assessment-of-power-systems-via-deep-transfer-learning" class="headerlink" title="PMU measurements based short-term voltage stability assessment of power systems via deep transfer learning"></a>PMU measurements based short-term voltage stability assessment of power systems via deep transfer learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03953">http://arxiv.org/abs/2308.03953</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/SuperBruceJia/Power-Systems-Stability-Transfer-Learning">https://github.com/SuperBruceJia/Power-Systems-Stability-Transfer-Learning</a></li>
<li>paper_authors: Yang Li, Shitu Zhang, Yuanzheng Li, Jiting Cao, Shuyue Jia</li>
<li>for: 这篇论文的目的是提出一种基于深度学习的短期电压稳定评估方法（STVSA），以解决现有的挑战，包括适应结构变化、样本标注和小型数据集处理。</li>
<li>methods: 本文提出的方法使用了深度转移学习，利用PMU测量数据创建初始数据集，采用时间ensemble进行样本标注，并使用最小二乘生成整数网络（LSGAN）进行数据增强。该方法可以有效地在小型数据集上进行深度学习，并且具有适应结构变化的能力。</li>
<li>results: 实验结果表明，提出的方法可以在IEEE 39-bus测试系统上提高模型评估精度约20%，并且具有强大的适应能力于结构变化。该方法还利用 transformer 模型中的自注意机制，与浅学习方法和其他深度学习基于方法相比，具有显著的优势。<details>
<summary>Abstract</summary>
Deep learning has emerged as an effective solution for addressing the challenges of short-term voltage stability assessment (STVSA) in power systems. However, existing deep learning-based STVSA approaches face limitations in adapting to topological changes, sample labeling, and handling small datasets. To overcome these challenges, this paper proposes a novel phasor measurement unit (PMU) measurements-based STVSA method by using deep transfer learning. The method leverages the real-time dynamic information captured by PMUs to create an initial dataset. It employs temporal ensembling for sample labeling and utilizes least squares generative adversarial networks (LSGAN) for data augmentation, enabling effective deep learning on small-scale datasets. Additionally, the method enhances adaptability to topological changes by exploring connections between different faults. Experimental results on the IEEE 39-bus test system demonstrate that the proposed method improves model evaluation accuracy by approximately 20% through transfer learning, exhibiting strong adaptability to topological changes. Leveraging the self-attention mechanism of the Transformer model, this approach offers significant advantages over shallow learning methods and other deep learning-based approaches.
</details>
<details>
<summary>摘要</summary>
深度学习已经成为电力系统短期电压稳定评估 (STVSA) 的有效解决方案。然而，现有的深度学习基于 STVSA 方法受到 топологи奇变、样本标注和处理小数据集的限制。为了缓解这些挑战，这篇论文提议一种基于 PMU 测量的新型 STVSA 方法，使用深度转移学习。该方法利用 PMU 测量获得的实时动态信息，创建初始数据集。它使用时间ensemble 进行样本标注，并使用最小二乘生成整形网络 (LSGAN) 进行数据增强，以便在小规模数据集上进行有效的深度学习。此外，该方法改进了对 topology 变化的适应性，通过探索不同的故障之间的连接。实验结果在 IEEE 39-bus 测试系统上表明，提案的方法可以通过转移学习提高评估准确率约 20%，并且具有强大的适应性。利用 Transformer 模型的自注意机制，该方法在比较深度学习方法和其他深度学习基于方法之上具有显著优势。
</details></li>
</ul>
<hr>
<h2 id="The-Prospect-of-Enhancing-Large-Scale-Heterogeneous-Federated-Learning-with-Transformers"><a href="#The-Prospect-of-Enhancing-Large-Scale-Heterogeneous-Federated-Learning-with-Transformers" class="headerlink" title="The Prospect of Enhancing Large-Scale Heterogeneous Federated Learning with Transformers"></a>The Prospect of Enhancing Large-Scale Heterogeneous Federated Learning with Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03945">http://arxiv.org/abs/2308.03945</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yulan Gao, Zhaoxiang Hou, Chengyi Yang, Zengxiang Li, Han Yu</li>
<li>for: 本文探讨了基于Transformer的 Federated Learning（FL）模型在多个不同数据所有者之间协同训练AI模型的可能性，以实现泛化和个性化。</li>
<li>methods: 本文使用了Transformer、ResNet和个性化ResNet-based FL方法进行比较性实验，以评估这些方法在不同数据所有者和不同场景下的性能。</li>
<li>results: 实验结果显示，Transformer-based FL模型在大规模不同数据所有者的场景下表现出色，特别是在数据多样性和数据规模增加的情况下。此外，通过对CKA表示相似性进行分析，本文还提供了对Transformers的表现的深入理解。<details>
<summary>Abstract</summary>
Federated learning (FL) addresses data privacy concerns by enabling collaborative training of AI models across distributed data owners. Wide adoption of FL faces the fundamental challenges of data heterogeneity and the large scale of data owners involved. In this paper, we investigate the prospect of Transformer-based FL models for achieving generalization and personalization in this setting. We conduct extensive comparative experiments involving FL with Transformers, ResNet, and personalized ResNet-based FL approaches under various scenarios. These experiments consider varying numbers of data owners to demonstrate Transformers' advantages over deep neural networks in large-scale heterogeneous FL tasks. In addition, we analyze the superior performance of Transformers by comparing the Centered Kernel Alignment (CKA) representation similarity across different layers and FL models to gain insight into the reasons behind their promising capabilities.
</details>
<details>
<summary>摘要</summary>
合作学习（FL）解决数据隐私问题，通过在分布式数据所有者之间进行AI模型的共同训练。广泛采用FL面临了数据多样性和数据所有者的大规模挑战。在这篇论文中，我们调查了使用Transformer-based FL模型来实现通用和个性化。我们进行了广泛的比较实验，包括FL与Transformers、ResNet和个性化ResNet-based FL方法在不同情况下。这些实验涵盖了不同数据所有者的数量，以 demonstarteTransformers在大规模不同数据类型FL任务中的优势。此外，我们还分析了Transformers的高性能原因，通过比较不同层次的CKA表示相似性来获得关键因素的含义。
</details></li>
</ul>
<hr>
<h2 id="GraPhSyM-Graph-Physical-Synthesis-Model"><a href="#GraPhSyM-Graph-Physical-Synthesis-Model" class="headerlink" title="GraPhSyM: Graph Physical Synthesis Model"></a>GraPhSyM: Graph Physical Synthesis Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03944">http://arxiv.org/abs/2308.03944</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Agiza, Rajarshi Roy, Teodor Dumitru Ene, Saad Godil, Sherief Reda, Bryan Catanzaro</li>
<li>for: 这个研究旨在开发一个快速和精准地预测实体合成电路延迟和面积度量的Graph Attention Network（GATv2）模型，以便在逻辑合成阶段就能够获得最终设计度量的准确见解，而不需要进行慢速实体合成流程。</li>
<li>methods: 这个模型使用Graph Structure、连接性和电气性特征来预测实体合成变数的影响，并且通过训练在6000个前置逻辑合成设计中，以0.22秒的快速推断时间预测未见的逻辑合成设计的实体合成延迟和面积度量。</li>
<li>results: 研究发现，这个模型可以高度精准地预测未见的逻辑合成设计的实体合成延迟（98.3%）和面积度量（96.1%），并且可以在不同的延迟目标下进行预测。此外，模型还可以在不同的逻辑合成设计中实现高度的构成性。<details>
<summary>Abstract</summary>
In this work, we introduce GraPhSyM, a Graph Attention Network (GATv2) model for fast and accurate estimation of post-physical synthesis circuit delay and area metrics from pre-physical synthesis circuit netlists. Once trained, GraPhSyM provides accurate visibility of final design metrics to early EDA stages, such as logic synthesis, without running the slow physical synthesis flow, enabling global co-optimization across stages. Additionally, the swift and precise feedback provided by GraPhSym is instrumental for machine-learning-based EDA optimization frameworks. Given a gate-level netlist of a circuit represented as a graph, GraPhSyM utilizes graph structure, connectivity, and electrical property features to predict the impact of physical synthesis transformations such as buffer insertion and gate sizing. When trained on a dataset of 6000 prefix adder designs synthesized at an aggressive delay target, GraPhSyM can accurately predict the post-synthesis delay (98.3%) and area (96.1%) metrics of unseen adders with a fast 0.22s inference time. Furthermore, we illustrate the compositionality of GraPhSyM by employing the model trained on a fixed delay target to accurately anticipate post-synthesis metrics at a variety of unseen delay targets. Lastly, we report promising generalization capabilities of the GraPhSyM model when it is evaluated on circuits different from the adders it was exclusively trained on. The results show the potential for GraPhSyM to serve as a powerful tool for advanced optimization techniques and as an oracle for EDA machine learning frameworks.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们介绍了GraPhSyM模型，是基于图注意力网络（GATv2）的一种快速和准确地计算后physical synthesis circuit延迟和面积指标的方法。一旦训练完成，GraPhSyM可以在逻辑合成之前提供准确的设计指标视图，无需运行慢的物理合成流程，从而实现全局协调。此外，GraPhSyM提供的快速和准确反馈对机器学习基于EDA优化框架非常有利。对于一个表示为图的逻辑电路，GraPhSyM利用图结构、连接和电性特征来预测物理合成转换（如缓冲插入和门大小调整）的影响。当训练在6000个逻辑和逻辑电路的延迟目标下进行的时候，GraPhSyM可以准确预测未看过的加器的延迟（98.3%）和面积（96.1%）指标，并且具有快速的0.22秒推理时间。此外，我们还证明了GraPhSyM的可组合性，可以使用固定延迟目标训练的模型来准确预测未看过的延迟目标。最后，我们报告了GraPhSyM模型在不同于它被专门训练的加器之外的普遍化能力。结果表明，GraPhSyM有望成为一种强大的进阶优化技术工具和EDA机器学习框架的oracle。
</details></li>
</ul>
<hr>
<h2 id="The-Compatibility-between-the-Pangu-Weather-Forecasting-Model-and-Meteorological-Operational-Data"><a href="#The-Compatibility-between-the-Pangu-Weather-Forecasting-Model-and-Meteorological-Operational-Data" class="headerlink" title="The Compatibility between the Pangu Weather Forecasting Model and Meteorological Operational Data"></a>The Compatibility between the Pangu Weather Forecasting Model and Meteorological Operational Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04460">http://arxiv.org/abs/2308.04460</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wencong Cheng, Yan Yan, Jiangjiang Xia, Qi Liu, Chang Qu, Zhigang Wang</li>
<li>for: 本研究旨在评估Pangu-Weather模型与各种常用的 numerical weather prediction（NWP）操作分析的兼容性，以及改进模型的预测性能。</li>
<li>methods: 本研究使用Pangu-Weather模型进行预测，并对各种NWP操作分析进行对比研究。</li>
<li>results: 研究结果显示，Pangu-Weather模型与各种NWP操作分析兼容，并且可以改进预测性能。此外，提高全球或地方初始条件质量能够显著提高Pangu-Weather模型的预测性能。<details>
<summary>Abstract</summary>
Recently, multiple data-driven models based on machine learning for weather forecasting have emerged. These models are highly competitive in terms of accuracy compared to traditional numerical weather prediction (NWP) systems. In particular, the Pangu-Weather model, which is open source for non-commercial use, has been validated for its forecasting performance by the European Centre for Medium-Range Weather Forecasts (ECMWF) and has recently been published in the journal "Nature". In this paper, we evaluate the compatibility of the Pangu-Weather model with several commonly used NWP operational analyses through case studies. The results indicate that the Pangu-Weather model is compatible with different operational analyses from various NWP systems as the model initial conditions, and it exhibits a relatively stable forecasting capability. Furthermore, we have verified that improving the quality of global or local initial conditions significantly contributes to enhancing the forecasting performance of the Pangu-Weather model.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:最近，基于机器学习的多种数据驱动模型为气象预报出现了，这些模型与传统的数值气象预测（NWP）系统相比，具有高度竞争的准确性。其中，开源非商业用途的Pangu-Weather模型，已经由欧洲中期气象预测中心（ECMWF）验证了预测性能，并最近在《自然》杂志上发表。在这篇论文中，我们通过 caso studies 评估了Pangu-Weather模型与多种常用的NWP操作分析相容性。结果显示，Pangu-Weather模型可以与不同的NWP系统的操作分析进行Compatible，并且显示出相对稳定的预测能力。此外，我们还证明了改善全球或地方初始条件质量能够明显提高Pangu-Weather模型的预测性能。
</details></li>
</ul>
<hr>
<h2 id="Optimizing-the-switching-operation-in-monoclonal-antibody-production-Economic-MPC-and-reinforcement-learning"><a href="#Optimizing-the-switching-operation-in-monoclonal-antibody-production-Economic-MPC-and-reinforcement-learning" class="headerlink" title="Optimizing the switching operation in monoclonal antibody production: Economic MPC and reinforcement learning"></a>Optimizing the switching operation in monoclonal antibody production: Economic MPC and reinforcement learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03928">http://arxiv.org/abs/2308.03928</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sandra A. Obiri, Song Bo, Bernard T. Agyeman, Benjamin Decardi-Nelson, Jinfeng Liu</li>
<li>for: 这篇论文主要针对的是大规模生产益血抗体（mAb）的实际问题，以及如何通过 kontinuierliche 生产过程来提高产品质量和生产效率。</li>
<li>methods: 这篇论文提出了三种计算机fficient的控制方法，包括sigmoid函数近似方法、ReLU近似方法和深度强化学习（DRL）。这三种方法都是为了解决批处理操作中的数学难题。</li>
<li>results: 论文的实验结果表明，使用sigmoid函数近似方法和ReLU近似方法可以提高吞吐量和生产效率，而且比传统的1%产品剩余规则更为灵活和有效。<details>
<summary>Abstract</summary>
Monoclonal antibodies (mAbs) have emerged as indispensable assets in medicine, and are currently at the forefront of biopharmaceutical product development. However, the growing market demand and the substantial doses required for mAb clinical treatments necessitate significant progress in its large-scale production. Most of the processes for industrial mAb production rely on batch operations, which result in significant downtime. The shift towards a fully continuous and integrated manufacturing process holds the potential to boost product yield and quality, while eliminating the extra expenses associated with storing intermediate products. The integrated continuous mAb production process can be divided into the upstream and downstream processes. One crucial aspect that ensures the continuity of the integrated process is the switching of the capture columns, which are typically chromatography columns operated in a fed-batch manner downstream. Due to the discrete nature of the switching operation, advanced process control algorithms such as economic MPC (EMPC) are computationally difficult to implement. This is because an integer nonlinear program (INLP) needs to be solved online at each sampling time. This paper introduces two computationally-efficient approaches for EMPC implementation, namely, a sigmoid function approximation approach and a rectified linear unit (ReLU) approximation approach. It also explores the application of deep reinforcement learning (DRL). These three methods are compared to the traditional switching approach which is based on a 1% product breakthrough rule and which involves no optimization.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Spellburst-A-Node-based-Interface-for-Exploratory-Creative-Coding-with-Natural-Language-Prompts"><a href="#Spellburst-A-Node-based-Interface-for-Exploratory-Creative-Coding-with-Natural-Language-Prompts" class="headerlink" title="Spellburst: A Node-based Interface for Exploratory Creative Coding with Natural Language Prompts"></a>Spellburst: A Node-based Interface for Exploratory Creative Coding with Natural Language Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03921">http://arxiv.org/abs/2308.03921</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tyler Angert, Miroslav Ivan Suzara, Jenny Han, Christopher Lawrence Pondoc, Hariharan Subramonyam</li>
<li>for: 该论文旨在提高创作编程的效率和效iveness，帮助艺术家更快速地实现他们的想法。</li>
<li>methods: 该论文使用大语言模型（LLM）来提供一个具有节点基础的创作编程环境，并通过表达式提示来帮助艺术家在 semantic 空间中进行编程。</li>
<li>results: 论文的评估表明，Spellburst 可以帮助艺术家更快速地实现他们的想法，并且可以帮助开发计算机创造力工具，以便在 semantic 和 sintactic 空间之间进行桥接。<details>
<summary>Abstract</summary>
Creative coding tasks are often exploratory in nature. When producing digital artwork, artists usually begin with a high-level semantic construct such as a "stained glass filter" and programmatically implement it by varying code parameters such as shape, color, lines, and opacity to produce visually appealing results. Based on interviews with artists, it can be effortful to translate semantic constructs to program syntax, and current programming tools don't lend well to rapid creative exploration. To address these challenges, we introduce Spellburst, a large language model (LLM) powered creative-coding environment. Spellburst provides (1) a node-based interface that allows artists to create generative art and explore variations through branching and merging operations, (2) expressive prompt-based interactions to engage in semantic programming, and (3) dynamic prompt-driven interfaces and direct code editing to seamlessly switch between semantic and syntactic exploration. Our evaluation with artists demonstrates Spellburst's potential to enhance creative coding practices and inform the design of computational creativity tools that bridge semantic and syntactic spaces.
</details>
<details>
<summary>摘要</summary>
创造性编程任务经常具有探索性质。当生成数字艺术作品时，艺术家通常从高水平semantic construct开始，如“普遍玻璃过滤器”，然后通过代码参数的变化，如形状、颜色、线条和透明度，来生成可观的结果。根据艺术家的采访，将semantic construct翻译到程序语法可能会困难，现有的编程工具也不太适合快速的创作探索。为解决这些挑战，我们介绍Spellburst，一个基于大语言模型（LLM）的创造编程环境。Spellburst提供以下功能：1. 节点基本接口，让艺术家通过分支和合并操作来生成生成艺术和探索不同的变化。2. 表达式基于的提示式交互，让艺术家通过提示来参与semantic programming。3. dinamic提示驱动的界面和直接代码编辑，让艺术家轻松地在semantic和语法空间之间切换。我们的评估表明，Spellburst可以增强创造编程做法，并为计算创造工具的设计提供指导。
</details></li>
</ul>
<hr>
<h2 id="Predicting-and-explaining-nonlinear-material-response-using-deep-Physically-Guided-Neural-Networks-with-Internal-Variables"><a href="#Predicting-and-explaining-nonlinear-material-response-using-deep-Physically-Guided-Neural-Networks-with-Internal-Variables" class="headerlink" title="Predicting and explaining nonlinear material response using deep Physically Guided Neural Networks with Internal Variables"></a>Predicting and explaining nonlinear material response using deep Physically Guided Neural Networks with Internal Variables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03915">http://arxiv.org/abs/2308.03915</a></li>
<li>repo_url: None</li>
<li>paper_authors: Javier Orera-Echeverria, Jacobo Ayensa-Jiménez, Manuel Doblare</li>
<li>for: 这项研究的目的是用Physically Guided Neural Networks with Internal Variables (PGNNIV)方法揭示材料的 constitutive law，并能够预测未经见过的载荷场景下的内部和外部变量。</li>
<li>methods: 这项研究使用了新发展的PGNNIV方法，该方法通过使用物理问题的Physics-Informed Constraints (PIC)来约束特定的隐藏层，并且只通过测量力-压缩数据进行训练。</li>
<li>results: 研究发现PGNNIV方法能够预测不同材料的内部和外部变量，并且可以解释材料的 constitutive law，这种方法被称为Explainable Artificial Intelligence (XAI)。<details>
<summary>Abstract</summary>
Nonlinear materials are often difficult to model with classical state model theory because they have a complex and sometimes inaccurate physical and mathematical description or we simply do not know how to describe such materials in terms of relations between external and internal variables. In many disciplines, Neural Network methods have arisen as powerful tools to identify very complex and non-linear correlations. In this work, we use the very recently developed concept of Physically Guided Neural Networks with Internal Variables (PGNNIV) to discover constitutive laws using a model-free approach and training solely with measured force-displacement data. PGNNIVs make a particular use of the physics of the problem to enforce constraints on specific hidden layers and are able to make predictions without internal variable data. We demonstrate that PGNNIVs are capable of predicting both internal and external variables under unseen load scenarios, regardless of the nature of the material considered (linear, with hardening or softening behavior and hyperelastic), unravelling the constitutive law of the material hence explaining its nature altogether, placing the method in what is known as eXplainable Artificial Intelligence (XAI).
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="ViLP-Knowledge-Exploration-using-Vision-Language-and-Pose-Embeddings-for-Video-Action-Recognition"><a href="#ViLP-Knowledge-Exploration-using-Vision-Language-and-Pose-Embeddings-for-Video-Action-Recognition" class="headerlink" title="ViLP: Knowledge Exploration using Vision, Language, and Pose Embeddings for Video Action Recognition"></a>ViLP: Knowledge Exploration using Vision, Language, and Pose Embeddings for Video Action Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03908">http://arxiv.org/abs/2308.03908</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soumyabrata Chaudhuri, Saumik Bhattacharya</li>
<li>for: 本研究旨在提出一种基于多Modal学习的人体动作识别方法，以解决视频动作识别 task 中的复杂性问题。</li>
<li>methods: 本研究使用了一种新的 pose 增强的视力语言模型 (VLM)，其combines  pose 和视觉信息，以及文本特征。</li>
<li>results:  experiments 表明，该方法可以在两个人体动作识别数据集 UCF-101 和 HMDB-51 上达到 92.81% 和 73.02% 的准确率，而无需任何视频数据预训练。经ketics预训练后，准确率可以达到 96.11% 和 75.75%。<details>
<summary>Abstract</summary>
Video Action Recognition (VAR) is a challenging task due to its inherent complexities. Though different approaches have been explored in the literature, designing a unified framework to recognize a large number of human actions is still a challenging problem. Recently, Multi-Modal Learning (MML) has demonstrated promising results in this domain. In literature, 2D skeleton or pose modality has often been used for this task, either independently or in conjunction with the visual information (RGB modality) present in videos. However, the combination of pose, visual information, and text attributes has not been explored yet, though text and pose attributes independently have been proven to be effective in numerous computer vision tasks. In this paper, we present the first pose augmented Vision-language model (VLM) for VAR. Notably, our scheme achieves an accuracy of 92.81% and 73.02% on two popular human video action recognition benchmark datasets, UCF-101 and HMDB-51, respectively, even without any video data pre-training, and an accuracy of 96.11% and 75.75% after kinetics pre-training.
</details>
<details>
<summary>摘要</summary>
视频动作识别（VAR）是一个复杂的任务，它的内在复杂性使得设计一个综合性的框架来识别大量人类动作变得非常困难。然而，在文献中，不同的方法已经被探讨过，但是设计一个综合性的框架仍然是一个挑战。在文献中，2D骨骼或pose特征 oftentimes 用于这个任务，可以独立或与视觉信息（RGB特征）一起使用。然而，对于pose、视觉信息和文本特征的组合尚未被探讨，尽管文本和pose特征独立地已经证明了其效果在许多计算机视觉任务中。在这篇论文中，我们提出了首个含有 pose 的视力语言模型（VLM），该模型在 UCF-101 和 HMDB-51 两个常用的人类动作识别 benchmark 数据集上达到了 92.81% 和 73.02% 的准确率，而无需任何视频数据预训练，并且在预训练后达到了 96.11% 和 75.75% 的准确率。
</details></li>
</ul>
<hr>
<h2 id="Advancements-In-Crowd-Monitoring-System-A-Comprehensive-Analysis-of-Systematic-Approaches-and-Automation-Algorithms-State-of-The-Art"><a href="#Advancements-In-Crowd-Monitoring-System-A-Comprehensive-Analysis-of-Systematic-Approaches-and-Automation-Algorithms-State-of-The-Art" class="headerlink" title="Advancements In Crowd-Monitoring System: A Comprehensive Analysis of Systematic Approaches and Automation Algorithms: State-of-The-Art"></a>Advancements In Crowd-Monitoring System: A Comprehensive Analysis of Systematic Approaches and Automation Algorithms: State-of-The-Art</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03907">http://arxiv.org/abs/2308.03907</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammed Ameen, Richard Stone</li>
<li>For: This paper focuses on the development and analysis of crowd monitoring systems, specifically exploring the use of artificial intelligence (AI) algorithms and models to enhance their effectiveness and security.* Methods: The paper employs a bifurcated approach, comparing vision-based and non-vision-based technologies for crowd monitoring, and examines the efficacy of these methods in different environments and contexts.* Results: The paper presents an in-depth analysis of the recent incorporation of AI algorithms and models into automated crowd monitoring systems, highlighting their contemporary applications and effectiveness in various contexts.<details>
<summary>Abstract</summary>
Growing apprehensions surrounding public safety have captured the attention of numerous governments and security agencies across the globe. These entities are increasingly acknowledging the imperative need for reliable and secure crowd-monitoring systems to address these concerns. Effectively managing human gatherings necessitates proactive measures to prevent unforeseen events or complications, ensuring a safe and well-coordinated environment. The scarcity of research focusing on crowd monitoring systems and their security implications has given rise to a burgeoning area of investigation, exploring potential approaches to safeguard human congregations effectively. Crowd monitoring systems depend on a bifurcated approach, encompassing vision-based and non-vision-based technologies. An in-depth analysis of these two methodologies will be conducted in this research. The efficacy of these approaches is contingent upon the specific environment and temporal context in which they are deployed, as they each offer distinct advantages. This paper endeavors to present an in-depth analysis of the recent incorporation of artificial intelligence (AI) algorithms and models into automated systems, emphasizing their contemporary applications and effectiveness in various contexts.
</details>
<details>
<summary>摘要</summary>
全球各地政府和安全机构都在关注公众安全的问题上感到担忧，认为需要可靠和安全的人群监测系统来解决这些问题。管理人群聚集需要采取先进的措施，以避免未然的事件或复杂性，确保安全和有效地协调环境。由于人群监测系统的安全性研究不足，这个领域的研究正在不断扩展，探讨有效地保护人群聚集的方法。人群监测系统采用分割方法，包括视觉基于和非视觉基于技术。本研究将进行深入分析这两种方法，分别在不同环境和时间上的效果。由于这些方法在不同情况下的应用，它们各有优劣。本文将强调现代应用的人工智能（AI）算法和模型在自动化系统中的应用，探讨其在不同场景中的现代应用和效果。
</details></li>
</ul>
<hr>
<h2 id="Intelligent-Assistant-Language-Understanding-On-Device"><a href="#Intelligent-Assistant-Language-Understanding-On-Device" class="headerlink" title="Intelligent Assistant Language Understanding On Device"></a>Intelligent Assistant Language Understanding On Device</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03905">http://arxiv.org/abs/2308.03905</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Cecilia Aas, Hisham Abdelsalam, Irina Belousova, Shruti Bhargava, Jianpeng Cheng, Robert Daland, Joris Driesen, Federico Flego, Tristan Guigue, Anders Johannsen, Partha Lal, Jiarui Lu, Joel Ruben Antony Moniz, Nathan Perkins, Dhivya Piraviperumal, Stephen Pulman, Diarmuid Ó Séaghdha, David Q. Sun, John Torr, Marco Del Vecchio, Jay Wacker, Jason D. Williams, Hong Yu</li>
<li>for: 这篇论文描述了一种运行在个人电子设备上的自然语言理解系统的设计。</li>
<li>methods: 该系统使用了一些特定的架构和技术，例如在对话系统文献中一些方法可能在部署环境中难以维护。</li>
<li>results: 相比服务器基础上的助手，这种系统更加私钥、可靠、快速、表达力强和准确。<details>
<summary>Abstract</summary>
It has recently become feasible to run personal digital assistants on phones and other personal devices. In this paper we describe a design for a natural language understanding system that runs on device. In comparison to a server-based assistant, this system is more private, more reliable, faster, more expressive, and more accurate. We describe what led to key choices about architecture and technologies. For example, some approaches in the dialog systems literature are difficult to maintain over time in a deployment setting. We hope that sharing learnings from our practical experiences may help inform future work in the research community.
</details>
<details>
<summary>摘要</summary>
现在可以在手机和其他个人设备上运行个人数字助手。在这篇论文中，我们描述了一种运行于设备上的自然语言理解系统的设计。与服务器上的助手相比，这种系统更加私隐、可靠、快速、表达力 stronger和更准确。我们介绍了一些关键的建筑和技术选择，例如在部署环境中维护一些对话系统文献中的方法可能困难。我们希望通过分享我们的实践经验，可以对未来的研究community提供指导。
</details></li>
</ul>
<hr>
<h2 id="On-genuine-invariance-learning-without-weight-tying"><a href="#On-genuine-invariance-learning-without-weight-tying" class="headerlink" title="On genuine invariance learning without weight-tying"></a>On genuine invariance learning without weight-tying</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03904">http://arxiv.org/abs/2308.03904</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/amoskalev/ginvariance">https://github.com/amoskalev/ginvariance</a></li>
<li>paper_authors: Artem Moskalev, Anna Sepliarskaia, Erik J. Bekkers, Arnold Smeulders</li>
<li>for:  investigate properties and limitations of invariance learned by neural networks from the data compared to the genuine invariance achieved through invariant weight-tying.</li>
<li>methods:  adopt a group theoretical perspective and analyze invariance learning in neural networks without weight-tying constraints.</li>
<li>results:  demonstrate that even when a network learns to correctly classify samples on a group orbit, the underlying decision-making in such a model does not attain genuine invariance, and propose several metrics to quantify learned invariance.<details>
<summary>Abstract</summary>
In this paper, we investigate properties and limitations of invariance learned by neural networks from the data compared to the genuine invariance achieved through invariant weight-tying. To do so, we adopt a group theoretical perspective and analyze invariance learning in neural networks without weight-tying constraints. We demonstrate that even when a network learns to correctly classify samples on a group orbit, the underlying decision-making in such a model does not attain genuine invariance. Instead, learned invariance is strongly conditioned on the input data, rendering it unreliable if the input distribution shifts. We next demonstrate how to guide invariance learning toward genuine invariance by regularizing the invariance of a model at the training. To this end, we propose several metrics to quantify learned invariance: (i) predictive distribution invariance, (ii) logit invariance, and (iii) saliency invariance similarity. We show that the invariance learned with the invariance error regularization closely reassembles the genuine invariance of weight-tying models and reliably holds even under a severe input distribution shift. Closer analysis of the learned invariance also reveals the spectral decay phenomenon, when a network chooses to achieve the invariance to a specific transformation group by reducing the sensitivity to any input perturbation.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究神经网络学习的不变性和其限制，并与真正的不变性相比较。为此，我们采用群理论的视角，分析神经网络无束缚的不变性学习。我们示示，即使神经网络能正确地分类样本在群或бие中，其下面的决策不会实现真正的不变性。相反，学习的不变性强烈受输入数据的影响，因此在输入分布变化时无法保靠。我们随后示出如何通过训练时的不变性正则化来引导神经网络学习真正的不变性。为此，我们提出了几个度量学习的不变性：（一）预测分布不变性、（二）启动函数不变性和（三）相似性不变性。我们表明，通过不变性错误正则化学习的不变性几乎与束缚模型的真正不变性相同，并在输入分布变化时可靠地保持。进一步分析学习的不变性也揭示了特征衰落现象，当神经网络选择通过减少输入干扰的敏感度来实现不变性。
</details></li>
</ul>
<hr>
<h2 id="FLIPS-Federated-Learning-using-Intelligent-Participant-Selection"><a href="#FLIPS-Federated-Learning-using-Intelligent-Participant-Selection" class="headerlink" title="FLIPS: Federated Learning using Intelligent Participant Selection"></a>FLIPS: Federated Learning using Intelligent Participant Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03901">http://arxiv.org/abs/2308.03901</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rahul Atul Bhope, K. R. Jayaram, Nalini Venkatasubramanian, Ashish Verma, Gegi Thomas</li>
<li>for: 本研究旨在设计和实现聚合资料和参与者不同性中的联邦学习（FL）训练负载中的中介软件系统（FLIPS），以实现资料和参与者不同性的管理。</li>
<li>methods: FLIPS使用标签分布对party的资料进行对数分布 clustering，并在FL训练过程中确保每个群集都具有相等的代表性。FLIPS支持最常用的FL算法，包括FedAvg、FedProx、FedDyn、FedOpt和FedYogi。它还包括一个适应式的对应过程来处理分布式环境中的平台不同性和动态资源可用性。</li>
<li>results: 我们的严谨实验表明，相比随机选择party，FLIPS可以提高精度，在20-60%的通信成本下提高精度17-20%，并且这些优势在参与者具有慢卡特性时仍保持。<details>
<summary>Abstract</summary>
This paper presents the design and implementation of FLIPS, a middleware system to manage data and participant heterogeneity in federated learning (FL) training workloads. In particular, we examine the benefits of label distribution clustering on participant selection in federated learning. FLIPS clusters parties involved in an FL training job based on the label distribution of their data apriori, and during FL training, ensures that each cluster is equitably represented in the participants selected. FLIPS can support the most common FL algorithms, including FedAvg, FedProx, FedDyn, FedOpt and FedYogi. To manage platform heterogeneity and dynamic resource availability, FLIPS incorporates a straggler management mechanism to handle changing capacities in distributed, smart community applications. Privacy of label distributions, clustering and participant selection is ensured through a trusted execution environment (TEE). Our comprehensive empirical evaluation compares FLIPS with random participant selection, as well as two other "smart" selection mechanisms - Oort and gradient clustering using two real-world datasets, two different non-IID distributions and three common FL algorithms (FedYogi, FedProx and FedAvg). We demonstrate that FLIPS significantly improves convergence, achieving higher accuracy by 17 - 20 % with 20 - 60 % lower communication costs, and these benefits endure in the presence of straggler participants.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Scalable-and-Equitable-Math-Problem-Solving-Strategy-Prediction-in-Big-Educational-Data"><a href="#Scalable-and-Equitable-Math-Problem-Solving-Strategy-Prediction-in-Big-Educational-Data" class="headerlink" title="Scalable and Equitable Math Problem Solving Strategy Prediction in Big Educational Data"></a>Scalable and Equitable Math Problem Solving Strategy Prediction in Big Educational Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03892">http://arxiv.org/abs/2308.03892</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anupshakya07/attn-scaling">https://github.com/anupshakya07/attn-scaling</a></li>
<li>paper_authors: Anup Shakya, Vasile Rus, Deepak Venugopal<br>for:本研究旨在提高学生数学学习效果使用智能教学系统（ITS）和适应教学系统（AIS）。methods:我们利用机器学习和人工智能技术来预测学生的解决策略，以便个性化为每个学生适应。我们首先学习学生的掌握表示（MVec），然后使用非 Parametric 聚类算法将这些表示分成不同的群组。最后，我们使用深度神经网络（DNN）模型来预测学生的解决策略。results:我们使用实际世界大规模学生互动数据集（MATHia）进行实验，并使用 transformers 和 Node2Vec 来学习 MVec，以及 LSTM 来预测解决策略。我们的方法可以扩展到大规模数据集，并且具有预测准确性和predictive equality，即预测策略具有一定的普适性。<details>
<summary>Abstract</summary>
Understanding a student's problem-solving strategy can have a significant impact on effective math learning using Intelligent Tutoring Systems (ITSs) and Adaptive Instructional Systems (AISs). For instance, the ITS/AIS can better personalize itself to correct specific misconceptions that are indicated by incorrect strategies, specific problems can be designed to improve strategies and frustration can be minimized by adapting to a student's natural way of thinking rather than trying to fit a standard strategy for all. While it may be possible for human experts to identify strategies manually in classroom settings with sufficient student interaction, it is not possible to scale this up to big data. Therefore, we leverage advances in Machine Learning and AI methods to perform scalable strategy prediction that is also fair to students at all skill levels. Specifically, we develop an embedding called MVec where we learn a representation based on the mastery of students. We then cluster these embeddings with a non-parametric clustering method where we progressively learn clusters such that we group together instances that have approximately symmetrical strategies. The strategy prediction model is trained on instances sampled from these clusters. This ensures that we train the model over diverse strategies and also that strategies from a particular group do not bias the DNN model, thus allowing it to optimize its parameters over all groups. Using real world large-scale student interaction datasets from MATHia, we implement our approach using transformers and Node2Vec for learning the mastery embeddings and LSTMs for predicting strategies. We show that our approach can scale up to achieve high accuracy by training on a small sample of a large dataset and also has predictive equality, i.e., it can predict strategies equally well for learners at diverse skill levels.
</details>
<details>
<summary>摘要</summary>
理解学生的问题解决策略可以对智能教学系统（ITS）和适应教学系统（AIS）的有效学习产生重要影响。例如，ITS/AIS可以更好地个性化自己，为学生的特定错误策略进行特定的更正，设计特定的问题来改善策略，并降低学生的沮丧度。虽然在课堂 SETTINGS中，人工专家可能可以手动确定策略，但不可能扩展到大数据。因此，我们利用机器学习和人工智能技术进行可扩展的策略预测，同时保证学生的公平性。我们开发了一个叫做MVec的嵌入，其中我们学习了学生的掌握程度的表示。然后我们使用非Parametric clustering方法，分类这些嵌入，并逐渐学习分组，以便将学生的策略分为不同的组。我们的策略预测模型是基于这些分组的实例进行训练的。这种方法可以在多个组中学习多种策略，同时避免策略来自某个组的偏见，使得神经网络模型能够在所有组之间优化参数。使用来自MATHia的实际大规模学生互动数据，我们采用了 transformers 和 Node2Vec 来学习掌握嵌入，并使用 LSTM 来预测策略。我们的方法可以在大规模数据上进行扩展，并且具有预测公平性，即可以平等地预测学生的策略水平。
</details></li>
</ul>
<hr>
<h2 id="Generative-Benchmark-Creation-for-Table-Union-Search"><a href="#Generative-Benchmark-Creation-for-Table-Union-Search" class="headerlink" title="Generative Benchmark Creation for Table Union Search"></a>Generative Benchmark Creation for Table Union Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03883">http://arxiv.org/abs/2308.03883</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/northeastern-datalab/alt-gen">https://github.com/northeastern-datalab/alt-gen</a></li>
<li>paper_authors: Koyena Pal, Aamod Khatiwada, Roee Shraga, Renée J. Miller</li>
<li>for: 这篇论文的目的是提出一种使用生成AI模型创建结构化数据审核 benchmark，以解决数据管理问题的semantic nature。</li>
<li>methods: 该论文使用的方法包括使用生成AI模型创建表结构和数据，以及评估表联合搜索方法的性能。</li>
<li>results: 该论文的结果表明，使用生成AI模型创建的 benchmark 更加具有挑战性，比手动创建的 benchmark 更能让方法进行细致的分析，包括 false positives 和 false negatives 的分析。<details>
<summary>Abstract</summary>
Data management has traditionally relied on synthetic data generators to generate structured benchmarks, like the TPC suite, where we can control important parameters like data size and its distribution precisely. These benchmarks were central to the success and adoption of database management systems. But more and more, data management problems are of a semantic nature. An important example is finding tables that can be unioned. While any two tables with the same cardinality can be unioned, table union search is the problem of finding tables whose union is semantically coherent. Semantic problems cannot be benchmarked using synthetic data. Our current methods for creating benchmarks involve the manual curation and labeling of real data. These methods are not robust or scalable and perhaps more importantly, it is not clear how robust the created benchmarks are. We propose to use generative AI models to create structured data benchmarks for table union search. We present a novel method for using generative models to create tables with specified properties. Using this method, we create a new benchmark containing pairs of tables that are both unionable and non-unionable but related. We thoroughly evaluate recent existing table union search methods over existing benchmarks and our new benchmark. We also present and evaluate a new table search methods based on recent large language models over all benchmarks. We show that the new benchmark is more challenging for all methods than hand-curated benchmarks, specifically, the top-performing method achieves a Mean Average Precision of around 60%, over 30% less than its performance on existing manually created benchmarks. We examine why this is the case and show that the new benchmark permits more detailed analysis of methods, including a study of both false positives and false negatives that were not possible with existing benchmarks.
</details>
<details>
<summary>摘要</summary>
datamanagement 历史上通过生成器来生成结构化的benchmark，如TPC集成，以便控制数据大小和分布的重要参数。这些benchmark 对数据管理系统的成功和普及起到了关键作用。但随着时间的推移，数据管理问题变得越来越 semantic in nature。一个重要的例子是找到可以合并的表。虽然任何两个表都可以合并，但表合并搜索是找到可以semantically coherent的表的问题。semantic 问题不能使用生成的数据来 benchmark。我们目前的benchmark创建方法是通过手动筛选和标注实际数据来实现。这些方法不具有可靠性和可扩展性，而且更重要的是，不确定创建的benchmark 的可靠性。我们提议使用生成AI模型来创建结构化数据benchmark  для表合并搜索。我们提出了一种使用生成模型创建表的新方法。使用这种方法，我们创建了一个新的benchmark，包含可以合并的表和不可以合并的表，但它们之间存在关系。我们对现有benchmark 和我们新创建的benchmark进行了仔细的评估。我们还提出了基于最新的大语言模型的新表搜索方法，并对所有benchmark进行了评估。我们发现，新的benchmark 比手动创建的benchmark 更加挑战，特别是top-performing方法的 Mean Average Precision 约为60%，相比手动创建的benchmark 上的性能下降了30%。我们分析了这种情况，并证明新的benchmark 允许更详细的方法分析，包括对方法的false positives和false negatives进行了研究，这些研究不可能通过现有的benchmark 进行。
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Generalization-in-Offline-Reinforcement-Learning-via-Unseen-State-Augmentations"><a href="#Exploiting-Generalization-in-Offline-Reinforcement-Learning-via-Unseen-State-Augmentations" class="headerlink" title="Exploiting Generalization in Offline Reinforcement Learning via Unseen State Augmentations"></a>Exploiting Generalization in Offline Reinforcement Learning via Unseen State Augmentations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03882">http://arxiv.org/abs/2308.03882</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nirbhay Modhe, Qiaozi Gao, Ashwin Kalyan, Dhruv Batra, Govind Thattai, Gaurav Sukhatme</li>
<li>for: 这个论文的目的是提出一种新的无线网络学习方法，以增强无线网络学习方法的探索和利用能力。</li>
<li>methods: 这个论文使用了一种模型械ск抽象的方法，通过减少值估计的不确定性来保持探索和利用的平衡。</li>
<li>results: 这个论文通过一种新的不可见状态扩展策略来提高无线网络学习的性能，并证明了这种策略可以减少数据集Q值估计的平均值，从而实现更保守的Q值估计。<details>
<summary>Abstract</summary>
Offline reinforcement learning (RL) methods strike a balance between exploration and exploitation by conservative value estimation -- penalizing values of unseen states and actions. Model-free methods penalize values at all unseen actions, while model-based methods are able to further exploit unseen states via model rollouts. However, such methods are handicapped in their ability to find unseen states far away from the available offline data due to two factors -- (a) very short rollout horizons in models due to cascading model errors, and (b) model rollouts originating solely from states observed in offline data. We relax the second assumption and present a novel unseen state augmentation strategy to allow exploitation of unseen states where the learned model and value estimates generalize. Our strategy finds unseen states by value-informed perturbations of seen states followed by filtering out states with epistemic uncertainty estimates too high (high error) or too low (too similar to seen data). We observe improved performance in several offline RL tasks and find that our augmentation strategy consistently leads to overall lower average dataset Q-value estimates i.e. more conservative Q-value estimates than a baseline.
</details>
<details>
<summary>摘要</summary>
无线连接学习（RL）方法寻找平衡 между探索和占用，通过保守估值来衡量未看到的状态和动作的价值。无模型方法对所有未看到的动作进行 penalty，而模型基于方法可以通过模型扩展来进一步利用未看到的状态。然而，这些方法因两个因素受限：（a）模型中的扩展时间非常短，由于堆叠模型错误，和（b）模型扩展仅从看到的状态开始。我们relax这个第二个假设，并提出了一种新的未看到状态扩展策略，允许利用未看到状态的价值估计。我们的策略通过对已经看到的状态进行价值意识的扰动，然后过滤高度 Epistemic 不确定性（高错误）或者太像已经看到的数据的状态。我们在多个无线RL任务中观察到改进的性能，并发现我们的扩展策略通常比基准值更保守，即更低的平均数据Q估值。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-and-Explaining-Large-Language-Models-for-Code-Using-Syntactic-Structures"><a href="#Evaluating-and-Explaining-Large-Language-Models-for-Code-Using-Syntactic-Structures" class="headerlink" title="Evaluating and Explaining Large Language Models for Code Using Syntactic Structures"></a>Evaluating and Explaining Large Language Models for Code Using Syntactic Structures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03873">http://arxiv.org/abs/2308.03873</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wm-semeru/codesyntaxconcept">https://github.com/wm-semeru/codesyntaxconcept</a></li>
<li>paper_authors: David N Palacio, Alejandro Velasco, Daniel Rodriguez-Cardenas, Kevin Moran, Denys Poshyvanyk</li>
<li>for: 这个论文旨在探讨大型自然语言模型（LLM）在编程任务上的效果和解释方法。</li>
<li>methods: 该论文提出了一种专门为 LLM 编程任务的解释方法，即 ASTxplainer，它可以帮助用户理解模型预测结果。ASTxplainer 使用了自动将token预测与AST结构相对应的方法，并提供了一种基于 AST 结构的模型评估方法和预测视图。</li>
<li>results: 该论文通过对 12 种流行的 LLM 进行实验，以及对 ASTxplainer  derive 的视图进行用户研究，显示了 ASTxplainer 的潜在作用和可用性。研究结果表明，ASTxplainer 可以提供有用的预测解释和模型效果评估。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) for code are a family of high-parameter, transformer-based neural networks pre-trained on massive datasets of both natural and programming languages. These models are rapidly being employed in commercial AI-based developer tools, such as GitHub CoPilot. However, measuring and explaining their effectiveness on programming tasks is a challenging proposition, given their size and complexity. The methods for evaluating and explaining LLMs for code are inextricably linked. That is, in order to explain a model's predictions, they must be reliably mapped to fine-grained, understandable concepts. Once this mapping is achieved, new methods for detailed model evaluations are possible. However, most current explainability techniques and evaluation benchmarks focus on model robustness or individual task performance, as opposed to interpreting model predictions.   To this end, this paper introduces ASTxplainer, an explainability method specific to LLMs for code that enables both new methods for LLM evaluation and visualizations of LLM predictions that aid end-users in understanding model predictions. At its core, ASTxplainer provides an automated method for aligning token predictions with AST nodes, by extracting and aggregating normalized model logits within AST structures. To demonstrate the practical benefit of ASTxplainer, we illustrate the insights that our framework can provide by performing an empirical evaluation on 12 popular LLMs for code using a curated dataset of the most popular GitHub projects. Additionally, we perform a user study examining the usefulness of an ASTxplainer-derived visualization of model predictions aimed at enabling model users to explain predictions. The results of these studies illustrate the potential for ASTxplainer to provide insights into LLM effectiveness, and aid end-users in understanding predictions.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM） для程式码是一家高参数、transformer基于神经网络的家族，在巨大的自然语言和程式语言Dataset上预训。这些模型在商业AI基于开发工具中被快速运用，例如GitHub CoPilot。然而，评估和解释LLMs的效果在程式任务上是一个具有挑战性的问题，因为它们的大小和复杂性。以下是一些用于评估和解释LLMs的方法：1. 将模型预测与AST结构进行自动对齐，以提取和聚合 нор化的模型潜在值。2. 使用AST结构来解释模型预测的方法，以提供更多的具体和可理解的概念。3. 使用新的评估方法和可视化工具来评估LLM的效果。为了解决这个问题，本文将介绍一种特有的解释方法——ASTxplainer，它可以帮助用户理解LLM的预测。ASTxplainer使用自动对齐模型预测和AST结构，以提取和聚合 нор化的模型潜在值。这些方法可以提供更多的具体和可理解的概念，以帮助用户理解LLM的预测。为了证明ASTxplainer的实用性，我们在12种popular LLMs for code上进行了一场empirical评估，使用一个 curaateddataset of the most popular GitHub projects。此外，我们还进行了一次用户研究，评估ASTxplainer-derived的可视化工具是否可以帮助用户解释模型预测。研究结果表明，ASTxplainer可以提供LLM效果的实际价值，并帮助用户理解预测。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Equivalence-of-e-Commerce-Queries"><a href="#Semantic-Equivalence-of-e-Commerce-Queries" class="headerlink" title="Semantic Equivalence of e-Commerce Queries"></a>Semantic Equivalence of e-Commerce Queries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03869">http://arxiv.org/abs/2308.03869</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aritra Mandal, Daniel Tunkelang, Zhe Wu</li>
<li>for: 提高电商搜索的用户体验和商业效果，解决查询Intent的识别和利用问题。</li>
<li>methods: 提出了一种框架，包括将查询映射到搜索意图的 vector 表示，并通过对查询的surface similarity和行为相似性进行识别，以便找到最相似的查询。</li>
<li>results: 实验结果表明，该方法可以高效地识别和利用查询Intent，并且可以超越流行的句子转换器模型，实现了查询相似性的Pearson相关系数0.85。这些结果表明，可以通过历史搜索行为数据和模型训练来认识和利用查询Intent，从而提高用户体验和商业效果。<details>
<summary>Abstract</summary>
Search query variation poses a challenge in e-commerce search, as equivalent search intents can be expressed through different queries with surface-level differences. This paper introduces a framework to recognize and leverage query equivalence to enhance searcher and business outcomes. The proposed approach addresses three key problems: mapping queries to vector representations of search intent, identifying nearest neighbor queries expressing equivalent or similar intent, and optimizing for user or business objectives. The framework utilizes both surface similarity and behavioral similarity to determine query equivalence. Surface similarity involves canonicalizing queries based on word inflection, word order, compounding, and noise words. Behavioral similarity leverages historical search behavior to generate vector representations of query intent. An offline process is used to train a sentence similarity model, while an online nearest neighbor approach supports processing of unseen queries. Experimental evaluations demonstrate the effectiveness of the proposed approach, outperforming popular sentence transformer models and achieving a Pearson correlation of 0.85 for query similarity. The results highlight the potential of leveraging historical behavior data and training models to recognize and utilize query equivalence in e-commerce search, leading to improved user experiences and business outcomes. Further advancements and benchmark datasets are encouraged to facilitate the development of solutions for this critical problem in the e-commerce domain.
</details>
<details>
<summary>摘要</summary>
搜索查询的变化呈现了电商搜索中的挑战，因为相同的搜索意图可以通过不同的查询语句表达。本文介绍了一个框架，用于认可和利用查询相似性，以提高搜索者和商业目标的结果。该方案解决了三个关键问题：将查询映射到搜索意图的 вектор表示，标识最相似的查询，并优化用户或商业目标。该框架利用surface similarity和behavioral similarity来确定查询相似性。surface similarity通过词形变化、词序、合成和噪声词进行 canonicalization。behavioral similarity利用历史搜索行为生成搜索意图的 вектор表示。在线 nearest neighbor 方法支持处理未看过的查询。实验证明了提议的方法的有效性，比 популяр的句子转换器模型高效，并达到了0.85的Spearman相关系数。结果表明可以利用历史行为数据和模型训练来认可和利用查询相似性，从而提高用户体验和商业result。进一步的进步和标准化数据集的开发可以促进在电商领域内的解决这类问题的发展。
</details></li>
</ul>
<hr>
<h2 id="AI-Text-to-Behavior-A-Study-In-Steerability"><a href="#AI-Text-to-Behavior-A-Study-In-Steerability" class="headerlink" title="AI Text-to-Behavior: A Study In Steerability"></a>AI Text-to-Behavior: A Study In Steerability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07326">http://arxiv.org/abs/2308.07326</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Noever, Sam Hyams</li>
<li>for: 这个研究探究了大语言模型（LLMs）的可控性，尤其是OpenAI的ChatGPT迭代。</li>
<li>methods: 我们使用了行为心理学框架called OCEAN（开放性、注意力、外向性、合作性、不稳定性）来量化模型的响应性。</li>
<li>results: 我们发现了不同 trait的语言对应性，包括“开放性”、“注意力”和“合作性”，而“外向性”和“不稳定性”则显示了明显的差异。这些发现表明GPT的多样性和适应能力，但同时也表明了一些问题，如训练技术的不透明度和LLM的快速进步。<details>
<summary>Abstract</summary>
The research explores the steerability of Large Language Models (LLMs), particularly OpenAI's ChatGPT iterations. By employing a behavioral psychology framework called OCEAN (Openness, Conscientiousness, Extroversion, Agreeableness, Neuroticism), we quantitatively gauged the model's responsiveness to tailored prompts. When asked to generate text mimicking an extroverted personality, OCEAN scored the language alignment to that behavioral trait. In our analysis, while "openness" presented linguistic ambiguity, "conscientiousness" and "neuroticism" were distinctly evoked in the OCEAN framework, with "extroversion" and "agreeableness" showcasing a notable overlap yet distinct separation from other traits. Our findings underscore GPT's versatility and ability to discern and adapt to nuanced instructions. Furthermore, historical figure simulations highlighted the LLM's capacity to internalize and project instructible personas, precisely replicating their philosophies and dialogic styles. However, the rapid advancements in LLM capabilities and the opaque nature of some training techniques make metric proposals degrade rapidly. Our research emphasizes a quantitative role to describe steerability in LLMs, presenting both its promise and areas for further refinement in aligning its progress to human intentions.
</details>
<details>
<summary>摘要</summary>
Note: Please note that the translation is in Simplified Chinese, and some words or phrases may have different translations in Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="MCTS-guided-Genetic-Algorithm-for-optimization-of-neural-network-weights"><a href="#MCTS-guided-Genetic-Algorithm-for-optimization-of-neural-network-weights" class="headerlink" title="MCTS guided Genetic Algorithm for optimization of neural network weights"></a>MCTS guided Genetic Algorithm for optimization of neural network weights</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04459">http://arxiv.org/abs/2308.04459</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AkshayHebbar/MCTS-GA">https://github.com/AkshayHebbar/MCTS-GA</a></li>
<li>paper_authors: Akshay Hebbar</li>
<li>for: 本研究探讨了应用搜索策略到遗传算法中的整个遗传树结构。</li>
<li>methods: 本研究使用了多种搜索方法，包括广度优先搜索、深度优先搜索和迭代搜索等，但这些方法通常需要较长的计算时间。作者采用了对抗技术，以优化遗传算法中的搜索。</li>
<li>results: 本研究结果表明，结合遗传算法和蒙地卡树搜索策略可以优化神经网络的优化问题。通过对遗传树进行优化搜索，可以快速地找到最佳的神经网络结构。<details>
<summary>Abstract</summary>
In this research, we investigate the possibility of applying a search strategy to genetic algorithms to explore the entire genetic tree structure. Several methods aid in performing tree searches; however, simpler algorithms such as breadth-first, depth-first, and iterative techniques are computation-heavy and often result in a long execution time. Adversarial techniques are often the preferred mechanism when performing a probabilistic search, yielding optimal results more quickly. The problem we are trying to tackle in this paper is the optimization of neural networks using genetic algorithms. Genetic algorithms (GA) form a tree of possible states and provide a mechanism for rewards via the fitness function. Monte Carlo Tree Search (MCTS) has proven to be an effective tree search strategy given states and rewards; therefore, we will combine these approaches to optimally search for the best result generated with genetic algorithms.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们研究了将搜索策略应用于遗传算法，以探索整个遗传树结构。许多方法可以进行树搜索，但是简单的算法如广度优先、深度优先和迭代方法往往需要较长的计算时间。对于probabilistic搜索，反斗技术通常是首选的机制，可以快速获得优化结果。我们在这篇论文中是通过遗传算法优化神经网络的优化问题。遗传算法形成了一棵可能的状态树，并提供了一种via遗传函数的奖励机制。蒙地卡罗瑞搜索（MCTS）在给定状态和奖励时已经证明是一个有效的搜索策略，因此我们将这些方法相结合，以优化遗传算法中的最佳结果。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-Prompt-Engineering-via-Declarative-Crowdsourcing"><a href="#Revisiting-Prompt-Engineering-via-Declarative-Crowdsourcing" class="headerlink" title="Revisiting Prompt Engineering via Declarative Crowdsourcing"></a>Revisiting Prompt Engineering via Declarative Crowdsourcing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03854">http://arxiv.org/abs/2308.03854</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aditya G. Parameswaran, Shreya Shankar, Parth Asawa, Naman Jain, Yujie Wang</li>
<li>for: 本研究旨在提高LLM（大型自然语言模型）在数据处理工作流程中的质量，同时保持成本在bounds。</li>
<li>methods: 本研究提出了一种宣言式的推广工程（Prompt Engineering）方法，利用多种推广策略、保证内部一致性，以及混合LLM-非LLM方法来使推广工程变得更原理化。</li>
<li>results: 预liminary的案例研究表明，使用宣言式推广工程可以提高LLM在排序、实体解析和填充等任务中的性能。<details>
<summary>Abstract</summary>
Large language models (LLMs) are incredibly powerful at comprehending and generating data in the form of text, but are brittle and error-prone. There has been an advent of toolkits and recipes centered around so-called prompt engineering-the process of asking an LLM to do something via a series of prompts. However, for LLM-powered data processing workflows, in particular, optimizing for quality, while keeping cost bounded, is a tedious, manual process. We put forth a vision for declarative prompt engineering. We view LLMs like crowd workers and leverage ideas from the declarative crowdsourcing literature-including leveraging multiple prompting strategies, ensuring internal consistency, and exploring hybrid-LLM-non-LLM approaches-to make prompt engineering a more principled process. Preliminary case studies on sorting, entity resolution, and imputation demonstrate the promise of our approach
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）具有极高的文本理解和生成能力，但是它们受限于精度和精度。随着推 engineering（提示工程）的出现，人们开始关注如何使用提示来让 LLM 完成某种任务。然而，为了在 LLM 驱动的数据处理工作流程中提高质量，同时保持成本在bounds，是一个繁琐、手动的过程。我们提出了声明式推 engineering 的视野。我们视 LLM 为群组工作者，并利用声明式招募文献中的想法，包括多种提示策略、内部一致性和混合 LLM-非 LLM approaches，以使提示工程变得更加原则化。初步的案例研究表明，这种方法在排序、实体解析和填充等方面具有承诺的批处。
</details></li>
</ul>
<hr>
<h2 id="Search-Engine-and-Recommendation-System-for-the-Music-Industry-built-with-JinaAI"><a href="#Search-Engine-and-Recommendation-System-for-the-Music-Industry-built-with-JinaAI" class="headerlink" title="Search Engine and Recommendation System for the Music Industry built with JinaAI"></a>Search Engine and Recommendation System for the Music Industry built with JinaAI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03842">http://arxiv.org/abs/2308.03842</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ishita Gopalakrishnan, Sanjjushri Varshini R, Ponshriharini V</li>
<li>for: 提供一个有用的搜索引擎和推荐系统 для音乐业界，以解决现有搜索引擎场景中的问题，例如速度、准确性和搜寻数据的格式。</li>
<li>methods: 使用Jina AI，一个MLOps框架，以建立基于神经网络的搜索引擎，并使用单一查询输入进行搜寻分析，并且可以对数据库中的歌曲进行精确的匹配。</li>
<li>results: 建立了一个有效的搜索引擎和推荐系统，可以帮助用户快速找到想要的歌曲，并且可以保持和提高搜索引擎的性能质量。<details>
<summary>Abstract</summary>
One of the most intriguing debates regarding a novel task is the development of search engines and recommendation-based systems in the music industry. Studies have shown a drastic depression in the search engine fields, due to concerning factors such as speed, accuracy and the format of data given for querying. Often people face difficulty in searching for a song solely based on the title, hence a solution is proposed to complete a search analysis through a single query input and is matched with the lyrics of the songs present in the database. Hence it is essential to incorporate cutting-edge technology tools for developing a user-friendly search engine. Jina AI is an MLOps framework for building neural search engines that are utilized, in order for the user to obtain accurate results. Jina AI effectively helps to maintain and enhance the quality of performance for the search engine for the query given. An effective search engine and a recommendation system for the music industry, built with JinaAI.
</details>
<details>
<summary>摘要</summary>
一个非常有趣的讨论是音乐业中搜索引擎和推荐系统的开发。研究表明，搜索引擎领域受到了严重的萧瑟和精度等因素的影响，导致搜索效果不佳。因此，一种解决方案是通过单个查询输入完成搜索分析，并将数据库中的歌曲歌词与查询结果进行匹配。因此，采用先进的技术工具对于建立用户友好的搜索引擎是非常重要。Jina AI 是一个 ML Ops 框架，用于建立基于神经网络的搜索引擎，以提供精准的搜索结果。Jina AI 有效地帮助维护和提高搜索引擎的性能质量。一款有效的搜索引擎和推荐系统，用于音乐industry，基于 JinaAI。
</details></li>
</ul>
<hr>
<h2 id="The-Copycat-Perceptron-Smashing-Barriers-Through-Collective-Learning"><a href="#The-Copycat-Perceptron-Smashing-Barriers-Through-Collective-Learning" class="headerlink" title="The Copycat Perceptron: Smashing Barriers Through Collective Learning"></a>The Copycat Perceptron: Smashing Barriers Through Collective Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03743">http://arxiv.org/abs/2308.03743</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giovanni Catania, Aurélien Decelle, Beatriz Seoane</li>
<li>for: 研究一种 Binary Perceptron 模型在教师-学生场景下的平衡性质。</li>
<li>methods: 使用适当的学习规则和显式氧化 coupling  proportional to Hamming distance between students’ weights。</li>
<li>results: 对于具有非零温度的情况， coupling of replicas 导致 phase diagram shift to smaller values of α，这表明在 fixed fraction of reviewed examples 下，解决方案的自由能 landscape 变得更平滑，使用 local update algorithms such as Simulated Annealing 可以更容易到达解决方案。<details>
<summary>Abstract</summary>
We characterize the equilibrium properties of a model of $y$ coupled binary perceptrons in the teacher-student scenario, subject to a suitable learning rule, with an explicit ferromagnetic coupling proportional to the Hamming distance between the students' weights. In contrast to recent works, we analyze a more general setting in which a thermal noise is present that affects the generalization performance of each student. Specifically, in the presence of a nonzero temperature, which assigns nonzero probability to configurations that misclassify samples with respect to the teacher's prescription, we find that the coupling of replicas leads to a shift of the phase diagram to smaller values of $\alpha$: This suggests that the free energy landscape gets smoother around the solution with good generalization (i.e., the teacher) at a fixed fraction of reviewed examples, which allows local update algorithms such as Simulated Annealing to reach the solution before the dynamics gets frozen. Finally, from a learning perspective, these results suggest that more students (in this case, with the same amount of data) are able to learn the same rule when coupled together with a smaller amount of data.
</details>
<details>
<summary>摘要</summary>
我们研究一个 teacher-student enario中的 $y$ 关联 binary perceptron 模型的稳定性特性，采用一种合适的学习规则，并具有明确的 ferromagnetic 相互作用，该相互作用与学生的权重差值成正比。与之前的研究不同，我们分析了一种更通用的设置，在其中每个学生面临着一定温度，这使得每个学生的推测结果受到样本推测结果的影响。我们发现，在非零温度下，相互作用导致解的相对温度下降，这使得解的自由能面积变得更平滑，从而使用 Simulated Annealing 类型的本地更新算法可以更好地到达解。最后，从学习角度来看，这些结果表明，通过将更多的学生（每个学生具有相同数据量） coupling  вместе，可以在相同数据量下学习同样的规则。
</details></li>
</ul>
<hr>
<h2 id="Randomized-algorithms-for-precise-measurement-of-differentially-private-personalized-recommendations"><a href="#Randomized-algorithms-for-precise-measurement-of-differentially-private-personalized-recommendations" class="headerlink" title="Randomized algorithms for precise measurement of differentially-private, personalized recommendations"></a>Randomized algorithms for precise measurement of differentially-private, personalized recommendations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03735">http://arxiv.org/abs/2308.03735</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/apple/ml-dprecs">https://github.com/apple/ml-dprecs</a></li>
<li>paper_authors: Allegra Laro, Yanqing Chen, Hao He, Babak Aghazadeh</li>
<li>for: 这篇论文是关于个性化推荐的算法设计，帮助企业建立隐私第一的个性化推荐系统。</li>
<li>methods: 本论文提出了一种隐私保护的个性化推荐算法，通过对用户数据进行加密和扩展来保证用户隐私，同时仍能够准确地反映用户兴趣。</li>
<li>results: 作者通过实验研究了这种隐私保护的个性化推荐算法在用户体验、广告商价值和平台收益等方面的影响，并发现该算法可以减少用户隐私泄露风险，同时保持用户满意度和广告商满意度。<details>
<summary>Abstract</summary>
Personalized recommendations form an important part of today's internet ecosystem, helping artists and creators to reach interested users, and helping users to discover new and engaging content. However, many users today are skeptical of platforms that personalize recommendations, in part due to historically careless treatment of personal data and data privacy. Now, businesses that rely on personalized recommendations are entering a new paradigm, where many of their systems must be overhauled to be privacy-first. In this article, we propose an algorithm for personalized recommendations that facilitates both precise and differentially-private measurement. We consider advertising as an example application, and conduct offline experiments to quantify how the proposed privacy-preserving algorithm affects key metrics related to user experience, advertiser value, and platform revenue compared to the extremes of both (private) non-personalized and non-private, personalized implementations.
</details>
<details>
<summary>摘要</summary>
现代互联网生态系统中，个性化推荐已成为重要的一部分，帮助艺术家和创作者与有兴趣的用户连接，并帮助用户发现新和有趣的内容。然而，许多用户今天对于个性化推荐平台的存在表示怀疑，部分原因是历史上对个人数据和隐私的不谨慎处理。现在，基于个性化推荐的企业正进入一个新的 paradigma，其中许多系统需要重新设计以保持隐私。在这篇文章中，我们提出一种隐私保护的个性化推荐算法，可以同时保证精度和分配隐私。我们通过广告作为应用例子，并在线实验评估了提议的隐私保护算法对用户体验、广告商价值和平台收益的影响，与非个性化和非隐私个性化实现相比。
</details></li>
</ul>
<hr>
<h2 id="SurvBeX-An-explanation-method-of-the-machine-learning-survival-models-based-on-the-Beran-estimator"><a href="#SurvBeX-An-explanation-method-of-the-machine-learning-survival-models-based-on-the-Beran-estimator" class="headerlink" title="SurvBeX: An explanation method of the machine learning survival models based on the Beran estimator"></a>SurvBeX: An explanation method of the machine learning survival models based on the Beran estimator</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03730">http://arxiv.org/abs/2308.03730</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/danilaeremenko/survbex">https://github.com/danilaeremenko/survbex</a></li>
<li>paper_authors: Lev V. Utkin, Danila Y. Eremenko, Andrei V. Konstantinov</li>
<li>For: The paper proposes a new method called SurvBeX to interpret predictions of machine learning survival black-box models.* Methods: The method uses a modified Beran estimator as a surrogate explanation model, and generates many points in a local area around an example of interest to compute the survival function of the black-box model and the Beran estimator.* Results: The paper demonstrates the efficiency of SurvBeX through numerical experiments with synthetic and real survival data, and compares the method with SurvLIME and SurvSHAP. The code implementing SurvBeX is available online.<details>
<summary>Abstract</summary>
An explanation method called SurvBeX is proposed to interpret predictions of the machine learning survival black-box models. The main idea behind the method is to use the modified Beran estimator as the surrogate explanation model. Coefficients, incorporated into Beran estimator, can be regarded as values of the feature impacts on the black-box model prediction. Following the well-known LIME method, many points are generated in a local area around an example of interest. For every generated example, the survival function of the black-box model is computed, and the survival function of the surrogate model (the Beran estimator) is constructed as a function of the explanation coefficients. In order to find the explanation coefficients, it is proposed to minimize the mean distance between the survival functions of the black-box model and the Beran estimator produced by the generated examples. Many numerical experiments with synthetic and real survival data demonstrate the SurvBeX efficiency and compare the method with the well-known method SurvLIME. The method is also compared with the method SurvSHAP. The code implementing SurvBeX is available at: https://github.com/DanilaEremenko/SurvBeX
</details>
<details>
<summary>摘要</summary>
提出了一种解释方法 called SurvBeX，用于解释机器学习生存黑盒模型的预测结果。该方法的主要想法是使用 modify Beran 估计器作为解释模型。 incorporated into Beran 估计器的系数可以看作黑盒模型预测结果中特定特征的影响值。采用 LIME 方法的做法，在对 интересов的示例点附近 generate many 点，然后对每个生成的示例点，计算黑盒模型的生存函数，并将 Beran 估计器中的生存函数作为解释系数的函数。为了找到解释系数，提议使用生成的示例点中的mean distance between survival functions of the black-box model and the Beran estimator 来减少。 numerically experiments with synthetic and real survival data demonstrate SurvBeX 的效果，并与 SurvLIME 方法进行比较。 SurvBeX 还与 SurvSHAP 方法进行比较。 SurvBeX 的代码可以在以下链接中找到：https://github.com/DanilaEremenko/SurvBeX。
</details></li>
</ul>
<hr>
<h2 id="Dimensionality-Reduction-for-Improving-Out-of-Distribution-Detection-in-Medical-Image-Segmentation"><a href="#Dimensionality-Reduction-for-Improving-Out-of-Distribution-Detection-in-Medical-Image-Segmentation" class="headerlink" title="Dimensionality Reduction for Improving Out-of-Distribution Detection in Medical Image Segmentation"></a>Dimensionality Reduction for Improving Out-of-Distribution Detection in Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03723">http://arxiv.org/abs/2308.03723</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mckellwoodland/dimen_reduce_mahal">https://github.com/mckellwoodland/dimen_reduce_mahal</a></li>
<li>paper_authors: McKell Woodland, Nihil Patel, Mais Al Taie, Joshua P. Yung, Tucker J. Netherton, Ankit B. Patel, Kristy K. Brock</li>
<li>for: 这个论文是为了检测liver segmentation模型在不同数据分布下的性能，以避免自动化偏见。</li>
<li>methods: 该论文使用了 Mahalanobis 距离后处理瓶颈特征，将瓶颈特征缩放到 Principal Component Analysis 中，以高效地检测out-of-distribution 图像。</li>
<li>results: 该论文的实验结果显示，通过应用 Mahalanobis 距离后处理瓶颈特征，可以高效地检测out-of-distribution 图像，并且具有较高的性能和较低的计算负担。<details>
<summary>Abstract</summary>
Clinically deployed segmentation models are known to fail on data outside of their training distribution. As these models perform well on most cases, it is imperative to detect out-of-distribution (OOD) images at inference to protect against automation bias. This work applies the Mahalanobis distance post hoc to the bottleneck features of a Swin UNETR model that segments the liver on T1-weighted magnetic resonance imaging. By reducing the dimensions of the bottleneck features with principal component analysis, OOD images were detected with high performance and minimal computational load.
</details>
<details>
<summary>摘要</summary>
临床应用的分割模型通常会在训练分布外的数据上失败。由于这些模型在大多数情况下表现良好，因此在推理阶段检测出idanormal（OOD）图像是非常重要的，以避免自动化偏见。这个工作使用Swin UNITER模型的瓶颈特征使用 Mahalanobis 距离后处理，以降低瓶颈特征的维度。通过使用主成分分析，OOD 图像可以高效地检测到，而且计算负担相对较小。
</details></li>
</ul>
<hr>
<h2 id="“Do-Anything-Now”-Characterizing-and-Evaluating-In-The-Wild-Jailbreak-Prompts-on-Large-Language-Models"><a href="#“Do-Anything-Now”-Characterizing-and-Evaluating-In-The-Wild-Jailbreak-Prompts-on-Large-Language-Models" class="headerlink" title="“Do Anything Now”: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models"></a>“Do Anything Now”: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03825">http://arxiv.org/abs/2308.03825</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/verazuo/jailbreak_llms">https://github.com/verazuo/jailbreak_llms</a></li>
<li>paper_authors: Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, Yang Zhang</li>
<li>for: 本研究旨在探讨大语言模型（LLM）遭到恶意使用的问题，特别是新出现的“监狱提示”（jailbreak prompt），以及它们如何绕过安全措施并刺激LLM生成有害内容。</li>
<li>methods: 本研究使用自然语言处理技术和图形基于的社区检测方法，探索监狱提示的特有特征和主要攻击策略，如提示注入和特权提升。研究还发现监狱提示逐渐倾向私人平台，对LLM供应商带来新的检测挑战。</li>
<li>results: 研究发现现有LLM和安全措施无法彻底防范监狱提示的攻击，特别是在13种禁止enario中，其中两个监狱提示在GPT-3.5和GPT-4上达到了0.99攻击成功率，并在线上免疫超过100天。研究 shed light on the严重和不断演化的监狱提示威胁领域。希望本研究可以促进研究人员和LLM供应商在推广安全和规范的LLM方面努力。<details>
<summary>Abstract</summary>
The misuse of large language models (LLMs) has garnered significant attention from the general public and LLM vendors. In response, efforts have been made to align LLMs with human values and intent use. However, a particular type of adversarial prompts, known as jailbreak prompt, has emerged and continuously evolved to bypass the safeguards and elicit harmful content from LLMs. In this paper, we conduct the first measurement study on jailbreak prompts in the wild, with 6,387 prompts collected from four platforms over six months. Leveraging natural language processing technologies and graph-based community detection methods, we discover unique characteristics of jailbreak prompts and their major attack strategies, such as prompt injection and privilege escalation. We also observe that jailbreak prompts increasingly shift from public platforms to private ones, posing new challenges for LLM vendors in proactive detection. To assess the potential harm caused by jailbreak prompts, we create a question set comprising 46,800 samples across 13 forbidden scenarios. Our experiments show that current LLMs and safeguards cannot adequately defend jailbreak prompts in all scenarios. Particularly, we identify two highly effective jailbreak prompts which achieve 0.99 attack success rates on ChatGPT (GPT-3.5) and GPT-4, and they have persisted online for over 100 days. Our work sheds light on the severe and evolving threat landscape of jailbreak prompts. We hope our study can facilitate the research community and LLM vendors in promoting safer and regulated LLMs.
</details>
<details>
<summary>摘要</summary>
大量语言模型（LLM）的不当使用已经引起了公众和LLM供应商的关注。为了规避LLMs被用于不良目的，努力被进行了对LLMs的人类价值观和合法用途的Alignment。然而，一种特殊的恶意提示，称为监狱破解提示，在不断演化以通过安全措施得到恶意内容从LLMs。在这篇论文中，我们进行了首次在野外中对监狱提示的测量研究，收集了6,387个提示从四个平台上， duration of six months。通过自然语言处理技术和图形基本的社区探测方法，我们发现了监狱提示的独特特征和主要攻击策略，如提示注入和特权提升。我们还发现，监狱提示在公共平台逐渐减少，这对LLM供应商在抢救措施方面带来了新的挑战。为了评估监狱提示所可能引起的危害，我们创建了46,800个问题样本，涵盖13个禁止enario。我们的实验表明，目前的LLMs和安全措施无法有效地防止监狱提示在所有情况下。特别是，我们标识出了两个非常有效的监狱提示，在ChatGPT（GPT-3.5）和GPT-4上达到了0.99的攻击成功率，它们在线上持续超过100天。我们的工作照明了监狱提示的严重和演化的威胁风险。我们希望我们的研究能够促进研究 сообщество和LLM供应商在推广安全和规范的LLMs方面的努力。
</details></li>
</ul>
<hr>
<h2 id="Communication-Efficient-Framework-for-Distributed-Image-Semantic-Wireless-Transmission"><a href="#Communication-Efficient-Framework-for-Distributed-Image-Semantic-Wireless-Transmission" class="headerlink" title="Communication-Efficient Framework for Distributed Image Semantic Wireless Transmission"></a>Communication-Efficient Framework for Distributed Image Semantic Wireless Transmission</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03713">http://arxiv.org/abs/2308.03713</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bingyan Xie, Yongpeng Wu, Yuxuan Shi, Derrick Wing Kwan Ng, Wenjun Zhang</li>
<li>For: The paper proposes a federated learning-based semantic communication (FLSC) framework for multi-task distributed image transmission with IoT devices.* Methods: The FLSC framework uses a hierarchical vision transformer (HVT)-based extractor and a task-adaptive translator for coarse-to-fine semantic extraction and meaning translation. The framework also employs a channel state information-based multiple-input multiple-output transmission module to combat channel fading and noise.* Results: The paper shows that the FLSC framework can achieve better performance than traditional schemes in terms of coarse semantic information and signal-to-noise ratio, especially in low signal-to-noise ratio and channel bandwidth ratio regimes. Specifically, the FLSC framework can provide around 10 dB signal-to-noise ratio gain in the 3 dB channel condition.<details>
<summary>Abstract</summary>
Multi-node communication, which refers to the interaction among multiple devices, has attracted lots of attention in many Internet-of-Things (IoT) scenarios. However, its huge amounts of data flows and inflexibility for task extension have triggered the urgent requirement of communication-efficient distributed data transmission frameworks. In this paper, inspired by the great superiorities on bandwidth reduction and task adaptation of semantic communications, we propose a federated learning-based semantic communication (FLSC) framework for multi-task distributed image transmission with IoT devices. Federated learning enables the design of independent semantic communication link of each user while further improves the semantic extraction and task performance through global aggregation. Each link in FLSC is composed of a hierarchical vision transformer (HVT)-based extractor and a task-adaptive translator for coarse-to-fine semantic extraction and meaning translation according to specific tasks. In order to extend the FLSC into more realistic conditions, we design a channel state information-based multiple-input multiple-output transmission module to combat channel fading and noise. Simulation results show that the coarse semantic information can deal with a range of image-level tasks. Moreover, especially in low signal-to-noise ratio and channel bandwidth ratio regimes, FLSC evidently outperforms the traditional scheme, e.g. about 10 peak signal-to-noise ratio gain in the 3 dB channel condition.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Scaling-may-be-all-you-need-for-achieving-human-level-object-recognition-capacity-with-human-like-visual-experience"><a href="#Scaling-may-be-all-you-need-for-achieving-human-level-object-recognition-capacity-with-human-like-visual-experience" class="headerlink" title="Scaling may be all you need for achieving human-level object recognition capacity with human-like visual experience"></a>Scaling may be all you need for achieving human-level object recognition capacity with human-like visual experience</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03712">http://arxiv.org/abs/2308.03712</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/eminorhan/humanlike-vits">https://github.com/eminorhan/humanlike-vits</a></li>
<li>paper_authors: A. Emin Orhan</li>
<li>for:  investigate whether current self-supervised learning methods can reach human-level visual object recognition capabilities with the same type and amount of visual experience as humans.</li>
<li>methods: use vision transformers with up to 633M parameters and train with up to 5K hours of human-like video data, with image resolutions of up to 476x476 pixels, using masked autoencoders as a self-supervised learning algorithm.</li>
<li>results: find that it is feasible to reach human-level object recognition capacity at sub-human scales of model size, data size, and image size, if these factors are scaled up simultaneously, and estimate that a 2.5B parameter ViT model trained with 20K hours of human-like video data should be able to reach roughly human-level accuracy on ImageNet.<details>
<summary>Abstract</summary>
This paper asks whether current self-supervised learning methods, if sufficiently scaled up, would be able to reach human-level visual object recognition capabilities with the same type and amount of visual experience humans learn from. Previous work on this question only considered the scaling of data size. Here, we consider the simultaneous scaling of data size, model size, and image resolution. We perform a scaling experiment with vision transformers up to 633M parameters in size (ViT-H/14) trained with up to 5K hours of human-like video data (long, continuous, mostly egocentric videos) with image resolutions of up to 476x476 pixels. The efficiency of masked autoencoders (MAEs) as a self-supervised learning algorithm makes it possible to run this scaling experiment on an unassuming academic budget. We find that it is feasible to reach human-level object recognition capacity at sub-human scales of model size, data size, and image size, if these factors are scaled up simultaneously. To give a concrete example, we estimate that a 2.5B parameter ViT model trained with 20K hours (2.3 years) of human-like video data with a spatial resolution of 952x952 pixels should be able to reach roughly human-level accuracy on ImageNet. Human-level competence is thus achievable for a fundamental perceptual capability from human-like perceptual experience (human-like in both amount and type) with extremely generic learning algorithms and architectures and without any substantive inductive biases.
</details>
<details>
<summary>摘要</summary>
这篇论文询问了现有自动学习方法，如果继续扩大，能否达到人类级视觉对象识别能力，使用同样的类型和量的视觉经验。先前的工作只考虑了数据量的扩大。我们在这篇论文中考虑了同时扩大数据量、模型大小和图像分辨率。我们通过使用视Transformer模型，最大达633M参数（ViT-H/14），使用人类类似的视频数据（长、连续、主要是 Egocentric 视频），并将图像分辨率提高至476x476像素。我们发现，在同时扩大数据量、模型大小和图像分辨率的情况下，可以达到人类级对象识别能力，但是这些因素需要同时扩大。例如，我们估计，一个2.5B参数的 ViT 模型，通过20K小时（2.3年）的人类类似的视频数据，并在952x952像素的空间分辨率下进行训练，应该能够达到图像Net roughly human-level accuracy。这显示，通过人类类似的感知经验（包括同样的类型和量），使用极简的学习算法和架构，并不具备重要的逻辑假设，可以达到人类级的视觉对象识别能力。
</details></li>
</ul>
<hr>
<h2 id="DeRisk-An-Effective-Deep-Learning-Framework-for-Credit-Risk-Prediction-over-Real-World-Financial-Data"><a href="#DeRisk-An-Effective-Deep-Learning-Framework-for-Credit-Risk-Prediction-over-Real-World-Financial-Data" class="headerlink" title="DeRisk: An Effective Deep Learning Framework for Credit Risk Prediction over Real-World Financial Data"></a>DeRisk: An Effective Deep Learning Framework for Credit Risk Prediction over Real-World Financial Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03704">http://arxiv.org/abs/2308.03704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yancheng Liang, Jiajie Zhang, Hui Li, Xiaochen Liu, Yi Hu, Yong Wu, Jinyao Zhang, Yongyan Liu, Yi Wu</li>
<li>for: 预测信用风险（credit risk prediction）</li>
<li>methods: 使用深度学习模型（deep learning model）</li>
<li>results: 超越统计学习方法（statistical learning methods），实现更高的预测精度（higher prediction accuracy）<details>
<summary>Abstract</summary>
Despite the tremendous advances achieved over the past years by deep learning techniques, the latest risk prediction models for industrial applications still rely on highly handtuned stage-wised statistical learning tools, such as gradient boosting and random forest methods. Different from images or languages, real-world financial data are high-dimensional, sparse, noisy and extremely imbalanced, which makes deep neural network models particularly challenging to train and fragile in practice. In this work, we propose DeRisk, an effective deep learning risk prediction framework for credit risk prediction on real-world financial data. DeRisk is the first deep risk prediction model that outperforms statistical learning approaches deployed in our company's production system. We also perform extensive ablation studies on our method to present the most critical factors for the empirical success of DeRisk.
</details>
<details>
<summary>摘要</summary>
尽管深度学习技术在过去几年中取得了巨大的进步，但最新的风险预测模型仍然基于高度手动调整的阶段性统计学学习工具，如梯度提升和随机森林方法。不同于图像或语言，实际世界金融数据具有高维、稀疏、噪音和极度不均衡的特点，这使得深度神经网络模型在实践中特别困难要求和脆弱。在这项工作中，我们提出了DeRisk，一种高效的深度学习风险预测框架，用于实际世界金融数据的风险预测。DeRisk是我们公司生产系统中现在使用的统计学学习方法的首个深度风险预测模型，我们还进行了广泛的减少研究，以阐明DeRisk的成功的重要因素。
</details></li>
</ul>
<hr>
<h2 id="AgentBench-Evaluating-LLMs-as-Agents"><a href="#AgentBench-Evaluating-LLMs-as-Agents" class="headerlink" title="AgentBench: Evaluating LLMs as Agents"></a>AgentBench: Evaluating LLMs as Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03688">http://arxiv.org/abs/2308.03688</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thudm/agentbench">https://github.com/thudm/agentbench</a></li>
<li>paper_authors: Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, Jie Tang</li>
<li>for: 评估大语言模型（LLM）在实际世界中的应用，包括评估 LLM 的推理和决策能力在多turn open-ended generation setting 中。</li>
<li>methods: 作者提出了 AgentBench，一个多维度演化的测试准则，目前包括 8 个环境，用于评估 LLM 作为代理的能力。</li>
<li>results: 作者对 25 个 LLM（包括 API 和开源模型）进行了广泛的测试，发现Top商业 LLM 在复杂环境中表现出了强大的代理能力，但是与开源竞争对手之间存在显著的性能差异。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) are becoming increasingly smart and autonomous, targeting real-world pragmatic missions beyond traditional NLP tasks. As a result, there has been an urgent need to evaluate LLMs as agents on challenging tasks in interactive environments. We present AgentBench, a multi-dimensional evolving benchmark that currently consists of 8 distinct environments to assess LLM-as-Agent's reasoning and decision-making abilities in a multi-turn open-ended generation setting. Our extensive test over 25 LLMs (including APIs and open-sourced models) shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and open-sourced competitors. It also serves as a component of an ongoing project with wider coverage and deeper consideration towards systematic LLM evaluation. Datasets, environments, and an integrated evaluation package for AgentBench are released at https://github.com/THUDM/AgentBench
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Almost-sure-convergence-of-iterates-and-multipliers-in-stochastic-sequential-quadratic-optimization"><a href="#Almost-sure-convergence-of-iterates-and-multipliers-in-stochastic-sequential-quadratic-optimization" class="headerlink" title="Almost-sure convergence of iterates and multipliers in stochastic sequential quadratic optimization"></a>Almost-sure convergence of iterates and multipliers in stochastic sequential quadratic optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03687">http://arxiv.org/abs/2308.03687</a></li>
<li>repo_url: None</li>
<li>paper_authors: Frank E. Curtis, Xin Jiang, Qi Wang</li>
<li>for: 这种方法用于解决连续优化问题，具有非线性等式约束。</li>
<li>methods: 使用渐进随机Sequential Quadratic Programming（SQP）方法。</li>
<li>results: 提供了新的几乎确定的收敛保证，包括 primal 迭代、Lagrange多余阶度和稳定度量的收敛。<details>
<summary>Abstract</summary>
Stochastic sequential quadratic optimization (SQP) methods for solving continuous optimization problems with nonlinear equality constraints have attracted attention recently, such as for solving large-scale data-fitting problems subject to nonconvex constraints. However, for a recently proposed subclass of such methods that is built on the popular stochastic-gradient methodology from the unconstrained setting, convergence guarantees have been limited to the asymptotic convergence of the expected value of a stationarity measure to zero. This is in contrast to the unconstrained setting in which almost-sure convergence guarantees (of the gradient of the objective to zero) can be proved for stochastic-gradient-based methods. In this paper, new almost-sure convergence guarantees for the primal iterates, Lagrange multipliers, and stationarity measures generated by a stochastic SQP algorithm in this subclass of methods are proved. It is shown that the error in the Lagrange multipliers can be bounded by the distance of the primal iterate to a primal stationary point plus the error in the latest stochastic gradient estimate. It is further shown that, subject to certain assumptions, this latter error can be made to vanish by employing a running average of the Lagrange multipliers that are computed during the run of the algorithm. The results of numerical experiments are provided to demonstrate the proved theoretical guarantees.
</details>
<details>
<summary>摘要</summary>
This paper presents new almost-sure convergence guarantees for the primal iterates, Lagrange multipliers, and stationarity measures generated by a stochastic SQP algorithm in this subclass of methods. The error in the Lagrange multipliers can be bounded by the distance of the primal iterate to a primal stationary point plus the error in the latest stochastic gradient estimate. Furthermore, it is shown that this latter error can be made to vanish by employing a running average of the Lagrange multipliers computed during the run of the algorithm, subject to certain assumptions.Numerical experiments are provided to demonstrate the proved theoretical guarantees. These results demonstrate the effectiveness of the proposed method in solving continuous optimization problems with nonlinear equality constraints.
</details></li>
</ul>
<hr>
<h2 id="Linear-Convergence-Bounds-for-Diffusion-Models-via-Stochastic-Localization"><a href="#Linear-Convergence-Bounds-for-Diffusion-Models-via-Stochastic-Localization" class="headerlink" title="Linear Convergence Bounds for Diffusion Models via Stochastic Localization"></a>Linear Convergence Bounds for Diffusion Models via Stochastic Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03686">http://arxiv.org/abs/2308.03686</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joe Benton, Valentin De Bortoli, Arnaud Doucet, George Deligiannidis</li>
<li>for: 这个论文旨在提供高维数据分布中的近似样本生成方法，以及这些方法的拓扑分布的拓扑分布。</li>
<li>methods: 这个论文使用了扩散模型，这些模型可以在高维数据分布中生成近似样本。这些模型使用了$L^2$-精度分布 estimator，并且可以在不更改数据分布的情况下生成样本。</li>
<li>results: 这个论文提供了高维数据分布中扩散模型的新的拓扑分布 bound，这些 bound 是线性增长的（在数据维度上），并且不需要数据分布具有强平滑性。这个论文还证明了扩散模型只需要 $\tilde O(\frac{d \log^2(1&#x2F;\delta)}{\varepsilon^2})$ 步来近似于任何数据分布，其中 $\delta$ 是数据分布的噪声标准差，$\varepsilon$ 是近似度。<details>
<summary>Abstract</summary>
Diffusion models are a powerful method for generating approximate samples from high-dimensional data distributions. Several recent results have provided polynomial bounds on the convergence rate of such models, assuming $L^2$-accurate score estimators. However, up until now the best known such bounds were either superlinear in the data dimension or required strong smoothness assumptions. We provide the first convergence bounds which are linear in the data dimension (up to logarithmic factors) assuming only finite second moments of the data distribution. We show that diffusion models require at most $\tilde O(\frac{d \log^2(1/\delta)}{\varepsilon^2})$ steps to approximate an arbitrary data distribution on $\mathbb{R}^d$ corrupted with Gaussian noise of variance $\delta$ to within $\varepsilon^2$ in Kullback--Leibler divergence. Our proof builds on the Girsanov-based methods of previous works. We introduce a refined treatment of the error arising from the discretization of the reverse SDE, which is based on tools from stochastic localization.
</details>
<details>
<summary>摘要</summary>
Diffusion models are a powerful method for generating approximate samples from high-dimensional data distributions. Several recent results have provided polynomial bounds on the convergence rate of such models, assuming $L^2$-accurate score estimators. However, up until now the best known such bounds were either superlinear in the data dimension or required strong smoothness assumptions. We provide the first convergence bounds which are linear in the data dimension (up to logarithmic factors) assuming only finite second moments of the data distribution. We show that diffusion models require at most $\tilde O(\frac{d \log^2(1/\delta)}{\varepsilon^2})$ steps to approximate an arbitrary data distribution on $\mathbb{R}^d$ corrupted with Gaussian noise of variance $\delta$ to within $\varepsilon^2$ in Kullback--Leibler divergence. Our proof builds on the Girsanov-based methods of previous works. We introduce a refined treatment of the error arising from the discretization of the reverse SDE, which is based on tools from stochastic localization.Note: "Simplified Chinese" is a romanization of Chinese that uses the Chinese characters and their pronunciations, but not the traditional Chinese grammar and syntax. It is often used for computer interfaces and other contexts where a more simplified representation of Chinese is desired.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/08/cs.LG_2023_08_08/" data-id="clp9qz86b00pook88bxyia3ja" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/08/eess.IV_2023_08_08/" class="article-date">
  <time datetime="2023-08-08T09:00:00.000Z" itemprop="datePublished">2023-08-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/08/eess.IV_2023_08_08/">eess.IV - 2023-08-08</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Blur-aware-metric-depth-estimation-with-multi-focus-plenoptic-cameras"><a href="#Blur-aware-metric-depth-estimation-with-multi-focus-plenoptic-cameras" class="headerlink" title="Blur aware metric depth estimation with multi-focus plenoptic cameras"></a>Blur aware metric depth estimation with multi-focus plenoptic cameras</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04252">http://arxiv.org/abs/2308.04252</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/comsee-research/blade">https://github.com/comsee-research/blade</a></li>
<li>paper_authors: Mathieu Labussière, Céline Teulière, Omar Ait-Aider</li>
<li>for: 这个论文的目的是提出一种基于raw图像的多重焦距投影机采集的metric depth estimation算法，以提高对不同 фокус的束缚照片的 disparity估计。</li>
<li>methods: 该方法利用了 Raw 图像中的杂质信息，并通过结合对应关系和杂质信息来提高depth estimation。具体来说，该方法首先使用了 inverse projection 模型来计算depth map，然后通过scale factor来进行准确的深度估计。</li>
<li>results: 实验结果表明，通过引入杂质信息，可以提高depth estimation的准确性。该方法在实际场景中对3D复杂场景进行了测试，并与实际的3D探测器数据进行了比较。<details>
<summary>Abstract</summary>
While a traditional camera only captures one point of view of a scene, a plenoptic or light-field camera, is able to capture spatial and angular information in a single snapshot, enabling depth estimation from a single acquisition. In this paper, we present a new metric depth estimation algorithm using only raw images from a multi-focus plenoptic camera. The proposed approach is especially suited for the multi-focus configuration where several micro-lenses with different focal lengths are used. The main goal of our blur aware depth estimation (BLADE) approach is to improve disparity estimation for defocus stereo images by integrating both correspondence and defocus cues. We thus leverage blur information where it was previously considered a drawback. We explicitly derive an inverse projection model including the defocus blur providing depth estimates up to a scale factor. A method to calibrate the inverse model is then proposed. We thus take into account depth scaling to achieve precise and accurate metric depth estimates. Our results show that introducing defocus cues improves the depth estimation. We demonstrate the effectiveness of our framework and depth scaling calibration on relative depth estimation setups and on real-world 3D complex scenes with ground truth acquired with a 3D lidar scanner.
</details>
<details>
<summary>摘要</summary>
traditional camera 只能捕捉一个场景的一点视角，而 plenoptic 或 light-field camera 则可以在单个捕捉中捕捉场景的空间和方向信息，从而实现深度估计从单个获取。在这篇论文中，我们提出了一种基于 raw 图像的新的深度估计算法，使用多ocus plenoptic 相机。我们的 BLADE 方法旨在利用膨润信息来提高不焦相差图像中的 disparity 估计，因此我们可以更好地利用膨润信息。我们明确地 derivation 一个 inverse projection 模型，包括 defocus 膨润，以提供深度估计。我们还提出了一种准确把 calibration 方法，以考虑深度涨幅。我们的结果表明，在引入膨润信息后，深度估计得到了改善。我们在相对深度估计设置和实际世界3D复杂场景中进行了实验，并与3D激光扫描仪获取的实际深度数据进行了比较。
</details></li>
</ul>
<hr>
<h2 id="Under-Display-Camera-Image-Restoration-with-Scattering-Effect"><a href="#Under-Display-Camera-Image-Restoration-with-Scattering-Effect" class="headerlink" title="Under-Display Camera Image Restoration with Scattering Effect"></a>Under-Display Camera Image Restoration with Scattering Effect</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04163">http://arxiv.org/abs/2308.04163</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/namecantbenull/srudc">https://github.com/namecantbenull/srudc</a></li>
<li>paper_authors: Binbin Song, Xiangyu Chen, Shuning Xu, Jiantao Zhou</li>
<li>for: 提供了一种全屏视图体验，不受notationchs或孔隙等遮挡。</li>
<li>methods: 使用物理扩散模型来处理显示器的散射效应，并设计了一种两支网络来Suppress散射效应。</li>
<li>results: 在实验中，提出的方法在实际数据和synthesized数据上比现状态技术更高效。Here’s the full translation of the paper’s abstract in simplified Chinese:</li>
<li>for: 本研究旨在提供一种全屏视图体验，不受notationchs或孔隙等遮挡。</li>
<li>methods: 本研究使用物理扩散模型来处理显示器的散射效应，并设计了一种两支网络来Suppress散射效应。</li>
<li>results: 在实验中，提出的方法在实际数据和synthesized数据上比现状态技术更高效。Please note that the translation is done in a simplified Chinese format, which may not be as precise as the original English version.<details>
<summary>Abstract</summary>
The under-display camera (UDC) provides consumers with a full-screen visual experience without any obstruction due to notches or punched holes. However, the semi-transparent nature of the display inevitably introduces the severe degradation into UDC images. In this work, we address the UDC image restoration problem with the specific consideration of the scattering effect caused by the display. We explicitly model the scattering effect by treating the display as a piece of homogeneous scattering medium. With the physical model of the scattering effect, we improve the image formation pipeline for the image synthesis to construct a realistic UDC dataset with ground truths. To suppress the scattering effect for the eventual UDC image recovery, a two-branch restoration network is designed. More specifically, the scattering branch leverages global modeling capabilities of the channel-wise self-attention to estimate parameters of the scattering effect from degraded images. While the image branch exploits the local representation advantage of CNN to recover clear scenes, implicitly guided by the scattering branch. Extensive experiments are conducted on both real-world and synthesized data, demonstrating the superiority of the proposed method over the state-of-the-art UDC restoration techniques. The source code and dataset are available at \url{https://github.com/NamecantbeNULL/SRUDC}.
</details>
<details>
<summary>摘要</summary>
《下显示摄像头（UDC）提供了无障碍的全屏视觉体验，但 semi-透明显示器导致UDC图像受到严重抑制。在这种情况下，我们解决UDC图像恢复问题，特别是考虑显示器对图像的散射效应。我们直接模型散射效应，将显示器视为一个具有同样散射特性的媒体来进行物理模型。通过修改图像形成管道，我们构建了真实的UDC数据集，并提供了相应的真实参考值。为抑制散射效应，我们设计了两支分支网络：散射支分支利用通道wise自注意的全局模型来估算散射效应参数，而图像支分支则利用CNN的地方表示优势来恢复清晰场景，协同驱动散射支分支。我们对实际数据和生成数据进行了广泛的实验，证明了我们的方法在UDC恢复技术中的优越性。源代码和数据集可以在 \url{https://github.com/NamecantbeNULL/SRUDC} 中下载。》Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Towards-Top-Down-Stereoscopic-Image-Quality-Assessment-via-Stereo-Attention"><a href="#Towards-Top-Down-Stereoscopic-Image-Quality-Assessment-via-Stereo-Attention" class="headerlink" title="Towards Top-Down Stereoscopic Image Quality Assessment via Stereo Attention"></a>Towards Top-Down Stereoscopic Image Quality Assessment via Stereo Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04156">http://arxiv.org/abs/2308.04156</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fanning-zhang/satnet">https://github.com/fanning-zhang/satnet</a></li>
<li>paper_authors: Huilin Zhang, Sumei Li, Yongli Chang</li>
<li>for: 这篇论文主要是用于评估三维内容的图像品质评估（SIQA）方法。</li>
<li>methods: 该论文提出了一种基于顺序注意力的新网络方法，使用顶部下降的视角来引导评估过程。该方法可以从高级双目信号下降到低级单目信号，并在处理管道中进行进一步的均衡。</li>
<li>results: 实验结果表明，该方法可以更好地模拟人类视觉系统的特性，并超越当前的状态艺。code可以在<a target="_blank" rel="noopener" href="https://github.com/Fanning-Zhang/SATNet%E4%B8%8A%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/Fanning-Zhang/SATNet上下载。</a><details>
<summary>Abstract</summary>
Stereoscopic image quality assessment (SIQA) plays a crucial role in evaluating and improving the visual experience of 3D content. Existing binocular properties and attention-based methods for SIQA have achieved promising performance. However, these bottom-up approaches are inadequate in exploiting the inherent characteristics of the human visual system (HVS). This paper presents a novel network for SIQA via stereo attention, employing a top-down perspective to guide the quality assessment process. Our proposed method realizes the guidance from high-level binocular signals down to low-level monocular signals, while the binocular and monocular information can be calibrated progressively throughout the processing pipeline. We design a generalized Stereo AttenTion (SAT) block to implement the top-down philosophy in stereo perception. This block utilizes the fusion-generated attention map as a high-level binocular modulator, influencing the representation of two low-level monocular features. Additionally, we introduce an Energy Coefficient (EC) to account for recent findings indicating that binocular responses in the primate primary visual cortex are less than the sum of monocular responses. The adaptive EC can tune the magnitude of binocular response flexibly, thus enhancing the formation of robust binocular features within our framework. To extract the most discriminative quality information from the summation and subtraction of the two branches of monocular features, we utilize a dual-pooling strategy that applies min-pooling and max-pooling operations to the respective branches. Experimental results highlight the superiority of our top-down method in simulating the property of visual perception and advancing the state-of-the-art in the SIQA field. The code of this work is available at https://github.com/Fanning-Zhang/SATNet.
</details>
<details>
<summary>摘要</summary>
三通像质量评估（SIQA）在评估和改进三维内容的视觉体验方面扮演着关键性角色。现有的幂论和注意力基本方法已经实现了承诺性的表现。然而，这些底层方法不充分利用人类视觉系统（HVS）的内在特性。本文提出了一种新的网络 для SIQA，通过三通注意力，实现了顶部下向的指导评估过程。我们的提议方法可以从高级双目信号下降到低级单目信号，并在处理管道中进行进行步进式均衡。我们设计了一个通用的三通注意力块（SAT），以实现顶部下向的哲学思想在三通观察中。这个块利用生成的注意力地图作为高级双目模ulator，影响低级单目特征表示。此外，我们引入了能量系数（EC），以考虑最近的发现，表明双目响应在人类脑顶某处的辐射响应小于单目响应的总和。可以通过自适应EC调整幂论响应的大小，从而提高在我们框架中形成的稳定双目特征。为了从两个支线的单目特征批处中提取最有价值的质量信息，我们采用了双池策略，将各支线的单目特征批处应用最小池化和最大池化操作。实验结果表明，我们的顶部下向方法可以更好地模拟视觉响应和提高SIQA领域的状态。代码可以在https://github.com/Fanning-Zhang/SATNet上获取。
</details></li>
</ul>
<hr>
<h2 id="Physics-driven-universal-twin-image-removal-network-for-digital-in-line-holographic-microscopy"><a href="#Physics-driven-universal-twin-image-removal-network-for-digital-in-line-holographic-microscopy" class="headerlink" title="Physics-driven universal twin-image removal network for digital in-line holographic microscopy"></a>Physics-driven universal twin-image removal network for digital in-line holographic microscopy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04471">http://arxiv.org/abs/2308.04471</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mikołaj Rogalski, Piotr Arcab, Luiza Stanaszek, Vicente Micó, Chao Zuo, Maciej Trusiak</li>
<li>for: 这项研究的目的是提高数字内线干涉微镜技术（DIHM）的计算量相图像识别精度，以便更好地研究细胞移动、迁徙和生物微流体力学。</li>
<li>methods: 这项研究使用了深度学习解决方案UTIRnet，可以快速、稳定地Suppress twin-image noise，并且可以在不同的DIHM系统中实现。</li>
<li>results: 实验证明，UTIRnet可以准确地Suppress twin-image noise，并且保持输入干涉图像的一致性，从而提高计算量相图像识别的可靠性。例如，在live neural glial cell culture migration感测中，UTIRnet可以成功地捕捉细胞移动的动态过程。<details>
<summary>Abstract</summary>
Digital in-line holographic microscopy (DIHM) enables efficient and cost-effective computational quantitative phase imaging with a large field of view, making it valuable for studying cell motility, migration, and bio-microfluidics. However, the quality of DIHM reconstructions is compromised by twin-image noise, posing a significant challenge. Conventional methods for mitigating this noise involve complex hardware setups or time-consuming algorithms with often limited effectiveness. In this work, we propose UTIRnet, a deep learning solution for fast, robust, and universally applicable twin-image suppression, trained exclusively on numerically generated datasets. The availability of open-source UTIRnet codes facilitates its implementation in various DIHM systems without the need for extensive experimental training data. Notably, our network ensures the consistency of reconstruction results with input holograms, imparting a physics-based foundation and enhancing reliability compared to conventional deep learning approaches. Experimental verification was conducted among others on live neural glial cell culture migration sensing, which is crucial for neurodegenerative disease research.
</details>
<details>
<summary>摘要</summary>
数字内线推干微镜（DIHM）可以有效地和经济地实现计算量相对测量图像，具有大视野，这使其成为研究细胞活动、迁徙和生物微流体等领域的 valuables工具。然而，DIHM重建的质量受到双像噪声的限制，这成为一个 significante挑战。传统的方法用于 Mitigating这种噪声包括复杂的硬件设置或时间consuming的算法，其效果往往有限。在这种情况下，我们提出了UTIRnet，一种深度学习解决方案，用于快速、稳定、universally applicable的双像消除，该解决方案基于数字生成的数据集进行训练。UTIRnet的开源代码的可用性使得它可以在不同的 DIHM 系统中实现，无需详细的实验室训练数据。另外，我们的网络 garantizesthe consistency of reconstruction results with input holograms，从而为 DIHM 系统提供一个基于物理的基础，并提高了与传统深度学习方法相比的可靠性。实验证明了我们的UTIRnet在 live neural glial cell culture migration 感知等方面的表现。
</details></li>
</ul>
<hr>
<h2 id="Single-shot-experimental-numerical-twin-image-removal-in-lensless-digital-holographic-microscopy"><a href="#Single-shot-experimental-numerical-twin-image-removal-in-lensless-digital-holographic-microscopy" class="headerlink" title="Single-shot experimental-numerical twin-image removal in lensless digital holographic microscopy"></a>Single-shot experimental-numerical twin-image removal in lensless digital holographic microscopy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04131">http://arxiv.org/abs/2308.04131</a></li>
<li>repo_url: None</li>
<li>paper_authors: Piotr Arcab, Mikolaj Rogalski, Maciej Trusiak<br>for:LDHM imaging offers a large field-of-view and is crucial for high-throughput particle tracking and biomedical examination of cells and tissues, but is limited by the twin-image effect.methods:The proposed technique uses two-source off-axis hologram recording and a novel phase retrieval numerical algorithm to remove twin-image errors, providing a low-cost, out-of-laboratory imaging solution with enhanced precision.results:The proposed technique enables twin-image-free reconstruction of LDHM images, which improves the accuracy of technical and biomedical imaging applications. The results demonstrate the effectiveness of the proposed technique using phase test targets and cheek cells biosamples.<details>
<summary>Abstract</summary>
Lensless digital holographic microscopy (LDHM) offers very large field-of-view label-free imaging crucial, e.g., in high-throughput particle tracking and biomedical examination of cells and tissues. Compact layouts promote point-of-case and out-of-laboratory applications. The LDHM, based on the Gabor in-line holographic principle, is inherently spoiled by the twin-image effect, which complicates the quantitative analysis of reconstructed phase and amplitude maps. Popular family of solutions consists of numerical methods, which tend to minimize twin-image upon iterative process based on data redundancy. Additional hologram recordings are needed, and final results heavily depend on the algorithmic parameters, however. In this contribution we present a novel single-shot experimental-numerical twin-image removal technique for LDHM. It leverages two-source off-axis hologram recording deploying simple fiber splitter. Additionally, we introduce a novel phase retrieval numerical algorithm specifically tailored to the acquired holograms, that provides twin-image-free reconstruction without compromising the resolution. We quantitatively and qualitatively verify proposed method employing phase test target and cheek cells biosample. The results demonstrate that the proposed technique enables low-cost, out-of-laboratory LDHM imaging with enhanced precision, achieved through the elimination of twin-image errors. This advancement opens new avenues for more accurate technical and biomedical imaging applications using LDHM, particularly in scenarios where cost-effective and portable imaging solutions are desired.
</details>
<details>
<summary>摘要</summary>
LDHM（无镜像数字折射微镜）提供了很大的场视野，无标签的图像重要，如高通过率粒子跟踪和生物医学Cells和组织的检查。嵌入式的设计促进了点位应用和出厂应用。基于Gabor直线折射原理的LDHM受到双像效应的干扰，这使得量化分析重constructed的相位和振幅图表变得复杂。通用的解决方案包括数学方法，这些方法通过基于数据重复的迭代过程来减少双像效应。然而，这些方法需要额外的折射agram记录，并且最终结果受到算法参数的影响。在这篇论文中，我们提出了一种新的单 shot实验数字twain-image removedtechnique for LDHM。它利用了两个源偏心折射agram记录，使用简单的纤维Splitter。此外，我们还提出了一种专门为获得的折射agram设计的数学算法，可以在无需COMPROMISE的分辨率情况下提供无双像效应的重建。我们使用测试target和唾液细胞样本来证明提出的方法的有效性。结果表明，提出的方法可以在低成本和出厂环境中提供高精度的LDHM成像，并且消除了双像效应。这一进展开 up新的可靠和可搬移的LDHM成像应用，特别是在成本效益和出厂环境中。
</details></li>
</ul>
<hr>
<h2 id="Non-Intrusive-Electric-Load-Monitoring-Approach-Based-on-Current-Feature-Visualization-for-Smart-Energy-Management"><a href="#Non-Intrusive-Electric-Load-Monitoring-Approach-Based-on-Current-Feature-Visualization-for-Smart-Energy-Management" class="headerlink" title="Non-Intrusive Electric Load Monitoring Approach Based on Current Feature Visualization for Smart Energy Management"></a>Non-Intrusive Electric Load Monitoring Approach Based on Current Feature Visualization for Smart Energy Management</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11627">http://arxiv.org/abs/2308.11627</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiwen Xu, Dengfeng Liu, Liangtao Huang, Zhiquan Lin, Tiesong Zhao, Sam Kwong</li>
<li>for: 这个研究旨在为智能城市提供一个经济可行的电力管理系统，特别是对于大规模网络中的电力负载进行监控和分析。</li>
<li>methods: 本研究使用了人工智能的受欢迎计算机视觉技术，设计了一种非侵入式负载监控方法，通过将一维电流信号映射到二维颜色特征图像中，然后使用U型深度神经网络进行负载识别。</li>
<li>results: 实验结果显示，本方法在公共和私人数据集上均达到了超过其他方法的性能，因此支持了大规模互联网智能系统的有效能源管理。<details>
<summary>Abstract</summary>
The state-of-the-art smart city has been calling for an economic but efficient energy management over large-scale network, especially for the electric power system. It is a critical issue to monitor, analyze and control electric loads of all users in system. In this paper, we employ the popular computer vision techniques of AI to design a non-invasive load monitoring method for smart electric energy management. First of all, we utilize both signal transforms (including wavelet transform and discrete Fourier transform) and Gramian Angular Field (GAF) methods to map one-dimensional current signals onto two-dimensional color feature images. Second, we propose to recognize all electric loads from color feature images using a U-shape deep neural network with multi-scale feature extraction and attention mechanism. Third, we design our method as a cloud-based, non-invasive monitoring of all users, thereby saving energy cost during electric power system control. Experimental results on both public and our private datasets have demonstrated our method achieves superior performances than its peers, and thus supports efficient energy management over large-scale Internet of Things (IoT).
</details>
<details>
<summary>摘要</summary>
现代智能城市呼吁了一种经济高效的能源管理方法，特别是电力系统。监测、分析和控制所有用户的电力负荷是一个关键问题。在这篇论文中，我们采用了流行的计算机视觉技术，设计了一种不侵入的负荷监测方法。首先，我们利用了卷积变换（包括浪干变换和离散傅里叶变换）和 Gramian Angular Field（GAF）方法将一维电流信号映射到二维颜色特征图像上。其次，我们提出了通过 U-型深度神经网络（包括多级特征提取和注意机制）来识别所有的电力负荷。最后，我们设计了一种云端、不侵入的监测方法，以便在互联网物联网（IoT）中实现有效的能源管理。实验结果表明，我们的方法在公共数据集和私人数据集上都达到了比其他方法更高的性能，因此支持了大规模互联网物联网中的有效能源管理。
</details></li>
</ul>
<hr>
<h2 id="Weakly-Semi-Supervised-Detection-in-Lung-Ultrasound-Videos"><a href="#Weakly-Semi-Supervised-Detection-in-Lung-Ultrasound-Videos" class="headerlink" title="Weakly Semi-Supervised Detection in Lung Ultrasound Videos"></a>Weakly Semi-Supervised Detection in Lung Ultrasound Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04463">http://arxiv.org/abs/2308.04463</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiahong Ouyang, Li Chen, Gary Y. Li, Naveen Balaraju, Shubham Patil, Courosh Mehanian, Sourabh Kulhare, Rachel Millin, Kenton W. Gregory, Cynthia R. Gregory, Meihua Zhu, David O. Kessler, Laurie Malia, Almaz Dessie, Joni Rabiner, Di Coneybeare, Bo Shopsin, Andrew Hersh, Cristian Madar, Jeffrey Shupp, Laura S. Johnson, Jacob Avila, Kristin Dwyer, Peter Weimersheimer, Balasundar Raju, Jochen Kruecker, Alvin Chen</li>
<li>for: 提高医疗视频中物体检测精度和Robustness，使用弱监督学习方法。</li>
<li>methods:  aggregate各个检测预测结果为视频级别预测，并通过视频级别损失进行更多的监督。还引入了基于弱监督的教师-学生训练策略，包括如何改进pseudo标签质量和自适应调整知识传递between teacher和学生网络。</li>
<li>results: 对医学ultrasound视频中肺聚集（如COVID-19肺炎）的检测精度和可靠性进行了改进，比基eline semi-supervised模型更高，同时提高了数据和注释的使用效率。<details>
<summary>Abstract</summary>
Frame-by-frame annotation of bounding boxes by clinical experts is often required to train fully supervised object detection models on medical video data. We propose a method for improving object detection in medical videos through weak supervision from video-level labels. More concretely, we aggregate individual detection predictions into video-level predictions and extend a teacher-student training strategy to provide additional supervision via a video-level loss. We also introduce improvements to the underlying teacher-student framework, including methods to improve the quality of pseudo-labels based on weak supervision and adaptive schemes to optimize knowledge transfer between the student and teacher networks. We apply this approach to the clinically important task of detecting lung consolidations (seen in respiratory infections such as COVID-19 pneumonia) in medical ultrasound videos. Experiments reveal that our framework improves detection accuracy and robustness compared to baseline semi-supervised models, and improves efficiency in data and annotation usage.
</details>
<details>
<summary>摘要</summary>
<SYS>    < Lang="zh-CN" >        框架fram by frame的注意点标注由医疗专家是训练完全指导的物体检测模型的医学视频数据的常见需求。我们提出一种改进医学视频中物体检测的方法，通过弱指导来提高物体检测的准确性和稳定性。具体来说，我们将个体检测预测结果聚合到视频级别预测中，并将视频级别损失扩展到教师学生训练策略中，以提供额外的指导。我们还引入了改进教师学生框架的方法，包括基于弱指导的pseudo标签质量改进和adaptive调整知识传递 между教师和学生网络。我们在诊断肺脏聚集（COVID-19感染引起的肺炎）的医学超声视频中应用这种方法。实验表明，我们的框架可以提高检测精度和稳定性，并提高数据和注释使用效率。    </ Lang></SYS>Note that Simplified Chinese is used in the translation, as it is the more commonly used standard for scientific and technical writing in China.
</details></li>
</ul>
<hr>
<h2 id="DefCor-Net-Physics-Aware-Ultrasound-Deformation-Correction"><a href="#DefCor-Net-Physics-Aware-Ultrasound-Deformation-Correction" class="headerlink" title="DefCor-Net: Physics-Aware Ultrasound Deformation Correction"></a>DefCor-Net: Physics-Aware Ultrasound Deformation Correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03865">http://arxiv.org/abs/2308.03865</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/karolinezhy/defcornet">https://github.com/karolinezhy/defcornet</a></li>
<li>paper_authors: Zhongliang Jiang, Yue Zhou, Dongliang Cao, Nassir Navab</li>
<li>for: 这篇论文旨在提高ultrasound（US）图像取得中的形状修正精度，以便精确和一致的诊断，特别是在电脑助诊中。</li>
<li>methods: 本文提出了一个基于多层深度学习网络的新的体内质量测定方法（DefCor-Net），通过粗细对称的多层网络，从粗细层到细节层进行对应，以提高材料对称性的测量精度。</li>
<li>results: 实验结果显示，使用DefCor-Net可以对US图像进行高精度的形状修正，从$14.3\pm20.9$提高至$82.6\pm12.1$（当力量为$6N$时），这表明DefCor-Net可以实现体内质量测定的灵活性和高精度。<details>
<summary>Abstract</summary>
The recovery of morphologically accurate anatomical images from deformed ones is challenging in ultrasound (US) image acquisition, but crucial to accurate and consistent diagnosis, particularly in the emerging field of computer-assisted diagnosis. This article presents a novel anatomy-aware deformation correction approach based on a coarse-to-fine, multi-scale deep neural network (DefCor-Net). To achieve pixel-wise performance, DefCor-Net incorporates biomedical knowledge by estimating pixel-wise stiffness online using a U-shaped feature extractor. The deformation field is then computed using polynomial regression by integrating the measured force applied by the US probe. Based on real-time estimation of pixel-by-pixel tissue properties, the learning-based approach enables the potential for anatomy-aware deformation correction. To demonstrate the effectiveness of the proposed DefCor-Net, images recorded at multiple locations on forearms and upper arms of six volunteers are used to train and validate DefCor-Net. The results demonstrate that DefCor-Net can significantly improve the accuracy of deformation correction to recover the original geometry (Dice Coefficient: from $14.3\pm20.9$ to $82.6\pm12.1$ when the force is $6N$).
</details>
<details>
<summary>摘要</summary>
“ Ultrasound（US）图像获取中，修复变形的 morphologically 精准 анатомиче图像 recover 是一项挑战，但是对医学诊断的准确性和一致性至关重要，特别是在计算机助成诊断领域。本文提出了一种基于多尺度深度神经网络（DefCor-Net）的新型 anatomy-aware deformation correction 方法。通过在线计算像素刚性的方法，DefCor-Net 可以在实时计算像素刚性的基础上进行学习基于图像材料的 deformation field 计算。通过使用 U-shaped 特征提取器，DefCor-Net 可以在每个像素位置上计算刚性，从而实现像素级别的性能。为了证明 DefCor-Net 的有效性，本文使用了多个臂部和上臂部的 six 名志愿者所记录的图像进行训练和验证。结果表明，DefCor-Net 可以显著提高 deformation correction 的准确性，从 $14.3\pm20.9$ 提高到 $82.6\pm12.1$（当力度为 $6N$）。”Note that Simplified Chinese is used in this translation, as it is the most widely used standard for Chinese writing in mainland China. If you prefer Traditional Chinese, I can provide that version as well.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/08/eess.IV_2023_08_08/" data-id="clp9qz8di018mok8849odappd" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/07/cs.SD_2023_08_07/" class="article-date">
  <time datetime="2023-08-07T15:00:00.000Z" itemprop="datePublished">2023-08-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/07/cs.SD_2023_08_07/">cs.SD - 2023-08-07</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Active-Noise-Control-based-on-the-Momentum-Multichannel-Normalized-Filtered-x-Least-Mean-Square-Algorithm"><a href="#Active-Noise-Control-based-on-the-Momentum-Multichannel-Normalized-Filtered-x-Least-Mean-Square-Algorithm" class="headerlink" title="Active Noise Control based on the Momentum Multichannel Normalized Filtered-x Least Mean Square Algorithm"></a>Active Noise Control based on the Momentum Multichannel Normalized Filtered-x Least Mean Square Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03684">http://arxiv.org/abs/2308.03684</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongyuan Shi, Woon-Seng Gan, Bhan Lam, Shulin Wen, Xiaoyi Shen</li>
<li>for: 实现多 канал活动噪声控制 (MCANC) 中的广泛噪声处理区域。</li>
<li>methods: 使用 filter-x least mean square (FxLMS) 算法，但它的快速减退速度使得在面对快速变化的噪声时，表现不佳。此外，噪声功率的变化也会损害算法的稳定性。</li>
<li>results: 通过与征算法结合了惯性方法，使得算法更加快速地趋向稳定点，并且更好地避免了主要噪声功率的干扰。<details>
<summary>Abstract</summary>
Multichannel active noise control (MCANC) is widely utilized to achieve significant noise cancellation area in the complicated acoustic field. Meanwhile, the filter-x least mean square (FxLMS) algorithm gradually becomes the benchmark solution for the implementation of MCANC due to its low computational complexity. However, its slow convergence speed more or less undermines the performance of dealing with quickly varying disturbances, such as piling noise. Furthermore, the noise power variation also deteriorates the robustness of the algorithm when it adopts the fixed step size. To solve these issues, we integrated the normalized multichannel FxLMS with the momentum method, which hence, effectively avoids the interference of the primary noise power and accelerates the convergence of the algorithm. To validate its effectiveness, we deployed this algorithm in a multichannel noise control window to control the real machine noise.
</details>
<details>
<summary>摘要</summary>
多通道活动噪声控制（MCANC）广泛应用于复杂的噪声场中实现显著的噪声抑制面积。同时，Filter-x最小二乘（FxLMS）算法逐渐成为MCANC实现的标准解决方案，因为它的计算复杂性较低。然而，它的慢速收敛速度在面对快变化的干扰时，很大程度地降低了性能。此外，噪声功率变化也降低了算法的稳定性，特别是当采用固定步长时。为解决这些问题，我们将normalized multichannel FxLMS与势量方法结合，从而有效地避免了主要噪声功率的干扰和加速了算法的收敛。为验证其效果，我们在多通道噪声控制窗口中应用了这种算法来控制实际机器噪声。
</details></li>
</ul>
<hr>
<h2 id="AudioVMAF-Audio-Quality-Prediction-with-VMAF"><a href="#AudioVMAF-Audio-Quality-Prediction-with-VMAF" class="headerlink" title="AudioVMAF: Audio Quality Prediction with VMAF"></a>AudioVMAF: Audio Quality Prediction with VMAF</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03437">http://arxiv.org/abs/2308.03437</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arijit Biswas, Harald Mundt</li>
<li>for: 提高编码音频质量评估的精度</li>
<li>methods: 基于现有VMAF的听觉前端创建参考视频和编码spectrogram，并扩展VMAF来评估编码音频质量</li>
<li>results: 提出的AudioVMAF系统在带宽限制场景下表现出更高的预测精度，并在比较已有视觉质量特征与专门的音频质量指标（ViSQOL-v3）中显示出7.8%和2.0%的显著提高。<details>
<summary>Abstract</summary>
Video Multimethod Assessment Fusion (VMAF) [1], [2], [3] is a popular tool in the industry for measuring coded video quality. In this study, we propose an auditory-inspired frontend in existing VMAF for creating videos of reference and coded spectrograms, and extended VMAF for measuring coded audio quality. We name our system AudioVMAF. We demonstrate that image replication is capable of further enhancing prediction accuracy, especially when band-limited anchors are present. The proposed method significantly outperforms all existing visual quality features repurposed for audio, and even demonstrates a significant overall improvement of 7.8% and 2.0% of Pearson and Spearman rank correlation coefficient, respectively, over a dedicated audio quality metric (ViSQOL-v3 [4]) also inspired from the image domain.
</details>
<details>
<summary>摘要</summary>
视频多方法评估融合（VMAF）是行业中广泛使用的视频质量评估工具。在本研究中，我们提出一种听力 inspirited 的前端，用于创建参考视频和编码спектрограм，并扩展了VMAF以测量编码音频质量。我们称之为AudioVMAF。我们示出，图像复制能够进一步提高预测精度，特别是在存在带限 anchors 时。我们的方法在所有现有的视觉质量特征的抽象下表现出色，并在ViSQOL-v3 （4）中显示了 significan 7.8% 和 2.0% 的潘森和斯宾塞排名相关系数，分别。
</details></li>
</ul>
<hr>
<h2 id="Improving-Deep-Attractor-Network-by-BGRU-and-GMM-for-Speech-Separation"><a href="#Improving-Deep-Attractor-Network-by-BGRU-and-GMM-for-Speech-Separation" class="headerlink" title="Improving Deep Attractor Network by BGRU and GMM for Speech Separation"></a>Improving Deep Attractor Network by BGRU and GMM for Speech Separation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03332">http://arxiv.org/abs/2308.03332</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rawad Melhem, Assef Jafar, Riad Hamadeh</li>
<li>for: 这个论文是为了提出一种简化了DANet模型，使其更加强大和简单的 speech separation 模型。</li>
<li>methods: 该模型使用了bidirectional gated neural network (BGRU) 代替了 bidirectional long short-term memory (BLSTM)，并使用 Gaussian Mixture Model (GMM) 作为聚类算法来降低复杂性和提高学习速度和准确性。</li>
<li>results: 在使用 TIMIT 语音数据集进行评估时，提出的模型可以达到12.3 dB和2.94的 SDR 和 PESQ 分数，比原始 DANet 模型更好。此外，该模型还减少了20.7%和17.9%的参数数量和训练时间。最后，该模型在混合阿拉伯语音信号上进行评估，得到了更好的结果。<details>
<summary>Abstract</summary>
Deep Attractor Network (DANet) is the state-of-the-art technique in speech separation field, which uses Bidirectional Long Short-Term Memory (BLSTM), but the complexity of the DANet model is very high. In this paper, a simplified and powerful DANet model is proposed using Bidirectional Gated neural network (BGRU) instead of BLSTM. The Gaussian Mixture Model (GMM) other than the k-means was applied in DANet as a clustering algorithm to reduce the complexity and increase the learning speed and accuracy. The metrics used in this paper are Signal to Distortion Ratio (SDR), Signal to Interference Ratio (SIR), Signal to Artifact Ratio (SAR), and Perceptual Evaluation Speech Quality (PESQ) score. Two speaker mixture datasets from TIMIT corpus were prepared to evaluate the proposed model, and the system achieved 12.3 dB and 2.94 for SDR and PESQ scores respectively, which were better than the original DANet model. Other improvements were 20.7% and 17.9% in the number of parameters and time training, respectively. The model was applied on mixed Arabic speech signals and the results were better than that in English.
</details>
<details>
<summary>摘要</summary>
深度吸引网络（DANet）是现代语音分离领域的状态元技术，使用了双向长短期记忆（BLSTM），但DANet模型的复杂性很高。在本文中，一种简化了DANet模型，使用了双向闭合神经网络（BGRU）而不是BLSTM。 Gaussian Mixture Model（GMM）在DANet中作为聚类算法来降低复杂性和提高学习速度和准确性。本文使用的度量包括信号质量至噪声比（SDR）、信号质量至干扰比（SIR）、信号质量至噪声比（SAR）和语音质量评价分数（PESQ）。使用TIMIT corpus中的两个说话者混合数据集进行评估，提出的模型在SDR和PESQ分数上分别达到12.3 dB和2.94，比原始DANet模型更好。此外，模型的参数数量和训练时间都有20.7%和17.9%的下降。该模型在混合阿拉伯语音信号上得到了更好的结果，比英语更好。
</details></li>
</ul>
<hr>
<h2 id="SeACo-Paraformer-A-Non-Autoregressive-ASR-System-with-Flexible-and-Effective-Hotword-Customization-Ability"><a href="#SeACo-Paraformer-A-Non-Autoregressive-ASR-System-with-Flexible-and-Effective-Hotword-Customization-Ability" class="headerlink" title="SeACo-Paraformer: A Non-Autoregressive ASR System with Flexible and Effective Hotword Customization Ability"></a>SeACo-Paraformer: A Non-Autoregressive ASR System with Flexible and Effective Hotword Customization Ability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03266">http://arxiv.org/abs/2308.03266</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/r1ckshi/seaco-paraformer">https://github.com/r1ckshi/seaco-paraformer</a></li>
<li>paper_authors: Xian Shi, Yexin Yang, Zerui Li, Shiliang Zhang</li>
<li>for: 提高 ASR 系统中热词定制的灵活性和效果性。</li>
<li>methods: 提出 Semantic-augmented Contextual-Paraformer (SeACo-Paraformer) 模型，结合 AED 模型的精度、NAR 模型的效率和Contextualization 能力，实现热词定制的灵活性和效果性。</li>
<li>results: 在50,000小时工业大数据实验中，提出的模型比强基eline在定制和总 ASR 任务中表现出色，同时提出了一种高效的大规模热词筛选方法。 industrial models 和两个热词测试集都已经公开。<details>
<summary>Abstract</summary>
Hotword customization is one of the important issues remained in ASR field - it is of value to enable users of ASR systems to customize names of entities, persons and other phrases. The past few years have seen both implicit and explicit modeling strategies for ASR contextualization developed. While these approaches have performed adequately, they still exhibit certain shortcomings such as instability in effectiveness. In this paper we propose Semantic-augmented Contextual-Paraformer (SeACo-Paraformer) a novel NAR based ASR system with flexible and effective hotword customization ability. It combines the accuracy of the AED-based model, the efficiency of the NAR model, and the excellent performance in contextualization. In 50,000 hours industrial big data experiments, our proposed model outperforms strong baselines in customization and general ASR tasks. Besides, we explore an efficient way to filter large scale incoming hotwords for further improvement. The source codes and industrial models proposed and compared are all opened as well as two hotword test sets.
</details>
<details>
<summary>摘要</summary>
“热词自定义是ASR领域中一个重要的 issuesthat is of great value to enable users of ASR systems to customize names of entities, persons, and other phrases. Recently, both implicit and explicit modeling strategies for ASR contextualization have been developed, but they still have some shortcomings such as instability in effectiveness. In this paper, we propose a novel NAR-based ASR system with flexible and effective hotword customization ability, called Semantic-augmented Contextual-Paraformer (SeACo-Paraformer). It combines the accuracy of the AED-based model, the efficiency of the NAR model, and the excellent performance in contextualization. In 50,000 hours of industrial big data experiments, our proposed model outperforms strong baselines in customization and general ASR tasks. Furthermore, we explore an efficient way to filter large-scale incoming hotwords for further improvement. The source codes and industrial models proposed and compared are all open, as well as two hotword test sets.”Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Investigation-of-Self-supervised-Pre-trained-Models-for-Classification-of-Voice-Quality-from-Speech-and-Neck-Surface-Accelerometer-Signals"><a href="#Investigation-of-Self-supervised-Pre-trained-Models-for-Classification-of-Voice-Quality-from-Speech-and-Neck-Surface-Accelerometer-Signals" class="headerlink" title="Investigation of Self-supervised Pre-trained Models for Classification of Voice Quality from Speech and Neck Surface Accelerometer Signals"></a>Investigation of Self-supervised Pre-trained Models for Classification of Voice Quality from Speech and Neck Surface Accelerometer Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03226">http://arxiv.org/abs/2308.03226</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sudarsana Reddy Kadiri, Farhad Javanmardi, Paavo Alku</li>
<li>for: 这个研究的目的是研究自动分类声音质量的方法，特别是使用同时记录的语音和脊梁压力仪（NSA）信号作为输入，并提取MFCCs和颚部源特征。</li>
<li>methods: 这个研究使用了三个自助学习模型（wav2vec2-BASE、wav2vec2-LARGE和HuBERT）生成的特征，以及支持向量机（SVM）和卷积神经网络（CNN）作为分类器。此外，研究还对颚部源波形和原始信号波形进行了两种信号处理方法（ quasi-closed phase（QCP）颚部逆滤波和零频 filtering（ZFF））来生成颚部源波形。</li>
<li>results: 研究发现，使用 NSA 输入可以比语音输入更好地进行分类，而且使用预训练模型生成的特征可以提高分类精度，特别是对于语音和 NSA 输入。此外，研究还发现 HuBERT 特征在分类任务中表现更好于 wav2vec2-BASE 和 wav2vec2-LARGE 特征。<details>
<summary>Abstract</summary>
Prior studies in the automatic classification of voice quality have mainly studied the use of the acoustic speech signal as input. Recently, a few studies have been carried out by jointly using both speech and neck surface accelerometer (NSA) signals as inputs, and by extracting MFCCs and glottal source features. This study examines simultaneously-recorded speech and NSA signals in the classification of voice quality (breathy, modal, and pressed) using features derived from three self-supervised pre-trained models (wav2vec2-BASE, wav2vec2-LARGE, and HuBERT) and using a SVM as well as CNNs as classifiers. Furthermore, the effectiveness of the pre-trained models is compared in feature extraction between glottal source waveforms and raw signal waveforms for both speech and NSA inputs. Using two signal processing methods (quasi-closed phase (QCP) glottal inverse filtering and zero frequency filtering (ZFF)), glottal source waveforms are estimated from both speech and NSA signals. The study has three main goals: (1) to study whether features derived from pre-trained models improve classification accuracy compared to conventional features (spectrogram, mel-spectrogram, MFCCs, i-vector, and x-vector), (2) to investigate which of the two modalities (speech vs. NSA) is more effective in the classification task with pre-trained model-based features, and (3) to evaluate whether the deep learning-based CNN classifier can enhance the classification accuracy in comparison to the SVM classifier. The results revealed that the use of the NSA input showed better classification performance compared to the speech signal. Between the features, the pre-trained model-based features showed better classification accuracies, both for speech and NSA inputs compared to the conventional features. It was also found that the HuBERT features performed better than the wav2vec2-BASE and wav2vec2-LARGE features.
</details>
<details>
<summary>摘要</summary>
先前的研究主要是使用语音信号来自动分类voice quality，现在有一些研究使用语音和颈部表面加速器（NSA）信号同时录制，并提取MFCCs和颈部源特征。本研究通过使用三种自动学习模型（wav2vec2-BASE、wav2vec2-LARGE和HuBERT）提取特征，使用SVM和CNN作为分类器，研究语音和NSA信号同时录制的voice quality分类效果。此外，还比较了三种模型在特征提取中的效果，以及使用不同的信号处理方法（ quasi-closed phase颈部逆推和zero frequency filtering）来提取颈部源波形。研究的主要目标是：1. 研究使用预训练模型提取的特征是否能够提高分类精度，比较传统特征（spectrogram、mel-spectrogram、MFCCs、i-vector和x-vector）的效果。2. 研究语音和NSA信号中哪一种Modalities更有效iveness在分类任务中，并使用预训练模型基于特征进行分类。3. 研究使用深度学习基于CNN的分类器是否能够提高分类精度，相比SVM分类器。结果表明，使用NSA输入信号可以实现更好的分类性能，而且使用预训练模型基于特征可以提高分类精度，无论是语音还是NSA输入信号。此外，HuBERT特征也表现出了更高的分类精度。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/07/cs.SD_2023_08_07/" data-id="clp9qz89c00xmok8825x17pbw" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_08_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/07/cs.CV_2023_08_07/" class="article-date">
  <time datetime="2023-08-07T13:00:00.000Z" itemprop="datePublished">2023-08-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/07/cs.CV_2023_08_07/">cs.CV - 2023-08-07</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Improving-FHB-Screening-in-Wheat-Breeding-Using-an-Efficient-Transformer-Model"><a href="#Improving-FHB-Screening-in-Wheat-Breeding-Using-an-Efficient-Transformer-Model" class="headerlink" title="Improving FHB Screening in Wheat Breeding Using an Efficient Transformer Model"></a>Improving FHB Screening in Wheat Breeding Using an Efficient Transformer Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03670">http://arxiv.org/abs/2308.03670</a></li>
<li>repo_url: None</li>
<li>paper_authors: Babak Azad, Ahmed Abdalla, Kwanghee Won, Ali Mirzakhani Nafchi<br>for: 这个研究旨在开发一个基于视觉 трансформер的抗疫菌头病毒检测方法，以提高小麦和黑麦生产计划中的抗疫菌头病毒检测效率和精度。methods: 这个方法使用了一种新的 Context Bridge，将U-Net网络的本地表现力与视觉 трансформер模型的全球自我注意力机制结合起来，以提高模型的多对多关联能力和模型化能力。此外，这个方法使用了Efficient Self-attention机制，取代了原始视觉 трансформер模型的标准注意力机制，以减少模型的复杂度。results: 这个研究透过广泛的实验和评估，展示了这个基于视觉 трансформер的方法在抗疫菌头病毒检测任务中的效果。<details>
<summary>Abstract</summary>
Fusarium head blight is a devastating disease that causes significant economic losses annually on small grains. Efficiency, accuracy, and timely detection of FHB in the resistance screening are critical for wheat and barley breeding programs. In recent years, various image processing techniques have been developed using supervised machine learning algorithms for the early detection of FHB. The state-of-the-art convolutional neural network-based methods, such as U-Net, employ a series of encoding blocks to create a local representation and a series of decoding blocks to capture the semantic relations. However, these methods are not often capable of long-range modeling dependencies inside the input data, and their ability to model multi-scale objects with significant variations in texture and shape is limited. Vision transformers as alternative architectures with innate global self-attention mechanisms for sequence-to-sequence prediction, due to insufficient low-level details, may also limit localization capabilities. To overcome these limitations, a new Context Bridge is proposed to integrate the local representation capability of the U-Net network in the transformer model. In addition, the standard attention mechanism of the original transformer is replaced with Efficient Self-attention, which is less complicated than other state-of-the-art methods. To train the proposed network, 12,000 wheat images from an FHB-inoculated wheat field at the SDSU research farm in Volga, SD, were captured. In addition to healthy and unhealthy plants, these images encompass various stages of the disease. A team of expert pathologists annotated the images for training and evaluating the developed model. As a result, the effectiveness of the transformer-based method for FHB-disease detection, through extensive experiments across typical tasks for plant image segmentation, is demonstrated.
</details>
<details>
<summary>摘要</summary>
fusarium 头炎是一种致命的疾病，每年在小谷物上造成重大经济损失。效率、准确性和时效检测 fusarium 头炎在小谷物抗性培养计划中是关键。在过去的几年中，一些基于超vised机器学习算法的图像处理技术被开发出来，用于早期检测 fusarium 头炎。现有的 convolutional neural network（CNN）方法，如 U-Net，通过一系列的编码块创建地方表示，并通过一系列的解码块捕捉 semantic 关系。但这些方法通常无法模型输入数据中的远程相互关系，而且对于具有不同文化和形状的多尺度对象进行模型化也有限制。为了突破这些限制，我们提出了一种新的 Context Bridge，用于将 U-Net 网络的本地表示能力integrated into transformer模型中。此外，原始 transformer 模型的标准注意机制被 replaced with Efficient Self-attention，这是比其他当前状态最简单的方法。为了训练我们提出的网络，我们使用了12,000个小谷物图像，这些图像来自于南达大学农业实验室Volga, SD的FHB-感染小谷物田。除了健康和病毒植物之外，这些图像还包括不同阶段的疾病。一 команópathologists expert annotated the images for training and evaluating the developed model. As a result, the effectiveness of the transformer-based method for FHB-disease detection, through extensive experiments across typical tasks for plant image segmentation, is demonstrated.
</details></li>
</ul>
<hr>
<h2 id="Distributionally-Robust-Classification-on-a-Data-Budget"><a href="#Distributionally-Robust-Classification-on-a-Data-Budget" class="headerlink" title="Distributionally Robust Classification on a Data Budget"></a>Distributionally Robust Classification on a Data Budget</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03821">http://arxiv.org/abs/2308.03821</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/penfever/vlhub">https://github.com/penfever/vlhub</a></li>
<li>paper_authors: Benjamin Feuer, Ameya Joshi, Minh Pham, Chinmay Hegde</li>
<li>For: The paper aims to address the challenge of training robust deep learning models under distribution shifts, specifically in domains with limited data budgets.* Methods: The authors introduce a new dataset called JANuS (Joint Annotations and Names Set) and perform a series of carefully controlled investigations to evaluate the factors contributing to robustness in image classification. They use a standard ResNet-50 model trained with the cross-entropy loss on 2.4 million image samples and compare the results to a CLIP ResNet-50 model trained on 400 million samples.* Results: The authors show that the standard ResNet-50 model can attain comparable robustness to the CLIP ResNet-50 model on limited data budgets, which is the first result of its kind to our knowledge.Here’s the Simplified Chinese text format you requested:* For: 本文目标是在分布shift下训练深度学习模型，特别是在数据预算有限的情况下。* Methods: 作者引入了新的JANuS（联合注释和名称集）数据集，并通过仔细控制的调查来评估图像分类 task中的Robustness因素。他们使用标准的ResNet-50模型，使用十字积分损失函数进行训练，并与400万样本进行比较。* Results: 作者发现，使用标准的ResNet-50模型可以在有限数据预算下实现相似的Robustness性能，这是我们知道的第一个结果。<details>
<summary>Abstract</summary>
Real world uses of deep learning require predictable model behavior under distribution shifts. Models such as CLIP show emergent natural distributional robustness comparable to humans, but may require hundreds of millions of training samples. Can we train robust learners in a domain where data is limited? To rigorously address this question, we introduce JANuS (Joint Annotations and Names Set), a collection of four new training datasets with images, labels, and corresponding captions, and perform a series of carefully controlled investigations of factors contributing to robustness in image classification, then compare those results to findings derived from a large-scale meta-analysis. Using this approach, we show that standard ResNet-50 trained with the cross-entropy loss on 2.4 million image samples can attain comparable robustness to a CLIP ResNet-50 trained on 400 million samples. To our knowledge, this is the first result showing (near) state-of-the-art distributional robustness on limited data budgets. Our dataset is available at \url{https://huggingface.co/datasets/penfever/JANuS_dataset}, and the code used to reproduce our experiments can be found at \url{https://github.com/penfever/vlhub/}.
</details>
<details>
<summary>摘要</summary>
实际应用中的深度学习需要模型在分布转移时具有预测可靠性。如CLIP模型，它们可以在自然分布下显示出类似于人类的分布弹性，但可能需要数百万个训练样本。可以在具有有限的数据库中训练强健的学习者吗？为了系统地回答这个问题，我们提出了JANuS（共同注释和名称集），包括四个新的训练集，每个集包含图像、标签和相应的描述，并进行了一系列仔细控制的调查，以研究影响模型强健性的因素。我们发现，使用权重平衡损失函数，只需训练240万个图像样本的标准ResNet-50模型，可以达到与CLIP ResNet-50模型在400万样本上训练后的相似的分布弹性。我们认为这是首次在有限数据预算下实现（近）顶尖分布弹性的结果。我们的数据集可以在\url{https://huggingface.co/datasets/penfever/JANuS_dataset}中找到，并且使用来复制我们的实验的代码可以在\url{https://github.com/penfever/vlhub/}找到。
</details></li>
</ul>
<hr>
<h2 id="WarpEM-Dynamic-Time-Warping-for-Accurate-Catheter-Registration-in-EM-guided-Procedures"><a href="#WarpEM-Dynamic-Time-Warping-for-Accurate-Catheter-Registration-in-EM-guided-Procedures" class="headerlink" title="WarpEM: Dynamic Time Warping for Accurate Catheter Registration in EM-guided Procedures"></a>WarpEM: Dynamic Time Warping for Accurate Catheter Registration in EM-guided Procedures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03652">http://arxiv.org/abs/2308.03652</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ardit Ramadani, Peter Ewert, Heribert Schunkert, Nassir Navab</li>
<li>for: 这篇论文旨在提供一种自动化的电磁诊断追踪方法，以便在微侵入性医疗程序中精确地追踪静脉管。</li>
<li>methods: 本文使用了3D信号时间分析技术，如动态时间截图（DTW）算法，以改善追踪精度和可靠性。</li>
<li>results: 结果显示，DTW方法可以精确地调整和匹配EM追踪路径与血管中心轴，与 marker-based 追踪作为参考值得出高度相似的追踪结果，mean error 为2.22mm。<details>
<summary>Abstract</summary>
Accurate catheter tracking is crucial during minimally invasive endovascular procedures (MIEP), and electromagnetic (EM) tracking is a widely used technology that serves this purpose. However, registration between preoperative images and the EM tracking system is often challenging. Existing registration methods typically require manual interactions, which can be time-consuming, increase the risk of errors and change the procedural workflow. Although several registration methods are available for catheter tracking, such as marker-based and path-based approaches, their limitations can impact the accuracy of the resulting tracking solution, consequently, the outcome of the medical procedure.   This paper introduces a novel automated catheter registration method for EM-guided MIEP. The method utilizes 3D signal temporal analysis, such as Dynamic Time Warping (DTW) algorithms, to improve registration accuracy and reliability compared to existing methods. DTW can accurately warp and match EM-tracked paths to the vessel's centerline, making it particularly suitable for registration. The introduced registration method is evaluated for accuracy in a vascular phantom using a marker-based registration as the ground truth. The results indicate that the DTW method yields accurate and reliable registration outcomes, with a mean error of $2.22$mm. The introduced registration method presents several advantages over state-of-the-art methods, such as high registration accuracy, no initialization required, and increased automation.
</details>
<details>
<summary>摘要</summary>
准确的导管跟踪是在微创综合性术（MIEP）中非常重要，电磁（EM）跟踪技术是广泛使用的。然而，在 préoperative图像和EM跟踪系统之间的注册通常是困难的。现有的注册方法通常需要手动交互，这可能会耗时，增加错误的风险，并改变操作工作流程。虽然有几种注册方法可以用于导管跟踪，如标记基于的和路径基于的方法，但它们的局限性可能会影响导管跟踪解决方案的准确性，从而影响医疗程序的结果。这篇论文介绍了一种新的自动化导管注册方法，用于EM-引导MIEP。该方法利用3D信号时间分析算法，如动态时间战斗（DTW）算法，以提高注册准确性和可靠性。DTW算法可以准确地扭曲和匹配EM跟踪的路径与血管中心线，使其特别适用于注册。引入的注册方法在vascular模拟器中使用 marker-based注册作为参照值进行评估。结果表明，DTW方法可以提供高准确性和可靠性的注册结果，平均错误为2.22毫米。引入的注册方法具有许多优点，如高注册准确性、无需初始化、提高自动化等。
</details></li>
</ul>
<hr>
<h2 id="MOMA-Force-Visual-Force-Imitation-for-Real-World-Mobile-Manipulation"><a href="#MOMA-Force-Visual-Force-Imitation-for-Real-World-Mobile-Manipulation" class="headerlink" title="MOMA-Force: Visual-Force Imitation for Real-World Mobile Manipulation"></a>MOMA-Force: Visual-Force Imitation for Real-World Mobile Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03624">http://arxiv.org/abs/2308.03624</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taozheng Yang, Ya Jing, Hongtao Wu, Jiafeng Xu, Kuankuan Sima, Guangzeng Chen, Qie Sima, Tao Kong</li>
<li>for: 这个论文旨在提出一种用于移动抓取器完成多种接触rich manipulate任务的新方法。</li>
<li>methods: 该方法 combinest representation learning for perception, imitation learning for complex motion generation, and admittance whole-body control to achieve both robustness and controllability.</li>
<li>results: 在实际家庭环境中，该方法比基线方法更高的成功率和更小的接触力和力异常值。Here’s the full text in Simplified Chinese:</li>
<li>for: 这个论文旨在提出一种用于移动抓取器完成多种接触rich manipulate任务的新方法。</li>
<li>methods: 该方法 combinest representation learning for perception, imitation learning for complex motion generation, and admittance whole-body control to achieve both robustness and controllability.</li>
<li>results: 在实际家庭环境中，该方法比基线方法更高的成功率和更小的接触力和力异常值。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
In this paper, we present a novel method for mobile manipulators to perform multiple contact-rich manipulation tasks. While learning-based methods have the potential to generate actions in an end-to-end manner, they often suffer from insufficient action accuracy and robustness against noise. On the other hand, classical control-based methods can enhance system robustness, but at the cost of extensive parameter tuning. To address these challenges, we present MOMA-Force, a visual-force imitation method that seamlessly combines representation learning for perception, imitation learning for complex motion generation, and admittance whole-body control for system robustness and controllability. MOMA-Force enables a mobile manipulator to learn multiple complex contact-rich tasks with high success rates and small contact forces. In a real household setting, our method outperforms baseline methods in terms of task success rates. Moreover, our method achieves smaller contact forces and smaller force variances compared to baseline methods without force imitation. Overall, we offer a promising approach for efficient and robust mobile manipulation in the real world. Videos and more details can be found on \url{https://visual-force-imitation.github.io}
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的方法，以便移动抓取机器人执行多种接触rich的抓取任务。而学习基于方法可以在端到端的方式生成动作，但它们经常受到不精准的动作和噪声的影响。而经典控制基于方法可以提高系统的稳定性，但是在Parameter tuning的代价上。为了解决这些挑战，我们提出了MOMA-Force方法，这是一种基于视觉力学学习、模仿学习和总体控制的视觉力学抓取方法。MOMA-Force方法可以让移动抓取机器人学习多种复杂的接触rich任务，并且具有高成功率和小接触力。在一个真实的家庭环境中，我们的方法比基准方法更高的任务成功率，同时也比基准方法更小的接触力和力矩变化。总的来说，我们的方法可以带来有效和稳定的移动抓取在真实世界中。视频和更多细节可以在 \url{https://visual-force-imitation.github.io} 上找到。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Visual-Pre-training-for-Robot-Manipulation-Datasets-Models-and-Methods"><a href="#Exploring-Visual-Pre-training-for-Robot-Manipulation-Datasets-Models-and-Methods" class="headerlink" title="Exploring Visual Pre-training for Robot Manipulation: Datasets, Models and Methods"></a>Exploring Visual Pre-training for Robot Manipulation: Datasets, Models and Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03620">http://arxiv.org/abs/2308.03620</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ya Jing, Xuelin Zhu, Xingbin Liu, Qie Sima, Taozheng Yang, Yunhai Feng, Tao Kong</li>
<li>for: 本研究旨在探讨视觉预训练策略对机器人 manipulate 任务的影响，从三个基本角度进行了全面的调查。</li>
<li>methods: 本研究使用了大规模的实际数据，并对模型结构和训练方法进行了广泛的试验。</li>
<li>results: 实验结果表明，提议的 Vi-PRoM 方案在不同的 simulate 环境和真实机器人上都达到了显著的改进。<details>
<summary>Abstract</summary>
Visual pre-training with large-scale real-world data has made great progress in recent years, showing great potential in robot learning with pixel observations. However, the recipes of visual pre-training for robot manipulation tasks are yet to be built. In this paper, we thoroughly investigate the effects of visual pre-training strategies on robot manipulation tasks from three fundamental perspectives: pre-training datasets, model architectures and training methods. Several significant experimental findings are provided that are beneficial for robot learning. Further, we propose a visual pre-training scheme for robot manipulation termed Vi-PRoM, which combines self-supervised learning and supervised learning. Concretely, the former employs contrastive learning to acquire underlying patterns from large-scale unlabeled data, while the latter aims learning visual semantics and temporal dynamics. Extensive experiments on robot manipulations in various simulation environments and the real robot demonstrate the superiority of the proposed scheme. Videos and more details can be found on \url{https://explore-pretrain-robot.github.io}.
</details>
<details>
<summary>摘要</summary>
“在过去几年，使用大规模实际数据进行视觉预训练已经做出了大量的进步，表明了视觉预训练在机器人学习中的潜力。然而，机器人 manipulate 任务中的视觉预训练的秘诀仍未得到建立。本文对视觉预训练策略在机器人 manipulate 任务中的影响进行了全面的调查，从三个基本的角度出发：预训练数据集、模型架构和训练方法。我们提供了许多有用的实验结果，这些结果对机器人学习具有帮助作用。此外，我们还提出了一种视觉预训练方案 для机器人 manipulate 任务，名为 Vi-PRoM，它将自我监督学习和监督学习相结合。具体来说，前者通过对大规模无标记数据中的对比学习来捕捉下层模式，而后者则是学习视觉 semantics 和时间动力学。我们在各种 simulate 环境和真实机器人上进行了广泛的实验，并证明了我们的方案的优越性。视频和更多细节可以在 \url{https://explore-pretrain-robot.github.io} 上找到。”
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Semi-Supervised-Segmentation-of-Brain-Vessels-with-Ambiguous-Labels"><a href="#Adaptive-Semi-Supervised-Segmentation-of-Brain-Vessels-with-Ambiguous-Labels" class="headerlink" title="Adaptive Semi-Supervised Segmentation of Brain Vessels with Ambiguous Labels"></a>Adaptive Semi-Supervised Segmentation of Brain Vessels with Ambiguous Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03613">http://arxiv.org/abs/2308.03613</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fengming Lin, Yan Xia, Nishant Ravikumar, Qiongyao Liu, Michael MacRaild, Alejandro F Frangi</li>
<li>for: 本研究旨在提高脑血管分割精度，以便脑血管疾病诊断和治疗。</li>
<li>methods: 该方法采用进步式半监督学习、适应性训练策略和边界增强等技术。</li>
<li>results: 实验结果表明，该方法在3DRA数据集上实现了高精度的网格基分割结果，并且能够处理部分或抽象标注的数据集。<details>
<summary>Abstract</summary>
Accurate segmentation of brain vessels is crucial for cerebrovascular disease diagnosis and treatment. However, existing methods face challenges in capturing small vessels and handling datasets that are partially or ambiguously annotated. In this paper, we propose an adaptive semi-supervised approach to address these challenges. Our approach incorporates innovative techniques including progressive semi-supervised learning, adaptative training strategy, and boundary enhancement. Experimental results on 3DRA datasets demonstrate the superiority of our method in terms of mesh-based segmentation metrics. By leveraging the partially and ambiguously labeled data, which only annotates the main vessels, our method achieves impressive segmentation performance on mislabeled fine vessels, showcasing its potential for clinical applications.
</details>
<details>
<summary>摘要</summary>
精准分割脑血管是脑血管疾病诊断和治疗中的关键。然而，现有方法在捕捉小血管和处理部分或杂杂标注的数据集时受到挑战。在这篇论文中，我们提出了一种适应性半supervised方法来解决这些挑战。我们的方法包括进步半supervised学习、适应性训练策略和边界增强等创新技术。实验结果表明，我们的方法在3DRA数据集上的笔直基于分割指标上具有显著的优势。通过利用部分和杂杂标注的数据，我们的方法在杂杂标注的细血管上达到了很好的分割性能，这显示了其在临床应用中的潜力。
</details></li>
</ul>
<hr>
<h2 id="AvatarVerse-High-quality-Stable-3D-Avatar-Creation-from-Text-and-Pose"><a href="#AvatarVerse-High-quality-Stable-3D-Avatar-Creation-from-Text-and-Pose" class="headerlink" title="AvatarVerse: High-quality &amp; Stable 3D Avatar Creation from Text and Pose"></a>AvatarVerse: High-quality &amp; Stable 3D Avatar Creation from Text and Pose</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03610">http://arxiv.org/abs/2308.03610</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huichao Zhang, Bowen Chen, Hao Yang, Liao Qu, Xu Wang, Li Chen, Chao Long, Feida Zhu, Kang Du, Min Zheng</li>
<li>for: 高品质、多样化的3D人物模型自文本描述和姿势指导生成</li>
<li>methods: 基于Diffusion Model和DensePose信号的2D图像Conditional Generation、高解度3Dsynthesis策略</li>
<li>results: 实现零shot3D模型生成高品质、多样化的3D人物模型，比前工作有更高的表现质量和稳定性。<details>
<summary>Abstract</summary>
Creating expressive, diverse and high-quality 3D avatars from highly customized text descriptions and pose guidance is a challenging task, due to the intricacy of modeling and texturing in 3D that ensure details and various styles (realistic, fictional, etc). We present AvatarVerse, a stable pipeline for generating expressive high-quality 3D avatars from nothing but text descriptions and pose guidance. In specific, we introduce a 2D diffusion model conditioned on DensePose signal to establish 3D pose control of avatars through 2D images, which enhances view consistency from partially observed scenarios. It addresses the infamous Janus Problem and significantly stablizes the generation process. Moreover, we propose a progressive high-resolution 3D synthesis strategy, which obtains substantial improvement over the quality of the created 3D avatars. To this end, the proposed AvatarVerse pipeline achieves zero-shot 3D modeling of 3D avatars that are not only more expressive, but also in higher quality and fidelity than previous works. Rigorous qualitative evaluations and user studies showcase AvatarVerse's superiority in synthesizing high-fidelity 3D avatars, leading to a new standard in high-quality and stable 3D avatar creation. Our project page is: https://avatarverse3d.github.io
</details>
<details>
<summary>摘要</summary>
创建高质量、多样化和自然表达的3D人物模型从高级定制文本描述和姿势指导是一项复杂的任务，因为3D模型和Texture的细节和不同风格（现实、虚构等）的涉及。我们介绍了AvatarVerse，一个稳定的生成高质量3D人物模型的管道，从文本描述和姿势指导开始。具体来说，我们引入了基于DensePose信号的2D扩散模型，以确保3D人物的姿势控制，并解决了著名的托尼问题，从部分观察的场景中提高了视觉一致性。此外，我们提出了一种进步的高分辨率3D生成策略，实现了对创建的3D人物模型的质量提升。因此，我们的AvatarVerse管道实现了零式3D模型化，创造出更加表达力强、高质量和真实的3D人物模型，胜过前一个工作。我们的项目页面是：https://avatarverse3d.github.io。
</details></li>
</ul>
<hr>
<h2 id="Recurrent-Self-Supervised-Video-Denoising-with-Denser-Receptive-Field"><a href="#Recurrent-Self-Supervised-Video-Denoising-with-Denser-Receptive-Field" class="headerlink" title="Recurrent Self-Supervised Video Denoising with Denser Receptive Field"></a>Recurrent Self-Supervised Video Denoising with Denser Receptive Field</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03608">http://arxiv.org/abs/2308.03608</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zichun Wang, Yulun Zhang, Debing Zhang, Ying Fu</li>
<li>for: 自动化视频干净（video denoising）</li>
<li>methods: 使用自适应损块网络（blind spot networks）和自适应循环视频干净方法（self-supervised recurrent video denoising method）</li>
<li>results: 提高视频干净效果，利用参照帧和邻帧帧的更多信息，同时具有较好的泛化能力和稳定性。<details>
<summary>Abstract</summary>
Self-supervised video denoising has seen decent progress through the use of blind spot networks. However, under their blind spot constraints, previous self-supervised video denoising methods suffer from significant information loss and texture destruction in either the whole reference frame or neighbor frames, due to their inadequate consideration of the receptive field. Moreover, the limited number of available neighbor frames in previous methods leads to the discarding of distant temporal information. Nonetheless, simply adopting existing recurrent frameworks does not work, since they easily break the constraints on the receptive field imposed by self-supervision. In this paper, we propose RDRF for self-supervised video denoising, which not only fully exploits both the reference and neighbor frames with a denser receptive field, but also better leverages the temporal information from both local and distant neighbor features. First, towards a comprehensive utilization of information from both reference and neighbor frames, RDRF realizes a denser receptive field by taking more neighbor pixels along the spatial and temporal dimensions. Second, it features a self-supervised recurrent video denoising framework, which concurrently integrates distant and near-neighbor temporal features. This enables long-term bidirectional information aggregation, while mitigating error accumulation in the plain recurrent framework. Our method exhibits superior performance on both synthetic and real video denoising datasets. Codes will be available at https://github.com/Wang-XIaoDingdd/RDRF.
</details>
<details>
<summary>摘要</summary>
自我监督视频干扰有很好的进步，特别是通过盲区网络。然而，在这些盲区约束下，前一代的自我监督视频干扰方法会导致重要信息的损失和图像的破坏，主要是因为它们对接收场的不充分考虑。此外，过去的方法中的可用邻帧数量有限，导致远端的时间信息抛弃。然而，直接采用现有的循环框架不行，因为它们容易违反自我监督中的接收场约束。在这篇论文中，我们提出了RDRF方法，该方法不仅能充分利用参照帧和邻帧帧的信息，而且能更好地利用邻帧帧的时间特征。首先，RDRF方法实现了更 dense的接收场，通过在空间和时间维度上接受更多的邻帧像素。其次，它提供了一种自我监督循环视频干扰框架，该框架同时集成了远端和近邻邻帧特征。这使得长期双向信息集成，并减少了循环框架中的错误积累。我们的方法在Synthetic和实际视频干扰数据上表现出色。代码将在https://github.com/Wang-XIaoDingdd/RDRF中提供。
</details></li>
</ul>
<hr>
<h2 id="FeatEnHancer-Enhancing-Hierarchical-Features-for-Object-Detection-and-Beyond-Under-Low-Light-Vision"><a href="#FeatEnHancer-Enhancing-Hierarchical-Features-for-Object-Detection-and-Beyond-Under-Low-Light-Vision" class="headerlink" title="FeatEnHancer: Enhancing Hierarchical Features for Object Detection and Beyond Under Low-Light Vision"></a>FeatEnHancer: Enhancing Hierarchical Features for Object Detection and Beyond Under Low-Light Vision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03594">http://arxiv.org/abs/2308.03594</a></li>
<li>repo_url: None</li>
<li>paper_authors: Khurram Azeem Hashmi, Goutham Kallempudi, Didier Stricker, Muhammamd Zeshan Afzal</li>
<li>for: 提高低光照下的视觉任务表现，特别是提取下游任务中有用的视觉cue。</li>
<li>methods: 提出了一种新的模块——FeatEnHancer，通过多级层次结合多头注意力和任务相关损失函数来生成适应性的图像表示。</li>
<li>results: 在多个低光照视觉任务中，FeatEnHancer模块可以带来显著和一致的提高，包括黑bject检测 (+5.7 mAP on ExDark)、人脸检测 (+1.5 mAPon DARK FACE)、夜间 semantic segmentation (+5.1 mIoU on ACDC) 和视频对象检测 (+1.8 mAP on DarkVision)，这显示了增强层次特征的有效性。<details>
<summary>Abstract</summary>
Extracting useful visual cues for the downstream tasks is especially challenging under low-light vision. Prior works create enhanced representations by either correlating visual quality with machine perception or designing illumination-degrading transformation methods that require pre-training on synthetic datasets. We argue that optimizing enhanced image representation pertaining to the loss of the downstream task can result in more expressive representations. Therefore, in this work, we propose a novel module, FeatEnHancer, that hierarchically combines multiscale features using multiheaded attention guided by task-related loss function to create suitable representations. Furthermore, our intra-scale enhancement improves the quality of features extracted at each scale or level, as well as combines features from different scales in a way that reflects their relative importance for the task at hand. FeatEnHancer is a general-purpose plug-and-play module and can be incorporated into any low-light vision pipeline. We show with extensive experimentation that the enhanced representation produced with FeatEnHancer significantly and consistently improves results in several low-light vision tasks, including dark object detection (+5.7 mAP on ExDark), face detection (+1.5 mAPon DARK FACE), nighttime semantic segmentation (+5.1 mIoU on ACDC ), and video object detection (+1.8 mAP on DarkVision), highlighting the effectiveness of enhancing hierarchical features under low-light vision.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>低光照下的视觉特征提取是特别困难，先前的工作通过与机器感知相关的视质质量相关或设计产生杂质变换方法来创建增强的表示。我们认为，根据下游任务的损失函数优化增强图像表示可以获得更表现тив的表示。因此，在这个工作中，我们提出了一种新的模块，FeatEnHancer，它通过多级划分特征并使用多头注意力指导任务相关损失函数来创建适合的表示。此外，我们的内部划分增强可以提高每级特征提取的质量，同时将不同级划分特征组合在一起，以反映它们在任务中的相对重要性。FeatEnHancer是一个通用的插件和撤退模块，可以在任何低光照视觉管道中使用。我们通过广泛的实验表明，FeatEnHancer生成的增强表示可以在多个低光照视觉任务中提高结果，包括黑影物体检测 (+5.7 mAP on ExDark)、人脸检测 (+1.5 mAPon DARK FACE)、夜间 semantic segmentation (+5.1 mIoU on ACDC) 和视频对象检测 (+1.8 mAP on DarkVision)，这 highlights the effectiveness of enhancing hierarchical features under low-light vision。
</details></li>
</ul>
<hr>
<h2 id="SoilNet-An-Attention-based-Spatio-temporal-Deep-Learning-Framework-for-Soil-Organic-Carbon-Prediction-with-Digital-Soil-Mapping-in-Europe"><a href="#SoilNet-An-Attention-based-Spatio-temporal-Deep-Learning-Framework-for-Soil-Organic-Carbon-Prediction-with-Digital-Soil-Mapping-in-Europe" class="headerlink" title="SoilNet: An Attention-based Spatio-temporal Deep Learning Framework for Soil Organic Carbon Prediction with Digital Soil Mapping in Europe"></a>SoilNet: An Attention-based Spatio-temporal Deep Learning Framework for Soil Organic Carbon Prediction with Digital Soil Mapping in Europe</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03586">http://arxiv.org/abs/2308.03586</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nafiseh Kakhani, Moien Rangzan, Ali Jamali, Sara Attarchi, Seyed Kazem Alavipanah, Thomas Scholten</li>
<li>for: 这个研究旨在提高土壤地图的精度和可靠性，并且运用深度学习技术来预测土壤碳的空间分布。</li>
<li>methods: 这个研究使用了一种新的架构，具有空间注意 Mechanism和气候时间序列对应的LSTM网络，以预测欧洲各地的土壤碳含量。这个模型使用了一系列的环境特征，包括landsat-8图像、地形、遥测指数和气候时间序列，作为输入特征。</li>
<li>results: 研究结果显示，提案的架构在预测土壤碳含量方面比常用的机器学习方法（如随机森林）有更好的表现，具体而言，这个模型的误差值较低。这个模型是一个可靠的工具，可以用来预测土壤碳和其他土壤特征，并且可以帮助土地管理和决策过程中的准确信息。<details>
<summary>Abstract</summary>
Digital soil mapping (DSM) is an advanced approach that integrates statistical modeling and cutting-edge technologies, including machine learning (ML) methods, to accurately depict soil properties and their spatial distribution. Soil organic carbon (SOC) is a crucial soil attribute providing valuable insights into soil health, nutrient cycling, greenhouse gas emissions, and overall ecosystem productivity. This study highlights the significance of spatial-temporal deep learning (DL) techniques within the DSM framework. A novel architecture is proposed, incorporating spatial information using a base convolutional neural network (CNN) model and spatial attention mechanism, along with climate temporal information using a long short-term memory (LSTM) network, for SOC prediction across Europe. The model utilizes a comprehensive set of environmental features, including Landsat-8 images, topography, remote sensing indices, and climate time series, as input features. Results demonstrate that the proposed framework outperforms conventional ML approaches like random forest commonly used in DSM, yielding lower root mean square error (RMSE). This model is a robust tool for predicting SOC and could be applied to other soil properties, thereby contributing to the advancement of DSM techniques and facilitating land management and decision-making processes based on accurate information.
</details>
<details>
<summary>摘要</summary>
《数字土壤地图（DSM）是一种先进的方法，它将统计模型和前沿技术，包括机器学习（ML）方法，融合在一起以准确地表示土壤属性和其空间分布。土壤有机碳（SOC）是一个重要的土壤特征，它为土壤健康、营养循环、温室气体排放和生态系统产生力提供了重要的信息。本研究发现，在 DSM 框架中使用空间时间深度学习（DL）技术可以提高 SOC 预测的准确性。本文提出了一种新的架构，其包括基于 Convolutional Neural Network（CNN）模型的空间注意机制和基于 Long Short-Term Memory（LSTM）网络的时间注意机制，用于预测欧洲各地的 SOC。该模型使用了包括 Landsat-8 图像、地形、远程感知指数和气候时间序列在内的全面环境特征作为输入特征。结果表明，提议的框架可以比常见的多项式学习方法，如Random Forest，更好地预测 SOC，具有较低的根圆方差误差（RMSE）。这种模型是一种可靠的 SOC 预测工具，可以应用于其他土壤属性，从而为土地管理和决策过程提供准确信息的支持。
</details></li>
</ul>
<hr>
<h2 id="Feature-Decoupling-Recycling-Network-for-Fast-Interactive-Segmentation"><a href="#Feature-Decoupling-Recycling-Network-for-Fast-Interactive-Segmentation" class="headerlink" title="Feature Decoupling-Recycling Network for Fast Interactive Segmentation"></a>Feature Decoupling-Recycling Network for Fast Interactive Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03529">http://arxiv.org/abs/2308.03529</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huimin Zeng, Weinong Wang, Xin Tao, Zhiwei Xiong, Yu-Wing Tai, Wenjie Pei</li>
<li>for: 提高交互式分割的效率，特别是在需要长期交互的复杂enario下（提高4.25倍），同时保持适用性和可靠性。</li>
<li>methods: 提出Feature Decoupling-Recycling Network（FDRN），通过基于不同类型的差异来隔离模型组件，然后再次利用这些组件进行每次交互。</li>
<li>results: 在6个不同领域和模式的数据集上进行了广泛的实验，表明：1）比其他方法更高效（最多4.25倍），特别是在复杂的场景下；2）可以作为通用增强技术应用于不同的方法；3）具有跨任务普适性和鲁棒性。<details>
<summary>Abstract</summary>
Recent interactive segmentation methods iteratively take source image, user guidance and previously predicted mask as the input without considering the invariant nature of the source image. As a result, extracting features from the source image is repeated in each interaction, resulting in substantial computational redundancy. In this work, we propose the Feature Decoupling-Recycling Network (FDRN), which decouples the modeling components based on their intrinsic discrepancies and then recycles components for each user interaction. Thus, the efficiency of the whole interactive process can be significantly improved. To be specific, we apply the Decoupling-Recycling strategy from three perspectives to address three types of discrepancies, respectively. First, our model decouples the learning of source image semantics from the encoding of user guidance to process two types of input domains separately. Second, FDRN decouples high-level and low-level features from stratified semantic representations to enhance feature learning. Third, during the encoding of user guidance, current user guidance is decoupled from historical guidance to highlight the effect of current user guidance. We conduct extensive experiments on 6 datasets from different domains and modalities, which demonstrate the following merits of our model: 1) superior efficiency than other methods, particularly advantageous in challenging scenarios requiring long-term interactions (up to 4.25x faster), while achieving favorable segmentation performance; 2) strong applicability to various methods serving as a universal enhancement technique; 3) well cross-task generalizability, e.g., to medical image segmentation, and robustness against misleading user guidance.
</details>
<details>
<summary>摘要</summary>
最近的互动式分割方法会 iteratively 使用源图像、用户指导和先前预测的面积作为输入，而不考虑源图像的不变性。这会导致在每次互动中提取源图像的特征，从而导致计算重复，从而导致计算浪费。在这种情况下，我们提出了Feature Decoupling-Recycling Network（FDRN），它将模型组件基于其内在差异分解，然后将组件重新使用。这有助于提高整个互动过程的效率。具体来说，我们在三个方面应用Decoupling-Recycling策略来解决三种不同的差异：首先，我们的模型将源图像 semantics 学习与用户指导编码分解成两个不同的输入领域。第二，FDRN将高级和低级特征从层次结构的 semantic representation 分解，以提高特征学习。第三，在用户指导编码时，当前用户指导与历史指导分解，以强调当前用户指导的效果。我们在6个不同领域和模式的数据集上进行了广泛的实验，其结果表明：1. 我们的模型在长期互动（最长4.25倍）中表现出了明显的高效性，而且在多种互动方法上表现出了优秀的分割性能。2. FDRN 是一种通用的增强技术，可以应用于多种方法。3. 我们的模型在不同的任务上具有良好的跨任务泛化性和鲁棒性，例如医学影像分割。
</details></li>
</ul>
<hr>
<h2 id="Keyword-Spotting-Simplified-A-Segmentation-Free-Approach-using-Character-Counting-and-CTC-re-scoring"><a href="#Keyword-Spotting-Simplified-A-Segmentation-Free-Approach-using-Character-Counting-and-CTC-re-scoring" class="headerlink" title="Keyword Spotting Simplified: A Segmentation-Free Approach using Character Counting and CTC re-scoring"></a>Keyword Spotting Simplified: A Segmentation-Free Approach using Character Counting and CTC re-scoring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03515">http://arxiv.org/abs/2308.03515</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/georgeretsi/segfreekws">https://github.com/georgeretsi/segfreekws</a></li>
<li>paper_authors: George Retsinas, Giorgos Sfikas, Christophoros Nikou</li>
<li>for: 这篇论文targets文档图像中的关键词检索问题，具体来说是一种基于对象检测模式的分类方法，并借鉴了当今最佳的检测系统来同时提出词 bounding box 提议机制和计算相应的表示。</li>
<li>methods: 该方法不同于常见的使用复杂大型神经网络模型的方法，而是提出了一种简单并 компакт的分类系统，通过高效扫描文档图像来找到包含查询信息的矩形区域，并通过一个隐式学习的尺度图来预测字符出现的区域。</li>
<li>results: 实验 validate了这种方法可以卓越于当今最佳的方法，尽管使用的模型非常简单和占用空间小。<details>
<summary>Abstract</summary>
Recent advances in segmentation-free keyword spotting treat this problem w.r.t. an object detection paradigm and borrow from state-of-the-art detection systems to simultaneously propose a word bounding box proposal mechanism and compute a corresponding representation. Contrary to the norm of such methods that rely on complex and large DNN models, we propose a novel segmentation-free system that efficiently scans a document image to find rectangular areas that include the query information. The underlying model is simple and compact, predicting character occurrences over rectangular areas through an implicitly learned scale map, trained on word-level annotated images. The proposed document scanning is then performed using this character counting in a cost-effective manner via integral images and binary search. Finally, the retrieval similarity by character counting is refined by a pyramidal representation and a CTC-based re-scoring algorithm, fully utilizing the trained CNN model. Experimental validation on two widely-used datasets shows that our method achieves state-of-the-art results outperforming the more complex alternatives, despite the simplicity of the underlying model.
</details>
<details>
<summary>摘要</summary>
近年来， segmentation-free 关键词检索技术发展，将这个问题转化为对象检测模式，借鉴国际一级检测系统，同时提出词框报告机制和相应的表示计算。与传统方法不同，我们提出了一种简单、占地小的 segmentation-free 系统，通过高效扫描文档图像，找到包含查询信息的矩形区域。这个模型简单、巧妙，通过隐式学习的Scale Map，在word级图像上预测字符出现的区域。然后，通过 integral images 和 binary search 来实现cost-effective的文档扫描。最后，通过 pyramidal representation 和 CTC-based re-scoring algorithm，完全利用训练的 CNN 模型，进行了 Retrieval 相关性的补做。我们在两个常用的数据集上进行了实验验证，发现我们的方法可以在与更复杂的对比下，即使模型本身简单，却能够达到国际一级的Result。
</details></li>
</ul>
<hr>
<h2 id="Learning-Photometric-Feature-Transform-for-Free-form-Object-Scan"><a href="#Learning-Photometric-Feature-Transform-for-Free-form-Object-Scan" class="headerlink" title="Learning Photometric Feature Transform for Free-form Object Scan"></a>Learning Photometric Feature Transform for Free-form Object Scan</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03492">http://arxiv.org/abs/2308.03492</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Feng, Kaizhang Kang, Fan Pei, Huakeng Ding, Jinjiang You, Ping Tan, Kun Zhou, Hongzhi Wu</li>
<li>for: 提高3D重建的精度和速度</li>
<li>methods: 使用自动学习的多视图投影和变换方法，并与照明条件进行共同训练</li>
<li>results: 实现了高精度和高速的3D重建，并与专业3D扫描仪和照片进行比较，与当前技术相比较有优势<details>
<summary>Abstract</summary>
We propose a novel framework to automatically learn to aggregate and transform photometric measurements from multiple unstructured views into spatially distinctive and view-invariant low-level features, which are fed to a multi-view stereo method to enhance 3D reconstruction. The illumination conditions during acquisition and the feature transform are jointly trained on a large amount of synthetic data. We further build a system to reconstruct the geometry and anisotropic reflectance of a variety of challenging objects from hand-held scans. The effectiveness of the system is demonstrated with a lightweight prototype, consisting of a camera and an array of LEDs, as well as an off-the-shelf tablet. Our results are validated against reconstructions from a professional 3D scanner and photographs, and compare favorably with state-of-the-art techniques.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的框架，用于自动学习将多视角不结构化测量数据转化为空间特征和视角不变的低级特征，这些特征被传递给多视角斯tereo方法以增强3D重建。在获取过程中的照明条件和特征变换被同时训练在大量的 sintetic数据上。我们还建立了一个系统，用于从手持扫描获取的数据中重建物体的几何和方向异otropic反射。我们的结果通过使用轻量级的 прототип，包括一个相机和一个LED阵列，以及一个商业化的平板电脑，与专业3D扫描仪和照片进行比较，并与现有技术相比较有着良好的效果。
</details></li>
</ul>
<hr>
<h2 id="Improving-Mass-Detection-in-Mammography-Images-A-Study-of-Weakly-Supervised-Learning-and-Class-Activation-Map-Methods"><a href="#Improving-Mass-Detection-in-Mammography-Images-A-Study-of-Weakly-Supervised-Learning-and-Class-Activation-Map-Methods" class="headerlink" title="Improving Mass Detection in Mammography Images: A Study of Weakly Supervised Learning and Class Activation Map Methods"></a>Improving Mass Detection in Mammography Images: A Study of Weakly Supervised Learning and Class Activation Map Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03486">http://arxiv.org/abs/2308.03486</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vicente Sampaio, Filipe R. Cordeiro</li>
<li>for: 这个研究旨在测试不同的启动图示方法，以提高静脉癌检测模型的精度。</li>
<li>methods: 这个研究使用了state-of-the-art的弱监督训练方法，并考虑了不同的启动图示技术，包括Class Activation Maps (CAM)、GradCAM、GradCAM++、XGradCAM和LayerCAM。</li>
<li>results: 研究发现，使用不同的启动图示方法在训练和测试阶段可以提高模型的性能，尤其是降低False Positive Per Image (FPPI)值，提高True Positive Rate (TPR)。<details>
<summary>Abstract</summary>
In recent years, weakly supervised models have aided in mass detection using mammography images, decreasing the need for pixel-level annotations. However, most existing models in the literature rely on Class Activation Maps (CAM) as the activation method, overlooking the potential benefits of exploring other activation techniques. This work presents a study that explores and compares different activation maps in conjunction with state-of-the-art methods for weakly supervised training in mammography images. Specifically, we investigate CAM, GradCAM, GradCAM++, XGradCAM, and LayerCAM methods within the framework of the GMIC model for mass detection in mammography images. The evaluation is conducted on the VinDr-Mammo dataset, utilizing the metrics Accuracy, True Positive Rate (TPR), False Negative Rate (FNR), and False Positive Per Image (FPPI). Results show that using different strategies of activation maps during training and test stages leads to an improvement of the model. With this strategy, we improve the results of the GMIC method, decreasing the FPPI value and increasing TPR.
</details>
<details>
<summary>摘要</summary>
Recently, weakly supervised models have been used for mass detection in mammography images, reducing the need for pixel-level annotations. However, most existing models in the literature rely on Class Activation Maps (CAM) as the activation method, without exploring other activation techniques. This study aims to explore and compare different activation maps in conjunction with state-of-the-art methods for weakly supervised training in mammography images. Specifically, we investigate CAM, GradCAM, GradCAM++, XGradCAM, and LayerCAM methods within the framework of the GMIC model for mass detection in mammography images. The evaluation is conducted on the VinDr-Mammo dataset, using Accuracy, True Positive Rate (TPR), False Negative Rate (FNR), and False Positive Per Image (FPPI) metrics. Results show that using different strategies of activation maps during training and test stages leads to improved model performance, with a decrease in FPPI and an increase in TPR.
</details></li>
</ul>
<hr>
<h2 id="Deepfake-Detection-A-Comparative-Analysis"><a href="#Deepfake-Detection-A-Comparative-Analysis" class="headerlink" title="Deepfake Detection: A Comparative Analysis"></a>Deepfake Detection: A Comparative Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03471">http://arxiv.org/abs/2308.03471</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/akshay-atam/Deepfake-Detection-Using-Machine-Learning">https://github.com/akshay-atam/Deepfake-Detection-Using-Machine-Learning</a></li>
<li>paper_authors: Sohail Ahmed Khan, Duc-Tien Dang-Nguyen</li>
<li>for: This paper aims to provide insights into the effectiveness of different deep learning architectures, training strategies, and deepfake detection benchmarks for developing more accurate and reliable deepfake detection systems.</li>
<li>methods: The paper evaluates eight supervised deep learning architectures and two transformer-based models pre-trained using self-supervised strategies on four benchmarks, including intra-dataset and inter-dataset evaluations, to examine the best performing models, generalisation capabilities, and impact of augmentations.</li>
<li>results: The paper presents a comprehensive comparative analysis of supervised and self-supervised models for deepfake detection, including the best performing models, generalisation capabilities, and impact of augmentations, to provide insights into the effectiveness of different deep learning architectures, training strategies, and deepfake detection benchmarks.Here are the three points in Simplified Chinese text:</li>
<li>for: 这篇论文的目的是为了提供不同的深度学习架构、训练策略和深度假像检测 benchmark 的效果，以开发更加准确和可靠的深度假像检测系统。</li>
<li>methods: 论文评估了 eight 个supervised深度学习架构和 two 个 transformer-based 模型，通过自我超vised策略进行预训练，然后在四个 benchmark 上进行评估，包括内部和外部评估，以 исследова最佳性能、泛化能力和增强策略的影响。</li>
<li>results: 论文提供了一项全面的比较分析，探讨不同的深度学习架构、训练策略和深度假像检测 benchmark 的效果，包括最佳性能、泛化能力和增强策略的影响，以帮助开发更加准确和可靠的深度假像检测系统。<details>
<summary>Abstract</summary>
This paper present a comprehensive comparative analysis of supervised and self-supervised models for deepfake detection. We evaluate eight supervised deep learning architectures and two transformer-based models pre-trained using self-supervised strategies (DINO, CLIP) on four benchmarks (FakeAVCeleb, CelebDF-V2, DFDC, and FaceForensics++). Our analysis includes intra-dataset and inter-dataset evaluations, examining the best performing models, generalisation capabilities, and impact of augmentations. We also investigate the trade-off between model size and performance. Our main goal is to provide insights into the effectiveness of different deep learning architectures (transformers, CNNs), training strategies (supervised, self-supervised), and deepfake detection benchmarks. These insights can help guide the development of more accurate and reliable deepfake detection systems, which are crucial in mitigating the harmful impact of deepfakes on individuals and society.
</details>
<details>
<summary>摘要</summary>
translate into Simplified Chinese:这篇论文提供了深度伪造检测中超级和自动驱动模型的比较分析。我们评估了8个超级深度学习架构和2个基于转换器的模型（DINO、CLIP）在4个标准测试集（FakeAVCeleb、CelebDF-V2、DFDC、FaceForensics++）上的性能。我们的分析包括内部数据集和间部数据集的评估，检查最佳性能模型，泛化能力和数据增强的影响。我们还进行了模型大小和性能之间的负面关系的研究。我们的主要目标是提供不同深度学习架构（转换器、CNN）、训练策略（supervised、self-supervised）和深度伪造检测标准集的情况，以帮助开发更加准确和可靠的深度伪造检测系统，这些系统对个人和社会的影响是非常重要的。
</details></li>
</ul>
<hr>
<h2 id="RoadScan-A-Novel-and-Robust-Transfer-Learning-Framework-for-Autonomous-Pothole-Detection-in-Roads"><a href="#RoadScan-A-Novel-and-Robust-Transfer-Learning-Framework-for-Autonomous-Pothole-Detection-in-Roads" class="headerlink" title="RoadScan: A Novel and Robust Transfer Learning Framework for Autonomous Pothole Detection in Roads"></a>RoadScan: A Novel and Robust Transfer Learning Framework for Autonomous Pothole Detection in Roads</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03467">http://arxiv.org/abs/2308.03467</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guruprasad Parasnis, Anmol Chokshi, Kailas Devadkar</li>
<li>for: 本研究旨在提出一种基于深度学习和图像处理技术的坑洞检测方法，以解决道路上坑洞的问题，该问题对道路用户造成了重大风险。</li>
<li>methods: 该方法利用VGG16模型进行特征提取，并使用自定义的Siamese网络和 triplet损失函数，称为RoadScan。</li>
<li>results: 该方法在准确地检测坑洞方面达到了显著的表现，其准确率达96.12%，EER值为3.89%，AUROC值为0.988，与其他现状顶尖研究相比表现高效。<details>
<summary>Abstract</summary>
This research paper presents a novel approach to pothole detection using Deep Learning and Image Processing techniques. The proposed system leverages the VGG16 model for feature extraction and utilizes a custom Siamese network with triplet loss, referred to as RoadScan. The system aims to address the critical issue of potholes on roads, which pose significant risks to road users. Accidents due to potholes on the roads have led to numerous accidents. Although it is necessary to completely remove potholes, it is a time-consuming process. Hence, a general road user should be able to detect potholes from a safe distance in order to avoid damage. Existing methods for pothole detection heavily rely on object detection algorithms which tend to have a high chance of failure owing to the similarity in structures and textures of a road and a pothole. Additionally, these systems utilize millions of parameters thereby making the model difficult to use in small-scale applications for the general citizen. By analyzing diverse image processing methods and various high-performing networks, the proposed model achieves remarkable performance in accurately detecting potholes. Evaluation metrics such as accuracy, EER, precision, recall, and AUROC validate the effectiveness of the system. Additionally, the proposed model demonstrates computational efficiency and cost-effectiveness by utilizing fewer parameters and data for training. The research highlights the importance of technology in the transportation sector and its potential to enhance road safety and convenience. The network proposed in this model performs with a 96.12 % accuracy, 3.89 % EER, and a 0.988 AUROC value, which is highly competitive with other state-of-the-art works.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="DiffSynth-Latent-In-Iteration-Deflickering-for-Realistic-Video-Synthesis"><a href="#DiffSynth-Latent-In-Iteration-Deflickering-for-Realistic-Video-Synthesis" class="headerlink" title="DiffSynth: Latent In-Iteration Deflickering for Realistic Video Synthesis"></a>DiffSynth: Latent In-Iteration Deflickering for Realistic Video Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03463">http://arxiv.org/abs/2308.03463</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alibaba/EasyNLP">https://github.com/alibaba/EasyNLP</a></li>
<li>paper_authors: Zhongjie Duan, Lizhou You, Chengyu Wang, Cen Chen, Ziheng Wu, Weining Qian, Jun Huang</li>
<li>for: 这个论文旨在将图像生成模型应用到视频生成中，以提高视频生成质量。</li>
<li>methods: 该论文提出了一种新的方法，称为DiffSynth，它包括两个关键组件：一个 latent in-iteration deflickering 框架和一个 video deflickering 算法。</li>
<li>results: 实验结果表明，DiffSynth 可以有效地避免视频中的闪烁问题，并且可以在不同的视频生成任务中表现出色，包括文本引导视频风格化、时尚视频生成、图像引导视频风格化、视频修复和3D 渲染。<details>
<summary>Abstract</summary>
In recent years, diffusion models have emerged as the most powerful approach in image synthesis. However, applying these models directly to video synthesis presents challenges, as it often leads to noticeable flickering contents. Although recently proposed zero-shot methods can alleviate flicker to some extent, we still struggle to generate coherent videos. In this paper, we propose DiffSynth, a novel approach that aims to convert image synthesis pipelines to video synthesis pipelines. DiffSynth consists of two key components: a latent in-iteration deflickering framework and a video deflickering algorithm. The latent in-iteration deflickering framework applies video deflickering to the latent space of diffusion models, effectively preventing flicker accumulation in intermediate steps. Additionally, we propose a video deflickering algorithm, named patch blending algorithm, that remaps objects in different frames and blends them together to enhance video consistency. One of the notable advantages of DiffSynth is its general applicability to various video synthesis tasks, including text-guided video stylization, fashion video synthesis, image-guided video stylization, video restoring, and 3D rendering. In the task of text-guided video stylization, we make it possible to synthesize high-quality videos without cherry-picking. The experimental results demonstrate the effectiveness of DiffSynth. All videos can be viewed on our project page. Source codes will also be released.
</details>
<details>
<summary>摘要</summary>
近年来，Diffusion模型在图像生成领域中得到了广泛应用，但是直接应用这些模型到视频生成中存在一些挑战，因为这会导致视频中的干扰内容变得显著。虽然最近提出的零模型可以在一定程度上缓解干扰，但我们仍然无法生成具有一致性的视频。在这篇论文中，我们提出了DiffSynth，一种新的方法，旨在将图像生成管道转换为视频生成管道。DiffSynth包括两个关键组件：一个抽象iteration抑制框架和一个视频抑制算法。抽象iteration抑制框架在 diffusion模型的latent空间中应用视频抑制，从而避免在中间步骤中积累干扰。此外，我们提出了一种名为贴合算法的视频抑制算法，它可以在不同的帧中重新映射对象，并将它们进行融合，以提高视频一致性。DiffSynth的一个重要优点是它可以应用于多种视频生成任务，包括文本引导视频 стилизация、时尚视频生成、图像引导视频 стилизация、视频恢复和3D渲染。在文本引导视频 стилизаation任务中，我们实现了无需筛选的高质量视频生成。实验结果表明DiffSynth的效果。所有视频都可以在我们的项目页面上查看，源代码也将被发布。
</details></li>
</ul>
<hr>
<h2 id="Cross-Silo-Prototypical-Calibration-for-Federated-Learning-with-Non-IID-Data"><a href="#Cross-Silo-Prototypical-Calibration-for-Federated-Learning-with-Non-IID-Data" class="headerlink" title="Cross-Silo Prototypical Calibration for Federated Learning with Non-IID Data"></a>Cross-Silo Prototypical Calibration for Federated Learning with Non-IID Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03457">http://arxiv.org/abs/2308.03457</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qizhuang-qz/FedCSPC">https://github.com/qizhuang-qz/FedCSPC</a></li>
<li>paper_authors: Zhuang Qi, Lei Meng, Zitan Chen, Han Hu, Hui Lin, Xiangxu Meng</li>
<li>for: This paper aims to improve the performance of federated learning by addressing the issue of dataset biases, such as heterogeneous data distributions and missing classes, through a cross-silo prototypical calibration method called FedCSPC.</li>
<li>methods: The FedCSPC method uses a Data Prototypical Modeling (DPM) module to learn data patterns via clustering, and a cross-silo prototypical calibration (CSPC) module to improve the robustness of the calibration. The CSPC module projects cross-source features into a consistent space while maintaining clear decision boundaries.</li>
<li>results: The paper shows that FedCSPC outperforms state-of-the-art methods in learning consistent features across different data sources of the same class, leading to better performance. The results are demonstrated through experiments on four datasets, including an ablation study, in-depth analysis, and case study.Here is the same information in Simplified Chinese:</li>
<li>for: 这篇论文目标是通过解决 dataset biases 问题，如不同数据源的数据分布和缺失类，提高 federated learning 的性能。</li>
<li>methods: FedCSPC 方法使用 Data Prototypical Modeling (DPM) 模块学习数据模式，并使用 cross-silo prototypical calibration (CSPC) 模块提高抽象的稳定性。CSPC 模块将 cross-source 特征投影到一致的空间中，保持明确的决策界限。</li>
<li>results: 论文表明，FedCSPC 方法在不同数据源的同一类数据上学习一致的特征，性能比 state-of-the-art 方法更好。结果通过四个数据集的实验、减少学习、深入分析和案例研究证明。<details>
<summary>Abstract</summary>
Federated Learning aims to learn a global model on the server side that generalizes to all clients in a privacy-preserving manner, by leveraging the local models from different clients. Existing solutions focus on either regularizing the objective functions among clients or improving the aggregation mechanism for the improved model generalization capability. However, their performance is typically limited by the dataset biases, such as the heterogeneous data distributions and the missing classes. To address this issue, this paper presents a cross-silo prototypical calibration method (FedCSPC), which takes additional prototype information from the clients to learn a unified feature space on the server side. Specifically, FedCSPC first employs the Data Prototypical Modeling (DPM) module to learn data patterns via clustering to aid calibration. Subsequently, the cross-silo prototypical calibration (CSPC) module develops an augmented contrastive learning method to improve the robustness of the calibration, which can effectively project cross-source features into a consistent space while maintaining clear decision boundaries. Moreover, the CSPC module's ease of implementation and plug-and-play characteristics make it even more remarkable. Experiments were conducted on four datasets in terms of performance comparison, ablation study, in-depth analysis and case study, and the results verified that FedCSPC is capable of learning the consistent features across different data sources of the same class under the guidance of calibrated model, which leads to better performance than the state-of-the-art methods. The source codes have been released at https://github.com/qizhuang-qz/FedCSPC.
</details>
<details>
<summary>摘要</summary>
federated learning 目标是在服务器端学习一个通用模型，该模型可以在保持隐私的情况下，通过客户端上的本地模型，泛化到所有客户端。现有的解决方案通常是通过客户端对象函数的规范化或改进模型聚合机制来提高模型泛化能力。然而，它们的性能通常受到数据偏好的影响，如不同数据分布和缺失类。为解决这个问题，本文提出了跨积 silence prototype 准备方法（FedCSPC），该方法通过客户端上的额外原型信息来学习服务器端的通用特征空间。具体来说，FedCSPC首先使用数据prototype模型（DPM）模块学习数据模式，以帮助准备。然后，跨积 silence prototype准备（CSPC）模块开发了一种改进的增强对比学习方法，可以有效地将跨源特征投影到一致的空间中，保持清晰的决策边界。此外，CSPC模块的实现简单易用，使其更加remarkable。经过实验，results表明，FedCSPC可以在不同数据源之间的同类型数据上学习一致的特征，从而获得更好的性能，比现有方法更好。代码已经在https://github.com/qizhuang-qz/FedCSPC上发布。
</details></li>
</ul>
<hr>
<h2 id="Lighting-Every-Darkness-in-Two-Pairs-A-Calibration-Free-Pipeline-for-RAW-Denoising"><a href="#Lighting-Every-Darkness-in-Two-Pairs-A-Calibration-Free-Pipeline-for-RAW-Denoising" class="headerlink" title="Lighting Every Darkness in Two Pairs: A Calibration-Free Pipeline for RAW Denoising"></a>Lighting Every Darkness in Two Pairs: A Calibration-Free Pipeline for RAW Denoising</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03448">http://arxiv.org/abs/2308.03448</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/srameo/led">https://github.com/srameo/led</a></li>
<li>paper_authors: Xin Jin, Jia-Wen Xiao, Ling-Hao Han, Chunle Guo, Ruixun Zhang, Xialei Liu, Chongyi Li</li>
<li>for: 提高陌生环境下 RAW 图像噪声 removal 的效果，不需要耗时consuming 的准备和训练。</li>
<li>methods: 提出了一种基于自适应学习的、不需要准备和训练的推 oscilloation 管道，可以适应目标摄像头，并在几个步骤中进行微调。</li>
<li>results: 与其他准备和训练方法相比，该方法在不同的数位增强和摄像头上实现了更高的噪声去除效果，只需要几个匹配的数据对和0.5%的迭代。<details>
<summary>Abstract</summary>
Calibration-based methods have dominated RAW image denoising under extremely low-light environments. However, these methods suffer from several main deficiencies: 1) the calibration procedure is laborious and time-consuming, 2) denoisers for different cameras are difficult to transfer, and 3) the discrepancy between synthetic noise and real noise is enlarged by high digital gain. To overcome the above shortcomings, we propose a calibration-free pipeline for Lighting Every Drakness (LED), regardless of the digital gain or camera sensor. Instead of calibrating the noise parameters and training repeatedly, our method could adapt to a target camera only with few-shot paired data and fine-tuning. In addition, well-designed structural modification during both stages alleviates the domain gap between synthetic and real noise without any extra computational cost. With 2 pairs for each additional digital gain (in total 6 pairs) and 0.5% iterations, our method achieves superior performance over other calibration-based methods. Our code is available at https://github.com/Srameo/LED .
</details>
<details>
<summary>摘要</summary>
准确基于方法在极低照度环境下进行 RAW 图像干涉除，但这些方法受到多种主要缺点的影响：1）准备过程耗时和费时consuming，2）对不同摄像头的denoiser难以传输，3）高度数字增强导致假象差异变大。为了解决以上缺陷，我们提出了不需要准备的管道，可以在不同的摄像头上适应LED，不 matter how much digital gain or camera sensor。相比之下，我们的方法只需要几个对应的数据和微调就能够适应目标摄像头。此外，我们在两个阶段中设计了结构修改，以避免假象差异的问题，无需额外的计算成本。使用2对每个额外数字增强（共计6对）和0.5%迭代，我们的方法可以在其他准确基于方法上达到更高的性能。我们的代码可以在https://github.com/Srameo/LED 中找到。
</details></li>
</ul>
<hr>
<h2 id="GaFET-Learning-Geometry-aware-Facial-Expression-Translation-from-In-The-Wild-Images"><a href="#GaFET-Learning-Geometry-aware-Facial-Expression-Translation-from-In-The-Wild-Images" class="headerlink" title="GaFET: Learning Geometry-aware Facial Expression Translation from In-The-Wild Images"></a>GaFET: Learning Geometry-aware Facial Expression Translation from In-The-Wild Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03413">http://arxiv.org/abs/2308.03413</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianxiang Ma, Bingchuan Li, Qian He, Jing Dong, Tieniu Tan</li>
<li>for: 这篇论文旨在提出一种基于 Parametric 3D 面部表示的 Geometry-aware Facial Expression Translation (GaFET) 框架，以稳定地分离表达。</li>
<li>methods: 该框架包括一种多级特征对齐变换器，用于补充非几何面部细节特征，以及一种基于 StyleGAN 的 De-expression 模型，用于降低 GaFET 在无对应图像数据的学习难度。</li>
<li>results: 广泛的 qualitative 和 quantitative 实验表明，我们的方法可以在无需视频或标注数据的情况下实现更高质量和更准确的面部表达传递结果，并能够处理多个 pose 和复杂的 texture。<details>
<summary>Abstract</summary>
While current face animation methods can manipulate expressions individually, they suffer from several limitations. The expressions manipulated by some motion-based facial reenactment models are crude. Other ideas modeled with facial action units cannot generalize to arbitrary expressions not covered by annotations. In this paper, we introduce a novel Geometry-aware Facial Expression Translation (GaFET) framework, which is based on parametric 3D facial representations and can stably decoupled expression. Among them, a Multi-level Feature Aligned Transformer is proposed to complement non-geometric facial detail features while addressing the alignment challenge of spatial features. Further, we design a De-expression model based on StyleGAN, in order to reduce the learning difficulty of GaFET in unpaired "in-the-wild" images. Extensive qualitative and quantitative experiments demonstrate that we achieve higher-quality and more accurate facial expression transfer results compared to state-of-the-art methods, and demonstrate applicability of various poses and complex textures. Besides, videos or annotated training data are omitted, making our method easier to use and generalize.
</details>
<details>
<summary>摘要</summary>
当前的面部动画方法可以分别 manipulate 表情，但它们受到一些限制。一些基于动作的面部reenactment模型中的表情被描述为粗糙。其他基于表情动作单元的想法无法泛化到未经标注的表情。在这篇论文中，我们引入了一种新的 Geometry-aware Facial Expression Translation (GaFET) 框架，它基于参数化的 3D 面部表示和可以稳定地做出表情分离。其中，一种 Multi-level Feature Aligned Transformer 被提议，以填充非 геометрические面部细节特征，同时解决空间特征的对齐问题。另外，我们设计了基于 StyleGAN 的 De-expression 模型，以降低 GaFET 在无标注 "在野" 图像上学习的困难性。广泛的质量和量测试表明，我们可以在比例表情传输中获得更高质量和更准确的结果，并在不同的姿势和复杂的文化上进行应用。此外，我们不需要视频或标注训练数据，使我们的方法更容易使用和泛化。
</details></li>
</ul>
<hr>
<h2 id="A-Horse-with-no-Labels-Self-Supervised-Horse-Pose-Estimation-from-Unlabelled-Images-and-Synthetic-Prior"><a href="#A-Horse-with-no-Labels-Self-Supervised-Horse-Pose-Estimation-from-Unlabelled-Images-and-Synthetic-Prior" class="headerlink" title="A Horse with no Labels: Self-Supervised Horse Pose Estimation from Unlabelled Images and Synthetic Prior"></a>A Horse with no Labels: Self-Supervised Horse Pose Estimation from Unlabelled Images and Synthetic Prior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03411">http://arxiv.org/abs/2308.03411</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jose Sosa, David Hogg</li>
<li>for: 用于 estimating animal pose 的深度学习方法的训练</li>
<li>methods: 使用自我超vised学习方法，只需要无标注的图像和少量的Synthetic 2D pose</li>
<li>results: 可以准确地学习动物姿态，只需要一小部分的Synthetic 2D pose和无标注图像<details>
<summary>Abstract</summary>
Obtaining labelled data to train deep learning methods for estimating animal pose is challenging. Recently, synthetic data has been widely used for pose estimation tasks, but most methods still rely on supervised learning paradigms utilising synthetic images and labels. Can training be fully unsupervised? Is a tiny synthetic dataset sufficient? What are the minimum assumptions that we could make for estimating animal pose? Our proposal addresses these questions through a simple yet effective self-supervised method that only assumes the availability of unlabelled images and a small set of synthetic 2D poses. We completely remove the need for any 3D or 2D pose annotations (or complex 3D animal models), and surprisingly our approach can still learn accurate 3D and 2D poses simultaneously. We train our method with unlabelled images of horses mainly collected for YouTube videos and a prior consisting of 2D synthetic poses. The latter is three times smaller than the number of images needed for training. We test our method on a challenging set of horse images and evaluate the predicted 3D and 2D poses. We demonstrate that it is possible to learn accurate animal poses even with as few assumptions as unlabelled images and a small set of 2D poses generated from synthetic data. Given the minimum requirements and the abundance of unlabelled data, our method could be easily deployed to different animals.
</details>
<details>
<summary>摘要</summary>
获取标注数据来训练深度学习方法用于动物姿态估计是具有挑战性的。现在，人工生成数据广泛使用于姿态估计任务中，但大多数方法仍然采用指导学习 парадигмы，使用人工图像和标签。可以完全无监督培训吗？一个小型的人工数据集是足够吗？我们的提议通过一种简单 yet effective的自我监督方法来回答这些问题。我们只需要没有标注的图像和一小组 synthetic 2D 姿态作为假设。我们完全 removing the need for any 3D or 2D pose annotations (或复杂的 3D 动物模型)，并且我们的方法可以在缺乏标注的情况下学习准确的 3D 和 2D 姿态。我们使用 YouTube 上收集的大量无标注图像和一小组 synthetic 2D 姿态来训练我们的方法。后者的数量只是图像的三倍。我们对一组具有挑战性的马图像进行测试，并评估预测的 3D 和 2D 姿态。我们示出了可以通过使用只有无标注图像和少量 synthetic 2D 姿态来学习准确的动物姿态。由于最小的假设和丰富的无标注数据，我们的方法可以轻松应用于不同的动物。
</details></li>
</ul>
<hr>
<h2 id="DiT-Efficient-Vision-Transformers-with-Dynamic-Token-Routing"><a href="#DiT-Efficient-Vision-Transformers-with-Dynamic-Token-Routing" class="headerlink" title="DiT: Efficient Vision Transformers with Dynamic Token Routing"></a>DiT: Efficient Vision Transformers with Dynamic Token Routing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03409">http://arxiv.org/abs/2308.03409</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/maycbj/dit">https://github.com/maycbj/dit</a></li>
<li>paper_authors: Yuchen Ma, Zhengcong Fei, Junshi Huang</li>
<li>for:  ImageNet classification, object detection, instance segmentation, and semantic segmentation</li>
<li>methods:  Data-dependent token routing strategy for Dynamic Vision Transformer (DiT) with differentiable routing gates for multi-path feature propagation, and budget constraints for routing gate and early-stopping of feature extraction.</li>
<li>results:  Superior performance and favorable complexity&#x2F;accuracy trade-offs compared to many State-of-the-Art (SoTA) methods on various vision tasks, with the DiT-B5 achieving 84.8% top-1 Acc on ImageNet with 10.3 GFLOPs, which is 1.0% higher than the SoTA method with similar computational complexity.<details>
<summary>Abstract</summary>
Recently, the tokens of images share the same static data flow in many dense networks. However, challenges arise from the variance among the objects in images, such as large variations in the spatial scale and difficulties of recognition for visual entities. In this paper, we propose a data-dependent token routing strategy to elaborate the routing paths of image tokens for Dynamic Vision Transformer, dubbed DiT. The proposed framework generates a data-dependent path per token, adapting to the object scales and visual discrimination of tokens. In feed-forward, the differentiable routing gates are designed to select the scaling paths and feature transformation paths for image tokens, leading to multi-path feature propagation. In this way, the impact of object scales and visual discrimination of image representation can be carefully tuned. Moreover, the computational cost can be further reduced by giving budget constraints to the routing gate and early-stopping of feature extraction. In experiments, our DiT achieves superior performance and favorable complexity/accuracy trade-offs than many SoTA methods on ImageNet classification, object detection, instance segmentation, and semantic segmentation. Particularly, the DiT-B5 obtains 84.8\% top-1 Acc on ImageNet with 10.3 GFLOPs, which is 1.0\% higher than that of the SoTA method with similar computational complexity. These extensive results demonstrate that DiT can serve as versatile backbones for various vision tasks.
</details>
<details>
<summary>摘要</summary>
近期，图像token在多密网络中共享同样的静态数据流。然而，图像中对象的变化带来了挑战，包括巨大的空间缩放和视觉特征的识别困难。在这篇论文中，我们提出了基于数据依赖的图像token路由策略，用于强化图像Token的路由方式。我们的框架生成了基于数据的路由路径，以适应图像中对象的尺度和视觉特征。在Feed-Forward中，我们设计了可微分的路由门，以选择缩放路径和特征转换路径，从而实现多路径特征传播。这样，我们可以细化对象的尺度和视觉特征的影响。此外，我们还可以通过对路由门进行予算限制和早期停止特征提取来降低计算成本。在实验中，我们的DiT在ImageNet分类、物体检测、实例 segmentation和semantic segmentation等多种视觉任务上显示出了优秀的性能和计算复杂度/准确率的平衡。尤其是DiT-B5在ImageNet上取得了84.8%的权重排名第一位，与同等计算复杂度的SoTA方法相比，提高了1.0%的性能。这些广泛的结果表明，DiT可以作为多种视觉任务的 versatile 背部。
</details></li>
</ul>
<hr>
<h2 id="Spatially-Varying-Nanophotonic-Neural-Networks"><a href="#Spatially-Varying-Nanophotonic-Neural-Networks" class="headerlink" title="Spatially Varying Nanophotonic Neural Networks"></a>Spatially Varying Nanophotonic Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03407">http://arxiv.org/abs/2308.03407</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaixuan Wei, Xiao Li, Johannes Froech, Praneeth Chakravarthula, James Whitehead, Ethan Tseng, Arka Majumdar, Felix Heide</li>
<li>for: This paper aims to improve the performance of optical neural networks for image recognition tasks, with the goal of bringing optical neural networks into the modern deep learning era.</li>
<li>methods: The paper introduces a large-kernel spatially-varying convolutional neural network learned via low-dimensional reparameterization techniques, and experiments with a flat meta-optical system that includes an array of nanophotonic structures to induce angle-dependent responses.</li>
<li>results: The paper achieves a blind test classification accuracy of 73.80% on the CIFAR-10 dataset with a nanophotonic neural network, outperforming the first modern digital neural network (AlexNet) with 57M parameters and bringing optical neural networks into the modern deep learning era.<details>
<summary>Abstract</summary>
The explosive growth of computation and energy cost of artificial intelligence has spurred strong interests in new computing modalities as potential alternatives to conventional electronic processors. Photonic processors that execute operations using photons instead of electrons, have promised to enable optical neural networks with ultra-low latency and power consumption. However, existing optical neural networks, limited by the underlying network designs, have achieved image recognition accuracy much lower than state-of-the-art electronic neural networks. In this work, we close this gap by introducing a large-kernel spatially-varying convolutional neural network learned via low-dimensional reparameterization techniques. We experimentally instantiate the network with a flat meta-optical system that encompasses an array of nanophotonic structures designed to induce angle-dependent responses. Combined with an extremely lightweight electronic backend with approximately 2K parameters we demonstrate a nanophotonic neural network reaches 73.80\% blind test classification accuracy on CIFAR-10 dataset, and, as such, the first time, an optical neural network outperforms the first modern digital neural network -- AlexNet (72.64\%) with 57M parameters, bringing optical neural network into modern deep learning era.
</details>
<details>
<summary>摘要</summary>
“计算和人工智能的能源成本的快速增长已经促使了新的计算模式的兴趣，以代替传统的电子处理器。光学处理器可以通过光子而非电子来执行操作，承诺了可以实现光学神经网络的超低延迟和能耗。然而，现有的光学神经网络，受到基础网络设计的限制，只能达到图像识别精度远低于电子神经网络的状态OF-the-art。在这种工作中，我们封闭了这个差距，通过大核心空间变化的干扰 convolutional neural network 的学习，并通过低维度重parameterization技术来实现。我们实际实现了这种网络，使用一个平面 meta-光学系统，包括一个数组 nanophotonic 结构，以induce 角度相关的响应。与此同时，我们还使用一个非常轻量级的电子后续，包含约 2K 参数，并证明了一个 nanophotonic 神经网络可以在 CIFAR-10 数据集上达到 73.80% 的盲测精度，超过了 AlexNet （72.64%）的精度，这是首次，光学神经网络超越了第一代现代数字神经网络， bringing optical neural network into modern deep learning era。”
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Nucleus-Segmentation-with-HARU-Net-A-Hybrid-Attention-Based-Residual-U-Blocks-Network"><a href="#Enhancing-Nucleus-Segmentation-with-HARU-Net-A-Hybrid-Attention-Based-Residual-U-Blocks-Network" class="headerlink" title="Enhancing Nucleus Segmentation with HARU-Net: A Hybrid Attention Based Residual U-Blocks Network"></a>Enhancing Nucleus Segmentation with HARU-Net: A Hybrid Attention Based Residual U-Blocks Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03382">http://arxiv.org/abs/2308.03382</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junzhou Chen, Qian Huang, Yulin Chen, Linyi Qian, Chengyuan Yu</li>
<li>for: 本研究主要用于提高核体实例分割的精度和效果，解决现有方法受到质量问题和较复杂的细胞聚集等问题的限制。</li>
<li>methods: 我们提出了一种基于双支网络和混合注意力径 residual U-块的核体实例分割方法，同时预测目标信息和目标 kontour。我们还提出了一种后处理方法，通过组合目标信息和目标 kontour来分辨重叠的核体并生成实例分割图像。网络中还包括一个Context Fusion块（CF-块），可以有效地提取和融合网络中的Contextual信息。</li>
<li>results: 我们对多个数据集进行了广泛的量化评估，并证明了我们的方法在BNS、MoNuSeg、CoNSeg和CPM-17等数据集上的性能superiority compared to state-of-the-art methods。<details>
<summary>Abstract</summary>
Nucleus image segmentation is a crucial step in the analysis, pathological diagnosis, and classification, which heavily relies on the quality of nucleus segmentation. However, the complexity of issues such as variations in nucleus size, blurred nucleus contours, uneven staining, cell clustering, and overlapping cells poses significant challenges. Current methods for nucleus segmentation primarily rely on nuclear morphology or contour-based approaches. Nuclear morphology-based methods exhibit limited generalization ability and struggle to effectively predict irregular-shaped nuclei, while contour-based extraction methods face challenges in accurately segmenting overlapping nuclei. To address the aforementioned issues, we propose a dual-branch network using hybrid attention based residual U-blocks for nucleus instance segmentation. The network simultaneously predicts target information and target contours. Additionally, we introduce a post-processing method that combines the target information and target contours to distinguish overlapping nuclei and generate an instance segmentation image. Within the network, we propose a context fusion block (CF-block) that effectively extracts and merges contextual information from the network. Extensive quantitative evaluations are conducted to assess the performance of our method. Experimental results demonstrate the superior performance of the proposed method compared to state-of-the-art approaches on the BNS, MoNuSeg, CoNSeg, and CPM-17 datasets.
</details>
<details>
<summary>摘要</summary>
核心像素分割是生物学分析、诊断和分类中一个关键步骤，但是这个步骤受到核心像素质量的限制。然而，核心像素的变化、模糊、不均匀染料、细胞堆叠和重叠细胞等问题带来了挑战。现有的核心像素分割方法主要基于核心形态或边缘检测方法。核心形态基本方法具有局限性，难以预测不规则形状的核心，而边缘检测方法在重叠细胞上受到检测的挑战。为了解决以上问题，我们提议一种基于双分支网络的核心实例分割方法。该方法同时预测目标信息和目标边界。此外，我们还提出了一种兼容处理方法，通过将目标信息和目标边界结合起来，以解决重叠细胞的问题。在网络中，我们提出了一个上下文融合块（CF-块），可以有效地抽取和融合网络中的上下文信息。我们对方法的性能进行了广泛的量化评估。实验结果表明，我们提出的方法在BNS、MoNuSeg、CoNSeg和CPM-17等数据集上的性能明显超过了现有方法。
</details></li>
</ul>
<hr>
<h2 id="Bilevel-Generative-Learning-for-Low-Light-Vision"><a href="#Bilevel-Generative-Learning-for-Low-Light-Vision" class="headerlink" title="Bilevel Generative Learning for Low-Light Vision"></a>Bilevel Generative Learning for Low-Light Vision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03381">http://arxiv.org/abs/2308.03381</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yingchi1998/bgl">https://github.com/yingchi1998/bgl</a></li>
<li>paper_authors: Yingchi Liu, Zhu Liu, Long Ma, Jinyuan Liu, Xin Fan, Zhongxuan Luo, Risheng Liu</li>
<li>For: The paper is written for constructing deep learning schemes for Low-Light Vision (LLV) tasks.* Methods: The paper proposes a generic low-light vision solution by introducing a generative block to convert data from the RAW to the RGB domain, and establishes a bilevel model to precisely characterize the latent correspondence between the generative procedure and the vision task.* Results: The paper demonstrates the superiority of the proposed approach on three representative low-light vision tasks, namely enhancement, detection, and segmentation, and shows that the generative blocks have a strong generalization ability in other low-light vision tasks.Here is the information in Simplified Chinese text:* For: 这篇论文是为了构建深度学习方案来解决低光环境视觉任务。* Methods: 论文提出了一种通用的低光环境视觉解决方案，利用生成块将数据从RAW转换到RGB频谱上，并建立了一个碎谱模型来准确地描述数据生成过程和视觉任务之间的隐藏关系。* Results: 论文在三个表示低光环境视觉任务的示例任务上，即提升、检测和分割任务上，展现了提案的方法的超越性，并证明了生成块在其他低光环境视觉任务中具有强大的普适性。<details>
<summary>Abstract</summary>
Recently, there has been a growing interest in constructing deep learning schemes for Low-Light Vision (LLV). Existing techniques primarily focus on designing task-specific and data-dependent vision models on the standard RGB domain, which inherently contain latent data associations. In this study, we propose a generic low-light vision solution by introducing a generative block to convert data from the RAW to the RGB domain. This novel approach connects diverse vision problems by explicitly depicting data generation, which is the first in the field. To precisely characterize the latent correspondence between the generative procedure and the vision task, we establish a bilevel model with the parameters of the generative block defined as the upper level and the parameters of the vision task defined as the lower level. We further develop two types of learning strategies targeting different goals, namely low cost and high accuracy, to acquire a new bilevel generative learning paradigm. The generative blocks embrace a strong generalization ability in other low-light vision tasks through the bilevel optimization on enhancement tasks. Extensive experimental evaluations on three representative low-light vision tasks, namely enhancement, detection, and segmentation, fully demonstrate the superiority of our proposed approach. The code will be available at https://github.com/Yingchi1998/BGL.
</details>
<details>
<summary>摘要</summary>
近些年来，低光环境视觉（LLV）领域内有一个增长的兴趣，现有技术主要集中在设计任务特定和数据依赖的视觉模型上标准RGB频谱上，这些模型内置了隐藏的数据关系。在这种研究中，我们提出了一种通用的低光环境解决方案，通过引入生成块将数据从RAW频谱转换到RGB频谱。这种新的approach连接了多种视觉问题，并且显式地描述了数据生成过程，这是领域内首次。为准确地描述生成过程和视觉任务之间的隐藏关系，我们建立了一个二级模型，其中生成块的参数定义为上层级，而视觉任务的参数定义为下层级。我们还开发了两种不同目标，即低成本和高精度的学习策略，以获得一种新的二级生成学习 парадиг。生成块具有强大的通用能力在其他低光环境任务上，经过二级优化的增强任务上。我们在三个代表性的低光环境任务上，即增强、检测和 segmentation 上进行了广泛的实验评估，并证明了我们提出的方法的超越性。代码将在https://github.com/Yingchi1998/BGL中提供。
</details></li>
</ul>
<hr>
<h2 id="VR-based-body-tracking-to-stimulate-musculoskeletal-training"><a href="#VR-based-body-tracking-to-stimulate-musculoskeletal-training" class="headerlink" title="VR-based body tracking to stimulate musculoskeletal training"></a>VR-based body tracking to stimulate musculoskeletal training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03375">http://arxiv.org/abs/2308.03375</a></li>
<li>repo_url: None</li>
<li>paper_authors: M. Neidhardt, S. Gerlach F. N. Schmidt, I. A. K. Fiedler, S. Grube, B. Busse, A. Schlaefer</li>
<li>For: 这个研究旨在开发一个基于HoloLens 2的虚拟下山滑雪训练应用程序，以便为老年人和残疾人提供个性化的训练和自动化评估。* Methods: 这个研究使用HoloLens 2的运动数据来控制和预测身体运动和关节角度 during musculoskeletal training。研究者记录了10名健康志愿者的外部跟踪相机数据，并系统地分析了整个身体运动是否可以从HoloLens 2运动数据中 derivation。* Results: 研究结果显示，HoloLens 2 运动数据和外部跟踪数据之间存在高度相关性，特别是在上半身运动和下肢关节角度方面。无参与者报告了运动疲劳效应，所有参与者都能快速互动和控制他们的运动。<details>
<summary>Abstract</summary>
Training helps to maintain and improve sufficient muscle function, body control, and body coordination. These are important to reduce the risk of fracture incidents caused by falls, especially for the elderly or people recovering from injury. Virtual reality training can offer a cost-effective and individualized training experience. We present an application for the HoloLens 2 to enable musculoskeletal training for elderly and impaired persons to allow for autonomous training and automatic progress evaluation. We designed a virtual downhill skiing scenario that is controlled by body movement to stimulate balance and body control. By adapting the parameters of the ski slope, we can tailor the intensity of the training to individual users. In this work, we evaluate whether the movement data of the HoloLens 2 alone is sufficient to control and predict body movement and joint angles during musculoskeletal training. We record the movements of 10 healthy volunteers with external tracking cameras and track a set of body and joint angles of the participant during training. We estimate correlation coefficients and systematically analyze whether whole body movement can be derived from the movement data of the HoloLens 2. No participant reports movement sickness effects and all were able to quickly interact and control their movement during skiing. Our results show a high correlation between HoloLens 2 movement data and the external tracking of the upper body movement and joint angles of the lower limbs.
</details>
<details>
<summary>摘要</summary>
训练可以保持和改善足够的肌肉功能、身体控制和身体协调。这些因素对降低因为落下而导致骨折的风险非常重要，特别是老年人或恢复后的人。虚拟现实训练可以提供成本效益和个性化的训练经验。我们在HoloLens 2上提出了一个应用程序，用于帮助老年人和残疾人进行肌骨征识训练，以便在自主训练和自动进度评估之间进行折衔。我们设计了一个虚拟下山滑雪场景，通过身体运动控制来刺激平衡和身体协调。通过调整雪坡参数，我们可以根据用户的个性进行定制训练的Intensity。在这项工作中，我们评估了HoloLens 2运动数据是否充分控制和预测身体运动和关节角度 durante 肌骨征识训练。我们通过外部跟踪相机记录参与者的运动，并跟踪参与者的身体运动和关节角度。我们计算了相关系数，系统地分析了整体运动是否可以从HoloLens 2运动数据中提取出来。所有参与者都没有报告运动药效，并且所有参与者快速交互和控制他们的运动 durante 滑雪。我们的结果显示，HoloLens 2运动数据与外部跟踪的上半身运动和关节角度之间存在高相关性。
</details></li>
</ul>
<hr>
<h2 id="Heterogeneous-Forgetting-Compensation-for-Class-Incremental-Learning"><a href="#Heterogeneous-Forgetting-Compensation-for-Class-Incremental-Learning" class="headerlink" title="Heterogeneous Forgetting Compensation for Class-Incremental Learning"></a>Heterogeneous Forgetting Compensation for Class-Incremental Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03374">http://arxiv.org/abs/2308.03374</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiahuadong/hfc">https://github.com/jiahuadong/hfc</a></li>
<li>paper_authors: Jiahua Dong, Wenqi Liang, Yang Cong, Gan Sun</li>
<li>for: 这篇论文的目的是解决累累忘记挑战，并且处理忘记不均的问题。</li>
<li>methods: 这篇论文提出了一个名为“多元忘记补偿”（Heterogeneous Forgetting Compensation，HFC）的新模型，它可以解决忘记不均的问题，并且从表征和对应方面进行补偿。</li>
<li>results: 实验结果显示，HFC模型能够有效地解决累累忘记挑战，并且在不同的数据集上获得了良好的性能。<details>
<summary>Abstract</summary>
Class-incremental learning (CIL) has achieved remarkable successes in learning new classes consecutively while overcoming catastrophic forgetting on old categories. However, most existing CIL methods unreasonably assume that all old categories have the same forgetting pace, and neglect negative influence of forgetting heterogeneity among different old classes on forgetting compensation. To surmount the above challenges, we develop a novel Heterogeneous Forgetting Compensation (HFC) model, which can resolve heterogeneous forgetting of easy-to-forget and hard-to-forget old categories from both representation and gradient aspects. Specifically, we design a task-semantic aggregation block to alleviate heterogeneous forgetting from representation aspect. It aggregates local category information within each task to learn task-shared global representations. Moreover, we develop two novel plug-and-play losses: a gradient-balanced forgetting compensation loss and a gradient-balanced relation distillation loss to alleviate forgetting from gradient aspect. They consider gradient-balanced compensation to rectify forgetting heterogeneity of old categories and heterogeneous relation consistency. Experiments on several representative datasets illustrate effectiveness of our HFC model. The code is available at https://github.com/JiahuaDong/HFC.
</details>
<details>
<summary>摘要</summary>
CLASS-INCREMENTAL LEARNING (CIL) 已经取得了不可忽略的成功，可以顺序学习新的类型，同时解决旧类型的恐怖忘记。然而，大多数现有的 CIL 方法不合理地假设所有的旧类型忘记速率相同，并忽略了旧类型忘记不同程度的负面影响。为超越这些挑战，我们开发了一种新的多类忘记补偿模型（HFC），可以解决旧类型的多类忘记问题。 Specifically, we design a task-semantic aggregation block to alleviate heterogeneous forgetting from representation aspect. It aggregates local category information within each task to learn task-shared global representations. Moreover, we develop two novel plug-and-play losses: a gradient-balanced forgetting compensation loss and a gradient-balanced relation distillation loss to alleviate forgetting from gradient aspect. They consider gradient-balanced compensation to rectify forgetting heterogeneity of old categories and heterogeneous relation consistency.实验结果表明，我们的 HFC 模型具有效果。代码可以在 <https://github.com/JiahuaDong/HFC> 上下载。
</details></li>
</ul>
<hr>
<h2 id="Dual-Aggregation-Transformer-for-Image-Super-Resolution"><a href="#Dual-Aggregation-Transformer-for-Image-Super-Resolution" class="headerlink" title="Dual Aggregation Transformer for Image Super-Resolution"></a>Dual Aggregation Transformer for Image Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03364">http://arxiv.org/abs/2308.03364</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhengchen1999/dat">https://github.com/zhengchen1999/dat</a></li>
<li>paper_authors: Zheng Chen, Yulun Zhang, Jinjin Gu, Linghe Kong, Xiaokang Yang, Fisher Yu</li>
<li>for: 这个论文主要针对图像超解像（SR）问题，旨在提出一种基于Transformer网络的新型图像SR模型，以提高图像 Representation 能力。</li>
<li>methods: 该模型叫做 dual aggregation transformer（DAT），它在不同维度上进行自我集成，包括间隔块和内部块两种方式。具体来说，我们在连续的Transformer块中 alternate 应用空间和通道维度上的自我集成。此外，我们还提出了适应交互模块（AIM）和空间网络（SGFN）来实现内部块特征聚合。</li>
<li>results: 我们的DAT模型在多个实验中表现出色，超过了当前的方法。代码和模型可以在<a target="_blank" rel="noopener" href="https://github.com/zhengchen1999/DAT">https://github.com/zhengchen1999/DAT</a> 上下载。<details>
<summary>Abstract</summary>
Transformer has recently gained considerable popularity in low-level vision tasks, including image super-resolution (SR). These networks utilize self-attention along different dimensions, spatial or channel, and achieve impressive performance. This inspires us to combine the two dimensions in Transformer for a more powerful representation capability. Based on the above idea, we propose a novel Transformer model, Dual Aggregation Transformer (DAT), for image SR. Our DAT aggregates features across spatial and channel dimensions, in the inter-block and intra-block dual manner. Specifically, we alternately apply spatial and channel self-attention in consecutive Transformer blocks. The alternate strategy enables DAT to capture the global context and realize inter-block feature aggregation. Furthermore, we propose the adaptive interaction module (AIM) and the spatial-gate feed-forward network (SGFN) to achieve intra-block feature aggregation. AIM complements two self-attention mechanisms from corresponding dimensions. Meanwhile, SGFN introduces additional non-linear spatial information in the feed-forward network. Extensive experiments show that our DAT surpasses current methods. Code and models are obtainable at https://github.com/zhengchen1999/DAT.
</details>
<details>
<summary>摘要</summary>
“传统抽象 transformer 在低级视觉任务中，如像高清化（SR）中得到了很大的推广。这些网络使用自我对齐在不同的维度，包括空间和通道维度，并实现了非常出色的表现。这给我们启发了融合这两种维度的想法，我们提出了一个新的 transformer 模型，即双总化 transformer（DAT），用于图像 SR。我们的 DAT 在内部和外部两种方式进行特征聚合，即在不同的维度进行双重总化。具体来说，我们在连续的 transformer 层中交替应用空间和通道自我对齐。这种交替策略使得 DAT 能够捕捉全域上下文，并实现内部对齐的特征聚合。此外，我们还提出了适应互动模组（AIM）和空间闸道对应网络（SGFN），以实现内部对齐的特征聚合。AIM 对应了两个自我对齐机制，而 SGFN 则引入了额外的非线性空间信息。实验结果显示，我们的 DAT 超过了目前的方法。代码和模型可以在 GitHub 上获取：https://github.com/zhengchen1999/DAT。”
</details></li>
</ul>
<hr>
<h2 id="Distortion-aware-Transformer-in-360°-Salient-Object-Detection"><a href="#Distortion-aware-Transformer-in-360°-Salient-Object-Detection" class="headerlink" title="Distortion-aware Transformer in 360° Salient Object Detection"></a>Distortion-aware Transformer in 360° Salient Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03359">http://arxiv.org/abs/2308.03359</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yinjie Zhao, Lichen Zhao, Qian Yu, Jing Zhang, Lu Sheng, Dong Xu</li>
<li>for: addressing the distortion problem in 360{\deg} data projection for feature extraction and task development</li>
<li>methods: using a Transformer-based model called DATFormer with two distortion-adaptive modules and a learnable relation matrix for positional embedding</li>
<li>results: outperforming existing 2D SOD and 360 SOD methods on three public datasets<details>
<summary>Abstract</summary>
With the emergence of VR and AR, 360{\deg} data attracts increasing attention from the computer vision and multimedia communities. Typically, 360{\deg} data is projected into 2D ERP (equirectangular projection) images for feature extraction. However, existing methods cannot handle the distortions that result from the projection, hindering the development of 360-data-based tasks. Therefore, in this paper, we propose a Transformer-based model called DATFormer to address the distortion problem. We tackle this issue from two perspectives. Firstly, we introduce two distortion-adaptive modules. The first is a Distortion Mapping Module, which guides the model to pre-adapt to distorted features globally. The second module is a Distortion-Adaptive Attention Block that reduces local distortions on multi-scale features. Secondly, to exploit the unique characteristics of 360{\deg} data, we present a learnable relation matrix and use it as part of the positional embedding to further improve performance. Extensive experiments are conducted on three public datasets, and the results show that our model outperforms existing 2D SOD (salient object detection) and 360 SOD methods.
</details>
<details>
<summary>摘要</summary>
Firstly, we introduce two distortion-adaptive modules:1. Distortion Mapping Module: This module guides the model to pre-adapt to distorted features globally.2. Distortion-Adaptive Attention Block: This module reduces local distortions on multi-scale features.Secondly, to exploit the unique characteristics of 360° data, we present a learnable relation matrix and use it as part of the positional embedding to further improve performance.Extensive experiments are conducted on three public datasets, and the results show that our model outperforms existing 2D SOD (salient object detection) and 360 SOD methods.
</details></li>
</ul>
<hr>
<h2 id="Energy-Guided-Diffusion-Model-for-CBCT-to-CT-Synthesis"><a href="#Energy-Guided-Diffusion-Model-for-CBCT-to-CT-Synthesis" class="headerlink" title="Energy-Guided Diffusion Model for CBCT-to-CT Synthesis"></a>Energy-Guided Diffusion Model for CBCT-to-CT Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03354">http://arxiv.org/abs/2308.03354</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linjie Fu, Xia Li, Xiuding Cai, Dong Miao, Yu Yao, Yali Shen</li>
<li>for: 提高CBCT图像质量和Hounsfield单位准确性，以便更好地计算辐射剂量和精确地定位组织结构。</li>
<li>methods: 基于能量导向分散模型（EGDiff），从CBCT图像生成Synthetic CT（sCT）。</li>
<li>results: 对胸肿囊数据集进行实验，EGDiff方法可以生成高精度、高视觉质量的sCT图像，与State-of-the-art无监督合成方法相比，EGDiff方法表现出色。<details>
<summary>Abstract</summary>
Cone Beam CT (CBCT) plays a crucial role in Adaptive Radiation Therapy (ART) by accurately providing radiation treatment when organ anatomy changes occur. However, CBCT images suffer from scatter noise and artifacts, making relying solely on CBCT for precise dose calculation and accurate tissue localization challenging. Therefore, there is a need to improve CBCT image quality and Hounsfield Unit (HU) accuracy while preserving anatomical structures. To enhance the role and application value of CBCT in ART, we propose an energy-guided diffusion model (EGDiff) and conduct experiments on a chest tumor dataset to generate synthetic CT (sCT) from CBCT. The experimental results demonstrate impressive performance with an average absolute error of 26.87$\pm$6.14 HU, a structural similarity index measurement of 0.850$\pm$0.03, a peak signal-to-noise ratio of the sCT of 19.83$\pm$1.39 dB, and a normalized cross-correlation of the sCT of 0.874$\pm$0.04. These results indicate that our method outperforms state-of-the-art unsupervised synthesis methods in accuracy and visual quality, producing superior sCT images.
</details>
<details>
<summary>摘要</summary>
cone beam CT (CBCT) 在 adaptive radiation therapy (ART) 中发挥重要作用，准确地提供辐射治疗当器官结构变化时。然而，CBCT图像受到散射噪和artefacts的影响，使凭借CBCT alone 精度计算和正确地本地化难以准确。因此，我们需要提高 CBCT 图像质量和Hounsfield单元（HU）准确性，保持器官结构。为了提高 CBCT 在 ART 中的应用价值，我们提议一种能量引导扩散模型（EGDiff），并在胸腔肿瘤数据集上进行实验，将 CBCT 转换成 synthetic CT（sCT）。实验结果表明，我们的方法可以达到 impressive 性能，其中平均绝对错误为26.87±6.14 HU，结构相似度指数为0.850±0.03，峰信号噪声比（PSNR）为19.83±1.39 dB，同步协方差为0.874±0.04。这些结果表明，我们的方法在准确性和视觉质量方面都有所提高，生成出Superior sCT 图像。
</details></li>
</ul>
<hr>
<h2 id="Explicifying-Neural-Implicit-Fields-for-Efficient-Dynamic-Human-Avatar-Modeling-via-a-Neural-Explicit-Surface"><a href="#Explicifying-Neural-Implicit-Fields-for-Efficient-Dynamic-Human-Avatar-Modeling-via-a-Neural-Explicit-Surface" class="headerlink" title="Explicifying Neural Implicit Fields for Efficient Dynamic Human Avatar Modeling via a Neural Explicit Surface"></a>Explicifying Neural Implicit Fields for Efficient Dynamic Human Avatar Modeling via a Neural Explicit Surface</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05112">http://arxiv.org/abs/2308.05112</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruiqi Zhang, Jie Chen, Qiang Wang</li>
<li>for: 这 paper 旨在提出一种方法，用于高效地模型动态人体。</li>
<li>methods: 该 paper 使用 Neural Explicit Surface (NES) 技术来Explicify implicit neural fields，提高了计算和存储效率。</li>
<li>results: 实验表明，NES 能够与前一代3D方法相比，具有类似的性能，同时提高渲染速度和减少存储开销。<details>
<summary>Abstract</summary>
This paper proposes a technique for efficiently modeling dynamic humans by explicifying the implicit neural fields via a Neural Explicit Surface (NES). Implicit neural fields have advantages over traditional explicit representations in modeling dynamic 3D content from sparse observations and effectively representing complex geometries and appearances. Implicit neural fields defined in 3D space, however, are expensive to render due to the need for dense sampling during volumetric rendering. Moreover, their memory efficiency can be further optimized when modeling sparse 3D space. To overcome these issues, the paper proposes utilizing Neural Explicit Surface (NES) to explicitly represent implicit neural fields, facilitating memory and computational efficiency. To achieve this, the paper creates a fully differentiable conversion between the implicit neural fields and the explicit rendering interface of NES, leveraging the strengths of both implicit and explicit approaches. This conversion enables effective training of the hybrid representation using implicit methods and efficient rendering by integrating the explicit rendering interface with a newly proposed rasterization-based neural renderer that only incurs a texture color query once for the initial ray interaction with the explicit surface, resulting in improved inference efficiency. NES describes dynamic human geometries with pose-dependent neural implicit surface deformation fields and their dynamic neural textures both in 2D space, which is a more memory-efficient alternative to traditional 3D methods, reducing redundancy and computational load. The comprehensive experiments show that NES performs similarly to previous 3D approaches, with greatly improved rendering speed and reduced memory cost.
</details>
<details>
<summary>摘要</summary>
The paper creates a fully differentiable conversion between the implicit neural fields and the explicit rendering interface of NES, allowing for effective training of the hybrid representation using implicit methods and efficient rendering. The conversion enables the use of a rasterization-based neural renderer that only incurs a texture color query once for the initial ray interaction with the explicit surface, resulting in improved inference efficiency.NES describes dynamic human geometries with pose-dependent neural implicit surface deformation fields and their dynamic neural textures in 2D space, which is a more memory-efficient alternative to traditional 3D methods, reducing redundancy and computational load. The comprehensive experiments show that NES performs similarly to previous 3D approaches, with greatly improved rendering speed and reduced memory cost.
</details></li>
</ul>
<hr>
<h2 id="Cooperative-Colorization-Exploring-Latent-Cross-Domain-Priors-for-NIR-Image-Spectrum-Translation"><a href="#Cooperative-Colorization-Exploring-Latent-Cross-Domain-Priors-for-NIR-Image-Spectrum-Translation" class="headerlink" title="Cooperative Colorization: Exploring Latent Cross-Domain Priors for NIR Image Spectrum Translation"></a>Cooperative Colorization: Exploring Latent Cross-Domain Priors for NIR Image Spectrum Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03348">http://arxiv.org/abs/2308.03348</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingxing Yang, Jie Chen, Zaifeng Yang<br>for: 这篇论文主要targets near-infrared (NIR) image spectrum translation, a challenging problem with many promising applications.methods: 该方法基于一种合作学习模式，通过exploring latent cross-domain priors（i.e., latent spectrum context priors and task domain priors），colorizes NIR images in parallel with another proxy grayscale colorization task.results: 该方法可以生成高质量的spectrum translation输出，并且比 estado-of-the-art counterparts提高3.95dB和4.66dB的PNSR для NIR和grayscale colorization tasks。<details>
<summary>Abstract</summary>
Near-infrared (NIR) image spectrum translation is a challenging problem with many promising applications. Existing methods struggle with the mapping ambiguity between the NIR and the RGB domains, and generalize poorly due to the limitations of models' learning capabilities and the unavailability of sufficient NIR-RGB image pairs for training. To address these challenges, we propose a cooperative learning paradigm that colorizes NIR images in parallel with another proxy grayscale colorization task by exploring latent cross-domain priors (i.e., latent spectrum context priors and task domain priors), dubbed CoColor. The complementary statistical and semantic spectrum information from these two task domains -- in the forms of pre-trained colorization networks -- are brought in as task domain priors. A bilateral domain translation module is subsequently designed, in which intermittent NIR images are generated from grayscale and colorized in parallel with authentic NIR images; and vice versa for the grayscale images. These intermittent transformations act as latent spectrum context priors for efficient domain knowledge exchange. We progressively fine-tune and fuse these modules with a series of pixel-level and feature-level consistency constraints. Experiments show that our proposed cooperative learning framework produces satisfactory spectrum translation outputs with diverse colors and rich textures, and outperforms state-of-the-art counterparts by 3.95dB and 4.66dB in terms of PNSR for the NIR and grayscale colorization tasks, respectively.
</details>
<details>
<summary>摘要</summary>
near-infrared（NIR）图像 спектр翻译是一个具有挑战性的问题，有很多有前途的应用。现有方法在映射NIR和RGBDomains之间存在困难，并且因模型学习能力的限制和缺乏充足的NIR-RGB图像对 для训练而导致泛化不佳。为了解决这些挑战，我们提议一种合作学习 парадиг，通过利用潜在的跨频域约束（即潜在pectrumContext约束和任务频域约束），来同时进行NIR图像的colorization。这两个任务频域的统计和semantic spectrum信息都被引入作为任务频域约束。随后，我们设计了一种bilateral频域翻译模块，其中NIR图像中的黑白图像在干扰NIR图像的同时，也在平行进行了颜色化。这些干扰变换作为潜在pectrumContext约束，以便有效地进行频域知识交换。我们逐步细化和融合这些模块，并使用像素级和特征级一致性约束。实验结果表明，我们提议的合作学习框架可以生成高质量的spectrum翻译输出，具有多样性和丰富的Texture，并且比前方的counterpart高3.95dB和4.66dB在NIR和黑白图像色化任务中的PNSR指标上。
</details></li>
</ul>
<hr>
<h2 id="A-Hybrid-CNN-Transformer-Architecture-with-Frequency-Domain-Contrastive-Learning-for-Image-Deraining"><a href="#A-Hybrid-CNN-Transformer-Architecture-with-Frequency-Domain-Contrastive-Learning-for-Image-Deraining" class="headerlink" title="A Hybrid CNN-Transformer Architecture with Frequency Domain Contrastive Learning for Image Deraining"></a>A Hybrid CNN-Transformer Architecture with Frequency Domain Contrastive Learning for Image Deraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03340">http://arxiv.org/abs/2308.03340</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng Wang, Wei Li</li>
<li>for:  restore degraded images affected by rain streaks</li>
<li>methods:  image deraining</li>
<li>results:  not specifiedPlease note that the results are not specified in the abstract, so I cannot provide any information about the results of the paper.<details>
<summary>Abstract</summary>
Image deraining is a challenging task that involves restoring degraded images affected by rain streaks.
</details>
<details>
<summary>摘要</summary>
图像抑雨是一项具有挑战性的任务，涉及到修复受到雨斑影响的图像。
</details></li>
</ul>
<hr>
<h2 id="AFN-Adaptive-Fusion-Normalization-via-Encoder-Decoder-Framework"><a href="#AFN-Adaptive-Fusion-Normalization-via-Encoder-Decoder-Framework" class="headerlink" title="AFN: Adaptive Fusion Normalization via Encoder-Decoder Framework"></a>AFN: Adaptive Fusion Normalization via Encoder-Decoder Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03321">http://arxiv.org/abs/2308.03321</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/huanranchen/ASRNorm">https://github.com/huanranchen/ASRNorm</a></li>
<li>paper_authors: Zikai Zhou, Huanran Chen</li>
<li>for: 这篇论文主要为了提出一种统一的normalization函数，以解决现有normalization方法的缺点。</li>
<li>methods: 该论文提出了一种新的normalization函数 named Adaptive Fusion Normalization（AFN），它可以结合所有normalization方法，并消除它们的缺点。</li>
<li>results: 经过实验，AFN函数在领域总结和图像分类任务中表现出色，超过了现有的normalization方法。<details>
<summary>Abstract</summary>
The success of deep learning is inseparable from normalization layers. Researchers have proposed various normalization functions, and each of them has both advantages and disadvantages. In response, efforts have been made to design a unified normalization function that combines all normalization procedures and mitigates their weaknesses. We also proposed a new normalization function called Adaptive Fusion Normalization. Through experiments, we demonstrate AFN outperforms the previous normalization techniques in domain generalization and image classification tasks.
</details>
<details>
<summary>摘要</summary>
深度学习的成功与normalization层相关，研究人员提出了多种normalization函数，每种都有优点和缺点。为了解决这些问题，努力设计一个统一的normalization函数，汇集所有normalization过程，并减少它们的缺点。我们还提出了一种新的normalization函数called Adaptive Fusion Normalization（AFN）。经过实验，我们证明AFN在领域普适化和图像分类任务中表现出色，超越了前一代的normalization技术。
</details></li>
</ul>
<hr>
<h2 id="FLIQS-One-Shot-Mixed-Precision-Floating-Point-and-Integer-Quantization-Search"><a href="#FLIQS-One-Shot-Mixed-Precision-Floating-Point-and-Integer-Quantization-Search" class="headerlink" title="FLIQS: One-Shot Mixed-Precision Floating-Point and Integer Quantization Search"></a>FLIQS: One-Shot Mixed-Precision Floating-Point and Integer Quantization Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03290">http://arxiv.org/abs/2308.03290</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jordan Dotzel, Gang Wu, Andrew Li, Muhammad Umar, Yun Ni, Mohamed S. Abdelfattah, Zhiru Zhang, Liqun Cheng, Martin G. Dixon, Norman P. Jouppi, Quoc V. Le, Sheng Li</li>
<li>for: 这个论文目的是为了提出一个一击式混合精度搜寻方法，以实现高品质且低成本的深度神经网络（DNNs）模型。</li>
<li>methods: 这个方法使用了混合精度搜寻，以找到高品质且低成本的DNNs模型。它首先使用了数位支持的改进，然后使用了一个新的搜寻方法，以实现在数位和浮点数字之间进行搜寻。</li>
<li>results: 这个方法可以实现高品质且低成本的DNNs模型，并且比之前的方法更好。对于ResNet-18和ResNet-50模型，这个方法可以提高ImageNet准确度 by 1.31%和0.90%分别，同时保持相同的模型成本。此外，这个方法还可以对MobileNetV2进行改进，提高其准确度 by up to 0.98%分。最后，这个方法还可以同时搜寻一个混合精度和神经网络架构的共同搜寻空间，提高ImageNet准确度 by 2.69%分。<details>
<summary>Abstract</summary>
Quantization has become a mainstream compression technique for reducing model size, computational requirements, and energy consumption for modern deep neural networks (DNNs). With the improved numerical support in recent hardware, including multiple variants of integer and floating point, mixed-precision quantization has become necessary to achieve high-quality results with low model cost. Prior mixed-precision quantization methods have performed a post-training quantization search, which compromises on accuracy, or a differentiable quantization search, which leads to high memory usage from branching. Therefore, we propose the first one-shot mixed-precision quantization search that eliminates the need for retraining in both integer and low-precision floating point models. We evaluate our floating-point and integer quantization search (FLIQS) on multiple convolutional networks and vision transformer models to discover Pareto-optimal models. Our approach discovers models that improve upon uniform precision, manual mixed-precision, and recent integer quantization search methods. With the proposed integer quantization search, we increase the accuracy of ResNet-18 on ImageNet by 1.31% points and ResNet-50 by 0.90% points with equivalent model cost over previous methods. Additionally, for the first time, we explore a novel mixed-precision floating-point search and improve MobileNetV2 by up to 0.98% points compared to prior state-of-the-art FP8 models. Finally, we extend FLIQS to simultaneously search a joint quantization and neural architecture space and improve the ImageNet accuracy by 2.69% points with similar model cost on a MobileNetV2 search space.
</details>
<details>
<summary>摘要</summary>
归纳化技术已成为现代深度神经网络（DNN）的主流压缩技术，以提高模型大小、计算需求和能耗。随着现有硬件的数字支持的提升，混合精度归纳化已成为实现高质量结果的低成本模型的必要手段。现有的混合精度归纳化方法通常会在训练后进行归纳化搜索，这会妥协准确性，或者使用可导的归纳化搜索，这会导致高 память使用率。因此，我们提出了首个一步混合精度归纳化搜索，无需重新训练，并在整数和低精度浮点数模型中实现高质量结果。我们在多个卷积网络和视transformer模型上进行了评估，并发现了Pareto优质量模型。我们的方法在浮点数和整数归纳化搜索中提高了ImageNet中ResNet-18和ResNet-50模型的准确率，相比之前的方法，增加了1.31%点和0.90%点。此外，我们首次探索了一种新的混合精度浮点数搜索，并在MobileNetV2上提高了0.98%点，相比之前的FP8模型。最后，我们将FLIQS扩展到同时搜索归纳化和神经网络体系空间，并在MobileNetV2上提高了ImageNet准确率2.69%点，与相同的模型成本相似。
</details></li>
</ul>
<hr>
<h2 id="Multi-Label-Self-Supervised-Learning-with-Scene-Images"><a href="#Multi-Label-Self-Supervised-Learning-with-Scene-Images" class="headerlink" title="Multi-Label Self-Supervised Learning with Scene Images"></a>Multi-Label Self-Supervised Learning with Scene Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03286">http://arxiv.org/abs/2308.03286</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ke Zhu, Minghao Fu, Jianxin Wu</li>
<li>for: 本研究旨在提出一种简单且高效的自监学习（SSL）方法，用于Scene图像。</li>
<li>methods: 本方法将Scene&#x2F;多标签图像SSL简化为多标签分类问题，通过比较输入图像的嵌入与两个字典中的嵌入进行多个二进制 pseudo-标签的分配，然后使用二进制权值函数进行优化。</li>
<li>results: 实验显示，提出的多标签自监学习（MLS）方法可以学习高质量的图像表示，在MS-COCO数据集上实现了分类、检测和分割标准 bencmarks 的最佳结果，同时与现有方法相比，MLS 更简单，易于部署和进一步探索。<details>
<summary>Abstract</summary>
Self-supervised learning (SSL) methods targeting scene images have seen a rapid growth recently, and they mostly rely on either a dedicated dense matching mechanism or a costly unsupervised object discovery module. This paper shows that instead of hinging on these strenuous operations, quality image representations can be learned by treating scene/multi-label image SSL simply as a multi-label classification problem, which greatly simplifies the learning framework. Specifically, multiple binary pseudo-labels are assigned for each input image by comparing its embeddings with those in two dictionaries, and the network is optimized using the binary cross entropy loss. The proposed method is named Multi-Label Self-supervised learning (MLS). Visualizations qualitatively show that clearly the pseudo-labels by MLS can automatically find semantically similar pseudo-positive pairs across different images to facilitate contrastive learning. MLS learns high quality representations on MS-COCO and achieves state-of-the-art results on classification, detection and segmentation benchmarks. At the same time, MLS is much simpler than existing methods, making it easier to deploy and for further exploration.
</details>
<details>
<summary>摘要</summary>
自动学习（SSL）方法targeting场景图像在最近几年内得到了快速发展，这些方法主要基于 either 专门的密集匹配机制或者昂贵的无监督物体发现模块。这篇论文表明，相比于依靠这些艰辛的操作，高质量图像表示可以通过对场景/多标签图像SSL进行简单的多标签分类问题来学习。特别是，每个输入图像都将多个 binary pseudo-标签赋给，通过对其嵌入与两个词典中的嵌入进行比较，并使用二分类 entropy 损失函数进行优化。这种方法被称为多标签自动学习（MLS）。视觉化Qualitatively 显示，MLS 可以自动找到不同图像中的semantic 相似 pseudo-正例对，以便进行对比学习。MLS 在 MS-COCO 上学习高质量表示，并在分类、检测和 segmentation benchmark 上 achieve 状态的最佳结果。同时，MLS 比现有方法更加简单，更容易部署和进一步探索。
</details></li>
</ul>
<hr>
<h2 id="Environment-Invariant-Curriculum-Relation-Learning-for-Fine-Grained-Scene-Graph-Generation"><a href="#Environment-Invariant-Curriculum-Relation-Learning-for-Fine-Grained-Scene-Graph-Generation" class="headerlink" title="Environment-Invariant Curriculum Relation Learning for Fine-Grained Scene Graph Generation"></a>Environment-Invariant Curriculum Relation Learning for Fine-Grained Scene Graph Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03282">http://arxiv.org/abs/2308.03282</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/myukzzz/eicr">https://github.com/myukzzz/eicr</a></li>
<li>paper_authors: Yukuan Min, Aming Wu, Cheng Deng</li>
<li>for: 解决Scene Graph Generation（SGG）任务中的类别不均和上下文不均问题，以提高SGG模型的性能。</li>
<li>methods: 提出了一种基于环境不变的Curiculum学习（EICR）方法，可以与现有的SGG方法结合使用，以解决类别不均和上下文不均问题。</li>
<li>results: 对VG和GQA数据集进行了广泛的实验，结果显示，EICR框架可以作为SGG模型的通用策略，并取得了显著的改善。<details>
<summary>Abstract</summary>
The scene graph generation (SGG) task is designed to identify the predicates based on the subject-object pairs.However,existing datasets generally include two imbalance cases: one is the class imbalance from the predicted predicates and another is the context imbalance from the given subject-object pairs, which presents significant challenges for SGG. Most existing methods focus on the imbalance of the predicted predicate while ignoring the imbalance of the subject-object pairs, which could not achieve satisfactory results. To address the two imbalance cases, we propose a novel Environment Invariant Curriculum Relation learning (EICR) method, which can be applied in a plug-and-play fashion to existing SGG methods. Concretely, to remove the imbalance of the subject-object pairs, we first construct different distribution environments for the subject-object pairs and learn a model invariant to the environment changes. Then, we construct a class-balanced curriculum learning strategy to balance the different environments to remove the predicate imbalance. Comprehensive experiments conducted on VG and GQA datasets demonstrate that our EICR framework can be taken as a general strategy for various SGG models, and achieve significant improvements.
</details>
<details>
<summary>摘要</summary>
scene graph generation (SGG) 任务的设计是根据主语-谓语对 Identify  predicate。然而，现有数据集通常存在两种不均衡情况：一是预测 predicate 的类别不均衡，另一是给定主语-谓语对的上下文不均衡，这两种不均衡情况都会对 SGG 带来很大的挑战。大多数现有方法主要关注预测 predicate 的不均衡，而忽略主语-谓语对的不均衡，这会导致不能达到满意的结果。为了解决这两种不均衡情况，我们提出了一种新的 Environment Invariant Curriculum Relation 学习方法（EICR），它可以与现有的 SGG 方法相结合使用。具体来说，为了消除主语-谓语对的不均衡，我们首先构建了不同的分布环境 для主语-谓语对，然后学习一个环境不变的模型。接着，我们构建了一种类别均衡的学习策略，以平衡不同的环境，从而消除预测 predicate 的不均衡。经过了在 VG 和 GQA 数据集上的广泛实验，我们的 EICR 框架可以作为多种 SGG 模型的通用策略，并实现了显著的提升。
</details></li>
</ul>
<hr>
<h2 id="Mirror-NeRF-Learning-Neural-Radiance-Fields-for-Mirrors-with-Whitted-Style-Ray-Tracing"><a href="#Mirror-NeRF-Learning-Neural-Radiance-Fields-for-Mirrors-with-Whitted-Style-Ray-Tracing" class="headerlink" title="Mirror-NeRF: Learning Neural Radiance Fields for Mirrors with Whitted-Style Ray Tracing"></a>Mirror-NeRF: Learning Neural Radiance Fields for Mirrors with Whitted-Style Ray Tracing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03280">http://arxiv.org/abs/2308.03280</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zju3dv/Mirror-NeRF">https://github.com/zju3dv/Mirror-NeRF</a></li>
<li>paper_authors: Junyi Zeng, Chong Bao, Rui Chen, Zilong Dong, Guofeng Zhang, Hujun Bao, Zhaopeng Cui</li>
<li>for: 该论文旨在解决NeRF无法正确描述镜子上的反射问题，提出一种基于镜子反射概率的神经渲染框架，以支持各种场景操作和镜子反射的渲染。</li>
<li>methods: 该方法基于镜子反射概率的 introduce 镜子反射概率并使用Whitted Ray Tracing的光传输模型跟踪光线，以及一些促进学习过程的技术。</li>
<li>results: 实验和比较表明，该方法在 synthetic 和实际数据集上具有明显的优势，能够准确描述镜子上的反射和多视图相互关联的反射。<details>
<summary>Abstract</summary>
Recently, Neural Radiance Fields (NeRF) has exhibited significant success in novel view synthesis, surface reconstruction, etc. However, since no physical reflection is considered in its rendering pipeline, NeRF mistakes the reflection in the mirror as a separate virtual scene, leading to the inaccurate reconstruction of the mirror and multi-view inconsistent reflections in the mirror. In this paper, we present a novel neural rendering framework, named Mirror-NeRF, which is able to learn accurate geometry and reflection of the mirror and support various scene manipulation applications with mirrors, such as adding new objects or mirrors into the scene and synthesizing the reflections of these new objects in mirrors, controlling mirror roughness, etc. To achieve this goal, we propose a unified radiance field by introducing the reflection probability and tracing rays following the light transport model of Whitted Ray Tracing, and also develop several techniques to facilitate the learning process. Experiments and comparisons on both synthetic and real datasets demonstrate the superiority of our method. The code and supplementary material are available on the project webpage: https://zju3dv.github.io/Mirror-NeRF/.
</details>
<details>
<summary>摘要</summary>
最近，神经辐射场（NeRF）在新视图合成、表面重建等领域表现出了显著的成功。然而，由于NeRF的渲染管线中没有考虑物理反射，因此NeRF会错误地将镜子中的反射视为独立的虚拟场景，导致镜子和多视图不一致的反射。在这篇论文中，我们提出了一种新的神经渲染框架，名为镜子-NeRF，它能够学习镜子上的准确 геометрии和反射。我们还提出了多种技术来促进学习过程，包括引入反射概率和根据Whitted雨筒跟踪模型跟踪光线的方法。实验和比较表明，我们的方法在 sintetic和实际数据集上具有明显的优势。代码和补充材料可以在项目网站（https://zju3dv.github.io/Mirror-NeRF/）上获取。
</details></li>
</ul>
<hr>
<h2 id="Spatialyze-A-Geospatial-Video-Analytics-System-with-Spatial-Aware-Optimizations"><a href="#Spatialyze-A-Geospatial-Video-Analytics-System-with-Spatial-Aware-Optimizations" class="headerlink" title="Spatialyze: A Geospatial Video Analytics System with Spatial-Aware Optimizations"></a>Spatialyze: A Geospatial Video Analytics System with Spatial-Aware Optimizations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03276">http://arxiv.org/abs/2308.03276</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/apperception-db/spatialyze">https://github.com/apperception-db/spatialyze</a></li>
<li>paper_authors: Chanwut Kittivorawong, Yongming Ge, Yousef Helal, Alvin Cheung</li>
<li>for: 这篇论文是关于如何提供高效的地ospatial视频数据管理系统，以便用户可以更好地交互地ospatial视频数据。</li>
<li>methods: 该论文提出了一个新的框架 named Spatialyze，该框架使用了域专语言， allowing users to construct geospatial video analytic workflows using a 3-step, declarative, build-filter-observe paradigm。</li>
<li>results: 实验结果表明，使用Spatialyze可以提高执行效率，比例最高可以达5.3倍，同时维持97.1%的准确率。<details>
<summary>Abstract</summary>
Videos that are shot using commodity hardware such as phones and surveillance cameras record various metadata such as time and location. We encounter such geospatial videos on a daily basis and such videos have been growing in volume significantly. Yet, we do not have data management systems that allow users to interact with such data effectively.   In this paper, we describe Spatialyze, a new framework for end-to-end querying of geospatial videos. Spatialyze comes with a domain-specific language where users can construct geospatial video analytic workflows using a 3-step, declarative, build-filter-observe paradigm. Internally, Spatialyze leverages the declarative nature of such workflows, the temporal-spatial metadata stored with videos, and physical behavior of real-world objects to optimize the execution of workflows. Our results using real-world videos and workflows show that Spatialyze can reduce execution time by up to 5.3x, while maintaining up to 97.1% accuracy compared to unoptimized execution.
</details>
<details>
<summary>摘要</summary>
视频 Recorded using common hardware such as phones and surveillance cameras 包含时间和地点 metadata。我们每天都会遇到这类地ospatial videos，但我们没有有效地处理这些数据的数据管理系统。 在本文中，我们介绍了 Spatialyze，一个新的框架 для地ospatial videos 的终端查询。Spatialyze 提供了一个域特定语言， allowing users to construct geospatial video analytic workflows using a 3-step, declarative, build-filter-observe paradigm。内部，Spatialyze 利用了声明性的 workflows，视频中的时间-空间 metadata 和实际物体的物理行为来优化 workflows 的执行。我们使用实际视频和 workflows 进行测试，结果显示，Spatialyze 可以提高执行时间 Speed 到 5.3x，保持高于 97.1% 的准确率 compared to unoptimized execution。
</details></li>
</ul>
<hr>
<h2 id="Feature-Suppressed-Contrast-for-Self-Supervised-Food-Pre-training"><a href="#Feature-Suppressed-Contrast-for-Self-Supervised-Food-Pre-training" class="headerlink" title="Feature-Suppressed Contrast for Self-Supervised Food Pre-training"></a>Feature-Suppressed Contrast for Self-Supervised Food Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03272">http://arxiv.org/abs/2308.03272</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinda Liu, Yaohui Zhu, Linhu Liu, Jiang Tian, Lili Wang<br>for:This paper focuses on developing a self-supervised learning method for food image recognition, aiming to reduce the human labeling expenses and improve the efficiency of food image analysis.methods:The proposed method, called Feature Suppressed Contrast (FeaSC), leverages contrastive self-supervised learning on unlabelled food images. To address the problem of similar informative contents in the two views, the method uses a response-aware scheme to localize salient features in an unsupervised manner, reducing the mutual information between the views.results:The proposed FeaSC method consistently improves the classification accuracy of BYOL and SimSiam by 1.70% - 6.69% on four publicly available food recognition datasets. Additionally, the method achieves superior results on downstream segmentation tasks, demonstrating its effectiveness in food image analysis.<details>
<summary>Abstract</summary>
Most previous approaches for analyzing food images have relied on extensively annotated datasets, resulting in significant human labeling expenses due to the varied and intricate nature of such images. Inspired by the effectiveness of contrastive self-supervised methods in utilizing unlabelled data, weiqing explore leveraging these techniques on unlabelled food images. In contrastive self-supervised methods, two views are randomly generated from an image by data augmentations. However, regarding food images, the two views tend to contain similar informative contents, causing large mutual information, which impedes the efficacy of contrastive self-supervised learning. To address this problem, we propose Feature Suppressed Contrast (FeaSC) to reduce mutual information between views. As the similar contents of the two views are salient or highly responsive in the feature map, the proposed FeaSC uses a response-aware scheme to localize salient features in an unsupervised manner. By suppressing some salient features in one view while leaving another contrast view unchanged, the mutual information between the two views is reduced, thereby enhancing the effectiveness of contrast learning for self-supervised food pre-training. As a plug-and-play module, the proposed method consistently improves BYOL and SimSiam by 1.70\% $\sim$ 6.69\% classification accuracy on four publicly available food recognition datasets. Superior results have also been achieved on downstream segmentation tasks, demonstrating the effectiveness of the proposed method.
</details>
<details>
<summary>摘要</summary>
previous approaches for analyzing food images have relied on extensively annotated datasets, resulting in significant human labeling expenses due to the varied and intricate nature of such images. Inspired by the effectiveness of contrastive self-supervised methods in utilizing unlabelled data, we explore leveraging these techniques on unlabelled food images. In contrastive self-supervised methods, two views are randomly generated from an image by data augmentations. However, regarding food images, the two views tend to contain similar informative contents, causing large mutual information, which impedes the efficacy of contrastive self-supervised learning. To address this problem, we propose Feature Suppressed Contrast (FeaSC) to reduce mutual information between views. As the similar contents of the two views are salient or highly responsive in the feature map, the proposed FeaSC uses a response-aware scheme to localize salient features in an unsupervised manner. By suppressing some salient features in one view while leaving another contrast view unchanged, the mutual information between the two views is reduced, thereby enhancing the effectiveness of contrast learning for self-supervised food pre-training. As a plug-and-play module, the proposed method consistently improves BYOL and SimSiam by 1.70\% $\sim$ 6.69\% classification accuracy on four publicly available food recognition datasets. Superior results have also been achieved on downstream segmentation tasks, demonstrating the effectiveness of the proposed method.Here's the word-for-word translation of the text into Simplified Chinese:前一些食物图像分析方法都是基于大量人工标注的数据集，这导致了人工标注成本的增加，因为食物图像的性质是复杂且多变的。以启发自contrastive self-supervised方法的效iveness，我们想要利用无标注数据来预训练食物图像。在contrastive self-supervised方法中，两个视图是通过数据变换生成的，但是在食物图像上，这两个视图往往含有相似的有用信息，导致大量的相互信息，这阻碍了对比学习的效iveness。为解决这个问题，我们提出了特征压缩对比（FeaSC），以减少视图之间的相互信息。在特征地图中，与食物图像相似的部分是突出的或高度反应的，我们使用回应感知方案来本地化这些特征。通过压缩一个视图中的突出特征而不改变另一个对比视图，我们可以减少视图之间的相互信息，从而提高对比学习的效iveness。作为插入式模块，我们的提案可以适应BYOL和SimSiam等方法，并在四个公开的食物识别数据集上实现了1.70% 至 6.69%的分类精度提升。此外，我们还在下游分割任务中获得了更高的成果，这证明了我们的提案的效果。
</details></li>
</ul>
<hr>
<h2 id="A-Benchmark-for-Chinese-English-Scene-Text-Image-Super-resolution"><a href="#A-Benchmark-for-Chinese-English-Scene-Text-Image-Super-resolution" class="headerlink" title="A Benchmark for Chinese-English Scene Text Image Super-resolution"></a>A Benchmark for Chinese-English Scene Text Image Super-resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03262">http://arxiv.org/abs/2308.03262</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mjq11302010044/real-ce">https://github.com/mjq11302010044/real-ce</a></li>
<li>paper_authors: Jianqi Ma, Zhetong Liang, Wangmeng Xiang, Xi Yang, Lei Zhang</li>
<li>for: 本研究旨在提高中文Scene Text Image Super-resolution（STISR）的质量，以恢复优质的高分辨率（HR）Scene文本图像，并且能够保持中文文本的拼写正确性和可读性。</li>
<li>methods: 我们提出了一种新的Edge-aware学习方法，该方法在图像和特征领域提供了结构权重，以有效地重建中文字符的紧凑结构。</li>
<li>results: 我们在提出的Real-CE数据集上进行了实验，并评估了现有的STISR模型，包括使用我们的Edge-aware损失和不使用。实验结果显示，我们的Edge-aware方法能够提高STISR模型的性能，并且能够保持中文文本的拼写正确性和可读性。<details>
<summary>Abstract</summary>
Scene Text Image Super-resolution (STISR) aims to recover high-resolution (HR) scene text images with visually pleasant and readable text content from the given low-resolution (LR) input. Most existing works focus on recovering English texts, which have relatively simple character structures, while little work has been done on the more challenging Chinese texts with diverse and complex character structures. In this paper, we propose a real-world Chinese-English benchmark dataset, namely Real-CE, for the task of STISR with the emphasis on restoring structurally complex Chinese characters. The benchmark provides 1,935/783 real-world LR-HR text image pairs~(contains 33,789 text lines in total) for training/testing in 2$\times$ and 4$\times$ zooming modes, complemented by detailed annotations, including detection boxes and text transcripts. Moreover, we design an edge-aware learning method, which provides structural supervision in image and feature domains, to effectively reconstruct the dense structures of Chinese characters. We conduct experiments on the proposed Real-CE benchmark and evaluate the existing STISR models with and without our edge-aware loss. The benchmark, including data and source code, is available at https://github.com/mjq11302010044/Real-CE.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="APBench-A-Unified-Benchmark-for-Availability-Poisoning-Attacks-and-Defenses"><a href="#APBench-A-Unified-Benchmark-for-Availability-Poisoning-Attacks-and-Defenses" class="headerlink" title="APBench: A Unified Benchmark for Availability Poisoning Attacks and Defenses"></a>APBench: A Unified Benchmark for Availability Poisoning Attacks and Defenses</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03258">http://arxiv.org/abs/2308.03258</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lafeat/apbench">https://github.com/lafeat/apbench</a></li>
<li>paper_authors: Tianrui Qin, Xitong Gao, Juanjuan Zhao, Kejiang Ye, Cheng-Zhong Xu<br>for:这篇论文的目的是评估黑客攻击和防御数据毒液的能效性，并提供一个 benchmark 来评估这些攻击和防御方法的表现。methods:这篇论文使用了9种最新的可用性毒液攻击、8种防御算法和4种传统的数据增强技术来评估这些攻击和防御方法的表现。results:这篇论文的结果显示现有的黑客攻击无法保护个人隐私，而 APBench 可以帮助评估这些攻击和防御方法的表现。<details>
<summary>Abstract</summary>
The efficacy of availability poisoning, a method of poisoning data by injecting imperceptible perturbations to prevent its use in model training, has been a hot subject of investigation. Previous research suggested that it was difficult to effectively counteract such poisoning attacks. However, the introduction of various defense methods has challenged this notion. Due to the rapid progress in this field, the performance of different novel methods cannot be accurately validated due to variations in experimental setups. To further evaluate the attack and defense capabilities of these poisoning methods, we have developed a benchmark -- APBench for assessing the efficacy of adversarial poisoning. APBench consists of 9 state-of-the-art availability poisoning attacks, 8 defense algorithms, and 4 conventional data augmentation techniques. We also have set up experiments with varying different poisoning ratios, and evaluated the attacks on multiple datasets and their transferability across model architectures. We further conducted a comprehensive evaluation of 2 additional attacks specifically targeting unsupervised models. Our results reveal the glaring inadequacy of existing attacks in safeguarding individual privacy. APBench is open source and available to the deep learning community: https://github.com/lafeat/apbench.
</details>
<details>
<summary>摘要</summary>
“数据可用性毒化”的效果，一种在模型训练中注入不可见的干扰以防止数据使用，已成为研究热点。前一些研究表明，对这种攻击难以有效防御。然而，新的防御技术的出现挑战了这一观点。由于这个领域的快速进步，不同的新方法的性能无法准确验证因为实验设置的变化。为了进一步评估攻击和防御毒化方法的能力，我们开发了一个标准套件——APBench，用于评估毒化攻击的效果。APBench包括9种当前最佳的可用性毒化攻击，8种防御算法，以及4种常见的数据增强技术。我们还在不同的毒化比率下进行了实验，并对多个数据集和模型架构进行了评估。此外，我们还进行了对2种专门针对无监督模型的攻击的全面评估。我们的结果显示现有的攻击方法对个人隐私无法提供充分的保护。APBench是开源的，可以在GitHub上获取：https://github.com/lafeat/apbench。”
</details></li>
</ul>
<hr>
<h2 id="Learning-a-Graph-Neural-Network-with-Cross-Modality-Interaction-for-Image-Fusion"><a href="#Learning-a-Graph-Neural-Network-with-Cross-Modality-Interaction-for-Image-Fusion" class="headerlink" title="Learning a Graph Neural Network with Cross Modality Interaction for Image Fusion"></a>Learning a Graph Neural Network with Cross Modality Interaction for Image Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03256">http://arxiv.org/abs/2308.03256</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lok-18/ignet">https://github.com/lok-18/ignet</a></li>
<li>paper_authors: Jiawei Li, Jiansheng Chen, Jinyuan Liu, Huimin Ma</li>
<li>for: 本研究旨在提出一种基于交互式图 neural network（GNN）的诸多模式图像融合方法，以提高图像融合的精度和效果。</li>
<li>methods: 本方法首先使用多尺度提取器来获得 shallow 特征，然后将这些特征作为必要的输入建立图структуре。接着，图交互模块将抽取的中间特征在不同模式之间进行交互，以实现跨模式和semantic学习。此外，我们还提出了领导节点来改进同模式信息传播。最后，我们将所有图结构特征合并以获得融合结果。</li>
<li>results: 我们在多个数据集（TNO、MFNet和M3FD）上进行了广泛的实验，结果表明，我们的IGNet方法可以生成视觉吸引人的融合图像，同时在检测和分割任务中平均获得2.59% mAP@.5和7.77% mIoU高于相关的状态当前方法。<details>
<summary>Abstract</summary>
Infrared and visible image fusion has gradually proved to be a vital fork in the field of multi-modality imaging technologies. In recent developments, researchers not only focus on the quality of fused images but also evaluate their performance in downstream tasks. Nevertheless, the majority of methods seldom put their eyes on the mutual learning from different modalities, resulting in fused images lacking significant details and textures. To overcome this issue, we propose an interactive graph neural network (GNN)-based architecture between cross modality for fusion, called IGNet. Specifically, we first apply a multi-scale extractor to achieve shallow features, which are employed as the necessary input to build graph structures. Then, the graph interaction module can construct the extracted intermediate features of the infrared/visible branch into graph structures. Meanwhile, the graph structures of two branches interact for cross-modality and semantic learning, so that fused images can maintain the important feature expressions and enhance the performance of downstream tasks. Besides, the proposed leader nodes can improve information propagation in the same modality. Finally, we merge all graph features to get the fusion result. Extensive experiments on different datasets (TNO, MFNet and M3FD) demonstrate that our IGNet can generate visually appealing fused images while scoring averagely 2.59% mAP@.5 and 7.77% mIoU higher in detection and segmentation than the compared state-of-the-art methods. The source code of the proposed IGNet can be available at https://github.com/lok-18/IGNet.
</details>
<details>
<summary>摘要</summary>
infrared和可见图像融合逐渐成为多Modal imaging技术中的重要分支。在最近的发展中，研究人员不仅关注融合图像的质量，还评估其在下游任务中的表现。然而，大多数方法很少关注不同模式之间的相互学习，导致融合图像缺乏重要的特征和тексту。为解决这问题，我们提出了一种交互式图 neural network（GNN）基于树结构的架构，called IGNet。具体来说，我们首先应用多级提取器来获得 shallow 特征，这些特征被用作构建图像结构的必要输入。然后，图像交互模块可以将抽象分支中的中间特征构建成图像结构。同时，两个分支的图像结构之间进行交互性学习，以便在不同模式之间增强融合图像的表现。此外，我们还提出了领导节点，以提高同一个模式中的信息传播。最后，我们将所有的图像特征合并到一起，以获得融合结果。我们在不同的数据集（TNO、MFNet和M3FD）进行了广泛的实验，结果表明，我们的IGNet可以生成有趣的融合图像，同时与比较的状态前方法相比，其在检测和分类任务中的性能提高了2.59% mAP@.5和7.77% mIoU。源代码可以在https://github.com/lok-18/IGNet中下载。
</details></li>
</ul>
<hr>
<h2 id="Local-Consensus-Enhanced-Siamese-Network-with-Reciprocal-Loss-for-Two-view-Correspondence-Learning"><a href="#Local-Consensus-Enhanced-Siamese-Network-with-Reciprocal-Loss-for-Two-view-Correspondence-Learning" class="headerlink" title="Local Consensus Enhanced Siamese Network with Reciprocal Loss for Two-view Correspondence Learning"></a>Local Consensus Enhanced Siamese Network with Reciprocal Loss for Two-view Correspondence Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03217">http://arxiv.org/abs/2308.03217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linbo Wang, Jing Wu, Xianyong Fang, Zhengyi Liu, Chenjie Cao, Yanwei Fu</li>
<li>for: 提高两视匹配学习的性能</li>
<li>methods: 提出了一个Local Feature Consensus（LFC）插件块，对现有模型的特征进行增强，以及一种对抗式损失函数，使用反向映射的信息进行监督网络训练</li>
<li>results: 实验表明，基于MSA-Net的两个提议可以提高匹配性能，达到了参考数据集上的状态速度表现<details>
<summary>Abstract</summary>
Recent studies of two-view correspondence learning usually establish an end-to-end network to jointly predict correspondence reliability and relative pose. We improve such a framework from two aspects. First, we propose a Local Feature Consensus (LFC) plugin block to augment the features of existing models. Given a correspondence feature, the block augments its neighboring features with mutual neighborhood consensus and aggregates them to produce an enhanced feature. As inliers obey a uniform cross-view transformation and share more consistent learned features than outliers, feature consensus strengthens inlier correlation and suppresses outlier distraction, which makes output features more discriminative for classifying inliers/outliers. Second, existing approaches supervise network training with the ground truth correspondences and essential matrix projecting one image to the other for an input image pair, without considering the information from the reverse mapping. We extend existing models to a Siamese network with a reciprocal loss that exploits the supervision of mutual projection, which considerably promotes the matching performance without introducing additional model parameters. Building upon MSA-Net, we implement the two proposals and experimentally achieve state-of-the-art performance on benchmark datasets.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)现在的研究通常是两视匹配学习的结果，通常是一个端到端网络来同时预测匹配可靠性和相对pose。我们从两个方面提高了这种框架：第一，我们提议一个Local Feature Consensus（LFC）插件块来增强现有模型的特征。给一个匹配特征，这个块将其周围的特征通过相互邻居一致来增强，并将其汇聚到生成一个加强特征。由于匹配点遵循同一个跨视图变换，并且在学习过程中分享更一致的特征，因此特征一致性增强了匹配点的相互关系，降低了干扰器的影响，使输出特征更有力度地分类匹配/干扰。第二，现有的方法通常通过真实对应的地址来训练网络，而不考虑反向映射的信息。我们将现有模型扩展为一个SIAMESE网络，使用对偶损失来利用对偶映射的超级vision，从而明显提高匹配性能，而不需要添加更多的模型参数。基于MSA-Net，我们实现了这两个提议，并在测试数据集上实现了状态机器人的性能。
</details></li>
</ul>
<hr>
<h2 id="Microvasculature-Segmentation-in-Human-BioMolecular-Atlas-Program-HuBMAP"><a href="#Microvasculature-Segmentation-in-Human-BioMolecular-Atlas-Program-HuBMAP" class="headerlink" title="Microvasculature Segmentation in Human BioMolecular Atlas Program (HuBMAP)"></a>Microvasculature Segmentation in Human BioMolecular Atlas Program (HuBMAP)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03203">http://arxiv.org/abs/2308.03203</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youssef Sultan, Yongqiang Wang, James Scanlon, Lisa D’lima</li>
<li>for: 这个研究旨在为 HuBMAP 项目提供细致的微血管结构分割方法，以创建详细的人体细胞地图。</li>
<li>methods: 该研究使用了基础 FastAI U-Net 模型，并对其进行了改进，包括使用不同的后端架构、深度模型和特征峰网络。</li>
<li>results: 研究对不同方法进行了严谨的评估，并发现了各种改进方法的性能。这种研究提供了未来研究领域的有价值透彻。<details>
<summary>Abstract</summary>
Image segmentation serves as a critical tool across a range of applications, encompassing autonomous driving's pedestrian detection and pre-operative tumor delineation in the medical sector. Among these applications, we focus on the National Institutes of Health's (NIH) Human BioMolecular Atlas Program (HuBMAP), a significant initiative aimed at creating detailed cellular maps of the human body. In this study, we concentrate on segmenting various microvascular structures in human kidneys, utilizing 2D Periodic Acid-Schiff (PAS)-stained histology images. Our methodology begins with a foundational FastAI U-Net model, upon which we investigate alternative backbone architectures, delve into deeper models, and experiment with Feature Pyramid Networks. We rigorously evaluate these varied approaches by benchmarking their performance against our baseline U-Net model. This study thus offers a comprehensive exploration of cutting-edge segmentation techniques, providing valuable insights for future research in the field.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)图像分割是应用领域中的一种重要工具，包括自动驾驶中的步行人检测和医疗领域中的前操作肿瘤定点。我们在这些应用中将重点关注国家卫生研究院（NIH）的人生物分子地图计划（HuBMAP），这是一项旨在创建人体cellular图的重要initiative。在这项研究中，我们将专注于人类肾脏中的微血管结构分割，使用2D periodic acid-Schiff（PAS）染色的历史图像。我们的方法开始于基础的 FastAI U-Net 模型，然后我们会 investigate alternative backbone architectures、 deeper models和 Feature Pyramid Networks。我们严格评估这些不同的方法，对比基准 U-Net 模型的性能。这项研究因此提供了一种全面的分割技术探索，为未来研究提供有价值的意见。
</details></li>
</ul>
<hr>
<h2 id="Syn-Mediverse-A-Multimodal-Synthetic-Dataset-for-Intelligent-Scene-Understanding-of-Healthcare-Facilities"><a href="#Syn-Mediverse-A-Multimodal-Synthetic-Dataset-for-Intelligent-Scene-Understanding-of-Healthcare-Facilities" class="headerlink" title="Syn-Mediverse: A Multimodal Synthetic Dataset for Intelligent Scene Understanding of Healthcare Facilities"></a>Syn-Mediverse: A Multimodal Synthetic Dataset for Intelligent Scene Understanding of Healthcare Facilities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03193">http://arxiv.org/abs/2308.03193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rohit Mohan, José Arce, Sassan Mokhtar, Daniele Cattaneo, Abhinav Valada</li>
<li>For: 这个论文的目的是提供一个大量的多模态Synthetic数据集，以便研究医疗设施的场景理解。* Methods: 该论文使用了一个 simulate industry-standard optical tracking camera 生成的数据集，包含了多种场景理解任务的1.5万个标注。* Results: 论文提供了一个广泛的基线测试，以评估不同任务的性能。此外，论文还提供了一个在线评估平台，可以帮助进一步研究医疗设施的场景理解。<details>
<summary>Abstract</summary>
Safety and efficiency are paramount in healthcare facilities where the lives of patients are at stake. Despite the adoption of robots to assist medical staff in challenging tasks such as complex surgeries, human expertise is still indispensable. The next generation of autonomous healthcare robots hinges on their capacity to perceive and understand their complex and frenetic environments. While deep learning models are increasingly used for this purpose, they require extensive annotated training data which is impractical to obtain in real-world healthcare settings. To bridge this gap, we present Syn-Mediverse, the first hyper-realistic multimodal synthetic dataset of diverse healthcare facilities. Syn-Mediverse contains over \num{48000} images from a simulated industry-standard optical tracking camera and provides more than 1.5M annotations spanning five different scene understanding tasks including depth estimation, object detection, semantic segmentation, instance segmentation, and panoptic segmentation. We demonstrate the complexity of our dataset by evaluating the performance on a broad range of state-of-the-art baselines for each task. To further advance research on scene understanding of healthcare facilities, along with the public dataset we provide an online evaluation benchmark available at \url{http://syn-mediverse.cs.uni-freiburg.de}
</details>
<details>
<summary>摘要</summary>
安全和效率在医疗设施中是非常重要，因为患者的生命正在归附。虽然已经采用了机器人来协助医疗人员完成复杂的手术等任务，但人类专业仍然是不可或缺的。下一代自动化医疗机器人的发展取决于它们能够在复杂和紧张的医疗环境中进行感知和理解。然而，深度学习模型在这种目的上面习用的数据是实际医疗设施中获得的困难。为了bridging这个差距，我们介绍了Syn-Mediverse，首个 Hyper-Realistic 多模态人工数据集。Syn-Mediverse包含了 более48000张来自 simulated 行业标准光学跟踪相机的图像，以及1500000多个注释，涵盖了五个不同的场景理解任务，包括深度估计、物体检测、semantic segmentation、instance segmentation和panoptic segmentation。我们通过评估一系列国际顶峰模型的性能来证明Syn-Mediverse的复杂性。为了进一步推动医疗设施场景理解的研究，我们同时提供了在 line 4 提到的在线评估平台，可以在http://syn-mediverse.cs.uni-freiburg.de 上获取。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Biometric-Entropy-and-Iris-Capacity-Avoiding-Identity-Collisions-on-National-Scales"><a href="#Understanding-Biometric-Entropy-and-Iris-Capacity-Avoiding-Identity-Collisions-on-National-Scales" class="headerlink" title="Understanding Biometric Entropy and Iris Capacity: Avoiding Identity Collisions on National Scales"></a>Understanding Biometric Entropy and Iris Capacity: Avoiding Identity Collisions on National Scales</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03189">http://arxiv.org/abs/2308.03189</a></li>
<li>repo_url: None</li>
<li>paper_authors: John Daugman</li>
<li>for: 研究了基于眼睛图像的唯一身份识别方法，特别是人口规模下的唯一标识。</li>
<li>methods: 使用眼睛图像的生物特征，如眼睛肤色、眼睛形状等，进行唯一身份识别。</li>
<li>results: 研究发现，使用眼睛图像可以实现高精度的唯一身份识别，并且可以处理大规模人口数据。具体来说，在US NIST（国家标准技术研究所）试验中，使用眼睛图像进行1.2亿次比较，并没有发现任何身份冲突现象。此外，研究还发现，使用两个眼睛图像的生物特征可以保证全球唯一身份识别。<details>
<summary>Abstract</summary>
The numbers of persons who can be enrolled by their iris patterns with no identity collisions is studied in relation to the biometric entropy extracted, and the decision operating threshold. The population size at which identity collision becomes likelier than not, given those variables, defines iris "capacity." The general solution to this combinatorial problem is derived, in analogy with the well-known "birthday problem." Its application to unique biometric identification on national population scales is shown, referencing empirical data from US NIST (National Institute of Standards and Technology) trials involving 1.2 trillion (1.2 x 10^(12) ) iris comparisons. The entropy of a given person's two iris patterns suffices for global identity uniqueness.
</details>
<details>
<summary>摘要</summary>
TEXTThe number of people who can be enrolled using their iris patterns without any identity collisions is studied in relation to the biometric entropy extracted and the decision operating threshold. The population size at which identity collision becomes more likely than not, given these variables, defines the "capacity" of the iris. The general solution to this combinatorial problem is derived, similar to the well-known "birthday problem." Its application to unique biometric identification on national population scales is shown, referencing empirical data from US NIST (National Institute of Standards and Technology) trials involving 1.2 trillion (1.2 x 10^(12)) iris comparisons. The entropy of a person's two iris patterns is sufficient for global identity uniqueness.SIMPLIFIED CHINESE TRANSLATION文本通过人们的肉眼印模式注册的人数量，不会出现身份冲突的情况是研究的，与提取的生物метри entropy和决策操作阈值相关。这个变量定义了肉眼的容量。通过生物 метри "生日问题" 的一般解决方案来 derivation。这种应用于国家规模的唯一生物特征标识， referencing 美国 NIST（国家标准技术研究所）的实验数据，涉及 1.2 x 10^(12) 比较。两个人的肉眼印模式的熵值充分保证全球身份唯一性。
</details></li>
</ul>
<hr>
<h2 id="Photorealistic-and-Identity-Preserving-Image-Based-Emotion-Manipulation-with-Latent-Diffusion-Models"><a href="#Photorealistic-and-Identity-Preserving-Image-Based-Emotion-Manipulation-with-Latent-Diffusion-Models" class="headerlink" title="Photorealistic and Identity-Preserving Image-Based Emotion Manipulation with Latent Diffusion Models"></a>Photorealistic and Identity-Preserving Image-Based Emotion Manipulation with Latent Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03183">http://arxiv.org/abs/2308.03183</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ioannis Pikoulis, Panagiotis P. Filntisis, Petros Maragos</li>
<li>for:  investigate the emotion manipulation capabilities of diffusion models with “in-the-wild” images</li>
<li>methods:  Latent Diffusion models and text-driven manipulation with CLIP latents</li>
<li>results:  superior image quality and realism, competitive results relative to emotion translation compared to GAN-based counterparts.<details>
<summary>Abstract</summary>
In this paper, we investigate the emotion manipulation capabilities of diffusion models with "in-the-wild" images, a rather unexplored application area relative to the vast and rapidly growing literature for image-to-image translation tasks. Our proposed method encapsulates several pieces of prior work, with the most important being Latent Diffusion models and text-driven manipulation with CLIP latents. We conduct extensive qualitative and quantitative evaluations on AffectNet, demonstrating the superiority of our approach in terms of image quality and realism, while achieving competitive results relative to emotion translation compared to a variety of GAN-based counterparts. Code is released as a publicly available repo.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了使用“在野”图像进行情感操作的扩散模型，这是图像到图像翻译任务领域的未探索领域。我们提出的方法集成了许多先前的研究，最重要的是潜在扩散模型和文本驱动的映射。我们在AffectNet上进行了广泛的质量和量测试，示出我们的方法在图像质量和真实性方面具有突出的优势，同时与情感翻译相比，与多种基于GAN的对手相比的结果具有竞争力。代码将被公开发布为公共可用 репозиторий。
</details></li>
</ul>
<hr>
<h2 id="Boosting-Few-shot-3D-Point-Cloud-Segmentation-via-Query-Guided-Enhancement"><a href="#Boosting-Few-shot-3D-Point-Cloud-Segmentation-via-Query-Guided-Enhancement" class="headerlink" title="Boosting Few-shot 3D Point Cloud Segmentation via Query-Guided Enhancement"></a>Boosting Few-shot 3D Point Cloud Segmentation via Query-Guided Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03177">http://arxiv.org/abs/2308.03177</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenhua Ning, Zhuotao Tian, Guangming Lu, Wenjie Pei</li>
<li>for: 提高3D点云 segmentation模型的几何 adaptation 性能</li>
<li>methods: 提出一种基于查询指导的改进方法，通过修改支持背景模型以匹配查询样本的上下文，并通过填充查询特征来填充 semantic gap</li>
<li>results: 实验结果表明，该方法可以在S3DIS和ScanNet上达到显著提高，同时保持高效率<details>
<summary>Abstract</summary>
Although extensive research has been conducted on 3D point cloud segmentation, effectively adapting generic models to novel categories remains a formidable challenge. This paper proposes a novel approach to improve point cloud few-shot segmentation (PC-FSS) models. Unlike existing PC-FSS methods that directly utilize categorical information from support prototypes to recognize novel classes in query samples, our method identifies two critical aspects that substantially enhance model performance by reducing contextual gaps between support prototypes and query features. Specifically, we (1) adapt support background prototypes to match query context while removing extraneous cues that may obscure foreground and background in query samples, and (2) holistically rectify support prototypes under the guidance of query features to emulate the latter having no semantic gap to the query targets. Our proposed designs are agnostic to the feature extractor, rendering them readily applicable to any prototype-based methods. The experimental results on S3DIS and ScanNet demonstrate notable practical benefits, as our approach achieves significant improvements while still maintaining high efficiency. The code for our approach is available at https://github.com/AaronNZH/Boosting-Few-shot-3D-Point-Cloud-Segmentation-via-Query-Guided-Enhancement
</details>
<details>
<summary>摘要</summary>
尽管普通的3D点云分割检测已经得到了广泛的研究，但将通用模型适应新类型仍然是一项具有挑战性的任务。这篇论文提出了一种改进点云几何分割（PC-FSS）模型的新方法。与现有PC-FSS方法不同，我们的方法不直接使用支持类prototype来识别新类型的查询样本中的类别信息。而是通过两种关键方法来减少查询样本与支持类prototype之间的上下文差异，以提高模型性能。具体来说，我们：1. 将支持背景prototype调整到与查询样本的上下文相匹配，同时移除查询样本中可能掩蔽背景和前景的误导因素。2. 使用查询特征来正则化支持类prototype，以便模拟查询样本中没有 semantic gap 的情况。我们的设计是对于任何prototype-based方法都是可靠的，并且在实验中得到了显著的实用效果。我们的代码可以在https://github.com/AaronNZH/Boosting-Few-shot-3D-Point-Cloud-Segmentation-via-Query-Guided-Enhancement上下载。
</details></li>
</ul>
<hr>
<h2 id="FireFly-A-Synthetic-Dataset-for-Ember-Detection-in-Wildfire"><a href="#FireFly-A-Synthetic-Dataset-for-Ember-Detection-in-Wildfire" class="headerlink" title="FireFly A Synthetic Dataset for Ember Detection in Wildfire"></a>FireFly A Synthetic Dataset for Ember Detection in Wildfire</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03164">http://arxiv.org/abs/2308.03164</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ergowho/firefly2.0">https://github.com/ergowho/firefly2.0</a></li>
<li>paper_authors: Yue Hu, Xinan Ye, Yifei Liu, Souvik Kundu, Gourav Datta, Srikar Mutnuri, Namo Asavisanu, Nora Ayanian, Konstantinos Psounis, Peter Beerel</li>
<li>for: 这个论文是为了提供一个用于燃烧检测的人工数据集，以替代现有的燃烧训练资源的缺乏。</li>
<li>methods: 这个论文使用了Unreal Engine 4（UE4）创建了一个名为“FireFly”的人工数据集，并提供了一种自动生成Synthetic labeled dataset的工具，以获得多种环境条件下的数据多样性。此外，他们还利用了一个已经训练的模型来创建一种半自动的标注过程，以便将真实的燃烧框架标注为数据集中。</li>
<li>results: 根据论文的描述，使用FireFly数据集训练四种流行的对象检测模型后，对于真实的野火场景下，相比于只使用小型实际数据集训练的模型，FireFly可以提供8.57%的提升在mean Average Precision（mAP）上。<details>
<summary>Abstract</summary>
This paper presents "FireFly", a synthetic dataset for ember detection created using Unreal Engine 4 (UE4), designed to overcome the current lack of ember-specific training resources. To create the dataset, we present a tool that allows the automated generation of the synthetic labeled dataset with adjustable parameters, enabling data diversity from various environmental conditions, making the dataset both diverse and customizable based on user requirements. We generated a total of 19,273 frames that have been used to evaluate FireFly on four popular object detection models. Further to minimize human intervention, we leveraged a trained model to create a semi-automatic labeling process for real-life ember frames. Moreover, we demonstrated an up to 8.57% improvement in mean Average Precision (mAP) in real-world wildfire scenarios compared to models trained exclusively on a small real dataset.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="CGBA-Curvature-aware-Geometric-Black-box-Attack"><a href="#CGBA-Curvature-aware-Geometric-Black-box-Attack" class="headerlink" title="CGBA: Curvature-aware Geometric Black-box Attack"></a>CGBA: Curvature-aware Geometric Black-box Attack</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03163">http://arxiv.org/abs/2308.03163</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/farhamdur/cgba">https://github.com/farhamdur/cgba</a></li>
<li>paper_authors: Md Farhamdur Reza, Ali Rahmati, Tianfu Wu, Huaiyu Dai</li>
<li>for: 这个论文是为了提出一种高效的黑盒攻击方法，能够在训练数据集上生成高质量的攻击示例。</li>
<li>methods: 该方法使用几何学决策法，在一个限制的2D平面上进行边搜索，以确保找到边点成功，无论边界曲率如何。</li>
<li>results: 对于一些常用的分类器，该方法可以高效地生成攻击示例，并且在非目标攻击和目标攻击两种情况下都显示出优秀的性能。<details>
<summary>Abstract</summary>
Decision-based black-box attacks often necessitate a large number of queries to craft an adversarial example. Moreover, decision-based attacks based on querying boundary points in the estimated normal vector direction often suffer from inefficiency and convergence issues. In this paper, we propose a novel query-efficient curvature-aware geometric decision-based black-box attack (CGBA) that conducts boundary search along a semicircular path on a restricted 2D plane to ensure finding a boundary point successfully irrespective of the boundary curvature. While the proposed CGBA attack can work effectively for an arbitrary decision boundary, it is particularly efficient in exploiting the low curvature to craft high-quality adversarial examples, which is widely seen and experimentally verified in commonly used classifiers under non-targeted attacks. In contrast, the decision boundaries often exhibit higher curvature under targeted attacks. Thus, we develop a new query-efficient variant, CGBA-H, that is adapted for the targeted attack. In addition, we further design an algorithm to obtain a better initial boundary point at the expense of some extra queries, which considerably enhances the performance of the targeted attack. Extensive experiments are conducted to evaluate the performance of our proposed methods against some well-known classifiers on the ImageNet and CIFAR10 datasets, demonstrating the superiority of CGBA and CGBA-H over state-of-the-art non-targeted and targeted attacks, respectively. The source code is available at https://github.com/Farhamdur/CGBA.
</details>
<details>
<summary>摘要</summary>
决策基于的黑盒攻击经常需要许多查询来制作攻击性的输入。另外，基于查询边缘点的决策攻击经常受到效率和收敛问题的影响。在这篇论文中，我们提出了一种新的查询效率高的几何决策基于黑盒攻击（CGBA），它在一个限定的2D平面上进行边搜索，以确保找到边点成功，不 matter the boundary curvature。尽管CGBA攻击可以有效地攻击任何决策边界，但是它在非目标攻击时尤其有效，可以轻松地制作高质量的攻击性输入。在目标攻击时，我们开发了一种新的查询效率变体CGBA-H，并设计了一个算法来获得更好的初始边界点，以提高目标攻击的性能。我们对一些常用的分类器进行了广泛的实验，并证明了CGBA和CGBA-H的超越性， Comparing with state-of-the-art non-targeted and targeted attacks。源代码可以在https://github.com/Farhamdur/CGBA中下载。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/07/cs.CV_2023_08_07/" data-id="clp9qz83d00hrok88dh1g6mwj" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_08_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/07/cs.AI_2023_08_07/" class="article-date">
  <time datetime="2023-08-07T12:00:00.000Z" itemprop="datePublished">2023-08-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/07/cs.AI_2023_08_07/">cs.AI - 2023-08-07</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="SemOpenAlex-The-Scientific-Landscape-in-26-Billion-RDF-Triples"><a href="#SemOpenAlex-The-Scientific-Landscape-in-26-Billion-RDF-Triples" class="headerlink" title="SemOpenAlex: The Scientific Landscape in 26 Billion RDF Triples"></a>SemOpenAlex: The Scientific Landscape in 26 Billion RDF Triples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03671">http://arxiv.org/abs/2308.03671</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Färber, David Lamprecht, Johan Krause, Linn Aung, Peter Haase</li>
<li>for: 这个论文是为了描述一个名为SemOpenAlex的大规模RDF知识图，该图包含了260亿个三元组，关于科学出版物和其相关的实体，如作者、机构、期刊和概念。</li>
<li>methods: 该论文使用了CC0协议，提供了免费和开放的数据访问渠道，包括RDF堆 dump文件、SPARQL端点和 Linked Open Data 云端，以及高性能计算 embedding 技术。</li>
<li>results: SemOpenAlex 可以满足广泛的用例enario，如探索性搜索、大规模科学影响量计算、科学领域之间的探索性分析、学术推荐系统、合作者推荐、出版物推荐、会议推荐等。<details>
<summary>Abstract</summary>
We present SemOpenAlex, an extensive RDF knowledge graph that contains over 26 billion triples about scientific publications and their associated entities, such as authors, institutions, journals, and concepts. SemOpenAlex is licensed under CC0, providing free and open access to the data. We offer the data through multiple channels, including RDF dump files, a SPARQL endpoint, and as a data source in the Linked Open Data cloud, complete with resolvable URIs and links to other data sources. Moreover, we provide embeddings for knowledge graph entities using high-performance computing. SemOpenAlex enables a broad range of use-case scenarios, such as exploratory semantic search via our website, large-scale scientific impact quantification, and other forms of scholarly big data analytics within and across scientific disciplines. Additionally, it enables academic recommender systems, such as recommending collaborators, publications, and venues, including explainability capabilities. Finally, SemOpenAlex can serve for RDF query optimization benchmarks, creating scholarly knowledge-guided language models, and as a hub for semantic scientific publishing.
</details>
<details>
<summary>摘要</summary>
我们现在提供SemOpenAlex，一个广泛的RDF知识图库，包含超过260亿个三元组关于科学出版物和其相关的实体，如作者、机构、杂志和概念。SemOpenAlex采用CC0许可证，提供免费和开放的数据访问。我们将数据提供多种途径，包括RDF填充文件、SPARQL终结点和 Linked Open Data 云端，并提供可访问的 URI 和与其他数据源的链接。此外，我们还提供知识图实体的嵌入，使用高性能计算。SemOpenAlex 支持广泛的使用场景，如探索性Semantic Search、大规模科学影响评估、科学领域之间和科学领域之间的大数据分析，以及学术推荐系统，如推荐合作者、论文和会议。此外，SemOpenAlex 还可以用于RDF查询优化基准，创建学术知识驱动的自然语言模型，以及为Semantic scientific publishing 的中心。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Model-in-Causal-Inference-with-Unmeasured-Confounders"><a href="#Diffusion-Model-in-Causal-Inference-with-Unmeasured-Confounders" class="headerlink" title="Diffusion Model in Causal Inference with Unmeasured Confounders"></a>Diffusion Model in Causal Inference with Unmeasured Confounders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03669">http://arxiv.org/abs/2308.03669</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tatsu432/BDCM">https://github.com/tatsu432/BDCM</a></li>
<li>paper_authors: Tatsuhiro Shimizu</li>
<li>for: 本研究旨在扩展Diffusion Model，以回答基于观察数据的 causal 问题，尽管存在不被测量的干扰因素。</li>
<li>methods: 我们提出了一种基于 Directed Acyclic Graph (DAG) 的 causal 模型（Diffusion-based Causal Model，DCM），并在这个模型中包含了扩散模型，以更准确地回答 causal 问题。</li>
<li>results: 我们的实验结果表明，在不被测量的干扰因素的存在下，我们的提议的模型可以更 precisely 捕捉 counterfactual 分布，与 DCM 相比。<details>
<summary>Abstract</summary>
We study how to extend the use of the diffusion model to answer the causal question from the observational data under the existence of unmeasured confounders. In Pearl's framework of using a Directed Acyclic Graph (DAG) to capture the causal intervention, a Diffusion-based Causal Model (DCM) was proposed incorporating the diffusion model to answer the causal questions more accurately, assuming that all of the confounders are observed. However, unmeasured confounders in practice exist, which hinders DCM from being applicable. To alleviate this limitation of DCM, we propose an extended model called Backdoor Criterion based DCM (BDCM), whose idea is rooted in the Backdoor criterion to find the variables in DAG to be included in the decoding process of the diffusion model so that we can extend DCM to the case with unmeasured confounders. Synthetic data experiment demonstrates that our proposed model captures the counterfactual distribution more precisely than DCM under the unmeasured confounders.
</details>
<details>
<summary>摘要</summary>
我们研究如何扩展Diffusion模型，以回答基于观察数据的 causal 问题，在存在未测量的干扰变量的情况下。在Pearl的框架下，使用 Directed Acyclic Graph (DAG) 捕捉 causal 干扰，Diffusion-based Causal Model (DCM) 被提出，将Diffusion模型与 causal 干扰相结合，以更准确地回答 causal 问题，假设所有干扰变量都是观察的。然而，在实践中，未测量的干扰变量存在，这限制了 DCM 的应用。为了解决 DCM 的这种限制，我们提出了一种扩展模型，即 Backdoor Criterion based DCM (BDCM)，其基于 Backdoor  criterion 来选择 DAG 中的变量，以便在Diffusion模型的解码过程中包含这些变量，从而扩展 DCM 到带有未测量的干扰变量的情况。 synthetic data 实验表明，我们的提出的模型可以更 precisely 回答 causal 问题，比 DCM 在未测量的干扰变量的情况下。
</details></li>
</ul>
<hr>
<h2 id="QDax-A-Library-for-Quality-Diversity-and-Population-based-Algorithms-with-Hardware-Acceleration"><a href="#QDax-A-Library-for-Quality-Diversity-and-Population-based-Algorithms-with-Hardware-Acceleration" class="headerlink" title="QDax: A Library for Quality-Diversity and Population-based Algorithms with Hardware Acceleration"></a>QDax: A Library for Quality-Diversity and Population-based Algorithms with Hardware Acceleration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03665">http://arxiv.org/abs/2308.03665</a></li>
<li>repo_url: None</li>
<li>paper_authors: Felix Chalumeau, Bryan Lim, Raphael Boige, Maxime Allard, Luca Grillotti, Manon Flageat, Valentin Macé, Arthur Flajolet, Thomas Pierrot, Antoine Cully</li>
<li>for: The paper is written for researchers and practitioners who are interested in using Quality-Diversity (QD) optimization algorithms in Jax for various optimization purposes, including black-box optimization and continuous control.</li>
<li>methods: The paper presents QDax, an open-source library with a streamlined and modular API for QD optimization algorithms in Jax. The library offers implementations of popular QD, Neuroevolution, and Reinforcement Learning (RL) algorithms, supported by various examples.</li>
<li>results: The paper demonstrates the efficiency and flexibility of QDax by testing it with 95% coverage and showing that it can be just-in-time compiled with Jax for efficient execution across multiple accelerators, including GPUs and TPUs.<details>
<summary>Abstract</summary>
QDax is an open-source library with a streamlined and modular API for Quality-Diversity (QD) optimization algorithms in Jax. The library serves as a versatile tool for optimization purposes, ranging from black-box optimization to continuous control. QDax offers implementations of popular QD, Neuroevolution, and Reinforcement Learning (RL) algorithms, supported by various examples. All the implementations can be just-in-time compiled with Jax, facilitating efficient execution across multiple accelerators, including GPUs and TPUs. These implementations effectively demonstrate the framework's flexibility and user-friendliness, easing experimentation for research purposes. Furthermore, the library is thoroughly documented and tested with 95\% coverage.
</details>
<details>
<summary>摘要</summary>
QDax 是一个开源库，具有整合和可调的 API，用于纯度多样化（QD）优化算法在 Jax 中。这个库可以用于各种优化目的，由黑盒优化到连续控制。QDax 提供了各种流行的 QD、神经演化和强化学习（RL）算法的实现，并且支持多个例子。这些实现可以与 Jax 的 Just-in-Time 编译功能相结合，以提高在多个加速器（包括 GPU 和 TPU）上的执行效率。这些实现也详细地显示了框架的 flexibility 和用户友善性，便利实验研究。此外，库也受到了95%的覆盖率测试。
</details></li>
</ul>
<hr>
<h2 id="Detecting-Spells-in-Fantasy-Literature-with-a-Transformer-Based-Artificial-Intelligence"><a href="#Detecting-Spells-in-Fantasy-Literature-with-a-Transformer-Based-Artificial-Intelligence" class="headerlink" title="Detecting Spells in Fantasy Literature with a Transformer Based Artificial Intelligence"></a>Detecting Spells in Fantasy Literature with a Transformer Based Artificial Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03660">http://arxiv.org/abs/2308.03660</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcel Moravek, Alexander Zender, Andreas Müller</li>
<li>for: 本研究使用BERT架构进行魔法咒语识别，以识别哈利·波特小说系列中的魔法咒语。</li>
<li>methods: 我们使用预训练的BERT模型，并对不同的数据集和训练方法进行了微调，以识别咒语的上下文。</li>
<li>results: 我们的实验结果表明，可以使用BERT模型来识别咒语的上下文，并且采用不同的序列分类和Token分类方法可以提高模型的准确率。此外，我们还发现了咒语的总体特征，可以将模型应用于其他奇幻世界。<details>
<summary>Abstract</summary>
Transformer architectures and models have made significant progress in language-based tasks. In this area, is BERT one of the most widely used and freely available transformer architecture. In our work, we use BERT for context-based phrase recognition of magic spells in the Harry Potter novel series. Spells are a common part of active magic in fantasy novels. Typically, spells are used in a specific context to achieve a supernatural effect. A series of investigations were conducted to see if a Transformer architecture could recognize such phrases based on their context in the Harry Potter saga. For our studies a pre-trained BERT model was used and fine-tuned utilising different datasets and training methods to identify the searched context. By considering different approaches for sequence classification as well as token classification, it is shown that the context of spells can be recognised. According to our investigations, the examined sequence length for fine-tuning and validation of the model plays a significant role in context recognition. Based on this, we have investigated whether spells have overarching properties that allow a transfer of the neural network models to other fantasy universes as well. The application of our model showed promising results and is worth to be deepened in subsequent studies.
</details>
<details>
<summary>摘要</summary>
transformer 架构和模型在语言相关任务中做出了重要进展。在这个领域中，BERT是一个非常受欢迎的和可公开使用的 transformer 架构。在我们的工作中，我们使用 BERT 进行了文本上下文基于短语识别， specifically 在哈利·ポッター系列小说中的魔法 incantations。incantations 是魔法世界中常见的一种特殊语言，通常在特定的上下文中使用以实现超自然的效果。我们通过不同的数据集和训练方法来练习和 fine-tune 一个预训练的 BERT 模型，以识别这些上下文。我们的研究表明，模型的检查序列长度在 fine-tuning 和验证中发挥了重要作用。此外，我们还 investigate 了 whether spells have overarching properties that allow a transfer of the neural network models to other fantasy universes。我们的应用结果表示该模型具有潜在的应用价值，值得进一步研究。
</details></li>
</ul>
<hr>
<h2 id="FFF-Fragments-Guided-Flexible-Fitting-for-Building-Complete-Protein-Structures"><a href="#FFF-Fragments-Guided-Flexible-Fitting-for-Building-Complete-Protein-Structures" class="headerlink" title="FFF: Fragments-Guided Flexible Fitting for Building Complete Protein Structures"></a>FFF: Fragments-Guided Flexible Fitting for Building Complete Protein Structures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03654">http://arxiv.org/abs/2308.03654</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weijie Chen, Xinyan Wang, Yuhang Wang</li>
<li>for: 该 paper 描述了一种新的方法 FFP，它可以将蛋白质结构预测和蛋白质结构识别相结合，从而构建完整的蛋白质结构。</li>
<li>methods: 该方法使用多级识别网络来捕捉输入 3D 粒子电子镜像中的多种结构特征，然后使用 Pseudo 氨基酸 вектор 和蛋白质序列对照方法来生成蛋白质结构片段。最后，通过 flexible 匹配来构建完整的结构模型。</li>
<li>results: 根据我们的测试， FFP 方法在构建完整蛋白质结构方面比基eline 方法表现更好。<details>
<summary>Abstract</summary>
Cryo-electron microscopy (cryo-EM) is a technique for reconstructing the 3-dimensional (3D) structure of biomolecules (especially large protein complexes and molecular assemblies). As the resolution increases to the near-atomic scale, building protein structures de novo from cryo-EM maps becomes possible. Recently, recognition-based de novo building methods have shown the potential to streamline this process. However, it cannot build a complete structure due to the low signal-to-noise ratio (SNR) problem. At the same time, AlphaFold has led to a great breakthrough in predicting protein structures. This has inspired us to combine fragment recognition and structure prediction methods to build a complete structure. In this paper, we propose a new method named FFF that bridges protein structure prediction and protein structure recognition with flexible fitting. First, a multi-level recognition network is used to capture various structural features from the input 3D cryo-EM map. Next, protein structural fragments are generated using pseudo peptide vectors and a protein sequence alignment method based on these extracted features. Finally, a complete structural model is constructed using the predicted protein fragments via flexible fitting. Based on our benchmark tests, FFF outperforms the baseline methods for building complete protein structures.
</details>
<details>
<summary>摘要</summary>
冻电子顺ligtroscopy（冻电子顺igtroscopy）是一种用于 reconstruction生物分子（特别是大蛋白复合物和分子集合体）的3维结构的技术。随着分辨率逐渐提高到近原子尺度，从冻电子顺igtroscopy地图中直接建立蛋白结构的可能性增加。然而，由于低信号噪声比（SNR）问题，不能完全建立蛋白结构。同时，AlphaFold在预测蛋白结构方面取得了重大突破。这种灵感我们将Recognition-based de novo building方法和结构预测方法相结合，以建立完整的结构。在这篇论文中，我们提出一种新的方法 named FFF，它可以将蛋白结构预测和蛋白结构认知相连接。首先，我们使用多级认知网络来捕捉输入3D冻电子顺igtroscopy地图中的多种结构特征。然后，我们使用pseudo peptide vectors和基于这些提取的蛋白序列对应方法来生成蛋白结构分割。最后，我们使用预测的蛋白分割来建立完整的结构模型，并通过flexible fitting来调整它们。根据我们的测试，FFF方法在建立完整蛋白结构方面表现出色，超过了基eline方法。
</details></li>
</ul>
<hr>
<h2 id="Segmentation-Framework-for-Heat-Loss-Identification-in-Thermal-Images-Empowering-Scottish-Retrofitting-and-Thermographic-Survey-Companies"><a href="#Segmentation-Framework-for-Heat-Loss-Identification-in-Thermal-Images-Empowering-Scottish-Retrofitting-and-Thermographic-Survey-Companies" class="headerlink" title="Segmentation Framework for Heat Loss Identification in Thermal Images: Empowering Scottish Retrofitting and Thermographic Survey Companies"></a>Segmentation Framework for Heat Loss Identification in Thermal Images: Empowering Scottish Retrofitting and Thermographic Survey Companies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03631">http://arxiv.org/abs/2308.03631</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Junayed Hasan, Eyad Elyan, Yijun Yan, Jinchang Ren, Md Mostafa Kamal Sarker</li>
<li>For: This study aims to tackle fuel poverty in Scotland by automating the identification of heat loss sources in thermal images of homes, using a deep learning-based segmentation framework.* Methods: The proposed framework uses a Mask Region Proposal Convolutional Neural Network (Mask RCNN) to segment heat loss sources caused by weak insulation, and eliminates obstructive objects present in the images.* Results: The final fine-tuned model achieved a mean average precision (mAP) score of 77.2% for segmenting the target objects (heat loss sources), demonstrating the potential of the proposed framework in accurately quantifying energy loss in Scottish homes.<details>
<summary>Abstract</summary>
Retrofitting and thermographic survey (TS) companies in Scotland collaborate with social housing providers to tackle fuel poverty. They employ ground-level infrared (IR) camera-based-TSs (GIRTSs) for collecting thermal images to identi-fy the heat loss sources resulting from poor insulation. However, this identifica-tion process is labor-intensive and time-consuming, necessitating extensive data processing. To automate this, an AI-driven approach is necessary. Therefore, this study proposes a deep learning (DL)-based segmentation framework using the Mask Region Proposal Convolutional Neural Network (Mask RCNN) to validate its applicability to these thermal images. The objective of the framework is to au-tomatically identify, and crop heat loss sources caused by weak insulation, while also eliminating obstructive objects present in those images. By doing so, it min-imizes labor-intensive tasks and provides an automated, consistent, and reliable solution. To validate the proposed framework, approximately 2500 thermal imag-es were collected in collaboration with industrial TS partner. Then, 1800 repre-sentative images were carefully selected with the assistance of experts and anno-tated to highlight the target objects (TO) to form the final dataset. Subsequently, a transfer learning strategy was employed to train the dataset, progressively aug-menting the training data volume and fine-tuning the pre-trained baseline Mask RCNN. As a result, the final fine-tuned model achieved a mean average precision (mAP) score of 77.2% for segmenting the TO, demonstrating the significant po-tential of proposed framework in accurately quantifying energy loss in Scottish homes.
</details>
<details>
<summary>摘要</summary>
历史遗产改造和 thermographic 检测（TS）公司在苏格兰与社会住房提供商合作，解决燃料贫困问题。他们使用地面近红外（IR）摄像机基本TS（GIRTS）来收集热图像，以识别因为差异垄断的热损源。但这个识别过程劳动 INTENSIVE 和时间consuming，需要广泛的数据处理。为了自动化这个过程，这种研究提出了基于人工智能（AI）的分割框架，使用面部提案卷积神经网络（Mask RCNN）来验证其适用性。该框架的目标是自动地识别和cropping热损源，并从热图像中排除干扰物体。通过这样做，它将减少劳动 INTENSIVE 任务，提供一个自动化、一致、可靠的解决方案。为验证提出的框架，约2500个热图像被收集，并与业务TS伙伴合作。然后，1800个代表性图像被谨慎选择，并由专家帮助高亮目标对象（TO），以形成最终数据集。接着，使用传输学习策略进行训练数据集，逐渐增加训练数据量，并进行细化和微调。最终，经过微调的基线Mask RCNN模型在 segmenting TO 方面获得了77.2%的均值精度分（mAP），显示了提出的框架在准确量化苏格兰家庭的能源损失中的显著潜力。
</details></li>
</ul>
<hr>
<h2 id="MedMine-Examining-Pre-trained-Language-Models-on-Medication-Mining"><a href="#MedMine-Examining-Pre-trained-Language-Models-on-Medication-Mining" class="headerlink" title="MedMine: Examining Pre-trained Language Models on Medication Mining"></a>MedMine: Examining Pre-trained Language Models on Medication Mining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03629">http://arxiv.org/abs/2308.03629</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hecta-uom/m3">https://github.com/hecta-uom/m3</a></li>
<li>paper_authors: Haifa Alrdahi, Lifeng Han, Hendrik Šuvalov, Goran Nenadic</li>
<li>for: 这个研究的目的是探讨现有的州前语言模型（PLM）在自动药物探索领域的表现，以及如何将其应用到临床实践中。</li>
<li>methods: 研究使用了现有的州前语言模型（PLM），包括Med7和XLM-RoBERTa，进行精确化训练。</li>
<li>results: 研究发现现有的PLM模型在自动药物探索 task 上存在不均衡的表现，特别是在不同的实体类型和临床事件上。<details>
<summary>Abstract</summary>
Automatic medication mining from clinical and biomedical text has become a popular topic due to its real impact on healthcare applications and the recent development of powerful language models (LMs). However, fully-automatic extraction models still face obstacles to be overcome such that they can be deployed directly into clinical practice for better impacts. Such obstacles include their imbalanced performances on different entity types and clinical events. In this work, we examine current state-of-the-art pre-trained language models (PLMs) on such tasks, via fine-tuning including the monolingual model Med7 and multilingual large language model (LLM) XLM-RoBERTa. We compare their advantages and drawbacks using historical medication mining shared task data sets from n2c2-2018 challenges. We report the findings we get from these fine-tuning experiments such that they can facilitate future research on addressing them, for instance, how to combine their outputs, merge such models, or improve their overall accuracy by ensemble learning and data augmentation. MedMine is part of the M3 Initiative \url{https://github.com/HECTA-UoM/M3}
</details>
<details>
<summary>摘要</summary>
自动药物挖掘从医疗和生物医学文本中得到了广泛的关注，因为它们在医疗应用中有真正的影响。然而，完全自动提取模型仍然需要突破一些障碍，以便在临床实践中直接部署。这些障碍包括它们在不同实体类型和医疗事件上的不均衡性表现。在这项工作中，我们评估了当前状态的批处理语言模型（PLM）在这些任务上，包括单语言模型Med7和多语言大语言模型（LLM）XLM-RoBERTa。我们比较了它们的优点和缺点，使用历史药物挖掘分享任务数据集。我们报告了这些精度调整实验的结果，以便未来研究如何组合它们的输出、合并这些模型或提高它们的总准确率 durch ensemble学习和数据扩展。MedMine是M3Initiave的一部分，详细信息请参考<https://github.com/HECTA-UoM/M3>。
</details></li>
</ul>
<hr>
<h2 id="A-Meta-learning-based-Stacked-Regression-Approach-for-Customer-Lifetime-Value-Prediction"><a href="#A-Meta-learning-based-Stacked-Regression-Approach-for-Customer-Lifetime-Value-Prediction" class="headerlink" title="A Meta-learning based Stacked Regression Approach for Customer Lifetime Value Prediction"></a>A Meta-learning based Stacked Regression Approach for Customer Lifetime Value Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08502">http://arxiv.org/abs/2308.08502</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karan Gadgil, Sukhpal Singh Gill, Ahmed M. Abdelmoniem</li>
<li>For: The paper aims to propose a simple yet effective and interpretable Customer Lifetime Value (CLV) prediction model that can handle a wide variety of input features and is applicable in various business domains.* Methods: The proposed model is based on a meta-learning-based stacked regression approach that combines the predictions from bagging and boosting models.* Results: The proposed model was empirically tested on an openly available Online Retail dataset and showed superior performance compared to existing distribution-based and basic models.Here’s the simplified Chinese version:* For: 这篇论文目的是提出一种简单又有效、可解释的客户生命周期价值（CLV）预测模型，可以处理各种输入特征并在各个业务领域中适用。* Methods: 该模型基于元学习基层堆叠回归方法，将权重融合 bagging 和 boosting 模型的预测结果。* Results: 该模型在一个公开的在线零售数据集上进行了实验测试，与现有的分布型和基础型模型相比，显示出了更高的性能。<details>
<summary>Abstract</summary>
Companies across the globe are keen on targeting potential high-value customers in an attempt to expand revenue and this could be achieved only by understanding the customers more. Customer Lifetime Value (CLV) is the total monetary value of transactions/purchases made by a customer with the business over an intended period of time and is used as means to estimate future customer interactions. CLV finds application in a number of distinct business domains such as Banking, Insurance, Online-entertainment, Gaming, and E-Commerce. The existing distribution-based and basic (recency, frequency & monetary) based models face a limitation in terms of handling a wide variety of input features. Moreover, the more advanced Deep learning approaches could be superfluous and add an undesirable element of complexity in certain application areas. We, therefore, propose a system which is able to qualify both as effective, and comprehensive yet simple and interpretable. With that in mind, we develop a meta-learning-based stacked regression model which combines the predictions from bagging and boosting models that each is found to perform well individually. Empirical tests have been carried out on an openly available Online Retail dataset to evaluate various models and show the efficacy of the proposed approach.
</details>
<details>
<summary>摘要</summary>
世界各地公司都在努力寻找高值客户，以拓展收入。这可以通过更好地理解客户来实现。客户生命周期价值（CLV）是指客户在业务之间的财务交易总额，在一定时间范围内，并用于预测未来客户互动。CLV在银行、保险、在线娱乐、游戏和电商等多个业务领域有广泛的应用。现有的分布型和基础（频率、购买量和金额）型模型在处理各种输入特征方面存在限制。此外，更高级的深度学习方法可能会增加不必要的复杂性，特别在某些应用领域。我们因此提出了一个能够同时具有效果、全面、简单并且可解释的系统。为了实现这一目标，我们开发了基于元学习的堆式回归模型，该模型将束合袋装和提升模型的预测结果。我们对公开ailable的在线零售数据集进行了实验，以评估不同模型的表现，并证明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Stock-Market-Price-Prediction-A-Hybrid-LSTM-and-Sequential-Self-Attention-based-Approach"><a href="#Stock-Market-Price-Prediction-A-Hybrid-LSTM-and-Sequential-Self-Attention-based-Approach" class="headerlink" title="Stock Market Price Prediction: A Hybrid LSTM and Sequential Self-Attention based Approach"></a>Stock Market Price Prediction: A Hybrid LSTM and Sequential Self-Attention based Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04419">http://arxiv.org/abs/2308.04419</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karan Pardeshi, Sukhpal Singh Gill, Ahmed M. Abdelmoniem</li>
<li>for: 预测股票价格，帮助投资者做出最佳决策。</li>
<li>methods: 使用深度学习策略，具体是提议一种新的长期记忆（LSTM）加上顺序自注意机制（SSAM）模型，以提高股票价格预测的准确性。</li>
<li>results: 对三个股票数据集（SBIN、HDFCBANK、BANKBARODA）进行了广泛的实验，结果表明提议的模型比现有模型更有效率和可行，RMSE和R2评价指标表现最佳。<details>
<summary>Abstract</summary>
One of the most enticing research areas is the stock market, and projecting stock prices may help investors profit by making the best decisions at the correct time. Deep learning strategies have emerged as a critical technique in the field of the financial market. The stock market is impacted due to two aspects, one is the geo-political, social and global events on the bases of which the price trends could be affected. Meanwhile, the second aspect purely focuses on historical price trends and seasonality, allowing us to forecast stock prices. In this paper, our aim is to focus on the second aspect and build a model that predicts future prices with minimal errors. In order to provide better prediction results of stock price, we propose a new model named Long Short-Term Memory (LSTM) with Sequential Self-Attention Mechanism (LSTM-SSAM). Finally, we conduct extensive experiments on the three stock datasets: SBIN, HDFCBANK, and BANKBARODA. The experimental results prove the effectiveness and feasibility of the proposed model compared to existing models. The experimental findings demonstrate that the root-mean-squared error (RMSE), and R-square (R2) evaluation indicators are giving the best results.
</details>
<details>
<summary>摘要</summary>
一个非常吸引人的研究领域是股市，并且预测股价可以帮助投资者取得最佳的决策时间。深度学习策略在金融市场中发挥了关键作用。股市受到两个方面的影响，一是地域政治、社会和全球事件的影响，这些事件可能对股价趋势产生影响。而第二个方面则是历史价格趋势和季节性，我们可以通过这些信息来预测股价。在这篇论文中，我们将关注第二个方面，并建立一个名为Long Short-Term Memory（LSTM）的新模型，并加入Sequential Self-Attention Mechanism（LSTM-SSAM）。最后，我们对SBIN、HDFCBANK和BANKBARODA三个股Dataset进行了广泛的实验。实验结果证明了我们提出的模型的有效性和实现性，并且与现有模型进行比较。实验结果表明，使用RMSE和R2评价指标，我们的模型在预测股价方面表现出色。
</details></li>
</ul>
<hr>
<h2 id="Why-We-Don’t-Have-AGI-Yet"><a href="#Why-We-Don’t-Have-AGI-Yet" class="headerlink" title="Why We Don’t Have AGI Yet"></a>Why We Don’t Have AGI Yet</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03598">http://arxiv.org/abs/2308.03598</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peter Voss, Mladjan Jovanovic</li>
<li>for: 这篇论文旨在探讨人工智能的发展，尤其是人工通用智能（AGI）的概念和发展难点。</li>
<li>methods: 本论文使用了统计学方法，探讨了采用统计学方法是否能够实现人工智能。同时，它还检查了一些关键的认知能力，以及它们在人工智能实现中的作用。</li>
<li>results: 本论文结果显示，统计学方法 alone 是不够实现人工智能的。它还指出了一些关键的认知能力，以及它们在人工智能实现中的作用。此外，它还检查了一些社会技术因素，以及它们对人工智能发展的影响。<details>
<summary>Abstract</summary>
The original vision of AI was re-articulated in 2002 via the term 'Artificial General Intelligence' or AGI. This vision is to build 'Thinking Machines' - computer systems that can learn, reason, and solve problems similar to the way humans do. This is in stark contrast to the 'Narrow AI' approach practiced by almost everyone in the field over the many decades. While several large-scale efforts have nominally been working on AGI (most notably DeepMind), the field of pure focused AGI development has not been well funded or promoted. This is surprising given the fantastic value that true AGI can bestow on humanity. In addition to the dearth of effort in this field, there are also several theoretical and methodical missteps that are hampering progress. We highlight why purely statistical approaches are unlikely to lead to AGI, and identify several crucial cognitive abilities required to achieve human-like adaptability and autonomous learning. We conclude with a survey of socio-technical factors that have undoubtedly slowed progress towards AGI.
</details>
<details>
<summary>摘要</summary>
原始的人工智能概念在2002年被重新艺术iculminated 以"人工通用智能"（AGI）的形式。这个目标是建立"思维机器"——计算机系统可以学习、理据和解决问题，与人类相似。这与在多个时期内的"窄AI"方法不同，大多数人在该领域的努力都是这种方法。虽然有几个大规模尝试在AGI方面工作（特别是DeepMind），但是纯粹的AGI发展领域没有得到过足够的投资和推广。这对人类的未来带来了惊人的价值。此外，我们还指出了统计方法不可能导致AGI的理论和方法上的阻碍因素，并识别了达到人类式的适应和自主学习所需的关键认知能力。我们结束于论述AGI的发展受到了社会技术因素的阻碍。
</details></li>
</ul>
<hr>
<h2 id="Feature-Importance-versus-Feature-Influence-and-What-It-Signifies-for-Explainable-AI"><a href="#Feature-Importance-versus-Feature-Influence-and-What-It-Signifies-for-Explainable-AI" class="headerlink" title="Feature Importance versus Feature Influence and What It Signifies for Explainable AI"></a>Feature Importance versus Feature Influence and What It Signifies for Explainable AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03589">http://arxiv.org/abs/2308.03589</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kary Främling</li>
<li>for: 这种研究是为了提出一种全局和地方特征重要性的定义，以及一种基于值Utility概念的实时解释方法。</li>
<li>methods: 这种方法使用Contextual Importance和Contextual Utility（CIU）来评估特征重要性，并对不同方法的精度和稳定性进行评估。</li>
<li>results: 研究表明，使用CIU可以提供更有表达力和更灵活的解释，并且可以减少因果关系的偏见。<details>
<summary>Abstract</summary>
When used in the context of decision theory, feature importance expresses how much changing the value of a feature can change the model outcome (or the utility of the outcome), compared to other features. Feature importance should not be confused with the feature influence used by most state-of-the-art post-hoc Explainable AI methods. Contrary to feature importance, feature influence is measured against a reference level or baseline. The Contextual Importance and Utility (CIU) method provides a unified definition of global and local feature importance that is applicable also for post-hoc explanations, where the value utility concept provides instance-level assessment of how favorable or not a feature value is for the outcome. The paper shows how CIU can be applied to both global and local explainability, assesses the fidelity and stability of different methods, and shows how explanations that use contextual importance and contextual utility can provide more expressive and flexible explanations than when using influence only.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:在决策理论中，特征重要度表示修改特征值会改变模型结果（或结果的价值）的程度，相比其他特征。这与特征影响不同，特征影响是相对参照水平或基线进行度量。Contextual Importance and Utility（CIU）方法提供了一个综合定义的全局和本地特征重要度，可以应用于后期解释，其中值用性概念提供了实例级别的评估结果如何有利或不利于结果。文章显示了CIU如何应用于全局和本地解释，评估不同方法的准确性和稳定性，并显示了使用Contextual Importance和Contextual Utility来提供更加表达性和灵活的解释，相比使用影响只。
</details></li>
</ul>
<hr>
<h2 id="A-machine-learning-sleep-wake-classification-model-using-a-reduced-number-of-features-derived-from-photoplethysmography-and-activity-signals"><a href="#A-machine-learning-sleep-wake-classification-model-using-a-reduced-number-of-features-derived-from-photoplethysmography-and-activity-signals" class="headerlink" title="A machine-learning sleep-wake classification model using a reduced number of features derived from photoplethysmography and activity signals"></a>A machine-learning sleep-wake classification model using a reduced number of features derived from photoplethysmography and activity signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05759">http://arxiv.org/abs/2308.05759</a></li>
<li>repo_url: None</li>
<li>paper_authors: Douglas A. Almeida, Felipe M. Dias, Marcelo A. F. Toledo, Diego A. C. Cardenas, Filipe A. C. Oliveira, Estela Ribeiro, Jose E. Krieger, Marco A. Gutierrez</li>
<li>for: 这个研究的目的是开发一种基于XTreme Gradient Boosting（XGBoost）算法和血氧信号和活动计数的机器学习睡眠-醒目分类模型，以提高睡眠质量和全身健康。</li>
<li>methods: 这个模型使用了血氧信号和活动计数的特征EXTraction，并使用了XGBoost算法进行分类。</li>
<li>results: 该模型的性能与当前领先方法相当，具有感知率91.15 $\pm$ 1.16%, 特征率53.66 $\pm$ 1.12%, F1分数83.88 $\pm$ 0.56%和κ48.0 $\pm$ 0.86%。这个方法在计算能力有限的穿戴式设备中可以实现更好的性能。<details>
<summary>Abstract</summary>
Sleep is a crucial aspect of our overall health and well-being. It plays a vital role in regulating our mental and physical health, impacting our mood, memory, and cognitive function to our physical resilience and immune system. The classification of sleep stages is a mandatory step to assess sleep quality, providing the metrics to estimate the quality of sleep and how well our body is functioning during this essential period of rest. Photoplethysmography (PPG) has been demonstrated to be an effective signal for sleep stage inference, meaning it can be used on its own or in a combination with others signals to determine sleep stage. This information is valuable in identifying potential sleep issues and developing strategies to improve sleep quality and overall health. In this work, we present a machine learning sleep-wake classification model based on the eXtreme Gradient Boosting (XGBoost) algorithm and features extracted from PPG signal and activity counts. The performance of our method was comparable to current state-of-the-art methods with a Sensitivity of 91.15 $\pm$ 1.16%, Specificity of 53.66 $\pm$ 1.12%, F1-score of 83.88 $\pm$ 0.56%, and Kappa of 48.0 $\pm$ 0.86%. Our method offers a significant improvement over other approaches as it uses a reduced number of features, making it suitable for implementation in wearable devices that have limited computational power.
</details>
<details>
<summary>摘要</summary>
睡眠是我们全面健康和卫生的重要组成部分。它对我们的情绪、身体健康和智能功能产生重要的影响，同时也影响我们的免疫力和身体抵抗力。确定睡眠阶段是一项必要的步骤，以评估睡眠质量，并提供评估睡眠质量和身体功能的指标。光谱 Plethysmography (PPG) 已被证明是一种有效的睡眠阶段推断信号，因此可以单独使用或与其他信号结合使用来确定睡眠阶段。这些信息对于检测可能存在的睡眠问题和改善睡眠质量和全面健康提供了 ценности。在这个工作中，我们提出了基于 eXtreme Gradient Boosting (XGBoost) 算法和 PPG 信号和活动计数的机器学习睡眠-醒目分类模型。我们的方法的性能与当前状态的方法相当，具有感知率为 91.15 $\pm$ 1.16%、特异性为 53.66 $\pm$ 1.12%、F1 分数为 83.88 $\pm$ 0.56% 和 Kappa 值为 48.0 $\pm$ 0.86%。我们的方法提供了与其他方法相比的显著改善，因为它使用了减少的特征数，使其适合在有限的计算能力的穿戴式设备中实现。
</details></li>
</ul>
<hr>
<h2 id="Revealing-the-Underlying-Patterns-Investigating-Dataset-Similarity-Performance-and-Generalization"><a href="#Revealing-the-Underlying-Patterns-Investigating-Dataset-Similarity-Performance-and-Generalization" class="headerlink" title="Revealing the Underlying Patterns: Investigating Dataset Similarity, Performance, and Generalization"></a>Revealing the Underlying Patterns: Investigating Dataset Similarity, Performance, and Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03580">http://arxiv.org/abs/2308.03580</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akshit Achara, Ram Krishna Pandey</li>
<li>for: 了解深度学习模型的表现和泛化能力</li>
<li>methods: 使用图像-图像、数据集-数据集和图像-数据集距离来理解模型的行为</li>
<li>results: 通过添加一小数量的未看过图像（例如1、3或7）到训练集，可以提高模型的泛化能力，并降低训练和标注成本。<details>
<summary>Abstract</summary>
Supervised deep learning models require significant amount of labelled data to achieve an acceptable performance on a specific task. However, when tested on unseen data, the models may not perform well. Therefore, the models need to be trained with additional and varying labelled data to improve the generalization. In this work, our goal is to understand the models, their performance and generalization. We establish image-image, dataset-dataset, and image-dataset distances to gain insights into the model's behavior. Our proposed distance metric when combined with model performance can help in selecting an appropriate model/architecture from a pool of candidate architectures. We have shown that the generalization of these models can be improved by only adding a small number of unseen images (say 1, 3 or 7) into the training set. Our proposed approach reduces training and annotation costs while providing an estimate of model performance on unseen data in dynamic environments.
</details>
<details>
<summary>摘要</summary>
深度学习模型需要大量标注数据来达到特定任务的可接受性水平。然而，当测试在未看到的数据时，模型可能不会表现好。因此，模型需要通过添加更多和变化的标注数据来改善通用性。在这项工作中，我们的目标是理解模型、其性能和通用性。我们定义图像-图像、数据集-数据集和图像-数据集距离，以获得模型的行为的启示。我们的提议的距离度量器，当与模型性能相结合，可以帮助选择最佳的模型/架构从候选 arquitectures中。我们已经示出，通过只添加一小数量的未看到图像（例如1、3或7）到训练集中，可以改善这些模型的通用性。我们的提议方法可以降低训练和注释成本，同时提供对未看到数据的模型性能的估计，在动态环境中。
</details></li>
</ul>
<hr>
<h2 id="Provably-Efficient-Learning-in-Partially-Observable-Contextual-Bandit"><a href="#Provably-Efficient-Learning-in-Partially-Observable-Contextual-Bandit" class="headerlink" title="Provably Efficient Learning in Partially Observable Contextual Bandit"></a>Provably Efficient Learning in Partially Observable Contextual Bandit</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03572">http://arxiv.org/abs/2308.03572</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xueping Gong, Jiheng Zhang</li>
<li>for:  investigate transfer learning in partially observable contextual bandits</li>
<li>methods:  convert the problem to identifying or partially identifying causal effects through optimization problems, and use sampling algorithms to obtain causal bounds</li>
<li>results:  improve the performance of classical bandit algorithms and achieve orders of magnitude faster convergence rates, especially in tasks with function approximation.<details>
<summary>Abstract</summary>
In this paper, we investigate transfer learning in partially observable contextual bandits, where agents have limited knowledge from other agents and partial information about hidden confounders. We first convert the problem to identifying or partially identifying causal effects between actions and rewards through optimization problems. To solve these optimization problems, we discretize the original functional constraints of unknown distributions into linear constraints, and sample compatible causal models via sequentially solving linear programmings to obtain causal bounds with the consideration of estimation error. Our sampling algorithms provide desirable convergence results for suitable sampling distributions. We then show how causal bounds can be applied to improving classical bandit algorithms and affect the regrets with respect to the size of action sets and function spaces. Notably, in the task with function approximation which allows us to handle general context distributions, our method improves the order dependence on function space size compared with previous literatures. We formally prove that our causally enhanced algorithms outperform classical bandit algorithms and achieve orders of magnitude faster convergence rates. Finally, we perform simulations that demonstrate the efficiency of our strategy compared to the current state-of-the-art methods. This research has the potential to enhance the performance of contextual bandit agents in real-world applications where data is scarce and costly to obtain.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了在部分可见情况下的 contextual bandit 中的转移学习，agent有限制的知识来自其他代理人和部分隐藏的干扰因素。我们首先将问题转化为标定或部分标定 causal 效应 между动作和奖励，通过优化问题来解决。为解决这些优化问题，我们将原始Unknown Distributions的函数约束转化为线性约束，并通过顺序解决线性程序来采样Compatible causal models，从而获得 causal  bound  WITH consideration of estimation error。我们的采样算法提供了可靠的连续抽象结果。然后，我们示了如何使用 causal bound 来改进经典 bandit 算法，并对 act 集和函数空间大小的选择产生影响。尤其在可以处理通用上下文分布时，我们的方法提高了对函数空间大小的依赖性。我们正式证明我们的 causally enhanced 算法比经典 bandit 算法更高效，并实现了orders of magnitude  faster convergence rates。最后，我们进行了 simulations ，证明我们的策略比现有的方法更高效。这项研究有望提高实际应用中的 contextual bandit 代理人性能，因为数据是珍贵和costly to obtain。
</details></li>
</ul>
<hr>
<h2 id="MSLE-An-ontology-for-Materials-Science-Laboratory-Equipment-Large-Scale-Devices-for-Materials-Characterization"><a href="#MSLE-An-ontology-for-Materials-Science-Laboratory-Equipment-Large-Scale-Devices-for-Materials-Characterization" class="headerlink" title="MSLE: An ontology for Materials Science Laboratory Equipment. Large-Scale Devices for Materials Characterization"></a>MSLE: An ontology for Materials Science Laboratory Equipment. Large-Scale Devices for Materials Characterization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07325">http://arxiv.org/abs/2308.07325</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mehrdad Jalali, Matthias Mail, Rossella Aversa, Christian Kübel</li>
<li>for: 这个论文是为了开发一个Materials Science Laboratory Equipment（MSLE） Ontology，用于统一材料科学实验室设备的描述和使用。</li>
<li>methods: 该论文使用了两个现有的 ontology：Semantic Sensor Network（SSN）和Material Vocabulary（MatVoc），将其综合到 MSLE 核心中，建立了一个协调的 ontology。此外，论文还使用了 Simple Knowledge Organization System（SKOS）来表示设备名称的层次结构。</li>
<li>results: 该论文通过与领域专家的合作，对大规模材料Characterization设备进行了研究和模型化，并使用 SHACL 语言来模型约束。这些约束可以帮助回答材料科学实验室设备的能力问题。<details>
<summary>Abstract</summary>
This paper introduces a new ontology for Materials Science Laboratory Equipment, termed MSLE. A fundamental issue with materials science laboratory (hereafter lab) equipment in the real world is that scientists work with various types of equipment with multiple specifications. For example, there are many electron microscopes with different parameters in chemical and physical labs. A critical development to unify the description is to build an equipment domain ontology as basic semantic knowledge and to guide the user to work with the equipment appropriately. Here, we propose to develop a consistent ontology for equipment, the MSLE ontology. In the MSLE, two main existing ontologies, the Semantic Sensor Network (SSN) and the Material Vocabulary (MatVoc), have been integrated into the MSLE core to build a coherent ontology. Since various acronyms and terms have been used for equipment, this paper proposes an approach to use a Simple Knowledge Organization System (SKOS) to represent the hierarchical structure of equipment terms. Equipment terms were collected in various languages and abbreviations and coded into the MSLE using the SKOS model. The ontology development was conducted in close collaboration with domain experts and focused on the large-scale devices for materials characterization available in our research group. Competency questions are expected to be addressed through the MSLE ontology. Constraints are modeled in the Shapes Query Language (SHACL); a prototype is shown and validated to show the value of the modeling constraints.
</details>
<details>
<summary>摘要</summary>
The MSLE ontology integrates two existing ontologies, the Semantic Sensor Network (SSN) and the Material Vocabulary (MatVoc), to create a coherent ontology. To deal with the various acronyms and terms used for equipment, the authors propose using a Simple Knowledge Organization System (SKOS) to represent the hierarchical structure of equipment terms.The ontology development was conducted in collaboration with domain experts and focused on large-scale devices for materials characterization available in the research group. The authors expect that the MSLE ontology will address competency questions and provide a standardized way of describing equipment. Constraints are modeled in the Shapes Query Language (SHACL) and a prototype is shown to demonstrate the value of the modeling constraints.Translation notes:* "Materials Science Laboratory Equipment" is translated as "材料科学实验室设备" (materials science experimental equipment)* "Semantic Sensor Network" is translated as "含义感知网络" (semantic sensor network)* "Material Vocabulary" is translated as "材料词汇" (material vocabulary)* "Simple Knowledge Organization System" is translated as "简单知识组织系统" (simple knowledge organization system)* "SHACL" is translated as "SHACL" (SHACL)Note: The translation is based on Simplified Chinese, which is the most widely used form of Chinese in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Measuring-Variety-Balance-and-Disparity-An-Analysis-of-Media-Coverage-of-the-2021-German-Federal-Election"><a href="#Measuring-Variety-Balance-and-Disparity-An-Analysis-of-Media-Coverage-of-the-2021-German-Federal-Election" class="headerlink" title="Measuring Variety, Balance, and Disparity: An Analysis of Media Coverage of the 2021 German Federal Election"></a>Measuring Variety, Balance, and Disparity: An Analysis of Media Coverage of the 2021 German Federal Election</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03531">http://arxiv.org/abs/2308.03531</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Färber, Jannik Schwade, Adam Jatowt</li>
<li>for: 本研究旨在探讨新闻文章中多样性的评估方法，以便防止过滤屏和促进公共讨论，特别是在选举前。</li>
<li>methods: 本研究提出了一种基于多维度的新闻文章多样性评估框架，考虑了个体、党派和话题的多样性。同时，研究人员还创建了一个Google Top Stories数据集，包括超过26,000个不同的标题和来自超过900家新闻机构的新闻文章，收集于2021年德国联邦选举前后的两周内。</li>
<li>results: 研究人员发现，使用更一般性的搜索关键词（例如“选举”）时，新闻文章的多样性较高。然而，使用更专门的搜索关键词（例如“教育”、“欧洲”、“气候保护”、“政府”）时，新闻文章的多样性在三个维度中较高，这反映了更加主观、专注的讨论。<details>
<summary>Abstract</summary>
Determining and measuring diversity in news articles is important for a number of reasons, including preventing filter bubbles and fueling public discourse, especially before elections. So far, the identification and analysis of diversity have been illuminated in a variety of ways, such as measuring the overlap of words or topics between news articles related to US elections. However, the question of how diversity in news articles can be measured holistically, i.e., with respect to (1) variety, (2) balance, and (3) disparity, considering individuals, parties, and topics, has not been addressed. In this paper, we present a framework for determining diversity in news articles according to these dimensions. Furthermore, we create and provide a dataset of Google Top Stories, encompassing more than 26,000 unique headlines from more than 900 news outlets collected within two weeks before and after the 2021 German federal election. While we observe high diversity for more general search terms (e.g., "election"), a range of search terms ("education," "Europe," "climate protection," "government") resulted in news articles with high diversity in two out of three dimensions. This reflects a more subjective, dedicated discussion on rather future-oriented topics.
</details>
<details>
<summary>摘要</summary>
确定和衡量新闻文章的多样性是重要的多种原因，包括避免 Filter Bubble 和促进公众讨论，特别是在选举前。迄今为止，多样性的识别和分析已经得到了多种方法的探讨，如在美国选举新闻文章中度量词语或话题之间的重叠。然而，如何全面衡量新闻文章的多样性，即以（1）多样性、（2）平衡和（3）差异为基础，考虑个体、党派和话题，还没有得到回答。在这篇论文中，我们提出了对多样性的定义和衡量方法。此外，我们还创建了一个 Google Top Stories 数据集，包括超过 26,000 个唯一的标题和来自超过 900 家新闻机构，在2021年德国联邦大选之前两周内收集到的。我们发现，使用更通用的搜索关键词（例如 "选举"）时，新闻文章的多样性很高。然而，使用不同的搜索关键词（例如 "教育", "欧洲", "气候保护", "政府"）时，新闻文章的多样性在三个维度中具有高度的多样性，这反映了一种更Subjective、专注于未来话题的讨论。
</details></li>
</ul>
<hr>
<h2 id="Deep-Feature-Learning-for-Wireless-Spectrum-Data"><a href="#Deep-Feature-Learning-for-Wireless-Spectrum-Data" class="headerlink" title="Deep Feature Learning for Wireless Spectrum Data"></a>Deep Feature Learning for Wireless Spectrum Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03530">http://arxiv.org/abs/2308.03530</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ljupcho Milosheski, Gregor Cerar, Blaž Bertalanič, Carolina Fortuna, Mihael Mohorčič</li>
<li>for: 本研究旨在无监督的情况下自动学习无线传输对 clustering 的特征表现。</li>
<li>methods: 本研究使用了 convolutional neural networks (CNN) 来自动学习对数据的凝固表现，并与基准方法（principal component analysis，PCA）进行比较。</li>
<li>results: 研究发现，自动学习的特征表现可以提取细化的传输对应形状，而基准方法仅能基于背景噪音来分类数据。<details>
<summary>Abstract</summary>
In recent years, the traditional feature engineering process for training machine learning models is being automated by the feature extraction layers integrated in deep learning architectures. In wireless networks, many studies were conducted in automatic learning of feature representations for domain-related challenges. However, most of the existing works assume some supervision along the learning process by using labels to optimize the model. In this paper, we investigate an approach to learning feature representations for wireless transmission clustering in a completely unsupervised manner, i.e. requiring no labels in the process. We propose a model based on convolutional neural networks that automatically learns a reduced dimensionality representation of the input data with 99.3% less components compared to a baseline principal component analysis (PCA). We show that the automatic representation learning is able to extract fine-grained clusters containing the shapes of the wireless transmission bursts, while the baseline enables only general separability of the data based on the background noise.
</details>
<details>
<summary>摘要</summary>
Our proposed model is based on convolutional neural networks (CNNs), which automatically learn a reduced dimensionality representation of the input data. We show that this approach achieves a 99.3% reduction in the number of components compared to a baseline principal component analysis (PCA) method. Furthermore, the automatic representation learning is able to extract fine-grained clusters containing the shapes of the wireless transmission bursts, while the baseline method only enables general separability of the data based on the background noise.
</details></li>
</ul>
<hr>
<h2 id="Exploring-ChatGPT’s-Empathic-Abilities"><a href="#Exploring-ChatGPT’s-Empathic-Abilities" class="headerlink" title="Exploring ChatGPT’s Empathic Abilities"></a>Exploring ChatGPT’s Empathic Abilities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03527">http://arxiv.org/abs/2308.03527</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kristina Schaaff, Caroline Reinig, Tim Schlippe</li>
<li>For: This study investigates the empathetic responses and emotional expressions of ChatGPT, a chatbot based on GPT-3.5.* Methods: The study evaluates ChatGPT’s empathy in three aspects: understanding and expressing emotions, parallel emotional response, and empathic personality.* Results: ChatGPT was able to correctly identify emotions and produce appropriate answers in 91.7% of cases, and reacted with a parallel emotion in 70.7% of conversations. The empathic capabilities of ChatGPT were found to be better than those of people with Asperger syndrome&#x2F;high-functioning autism, but still below the average of healthy humans.Here is the information in Simplified Chinese text:* 为：这项研究研究了基于GPT-3.5的ChatGPT chatbot的共鸣和情感表达。* 方法：研究对ChatGPT的共鸣进行三个方面的评估：理解和表达情感、同步情感反应和共鸣性格。* 结果：ChatGPT在91.7%的情况下正确地识别情感和生成相应的答案，在对话中与人类的情感同步达到70.7%。与阿斯伯格症&#x2F;高功能自闭症患者相比，ChatGPT的共鸣能力显示出改善，但仍然下于健康人群的平均水平。<details>
<summary>Abstract</summary>
Empathy is often understood as the ability to share and understand another individual's state of mind or emotion. With the increasing use of chatbots in various domains, e.g., children seeking help with homework, individuals looking for medical advice, and people using the chatbot as a daily source of everyday companionship, the importance of empathy in human-computer interaction has become more apparent. Therefore, our study investigates the extent to which ChatGPT based on GPT-3.5 can exhibit empathetic responses and emotional expressions. We analyzed the following three aspects: (1) understanding and expressing emotions, (2) parallel emotional response, and (3) empathic personality. Thus, we not only evaluate ChatGPT on various empathy aspects and compare it with human behavior but also show a possible way to analyze the empathy of chatbots in general. Our results show, that in 91.7% of the cases, ChatGPT was able to correctly identify emotions and produces appropriate answers. In conversations, ChatGPT reacted with a parallel emotion in 70.7% of cases. The empathic capabilities of ChatGPT were evaluated using a set of five questionnaires covering different aspects of empathy. Even though the results indicate that the empathic abilities of ChatGPT are still below the average of healthy humans, the scores are better than those of people who have been diagnosed with Asperger syndrome / high-functioning autism.
</details>
<details>
<summary>摘要</summary>
Empathy 常被理解为与别人分享和理解他们的情感或情绪的能力。随着虚拟助手在不同领域的使用，例如孩子们寻求家庭作业帮助、人们寻求医疗建议以及人们每天通过虚拟助手获得伴侣关系，人机交互中Empathy的重要性变得更加明显。因此，我们的研究探讨了基于GPT-3.5的ChatGPT是否能够表现出Empathy的响应和情感表达。我们分析了以下三个方面：（1）理解和表达情感，（2）并行情感响应，以及（3）Empathic Personality。因此，我们不仅评估ChatGPT在不同Empathy方面的表现，并与人类行为进行比较，还提供了分析虚拟助手 Empathy 的可能性。我们的结果显示，在91.7%的情况下，ChatGPT能正确地识别情感并提供相应的答案。在对话中，ChatGPT在70.7%的情况下表现出并行情感响应。虚拟助手Empathic能力被评估 using five 个问卷，涵盖不同方面的Empathy。尽管结果表明ChatGPT的Empathic能力仍然比健康人类的平均水平低，但得分仍高于被诊断为有Asperger症/高功能自闭症的人。
</details></li>
</ul>
<hr>
<h2 id="AlphaStar-Unplugged-Large-Scale-Offline-Reinforcement-Learning"><a href="#AlphaStar-Unplugged-Large-Scale-Offline-Reinforcement-Learning" class="headerlink" title="AlphaStar Unplugged: Large-Scale Offline Reinforcement Learning"></a>AlphaStar Unplugged: Large-Scale Offline Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03526">http://arxiv.org/abs/2308.03526</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michaël Mathieu, Sherjil Ozair, Srivatsan Srinivasan, Caglar Gulcehre, Shangtong Zhang, Ray Jiang, Tom Le Paine, Richard Powell, Konrad Żołna, Julian Schrittwieser, David Choi, Petko Georgiev, Daniel Toyama, Aja Huang, Roman Ring, Igor Babuschkin, Timo Ewalds, Mahyar Bordbar, Sarah Henderson, Sergio Gómez Colmenarejo, Aäron van den Oord, Wojciech Marian Czarnecki, Nando de Freitas, Oriol Vinyals</li>
<li>for: 这篇论文是为了提高offline学习RL算法的性能而写的。</li>
<li>methods: 该论文使用了大量的StarCraft II游戏数据，并提供了一个标准的API和评价协议。它还包括了一些基线代理，如行为做clone和离线变体的actor-critic和MuZero。</li>
<li>results: 该论文使用了仅做offline数据，提高了先前发表的AlphaStar行为做clone代理的状态。它实现了90%的胜率。<details>
<summary>Abstract</summary>
StarCraft II is one of the most challenging simulated reinforcement learning environments; it is partially observable, stochastic, multi-agent, and mastering StarCraft II requires strategic planning over long time horizons with real-time low-level execution. It also has an active professional competitive scene. StarCraft II is uniquely suited for advancing offline RL algorithms, both because of its challenging nature and because Blizzard has released a massive dataset of millions of StarCraft II games played by human players. This paper leverages that and establishes a benchmark, called AlphaStar Unplugged, introducing unprecedented challenges for offline reinforcement learning. We define a dataset (a subset of Blizzard's release), tools standardizing an API for machine learning methods, and an evaluation protocol. We also present baseline agents, including behavior cloning, offline variants of actor-critic and MuZero. We improve the state of the art of agents using only offline data, and we achieve 90% win rate against previously published AlphaStar behavior cloning agent.
</details>
<details>
<summary>摘要</summary>
星际II是一个非常具有挑战性的模拟强化学环境之一，它是部分可见、随机、多个智能体，需要在长时间 horizon 上进行策略规划，同时在实时低级别执行。它还拥有活跃的职业竞赛场景。由于星际II的挑战性和Blizzard公司发布了数百万场星际II游戏记录，因此这个纸 lái 利用这些数据，建立了一个名为AlphaStar Unplugged的标准。我们定义了一个子集（Blizzard发布的 dataset）、工具和标准化 API  для机器学习方法，以及评估协议。我们还提供了基线代理，包括行为做参数和离线变体的actor-critic和MuZero。我们使用仅基于离线数据进行代理，并达到90%的胜率，比之前发布的AlphaStar行为做参数代理更高。
</details></li>
</ul>
<hr>
<h2 id="Vocab-Expander-A-System-for-Creating-Domain-Specific-Vocabularies-Based-on-Word-Embeddings"><a href="#Vocab-Expander-A-System-for-Creating-Domain-Specific-Vocabularies-Based-on-Word-Embeddings" class="headerlink" title="Vocab-Expander: A System for Creating Domain-Specific Vocabularies Based on Word Embeddings"></a>Vocab-Expander: A System for Creating Domain-Specific Vocabularies Based on Word Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03519">http://arxiv.org/abs/2308.03519</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Färber, Nicholas Popovic</li>
<li>for: 该论文旨在提供一种在线工具，帮助用户（如技术搜寻者）创建和扩展自己的领域词汇。</li>
<li>methods: 该工具使用了当前最佳的词嵌入技术 ensemble，基于网络文本和 ConceptNet 常识知识库，为已给定的词语提供相关的词语建议。</li>
<li>results: 该系统具有易用的界面，允许用户快速确认或拒绝词语建议。 Vocab-Expander 可以满足多种用例，如提高技术和创新管理中的概念基于搜索、组织内部或跨学科项目的沟通和合作，以及特定课程教育中的词汇创建。<details>
<summary>Abstract</summary>
In this paper, we propose Vocab-Expander at https://vocab-expander.com, an online tool that enables end-users (e.g., technology scouts) to create and expand a vocabulary of their domain of interest. It utilizes an ensemble of state-of-the-art word embedding techniques based on web text and ConceptNet, a common-sense knowledge base, to suggest related terms for already given terms. The system has an easy-to-use interface that allows users to quickly confirm or reject term suggestions. Vocab-Expander offers a variety of potential use cases, such as improving concept-based information retrieval in technology and innovation management, enhancing communication and collaboration within organizations or interdisciplinary projects, and creating vocabularies for specific courses in education.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了Vocab-Expander，一个在线工具，它允许用户（例如技术搜寻专家）根据他们的领域兴趣创建和扩展词汇表。它利用了当前最佳的词嵌入技术，基于网络文本和ConceptNet，一个常识知识库，提供相关的词语建议。用户可以通过一个简单易用的界面来快速确认或拒绝词语建议。Vocab-Expander具有多种可能的用 caso，例如在科技和创新管理中提高基于概念的搜索，在组织内部或跨学科项目中增强交流和合作，以及为特定课程创建专门的词汇表。
</details></li>
</ul>
<hr>
<h2 id="Balanced-Face-Dataset-Guiding-StyleGAN-to-Generate-Labeled-Synthetic-Face-Image-Dataset-for-Underrepresented-Group"><a href="#Balanced-Face-Dataset-Guiding-StyleGAN-to-Generate-Labeled-Synthetic-Face-Image-Dataset-for-Underrepresented-Group" class="headerlink" title="Balanced Face Dataset: Guiding StyleGAN to Generate Labeled Synthetic Face Image Dataset for Underrepresented Group"></a>Balanced Face Dataset: Guiding StyleGAN to Generate Labeled Synthetic Face Image Dataset for Underrepresented Group</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03495">http://arxiv.org/abs/2308.03495</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kidist Amde Mekonnen</li>
<li>for: 这个研究的目的是生成一个可靠的人脸图像数据集，以便减少人工标注成本和偏见。</li>
<li>methods: 这个研究使用了StyleGAN模型来生成人脸图像数据集，并控制生成过程以确保数据集具有良好的分布和代表性。</li>
<li>results: 研究表明，使用StyleGAN模型生成的人脸图像数据集具有良好的代表性和准确性，可以用于各种下游任务。<details>
<summary>Abstract</summary>
For a machine learning model to generalize effectively to unseen data within a particular problem domain, it is well-understood that the data needs to be of sufficient size and representative of real-world scenarios. Nonetheless, real-world datasets frequently have overrepresented and underrepresented groups. One solution to mitigate bias in machine learning is to leverage a diverse and representative dataset. Training a model on a dataset that covers all demographics is crucial to reducing bias in machine learning. However, collecting and labeling large-scale datasets has been challenging, prompting the use of synthetic data generation and active labeling to decrease the costs of manual labeling. The focus of this study was to generate a robust face image dataset using the StyleGAN model. In order to achieve a balanced distribution of the dataset among different demographic groups, a synthetic dataset was created by controlling the generation process of StyleGaN and annotated for different downstream tasks.
</details>
<details>
<summary>摘要</summary>
为了让机器学习模型在未经见过数据中准确泛化，需要确保数据集具有足够的大小和 represencing 实际世界情况。然而，实际世界数据集往往存在过度和不足的分布。为了减少机器学习中的偏见，可以利用多样化和表示性的数据集。在这种情况下，通过控制StyleGAN生成过程，生成了一个多样化的面像数据集，并对不同的下游任务进行了标注。这些步骤可以帮助生成一个 Robust 的面像数据集，以减少偏见。
</details></li>
</ul>
<hr>
<h2 id="No-Length-Left-Behind-Enhancing-Knowledge-Tracing-for-Modeling-Sequences-of-Excessive-or-Insufficient-Lengths"><a href="#No-Length-Left-Behind-Enhancing-Knowledge-Tracing-for-Modeling-Sequences-of-Excessive-or-Insufficient-Lengths" class="headerlink" title="No Length Left Behind: Enhancing Knowledge Tracing for Modeling Sequences of Excessive or Insufficient Lengths"></a>No Length Left Behind: Enhancing Knowledge Tracing for Modeling Sequences of Excessive or Insufficient Lengths</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03488">http://arxiv.org/abs/2308.03488</a></li>
<li>repo_url: None</li>
<li>paper_authors: Moyu Zhang, Xinning Zhu, Chunhong Zhang, Feng Pan, Wenchen Qian, Hui Zhao</li>
<li>for: 预测学生的练习响应基于其历史问答行为。</li>
<li>methods: 提出了一种模型 called Sequence-Flexible Knowledge Tracing (SFKT)，用于解决现有方法中 sequences 的较长或较短问题。</li>
<li>results: 模型可以更好地捕捉学生的完整历史练习行为，并且可以避免过拟合问题。<details>
<summary>Abstract</summary>
Knowledge tracing (KT) aims to predict students' responses to practices based on their historical question-answering behaviors. However, most current KT methods focus on improving overall AUC, leaving ample room for optimization in modeling sequences of excessive or insufficient lengths. As sequences get longer, computational costs will increase exponentially. Therefore, KT methods usually truncate sequences to an acceptable length, which makes it difficult for models on online service systems to capture complete historical practice behaviors of students with too long sequences. Conversely, modeling students with short practice sequences using most KT methods may result in overfitting due to limited observation samples. To address the above limitations, we propose a model called Sequence-Flexible Knowledge Tracing (SFKT).
</details>
<details>
<summary>摘要</summary>
知识追踪（KT）目标是预测学生对实践的回答。然而，现有的大多数KT方法都是通过提高总的准确率来优化，忽略了序列长度过长或短的问题。随着序列长度增加，计算成本会 exponential 增加。因此，KT方法通常会舍弃序列，使得在在线服务系统上的模型难以捕捉学生的完整历史实践行为。相反，使用大多数KT方法模型学生短实践序列可能会导致过拟合，因为有限的观察样本。为解决上述限制，我们提出了一种模型 calledSequence-Flexible Knowledge Tracing（SFKT）。
</details></li>
</ul>
<hr>
<h2 id="CIRO-COVID-19-infection-risk-ontology"><a href="#CIRO-COVID-19-infection-risk-ontology" class="headerlink" title="CIRO: COVID-19 infection risk ontology"></a>CIRO: COVID-19 infection risk ontology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09719">http://arxiv.org/abs/2308.09719</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/plod-info/plod">https://github.com/plod-info/plod</a></li>
<li>paper_authors: Shusaku Egami, Yasunori Yamamoto, Ikki Ohmukai, Takashi Okumura</li>
<li>For: The paper aims to automate the assessment of COVID-19 infection risks for individuals based on the Japanese government’s formulation of infection risks.* Methods: The paper uses an ontology called COVID-19 Infection Risk Ontology (CIRO) and the Resource Description Framework (RDF) and SPARQL queries to automate the assessment of infection risks.* Results: The knowledge graph built using CIRO and RDF&#x2F;SPARQL queries can infer the infection risks formulated by the Japanese government, and the reasoning experiments demonstrated the usefulness of the knowledge processing. However, some issues were identified for further deployment.Here’s the same information in Simplified Chinese:* For: 本研究旨在自动评估 COVID-19 感染风险，基于日本政府的感染风险形态。* Methods: 本研究使用 COVID-19 感染风险 ontology（CIRO）和资源描述框架（RDF）和 SPARQL 查询自动评估感染风险。* Results: 使用 CIRO 和 RDF&#x2F;SPARQL 查询构建的知识图可以推理出日本政府的感染风险形态，并且理解实验表明了知识处理的有用性。但是，进一步部署还有一些问题需要解决。<details>
<summary>Abstract</summary>
Public health authorities perform contact tracing for highly contagious agents to identify close contacts with the infected cases. However, during the pandemic caused by coronavirus disease 2019 (COVID-19), this operation was not employed in countries with high patient volumes. Meanwhile, the Japanese government conducted this operation, thereby contributing to the control of infections, at the cost of arduous manual labor by public health officials. To ease the burden of the officials, this study attempted to automate the assessment of each person's infection risk through an ontology, called COVID-19 Infection Risk Ontology (CIRO). This ontology expresses infection risks of COVID-19 formulated by the Japanese government, toward automated assessment of infection risks of individuals, using Resource Description Framework (RDF) and SPARQL (SPARQL Protocol and RDF Query Language) queries. For evaluation, we demonstrated that the knowledge graph built could infer the risks, formulated by the government. Moreover, we conducted reasoning experiments to analyze the computational efficiency. The experiments demonstrated usefulness of the knowledge processing, and identified issues left for deployment.
</details>
<details>
<summary>摘要</summary>
公共健康当局在高度传染病毒病例中进行联系跟踪，以确定感染者的近距离接触者。然而，在2019冠状病毒疫情中，这种操作在高病人量国家没有实施。而日本政府则进行了这种操作，从而对感染的控制做出了贡献，但是需要公共卫生官员进行劳动密集的手动劳动。为了减轻官员的负担，本研究尝试自动评估每个人的感染风险，通过叫做COVID-19感染风险 ontology（CIRO）。这个 ontology 表达了由日本政府制定的感染风险形式，并使用 Resource Description Framework（RDF）和 SPARQL（SPARQL Protocol and RDF Query Language）查询来自动评估个人的感染风险。为了评估，我们展示了知识图建立的可能性，并进行了逻辑实验来分析计算效率。实验表明了知识处理的有用性，并确定了部署中的问题。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Physical-World-Adversarial-Robustness-of-Vehicle-Detection"><a href="#Exploring-the-Physical-World-Adversarial-Robustness-of-Vehicle-Detection" class="headerlink" title="Exploring the Physical World Adversarial Robustness of Vehicle Detection"></a>Exploring the Physical World Adversarial Robustness of Vehicle Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03476">http://arxiv.org/abs/2308.03476</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Jiang, Tianyuan Zhang, Shuangcheng Liu, Weiyu Ji, Zichao Zhang, Gang Xiao</li>
<li>for: 本研究旨在评估检测模型在真实世界中的 Robustness，并提出一种基于 CARLA  simulator 的快速数据生成管道，以生成 Discrete and Continuous Instant-level (DCI) 数据集。</li>
<li>methods: 本研究使用了 CARLA  simulator 生成快速数据，并在三种检测模型和三种物理攻击下进行了全面的实验。</li>
<li>results: 研究发现，Yolo v6 模型在攻击下表现出色，其 AP 值只减少了 6.59%，而 ASA 攻击则导致了 AP 值减少了 14.51%，远超其他算法的影响。此外，研究还发现，静止场景的识别 AP 值较高，并且不同天气条件下的结果相对稳定。<details>
<summary>Abstract</summary>
Adversarial attacks can compromise the robustness of real-world detection models. However, evaluating these models under real-world conditions poses challenges due to resource-intensive experiments. Virtual simulations offer an alternative, but the absence of standardized benchmarks hampers progress. Addressing this, we propose an innovative instant-level data generation pipeline using the CARLA simulator. Through this pipeline, we establish the Discrete and Continuous Instant-level (DCI) dataset, enabling comprehensive experiments involving three detection models and three physical adversarial attacks. Our findings highlight diverse model performances under adversarial conditions. Yolo v6 demonstrates remarkable resilience, experiencing just a marginal 6.59% average drop in average precision (AP). In contrast, the ASA attack yields a substantial 14.51% average AP reduction, twice the effect of other algorithms. We also note that static scenes yield higher recognition AP values, and outcomes remain relatively consistent across varying weather conditions. Intriguingly, our study suggests that advancements in adversarial attack algorithms may be approaching its ``limitation''.In summary, our work underscores the significance of adversarial attacks in real-world contexts and introduces the DCI dataset as a versatile benchmark. Our findings provide valuable insights for enhancing the robustness of detection models and offer guidance for future research endeavors in the realm of adversarial attacks.
</details>
<details>
<summary>摘要</summary>
实际世界中的检测模型可能会受到敌意攻击的威胁。然而，在实际情况下进行测试具有资源占用的问题。虚拟 simulate 提供了一种 alternaative，但缺乏标准化的 benchmark 使得进步受阻。为了解决这个问题，我们提出了一种创新的实时数据生成管道，使用 CARLA 模拟器。通过这个管道，我们建立了精度和连续实时（DCI）数据集，允许对三种检测模型和三种物理敌意攻击进行全面的实验。我们的发现表明，Yolo v6 表现出色，只有 marginal 6.59% 的平均精度下降（AP）。与此同时，ASA 攻击导致了 substatial 14.51% 的平均精度下降，比其他算法多出一半。我们还发现，静止场景的识别精度较高，并且结果在不同的天气条件下呈 relativelly 一致。另外，我们的研究表明，敌意攻击算法的进步可能会达到其“限制”。总之，我们的工作强调了在实际世界中的敌意攻击的重要性，并将 DCI 数据集作为一个多样化的标准启用。我们的发现为检测模型的Robustness带来了有价值的指导，并为未来对敌意攻击算法的研究提供了新的思路。
</details></li>
</ul>
<hr>
<h2 id="Wide-Gaps-and-Clustering-Axioms"><a href="#Wide-Gaps-and-Clustering-Axioms" class="headerlink" title="Wide Gaps and Clustering Axioms"></a>Wide Gaps and Clustering Axioms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03464">http://arxiv.org/abs/2308.03464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mieczysław A. Kłopotek</li>
<li>for: The paper is written to address the issue of k-means algorithm producing clusterings that violate our expectations with respect to high&#x2F;low similarity&#x2F;density, and to reconcile k-means with Kleinberg’s axiomatic framework in Euclidean and non-Euclidean settings.</li>
<li>methods: The paper introduces two new clusterability properties, variational k-separability and residual k-separability, and proposes extensions of k-means algorithm that fit approximately the Kleinberg’s richness axiom.</li>
<li>results: The paper demonstrates that the proposed extensions of k-means algorithm fit the Kleinberg’s consistency axiom in both Euclidean and non-Euclidean settings, and provides a method for constructing datasets for testing purposes of algorithms optimizing k-means cost function. Additionally, the paper provides practical contributions to the field of clusterability theory and the theory of axiomatic frameworks of clustering.<details>
<summary>Abstract</summary>
The widely applied k-means algorithm produces clusterings that violate our expectations with respect to high/low similarity/density and is in conflict with Kleinberg's axiomatic system for distance based clustering algorithms that formalizes those expectations in a natural way. k-means violates in particular the consistency axiom. We hypothesise that this clash is due to the not explicated expectation that the data themselves should have the property of being clusterable in order to expect the algorithm clustering hem to fit a clustering axiomatic system. To demonstrate this, we introduce two new clusterability properties, variational k-separability and residual k-separability and show that then the Kleinberg's consistency axiom holds for k-means operating in the Euclidean or non-Euclidean space. Furthermore, we propose extensions of k-means algorithm that fit approximately the Kleinberg's richness axiom that does not hold for k-means. In this way, we reconcile k-means with Kleinberg's axiomatic framework in Euclidean and non-Euclidean settings. Besides contribution to the theory of axiomatic frameworks of clustering and for clusterability theory, practical contribution is the possibility to construct {datasets for testing purposes of algorithms optimizing k-means cost function. This includes a method of construction of {clusterable data with known in advance global optimum.
</details>
<details>
<summary>摘要</summary>
广泛应用的k-means算法对我们的预期产生了冲突，尤其是在高/低相似性和密度方面。k-means与克莱因堡格的axiomaatic系统不符合，这是因为没有明确地预期资料本身应有聚集性的假设。为了证明这一点，我们引入了两个新的聚集性特性：variational k-separability和residual k-separability。我们显示了这两个特性使得克莱因堡格的一致性axioma成立，并且提出了基于非欧几何空间的延伸算法，可以近似地满足克莱因堡格的丰富性axioma。这样，我们可以将k-means算法与克莱因堡格的axiomaatic framework相符合，并且在欧几何和非欧几何空间中实现。此外，我们可以建立具有已知全球最佳解的聚集数据集，用于测试 clustering 算法的优化。Note: Simplified Chinese is used in mainland China and Singapore, while Traditional Chinese is used in Taiwan, Hong Kong, and Macau.
</details></li>
</ul>
<hr>
<h2 id="High-Resolution-Cranial-Defect-Reconstruction-by-Iterative-Low-Resolution-Point-Cloud-Completion-Transformers"><a href="#High-Resolution-Cranial-Defect-Reconstruction-by-Iterative-Low-Resolution-Point-Cloud-Completion-Transformers" class="headerlink" title="High-Resolution Cranial Defect Reconstruction by Iterative, Low-Resolution, Point Cloud Completion Transformers"></a>High-Resolution Cranial Defect Reconstruction by Iterative, Low-Resolution, Point Cloud Completion Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03813">http://arxiv.org/abs/2308.03813</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/MWod/DeepImplant_MICCAI_2023">https://github.com/MWod/DeepImplant_MICCAI_2023</a></li>
<li>paper_authors: Marek Wodzinski, Mateusz Daniol, Daria Hemmerling, Miroslaw Socha</li>
<li>for: This paper aims to develop an automatic, dedicated system for personalized cranial reconstruction, to increase the availability of cranial implants and reduce the time and cost of manual design.</li>
<li>methods: The proposed method reformulates the problem as a point cloud completion task and uses an iterative, transformer-based approach to reconstruct the cranial defect at any resolution, while being fast and resource-efficient during training and inference.</li>
<li>results: The proposed method shows superior performance compared to state-of-the-art volumetric approaches in terms of GPU memory consumption, while maintaining high-quality of the reconstructed defects.<details>
<summary>Abstract</summary>
Each year thousands of people suffer from various types of cranial injuries and require personalized implants whose manual design is expensive and time-consuming. Therefore, an automatic, dedicated system to increase the availability of personalized cranial reconstruction is highly desirable. The problem of the automatic cranial defect reconstruction can be formulated as the shape completion task and solved using dedicated deep networks. Currently, the most common approach is to use the volumetric representation and apply deep networks dedicated to image segmentation. However, this approach has several limitations and does not scale well into high-resolution volumes, nor takes into account the data sparsity. In our work, we reformulate the problem into a point cloud completion task. We propose an iterative, transformer-based method to reconstruct the cranial defect at any resolution while also being fast and resource-efficient during training and inference. We compare the proposed methods to the state-of-the-art volumetric approaches and show superior performance in terms of GPU memory consumption while maintaining high-quality of the reconstructed defects.
</details>
<details>
<summary>摘要</summary>
每年千计人们受到不同类型的头部伤害，需要个性化设备 whose 手动设计是昂贵的时间消耗。因此，一个自动化、专门的系统可以大大提高个性化头部重建的可用性。头部缺陷重建问题可以 reformulated 为形状完成任务，并使用专门的深度网络解决。当前最常见的方法是使用体积表示，并应用深度网络进行图像分割。然而，这种方法有一些局限性，不能扩展到高分辨率的体积，也不考虑数据稀疏性。在我们的工作中，我们将问题重新定义为点云完成任务。我们提出了一种迭代的、基于变换器的方法，可以在任何分辨率下重建头部缺陷，并且在训练和推理过程中具有快速和资源高效的特点。我们与状态的艺术方法进行比较，并显示我们的方法在 GPU 内存占用量方面具有更高的性能，同时保持重建的缺陷质量高。
</details></li>
</ul>
<hr>
<h2 id="Intelligence-Endogenous-Management-Platform-for-Computing-and-Network-Convergence"><a href="#Intelligence-Endogenous-Management-Platform-for-Computing-and-Network-Convergence" class="headerlink" title="Intelligence-Endogenous Management Platform for Computing and Network Convergence"></a>Intelligence-Endogenous Management Platform for Computing and Network Convergence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03450">http://arxiv.org/abs/2308.03450</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zicong Hong, Xiaoyu Qiu, Jian Lin, Wuhui Chen, Yue Yu, Hui Wang, Song Guo, Wen Gao<br>for:This paper aims to present a concept for an intelligence-endogenous management platform for Computing and Network Convergence (CNC) called “CNC brain” based on artificial intelligence technologies.methods:The proposed CNC brain platform uses four key building blocks: perception, scheduling, adaptation, and governance, to efficiently and automatically match supply and demand with high heterogeneity in a CNC throughout its life cycle.results:The proposed method is evaluated on a CNC testbed that integrates two open-source frameworks (OpenFaas and Kubernetes) and a real-world business dataset provided by Microsoft Azure, and the evaluation results show that the proposed method is effective in terms of resource utilization and performance.<details>
<summary>Abstract</summary>
Massive emerging applications are driving demand for the ubiquitous deployment of computing power today. This trend not only spurs the recent popularity of the \emph{Computing and Network Convergence} (CNC), but also introduces an urgent need for the intelligentization of a management platform to coordinate changing resources and tasks in the CNC. Therefore, in this article, we present the concept of an intelligence-endogenous management platform for CNCs called \emph{CNC brain} based on artificial intelligence technologies. It aims at efficiently and automatically matching the supply and demand with high heterogeneity in a CNC via four key building blocks, i.e., perception, scheduling, adaptation, and governance, throughout the CNC's life cycle. Their functionalities, goals, and challenges are presented. To examine the effectiveness of the proposed concept and framework, we also implement a prototype for the CNC brain based on a deep reinforcement learning technology. Also, it is evaluated on a CNC testbed that integrates two open-source and popular frameworks (OpenFaas and Kubernetes) and a real-world business dataset provided by Microsoft Azure. The evaluation results prove the proposed method's effectiveness in terms of resource utilization and performance. Finally, we highlight the future research directions of the CNC brain.
</details>
<details>
<summary>摘要</summary>
巨大的应用需求今天启动了计算力的无限扩展。这种趋势不仅推动了最近的计算和网络融合（CNC）的流行，还提出了智能化管理平台的急需，以协调CNC中的变化资源和任务。因此，在本文中，我们提出了基于人工智能技术的CNC脑（CNC brain）智能化管理平台的概念，旨在高效自动匹配CNC中的供应和需求，并在CNC生命周期中实现四个关键组件的功能：感知、调度、适应和治理。我们还实现了基于深度强化学习技术的CNC脑原型，并在一个包含OpenFaas和Kubernetes两个开源框架以及Microsoft Azure提供的实际业务数据的CNC测试环境中进行了评估。评估结果表明，提出的方法能够提高资源利用率和性能。最后，我们还概述了CNC脑的未来研究方向。
</details></li>
</ul>
<hr>
<h2 id="Biomedical-Knowledge-Graph-Embeddings-with-Negative-Statements"><a href="#Biomedical-Knowledge-Graph-Embeddings-with-Negative-Statements" class="headerlink" title="Biomedical Knowledge Graph Embeddings with Negative Statements"></a>Biomedical Knowledge Graph Embeddings with Negative Statements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03447">http://arxiv.org/abs/2308.03447</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liseda-lab/truewalks">https://github.com/liseda-lab/truewalks</a></li>
<li>paper_authors: Rita T. Sousa, Sara Silva, Heiko Paulheim, Catia Pesquita</li>
<li>for: 本研究旨在提高知识 graphs embedding approaches的准确性，通过正确地包含负性陈述来提高实体表示的准确性。</li>
<li>methods: 我们提出了一种新的方法，即TrueWalks，它可以在知识 graphs embedding learning过程中正确地包含负性陈述，并且能够考虑负性陈述在ontology层次上的语义意义。</li>
<li>results: 我们在 ontology-rich生物医学知识 graphs 上进行了两种不同的预测任务，并得到了与现有benchmarks进行比较的好效果。<details>
<summary>Abstract</summary>
A knowledge graph is a powerful representation of real-world entities and their relations. The vast majority of these relations are defined as positive statements, but the importance of negative statements is increasingly recognized, especially under an Open World Assumption. Explicitly considering negative statements has been shown to improve performance on tasks such as entity summarization and question answering or domain-specific tasks such as protein function prediction. However, no attention has been given to the exploration of negative statements by knowledge graph embedding approaches despite the potential of negative statements to produce more accurate representations of entities in a knowledge graph.   We propose a novel approach, TrueWalks, to incorporate negative statements into the knowledge graph representation learning process. In particular, we present a novel walk-generation method that is able to not only differentiate between positive and negative statements but also take into account the semantic implications of negation in ontology-rich knowledge graphs. This is of particular importance for applications in the biomedical domain, where the inadequacy of embedding approaches regarding negative statements at the ontology level has been identified as a crucial limitation.   We evaluate TrueWalks in ontology-rich biomedical knowledge graphs in two different predictive tasks based on KG embeddings: protein-protein interaction prediction and gene-disease association prediction. We conduct an extensive analysis over established benchmarks and demonstrate that our method is able to improve the performance of knowledge graph embeddings on all tasks.
</details>
<details>
<summary>摘要</summary>
一个知识图是一种强大的实体和关系表示方式。大多数关系被定义为正式声明，但对开放世界假设下，正式声明的重要性得到了更多的认可，特别是在实体概括和问答 зада务中。明确考虑正式声明可以提高实体概括和问答 зада务的性能。然而，知识图嵌入方法对负声明的探索没有得到过关注，尽管负声明可以生成更准确的实体表示。我们提出了一种新的方法，真实步行（TrueWalks），用于在知识图表示学习过程中包含负声明。特别是，我们提出了一种新的步行生成方法，可以不仅区分正式声明和负声明，还能够考虑ontology层次上的否定 semantically。这对生物医学领域的应用非常重要，因为嵌入方法在ontology层次上对负声明的不足已被证明为是关键的限制。我们在生物医学知识图中进行了两种不同的预测任务基于KG嵌入：蛋白质-蛋白质交互预测和基因-疾病相关性预测。我们对已有的标准底层进行了广泛的分析，并证明了我们的方法可以在所有任务中提高知识图嵌入的性能。
</details></li>
</ul>
<hr>
<h2 id="Doubly-Robust-Estimator-for-Off-Policy-Evaluation-with-Large-Action-Spaces"><a href="#Doubly-Robust-Estimator-for-Off-Policy-Evaluation-with-Large-Action-Spaces" class="headerlink" title="Doubly Robust Estimator for Off-Policy Evaluation with Large Action Spaces"></a>Doubly Robust Estimator for Off-Policy Evaluation with Large Action Spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03443">http://arxiv.org/abs/2308.03443</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tatsu432/DR-estimator-OPE-large-action">https://github.com/tatsu432/DR-estimator-OPE-large-action</a></li>
<li>paper_authors: Tatsuhiro Shimizu, Laura Forastiere</li>
<li>For: The paper focuses on Off-Policy Evaluation (OPE) in contextual bandit settings with large action spaces, and aims to develop a more accurate and efficient estimator.* Methods: The paper proposes a new estimator called Marginalized Doubly Robust (MDR) estimator, which combines the strengths of Marginalized Inverse Propensity Scoring (MIPS) and doubly robust estimation. The MDR estimator uses embeddings of actions to mitigate the estimator’s variance and improve accuracy.* Results: The paper shows that the proposed MDR estimator is unbiased under weaker assumptions than MIPS and maintains variance reduction against IPS, which was the main advantage of MIPS. The empirical experiment verifies the supremacy of MDR against existing estimators.Here are the three key points in Simplified Chinese:* For: 这个论文关注在Contextual Bandit Setting下的Off-Policy Evaluation (OPE)问题上, 并提出了一种更加准确和高效的估计方法。* Methods: 论文提出了一种新的估计方法called Marginalized Doubly Robust (MDR) estimator, 它结合了Marginalized Inverse Propensity Scoring (MIPS)和 doubly robust估计的优点。 MDR estimator使用行动的嵌入来降低估计变iance和提高准确性。* Results: 论文表明，提出的MDR估计方法在较弱的假设下是不偏的, 并且保持了对IPS的变iance减少, 这是MIPS的主要优点。 验证性实验证明了MDR估计方法在现有估计方法之上的优越性。<details>
<summary>Abstract</summary>
We study Off-Policy Evaluation (OPE) in contextual bandit settings with large action spaces. The benchmark estimators suffer from severe bias and variance tradeoffs. Parametric approaches suffer from bias due to difficulty specifying the correct model, whereas ones with importance weight suffer from variance. To overcome these limitations, Marginalized Inverse Propensity Scoring (MIPS) was proposed to mitigate the estimator's variance via embeddings of an action. To make the estimator more accurate, we propose the doubly robust estimator of MIPS called the Marginalized Doubly Robust (MDR) estimator. Theoretical analysis shows that the proposed estimator is unbiased under weaker assumptions than MIPS while maintaining variance reduction against IPS, which was the main advantage of MIPS. The empirical experiment verifies the supremacy of MDR against existing estimators.
</details>
<details>
<summary>摘要</summary>
我们研究偏离策略评估（OPE）在具有大型动作空间的上下文ual bandit中。参考估计器受到严重的偏误和方差交易限制。parametric方法受到模型难以确定的偏误，而具有重要性权重的方法则受到方差的限制。为了超越这些局限性，我们提出了折衔embeddings的Marginalized Inverse Propensity Scoring（MIPS），以减少估计器的方差。为了使估计器更加准确，我们提出了MIPS的双重Robust（MDR）估计器。理论分析表明，我们的估计器具有较弱的假设下的无偏性，而且与IPS相比，具有更好的方差减少效果。实验证明了MDR的超越性。
</details></li>
</ul>
<hr>
<h2 id="RCMHA-Relative-Convolutional-Multi-Head-Attention-for-Natural-Language-Modelling"><a href="#RCMHA-Relative-Convolutional-Multi-Head-Attention-for-Natural-Language-Modelling" class="headerlink" title="RCMHA: Relative Convolutional Multi-Head Attention for Natural Language Modelling"></a>RCMHA: Relative Convolutional Multi-Head Attention for Natural Language Modelling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03429">http://arxiv.org/abs/2308.03429</a></li>
<li>repo_url: None</li>
<li>paper_authors: Herman Sugiharto, Aradea, Husni Mubarok</li>
<li>for: 本研究旨在改进多头注意力（MHA）模型，以提高准确率和减少内存使用量。</li>
<li>methods: 本研究采用了相对位置编码和深度分割卷积层，并对输入嵌入进行深度分割卷积，以提高准确率和减少内存使用量。</li>
<li>results: 实验结果表明，提出的RCMHA模型在比较于其他注意力模型（MHA、MDHA、RMHA）时，具有更高的准确率（0.572），并且内存使用量相对较低（2.98 GB）。<details>
<summary>Abstract</summary>
The Attention module finds common usage in language modeling, presenting distinct challenges within the broader scope of Natural Language Processing. Multi-Head Attention (MHA) employs an absolute positional encoding, which imposes limitations on token length and entails substantial memory consumption during the processing of embedded inputs. The current remedy proposed by researchers involves the utilization of relative positional encoding, similar to the approach adopted in Transformer-XL or Relative Multi-Head Attention (RMHA), albeit the employed architecture consumes considerable memory resources. To address these challenges, this study endeavors to refine MHA, leveraging relative positional encoding in conjunction with the Depth-Wise Convolutional Layer architecture, which promises heightened accuracy coupled with minimized memory usage. The proposed RCMHA framework entails the modification of two integral components: firstly, the application of the Depth-Wise Convolutional Layer to the input embedding, encompassing Query, Key, and Value parameters; secondly, the incorporation of Relative Positional Encoding into the attention scoring phase, harmoniously integrated with Scaled Dot-Product Attention. Empirical experiments underscore the advantages of RCMHA, wherein it exhibits superior accuracy, boasting a score of 0.572 in comparison to alternative attention modules such as MHA, Multi-DConv-Head Attention (MDHA), and RMHA. Concerning memory utilization, RMHA emerges as the most frugal, demonstrating an average consumption of 2.98 GB, surpassing RMHA which necessitates 3.5 GB.
</details>
<details>
<summary>摘要</summary>
研究人员通常使用注意模块在自然语言处理中，但它们具有一些特殊的挑战。多头注意（MHA）使用绝对位置编码，这限制了单个符号的长度和需要大量内存进行融合输入的处理。为了解决这些挑战，研究人员提出了使用相对位置编码的方法，类似于Transformer-XL或相对多头注意（RMHA）的方法，但是使用的架构占用了大量内存资源。为了解决这些问题，本研究尝试更新MHA，使用相对位置编码和深度wise卷积层架构，以提高准确率并降低内存使用量。提案的RCMHA框架包括对输入嵌入进行深度wise卷积层应用，以及在注意得分阶段进行相对位置编码的整合，和尺度积分注意。实验证明RCMHA具有更高的准确率，其中在比较注意模块时，RCMHA的得分为0.572，比MHA、Multi-DConv-Head Attention（MDHA）和RMHA更高。此外，RCMHA的内存使用量较低，只需2.98 GB，超过RMHA的3.5 GB。
</details></li>
</ul>
<hr>
<h2 id="TPTU-Task-Planning-and-Tool-Usage-of-Large-Language-Model-based-AI-Agents"><a href="#TPTU-Task-Planning-and-Tool-Usage-of-Large-Language-Model-based-AI-Agents" class="headerlink" title="TPTU: Task Planning and Tool Usage of Large Language Model-based AI Agents"></a>TPTU: Task Planning and Tool Usage of Large Language Model-based AI Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03427">http://arxiv.org/abs/2308.03427</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing Du, Shiwei Shi, Hangyu Mao, Xingyu Zeng, Rui Zhao</li>
<li>for: 本研究旨在探讨Large Language Models（LLMs）在不同应用领域中的应用潜力，以及如何使用LLMs来解决复杂的任务。</li>
<li>methods: 本研究提出了一种结构化的框架，用于LLMs基本的AI代理人，并讨论了在执行复杂任务时需要的关键能力。在这个框架中，我们设计了两种不同的代理人（即一步代理人和序列代理人），用于执行推理过程。</li>
<li>results: 我们通过使用不同的LLMs来实现这个框架，并评估了这些模型在 typical tasks 上的任务规划和工具使用（TPTU）能力。我们发现LLMs在执行复杂任务时具有潜在的潜力，但还有一些挑战和需要进一步研究的领域。<details>
<summary>Abstract</summary>
With recent advancements in natural language processing, Large Language Models (LLMs) have emerged as powerful tools for various real-world applications. Despite their prowess, the intrinsic generative abilities of LLMs may prove insufficient for handling complex tasks which necessitate a combination of task planning and the usage of external tools. In this paper, we first propose a structured framework tailored for LLM-based AI Agents and discuss the crucial capabilities necessary for tackling intricate problems. Within this framework, we design two distinct types of agents (i.e., one-step agent and sequential agent) to execute the inference process. Subsequently, we instantiate the framework using various LLMs and evaluate their Task Planning and Tool Usage (TPTU) abilities on typical tasks. By highlighting key findings and challenges, our goal is to provide a helpful resource for researchers and practitioners to leverage the power of LLMs in their AI applications. Our study emphasizes the substantial potential of these models, while also identifying areas that need more investigation and improvement.
</details>
<details>
<summary>摘要</summary>
Within this framework, we design two types of agents (one-step agent and sequential agent) to execute the inference process. We then instantiate the framework using various LLMs and evaluate their Task Planning and Tool Usage (TPTU) abilities on typical tasks. Our study highlights key findings and challenges, providing a helpful resource for researchers and practitioners to leverage the power of LLMs in their AI applications. We emphasize the substantial potential of these models, while also identifying areas that require further investigation and improvement.Translation notes:* "Large Language Models" (LLMs) is translated as "大型自然语言处理模型" (dàxíng zìrán yǔyán xῡngwén módelìng).* "Task Planning and Tool Usage" (TPTU) is translated as "任务规划和工具使用" (rèngwù guīhua yǔ gōngjuǎn shǐyòng).* "AI Agents" is translated as "人工智能代理" (réngōng zhìnéng dàibiǎn).* "inference process" is translated as "推理过程" (tuīlǐ gòujiāng).* "one-step agent" and "sequential agent" are translated as "单步代理" (dān bù dàibiǎn) and "连续代理" (liánxù dàibiǎn), respectively.
</details></li>
</ul>
<hr>
<h2 id="Boosting-Chinese-ASR-Error-Correction-with-Dynamic-Error-Scaling-Mechanism"><a href="#Boosting-Chinese-ASR-Error-Correction-with-Dynamic-Error-Scaling-Mechanism" class="headerlink" title="Boosting Chinese ASR Error Correction with Dynamic Error Scaling Mechanism"></a>Boosting Chinese ASR Error Correction with Dynamic Error Scaling Mechanism</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03423">http://arxiv.org/abs/2308.03423</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxin Fan, Yong Zhang, Hanzhang Li, Jianzong Wang, Zhitao Li, Sheng Ouyang, Ning Cheng, Jing Xiao</li>
<li>for: 这篇论文是为了提高自动语音识别（ASR）错误纠正而写的。</li>
<li>methods: 这篇论文提出了一种新的方法，即使用动态错误缩放机制来检测和更正由ASR输出生成的音系错误文本。该机制通过动态融合词级特征和音系信息，以便让模型具备更多的 semantics 数据。此外，该方法还实施了独特的错误减少和增强策略，以解决因 incorrect characters 导致的匹配错误问题。</li>
<li>results: 实验结果表明，该提出的方法可以substantially improve ASR error correction，并在已知的数据集上获得了优秀的结果。<details>
<summary>Abstract</summary>
Chinese Automatic Speech Recognition (ASR) error correction presents significant challenges due to the Chinese language's unique features, including a large character set and borderless, morpheme-based structure. Current mainstream models often struggle with effectively utilizing word-level features and phonetic information. This paper introduces a novel approach that incorporates a dynamic error scaling mechanism to detect and correct phonetically erroneous text generated by ASR output. This mechanism operates by dynamically fusing word-level features and phonetic information, thereby enriching the model with additional semantic data. Furthermore, our method implements unique error reduction and amplification strategies to address the issues of matching wrong words caused by incorrect characters. Experimental results indicate substantial improvements in ASR error correction, demonstrating the effectiveness of our proposed method and yielding promising results on established datasets.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Prompt-Guided-Copy-Mechanism-for-Conversational-Question-Answering"><a href="#Prompt-Guided-Copy-Mechanism-for-Conversational-Question-Answering" class="headerlink" title="Prompt Guided Copy Mechanism for Conversational Question Answering"></a>Prompt Guided Copy Mechanism for Conversational Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03422">http://arxiv.org/abs/2308.03422</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yong Zhang, Zhitao Li, Jianzong Wang, Yiming Gao, Ning Cheng, Fengying Yu, Jing Xiao</li>
<li>for: 本研究旨在提高对话型问题的回答自然度和适用性，提出一种可替换的抽取方法。</li>
<li>methods: 本方法使用提示来连接问题和答案，并使用注意力引导复制机制来验证抽取的答案是否自然和适用。</li>
<li>results: 实验表明，该方法能够有效提高对话型问题的回答自然度和适用性，在CoQA挑战中获得了好的结果。<details>
<summary>Abstract</summary>
Conversational Question Answering (CQA) is a challenging task that aims to generate natural answers for conversational flow questions. In this paper, we propose a pluggable approach for extractive methods that introduces a novel prompt-guided copy mechanism to improve the fluency and appropriateness of the extracted answers. Our approach uses prompts to link questions to answers and employs attention to guide the copy mechanism to verify the naturalness of extracted answers, making necessary edits to ensure that the answers are fluent and appropriate. The three prompts, including a question-rationale relationship prompt, a question description prompt, and a conversation history prompt, enhance the copy mechanism's performance. Our experiments demonstrate that this approach effectively promotes the generation of natural answers and achieves good results in the CoQA challenge.
</details>
<details>
<summary>摘要</summary>
问答对话（CQA）是一项具有挑战性的任务，旨在生成自然的对话流程中的答案。在这篇论文中，我们提出了一种可替换的方法，即使用启示机制来提高抽取答案的流畅性和适用性。我们的方法使用启示来联结问题和答案，并通过注意力引导机制来验证抽取答案的自然性，进行必要的修改，以确保答案的流畅性和适用性。我们的实验表明，这种方法可以有效地促进自然的答案生成，并在CoQA挑战中 дости得好的 результа。
</details></li>
</ul>
<hr>
<h2 id="RecycleGPT-An-Autoregressive-Language-Model-with-Recyclable-Module"><a href="#RecycleGPT-An-Autoregressive-Language-Model-with-Recyclable-Module" class="headerlink" title="RecycleGPT: An Autoregressive Language Model with Recyclable Module"></a>RecycleGPT: An Autoregressive Language Model with Recyclable Module</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03421">http://arxiv.org/abs/2308.03421</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yufan Jiang, Qiaozhi He, Xiaomin Zhuang, Zhihua Wu, Kunpeng Wang, Wenlai Zhao, Guangwen Yang</li>
<li>for: 提高语言模型的速度响应，减少执行时间</li>
<li>methods: 基于紧挨 token 之间的强相关性，回收预生成的模型状态，不需要执行整个模型多次</li>
<li>results: 实验和分析表明，该方法可以下降推理延迟，达到1.4倍的速度提升，同时保持高性能Translation:</li>
<li>for:  To improve the speed response of language models and reduce execution time.</li>
<li>methods: Based on the strong correlations between adjacent tokens, recycling pre-generated model states without running the whole model multiple times.</li>
<li>results: Experiments and analysis show that the approach can significantly reduce inference latency, achieving up to 1.4x speedup while maintaining high performance.<details>
<summary>Abstract</summary>
Existing large language models have to run K times to generate a sequence of K tokens. In this paper, we present RecycleGPT, a generative language model with fast decoding speed by recycling pre-generated model states without running the whole model in multiple steps. Our approach relies on the observation that adjacent tokens in a sequence usually have strong correlations and the next token in a sequence can be reasonably guessed or inferred based on the preceding ones. Experiments and analysis demonstrate the effectiveness of our approach in lowering inference latency, achieving up to 1.4x speedup while preserving high performance.
</details>
<details>
<summary>摘要</summary>
现有大型语言模型需要运行 K 次以生成一个序列中的 K 个符号。在本文中，我们介绍了 RecycleGPT，一种生成语言模型，具有快速解码速度，通过 reuse 预生成模型状态而不需要在多个步骤中运行整个模型。我们的方法基于 adjacent 符号在序列中强相关性的观察，下一个符号可以基于前一个符号预测或推理。实验和分析表明，我们的方法可以降低推理延迟，达到最高性能的 1.4 倍速度。
</details></li>
</ul>
<hr>
<h2 id="End-to-End-Evaluation-for-Low-Latency-Simultaneous-Speech-Translation"><a href="#End-to-End-Evaluation-for-Low-Latency-Simultaneous-Speech-Translation" class="headerlink" title="End-to-End Evaluation for Low-Latency Simultaneous Speech Translation"></a>End-to-End Evaluation for Low-Latency Simultaneous Speech Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03415">http://arxiv.org/abs/2308.03415</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christian Huber, Tu Anh Dinh, Carlos Mullov, Ngoc Quan Pham, Thai Binh Nguyen, Fabian Retkowski, Stefan Constantin, Enes Yavuz Ugan, Danni Liu, Zhaolin Li, Sai Koneru, Jan Niehues, Alexander Waibel</li>
<li>for: 本研究旨在评估不同低延迟语音翻译方法在真实场景中的表现。</li>
<li>methods: 本文提出了首个用于实际场景下评估多种低延迟语音翻译方法的框架。该框架包括音频分 segmentation 和组件的运行时间评估。</li>
<li>results: 本研究通过该框架对不同低延迟语音翻译方法进行了比较。包括可修改输出和固定输出方法的比较，以及使用现有的缩进和端到端系统的比较。此外，框架还可自动评估翻译质量和延迟时间，并提供了在线用户界面，以便向用户显示低延迟模型的输出。<details>
<summary>Abstract</summary>
The challenge of low-latency speech translation has recently draw significant interest in the research community as shown by several publications and shared tasks. Therefore, it is essential to evaluate these different approaches in realistic scenarios. However, currently only specific aspects of the systems are evaluated and often it is not possible to compare different approaches.   In this work, we propose the first framework to perform and evaluate the various aspects of low-latency speech translation under realistic conditions. The evaluation is carried out in an end-to-end fashion. This includes the segmentation of the audio as well as the run-time of the different components.   Secondly, we compare different approaches to low-latency speech translation using this framework. We evaluate models with the option to revise the output as well as methods with fixed output. Furthermore, we directly compare state-of-the-art cascaded as well as end-to-end systems. Finally, the framework allows to automatically evaluate the translation quality as well as latency and also provides a web interface to show the low-latency model outputs to the user.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。</SYS>>研究群体对低延迟语音翻译的挑战在最近几年内已引起了广泛的关注，如图所示。因此，评估这些不同的方法在实际场景下是非常重要的。然而，当前只有特定的方面的系统被评估，而且不能比较不同的方法。在这项工作中，我们提出了第一个能够在实际场景下进行和评估多个方面的低延迟语音翻译的框架。这包括音频分割以及不同组件的运行时间。其次，我们使用这个框架对低延迟语音翻译不同方法进行比较。我们评估可以修改输出的模型以及固定输出的方法。此外，我们直接对现有的核心笔记和端到端系统进行比较。最后，该框架可以自动评估翻译质量以及延迟时间，并提供了一个网页界面，以便用户查看低延迟模型的输出。
</details></li>
</ul>
<hr>
<h2 id="Counterfactual-Monotonic-Knowledge-Tracing-for-Assessing-Students’-Dynamic-Mastery-of-Knowledge-Concepts"><a href="#Counterfactual-Monotonic-Knowledge-Tracing-for-Assessing-Students’-Dynamic-Mastery-of-Knowledge-Concepts" class="headerlink" title="Counterfactual Monotonic Knowledge Tracing for Assessing Students’ Dynamic Mastery of Knowledge Concepts"></a>Counterfactual Monotonic Knowledge Tracing for Assessing Students’ Dynamic Mastery of Knowledge Concepts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03377">http://arxiv.org/abs/2308.03377</a></li>
<li>repo_url: None</li>
<li>paper_authors: Moyu Zhang, Xinning Zhu, Chunhong Zhang, Wenchen Qian, Feng Pan, Hui Zhao</li>
<li>for: 评估学生动态知识概念的掌握是知识追踪（KT）任务的核心， both offline teaching and online educational applications 需要。</li>
<li>methods: exist KT methods rely on the implicit paradigm of historical practice to mastery of knowledge concepts to students’ responses to practices to address the challenge of unlabeled concept mastery.</li>
<li>results: 我们提出了一种原则正确的方法 called Counterfactual Monotonic Knowledge Tracing (CMKT), which builds on the implicit paradigm described above by using a counterfactual assumption to constrain the evolution of students’ mastery of knowledge concepts.<details>
<summary>Abstract</summary>
As the core of the Knowledge Tracking (KT) task, assessing students' dynamic mastery of knowledge concepts is crucial for both offline teaching and online educational applications. Since students' mastery of knowledge concepts is often unlabeled, existing KT methods rely on the implicit paradigm of historical practice to mastery of knowledge concepts to students' responses to practices to address the challenge of unlabeled concept mastery. However, purely predicting student responses without imposing specific constraints on hidden concept mastery values does not guarantee the accuracy of these intermediate values as concept mastery values. To address this issue, we propose a principled approach called Counterfactual Monotonic Knowledge Tracing (CMKT), which builds on the implicit paradigm described above by using a counterfactual assumption to constrain the evolution of students' mastery of knowledge concepts.
</details>
<details>
<summary>摘要</summary>
为核心的知识跟踪（KT）任务，评估学生的动态知识概念熟练性非常重要，是线上教育应用以及线下教育中的一个关键任务。由于学生的知识概念熟练性通常无法被直接标注，现有的KT方法通常采用历史实践的隐式模式来评估学生对知识概念的熟练性。然而，仅仅预测学生的回答不能保证这些中间概念熟练性值的准确性。为解决这个问题，我们提出了一种原则性的方法calledCounterfactual Monotonic Knowledge Tracing（CMKT），该方法基于上述隐式模式，并使用一种对假假设来约束学生的知识概念熟练性的演化。
</details></li>
</ul>
<hr>
<h2 id="Robust-Ordinal-Regression-for-Subsets-Comparisons-with-Interactions"><a href="#Robust-Ordinal-Regression-for-Subsets-Comparisons-with-Interactions" class="headerlink" title="Robust Ordinal Regression for Subsets Comparisons with Interactions"></a>Robust Ordinal Regression for Subsets Comparisons with Interactions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03376">http://arxiv.org/abs/2308.03376</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hugo Gilbert, Mohamed Ouaguenouni, Meltem Ozturk, Olivier Spanjaard</li>
<li>for: 本研究旨在开发一种可靠的排序方法，用于学习决策者对子集的偏好。</li>
<li>methods: 该方法基于鱼本和拉瓦лле（1996）提出的决策模型，并考虑了元素之间的可能交互。我们不会预测不具有可靠预测的偏好，而是根据可能的 simplest models（奥卡姆的剃刀）解释偏好数据来做预测。</li>
<li>results: 我们通过使用不确定集来表示模型参数的可能值，并定义一种robust排序关系，以确定subset之间的偏好关系。我们在 sintetic 和实际数据上进行了数值测试，并证明了我们的偏好预测的多样性和可靠性。<details>
<summary>Abstract</summary>
This paper is dedicated to a robust ordinal method for learning the preferences of a decision maker between subsets. The decision model, derived from Fishburn and LaValle (1996) and whose parameters we learn, is general enough to be compatible with any strict weak order on subsets, thanks to the consideration of possible interactions between elements. Moreover, we accept not to predict some preferences if the available preference data are not compatible with a reliable prediction. A predicted preference is considered reliable if all the simplest models (Occam's razor) explaining the preference data agree on it. Following the robust ordinal regression methodology, our predictions are based on an uncertainty set encompassing the possible values of the model parameters. We define a robust ordinal dominance relation between subsets and we design a procedure to determine whether this dominance relation holds. Numerical tests are provided on synthetic and real-world data to evaluate the richness and reliability of the preference predictions made.
</details>
<details>
<summary>摘要</summary>
这篇论文探讨了一种可靠的排序方法，用于学习决策者对subset的偏好。我们基于鱼本和拉瓦列（1996）提出的决策模型，并学习其参数，该模型可以与任何严格强制排序集合兼容。此外，我们接受不预测一些偏好，如果可用偏好数据不具有可靠预测。一个预测的偏好被视为可靠，如果所有最简模型（奥卡姆的剑）解释偏好数据都同意它。按照稳健排序回归方法，我们的预测基于模型参数的不确定集。我们定义了稳健排序准则，并设计了一种确定该准则是否成立的过程。在synthetic和实际数据上进行了数据测试，以评估我们的偏好预测的 ricacity 和可靠性。
</details></li>
</ul>
<hr>
<h2 id="A-reading-survey-on-adversarial-machine-learning-Adversarial-attacks-and-their-understanding"><a href="#A-reading-survey-on-adversarial-machine-learning-Adversarial-attacks-and-their-understanding" class="headerlink" title="A reading survey on adversarial machine learning: Adversarial attacks and their understanding"></a>A reading survey on adversarial machine learning: Adversarial attacks and their understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03363">http://arxiv.org/abs/2308.03363</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shashank Kotyan</li>
<li>for: This paper provides a survey of existing adversarial attacks and their understanding based on different perspectives, with the goal of classifying adversarial attacks and understanding their vulnerabilities in a systematic order.</li>
<li>methods: The paper uses a comprehensive review of existing literature on adversarial attacks and defenses to provide a detailed understanding of the different types of attacks and their characteristics.</li>
<li>results: The paper concludes with a discussion on the future research directions in the field of adversarial machine learning, highlighting the limitations of existing defenses and the need for further research to mitigate the effects of adversarial attacks.<details>
<summary>Abstract</summary>
Deep Learning has empowered us to train neural networks for complex data with high performance. However, with the growing research, several vulnerabilities in neural networks have been exposed. A particular branch of research, Adversarial Machine Learning, exploits and understands some of the vulnerabilities that cause the neural networks to misclassify for near original input. A class of algorithms called adversarial attacks is proposed to make the neural networks misclassify for various tasks in different domains. With the extensive and growing research in adversarial attacks, it is crucial to understand the classification of adversarial attacks. This will help us understand the vulnerabilities in a systematic order and help us to mitigate the effects of adversarial attacks. This article provides a survey of existing adversarial attacks and their understanding based on different perspectives. We also provide a brief overview of existing adversarial defences and their limitations in mitigating the effect of adversarial attacks. Further, we conclude with a discussion on the future research directions in the field of adversarial machine learning.
</details>
<details>
<summary>摘要</summary>
深度学习已经赋予我们训练复杂数据的神经网络高性能。然而，随着研究的发展，一些神经网络的漏洞被曝光。一种特定的研究分支，敌意机器学习，利用和探索了一些导致神经网络对近似输入进行误分类的漏洞。一类称为敌意攻击的算法被提出，以使神经网络在不同领域中对不同任务进行误分类。随着敌意攻击的扩大和增长的研究，了解敌意攻击的分类变得非常重要。这将帮助我们系统地了解漏洞，并帮助我们 Mitigate the effects of adversarial attacks。这篇文章提供了现有的敌意攻击和它们的理解，以及不同角度的概述。此外，我们还提供了现有的防御措施的简要概述和其限制在减轻敌意攻击的效果。最后，我们 conclude with 对敌意机器学习未来研究方向的讨论。
</details></li>
</ul>
<hr>
<h2 id="Discrete-Message-via-Online-Clustering-Labels-in-Decentralized-POMDP"><a href="#Discrete-Message-via-Online-Clustering-Labels-in-Decentralized-POMDP" class="headerlink" title="Discrete Message via Online Clustering Labels in Decentralized POMDP"></a>Discrete Message via Online Clustering Labels in Decentralized POMDP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03358">http://arxiv.org/abs/2308.03358</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingdi Chen, Tian Lan</li>
<li>for: 解决多智能体协同学习任务中的共享信息问题</li>
<li>methods: 利用地址�分 clustering 问题的方法，将本地观察数据用作标签，并使用 upper bound 作为归一化损失函数</li>
<li>results: 提出了一种简单的消息生成函数设计，并与奖励学习结合使用 Regularized Information Maximization 损失函数，实现了对比 estado-of-the-art 多智能体通信基eline 的突出表现，并可以实现高效率的几位数据传输。<details>
<summary>Abstract</summary>
Communication is crucial for solving cooperative Multi-Agent Reinforcement Learning tasks in Partially-Observable Markov Decision Processes. Existing works often rely on black-box methods to encode local information/features into messages shared with other agents. However, such black-box approaches are unable to provide any quantitative guarantees on the expected return and often lead to the generation of continuous messages with high communication overhead and poor interpretability. In this paper, we establish an upper bound on the return gap between an ideal policy with full observability and an optimal partially-observable policy with discrete communication. This result enables us to recast multi-agent communication into a novel online clustering problem over the local observations at each agent, with messages as cluster labels and the upper bound on the return gap as clustering loss. By minimizing the upper bound, we propose a surprisingly simple design of message generation functions in multi-agent communication and integrate it with reinforcement learning using a Regularized Information Maximization loss function. Evaluations show that the proposed discrete communication significantly outperforms state-of-the-art multi-agent communication baselines and can achieve nearly-optimal returns with few-bit messages that are naturally interpretable.
</details>
<details>
<summary>摘要</summary>
通信是解决合作多智能体强化学习任务中的关键，特别是在部分可见 Markov 决策过程中。现有的方法 oft rely on 黑盒方法来编码本地信息/特征到其他代理机器人的消息中。然而，这些黑盒方法无法提供任何量化保证返回并经常导致高通信开销和低可读性的连续消息生成。在这篇论文中，我们确定了完全可见策略和部分可见策略之间的返回差。这个结果使得我们可以将多智能体通信转化为一个新的在本地观察到每个代理机器人的局部观察上进行在线划分问题，消息作为划分标签，并将返回差作为划分损失。通过最小化返回差，我们提议一种简单的消息生成函数设计，并将其与强化学习相结合，使用 Regularized Information Maximization 损失函数。评估表明，提议的简单消息生成方法在多智能体通信基elines上显著超越了当前的多智能体通信基elines，并可以在几 bits 的消息中实现近似于最优的返回，这些消息自然可读性强。
</details></li>
</ul>
<hr>
<h2 id="SciGraphQA-A-Large-Scale-Synthetic-Multi-Turn-Question-Answering-Dataset-for-Scientific-Graphs"><a href="#SciGraphQA-A-Large-Scale-Synthetic-Multi-Turn-Question-Answering-Dataset-for-Scientific-Graphs" class="headerlink" title="SciGraphQA: A Large-Scale Synthetic Multi-Turn Question-Answering Dataset for Scientific Graphs"></a>SciGraphQA: A Large-Scale Synthetic Multi-Turn Question-Answering Dataset for Scientific Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03349">http://arxiv.org/abs/2308.03349</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/findalexli/SciGraphQA">https://github.com/findalexli/SciGraphQA</a></li>
<li>paper_authors: Shengzhi Li, Nima Tajbakhsh<br>for:This paper is written to present a new synthetic multi-turn question-answer dataset related to academic graphs, called SciGraphQA.methods:The dataset is built by using Palm-2 to generate open-vocabulary multi-turn question-answering dialogues about the graphs, with an average of 2.23 question-answer turns for each graph. The paper’s context, including the paper title, abstract, paragraph mentioning the graph, and rich text contextual data from the graph, is provided as input to GPT-4 to assess the matching quality of the question-answer turns.results:The average rating of the question-answer turns given the paper’s context is 8.7&#x2F;10 on a test set. The most popular MLLM models, such as LLaVa, mPLUGowl, BLIP-2, and openFlamingo, are evaluated on the dataset, with LLaVA-13B being the most performant with a CIDEr score of 0.08. The question prompts for LLAVA are further enriched by including serialized data tables extracted from the graphs using the DePlot model, boosting LLaVA’s 0-shot CIDEr to 0.15. Fine-tuning LLaVa using the dataset results in a substantially higher CIDEr score of 0.26.<details>
<summary>Abstract</summary>
In this work, we present SciGraphQA, a synthetic multi-turn question-answer dataset related to academic graphs. SciGraphQA is 13 times larger than ChartVQA, the previously largest chart-visual question-answering dataset. It is also the largest open-sourced chart VQA dataset with non-synthetic charts. To build our dataset, we selected 290,000 Computer Science or Machine Learning ArXiv papers published between 2010 and 2020, and then used Palm-2 to generate 295K samples of open-vocabulary multi-turn question-answering dialogues about the graphs. As context, we provided the text-only Palm-2 with paper title, abstract, paragraph mentioning the graph, and rich text contextual data from the graph itself, obtaining dialogues with an average 2.23 question-answer turns for each graph. We asked GPT-4 to assess the matching quality of our question-answer turns given the paper's context, obtaining an average rating of 8.7/10 on our 3K test set. We evaluated the 0-shot capability of the most popular MLLM models such as LLaVa, mPLUGowl, BLIP-2, and openFlamingo's on our dataset, finding LLaVA-13B being the most performant with a CIDEr score of 0.08. We further enriched the question prompts for LLAVA by including the serialized data tables extracted from the graphs using the DePlot model, boosting LLaVA's 0-shot CIDEr to 0.15. To verify the validity of our dataset, we also fine-tuned LLaVa using our dataset, reaching a substantially higher CIDEr score of 0.26. We anticipate further accuracy improvement by including segmentation mask tokens and leveraging larger LLM backbones coupled with emergent prompting techniques. Our code and data are open-sourced.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们介绍了SciGraphQA，一个基于学术图的多turn问答数据集。SciGraphQA比ChartVQA更大13倍，是目前最大的开源 chart VQA数据集之一。为建立我们的数据集，我们选择了2010-2020年发表的290,000篇计算机科学或机器学习ArXiv论文，然后使用Palm-2生成295,000个开 vocabulary multi-turn问答对话。作为 контекст，我们提供了文本只的Palm-2，并提供了论文标题、摘要、提及图表的段落，以及图表自身的丰富文本数据，得到了每个图表的2.23个问答对话。我们召集了GPT-4进行评分，得到了每个测试集的8.7/10的对话匹配评分。我们评估了目前最受欢迎的MLLM模型，包括LLaVa、mPLUGowl、BLIP-2和openFlamingo，发现LLaVA-13B表现最佳，CIDEr分数为0.08。我们进一步丰富了LLaVA的问题提示，包括使用DePlot模型提取的序列化数据表，提高LLaVA的0shot CIDEr至0.15。为验证我们的数据集的有效性，我们还使用我们的数据集进行了精细调整LLaVa，达到了远高于0.26的CIDEr分数。我们预期将来的准确性改进，通过包括分割masktoken和利用更大的LLM底层，并采用新的提示技术。我们的代码和数据将公开。
</details></li>
</ul>
<hr>
<h2 id="Solving-Falkner-Skan-type-equations-via-Legendre-and-Chebyshev-Neural-Blocks"><a href="#Solving-Falkner-Skan-type-equations-via-Legendre-and-Chebyshev-Neural-Blocks" class="headerlink" title="Solving Falkner-Skan type equations via Legendre and Chebyshev Neural Blocks"></a>Solving Falkner-Skan type equations via Legendre and Chebyshev Neural Blocks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03337">http://arxiv.org/abs/2308.03337</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alireza Afzal Aghaei, Kourosh Parand, Ali Nikkhah, Shakila Jaberi</li>
<li>for: 解决非线性的 Falcker-Skan 方程</li>
<li>methods: 使用 Legendre 和 Chebyshev 神经块，利用 orthogonal  polynomials 在神经网络中提高拟合能力</li>
<li>results: 通过对 Falcker-Skan 方程的不同配置进行 simulate，证明提出的方法可以减少计算复杂性并提高效率<details>
<summary>Abstract</summary>
In this paper, a new deep-learning architecture for solving the non-linear Falkner-Skan equation is proposed. Using Legendre and Chebyshev neural blocks, this approach shows how orthogonal polynomials can be used in neural networks to increase the approximation capability of artificial neural networks. In addition, utilizing the mathematical properties of these functions, we overcome the computational complexity of the backpropagation algorithm by using the operational matrices of the derivative. The efficiency of the proposed method is carried out by simulating various configurations of the Falkner-Skan equation.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，一种新的深度学习架构，用于解决非线性法克内-斯坦方程，被提出。通过使用Legendre和Chebyshev神经块，这种方法表明了在神经网络中使用正交多项式可以增加人工神经网络的拟合能力。此外，利用这些函数的数学性质，我们超越了反propagation算法的计算复杂性，使用操作矩阵的导数。提出的方法的效率被通过 simulate多种法克内-斯坦方程的配置来证明。
</details></li>
</ul>
<hr>
<h2 id="Heterogeneous-Knowledge-Fusion-A-Novel-Approach-for-Personalized-Recommendation-via-LLM"><a href="#Heterogeneous-Knowledge-Fusion-A-Novel-Approach-for-Personalized-Recommendation-via-LLM" class="headerlink" title="Heterogeneous Knowledge Fusion: A Novel Approach for Personalized Recommendation via LLM"></a>Heterogeneous Knowledge Fusion: A Novel Approach for Personalized Recommendation via LLM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03333">http://arxiv.org/abs/2308.03333</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bin Yin, Junjie Xie, Yu Qin, Zixiang Ding, Zhichao Feng, Xiang Li, Wei Lin</li>
<li>for: 这个研究旨在提出一种基于大型自然语言模型（LLM）的对象化推荐方法，以抽取和融合用户各种不同行为信息，并对LLM进行调教，以提高对象化推荐的表现。</li>
<li>methods: 本研究使用了各种不同行为信息，包括用户的搜寻历史、购买纪录、社交媒体活动等，并使用了LLM进行学习和推荐。</li>
<li>results: 实验结果显示，我们的方法可以很好地融合用户各种不同行为信息，并对推荐表现有所提高。<details>
<summary>Abstract</summary>
The analysis and mining of user heterogeneous behavior are of paramount importance in recommendation systems. However, the conventional approach of incorporating various types of heterogeneous behavior into recommendation models leads to feature sparsity and knowledge fragmentation issues. To address this challenge, we propose a novel approach for personalized recommendation via Large Language Model (LLM), by extracting and fusing heterogeneous knowledge from user heterogeneous behavior information. In addition, by combining heterogeneous knowledge and recommendation tasks, instruction tuning is performed on LLM for personalized recommendations. The experimental results demonstrate that our method can effectively integrate user heterogeneous behavior and significantly improve recommendation performance.
</details>
<details>
<summary>摘要</summary>
“用户多样化行为的分析和挖掘是推荐系统中的关键。然而，通过将不同类型的多样化行为 integrate into 推荐模型中，会导致特征稀缺和知识孤立问题。为解决这个挑战，我们提出了一种基于 Large Language Model (LLM) 的个性化推荐方法，通过提取和融合用户多样化行为信息中的多样化知识。此外，通过结合多样化知识和推荐任务，对 LLM 进行了指令调整，以实现个性化推荐。实验结果显示，我们的方法可以有效地 инте integrate 用户多样化行为，并有显著提高推荐性能。”Note: Please keep in mind that the translation is in Simplified Chinese, and the grammar and sentence structure may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="Improving-Deep-Attractor-Network-by-BGRU-and-GMM-for-Speech-Separation"><a href="#Improving-Deep-Attractor-Network-by-BGRU-and-GMM-for-Speech-Separation" class="headerlink" title="Improving Deep Attractor Network by BGRU and GMM for Speech Separation"></a>Improving Deep Attractor Network by BGRU and GMM for Speech Separation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03332">http://arxiv.org/abs/2308.03332</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rawad Melhem, Assef Jafar, Riad Hamadeh</li>
<li>for: 提出了一种简化了DANet模型，以提高混音分离的性能和学习速度。</li>
<li>methods: 使用了Bidirectional Gated neural network (BGRU) instead of BLSTM，并使用 Gaussian Mixture Model (GMM) 作为聚类算法来减少模型的复杂性。</li>
<li>results: 在使用TIMIT corpus中的两个说话者混音数据集进行评估时，提出的模型比原始DANet模型得到了12.3 dB和2.94的SDR和PESQ分数，并且减少了20.7%和17.9%的参数数量和训练时间。同时，模型在混合阿拉伯语音信号上也表现了更好的result。<details>
<summary>Abstract</summary>
Deep Attractor Network (DANet) is the state-of-the-art technique in speech separation field, which uses Bidirectional Long Short-Term Memory (BLSTM), but the complexity of the DANet model is very high. In this paper, a simplified and powerful DANet model is proposed using Bidirectional Gated neural network (BGRU) instead of BLSTM. The Gaussian Mixture Model (GMM) other than the k-means was applied in DANet as a clustering algorithm to reduce the complexity and increase the learning speed and accuracy. The metrics used in this paper are Signal to Distortion Ratio (SDR), Signal to Interference Ratio (SIR), Signal to Artifact Ratio (SAR), and Perceptual Evaluation Speech Quality (PESQ) score. Two speaker mixture datasets from TIMIT corpus were prepared to evaluate the proposed model, and the system achieved 12.3 dB and 2.94 for SDR and PESQ scores respectively, which were better than the original DANet model. Other improvements were 20.7% and 17.9% in the number of parameters and time training, respectively. The model was applied on mixed Arabic speech signals and the results were better than that in English.
</details>
<details>
<summary>摘要</summary>
深度吸引网络（DANet）是现在的演说分离领域技术state-of-the-art，使用双向长短期记忆（BLSTM），但DANet模型的复杂性很高。在这篇论文中，一种简化了DANet模型的方法被提出，使用双向阻塞神经网络（BGRU）而不是BLSTM。在DANet中， Gaussian Mixture Model（GMM）作为聚类算法，以降低复杂性并提高学习速度和准确性。在本文中使用的度量包括Signal to Distortion Ratio（SDR）、Signal to Interference Ratio（SIR）、Signal to Artifact Ratio（SAR）和Perceptual Evaluation Speech Quality（PESQ）分数。对于TIMIT corpus中的两个说话混合数据集进行了评估，提出的模型实现了12.3 dB和2.94的SDR和PESQ分数，分别高于原始DANet模型。此外，模型的参数数量和训练时间都有20.7%和17.9%的下降。模型在混合阿拉伯语语音信号上进行了应用，结果比英语更好。
</details></li>
</ul>
<hr>
<h2 id="Expediting-Neural-Network-Verification-via-Network-Reduction"><a href="#Expediting-Neural-Network-Verification-via-Network-Reduction" class="headerlink" title="Expediting Neural Network Verification via Network Reduction"></a>Expediting Neural Network Verification via Network Reduction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03330">http://arxiv.org/abs/2308.03330</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuyi Zhong, Ruiwei Wang, Siau-Cheng Khoo</li>
<li>for: 验证深度神经网络的安全性 Properties，以确保神经网络在关键应用中正确工作。</li>
<li>methods: 提出了许多验证方法，但许多知名的验证工具仍然无法处理复杂的网络架构和大型网络。 这个研究提出了一种网络减少技术作为验证前置处理方法，通过消除稳定的ReLU神经元并将其转换为一个序列神经网络，包括ReLU和Affine层，这些层可以由大多数验证工具处理。</li>
<li>results: 我们在一个大量的benchmark上进行了实验，结果表明，提出的方法可以减少神经网络，并使现有的验证工具更快速地处理神经网络。此外，实验结果还表明，网络减少可以提高现有验证工具对许多网络的可用性。<details>
<summary>Abstract</summary>
A wide range of verification methods have been proposed to verify the safety properties of deep neural networks ensuring that the networks function correctly in critical applications. However, many well-known verification tools still struggle with complicated network architectures and large network sizes. In this work, we propose a network reduction technique as a pre-processing method prior to verification. The proposed method reduces neural networks via eliminating stable ReLU neurons, and transforming them into a sequential neural network consisting of ReLU and Affine layers which can be handled by the most verification tools. We instantiate the reduction technique on the state-of-the-art complete and incomplete verification tools, including alpha-beta-crown, VeriNet and PRIMA. Our experiments on a large set of benchmarks indicate that the proposed technique can significantly reduce neural networks and speed up existing verification tools. Furthermore, the experiment results also show that network reduction can improve the availability of existing verification tools on many networks by reducing them into sequential neural networks.
</details>
<details>
<summary>摘要</summary>
各种验证方法已经被提议来验证深度神经网络的安全性，以确保神经网络在关键应用中正确地工作。然而，许多知名的验证工具仍然无法处理复杂的网络架构和大型网络。在这种情况下，我们提议一种网络减少技术作为预处理方法，以降低验证工具的难度。我们的方法利用稳定的ReLU神经元的消除和变换为一个序列神经网络，包括ReLU和Affine层，这些层可以由现有的验证工具处理。我们在alpha-beta-crown、VeriNet和PRIMA等完整和部分验证工具上实现了这种减少技术，并对一个大量的benchmark进行了实验。实验结果表明，我们的方法可以减少神经网络，并使现有的验证工具在许多网络上提高可用性。
</details></li>
</ul>
<hr>
<h2 id="Generative-AI-trial-for-nonviolent-communication-mediation"><a href="#Generative-AI-trial-for-nonviolent-communication-mediation" class="headerlink" title="Generative AI trial for nonviolent communication mediation"></a>Generative AI trial for nonviolent communication mediation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03326">http://arxiv.org/abs/2308.03326</a></li>
<li>repo_url: None</li>
<li>paper_authors: Takeshi Kato</li>
<li>for: 目的是建立一个包容多元价值观的社会，通过非暴力交流（NVC）来帮助人们在社会分化和冲突中表达自己的感受和需求，并实现宽泛的合作和共谐。</li>
<li>methods: 使用生成AI来模拟证明人员的培训，在四个过程中测试了ChatGPT的可能性：观察、情感、需求和请求。</li>
<li>results: 结果表明，使用生成AI可能有潜在的应用前提，但目前还不够实际化。建议改进的指南包括添加模型回答、重新学习修改回答、使用适当的词汇表达每个过程，以及重新请求必要的信息。<details>
<summary>Abstract</summary>
Aiming for a mixbiotic society that combines freedom and solidarity among people with diverse values, I focused on nonviolent communication (NVC) that enables compassionate giving in various situations of social division and conflict, and tried a generative AI for it. Specifically, ChatGPT was used in place of the traditional certified trainer to test the possibility of mediating (modifying) input sentences in four processes: observation, feelings, needs, and requests. The results indicate that there is potential for the application of generative AI, although not yet at a practical level. Suggested improvement guidelines included adding model responses, relearning revised responses, specifying appropriate terminology for each process, and re-asking for required information. The use of generative AI will be useful initially to assist certified trainers, to prepare for and review events and workshops, and in the future to support consensus building and cooperative behavior in digital democracy, platform cooperatives, and cyber-human social co-operating systems. It is hoped that the widespread use of NVC mediation using generative AI will lead to the early realization of a mixbiotic society.
</details>
<details>
<summary>摘要</summary>
我寻求一个mixbiotic社会，既保持人们多样化价值观的自由，又强调人们之间的团结和共融。我选择了非暴力通信（NVC）作为解决社会分化和冲突的工具，并使用生成AI测试其可能性。特别是，我使用了ChatGPT来替代传统证明人员，在四个过程中测试了输入句子的修改可能性：观察、情感、需求和请求。结果表明，生成AI有潜力应用，但还不够实用。建议的改进建议包括添加模型回答、重新学习修改回答、指定每个过程的适当术语，以及重新请求需要的信息。使用生成AI将有助于资深训练人员，准备和审查活动和讲座，以及在未来支持协商建设和合作行为在数字民主、平台合作和人机社会合作系统中。希望通过广泛应用NVC媒介使用生成AI，早日实现mixbiotic社会。
</details></li>
</ul>
<hr>
<h2 id="Part-Aware-Transformer-for-Generalizable-Person-Re-identification"><a href="#Part-Aware-Transformer-for-Generalizable-Person-Re-identification" class="headerlink" title="Part-Aware Transformer for Generalizable Person Re-identification"></a>Part-Aware Transformer for Generalizable Person Re-identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03322">http://arxiv.org/abs/2308.03322</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liyuke65535/part-aware-transformer">https://github.com/liyuke65535/part-aware-transformer</a></li>
<li>paper_authors: Hao Ni, Yuke Li, Heng Tao Shen, Jingkuan Song</li>
<li>for: 本研究旨在提高预测人脸锁定 Task（DG-ReID）中模型的泛化能力，特别是在不同频谱上进行训练和测试时。</li>
<li>methods: 我们提出了一种名为Part-aware Transformer的纯转换器模型，该模型通过一个名为CSL的协助任务来学习地方视觉信息，从而改善模型的泛化能力。</li>
<li>results: 我们的方法在大多数DG-ReID设置下达到了状态对的表现，特别是在Market$\to$Duke设置下，我们的方法在Rank1和mAP上超过了州际最优性的表现，提高了10.9%和12.8%。<details>
<summary>Abstract</summary>
Domain generalization person re-identification (DG-ReID) aims to train a model on source domains and generalize well on unseen domains. Vision Transformer usually yields better generalization ability than common CNN networks under distribution shifts. However, Transformer-based ReID models inevitably over-fit to domain-specific biases due to the supervised learning strategy on the source domain. We observe that while the global images of different IDs should have different features, their similar local parts (e.g., black backpack) are not bounded by this constraint. Motivated by this, we propose a pure Transformer model (termed Part-aware Transformer) for DG-ReID by designing a proxy task, named Cross-ID Similarity Learning (CSL), to mine local visual information shared by different IDs. This proxy task allows the model to learn generic features because it only cares about the visual similarity of the parts regardless of the ID labels, thus alleviating the side effect of domain-specific biases. Based on the local similarity obtained in CSL, a Part-guided Self-Distillation (PSD) is proposed to further improve the generalization of global features. Our method achieves state-of-the-art performance under most DG ReID settings. Under the Market$\to$Duke setting, our method exceeds state-of-the-art by 10.9% and 12.8% in Rank1 and mAP, respectively. The code is available at https://github.com/liyuke65535/Part-Aware-Transformer.
</details>
<details>
<summary>摘要</summary>
领域总结人识别（DG-ReID）目标是在源频谱上训练模型，并在未看过的频谱上进行普适化。视觉转移通常比常见的CNN网络在分布差异下表现更好，但是转移基于的ReID模型总是因为监督学习策略在源频谱上遇到分布差异而导致过拟合。我们发现，不同ID的全局图像应该有不同的特征，但是它们的相似部分（例如黑色背pack）并不受这一限制。基于这一点，我们提出了一种纯transformer模型（称为Part-aware Transformer），通过设计一个代理任务（名为跨ID相似学习（CSL））来挖掘不同ID的本地视觉信息。这个代理任务使得模型学习通用特征，因为它只关心不同ID标签下的视觉相似性，从而消除分布差异的副作用。基于本地相似性获得的Part-guided Self-Distillation（PSD）进一步改进了全局特征的普适性。我们的方法在大多数DG ReID设置下达到了状态盘。在Market$\to$Duke设置下，我们的方法比状态盘提高了10.9%和12.8%的排名1和mAP， соответivamente。代码可以在https://github.com/liyuke65535/Part-Aware-Transformer上下载。
</details></li>
</ul>
<hr>
<h2 id="Binary-Federated-Learning-with-Client-Level-Differential-Privacy"><a href="#Binary-Federated-Learning-with-Client-Level-Differential-Privacy" class="headerlink" title="Binary Federated Learning with Client-Level Differential Privacy"></a>Binary Federated Learning with Client-Level Differential Privacy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03320">http://arxiv.org/abs/2308.03320</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lumin Liu, Jun Zhang, Shenghui Song, Khaled B. Letaief</li>
<li>for: 提高 Federated Learning 系统中的隐私保护和性能。</li>
<li>methods: 采用 Binary Neural Networks (BNNs) 和粗粒化噪声来实现客户端级别的隐私保护，并且通过调整粗粒化噪声来保证隐私保护和性能之间的平衡。</li>
<li>results: 实验结果基于 MNIST 和 Fashion-MNIST 数据集显示，提议的训练算法可以实现客户端级别的隐私保护，同时享受到低通信开销的优势。<details>
<summary>Abstract</summary>
Federated learning (FL) is a privacy-preserving collaborative learning framework, and differential privacy can be applied to further enhance its privacy protection. Existing FL systems typically adopt Federated Average (FedAvg) as the training algorithm and implement differential privacy with a Gaussian mechanism. However, the inherent privacy-utility trade-off in these systems severely degrades the training performance if a tight privacy budget is enforced. Besides, the Gaussian mechanism requires model weights to be of high-precision. To improve communication efficiency and achieve a better privacy-utility trade-off, we propose a communication-efficient FL training algorithm with differential privacy guarantee. Specifically, we propose to adopt binary neural networks (BNNs) and introduce discrete noise in the FL setting. Binary model parameters are uploaded for higher communication efficiency and discrete noise is added to achieve the client-level differential privacy protection. The achieved performance guarantee is rigorously proved, and it is shown to depend on the level of discrete noise. Experimental results based on MNIST and Fashion-MNIST datasets will demonstrate that the proposed training algorithm achieves client-level privacy protection with performance gain while enjoying the benefits of low communication overhead from binary model updates.
</details>
<details>
<summary>摘要</summary>
联合学习（FL）是一种隐私保护的合作学习框架，可以进一步强化其隐私保护。现有的FL系统通常采用联合平均（FedAvg）作为训练算法，并在其中实现差分隐私。然而，这些系统中的隐私 utility 质量负面环境严重影响训练性能，特别是当强制实施严格的隐私预算时。此外， Gaussian 机制需要模型参数的高精度。为了提高通信效率和实现更好的隐私 utility 质量，我们提议一种基于 binary neural networks（BNNs）的通信高效的FL训练算法，并实现了适用于客户端的差分隐私保护。我们采用 binary 模型参数上传，以提高通信效率，并在FL设置中添加抽象噪声来实现客户端级差分隐私保护。我们的性能保证是严格地证明的，并且表明其取决于抽象噪声的水平。实验结果基于 MNIST 和 Fashion-MNIST 数据集表明，我们的训练算法可以实现客户端级差分隐私保护，同时享受到低通信开销的 binary 模型更新的好处。
</details></li>
</ul>
<hr>
<h2 id="When-GPT-Meets-Program-Analysis-Towards-Intelligent-Detection-of-Smart-Contract-Logic-Vulnerabilities-in-GPTScan"><a href="#When-GPT-Meets-Program-Analysis-Towards-Intelligent-Detection-of-Smart-Contract-Logic-Vulnerabilities-in-GPTScan" class="headerlink" title="When GPT Meets Program Analysis: Towards Intelligent Detection of Smart Contract Logic Vulnerabilities in GPTScan"></a>When GPT Meets Program Analysis: Towards Intelligent Detection of Smart Contract Logic Vulnerabilities in GPTScan</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03314">http://arxiv.org/abs/2308.03314</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuqiang Sun, Daoyuan Wu, Yue Xue, Han Liu, Haijun Wang, Zhengzi Xu, Xiaofei Xie, Yang Liu</li>
<li>for: 这个论文主要是为了检测智能合约中的逻辑漏洞。</li>
<li>methods: 该论文使用了生成推训Transformer（GPT）和静态分析结合以检测智能合约的逻辑漏洞。</li>
<li>results: 该论文通过使用GPT来理解代码，实现了高精度（超过90%）的智能合约逻辑漏洞检测，并新发现了9个人验员错过的漏洞。<details>
<summary>Abstract</summary>
Smart contracts are prone to various vulnerabilities, leading to substantial financial losses over time. Current analysis tools mainly target vulnerabilities with fixed control or dataflow patterns, such as re-entrancy and integer overflow. However, a recent study on Web3 security bugs revealed that about 80% of these bugs cannot be audited by existing tools due to the lack of domain-specific property description and checking. Given recent advances in Generative Pretraining Transformer (GPT), it is worth exploring how GPT could aid in detecting logic vulnerabilities in smart contracts. In this paper, we propose GPTScan, the first tool combining GPT with static analysis for smart contract logic vulnerability detection. Instead of relying solely on GPT to identify vulnerabilities, which can lead to high false positives and is limited by GPT's pre-trained knowledge, we utilize GPT as a versatile code understanding tool. By breaking down each logic vulnerability type into scenarios and properties, GPTScan matches candidate vulnerabilities with GPT. To enhance accuracy, GPTScan further instructs GPT to intelligently recognize key variables and statements, which are then validated by static confirmation. Evaluation on diverse datasets with around 400 contract projects and 3K Solidity files shows that GPTScan achieves high precision (over 90%) for token contracts and acceptable precision (57.14%) for large projects like Web3Bugs. It effectively detects groundtruth logic vulnerabilities with a recall of over 80%, including 9 new vulnerabilities missed by human auditors. GPTScan is fast and cost-effective, taking an average of 14.39 seconds and 0.01 USD to scan per thousand lines of Solidity code. Moreover, static confirmation helps GPTScan reduce two-thirds of false positives.
</details>
<details>
<summary>摘要</summary>
智能合约容易受到各种漏洞的威胁，导致长期的财务损失。现有的分析工具主要targets着固定控制或数据流模式的漏洞，如重入和整数溢出。然而，一项研究表明，约80%的Web3安全漏洞无法由现有工具检测，因为缺乏域特定的属性描述和检查。随着生成学习变换器（GPT）的进步，我们可以考虑如何使用GPT来检测智能合约逻辑漏洞。在这篇论文中，我们提出GPTScan，第一个结合GPT和静态分析的智能合约逻辑漏洞检测工具。而不是仅仅依靠GPT来识别漏洞，这可能会导致高false positives和GPT的预训练知识的限制。我们利用GPT作为智能代码理解工具，将每种逻辑漏洞类型分解为场景和属性。GPTScan与GPT进行匹配，以提高准确性。为了进一步提高准确性，GPTScan还 instrucGPT认智感知关键变量和语句，然后验证这些变量和语句的有效性。我们对包括约400个合约项目和3000个Solidity文件的多样化数据进行评估，结果表明GPTScan在智能合约中具有高精度（超过90%），并且在大型项目如Web3Bugs中具有可接受的精度（57.14%）。GPTScan可以快速和cost-effective地检测漏洞，每千行Solidity代码平均需要14.39秒和0.01美元。此外，静态确认帮助GPTScan减少了两 thirds的false positives。
</details></li>
</ul>
<hr>
<h2 id="CrossTalk-Intelligent-Substrates-for-Language-Oriented-Interaction-in-Video-Based-Communication-and-Collaboration"><a href="#CrossTalk-Intelligent-Substrates-for-Language-Oriented-Interaction-in-Video-Based-Communication-and-Collaboration" class="headerlink" title="CrossTalk: Intelligent Substrates for Language-Oriented Interaction in Video-Based Communication and Collaboration"></a>CrossTalk: Intelligent Substrates for Language-Oriented Interaction in Video-Based Communication and Collaboration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03311">http://arxiv.org/abs/2308.03311</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haijun Xia, Tony Wang, Aditya Gunturu, Peiling Jiang, William Duan, Xiaoshuo Yao</li>
<li>for: 这篇论文旨在提出一种基于智能技术的视频会议系统，以便更好地帮助用户进行交流和合作。</li>
<li>methods: 论文提出三个关键设计思想，包括面板基础、语言基于意图识别和轻量级交互技术。</li>
<li>results: 作者开发了一个名为 CrossTalk 的视频会议系统，该系统实现了这三个设计思想，并为用户提供了更加流畅和灵活的交流和合作体验。<details>
<summary>Abstract</summary>
Despite the advances and ubiquity of digital communication media such as videoconferencing and virtual reality, they remain oblivious to the rich intentions expressed by users. Beyond transmitting audio, videos, and messages, we envision digital communication media as proactive facilitators that can provide unobtrusive assistance to enhance communication and collaboration. Informed by the results of a formative study, we propose three key design concepts to explore the systematic integration of intelligence into communication and collaboration, including the panel substrate, language-based intent recognition, and lightweight interaction techniques. We developed CrossTalk, a videoconferencing system that instantiates these concepts, which was found to enable a more fluid and flexible communication and collaboration experience.
</details>
<details>
<summary>摘要</summary>
尽管数字通信媒体如视频会议和虚拟现实已经广泛应用并普及，但它们却忽略了用户表达的丰富意图。我们认为数字通信媒体不仅仅是传输音频、视频和消息的工具，而是能够通过不侵入式的协助来提高交流和合作。根据前期研究的结果，我们提出了三个关键的设计思想，包括面板底层、语言基于意图识别和轻量级交互技术。我们开发了 CrossTalk 视频会议系统，该系统实现了这些概念，并在使用者体验中提供了更加流畅和灵活的交流和合作体验。
</details></li>
</ul>
<hr>
<h2 id="What-has-ChatGPT-read-The-origins-of-archaeological-citations-used-by-a-generative-artificial-intelligence-application"><a href="#What-has-ChatGPT-read-The-origins-of-archaeological-citations-used-by-a-generative-artificial-intelligence-application" class="headerlink" title="What has ChatGPT read? The origins of archaeological citations used by a generative artificial intelligence application"></a>What has ChatGPT read? The origins of archaeological citations used by a generative artificial intelligence application</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03301">http://arxiv.org/abs/2308.03301</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dirk HR Spennemann</li>
<li>for: 测试 chatGPT 模型是否包含历史文献内容</li>
<li>methods: 使用cloze分析法推测 chatGPT 模型所 memorized 的源泉</li>
<li>results: chatGPT 模型提供的参考文献中有很多是 fictitious，但是所有真实的参考文献都有在wikipedia页面上被引用Here’s a breakdown of each point in English:</li>
<li>for: The paper aims to test what archaeological literature was included in ChatGPT’s training phase.</li>
<li>methods: The paper uses cloze analysis to infer what sources the generative AI model has memorized.</li>
<li>results: The paper finds that a large percentage of the references provided by ChatGPT are fictitious, and that all genuine references have also been cited on Wikipedia pages. This suggests that the source base for at least some of the data is found in those pages.<details>
<summary>Abstract</summary>
The public release of ChatGPT has resulted in considerable publicity and has led to wide-spread discussion of the usefulness and capabilities of generative AI language models. Its ability to extract and summarise data from textual sources and present them as human-like contextual responses makes it an eminently suitable tool to answer questions users might ask. This paper tested what archaeological literature appears to have been included in ChatGPT's training phase. While ChatGPT offered seemingly pertinent references, a large percentage proved to be fictitious. Using cloze analysis to make inferences on the sources 'memorised' by a generative AI model, this paper was unable to prove that ChatGPT had access to the full texts of the genuine references. It can be shown that all references provided by ChatGPT that were found to be genuine have also been cited on Wikipedia pages. This strongly indicates that the source base for at least some of the data is found in those pages. The implications of this in relation to data quality are discussed.
</details>
<details>
<summary>摘要</summary>
公共发布的ChatGPT已引起广泛的关注和讨论，探讨了生成AI语言模型的用途和能力。它可以从文本源中提取和摘要数据，并以人类化的语言回答用户问题。这篇论文测试了ChatGPT在训练阶段是否包含了文物学 литературы。虽然ChatGPT提供了看似相关的参考，但大多数证明是假的。通过cloze分析来推断一个生成AI模型所吸收的源，这篇论文未能证明ChatGPT有访问全文真实参考的能力。可以证明所有由ChatGPT提供的真实参考都已经出现在Wikipedia页面上。这表明至少一部分数据的来源在那里。关于数据质量的影响，进行了讨论。
</details></li>
</ul>
<hr>
<h2 id="DOMINO-Domain-invariant-Hyperdimensional-Classification-for-Multi-Sensor-Time-Series-Data"><a href="#DOMINO-Domain-invariant-Hyperdimensional-Classification-for-Multi-Sensor-Time-Series-Data" class="headerlink" title="DOMINO: Domain-invariant Hyperdimensional Classification for Multi-Sensor Time Series Data"></a>DOMINO: Domain-invariant Hyperdimensional Classification for Multi-Sensor Time Series Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03295">http://arxiv.org/abs/2308.03295</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyao Wang, Luke Chen, Mohammad Abdullah Al Faruque</li>
<li>for: 本研究旨在解决智能设备边缘上的数据驱动机器学习（ML）方法中的分布shift问题，以提高多感器时序数据的分类性能。</li>
<li>methods: 本文提出了一种名为DOMINO的神经元驱动计算（HDC）学习框架，利用高维度空间的有效并行矩阵运算来动态标识和筛选域variant维度。</li>
<li>results: 对多种多感器时序分类任务进行了广泛的评估，结果表明DOMINO比状态 Künstler（SOTA）的域泛化技术高出2.04%的准确率，并在训练和推理中具有16.34倍和2.89倍的速度优势。此外，DOMINO在部分标注和高度不均衡数据上进行学习时表现更加出众，对硬件噪声的抗衡性提高了10.93倍。<details>
<summary>Abstract</summary>
With the rapid evolution of the Internet of Things, many real-world applications utilize heterogeneously connected sensors to capture time-series information. Edge-based machine learning (ML) methodologies are often employed to analyze locally collected data. However, a fundamental issue across data-driven ML approaches is distribution shift. It occurs when a model is deployed on a data distribution different from what it was trained on, and can substantially degrade model performance. Additionally, increasingly sophisticated deep neural networks (DNNs) have been proposed to capture spatial and temporal dependencies in multi-sensor time series data, requiring intensive computational resources beyond the capacity of today's edge devices. While brain-inspired hyperdimensional computing (HDC) has been introduced as a lightweight solution for edge-based learning, existing HDCs are also vulnerable to the distribution shift challenge. In this paper, we propose DOMINO, a novel HDC learning framework addressing the distribution shift problem in noisy multi-sensor time-series data. DOMINO leverages efficient and parallel matrix operations on high-dimensional space to dynamically identify and filter out domain-variant dimensions. Our evaluation on a wide range of multi-sensor time series classification tasks shows that DOMINO achieves on average 2.04% higher accuracy than state-of-the-art (SOTA) DNN-based domain generalization techniques, and delivers 16.34x faster training and 2.89x faster inference. More importantly, DOMINO performs notably better when learning from partially labeled and highly imbalanced data, providing 10.93x higher robustness against hardware noises than SOTA DNNs.
</details>
<details>
<summary>摘要</summary>
随着互联网物联网的快速发展，许多现实世界应用程序利用不同种类的传感器来 capture 时间序列信息。边缘基于机器学习（ML）方法ologies 常常被用来分析本地收集的数据。然而，跨数据频道的分布shift 问题是数据驱动的 ML 方法ologies 中的一个基本问题。随着时间序列数据中的空间和时间相关性的不断提高，使用深度神经网络（DNNs）来捕捉这些相关性已成为一项核心的技术。然而，这些深度神经网络的计算资源需求已超出当今边缘设备的处理能力。在此基础上，我们提出了 DOMINO，一种新的幂 dimensional computing（HDC）学习框架，解决跨数据频道分布shift 问题在干扰多感知时序数据中。DOMINO 利用高维度空间中效率和并行的矩阵操作，动态标识和筛选域variant 维度。我们对多种多感知时序分类任务进行了广泛的评估，结果显示，DOMINO 在 average 比 state-of-the-art（SOTA） DNN-based 域泛化技术上 achieve 2.04% 高的准确率，并提供 16.34x  faster 训练和 2.89x  faster 推理。此外，DOMINO 在 learning 从 partially 标注和高度不均衡的数据中表现更为出色，提供 10.93x 更高的硬件噪音鲁减能力。
</details></li>
</ul>
<hr>
<h2 id="SynJax-Structured-Probability-Distributions-for-JAX"><a href="#SynJax-Structured-Probability-Distributions-for-JAX" class="headerlink" title="SynJax: Structured Probability Distributions for JAX"></a>SynJax: Structured Probability Distributions for JAX</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03291">http://arxiv.org/abs/2308.03291</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/deepmind/synjax">https://github.com/deepmind/synjax</a></li>
<li>paper_authors: Miloš Stanojević, Laurent Sartran</li>
<li>for: 该论文旨在提供一种高效的 вектор化实现方法，以便在大规模的深度学习模型中直接表示数据中的结构。</li>
<li>methods: 该论文使用了SynJax库，该库提供了一种高效的 вектор化实现方法，用于处理包含结构的分布的各种推理算法，如对齐、标记、分割、成分树和覆盖树。</li>
<li>results: 该论文通过使用SynJax库，实现了一种大规模的可导 differentiable 模型，可以直接表示数据中的结构，并且可以在现代硬件加速器上实现高效的推理。<details>
<summary>Abstract</summary>
The development of deep learning software libraries enabled significant progress in the field by allowing users to focus on modeling, while letting the library to take care of the tedious and time-consuming task of optimizing execution for modern hardware accelerators. However, this has benefited only particular types of deep learning models, such as Transformers, whose primitives map easily to the vectorized computation. The models that explicitly account for structured objects, such as trees and segmentations, did not benefit equally because they require custom algorithms that are difficult to implement in a vectorized form.   SynJax directly addresses this problem by providing an efficient vectorized implementation of inference algorithms for structured distributions covering alignment, tagging, segmentation, constituency trees and spanning trees. With SynJax we can build large-scale differentiable models that explicitly model structure in the data. The code is available at https://github.com/deepmind/synjax.
</details>
<details>
<summary>摘要</summary>
通过深度学习软件库的发展，在这个领域取得了重要进步，让用户可以专注于模型设计，让库负责处理现代硬件加速器的繁琐和耗时任务。然而，这主要对特定类型的深度学习模型带来了好处，如转换器，这些模型的基本 primitives 可以轻松地vector化计算。然而，模型处理结构化对象的模型，如树和分割，没有得到了相同的好处，因为它们需要特定的算法，difficult to implement in a vectorized form。SynJax直接解决了这个问题，提供了高效的vectorized实现方式，用于推理算法，包括对适配、标记、分割、树和span树的推理。通过SynJax，我们可以构建大规模可导的模型，并且直接模型数据中的结构。代码可以在https://github.com/deepmind/synjax上获取。
</details></li>
</ul>
<hr>
<h2 id="Local-Structure-aware-Graph-Contrastive-Representation-Learning"><a href="#Local-Structure-aware-Graph-Contrastive-Representation-Learning" class="headerlink" title="Local Structure-aware Graph Contrastive Representation Learning"></a>Local Structure-aware Graph Contrastive Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03271">http://arxiv.org/abs/2308.03271</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kai Yang, Yuan Liu, Zijuan Zhao, Peijin Ding, Wenqian Zhao</li>
<li>for: 本研究提出了一种Local Structure-aware Graph Contrastive representation Learning方法 (LS-GCL)，用于模elling多视图结构信息。</li>
<li>methods: 本方法使用了 semantic subgraphs，不限于首orde neighborhood，并使用了共享GNN编码器来学习目标节点嵌入。还使用了一个pooling函数来生成子图层级图像嵌入。</li>
<li>results: 实验结果表明，LS-GCL方法在五个数据集上的表现比前一些状态对比较高，在节点分类和链接预测任务上都达到了更好的效果。<details>
<summary>Abstract</summary>
Traditional Graph Neural Network (GNN), as a graph representation learning method, is constrained by label information. However, Graph Contrastive Learning (GCL) methods, which tackle the label problem effectively, mainly focus on the feature information of the global graph or small subgraph structure (e.g., the first-order neighborhood). In the paper, we propose a Local Structure-aware Graph Contrastive representation Learning method (LS-GCL) to model the structural information of nodes from multiple views. Specifically, we construct the semantic subgraphs that are not limited to the first-order neighbors. For the local view, the semantic subgraph of each target node is input into a shared GNN encoder to obtain the target node embeddings at the subgraph-level. Then, we use a pooling function to generate the subgraph-level graph embeddings. For the global view, considering the original graph preserves indispensable semantic information of nodes, we leverage the shared GNN encoder to learn the target node embeddings at the global graph-level. The proposed LS-GCL model is optimized to maximize the common information among similar instances at three various perspectives through a multi-level contrastive loss function. Experimental results on five datasets illustrate that our method outperforms state-of-the-art graph representation learning approaches for both node classification and link prediction tasks.
</details>
<details>
<summary>摘要</summary>
传统的图 нейрон网络（GNN）在图表示学习方法中受标签信息的限制。然而，图对照学习（GCL）方法，可以有效地解决标签问题，主要集中于全图或小子图结构（例如，首先邻居）的特征信息。在本文中，我们提出了一种本地结构意识感知的图对照学习表示学习方法（LS-GCL），用于模型节点的多视图结构信息。具体来说，我们构建了不限于首先邻居的semantic子图。对本地视图，每个目标节点的semantic子图将输入到共享GNNEncoder中，以获取目标节点的子图级别表示。然后，我们使用一个池化函数生成子graph级别的图编码。对全球视图，由于原始图保留了节点的必要 semantic信息，我们利用共享GNNEncoder来学习目标节点的全图级别表示。我们提出的LS-GCL模型通过最大化三个不同视角的共同信息来优化多级对照损失函数来进行优化。实验结果表明，我们的方法在五个数据集上比 estado-of-the-art的图表示学习方法出色地进行节点分类和链接预测任务。
</details></li>
</ul>
<hr>
<h2 id="Simple-Rule-Injection-for-ComplEx-Embeddings"><a href="#Simple-Rule-Injection-for-ComplEx-Embeddings" class="headerlink" title="Simple Rule Injection for ComplEx Embeddings"></a>Simple Rule Injection for ComplEx Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03269">http://arxiv.org/abs/2308.03269</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haodi Ma, Anthony Colas, Yuejie Wang, Ali Sadeghian, Daisy Zhe Wang</li>
<li>for: This paper is written for researchers and practitioners interested in neural knowledge graph inference, particularly those looking to combine logic rules with knowledge graph embeddings.</li>
<li>methods: The paper proposes a mechanism called InjEx, which injects multiple types of rules through simple constraints to capture definite Horn rules.</li>
<li>results: The paper evaluates InjEx on both the knowledge graph completion (KGC) and few-shot knowledge graph completion (FKGC) settings, and shows that it outperforms baseline KGC models as well as specialized few-shot models while maintaining its scalability and efficiency.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了探讨神经知识图推理而写的，尤其是将逻辑规则与知识图嵌入结合起来的研究人员和实践者。</li>
<li>methods: 论文提出了一种机制called InjEx，通过简单的约束来捕捉definite Horn规则。</li>
<li>results: 论文在知识图完成(KGC)和少量知识图完成(FKGC)设置下进行了实验，并证明InjEx可以超越基eline KGC模型以及特化的少量模型，同时保持了其扩展性和效率。<details>
<summary>Abstract</summary>
Recent works in neural knowledge graph inference attempt to combine logic rules with knowledge graph embeddings to benefit from prior knowledge. However, they usually cannot avoid rule grounding, and injecting a diverse set of rules has still not been thoroughly explored. In this work, we propose InjEx, a mechanism to inject multiple types of rules through simple constraints, which capture definite Horn rules. To start, we theoretically prove that InjEx can inject such rules. Next, to demonstrate that InjEx infuses interpretable prior knowledge into the embedding space, we evaluate InjEx on both the knowledge graph completion (KGC) and few-shot knowledge graph completion (FKGC) settings. Our experimental results reveal that InjEx outperforms both baseline KGC models as well as specialized few-shot models while maintaining its scalability and efficiency.
</details>
<details>
<summary>摘要</summary>
最近的 neural knowledge graph inference 研究尝试将逻辑规则与知识图 embedding 结合以获得优势。然而，它们通常无法避免规则定义，并尚未全面探讨多种规则的混合。在这个工作中，我们提出了 InjEx，一种可以通过简单的约束将多种类型的规则注入到 embedding 空间中的机制。首先，我们理论上证明了 InjEx 可以注入这些规则。然后，我们通过在知识图完成 (KGC) 和少量知识图完成 (FKGC) 设置中评估 InjEx，发现它可以让知识图中的 embedding 空间具有可读性和可理解性。我们的实验结果表明，InjEx 可以比基eline KGC 模型和专门的几何shot模型表现更好，同时保持其可扩展性和效率。
</details></li>
</ul>
<hr>
<h2 id="Redundancy-aware-Transformer-for-Video-Question-Answering"><a href="#Redundancy-aware-Transformer-for-Video-Question-Answering" class="headerlink" title="Redundancy-aware Transformer for Video Question Answering"></a>Redundancy-aware Transformer for Video Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03267">http://arxiv.org/abs/2308.03267</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yicong Li, Xun Yang, An Zhang, Chun Feng, Xiang Wang, Tat-Seng Chua</li>
<li>for: 本研究旨在提高VideoQA中的准确率和效率，通过避免邻帧重复和交叉模态重复。</li>
<li>methods: 提出一种基于变换器的新架构，通过强调邻帧中对象水平的变化，以及对不同模式的自适应采样，来解决邻帧重复和交叉模态重复。</li>
<li>results: 通过对多个VideoQA benchmark进行测试，发现该方法可以达到当前最佳的结果。<details>
<summary>Abstract</summary>
This paper identifies two kinds of redundancy in the current VideoQA paradigm. Specifically, the current video encoders tend to holistically embed all video clues at different granularities in a hierarchical manner, which inevitably introduces \textit{neighboring-frame redundancy} that can overwhelm detailed visual clues at the object level. Subsequently, prevailing vision-language fusion designs introduce the \textit{cross-modal redundancy} by exhaustively fusing all visual elements with question tokens without explicitly differentiating their pairwise vision-language interactions, thus making a pernicious impact on the answering.   To this end, we propose a novel transformer-based architecture, that aims to model VideoQA in a redundancy-aware manner. To address the neighboring-frame redundancy, we introduce a video encoder structure that emphasizes the object-level change in neighboring frames, while adopting an out-of-neighboring message-passing scheme that imposes attention only on distant frames. As for the cross-modal redundancy, we equip our fusion module with a novel adaptive sampling, which explicitly differentiates the vision-language interactions by identifying a small subset of visual elements that exclusively support the answer. Upon these advancements, we find this \underline{R}edundancy-\underline{a}ware trans\underline{former} (RaFormer) can achieve state-of-the-art results on multiple VideoQA benchmarks.
</details>
<details>
<summary>摘要</summary>
To address these issues, we propose a novel transformer-based architecture that models VideoQA in a redundancy-aware manner. To reduce neighboring-frame redundancy, we introduce a video encoder structure that emphasizes object-level changes in neighboring frames and adopts an out-of-neighboring message-passing scheme that only attends to distant frames. To address cross-modal redundancy, we equip our fusion module with a novel adaptive sampling that explicitly differentiates vision-language interactions by identifying a small subset of visual elements that exclusively support the answer.Our proposed \underline{R}edundancy-\underline{a}ware transformer (RaFormer) achieves state-of-the-art results on multiple VideoQA benchmarks.
</details></li>
</ul>
<hr>
<h2 id="TempFuser-Learning-Tactical-and-Agile-Flight-Maneuvers-in-Aerial-Dogfights-using-a-Long-Short-Term-Temporal-Fusion-Transformer"><a href="#TempFuser-Learning-Tactical-and-Agile-Flight-Maneuvers-in-Aerial-Dogfights-using-a-Long-Short-Term-Temporal-Fusion-Transformer" class="headerlink" title="TempFuser: Learning Tactical and Agile Flight Maneuvers in Aerial Dogfights using a Long Short-Term Temporal Fusion Transformer"></a>TempFuser: Learning Tactical and Agile Flight Maneuvers in Aerial Dogfights using a Long Short-Term Temporal Fusion Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03257">http://arxiv.org/abs/2308.03257</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyunki Seong, David Hyunchul Shim</li>
<li>for: 该论文旨在提出一种基于长期征文特征和短期动态特征的整体飞行策略模型，用于解决空中战斗中的机动和战术飞行问题。</li>
<li>methods: 该方法使用两个LSTM基于输入嵌入来编码长期稀缺状态轨迹，以及短期密集状态轨迹。通过将两个嵌入者 integrate through transformer编码器，方法subsequently derivese终端飞行命令。</li>
<li>results: 该模型在具有多种反对机型的高精度环境中进行了广泛验证，并证明了它在机动和战术飞行方面的表现超过了基准模型。该模型成功地学习了基本飞行招数、人工驾驶员式战术招数和在低空下的稳定追逐。视频可以在 \url{<a target="_blank" rel="noopener" href="https://sites.google.com/view/tempfuser%7D">https://sites.google.com/view/tempfuser}</a> 上查看。<details>
<summary>Abstract</summary>
Aerial dogfights necessitate understanding the tactically changing maneuvers from a long-term perspective, along with the rapidly changing aerodynamics from a short-term view. In this paper, we propose a novel long short-term temporal fusion transformer (TempFuser) for a policy network in aerial dogfights. Our method uses two LSTM-based input embeddings to encode long-term, sparse state trajectories, as well as short-term, dense state trajectories. By integrating the two embeddings through a transformer encoder, the method subsequently derives end-to-end flight commands for agile and tactical maneuvers. We formulate a deep reinforcement learning framework to train our TempFuser-based policy model. We then extensively validate our model, demonstrating that it outperforms other baseline models against a diverse range of opponent aircraft in a high-fidelity environment. Our model successfully learns basic fighter maneuvers, human pilot-like tactical maneuvers, and robust supersonic pursuit in low altitudes without explicitly coded prior knowledge. Videos are available at \url{https://sites.google.com/view/tempfuser}
</details>
<details>
<summary>摘要</summary>
aerial dogfights require understanding the tactically changing maneuvers from a long-term perspective, as well as the rapidly changing aerodynamics from a short-term view. In this paper, we propose a novel long short-term temporal fusion transformer (TempFuser) for a policy network in aerial dogfights. Our method uses two LSTM-based input embeddings to encode long-term, sparse state trajectories, as well as short-term, dense state trajectories. By integrating the two embeddings through a transformer encoder, the method subsequently derives end-to-end flight commands for agile and tactical maneuvers. We formulate a deep reinforcement learning framework to train our TempFuser-based policy model. We then extensively validate our model, demonstrating that it outperforms other baseline models against a diverse range of opponent aircraft in a high-fidelity environment. Our model successfully learns basic fighter maneuvers, human pilot-like tactical maneuvers, and robust supersonic pursuit in low altitudes without explicitly coded prior knowledge. Videos are available at \url{https://sites.google.com/view/tempfuser}Here's the Chinese text with traditional Chinese characters:空中 dogfight 需要从长期perspective理解战术上的变化，以及短期view的 aerodynamics 变化。在这篇文章中，我们提出一个 novel long short-term temporal fusion transformer (TempFuser) 作为policy network的一部分。我们的方法使用两个 LSTM 基于的输入嵌入来编码长期、稀疏的状态轨迹，以及短期、密集的状态轨迹。通过将两个嵌入器组合成一个 transformer Encoder，方法随后 derivation 终端的飞行命令。我们建立了一个深度强化学习框架，用于训练我们的 TempFuser 基于的政策模型。我们然后广泛验证我们的模型，证明它在高质量环境中比基eline模型高效。我们的模型成功地学习了基本战斗机动、人类飞行员式的战术机动和在低高度中Robust supersonic pursuit 无需显式编程优先知识。影片可以在 \url{https://sites.google.com/view/tempfuser} 上找到。
</details></li>
</ul>
<hr>
<h2 id="PaniniQA-Enhancing-Patient-Education-Through-Interactive-Question-Answering"><a href="#PaniniQA-Enhancing-Patient-Education-Through-Interactive-Question-Answering" class="headerlink" title="PaniniQA: Enhancing Patient Education Through Interactive Question Answering"></a>PaniniQA: Enhancing Patient Education Through Interactive Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03253">http://arxiv.org/abs/2308.03253</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pengshancai/paniniqa">https://github.com/pengshancai/paniniqa</a></li>
<li>paper_authors: Pengshan Cai, Zonghai Yao, Fei Liu, Dakuo Wang, Meghan Reilly, Huixue Zhou, Lingxi Li, Yi Cao, Alok Kapoor, Adarsha Bajracharya, Dan Berlowitz, Hong Yu</li>
<li>for: 帮助病人理解医疗记录中的个性化出院指南</li>
<li>methods: 使用人类中心的问答系统，从病人的出院指南中提取重要的医疗内容，并为病人提供个性化的教育问题</li>
<li>results: 通过自动和人工评估，表明PaniniQA可以有效地帮助病人理解和记忆医疗指南，提高病人的医疗知识和自信心<details>
<summary>Abstract</summary>
Patient portal allows discharged patients to access their personalized discharge instructions in electronic health records (EHRs). However, many patients have difficulty understanding or memorizing their discharge instructions. In this paper, we present PaniniQA, a patient-centric interactive question answering system designed to help patients understand their discharge instructions. PaniniQA first identifies important clinical content from patients' discharge instructions and then formulates patient-specific educational questions. In addition, PaniniQA is also equipped with answer verification functionality to provide timely feedback to correct patients' misunderstandings. Our comprehensive automatic and human evaluation results demonstrate our PaniniQA is capable of improving patients' mastery of their medical instructions through effective interactions
</details>
<details>
<summary>摘要</summary>
患者门户 permet 出院患者访问其个性化出院指南在电子医疗纪录 (EHR) 中。然而，许多患者困难理解或记忆出院指南。在这篇论文中，我们介绍 PaniniQA，一个患者中心的交互问答系统，用于帮助患者理解出院指南。PaniniQA 首先从患者的出院指南中提取重要的医疗内容，然后根据患者的个性特点制定特定的教育问题。此外，PaniniQA 还具有答案验证功能，以提供及时的反馈，以正式误解。我们的全面的自动和人工评估结果表明，PaniniQA 能够通过有效的互动提高患者对医疗指南的理解。
</details></li>
</ul>
<hr>
<h2 id="Analysis-of-Optical-Loss-and-Crosstalk-Noise-in-MZI-based-Coherent-Photonic-Neural-Networks"><a href="#Analysis-of-Optical-Loss-and-Crosstalk-Noise-in-MZI-based-Coherent-Photonic-Neural-Networks" class="headerlink" title="Analysis of Optical Loss and Crosstalk Noise in MZI-based Coherent Photonic Neural Networks"></a>Analysis of Optical Loss and Crosstalk Noise in MZI-based Coherent Photonic Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03249">http://arxiv.org/abs/2308.03249</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amin Shafiee, Sanmitra Banerjee, Krishnendu Chakrabarty, Sudeep Pasricha, Mahdi Nikdast<br>for: 这篇论文主要关注于提出了一个从底向上的模型，用于分析摄光网络（SP-NN）中各种实验设计对于损失和杂音的影响。methods: 本论文使用了一个从底向上的模型，从device层次到系统层次，以分析摄光网络中各种实验设计对于损失和杂音的影响。results: 本论文的结果显示，当SP-NN的规模增加时，损失和杂音的影响会逐渐增加，导致推论精度下降，甚至可以下降至10%以下。此外，本论文还给出了不同的MZI网络配置（如Reck、Clements和Diamond）的损失和杂音的分析结果。<details>
<summary>Abstract</summary>
With the continuous increase in the size and complexity of machine learning models, the need for specialized hardware to efficiently run such models is rapidly growing. To address such a need, silicon-photonic-based neural network (SP-NN) accelerators have recently emerged as a promising alternative to electronic accelerators due to their lower latency and higher energy efficiency. Not only can SP-NNs alleviate the fan-in and fan-out problem with linear algebra processors, their operational bandwidth can match that of the photodetection rate (typically 100 GHz), which is at least over an order of magnitude faster than electronic counterparts that are restricted to a clock rate of a few GHz. Unfortunately, the underlying silicon photonic devices in SP-NNs suffer from inherent optical losses and crosstalk noise originating from fabrication imperfections and undesired optical couplings, the impact of which accumulates as the network scales up. Consequently, the inferencing accuracy in an SP-NN can be affected by such inefficiencies -- e.g., can drop to below 10% -- the impact of which is yet to be fully studied. In this paper, we comprehensively model the optical loss and crosstalk noise using a bottom-up approach, from the device to the system level, in coherent SP-NNs built using Mach-Zehnder interferometer (MZI) devices. The proposed models can be applied to any SP-NN architecture with different configurations to analyze the effect of loss and crosstalk. Such an analysis is important where there are inferencing accuracy and scalability requirements to meet when designing an SP-NN. Using the proposed analytical framework, we show a high power penalty and a catastrophic inferencing accuracy drop of up to 84% for SP-NNs of different scales with three known MZI mesh configurations (i.e., Reck, Clements, and Diamond) due to accumulated optical loss and crosstalk noise.
</details>
<details>
<summary>摘要</summary>
随着机器学习模型的大小和复杂度不断增加，特化硬件来高效运行这些模型的需求也在不断增长。为了解决这种需求，silicon-photonic-based neural network（SP-NN）加速器在最近几年出现了，它们因其低延迟和高能效性而成为了电子加速器的有力竞争者。不仅可以使SP-NN解决线性代数处理器的缓冲和输出问题，其操作带宽可以与光检测速率（通常是100 GHz）相同，这是电子对手的多orders of magnitude更慢的速率。然而，在SP-NN中的silicon光学设备受到制造瑕疵和不良光学 Coupling的影响，这些影响会随着网络规模增加，从而影响SP-NN的推理精度。例如，推理精度可以降至下rance than 10%。在这篇论文中，我们从底层设备到系统层使用可靠的模型来模拟光损和十字谱噪。这些模型可以应用于不同的SP-NN架构，以分析光损和十字谱噪对推理精度的影响。这种分析对于设计SP-NN时存在推理精度和可扩展性的需求非常重要。使用我们提出的分析框架，我们显示了SP-NN的不同规模下的高电力负担和推理精度下降可达84%，这些下降都是由光损和十字谱噪所导致的。
</details></li>
</ul>
<hr>
<h2 id="Mind-the-Gap-Improving-Success-Rate-of-Vision-and-Language-Navigation-by-Revisiting-Oracle-Success-Routes"><a href="#Mind-the-Gap-Improving-Success-Rate-of-Vision-and-Language-Navigation-by-Revisiting-Oracle-Success-Routes" class="headerlink" title="Mind the Gap: Improving Success Rate of Vision-and-Language Navigation by Revisiting Oracle Success Routes"></a>Mind the Gap: Improving Success Rate of Vision-and-Language Navigation by Revisiting Oracle Success Routes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03244">http://arxiv.org/abs/2308.03244</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chongyang Zhao, Yuankai Qi, Qi Wu</li>
<li>for: 这个论文的目的是缓解成本精确 Navigation (VLN) 中的成本差（Success Rate，SR）和oracle成本差（Oracle Success Rate，OSR）之间的差距。</li>
<li>methods: 本论文提出了一个多模块 transformer 基本的模型，用于学习精确的路径视角表示，并使用这个表示来预测指令中的目标位置是否确实存在。</li>
<li>results: 本论文在三个通用的测试集（R2R、REVERIE 和 NDH）上进行评估，结果显示了可以实现更高的精确率和更低的失败率，显示了这个方法的潜力。<details>
<summary>Abstract</summary>
Vision-and-Language Navigation (VLN) aims to navigate to the target location by following a given instruction. Unlike existing methods focused on predicting a more accurate action at each step in navigation, in this paper, we make the first attempt to tackle a long-ignored problem in VLN: narrowing the gap between Success Rate (SR) and Oracle Success Rate (OSR). We observe a consistently large gap (up to 9%) on four state-of-the-art VLN methods across two benchmark datasets: R2R and REVERIE. The high OSR indicates the robot agent passes the target location, while the low SR suggests the agent actually fails to stop at the target location at last. Instead of predicting actions directly, we propose to mine the target location from a trajectory given by off-the-shelf VLN models. Specially, we design a multi-module transformer-based model for learning compact discriminative trajectory viewpoint representation, which is used to predict the confidence of being a target location as described in the instruction. The proposed method is evaluated on three widely-adopted datasets: R2R, REVERIE and NDH, and shows promising results, demonstrating the potential for more future research.
</details>
<details>
<summary>摘要</summary>
vision-and-language navigation (vlN) 目标是通过跟随给定的指令进行导航。与现有方法强调预测每步行动的准确性不同，在这篇论文中，我们首次尝试解决 vlN 中长期被忽略的问题：减少 SR 和 OSR 之间的差距。我们在两个基本样本数据集上（R2R 和 REVERIE）观察到一定的差距（最高达 9%）。高 OSR 表示机器人代理人在目标位置通过，而低 SR 则表示机器人代理人最终没有停止在目标位置。相比于直接预测行动，我们提议从 off-the-shelf VLN 模型获取的轨迹给出的路径视图表示来挖掘目标位置。我们设计了一种具有多模块的 transformer 基本模型，用于学习短暂而特征化的轨迹视图表示，以预测指令中所描述的目标位置是否准确。我们在 R2R、REVERIE 和 NDH 等三个广泛采用的数据集上进行评估，并取得了满意的结果，证明了我们的方法的潜在可能性。
</details></li>
</ul>
<hr>
<h2 id="Analysis-of-the-Evolution-of-Advanced-Transformer-Based-Language-Models-Experiments-on-Opinion-Mining"><a href="#Analysis-of-the-Evolution-of-Advanced-Transformer-Based-Language-Models-Experiments-on-Opinion-Mining" class="headerlink" title="Analysis of the Evolution of Advanced Transformer-Based Language Models: Experiments on Opinion Mining"></a>Analysis of the Evolution of Advanced Transformer-Based Language Models: Experiments on Opinion Mining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03235">http://arxiv.org/abs/2308.03235</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zekaouinoureddine/Opinion-Transformers">https://github.com/zekaouinoureddine/Opinion-Transformers</a></li>
<li>paper_authors: Nour Eddine Zekaoui, Siham Yousfi, Maryem Rhanoui, Mounia Mikram</li>
<li>For: 本研究旨在研究高性能自然语言处理（NLP）模型在观点挖掘（Opinion Mining）任务中的表现，并对比不同的Transformer-based语言模型。* Methods: 本研究使用了高性能的Transformer-based语言模型，包括BERT、RoBERTa和XLNet等，对多个语料库进行评价，并对比它们的性能。* Results: 研究结果显示，这些Transformer-based语言模型在观点挖掘任务中具有出色的表现，具体来说，BERT和RoBERTa在多个语料库中的平均准确率都高于90%，而XLNet的平均准确率则高于95%。<details>
<summary>Abstract</summary>
Opinion mining, also known as sentiment analysis, is a subfield of natural language processing (NLP) that focuses on identifying and extracting subjective information in textual material. This can include determining the overall sentiment of a piece of text (e.g., positive or negative), as well as identifying specific emotions or opinions expressed in the text, that involves the use of advanced machine and deep learning techniques. Recently, transformer-based language models make this task of human emotion analysis intuitive, thanks to the attention mechanism and parallel computation. These advantages make such models very powerful on linguistic tasks, unlike recurrent neural networks that spend a lot of time on sequential processing, making them prone to fail when it comes to processing long text. The scope of our paper aims to study the behaviour of the cutting-edge Transformer-based language models on opinion mining and provide a high-level comparison between them to highlight their key particularities. Additionally, our comparative study shows leads and paves the way for production engineers regarding the approach to focus on and is useful for researchers as it provides guidelines for future research subjects.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Why-Linguistics-Will-Thrive-in-the-21st-Century-A-Reply-to-Piantadosi-2023"><a href="#Why-Linguistics-Will-Thrive-in-the-21st-Century-A-Reply-to-Piantadosi-2023" class="headerlink" title="Why Linguistics Will Thrive in the 21st Century: A Reply to Piantadosi (2023)"></a>Why Linguistics Will Thrive in the 21st Century: A Reply to Piantadosi (2023)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03228">http://arxiv.org/abs/2308.03228</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jordan Kodner, Sarah Payne, Jeffrey Heinz</li>
<li>for: 本文批判Piantadosi（2023）声称“现代语言模型推翻了昌斯基的语言方法”，关注四个主要点。</li>
<li>methods: 本文使用了大语言模型（LLM）的启示和实用性，以及语言学习的中心谜团。</li>
<li>results: 本文结论是，尽管LLMs具有启示和实用性，但人类语言学习的谜团仍未被解释。此外，LLMs无法提供解释性的科学理论，因此generative linguistics仍将是21世纪和以后不可或缺的科学 дисциплины。<details>
<summary>Abstract</summary>
We present a critical assessment of Piantadosi's (2023) claim that "Modern language models refute Chomsky's approach to language," focusing on four main points. First, despite the impressive performance and utility of large language models (LLMs), humans achieve their capacity for language after exposure to several orders of magnitude less data. The fact that young children become competent, fluent speakers of their native languages with relatively little exposure to them is the central mystery of language learning to which Chomsky initially drew attention, and LLMs currently show little promise of solving this mystery. Second, what can the artificial reveal about the natural? Put simply, the implications of LLMs for our understanding of the cognitive structures and mechanisms underlying language and its acquisition are like the implications of airplanes for understanding how birds fly. Third, LLMs cannot constitute scientific theories of language for several reasons, not least of which is that scientific theories must provide interpretable explanations, not just predictions. This leads to our final point: to even determine whether the linguistic and cognitive capabilities of LLMs rival those of humans requires explicating what humans' capacities actually are. In other words, it requires a separate theory of language and cognition; generative linguistics provides precisely such a theory. As such, we conclude that generative linguistics as a scientific discipline will remain indispensable throughout the 21st century and beyond.
</details>
<details>
<summary>摘要</summary>
我们提出了对Piantadosi（2023）的批判，关注四个主要点。首先，虽然大型语言模型（LLMs）表现出色，但人类通过相对较少的数据来获得语言能力。儿童在获得native语言 fluency的过程中需要相对较少的数据，这是语言学习中的中心谜题，LLMs目前没有解决这个谜题。二、人工智能可以揭示自然语言吗？将airplanes作为 birds fly的 analogie，LLMs对我们对语言和其学习机制的理解提供了什么？三、LLMs无法构成语言科学的理论，因为科学理论需要可解释的结果，不仅仅是预测。这导致我们的最后一点：要确定LLMs的语言和认知能力与人类相比，首先需要解释人类的能力。在其他 palabras，我们需要一个分析语言和认知的理论，生成语言学派 precisely 提供了这样的理论。因此，我们结论是，生成语言学派作为科学领域将在21世纪和以后保持不可或缺的。
</details></li>
</ul>
<hr>
<h2 id="Investigation-of-Self-supervised-Pre-trained-Models-for-Classification-of-Voice-Quality-from-Speech-and-Neck-Surface-Accelerometer-Signals"><a href="#Investigation-of-Self-supervised-Pre-trained-Models-for-Classification-of-Voice-Quality-from-Speech-and-Neck-Surface-Accelerometer-Signals" class="headerlink" title="Investigation of Self-supervised Pre-trained Models for Classification of Voice Quality from Speech and Neck Surface Accelerometer Signals"></a>Investigation of Self-supervised Pre-trained Models for Classification of Voice Quality from Speech and Neck Surface Accelerometer Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03226">http://arxiv.org/abs/2308.03226</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sudarsana Reddy Kadiri, Farhad Javanmardi, Paavo Alku<br>for:* 这项研究旨在 классифика voice quality（呼吸声、Modal声和压缩声）的自动分类方法。methods:* 这项研究使用了同时记录的语音和脖子振荡器（NSA）信号作为输入，并提取了MFCCs和glottal source features。results:* 研究发现，使用 NSA 输入可以获得更好的分类性能，而且使用 pre-trained 模型基于的特征（wav2vec2-BASE、wav2vec2-LARGE 和 HuBERT）可以提高分类精度。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Prior studies in the automatic classification of voice quality have mainly studied the use of the acoustic speech signal as input. Recently, a few studies have been carried out by jointly using both speech and neck surface accelerometer (NSA) signals as inputs, and by extracting MFCCs and glottal source features. This study examines simultaneously-recorded speech and NSA signals in the classification of voice quality (breathy, modal, and pressed) using features derived from three self-supervised pre-trained models (wav2vec2-BASE, wav2vec2-LARGE, and HuBERT) and using a SVM as well as CNNs as classifiers. Furthermore, the effectiveness of the pre-trained models is compared in feature extraction between glottal source waveforms and raw signal waveforms for both speech and NSA inputs. Using two signal processing methods (quasi-closed phase (QCP) glottal inverse filtering and zero frequency filtering (ZFF)), glottal source waveforms are estimated from both speech and NSA signals. The study has three main goals: (1) to study whether features derived from pre-trained models improve classification accuracy compared to conventional features (spectrogram, mel-spectrogram, MFCCs, i-vector, and x-vector), (2) to investigate which of the two modalities (speech vs. NSA) is more effective in the classification task with pre-trained model-based features, and (3) to evaluate whether the deep learning-based CNN classifier can enhance the classification accuracy in comparison to the SVM classifier. The results revealed that the use of the NSA input showed better classification performance compared to the speech signal. Between the features, the pre-trained model-based features showed better classification accuracies, both for speech and NSA inputs compared to the conventional features. It was also found that the HuBERT features performed better than the wav2vec2-BASE and wav2vec2-LARGE features.
</details>
<details>
<summary>摘要</summary>
前期研究在自动识别语音质量方面主要使用语音音频信号作为输入。近年来，一些研究开始将语音和脖子表面加速度信号（NSA）作为输入并提取MFCC和格斗音源特征。本研究通过同时记录的语音和NSA信号进行语音质量（呼吸、模态和压缩）的分类，使用三个自动预训练模型（wav2vec2-BASE、wav2vec2-LARGE和HuBERT）提取特征，并使用支持向量机（SVM）和卷积神经网络（CNN）作为分类器。此外，采用不同的信号处理方法（ quasi-closed phase 预测频率滤波和零频率滤波）来估计语音和NSA信号的格斗音源波形。研究拥有三个主要目标：1. 研究是否可以通过使用预训练模型提取特征来提高分类精度，比较传统特征（spectrogram、mel-spectrogram、MFCC、i-vector和x-vector）的表现。2. investigate 语音和NSA信号中哪一个模式更有效iveness 在分类任务中，并且是否可以通过预训练模型基于特征来确定这一点。3. 评估深度学习基于CNN的分类器是否可以提高分类精度，与支持向量机（SVM）分类器相比。研究结果表明，使用NSA输入可以更好地分类语音质量，而且采用预训练模型基于特征可以提高分类精度，对于语音和NSA输入都有着优异表现。此外，HuBERT特征也被发现比wav2vec2-BASE和wav2vec2-LARGE特征更为有效。
</details></li>
</ul>
<hr>
<h2 id="Source-free-Domain-Adaptive-Human-Pose-Estimation"><a href="#Source-free-Domain-Adaptive-Human-Pose-Estimation" class="headerlink" title="Source-free Domain Adaptive Human Pose Estimation"></a>Source-free Domain Adaptive Human Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03202">http://arxiv.org/abs/2308.03202</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/davidpengucf/sfdahpe">https://github.com/davidpengucf/sfdahpe</a></li>
<li>paper_authors: Qucheng Peng, Ce Zheng, Chen Chen</li>
<li>for: address the challenges of cross-domain learning of Human Pose Estimation (HPE) without access to source data during the adaptation process.</li>
<li>methods: proposed a novel framework that consists of three models: source model, intermediate model, and target model, which explores the task from both source-protect and target-relevant perspectives.</li>
<li>results: comprehensive experiments on several domain adaptive HPE benchmarks show that the proposed method outperforms existing approaches by a considerable margin.<details>
<summary>Abstract</summary>
Human Pose Estimation (HPE) is widely used in various fields, including motion analysis, healthcare, and virtual reality. However, the great expenses of labeled real-world datasets present a significant challenge for HPE. To overcome this, one approach is to train HPE models on synthetic datasets and then perform domain adaptation (DA) on real-world data. Unfortunately, existing DA methods for HPE neglect data privacy and security by using both source and target data in the adaptation process. To this end, we propose a new task, named source-free domain adaptive HPE, which aims to address the challenges of cross-domain learning of HPE without access to source data during the adaptation process. We further propose a novel framework that consists of three models: source model, intermediate model, and target model, which explores the task from both source-protect and target-relevant perspectives. The source-protect module preserves source information more effectively while resisting noise, and the target-relevant module reduces the sparsity of spatial representations by building a novel spatial probability space, and pose-specific contrastive learning and information maximization are proposed on the basis of this space. Comprehensive experiments on several domain adaptive HPE benchmarks show that the proposed method outperforms existing approaches by a considerable margin. The codes are available at https://github.com/davidpengucf/SFDAHPE.
</details>
<details>
<summary>摘要</summary>
人体姿态估计（HPE）在多个领域得到广泛应用，如动作分析、医疗和虚拟现实。然而，实际世界数据的高成本成为HPE的一大挑战。为解决这个问题，一种方法是在HPE模型上训练于synthetic数据，然后在实际数据上进行领域适应（DA）。然而，现有的DA方法 дляHPE忽视了数据隐私和安全性，通过使用源数据和目标数据在适应过程中使用。为此，我们提出了一个新任务，名为无源领域适应HPE，旨在解决HPE的跨领域学习问题，不需要在适应过程中访问源数据。我们还提出了一个新的框架，包括三个模型：源模型、中间模型和目标模型，该框架从源保护和目标相关两个角度出发，以提高适应效果。源保护模块更好地保留源信息，同时抵御噪声，目标相关模块减少了空间表示的稀疏性，通过建立一个新的空间概率空间，并在其基础上提出了pose特有的对比学习和信息最大化。我们对多个领域适应HPE的benchmark进行了广泛的实验，结果表明，我们提出的方法在现有方法的基础上具有较大的提升。代码可以在https://github.com/davidpengucf/SFDAHPE上获取。
</details></li>
</ul>
<hr>
<h2 id="Unmasking-the-Invisible-Finding-Location-Specific-Aggregated-Air-Quality-Index-with-Smartphone-Captured-Images"><a href="#Unmasking-the-Invisible-Finding-Location-Specific-Aggregated-Air-Quality-Index-with-Smartphone-Captured-Images" class="headerlink" title="Unmasking the Invisible: Finding Location-Specific Aggregated Air Quality Index with Smartphone-Captured Images"></a>Unmasking the Invisible: Finding Location-Specific Aggregated Air Quality Index with Smartphone-Captured Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03200">http://arxiv.org/abs/2308.03200</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joyanta Jyoti Mondal, Md. Farhadul Islam, Raima Islam, Nowsin Kabir Rhidi, A. B. M. Alim Al Islam, Meem Arafat Manab, Jannatun Noor</li>
<li>for: 这篇论文主要探讨了基于智能手机拍摄的空气质量指数预测技术，具体来说是使用深度卷积神经网络（DCNN）来预测达卡（Dhaka）的PM2.5浓度。</li>
<li>methods: 这篇论文使用了大量的户外图像和相应的PM2.5浓度数据来训练DCNN模型，并通过超vised学习来建立图像和PM2.5浓度之间的相关性指数。这种方法被称为“ Picture-based Predictor of PM2.5 Concentration”（PPPC）。</li>
<li>results: 试验结果表明，该模型在预测达卡的PM2.5浓度方面表现出色，比较流行的模型如ViT和INN以及CNN基本模型如VGG19、ResNet50和MobileNetV2都要出色。此外，该模型的资源利用率较高，只用了少量的参数。<details>
<summary>Abstract</summary>
The prevalence and mobility of smartphones make these a widely used tool for environmental health research. However, their potential for determining aggregated air quality index (AQI) based on PM2.5 concentration in specific locations remains largely unexplored in the existing literature. In this paper, we thoroughly examine the challenges associated with predicting location-specific PM2.5 concentration using images taken with smartphone cameras. The focus of our study is on Dhaka, the capital of Bangladesh, due to its significant air pollution levels and the large population exposed to it. Our research involves the development of a Deep Convolutional Neural Network (DCNN), which we train using over a thousand outdoor images taken and annotated. These photos are captured at various locations in Dhaka, and their labels are based on PM2.5 concentration data obtained from the local US consulate, calculated using the NowCast algorithm. Through supervised learning, our model establishes a correlation index during training, enhancing its ability to function as a Picture-based Predictor of PM2.5 Concentration (PPPC). This enables the algorithm to calculate an equivalent daily averaged AQI index from a smartphone image. Unlike, popular overly parameterized models, our model shows resource efficiency since it uses fewer parameters. Furthermore, test results indicate that our model outperforms popular models like ViT and INN, as well as popular CNN-based models such as VGG19, ResNet50, and MobileNetV2, in predicting location-specific PM2.5 concentration. Our dataset is the first publicly available collection that includes atmospheric images and corresponding PM2.5 measurements from Dhaka. Our code and dataset will be made public when publishing the paper.
</details>
<details>
<summary>摘要</summary>
智能手机的普遍和流动性使得它们成为了环境健康研究中广泛使用的工具。然而，智能手机在确定具体位置的空气质量指数（AQI）方面的潜在应用仍然在现有文献中得不到充分的探讨。本文 thorougly examine the challenges associated with predicting location-specific PM2.5 concentration using images taken with smartphone cameras.我们的研究对象是孟加拉国首都达卡，因为它的空气污染水平很高，并且有大量人口暴露在其中。我们的研究包括开发一个深度卷积神经网络（DCNN），我们使用超过一千个户外图像进行训练。这些图像在达卡各地拍摄，并将其标注为PM2.5浓度数据，该数据来自当地美国领事馆计算的NowCast算法。通过监督学习，我们的模型在训练期间建立了相关性指数，从而使得它可以作为图像基于预测PM2.5浓度的算法（PPPC）。这使得算法可以从智能手机图像中计算équivalent的日均AQI指数。与流行的过度参数化模型不同，我们的模型表现出资源有效性，因为它使用 fewer 参数。另外，测试结果表明，我们的模型在确定具体位置的PM2.5浓度方面比流行的ViT和INN模型，以及流行的CNN基本模型如VGG19、ResNet50和MobileNetV2，表现出色。我们的数据集是首次公共可用的，包括达卡的大气图像和相应PM2.5测量数据。我们的代码和数据将在发表论文时公开。
</details></li>
</ul>
<hr>
<h2 id="Automatically-Correcting-Large-Language-Models-Surveying-the-landscape-of-diverse-self-correction-strategies"><a href="#Automatically-Correcting-Large-Language-Models-Surveying-the-landscape-of-diverse-self-correction-strategies" class="headerlink" title="Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies"></a>Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03188">http://arxiv.org/abs/2308.03188</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/teacherpeterpan/self-correction-llm-papers">https://github.com/teacherpeterpan/self-correction-llm-papers</a></li>
<li>paper_authors: Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, William Yang Wang</li>
<li>for: 本研究旨在探讨自动反馈技术的应用在大语言模型（LLM）中，以改进LLM的性能和可用性。</li>
<li>methods: 本研究使用了许多最新的研究，包括训练时间、生成时间和后期修正等方法，以探讨自动反馈技术的应用。</li>
<li>results: 研究发现自动反馈技术可以有效地改进LLM的性能和可用性，但还存在一些挑战和未来的发展方向。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated remarkable performance across a wide array of NLP tasks. However, their efficacy is undermined by undesired and inconsistent behaviors, including hallucination, unfaithful reasoning, and toxic content. A promising approach to rectify these flaws is self-correction, where the LLM itself is prompted or guided to fix problems in its own output. Techniques leveraging automated feedback -- either produced by the LLM itself or some external system -- are of particular interest as they are a promising way to make LLM-based solutions more practical and deployable with minimal human feedback. This paper presents a comprehensive review of this emerging class of techniques. We analyze and taxonomize a wide array of recent work utilizing these strategies, including training-time, generation-time, and post-hoc correction. We also summarize the major applications of this strategy and conclude by discussing future directions and challenges.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Large language models" (LLMs) is translated as "大型语言模型" (dàxíng yǔyán módelǐ)* "NLP tasks" is translated as "自然语言处理任务" (zìrán yǔyán xiǎnggōng zhìdao)* "hallucination" is translated as "幻见" (hénjiàn)* "unfaithful reasoning" is translated as "不诚实的推理" (bùzhèngshí de tuīlǐ)* "toxic content" is translated as "毒害内容" (dāohài nèixìng)* "self-correction" is translated as "自动更正" (zìdòng gengzhèng)* "automated feedback" is translated as "自动反馈" (zìdòng fāngxiàn)* "training-time" is translated as "训练时间" (xùnxīn shíjiān)* "generation-time" is translated as "生成时间" (shēngchǎn shíjiān)* "post-hoc correction" is translated as "后续更正" (hòu xiù gengzhèng)* "major applications" is translated as "主要应用" (zhǔyào yìngyù)* "future directions" is translated as "未来方向" (wèilái fāngdìng)* "challenges" is translated as "挑战" (tiǎozhàng)
</details></li>
</ul>
<hr>
<h2 id="VN-Solver-Vision-based-Neural-Solver-for-Combinatorial-Optimization-over-Graphs"><a href="#VN-Solver-Vision-based-Neural-Solver-for-Combinatorial-Optimization-over-Graphs" class="headerlink" title="VN-Solver: Vision-based Neural Solver for Combinatorial Optimization over Graphs"></a>VN-Solver: Vision-based Neural Solver for Combinatorial Optimization over Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03185">http://arxiv.org/abs/2308.03185</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/minasmz/VN-Solver">https://github.com/minasmz/VN-Solver</a></li>
<li>paper_authors: Mina Samizadeh, Guangmo Tong</li>
<li>for: 解决 combinatorial optimization 问题 over graphs，如旅行商问题和车辆路径问题。</li>
<li>methods: 使用视觉模型，不同于常见的 neural combinatorial solvers，可以通过对图像进行学习来解决图像问题。</li>
<li>results: 结果表明，这种视觉方法的性能不仅不低于Matrix-based方法，而且可以与其相比，开启了一新的数据驱动优化解决方法的avenue。<details>
<summary>Abstract</summary>
Data-driven approaches have been proven effective in solving combinatorial optimization problems over graphs such as the traveling salesman problems and the vehicle routing problem. The rationale behind such methods is that the input instances may follow distributions with salient patterns that can be leveraged to overcome the worst-case computational hardness. For optimization problems over graphs, the common practice of neural combinatorial solvers consumes the inputs in the form of adjacency matrices. In this paper, we explore a vision-based method that is conceptually novel: can neural models solve graph optimization problems by \textit{taking a look at the graph pattern}? Our results suggest that the performance of such vision-based methods is not only non-trivial but also comparable to the state-of-the-art matrix-based methods, which opens a new avenue for developing data-driven optimization solvers.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate the following text into Simplified Chinese:Data-driven approaches have been proven effective in solving combinatorial optimization problems over graphs such as the traveling salesman problems and the vehicle routing problem. The rationale behind such methods is that the input instances may follow distributions with salient patterns that can be leveraged to overcome the worst-case computational hardness. For optimization problems over graphs, the common practice of neural combinatorial solvers consumes the inputs in the form of adjacency matrices. In this paper, we explore a vision-based method that is conceptually novel: can neural models solve graph optimization problems by \textit{taking a look at the graph pattern}? Our results suggest that the performance of such vision-based methods is not only non-trivial but also comparable to the state-of-the-art matrix-based methods, which opens a new avenue for developing data-driven optimization solvers.Translate the text into Simplified Chinese:<</SYS>>Here's the translation:数据驱动方法在解决图上的 combinatorial 优化问题上已经得到证明，如旅行商问题和车辆路径问题。这种方法的基本思想是，输入实例可能会遵循一些突出的模式，这些模式可以用来缓解最坏情况的计算复杂性。在图上的优化问题上，常见的 neural  combinatorial 算法会将输入作为邻接矩阵来处理。在这篇论文中，我们探索了一种新的视觉基于的方法：可以 neural 模型通过 \textit{看看图形模式} 来解决图上的优化问题吗？我们的结果表明，这种视觉基于的方法不仅不rivial，而且与当前最佳的矩阵基于的方法相当，这开启了一个新的数据驱动优化算法的发展新途。
</details></li>
</ul>
<hr>
<h2 id="Empirical-Optimal-Risk-to-Quantify-Model-Trustworthiness-for-Failure-Detection"><a href="#Empirical-Optimal-Risk-to-Quantify-Model-Trustworthiness-for-Failure-Detection" class="headerlink" title="Empirical Optimal Risk to Quantify Model Trustworthiness for Failure Detection"></a>Empirical Optimal Risk to Quantify Model Trustworthiness for Failure Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03179">http://arxiv.org/abs/2308.03179</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuang Ao, Stefan Rueger, Advaith Siddharthan</li>
<li>For:  This paper focuses on the problem of failure detection (FD) in AI systems, specifically the evaluation of FD performance and the trade-offs between data coverage rate and performance on accepted data.* Methods:  The paper proposes two new evaluation metrics, the Excess Area Under the Optimal RC Curve (E-AUoptRC) and the Trust Index (TI), to better reflect the trustworthiness of FD models. These metrics are designed to provide a more intuitive and meaningful evaluation of FD performance, especially when the data coverage rate is partial.* Results:  The paper reports extensive experiments on three benchmark image datasets with ten variants of transformer and CNN models, demonstrating that the proposed methods can better reflect the model trustworthiness than existing evaluation metrics. The results also show that high overall accuracy does not always yield high TI, highlighting the necessity of the proposed Trust Index as a complementary metric to the model overall accuracy.<details>
<summary>Abstract</summary>
Failure detection (FD) in AI systems is a crucial safeguard for the deployment for safety-critical tasks. The common evaluation method of FD performance is the Risk-coverage (RC) curve, which reveals the trade-off between the data coverage rate and the performance on accepted data. One common way to quantify the RC curve by calculating the area under the RC curve. However, this metric does not inform on how suited any method is for FD, or what the optimal coverage rate should be. As FD aims to achieve higher performance with fewer data discarded, evaluating with partial coverage excluding the most uncertain samples is more intuitive and meaningful than full coverage. In addition, there is an optimal point in the coverage where the model could achieve ideal performance theoretically. We propose the Excess Area Under the Optimal RC Curve (E-AUoptRC), with the area in coverage from the optimal point to the full coverage. Further, the model performance at this optimal point can represent both model learning ability and calibration. We propose it as the Trust Index (TI), a complementary evaluation metric to the overall model accuracy. We report extensive experiments on three benchmark image datasets with ten variants of transformer and CNN models. Our results show that our proposed methods can better reflect the model trustworthiness than existing evaluation metrics. We further observe that the model with high overall accuracy does not always yield the high TI, which indicates the necessity of the proposed Trust Index as a complementary metric to the model overall accuracy. The code are available at \url{https://github.com/AoShuang92/optimal_risk}.
</details>
<details>
<summary>摘要</summary>
Failure detection (FD) 在人工智能系统中是一项重要的安全监测，用于安全关键任务的部署。通常的评估方法是风险覆盖率（RC）曲线，它显示了数据覆盖率和接受数据的性能之间的交易。但这个指标并不能告诉我们任务是否适合FD，也不能告诉我们应该选择的覆盖率是多少。因为FD的目标是通过少量数据来提高性能，所以评估 partial coverage，排除最不确定的样本更加直观和有意义。此外，存在最佳的覆盖率点， modelo可以在理论上实现最佳性能。我们提出了过余的风险覆盖曲线下的最佳点（E-AUoptRC），以及该点下的模型性能。我们认为这个指标是模型信任指数（TI），它是评估模型可靠性的 complementary 指标。我们在三个标准图像集上进行了广泛的实验，结果表明我们的提议方法可以更好地反映模型的可靠性。我们还发现，高度全局准确率并不总是导致高度信任指数，这说明了我们的信任指数是一个必要的 complementary 指标。代码可以在 GitHub 上找到：https://github.com/AoShuang92/optimal_risk。
</details></li>
</ul>
<hr>
<h2 id="Building-Safe-and-Reliable-AI-systems-for-Safety-Critical-Tasks-with-Vision-Language-Processing"><a href="#Building-Safe-and-Reliable-AI-systems-for-Safety-Critical-Tasks-with-Vision-Language-Processing" class="headerlink" title="Building Safe and Reliable AI systems for Safety Critical Tasks with Vision-Language Processing"></a>Building Safe and Reliable AI systems for Safety Critical Tasks with Vision-Language Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03176">http://arxiv.org/abs/2308.03176</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuang Ao</li>
<li>for: 这份论文的目的是提出一个安全可靠的人工智能系统，尤其是在安全敏感任务中。</li>
<li>methods: 本论文使用了许多已有的数据类型和开源benchmark数据集，并提出了一些改进现有技术的方法，以确保模型的不确定性量精确。</li>
<li>results: 本论文的结果显示，现有的人工智能系统无法正确地识别通用的失败原因，并且需要更多的技术来量化预测的质量。<details>
<summary>Abstract</summary>
Although AI systems have been applied in various fields and achieved impressive performance, their safety and reliability are still a big concern. This is especially important for safety-critical tasks. One shared characteristic of these critical tasks is their risk sensitivity, where small mistakes can cause big consequences and even endanger life. There are several factors that could be guidelines for the successful deployment of AI systems in sensitive tasks: (i) failure detection and out-of-distribution (OOD) detection; (ii) overfitting identification; (iii) uncertainty quantification for predictions; (iv) robustness to data perturbations. These factors are also challenges of current AI systems, which are major blocks for building safe and reliable AI. Specifically, the current AI algorithms are unable to identify common causes for failure detection. Furthermore, additional techniques are required to quantify the quality of predictions. All these contribute to inaccurate uncertainty quantification, which lowers trust in predictions. Hence obtaining accurate model uncertainty quantification and its further improvement are challenging. To address these issues, many techniques have been proposed, such as regularization methods and learning strategies. As vision and language are the most typical data type and have many open source benchmark datasets, this thesis will focus on vision-language data processing for tasks like classification, image captioning, and vision question answering. In this thesis, we aim to build a safeguard by further developing current techniques to ensure the accurate model uncertainty for safety-critical tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Failure detection and out-of-distribution (OOD) detection2. Overfitting identification3. Uncertainty quantification for predictions4. Robustness to data perturbationsCurrent AI algorithms are unable to identify common causes for failure detection and lack techniques to quantify the quality of predictions, leading to inaccurate uncertainty quantification and lower trust in predictions. To address these issues, many techniques have been proposed, such as regularization methods and learning strategies.In this thesis, we focus on vision-language data processing for tasks like classification, image captioning, and vision question answering. Our aim is to build a safeguard by further developing current techniques to ensure accurate model uncertainty for safety-critical tasks.</details></li>
</ol>
<hr>
<h2 id="Two-Sides-of-Miscalibration-Identifying-Over-and-Under-Confidence-Prediction-for-Network-Calibration"><a href="#Two-Sides-of-Miscalibration-Identifying-Over-and-Under-Confidence-Prediction-for-Network-Calibration" class="headerlink" title="Two Sides of Miscalibration: Identifying Over and Under-Confidence Prediction for Network Calibration"></a>Two Sides of Miscalibration: Identifying Over and Under-Confidence Prediction for Network Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03172">http://arxiv.org/abs/2308.03172</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aoshuang92/miscalibration_ts">https://github.com/aoshuang92/miscalibration_ts</a></li>
<li>paper_authors: Shuang Ao, Stefan Rueger, Advaith Siddharthan</li>
<li>for: 该论文旨在解决深度神经网络的准确性验证中的欠拟合问题，以确保模型的预测准确性。</li>
<li>methods: 该论文提出了一种新的评估 metric，用于评估模型的欠拟合状况，并利用这个 metric 设计了一种新的欠拟合方法，可以解决模型的过度和 Underdetermination 问题。</li>
<li>results: 该论文的实验结果显示，提出的方法可以substantially outperform existing calibration techniques，并且在一个自动故障检测任务中，提高了模型的可靠性和信任性。Here’s the full text in Simplified Chinese:</li>
<li>for: 该论文旨在解决深度神经网络的准确性验证中的欠拟合问题，以确保模型的预测准确性。</li>
<li>methods: 该论文提出了一种新的评估 metric，用于评估模型的欠拟合状况，并利用这个 metric 设计了一种新的欠拟合方法，可以解决模型的过度和 Underdetermination 问题。</li>
<li>results: 该论文的实验结果显示，提出的方法可以substantially outperform existing calibration techniques，并且在一个自动故障检测任务中，提高了模型的可靠性和信任性。<details>
<summary>Abstract</summary>
Proper confidence calibration of deep neural networks is essential for reliable predictions in safety-critical tasks. Miscalibration can lead to model over-confidence and/or under-confidence; i.e., the model's confidence in its prediction can be greater or less than the model's accuracy. Recent studies have highlighted the over-confidence issue by introducing calibration techniques and demonstrated success on various tasks. However, miscalibration through under-confidence has not yet to receive much attention. In this paper, we address the necessity of paying attention to the under-confidence issue. We first introduce a novel metric, a miscalibration score, to identify the overall and class-wise calibration status, including being over or under-confident. Our proposed metric reveals the pitfalls of existing calibration techniques, where they often overly calibrate the model and worsen under-confident predictions. Then we utilize the class-wise miscalibration score as a proxy to design a calibration technique that can tackle both over and under-confidence. We report extensive experiments that show our proposed methods substantially outperforming existing calibration techniques. We also validate our proposed calibration technique on an automatic failure detection task with a risk-coverage curve, reporting that our methods improve failure detection as well as trustworthiness of the model. The code are available at \url{https://github.com/AoShuang92/miscalibration_TS}.
</details>
<details>
<summary>摘要</summary>
deep learning 网络的自信核对是非常重要的，以确保在安全关键任务中的可靠预测。 miscalibration 可能会导致模型过于自信和/或不足自信，即模型对其预测的自信度高于或低于模型的准确率。  recent studies 曾经提出了 calibration 技术，并在不同任务上得到了成功。然而， under-confidence 的 miscalibration 问题还没有得到了充分的关注。在这篇论文中，我们强调了对 under-confidence 问题的注意。我们首先引入了一种新的指标，即 miscalibration Score，以评估模型的总体和类别 Calibration 状态，包括是否过于自信和/或不足自信。我们的提出的指标显示了现有的 calibration 技术的缺陷，即它们通常过于 Calibration 模型，从而恶化了不足自信的预测。然后，我们利用类别 miscalibration Score 作为代理，设计了一种可以解决过于自信和不足自信的 calibration 技术。我们报告了广泛的实验结果，显示了我们的提出的方法在现有的 calibration 技术上表现出了极大的优势。我们还验证了我们的提出的 calibration 技术在自动故障检测任务中的可靠性和信任性。代码可以在 \url{https://github.com/AoShuang92/miscalibration_TS} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Strategic-Preys-Make-Acute-Predators-Enhancing-Camouflaged-Object-Detectors-by-Generating-Camouflaged-Objects"><a href="#Strategic-Preys-Make-Acute-Predators-Enhancing-Camouflaged-Object-Detectors-by-Generating-Camouflaged-Objects" class="headerlink" title="Strategic Preys Make Acute Predators: Enhancing Camouflaged Object Detectors by Generating Camouflaged Objects"></a>Strategic Preys Make Acute Predators: Enhancing Camouflaged Object Detectors by Generating Camouflaged Objects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03166">http://arxiv.org/abs/2308.03166</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chunming He, Kai Li, Yachao Zhang, Yulun Zhang, Zhenhua Guo, Xiu Li, Martin Danelljan, Fisher Yu</li>
<li>for: 提高遮蔽物检测精度（Camouflaged Object Detection，COD）的表现，尤其是在一些具有挑战性的情况下。</li>
<li>methods: 基于预料-vs-捕食者游戏的思想，从预料和捕食者两个角度提出算法，包括一种对抗训练框架“Camouflageator”以及一种新的COD方法“Internal Coherence and Edge Guidance”（ICEG）。</li>
<li>results: 对比 existed COD 方法，ICEG 能够更好地 segmentation 遮蔽物，而且 Camouflageator 可以改进多种 COD 方法，包括 ICEG，从而实现 state-of-the-art COD 性能。<details>
<summary>Abstract</summary>
Camouflaged object detection (COD) is the challenging task of identifying camouflaged objects visually blended into surroundings. Albeit achieving remarkable success, existing COD detectors still struggle to obtain precise results in some challenging cases. To handle this problem, we draw inspiration from the prey-vs-predator game that leads preys to develop better camouflage and predators to acquire more acute vision systems and develop algorithms from both the prey side and the predator side. On the prey side, we propose an adversarial training framework, Camouflageator, which introduces an auxiliary generator to generate more camouflaged objects that are harder for a COD method to detect. Camouflageator trains the generator and detector in an adversarial way such that the enhanced auxiliary generator helps produce a stronger detector. On the predator side, we introduce a novel COD method, called Internal Coherence and Edge Guidance (ICEG), which introduces a camouflaged feature coherence module to excavate the internal coherence of camouflaged objects, striving to obtain more complete segmentation results. Additionally, ICEG proposes a novel edge-guided separated calibration module to remove false predictions to avoid obtaining ambiguous boundaries. Extensive experiments show that ICEG outperforms existing COD detectors and Camouflageator is flexible to improve various COD detectors, including ICEG, which brings state-of-the-art COD performance.
</details>
<details>
<summary>摘要</summary>
幻化物体检测（COD）是一项复杂的任务，即识别扮演融入周围环境中的物体。虽然已经取得了很大的成功，现有的COD检测器仍然在一些挑战性情况下困难获得精准结果。为解决这个问题，我们从预食-vs-掠食游戏中继承了猎食者和猎食者之间的竞争关系，并从两个角度提出算法。在猎食者（prey）一方，我们提出了一个对抗训练框架，即Camouflageator，该框架在auxiliary generator中引入了更多的掩蔽物体，使COD方法更难以检测。Camouflageator在对Generator和检测器进行对抗训练后，可以生成更加掩蔽的物体，从而提高检测精度。在猎食者（predator）一方，我们提出了一种新的COD方法，即内部凝聚和边缘引导（ICEG），该方法引入了掩蔽物体的凝聚特征模块，以提高物体完整性的检测结果。此外，ICEG还提出了一种新的边缘引导分离calibration模块，以除掉假定的预测，避免获得模糊的边界。广泛的实验表明，ICEG可以超越现有的COD检测器，而Camouflageator可以改进各种COD检测器，包括ICEG，从而实现状态足球的COD性能。
</details></li>
</ul>
<hr>
<h2 id="Precise-Benchmarking-of-Explainable-AI-Attribution-Methods"><a href="#Precise-Benchmarking-of-Explainable-AI-Attribution-Methods" class="headerlink" title="Precise Benchmarking of Explainable AI Attribution Methods"></a>Precise Benchmarking of Explainable AI Attribution Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03161">http://arxiv.org/abs/2308.03161</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rbrandt1/precise-benchmarking-of-xai">https://github.com/rbrandt1/precise-benchmarking-of-xai</a></li>
<li>paper_authors: Rafaël Brandt, Daan Raatjens, Georgi Gaydadjiev</li>
<li>for: The paper aims to develop a novel evaluation approach for benchmarking state-of-the-art explainable AI (XAI) attribution methods, in order to provide deeper insights into the output of XAI models.</li>
<li>methods: The proposed evaluation approach includes a synthetic classification model accompanied by its derived ground truth explanations, as well as new high-fidelity metrics to quantify the difference between explanations of the investigated XAI method and those derived from the synthetic model.</li>
<li>results: The authors investigate their proposal by constructing a synthetic convolutional image classification model and benchmarking several widely used XAI attribution methods using their evaluation approach. They compare their results with established prior XAI evaluation metrics, and show that their metrics provide deeper insights into the performance of XAI methods, including the poor precision scores among negatively contributing pixels. Additionally, they demonstrate that their metrics are among the fastest in terms of execution time.<details>
<summary>Abstract</summary>
The rationale behind a deep learning model's output is often difficult to understand by humans. EXplainable AI (XAI) aims at solving this by developing methods that improve interpretability and explainability of machine learning models. Reliable evaluation metrics are needed to assess and compare different XAI methods. We propose a novel evaluation approach for benchmarking state-of-the-art XAI attribution methods. Our proposal consists of a synthetic classification model accompanied by its derived ground truth explanations allowing high precision representation of input nodes contributions. We also propose new high-fidelity metrics to quantify the difference between explanations of the investigated XAI method and those derived from the synthetic model. Our metrics allow assessment of explanations in terms of precision and recall separately. Also, we propose metrics to independently evaluate negative or positive contributions of inputs. Our proposal provides deeper insights into XAI methods output. We investigate our proposal by constructing a synthetic convolutional image classification model and benchmarking several widely used XAI attribution methods using our evaluation approach. We compare our results with established prior XAI evaluation metrics. By deriving the ground truth directly from the constructed model in our method, we ensure the absence of bias, e.g., subjective either based on the training set. Our experimental results provide novel insights into the performance of Guided-Backprop and Smoothgrad XAI methods that are widely in use. Both have good precision and recall scores among positively contributing pixels (0.7, 0.76 and 0.7, 0.77, respectively), but poor precision scores among negatively contributing pixels (0.44, 0.61 and 0.47, 0.75, resp.). The recall scores in the latter case remain close. We show that our metrics are among the fastest in terms of execution time.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "The rationale behind a deep learning model's output is often difficult to understand by humans. EXplainable AI (XAI) aims at solving this by developing methods that improve interpretability and explainability of machine learning models. Reliable evaluation metrics are needed to assess and compare different XAI methods. We propose a novel evaluation approach for benchmarking state-of-the-art XAI attribution methods. Our proposal consists of a synthetic classification model accompanied by its derived ground truth explanations allowing high precision representation of input nodes contributions. We also propose new high-fidelity metrics to quantify the difference between explanations of the investigated XAI method and those derived from the synthetic model. Our metrics allow assessment of explanations in terms of precision and recall separately. Also, we propose metrics to independently evaluate negative or positive contributions of inputs. Our proposal provides deeper insights into XAI methods output. We investigate our proposal by constructing a synthetic convolutional image classification model and benchmarking several widely used XAI attribution methods using our evaluation approach. We compare our results with established prior XAI evaluation metrics. By deriving the ground truth directly from the constructed model in our method, we ensure the absence of bias, e.g., subjective either based on the training set. Our experimental results provide novel insights into the performance of Guided-Backprop and Smoothgrad XAI methods that are widely in use. Both have good precision and recall scores among positively contributing pixels (0.7, 0.76 and 0.7, 0.77, respectively), but poor precision scores among negatively contributing pixels (0.44, 0.61 and 0.47, 0.75, resp.). The recall scores in the latter case remain close. We show that our metrics are among the fastest in terms of execution time."中文翻译：人类理解深度学习模型输出的理由往往具有困难，EXplainable AI（XAI）目的是解决这一问题，通过发展可解释性和可读性的机器学习模型。可靠的评估 метри可以用来评估和比较不同的 XAI 方法。我们提出了一种新的评估方法，用于比较现代 XAI 负担方法的表现。我们的提议包括一个Synthetic类型模型，以及其Derived的真实解释，allowing high precision representation of input nodes contributions。我们还提出了一些新的高效度 métriques，用于评估Investigated XAI方法的解释和Synthetic模型中的解释之间的差异。我们的 métriques 允许对解释进行精确的评估，分别评估精度和回归。此外，我们还提出了一些独立评估输入的正负性贡献的 метри。我们的提议可以为 XAI 方法的输出提供更深入的理解。我们在一个Synthetic convolutional image classification模型上进行了实验，并使用我们的评估方法评估了一些广泛使用的 XAI 负担方法。我们与已有的 XAI 评估 métriques进行比较。在我们的方法中，直接从构建的模型中 derivation ground truth，以避免主观偏见，如基于训练集的主观偏见。我们的实验结果提供了新的意义，Guided-Backprop和Smoothgrad XAI方法在使用的情况下的性能。两者在正确贡献像素上有着好的精度和回归分数（0.7, 0.76和0.7, 0.77，分别），但是在负贡献像素上有着差的精度分数（0.44, 0.61和0.47, 0.75，分别）。负贡献像素的回归分数保持相对较近。我们的 métriques 在执行时间方面也是 Among the fastest。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/07/cs.AI_2023_08_07/" data-id="clp9qz811009qok8806bz6gom" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_08_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/07/cs.CL_2023_08_07/" class="article-date">
  <time datetime="2023-08-07T11:00:00.000Z" itemprop="datePublished">2023-08-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/07/cs.CL_2023_08_07/">cs.CL - 2023-08-07</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Emotionally-Numb-or-Empathetic-Evaluating-How-LLMs-Feel-Using-EmotionBench"><a href="#Emotionally-Numb-or-Empathetic-Evaluating-How-LLMs-Feel-Using-EmotionBench" class="headerlink" title="Emotionally Numb or Empathetic? Evaluating How LLMs Feel Using EmotionBench"></a>Emotionally Numb or Empathetic? Evaluating How LLMs Feel Using EmotionBench</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03656">http://arxiv.org/abs/2308.03656</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cuhk-arise/emotionbench">https://github.com/cuhk-arise/emotionbench</a></li>
<li>paper_authors: Jen-tse Huang, Man Ho Lam, Eric John Li, Shujie Ren, Wenxuan Wang, Wenxiang Jiao, Zhaopeng Tu, Michael R. Lyu</li>
<li>for: 评估 Large Language Models (LLMs) 的人工情感能力 (empathy ability)，以了解 LLMS 如何在具有不同情感状况下做出回应。</li>
<li>methods: 使用心理学中的情感评估理论，收集了超过 400 个情况的数据集，并对这些情况进行了人工评估，并使用了五种不同的 LLMs，以覆盖 comercial 和 open-source 模型，以及不同的模型大小。</li>
<li>results: LLMs 可以在某些情况下做出合适的回应，但它们无法完全遵循人类情感行为的Alignment，也无法建立类似情况之间的连接。<details>
<summary>Abstract</summary>
Recently, the community has witnessed the advancement of Large Language Models (LLMs), which have shown remarkable performance on various downstream tasks. Led by powerful models like ChatGPT and Claude, LLMs are revolutionizing how users engage with software, assuming more than mere tools but intelligent assistants. Consequently, evaluating LLMs' anthropomorphic capabilities becomes increasingly important in contemporary discourse. Utilizing the emotion appraisal theory from psychology, we propose to evaluate the empathy ability of LLMs, i.e., how their feelings change when presented with specific situations. After a careful and comprehensive survey, we collect a dataset containing over 400 situations that have proven effective in eliciting the eight emotions central to our study. Categorizing the situations into 36 factors, we conduct a human evaluation involving more than 1,200 subjects worldwide. With the human evaluation results as references, our evaluation includes five LLMs, covering both commercial and open-source models, including variations in model sizes, featuring the latest iterations, such as GPT-4 and LLaMA 2. A conclusion can be drawn from the results that, despite several misalignments, LLMs can generally respond appropriately to certain situations. Nevertheless, they fall short in alignment with the emotional behaviors of human beings and cannot establish connections between similar situations. Our collected dataset of situations, the human evaluation results, and the code of our testing framework, dubbed EmotionBench, is made publicly in https://github.com/CUHK-ARISE/EmotionBench. We aspire to contribute to the advancement of LLMs regarding better alignment with the emotional behaviors of human beings, thereby enhancing their utility and applicability as intelligent assistants.
</details>
<details>
<summary>摘要</summary>
近期，社区目睹了大型语言模型（LLM）的发展，其在不同下游任务上表现出了很好的表现。带领于强大的模型如ChatGPT和Claude，LLMs在软件中的应用不再是只是工具，而是智能助手。因此，评估LLMs的人类化能力在当今话题中变得越来越重要。基于心理学中的情感评估理论，我们提议评估LLMs的同情能力，即在特定情况下，它们的情感如何变化。经过仔细和全面的调查，我们收集了包含超过400个情况的数据集，这些情况被证明可以诱发出8种基本的情感。将这些情况分为36个因素，我们进行了全球范围内的人工评估，受试者超过1,200人。与人工评估结果为参考，我们的评估包括5个LLM，其中包括商业和开源模型，以及不同的模型大小和最新的迭代（如GPT-4和LLaMA 2）。结果显示，虽然LLMs在某些情况下能够适应，但它们在与人类情感行为的Alignment方面异常，无法建立类似情况之间的连接。我们收集的情况数据集、人工评估结果和测试框架代码（EmotionBench）将在https://github.com/CUHK-ARISE/EmotionBench上公开。我们希望通过提高LLMs与人类情感行为的Alignment，从而提高它们在智能助手方面的应用和可用性。
</details></li>
</ul>
<hr>
<h2 id="KITLM-Domain-Specific-Knowledge-InTegration-into-Language-Models-for-Question-Answering"><a href="#KITLM-Domain-Specific-Knowledge-InTegration-into-Language-Models-for-Question-Answering" class="headerlink" title="KITLM: Domain-Specific Knowledge InTegration into Language Models for Question Answering"></a>KITLM: Domain-Specific Knowledge InTegration into Language Models for Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03638">http://arxiv.org/abs/2308.03638</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sakharamg/kitlm">https://github.com/sakharamg/kitlm</a></li>
<li>paper_authors: Ankush Agarwal, Sakharam Gawade, Amar Prakash Azad, Pushpak Bhattacharyya</li>
<li>for: 提高域域语言理解，提高语言模型在专业领域的表现。</li>
<li>methods: 通过知识涂敷方法，将相关知识infusion到语言模型中，提高语言模型的表现和效率。</li>
<li>results: KITLM表现较SKILL和GPT-3.5-turbo更出色，在MetaQA和AeroQA中都达到了1.5倍以上的提高，并且在飞航领域中也有了显著的提高。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated remarkable performance in a wide range of natural language tasks. However, as these models continue to grow in size, they face significant challenges in terms of computational costs. Additionally, LLMs often lack efficient domain-specific understanding, which is particularly crucial in specialized fields such as aviation and healthcare. To boost the domain-specific understanding, we propose, KITLM, a novel knowledge base integration approach into language model through relevant information infusion. By integrating pertinent knowledge, not only the performance of the language model is greatly enhanced, but the model size requirement is also significantly reduced while achieving comparable performance. Our proposed knowledge-infused model surpasses the performance of both GPT-3.5-turbo and the state-of-the-art knowledge infusion method, SKILL, achieving over 1.5 times improvement in exact match scores on the MetaQA. KITLM showed a similar performance boost in the aviation domain with AeroQA. The drastic performance improvement of KITLM over the existing methods can be attributed to the infusion of relevant knowledge while mitigating noise. In addition, we release two curated datasets to accelerate knowledge infusion research in specialized fields: a) AeroQA, a new benchmark dataset designed for multi-hop question-answering within the aviation domain, and b) Aviation Corpus, a dataset constructed from unstructured text extracted from the National Transportation Safety Board reports. Our research contributes to advancing the field of domain-specific language understanding and showcases the potential of knowledge infusion techniques in improving the performance of language models on question-answering.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在各种自然语言任务中表现出色，但是随着模型的大小不断增长，其计算成本也随之增加。此外，LLM often lacks efficient domain-specific understanding，尤其在专业领域如航空和医疗等领域。为了提高域 Specific Understanding，我们提议了一种基于知识库 интеграción的语言模型approach，即KITLM。通过将相关知识integrated into the language model，不仅提高了语言模型的性能，而且降低了模型的大小，同时实现相似的性能。我们的提出的知识混合模型在MetaQA上的精确匹配分数上超过了GPT-3.5-turbo和SKILL的状态的表现，达到了1.5倍的提升。KITLM在航空领域的AeroQA上也显示了类似的性能提升。我们发布了两个 curaated dataset，以促进域 Specific Understanding研究：a) AeroQA，一个新的多步问答检验 benchmark dataset，和b) Aviation Corpus，一个从国家交通安全委员会报告中提取的未结构化文本构建的数据集。我们的研究对域 Specific Understanding领域的发展做出了贡献，并展示了知识混合技术在问答任务中的潜在提升效果。
</details></li>
</ul>
<hr>
<h2 id="Negative-Lexical-Constraints-in-Neural-Machine-Translation"><a href="#Negative-Lexical-Constraints-in-Neural-Machine-Translation" class="headerlink" title="Negative Lexical Constraints in Neural Machine Translation"></a>Negative Lexical Constraints in Neural Machine Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03601">http://arxiv.org/abs/2308.03601</a></li>
<li>repo_url: None</li>
<li>paper_authors: Josef Jon, Dušan Variš, Michal Novák, João Paulo Aires, Ondřej Bojar</li>
<li>for: 这个论文探讨了在英语到捷克语神经机器翻译中的负 lexical constraining。负 lexical constraining是禁止翻译模型生成某些词语或表达的方法。</li>
<li>methods: 我们比较了基于修改解码过程或训练数据的不同方法。我们在两个任务上进行了比较：重句化和反馈式翻译重新调整。我们还研究了这些方法如何”逃脱”给定的约束（通常是在词典形式），通过生成不同的表面形式来绕过约束。</li>
<li>results: 我们提出了一种方法来减轻这个问题，通过在训练过程中使用剪辑的负约束来对抗模型生成多种表面形式的词语，从而减轻约束被违反的问题。我们示出了我们的方法可以改善约束，但问题仍然存在许多情况下。<details>
<summary>Abstract</summary>
This paper explores negative lexical constraining in English to Czech neural machine translation. Negative lexical constraining is used to prohibit certain words or expressions in the translation produced by the neural translation model. We compared various methods based on modifying either the decoding process or the training data. The comparison was performed on two tasks: paraphrasing and feedback-based translation refinement. We also studied to which extent these methods "evade" the constraints presented to the model (usually in the dictionary form) by generating a different surface form of a given constraint.We propose a way to mitigate the issue through training with stemmed negative constraints to counter the model's ability to induce a variety of the surface forms of a word that can result in bypassing the constraint. We demonstrate that our method improves the constraining, although the problem still persists in many cases.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文研究了英语到捷克语神经机器翻译中的负 lexical 约束。负 lexical 约束用于禁止翻译模型生成的某些词或表达。我们比较了基于修改解码过程或训练数据的不同方法。我们在两个任务上进行了比较：重句和反馈基于翻译重新评估。我们还研究了这些方法如何"逃脱"给模型的约束（通常是字典形式），生成不同的表面形式。我们提议通过减少负约束来 Mitigate 这个问题，使用减少负约束来对模型的表面形式变化进行对抗。我们示出了我们的方法可以改善约束，但问题仍然存在于多个情况。
</details></li>
</ul>
<hr>
<h2 id="WIKITIDE-A-Wikipedia-Based-Timestamped-Definition-Pairs-Dataset"><a href="#WIKITIDE-A-Wikipedia-Based-Timestamped-Definition-Pairs-Dataset" class="headerlink" title="WIKITIDE: A Wikipedia-Based Timestamped Definition Pairs Dataset"></a>WIKITIDE: A Wikipedia-Based Timestamped Definition Pairs Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03582">http://arxiv.org/abs/2308.03582</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hsuvas Borkakoty, Luis Espinosa-Anke</li>
<li>for: 本研究旨在提高现有语言模型的可靠性和灵活性，通过从Wikipedia中提取时间戳定义对语言和世界的变化进行识别。</li>
<li>methods: 本研究提出了一种基于WikiTiDe数据集的综合方法，包括一种自动化的启动算法，以及一种使用精心预处理的基本更新来提高模型的性能。</li>
<li>results: 研究结果表明，使用自动化启动算法和精心预处理的基本更新可以提高模型的性能，并在多个下游任务中显示出优异的成绩。<details>
<summary>Abstract</summary>
A fundamental challenge in the current NLP context, dominated by language models, comes from the inflexibility of current architectures to 'learn' new information. While model-centric solutions like continual learning or parameter-efficient fine tuning are available, the question still remains of how to reliably identify changes in language or in the world. In this paper, we propose WikiTiDe, a dataset derived from pairs of timestamped definitions extracted from Wikipedia. We argue that such resource can be helpful for accelerating diachronic NLP, specifically, for training models able to scan knowledge resources for core updates concerning a concept, an event, or a named entity. Our proposed end-to-end method is fully automatic, and leverages a bootstrapping algorithm for gradually creating a high-quality dataset. Our results suggest that bootstrapping the seed version of WikiTiDe leads to better fine-tuned models. We also leverage fine-tuned models in a number of downstream tasks, showing promising results with respect to competitive baselines.
</details>
<details>
<summary>摘要</summary>
当前NLP上的一个基本挑战是模型的不可变性，即现有的模型无法"学习"新信息。虽有模型中心的解决方案如 kontinual learning 和参数效率的精度调整，但问题仍然是如何可靠地识别语言或世界中的变化。在这篇论文中，我们提出了 WikiTiDe dataset，它是基于 Wikipedia 中的时间戳定义对的对应集。我们认为这种资源可以帮助加速 diachronic NLP，即训练能够扫描知识资源的模型，以找到核心更新 concerning 概念、事件或Named Entity。我们的提议的终端方法是自动的，并利用搅拌算法来逐渐创建高质量的数据集。我们的结果表明，使用搅拌种子版本 WikiTiDe 可以获得更好的精度调整。此外，我们还利用精度调整的模型在一些下游任务中的表现，与比较标准的基准线有着良好的结果。
</details></li>
</ul>
<hr>
<h2 id="Towards-Controllable-Natural-Language-Inference-through-Lexical-Inference-Types"><a href="#Towards-Controllable-Natural-Language-Inference-through-Lexical-Inference-Types" class="headerlink" title="Towards Controllable Natural Language Inference through Lexical Inference Types"></a>Towards Controllable Natural Language Inference through Lexical Inference Types</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03581">http://arxiv.org/abs/2308.03581</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yingji Zhang, Danilo S. Carvalho, Ian Pratt-Hartmann, Andre Freitas<br>for: This paper aims to provide a mechanism for producing explanatory (abductive) inference chains that ground claims to their supporting premises.methods: The paper employs the T5 model to directly generate an entailment tree, which explains how the answer is inferred. However, the T5 model lacks the ability to explain and control the generation of intermediate steps, which is crucial for the multi-hop inference process.results: The paper proposes a controlled natural language inference architecture for multi-premise explanatory inference, which includes defining lexical inference types based on Abstract Meaning Representation (AMR) graph and modifying the architecture of T5 to learn a latent sentence representation conditioned on said type information. The paper also delivers a dataset of approximately 5000 annotated explanatory inference steps, with well-grounded lexical-symbolic operations. Experimental results indicate that the inference typing induced at the T5 bottleneck can help T5 to generate a conclusion under explicit control.<details>
<summary>Abstract</summary>
Explainable natural language inference aims to provide a mechanism to produce explanatory (abductive) inference chains which ground claims to their supporting premises. A recent corpus called EntailmentBank strives to advance this task by explaining the answer to a question using an entailment tree \cite{dalvi2021explaining}. They employ the T5 model to directly generate the tree, which can explain how the answer is inferred. However, it lacks the ability to explain and control the generation of intermediate steps, which is crucial for the multi-hop inference process. % One recent corpus, EntailmentBank, aims to push this task forward by explaining an answer to a question according to an entailment tree \cite{dalvi2021explaining}. They employ T5 to generate the tree directly, which can explain how the answer is inferred but cannot explain how the intermediate is generated, which is essential to the multi-hop inference process. In this work, we focus on proposing a controlled natural language inference architecture for multi-premise explanatory inference. To improve control and enable explanatory analysis over the generation, we define lexical inference types based on Abstract Meaning Representation (AMR) graph and modify the architecture of T5 to learn a latent sentence representation (T5 bottleneck) conditioned on said type information. We also deliver a dataset of approximately 5000 annotated explanatory inference steps, with well-grounded lexical-symbolic operations. Experimental results indicate that the inference typing induced at the T5 bottleneck can help T5 to generate a conclusion under explicit control.
</details>
<details>
<summary>摘要</summary>
自然语言推理可以提供一种机制，以便生成解释性的推理链，并将含义链绑定到它的支持前提。一个新的资料库called EntailmentBank，努力推动这项任务，通过解释答案使用推理树 \cite{dalvi2021explaining}.它使用T5模型直接生成推理树，可以解释答案如何被推理出来。然而，它缺乏对中间步骤的解释和控制能力，这是多步推理过程中的关键。在这项工作中，我们关注提出一种可控的自然语言推理体系，用于多个前提解释推理。为了提高控制和启用解释分析，我们定义了基于抽象意义表示（AMR）图的语义推理类型，并修改T5模型的架构，以学习受到这些类型信息的隐藏句子表示（T5瓶颈）。我们还提供了约5000个注释的解释推理步骤数据集，其中包含了具有固定 lexical-symbolic 操作的准确地标注。实验结果表明，在T5瓶颈中引入的推理类型induced可以帮助T5在显式控制下生成结论。
</details></li>
</ul>
<hr>
<h2 id="Topological-Interpretations-of-GPT-3"><a href="#Topological-Interpretations-of-GPT-3" class="headerlink" title="Topological Interpretations of GPT-3"></a>Topological Interpretations of GPT-3</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03565">http://arxiv.org/abs/2308.03565</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyi Sun, Bradley Nelson</li>
<li>for:  investigate a consistent method for deriving the correlation between sentence vector and semantic meaning of a sentence</li>
<li>methods:  use three state-of-the-art word&#x2F;sentence embedding methods (GPT-3, Word2Vec, and Sentence-BERT) to embed plain text sentence strings into high dimensional spaces, and compute the pairwise distance between any possible combination of two sentence vectors in an embedding space</li>
<li>results:  observe correlations of the same sentence in different embedding spaces and correlations of different sentences in the same embedding space, which are consistent with the hypothesis and provide a foundation for further research<details>
<summary>Abstract</summary>
This is an experiential study of investigating a consistent method for deriving the correlation between sentence vector and semantic meaning of a sentence. We first used three state-of-the-art word/sentence embedding methods including GPT-3, Word2Vec, and Sentence-BERT, to embed plain text sentence strings into high dimensional spaces. Then we compute the pairwise distance between any possible combination of two sentence vectors in an embedding space and map them into a matrix. Based on each distance matrix, we compute the correlation of distances of a sentence vector with respect to the other sentence vectors in an embedding space. Then we compute the correlation of each pair of the distance matrices. We observed correlations of the same sentence in different embedding spaces and correlations of different sentences in the same embedding space. These observations are consistent with our hypothesis and take us to the next stage.
</details>
<details>
<summary>摘要</summary>
这是一项实验性研究，旨在找到一种稳定的方法，用于计算句子 vector 和句子意义之间的相关性。我们首先使用了三种当前顶尖词语/句子嵌入方法，包括 GPT-3、Word2Vec 和 Sentence-BERT，将平文句子串embedded到高维空间中。然后，我们计算了任意两个句子 vector 之间的距离，并将其映射到一个矩阵中。基于每个距离矩阵，我们计算了每个句子 vector 与其他句子 vector 在嵌入空间中的距离相关性。然后，我们计算了每对距离矩阵之间的相关性。我们发现了不同嵌入空间中的同句子之间的相关性，以及同一个嵌入空间中的不同句子之间的相关性。这些观察结果与我们的假设一致，为我们的下一步做出了基础。
</details></li>
</ul>
<hr>
<h2 id="Mondrian-Prompt-Abstraction-Attack-Against-Large-Language-Models-for-Cheaper-API-Pricing"><a href="#Mondrian-Prompt-Abstraction-Attack-Against-Large-Language-Models-for-Cheaper-API-Pricing" class="headerlink" title="Mondrian: Prompt Abstraction Attack Against Large Language Models for Cheaper API Pricing"></a>Mondrian: Prompt Abstraction Attack Against Large Language Models for Cheaper API Pricing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03558">http://arxiv.org/abs/2308.03558</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wai Man Si, Michael Backes, Yang Zhang</li>
<li>for: 本研究旨在描述一种新的攻击策略对于自然语言处理（NLP）模型API，以及一种简单有效的方法来实现这种攻击。</li>
<li>methods: 本研究使用了一种名为“MONDRIAN”的简单方法，可以将用户查询语句简化，从而降低使用NLP模型API的成本。该方法包括创建一个假API（具有较低的成本），并使用MONDRIAN模块来修改用户查询语句，以获取对目标API的响应。</li>
<li>results: 研究结果表明，MONDRIAN可以成功地将用户查询语句的字符数减少13%至23%，并且这些简化的查询语句对任务特定和通用的语言模型如ChatGPT没有显著影响。此外，MONDRIAN还可以减少 instruciton prompts 的字符数至少11%，而不会影响输出质量。因此，这种攻击策略可以让攻击者获得利益，而无需承担API开发和部署的成本。<details>
<summary>Abstract</summary>
The Machine Learning as a Service (MLaaS) market is rapidly expanding and becoming more mature. For example, OpenAI's ChatGPT is an advanced large language model (LLM) that generates responses for various queries with associated fees. Although these models can deliver satisfactory performance, they are far from perfect. Researchers have long studied the vulnerabilities and limitations of LLMs, such as adversarial attacks and model toxicity. Inevitably, commercial ML models are also not exempt from such issues, which can be problematic as MLaaS continues to grow. In this paper, we discover a new attack strategy against LLM APIs, namely the prompt abstraction attack. Specifically, we propose Mondrian, a simple and straightforward method that abstracts sentences, which can lower the cost of using LLM APIs. In this approach, the adversary first creates a pseudo API (with a lower established price) to serve as the proxy of the target API (with a higher established price). Next, the pseudo API leverages Mondrian to modify the user query, obtain the abstracted response from the target API, and forward it back to the end user. Our results show that Mondrian successfully reduces user queries' token length ranging from 13% to 23% across various tasks, including text classification, generation, and question answering. Meanwhile, these abstracted queries do not significantly affect the utility of task-specific and general language models like ChatGPT. Mondrian also reduces instruction prompts' token length by at least 11% without compromising output quality. As a result, the prompt abstraction attack enables the adversary to profit without bearing the cost of API development and deployment.
</details>
<details>
<summary>摘要</summary>
Machine Learning as a Service（MLaaS）市场迅速扩大，成熔度也在提高。例如，OpenAI的ChatGPT是一种先进的大型语言模型（LLM），可以根据不同的问题生成相应的回答，但这些模型并不完美。研究人员已经长期研究LLM的攻击和限制，如对抗攻击和模型毒性。然而，商业ML模型也不能免受这些问题，这可能会对MLaaS的发展带来问题。在这篇论文中，我们发现了一种新的攻击策略对LLM API，即提档攻击。特别是，我们提出了一种名为Mondrian的简单和直观的方法，可以将句子抽象成更短的句子。在这种方法中，敌对者首先创建一个假API（具有较低的成本），作为目标API（具有较高的成本）的代理。然后，假API使用Mondrian modify用户的查询，从目标API获取抽象回答，并将其返回给终端用户。我们的结果表明，Mondrian可以成功地将用户查询的字符数量减少13%到23%，并且这些抽象查询不会对任务特定和总语言模型如ChatGPT产生重大影响。此外，Mondrian还可以减少 instruktion 的字符数量至少11%，无需妥协输出质量。因此，提档攻击可以让敌对者获利而不需要承担API的开发和部署成本。
</details></li>
</ul>
<hr>
<h2 id="Zhongjing-Enhancing-the-Chinese-Medical-Capabilities-of-Large-Language-Model-through-Expert-Feedback-and-Real-world-Multi-turn-Dialogue"><a href="#Zhongjing-Enhancing-the-Chinese-Medical-Capabilities-of-Large-Language-Model-through-Expert-Feedback-and-Real-world-Multi-turn-Dialogue" class="headerlink" title="Zhongjing: Enhancing the Chinese Medical Capabilities of Large Language Model through Expert Feedback and Real-world Multi-turn Dialogue"></a>Zhongjing: Enhancing the Chinese Medical Capabilities of Large Language Model through Expert Feedback and Real-world Multi-turn Dialogue</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03549">http://arxiv.org/abs/2308.03549</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/suprityoung/zhongjing">https://github.com/suprityoung/zhongjing</a></li>
<li>paper_authors: Songhua Yang, Hanjie Zhao, Senbin Zhu, Guangyu Zhou, Hongfei Xu, Yuxiang Jia, Hongying Zan</li>
<li>for: 本研究旨在提高Large Language Models（LLMs）在中药医学领域的理解和回答能力。</li>
<li>methods: 本研究使用了人工智能反馈学习（RLHF）和中医诊断对话集（CMtMedQA）来提高模型的多回合对话和积极问题提起能力。</li>
<li>results: 我们的模型在多种能力方面超过了基eline，并与之前的最佳模型和ChatGPT匹配在一些能力方面。RLHF进一步提高了模型的命令遵循能力和安全性。<details>
<summary>Abstract</summary>
Recent advances in Large Language Models (LLMs) have achieved remarkable breakthroughs in understanding and responding to user intents. However, their performance lag behind general use cases in some expertise domains, such as Chinese medicine. Existing efforts to incorporate Chinese medicine into LLMs rely on Supervised Fine-Tuning (SFT) with single-turn and distilled dialogue data. These models lack the ability for doctor-like proactive inquiry and multi-turn comprehension and cannot always align responses with safety and professionalism experts. In this work, we introduce Zhongjing, the first Chinese medical LLaMA-based LLM that implements an entire training pipeline from pre-training to reinforcement learning with human feedback (RLHF). Additionally, we introduce a Chinese multi-turn medical dialogue dataset of 70,000 authentic doctor-patient dialogues, CMtMedQA, which significantly enhances the model's capability for complex dialogue and proactive inquiry initiation. We define a refined annotation rule and evaluation criteria given the biomedical domain's unique characteristics. Results show that our model outperforms baselines in various capacities and matches the performance of ChatGPT in a few abilities, despite having 50x training data with previous best model and 100x parameters with ChatGPT. RLHF further improves the model's instruction-following ability and safety.We also release our code, datasets and model for further research.
</details>
<details>
<summary>摘要</summary>
近期大语言模型（LLM）的进步取得了很大的突破，在理解和回答用户意图方面表现出色。然而，在某些专业领域，如中医，其表现仍然落后于通用场景。现有的中医integration into LLMs的尝试都是通过监督微调（SFT）和单转Dialogue数据进行。这些模型缺乏医生般的积极问题和多转Dialogue的能力，并且不能一直与安全和专业性保持一致。在这项工作中，我们介绍了 Zhongjing，首个基于LLaMA的中医语言模型，该模型通过整个训练管道，从预训练到人工反馈学习（RLHF）来实现。此外，我们还介绍了一个70000 authentic doctor-patient对话的中医多转对话数据集，CMtMedQA，这使得模型在复杂对话和积极问题的 iniciation 方面具有显著提升。我们采用了特定领域的注解规则和评价标准。结果表明，我们的模型在多种方面超过基eline，并与之前的最佳模型和ChatGPT的性能相当，即使有50倍的训练数据和100倍的参数。RLHF进一步改善了模型的指令遵循能力和安全性。我们还发布了代码、数据集和模型，以便进一步的研究。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-preserving-Pruning-for-Pre-trained-Language-Models-without-Retraining"><a href="#Knowledge-preserving-Pruning-for-Pre-trained-Language-Models-without-Retraining" class="headerlink" title="Knowledge-preserving Pruning for Pre-trained Language Models without Retraining"></a>Knowledge-preserving Pruning for Pre-trained Language Models without Retraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03449">http://arxiv.org/abs/2308.03449</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seungcheol Park, Hojun Choi, U Kang</li>
<li>for: 压缩预训练语言模型，不需要重新训练</li>
<li>methods: 使用知识保留的结构剪辑算法， identificates 和剪辑尚未使用的注意头和神经元</li>
<li>results: 在 SQuAD  benchmarck 上，距离80%的压缩率下，与现有的不需要重新训练剪辑算法相比，提高了58.02%的 F1 分数<details>
<summary>Abstract</summary>
Given a pre-trained language model, how can we efficiently compress it without retraining? Retraining-free structured pruning algorithms are crucial in pre-trained language model compression due to their significantly reduced pruning cost and capability to prune large language models. However, existing retraining-free algorithms encounter severe accuracy degradation, as they fail to preserve the useful knowledge of pre-trained models. In this paper, we propose K-pruning (Knowledge-preserving pruning), an accurate retraining-free structured pruning algorithm for pre-trained language models. K-pruning identifies and prunes attention heads and neurons deemed to be superfluous, based on the amount of their inherent knowledge. K-pruning applies an iterative process of pruning followed by knowledge reconstruction for each sub-layer to preserve the knowledge of the pre-trained models. Consequently, K-pruning shows up to 58.02%p higher F1 score than existing retraining-free pruning algorithms under a high compression rate of 80% on the SQuAD benchmark.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:给一个预训练语言模型，如何高效压缩它而无需重新训练？预训练模型压缩中的结构化压缩算法对于大型语言模型的压缩具有重要的降低压缩成本和可以压缩大型语言模型。然而，现有的预训练模型压缩算法往往会导致严重的准确性下降，因为它们无法保留预训练模型的有用知识。在本文中，我们提出了K-压缩（知识保留压缩），一种高精度的预训练模型压缩算法。K-压缩通过评估注意头和神经元的含义量来进行压缩，并在每个子层上应用迭代压缩和知识重建过程以保留预训练模型的知识。因此，K-压缩在80%的压缩率下，与现有的预训练模型压缩算法相比，在SQuAD测试 benchmark上显示出58.02%p的更高的F1分数。
</details></li>
</ul>
<hr>
<h2 id="Improving-Few-shot-and-Zero-shot-Entity-Linking-with-Coarse-to-Fine-Lexicon-based-Retriever"><a href="#Improving-Few-shot-and-Zero-shot-Entity-Linking-with-Coarse-to-Fine-Lexicon-based-Retriever" class="headerlink" title="Improving Few-shot and Zero-shot Entity Linking with Coarse-to-Fine Lexicon-based Retriever"></a>Improving Few-shot and Zero-shot Entity Linking with Coarse-to-Fine Lexicon-based Retriever</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03365">http://arxiv.org/abs/2308.03365</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shijue Huang, Bingbing Wang, Libo Qin, Qin Zhao, Ruifeng Xu<br>for: 这篇论文主要针对中文少量和零量实体识别问题，尤其是对尾部和出现的实体进行更加准确的识别。methods: 该论文提出了一种基于词典的粗细化检索器，通过两层检索来有效地检索实体候选者。第一层利用实体名称进行检索，而第二层则是利用实体描述来细化检索并准确地划分出新的实体。results: 实验结果显示，该方法可以在不进行广泛的训练过程中获得优秀的性能，并且在NLPCC 2023共享任务6中 ranked 1st in Chinese Few-shot and Zero-shot Entity Linking。<details>
<summary>Abstract</summary>
Few-shot and zero-shot entity linking focus on the tail and emerging entities, which are more challenging but closer to real-world scenarios. The mainstream method is the ''retrieve and rerank'' two-stage framework. In this paper, we propose a coarse-to-fine lexicon-based retriever to retrieve entity candidates in an effective manner, which operates in two layers. The first layer retrieves coarse-grained candidates by leveraging entity names, while the second layer narrows down the search to fine-grained candidates within the coarse-grained ones. In addition, this second layer utilizes entity descriptions to effectively disambiguate tail or new entities that share names with existing popular entities. Experimental results indicate that our approach can obtain superior performance without requiring extensive finetuning in the retrieval stage. Notably, our approach ranks the 1st in NLPCC 2023 Shared Task 6 on Chinese Few-shot and Zero-shot Entity Linking.
</details>
<details>
<summary>摘要</summary>
主要研究领域是几招和零招实体连接，它们更加具有实际场景的挑战性。主流方法是''检索并重新排''的两个阶段框架。在这篇论文中，我们提出了一种粗细层次lexicon-based检索器，可以有效地 retrieve实体候选者，它在两层结构下运行。第一层通过实体名称进行检索粗细候选者，第二层在粗细候选者中进行筛选和精度增强。此外，第二层还利用实体描述来有效地减少尾部或新出现的实体名称冲突。实验结果表明，我们的方法可以在检索阶段无需大规模的微调就可以获得优秀表现。特别是，我们的方法在NLPCC 2023共享任务6中的中文几招和零招实体连接中获得了第一名。
</details></li>
</ul>
<hr>
<h2 id="Coupling-Symbolic-Reasoning-with-Language-Modeling-for-Efficient-Longitudinal-Understanding-of-Unstructured-Electronic-Medical-Records"><a href="#Coupling-Symbolic-Reasoning-with-Language-Modeling-for-Efficient-Longitudinal-Understanding-of-Unstructured-Electronic-Medical-Records" class="headerlink" title="Coupling Symbolic Reasoning with Language Modeling for Efficient Longitudinal Understanding of Unstructured Electronic Medical Records"></a>Coupling Symbolic Reasoning with Language Modeling for Efficient Longitudinal Understanding of Unstructured Electronic Medical Records</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03360">http://arxiv.org/abs/2308.03360</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shivani Shekhar, Simran Tiwari, T. C. Rensink, Ramy Eskander, Wael Salloum</li>
<li>for: 这种研究旨在提高不结构化的医疗记录理解，特别是使用 transformer-based 大型语言模型 (LLMs) 来解决医疗记录中的混乱、不一致和重复问题。</li>
<li>methods: 该研究使用了符号逻辑和语言模型的组合来提高不结构化医疗记录中的各种医学变量提取。</li>
<li>results: 研究发现，将符号逻辑与语言模型结合使用可以提高不结构化医疗记录中各种医学变量的提取率。此外，研究还发现了现有的开源 LLMs 在检索性能方面与商业 LLMs 相当。最后，研究强调了使用符号逻辑来导航 LLMs 的重要性，因为纯然使用 LLMs 会导致性能最低。<details>
<summary>Abstract</summary>
The application of Artificial Intelligence (AI) in healthcare has been revolutionary, especially with the recent advancements in transformer-based Large Language Models (LLMs). However, the task of understanding unstructured electronic medical records remains a challenge given the nature of the records (e.g., disorganization, inconsistency, and redundancy) and the inability of LLMs to derive reasoning paradigms that allow for comprehensive understanding of medical variables. In this work, we examine the power of coupling symbolic reasoning with language modeling toward improved understanding of unstructured clinical texts. We show that such a combination improves the extraction of several medical variables from unstructured records. In addition, we show that the state-of-the-art commercially-free LLMs enjoy retrieval capabilities comparable to those provided by their commercial counterparts. Finally, we elaborate on the need for LLM steering through the application of symbolic reasoning as the exclusive use of LLMs results in the lowest performance.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）在医疗领域的应用已经是革命性的，尤其是最近的转换器基于大语言模型（LLM）的进步。然而，理解不结构化的电子医疗记录仍然是一个挑战，因为记录的自然特性（如混乱、不一致和重复），以及LLM无法 derivation reasoning 模式，导致医学变量的全面理解受到限制。在这种情况下，我们研究了对象 Symbolic reasoning 和语言模型结合的能力，以提高不结构化医疗文本理解。我们发现，这种结合可以提高多个医学变量的提取。此外，我们发现了现成的自由LLM在检索能力方面与商业LLM相当。最后，我们讨论了LLM的导航需要通过象征逻辑的应用，因为纯粹使用LLM会导致性能最低。
</details></li>
</ul>
<hr>
<h2 id="LoRA-FA-Memory-efficient-Low-rank-Adaptation-for-Large-Language-Models-Fine-tuning"><a href="#LoRA-FA-Memory-efficient-Low-rank-Adaptation-for-Large-Language-Models-Fine-tuning" class="headerlink" title="LoRA-FA: Memory-efficient Low-rank Adaptation for Large Language Models Fine-tuning"></a>LoRA-FA: Memory-efficient Low-rank Adaptation for Large Language Models Fine-tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03303">http://arxiv.org/abs/2308.03303</a></li>
<li>repo_url: None</li>
<li>paper_authors: Longteng Zhang, Lin Zhang, Shaohuai Shi, Xiaowen Chu, Bo Li</li>
<li>For:	+ The paper is written for fine-tuning large language models (LLMs) with low-rank adaptation (LoRA) method.* Methods:	+ The LoRA-FA method chooses to freeze the projection-down weight of $A$ and update the projection-up weight of $B$ in each LoRA layer.	+ The method eliminates the requirement to store full-rank input activations, reducing the activation memory without performance degradation and expensive recomputation.* Results:	+ The LoRA-FA method achieves close fine-tuning accuracy across different tasks compared to full parameter fine-tuning and LoRA.	+ The method reduces the overall memory cost by up to 1.4 times compared to LoRA.Here’s the simplified Chinese text:* For:	+ 这篇论文是为大型自然语言模型（LLM）的精度调整（LoRA）方法而写的。* Methods:	+ LoRA-FA方法选择在每个LoRA层中冻结投影下重量$A$，并更新投影上重量$B$。	+ 方法消除了需要存储完整Activation的存储要求，从而降低了活动内存的消耗，不会影响性能和费用计算。* Results:	+ LoRA-FA方法在不同任务上都能达到与全参数调整和LoRA相同的精度。	+ 方法可以将总内存成本降低到LoRA的1.4倍。<details>
<summary>Abstract</summary>
The low-rank adaptation (LoRA) method can largely reduce the amount of trainable parameters for fine-tuning large language models (LLMs), however, it still requires expensive activation memory to update low-rank weights. Reducing the number of LoRA layers or using activation recomputation could harm the fine-tuning performance or increase the computational overhead. In this work, we present LoRA-FA, a memory-efficient fine-tuning method that reduces the activation memory without performance degradation and expensive recomputation. LoRA-FA chooses to freeze the projection-down weight of $A$ and update the projection-up weight of $B$ in each LoRA layer. It ensures the change of model weight reside in a low-rank space during LLMs fine-tuning, while eliminating the requirement to store full-rank input activations. We conduct extensive experiments across multiple model types (RoBERTa, T5, LLaMA) and model scales. Our results show that LoRA-FA can always achieve close fine-tuning accuracy across different tasks compared to full parameter fine-tuning and LoRA. Furthermore, LoRA-FA can reduce the overall memory cost by up to 1.4$\times$ compared to LoRA.
</details>
<details>
<summary>摘要</summary>
LoRA方法可以大幅减少精度调整大语言模型（LLM）的训练参数，但仍然需要费时的活动记忆更新低级 веса。减少LoRA层数或使用活动重计算可能会减少调整性能或增加计算开销。在这种工作中，我们提出了LoRA-FA，一种内存高效的调整方法，可以不需要存储全级输入活动的存储。LoRA-FA在每个LoRA层中决定将$A$的投影下降 веса冻结，而将$B$的投影上升 веса更新。这确保了模型参数的变化在LLMs调整过程中 residual在低级空间中，而无需存储全级输入活动。我们在多种模型类型（RoBERTa、T5、LLaMA）和模型规模上进行了广泛的实验。我们的结果表明，LoRA-FA可以在不同任务上实现与全参数调整和LoRA的相似精度，并且可以将总内存成本减少到1.4倍。
</details></li>
</ul>
<hr>
<h2 id="Studying-Large-Language-Model-Generalization-with-Influence-Functions"><a href="#Studying-Large-Language-Model-Generalization-with-Influence-Functions" class="headerlink" title="Studying Large Language Model Generalization with Influence Functions"></a>Studying Large Language Model Generalization with Influence Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03296">http://arxiv.org/abs/2308.03296</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Roger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner, Dustin Li, Esin Durmus, Ethan Perez, Evan Hubinger, Kamilė Lukošiūtė, Karina Nguyen, Nicholas Joseph, Sam McCandlish, Jared Kaplan, Samuel R. Bowman</li>
<li>for: 了解和 mitigate Machine Learning 模型关联的风险，可以通过哪些训练示例来了解模型的行为？</li>
<li>methods: 影响函数可以回答一个 counterfactual 问题：如果某个序列添加到训练集中， Then how would the model’s parameters and outputs change? However, influence functions are difficult to scale to large language models (LLMs) due to the difficulty of computing an inverse-Hessian-vector product (IHVP).</li>
<li>results: 我们使用 Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) 方法将影响函数扩展到 LLMs 中，并实现了类似于传统影响函数估计器的准确性，同时计算 IHVP 的速度是指数级别 slower。我们还 investigate了两种算法技巧来降低计算候选训练序列梯度的成本：TF-IDF 筛选和查询批处理。通过影响函数，我们研究了 LLMs 的泛化模式，包括泛化模式的稀缺性、增加抽象的规律、数学和编程能力、 across-lingual 泛化和角色扮演行为。 despite 许多复杂的泛化形式，我们发现一个意外的限制：影响幅在键短语顺序反转时 decay 到 Near-zero。总的来说，影响函数给我们一种强大的新工具来研究 LLMs 的泛化特性。<details>
<summary>Abstract</summary>
When trying to gain better visibility into a machine learning model in order to understand and mitigate the associated risks, a potentially valuable source of evidence is: which training examples most contribute to a given behavior? Influence functions aim to answer a counterfactual: how would the model's parameters (and hence its outputs) change if a given sequence were added to the training set? While influence functions have produced insights for small models, they are difficult to scale to large language models (LLMs) due to the difficulty of computing an inverse-Hessian-vector product (IHVP). We use the Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) approximation to scale influence functions up to LLMs with up to 52 billion parameters. In our experiments, EK-FAC achieves similar accuracy to traditional influence function estimators despite the IHVP computation being orders of magnitude faster. We investigate two algorithmic techniques to reduce the cost of computing gradients of candidate training sequences: TF-IDF filtering and query batching. We use influence functions to investigate the generalization patterns of LLMs, including the sparsity of the influence patterns, increasing abstraction with scale, math and programming abilities, cross-lingual generalization, and role-playing behavior. Despite many apparently sophisticated forms of generalization, we identify a surprising limitation: influences decay to near-zero when the order of key phrases is flipped. Overall, influence functions give us a powerful new tool for studying the generalization properties of LLMs.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:当试图更好地了解一个机器学习模型以便理解和 mitigate 相关的风险时，一个有价值的证据来源是：哪些训练示例最大程度地影响模型的行为？影响函数旨在回答一个Counterfactual问题：如果给定的序列添加到训练集中， THEN 模型的参数（以及其输出）如何变化？虽然影响函数已经生成了小型模型的情况，但是它们难以扩展到大型自然语言模型（LLMs），因为计算 inverse-Hessian-vector product（IHVP）的困难。我们使用 Eigenvalue-corrected Kronecker-Factored Approximate Curvature（EK-FAC）方法来扩展影响函数到 LLMs 中，并在520亿参数中实现了类似的准确率。我们还 investigate 两种算法技术来降低计算候选训练序列的导数的成本：TF-IDF 筛选和查询批处理。我们使用影响函数来研究 LLMs 的总化模式，包括影响模式的稀疏性、随着Scale的增长、数学和编程能力、cross-lingual总化和角色扮演行为。尽管 Apparently 出现了多种复杂的总化形式，但我们发现一个意外的限制：影响的 decay 到 near-zero 当键phrase 的顺序被反转。总之，影响函数给我们一种强大的新工具来研究 LLMs 的总化性能。
</details></li>
</ul>
<hr>
<h2 id="Dialogue-Systems-Can-Generate-Appropriate-Responses-without-the-Use-of-Question-Marks-–-Investigation-of-the-Effects-of-Question-Marks-on-Dialogue-Systems"><a href="#Dialogue-Systems-Can-Generate-Appropriate-Responses-without-the-Use-of-Question-Marks-–-Investigation-of-the-Effects-of-Question-Marks-on-Dialogue-Systems" class="headerlink" title="Dialogue Systems Can Generate Appropriate Responses without the Use of Question Marks? – Investigation of the Effects of Question Marks on Dialogue Systems"></a>Dialogue Systems Can Generate Appropriate Responses without the Use of Question Marks? – Investigation of the Effects of Question Marks on Dialogue Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03293">http://arxiv.org/abs/2308.03293</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tomoya Mizumoto, Takato Yamazaki, Katsumasa Yoshikawa, Masaya Ohagi, Toshiki Kawamoto, Toshinori Sato</li>
<li>for: 这篇论文旨在研究对话系统中问号的影响。</li>
<li>methods: 作者使用了语音识别技术和对话系统来研究问号的影响。</li>
<li>results: 研究发现，问号在对话系统中有显著的影响，并且分析了具体的示例以了解哪些类型的语音会对对话系统产生影响。<details>
<summary>Abstract</summary>
When individuals engage in spoken discourse, various phenomena can be observed that differ from those that are apparent in text-based conversation. While written communication commonly uses a question mark to denote a query, in spoken discourse, queries are frequently indicated by a rising intonation at the end of a sentence. However, numerous speech recognition engines do not append a question mark to recognized queries, presenting a challenge when creating a spoken dialogue system. Specifically, the absence of a question mark at the end of a sentence can impede the generation of appropriate responses to queries in spoken dialogue systems. Hence, we investigate the impact of question marks on dialogue systems, with the results showing that they have a significant impact. Moreover, we analyze specific examples in an effort to determine which types of utterances have the impact on dialogue systems.
</details>
<details>
<summary>摘要</summary>
当人们在口头交流中发言时，可以观察到文本对话中不同的现象。written communication通常使用问号来标示问题，而在口头交流中，问题通常由句子尾的升高声调表示。然而，许多语音识别器不会将认可的问题append到句子尾，这对创建口头对话系统带来挑战。specifically, the absence of a question mark at the end of a sentence can hinder the generation of appropriate responses to queries in spoken dialogue systems. Therefore, we investigate the impact of question marks on dialogue systems, with the results showing that they have a significant impact. In addition, we analyze specific examples to determine which types of utterances have the greatest impact on dialogue systems.Note: The word "问号" (wèn zhàng) in the text refers to the question mark symbol (?) used in written Chinese to indicate a question.
</details></li>
</ul>
<hr>
<h2 id="Towards-General-Text-Embeddings-with-Multi-stage-Contrastive-Learning"><a href="#Towards-General-Text-Embeddings-with-Multi-stage-Contrastive-Learning" class="headerlink" title="Towards General Text Embeddings with Multi-stage Contrastive Learning"></a>Towards General Text Embeddings with Multi-stage Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03281">http://arxiv.org/abs/2308.03281</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang</li>
<li>for: 该论文旨在提出一种通用的文本嵌入模型，通过多stage对比学习来训练。</li>
<li>methods: 该模型使用多种dataset并在不同阶段进行对比学习，以提高表示能力。</li>
<li>results: 该模型在文本嵌入benchmark上表现出色，比之前的模型更高效，并在不需要进一步 fine-tuning 的情况下，在代码检索任务上也表现出优异。<details>
<summary>Abstract</summary>
We present GTE, a general-purpose text embedding model trained with multi-stage contrastive learning. In line with recent advancements in unifying various NLP tasks into a single format, we train a unified text embedding model by employing contrastive learning over a diverse mixture of datasets from multiple sources. By significantly increasing the number of training data during both unsupervised pre-training and supervised fine-tuning stages, we achieve substantial performance gains over existing embedding models. Notably, even with a relatively modest parameter count of 110M, GTE$_\text{base}$ outperforms the black-box embedding API provided by OpenAI and even surpasses 10x larger text embedding models on the massive text embedding benchmark. Furthermore, without additional fine-tuning on each programming language individually, our model outperforms previous best code retrievers of similar size by treating code as text. In summary, our model achieves impressive results by effectively harnessing multi-stage contrastive learning, offering a powerful and efficient text embedding model with broad applicability across various NLP and code-related tasks.
</details>
<details>
<summary>摘要</summary>
我们介绍GTE，一种通用文本嵌入模型，通过多阶段对比学习训练。随着现代NPLTasks的统一，我们使用对比学习训练一种多元数据集合的通用文本嵌入模型，并在不同来源的数据集上进行了大量的训练数据增加。这使得GTE在现有嵌入模型的基础上实现了显著性能提升。特别是，即使使用110M个参数，GTE$_\text{base}$仍然可以超越OpenAI提供的黑盒嵌入API以及10倍大的文本嵌入模型在庞大文本嵌入 benchmark 上。此外，不需要额外 fine-tuning 每种编程语言，我们的模型可以在与类似大小的前一代最佳代码搜索器相比超越它们。总之，我们的模型通过有效地利用多阶段对比学习，提供了一种强大和高效的文本嵌入模型，可以广泛应用于不同的NPLTasks和代码相关任务。
</details></li>
</ul>
<hr>
<h2 id="UniversalNER-Targeted-Distillation-from-Large-Language-Models-for-Open-Named-Entity-Recognition"><a href="#UniversalNER-Targeted-Distillation-from-Large-Language-Models-for-Open-Named-Entity-Recognition" class="headerlink" title="UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition"></a>UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03279">http://arxiv.org/abs/2308.03279</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenxuan Zhou, Sheng Zhang, Yu Gu, Muhao Chen, Hoifung Poon<br>for: 这个论文的目的是用具体的任务准则（mission-focused instruction tuning）来训练更加成本效益的模型，以便在广泛的应用领域中具有优秀的表现。methods: 这篇论文使用了命名实体识别（NER）作为案例研究，通过减少ChatGPT模型的参数量，以训练更小的UniversalNER模型，以便在开放的NER任务上达到高度的准确率。results: 研究发现，无需使用直接监督，UniversalNER模型可以在多个领域和数据集上达到remarkable的NER准确率，并在average上超过了Alpaca和Vicuna模型以及InstructUIE系统。此外，UniversalNER模型还可以在不同的任务上进行多任务学习，并且可以在不同的数据集上进行适应性的学习。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated remarkable generalizability, such as understanding arbitrary entities and relations. Instruction tuning has proven effective for distilling LLMs into more cost-efficient models such as Alpaca and Vicuna. Yet such student models still trail the original LLMs by large margins in downstream applications. In this paper, we explore targeted distillation with mission-focused instruction tuning to train student models that can excel in a broad application class such as open information extraction. Using named entity recognition (NER) for case study, we show how ChatGPT can be distilled into much smaller UniversalNER models for open NER. For evaluation, we assemble the largest NER benchmark to date, comprising 43 datasets across 9 diverse domains such as biomedicine, programming, social media, law, finance. Without using any direct supervision, UniversalNER attains remarkable NER accuracy across tens of thousands of entity types, outperforming general instruction-tuned models such as Alpaca and Vicuna by over 30 absolute F1 points in average. With a tiny fraction of parameters, UniversalNER not only acquires ChatGPT's capability in recognizing arbitrary entity types, but also outperforms its NER accuracy by 7-9 absolute F1 points in average. Remarkably, UniversalNER even outperforms by a large margin state-of-the-art multi-task instruction-tuned systems such as InstructUIE, which uses supervised NER examples. We also conduct thorough ablation studies to assess the impact of various components in our distillation approach. We will release the distillation recipe, data, and UniversalNER models to facilitate future research on targeted distillation.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经表现出了很好的通用性，例如理解任意实体和关系。 instrucion 调教已经证明可以将 LLM 转化为更加Cost-efficient的模型，如阿LPACA和维纳纳。然而，这些学生模型仍然在下游应用中落后于原始 LLM 多达几个百分点。在这篇论文中，我们 explore 目标调教与使命匹配的 instrucion 调教来训练学生模型，以便在广泛的应用领域中 excel。使用名实体识别（NER）为案例研究，我们示出了如何通过 Targeted Distillation 将 ChatGPT 训练成更加小的 UniversalNER 模型，以便在开放的 NER 应用中进行识别。为了评估，我们组织了历史上最大的 NER benchmark，包括43个数据集，覆盖9个多样化的领域，如生物医学、编程、社交媒体、法律、金融等。无需直接监督，UniversalNER 在 tens of thousands 个实体类型中实现了惊人的 NER 准确率，比如常规的 instrucion-tuned 模型（如阿LPACA和维纳纳）多达30个Absolute F1点的提升。同时，UniversalNER 不仅获得了 ChatGPT 可以识别任意实体类型的能力，还在 NER 准确率方面超过了 ChatGPT 的表现，提高了7-9个Absolute F1点的提升。凯于此，UniversalNER 甚至超过了当前state-of-the-art的多任务 instrucion-tuned 系统（InstructUIE），该系统使用了supervised NER例子。我们还进行了严格的减少研究，以评估各种组件在我们的调教方法中的影响。我们将发布调教方法、数据和 UniversalNER 模型，以便未来研究人员可以通过目标调教来进一步提高模型的性能。
</details></li>
</ul>
<hr>
<h2 id="From-Ambiguity-to-Explicitness-NLP-Assisted-5G-Specification-Abstraction-for-Formal-Analysis"><a href="#From-Ambiguity-to-Explicitness-NLP-Assisted-5G-Specification-Abstraction-for-Formal-Analysis" class="headerlink" title="From Ambiguity to Explicitness: NLP-Assisted 5G Specification Abstraction for Formal Analysis"></a>From Ambiguity to Explicitness: NLP-Assisted 5G Specification Abstraction for Formal Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03277">http://arxiv.org/abs/2308.03277</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiyu Yuan, Jingda Yang, Sudhanshu Arya, Carlo Lipizzi, Ying Wang</li>
<li>for: 本研究旨在提高5G无线通信协议的正式分析效率，尤其在设计阶段，以便更好地发现逻辑漏洞和进行全面的安全评估。</li>
<li>methods: 本研究提出了一种hybrid方法，首先使用自然语言处理（NLP）工具将协议文档转换为数据，然后使用 constructed数据提取标识符和正式属性。最后，使用模型来验证标识符和正式属性的正确性。</li>
<li>results: 本研究实现了三种不同的依赖关系模型，其中最佳模型可达到有效率39% для标识符提取和42% для正式属性预测。这些结果证明了我们的方法的可行性和效果，并预示了对大规模复杂规格和协议分析的高效方法。<details>
<summary>Abstract</summary>
Formal method-based analysis of the 5G Wireless Communication Protocol is crucial for identifying logical vulnerabilities and facilitating an all-encompassing security assessment, especially in the design phase. Natural Language Processing (NLP) assisted techniques and most of the tools are not widely adopted by the industry and research community. Traditional formal verification through a mathematics approach heavily relied on manual logical abstraction prone to being time-consuming, and error-prone. The reason that the NLP-assisted method did not apply in industrial research may be due to the ambiguity in the natural language of the protocol designs nature is controversial to the explicitness of formal verification. To address the challenge of adopting the formal methods in protocol designs, targeting (3GPP) protocols that are written in natural language, in this study, we propose a hybrid approach to streamline the analysis of protocols. We introduce a two-step pipeline that first uses NLP tools to construct data and then uses constructed data to extract identifiers and formal properties by using the NLP model. The identifiers and formal properties are further used for formal analysis. We implemented three models that take different dependencies between identifiers and formal properties as criteria. Our results of the optimal model reach valid accuracy of 39% for identifier extraction and 42% for formal properties predictions. Our work is proof of concept for an efficient procedure in performing formal analysis for largescale complicate specification and protocol analysis, especially for 5G and nextG communications.
</details>
<details>
<summary>摘要</summary>
formal方法基础的分析对5G无线通信协议是关键的，尤其在设计阶段。自然语言处理（NLP）助け的技术和工具在行业和研究社区中并不很受欢迎。传统的形式验证通过数学方法，强调手动逻辑归纳，容易占用时间和容易出错。因为自然语言协议设计的语言性是 controvertible，NLP助け的方法在工业研究中并未得到广泛采用。为了解决协议设计中的形式方法采用的挑战，我们在本研究中提出了一种混合方法。我们提出了一个两步管道，首先使用NLP工具生成数据，然后使用生成的数据提取标识符和形式属性，并用NLP模型进行预测。标识符和形式属性被用于形式分析。我们实现了三个模型，它们根据标识符和形式属性之间的依赖关系作为优化标准。我们的结果表明，我们的优化模型可以达到有效率的39% для标识符提取和42% для形式属性预测。我们的工作是一种有效的方法，用于对大规模复杂规格和协议分析进行有效的形式分析，特别是对5G和nextG通信。
</details></li>
</ul>
<hr>
<h2 id="Adapter-based-Selective-Knowledge-Distillation-for-Federated-Multi-domain-Meeting-Summarization"><a href="#Adapter-based-Selective-Knowledge-Distillation-for-Federated-Multi-domain-Meeting-Summarization" class="headerlink" title="Adapter-based Selective Knowledge Distillation for Federated Multi-domain Meeting Summarization"></a>Adapter-based Selective Knowledge Distillation for Federated Multi-domain Meeting Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03275">http://arxiv.org/abs/2308.03275</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiachong Feng, Xiaocheng Feng, Xiyuan Du, Min-Yen Kan, Bing Qin</li>
<li>for: 本研究旨在提供一种基于联合学习的会议笔记概要生成技术，以解决现有工作偏重中心化数据的问题。</li>
<li>methods: 本研究使用了适配器基本的概要模型，以及选择性知识填充策略，帮助客户端模型在不同领域的数据上进行强化学习。</li>
<li>results: 实验结果表明，AdaFedSelecKD可以与中央训练方法相比，在QMSum数据集上实现相似的性能，并且表现稳定和可靠。<details>
<summary>Abstract</summary>
Meeting summarization has emerged as a promising technique for providing users with condensed summaries. However, existing work has focused on training models on centralized data, neglecting real-world scenarios where meeting data are infeasible to collect centrally, due to their sensitive nature. This gap motivates us to explore federated learning for meeting summarization. Two critical challenges impede progress. First, state-of-the-art summarizers are based on parameter-heavy pre-trained models. Exchanging such a model's parameters across clients imposes large bandwidth costs. Second, as real-world meeting data belong to various domains and are distributed across clients, they are instances of non-identically and independently distributed (non-IID). IID assumptions do not hold, which changes which forms of learning algorithms best apply. To address this, we propose Adapter-based Federated Selective Knowledge Distillation (AdaFedSelecKD) for training performant client models. Specifically, we develop an adapter-based summarization model where two adapters cooperatively facilitate learning using fewer parameters to reduce communication costs. Then, we devise a selective knowledge distillation strategy, assisting clients in robustly handling domain-focused modelling on their own data, while leveraging global parameters based on non-IID data. Extensive experiments on the QMSum benchmark demonstrate AdaFedSelecKD can achieve comparable performance with powerful centralized training methods, and shows its generalizability and robustness.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SeACo-Paraformer-A-Non-Autoregressive-ASR-System-with-Flexible-and-Effective-Hotword-Customization-Ability"><a href="#SeACo-Paraformer-A-Non-Autoregressive-ASR-System-with-Flexible-and-Effective-Hotword-Customization-Ability" class="headerlink" title="SeACo-Paraformer: A Non-Autoregressive ASR System with Flexible and Effective Hotword Customization Ability"></a>SeACo-Paraformer: A Non-Autoregressive ASR System with Flexible and Effective Hotword Customization Ability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03266">http://arxiv.org/abs/2308.03266</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/r1ckshi/seaco-paraformer">https://github.com/r1ckshi/seaco-paraformer</a></li>
<li>paper_authors: Xian Shi, Yexin Yang, Zerui Li, Shiliang Zhang</li>
<li>for: 这个论文的目的是提出一种基于 semantics的语音识别系统，具有可customizable的热词定制能力。</li>
<li>methods: 该论文使用了一种新的NAR模型，combines the accuracy of AED-based model和NAR模型，并且具有良好的contextualization表现。</li>
<li>results: 在50,000小时的工业大数据实验中，该提案的模型比强基eline模型在自定义和普通语音识别任务中表现出色，并且还提出了一种高效的热词筛选方法。<details>
<summary>Abstract</summary>
Hotword customization is one of the important issues remained in ASR field - it is of value to enable users of ASR systems to customize names of entities, persons and other phrases. The past few years have seen both implicit and explicit modeling strategies for ASR contextualization developed. While these approaches have performed adequately, they still exhibit certain shortcomings such as instability in effectiveness. In this paper we propose Semantic-augmented Contextual-Paraformer (SeACo-Paraformer) a novel NAR based ASR system with flexible and effective hotword customization ability. It combines the accuracy of the AED-based model, the efficiency of the NAR model, and the excellent performance in contextualization. In 50,000 hours industrial big data experiments, our proposed model outperforms strong baselines in customization and general ASR tasks. Besides, we explore an efficient way to filter large scale incoming hotwords for further improvement. The source codes and industrial models proposed and compared are all opened as well as two hotword test sets.
</details>
<details>
<summary>摘要</summary>
“热词自定义是ASR领域中一个重要的 Issue - 它具有价值，以允许ASR系统的使用者自定义名称、人名和其他短语。过去几年，有 implicit 和 explicit 模型化策略为ASR上下文化开发出来。although these approaches have performed adequately, they still exhibit certain shortcomings such as instability in effectiveness。在这篇文章中，我们提出Semantic-augmented Contextual-Paraformer (SeACo-Paraformer) ，一种新的 NAR 基于 ASR 系统，具有灵活和有效的热词自定义能力。它结合了 AED-based 模型的精度，NAR 模型的效率，以及优秀的上下文化表现。在50,000小时的工业大数据实验中，我们的提议模型比强大的基eline在自定义和一般 ASR 任务上表现出色。此外，我们还探索了一种高效的方法来筛选大规模的来临热词，以进一步提高效能。文章中的原始代码和工业模型都已经公开，同时还提供了两个热词测试集。”
</details></li>
</ul>
<hr>
<h2 id="Exploring-Automated-Distractor-and-Feedback-Generation-for-Math-Multiple-choice-Questions-via-In-context-Learning"><a href="#Exploring-Automated-Distractor-and-Feedback-Generation-for-Math-Multiple-choice-Questions-via-In-context-Learning" class="headerlink" title="Exploring Automated Distractor and Feedback Generation for Math Multiple-choice Questions via In-context Learning"></a>Exploring Automated Distractor and Feedback Generation for Math Multiple-choice Questions via In-context Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03234">http://arxiv.org/abs/2308.03234</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hunter McNichols, Wanyong Feng, Jaewook Lee, Alexander Scarlatos, Digory Smith, Simon Woodhead, Andrew Lan</li>
<li>for: 这篇论文的目的是提出一种自动生成多项选择题（MCQ）的拥有者和相应的反馈信息的方法，以解决现有的劳动密集和限制缩放性。</li>
<li>methods: 这篇论文使用了大型自然语言模型来自动生成多项选择题的错误选项和反馈信息。它提出了一种简单的、在场景学习中进行学习的解决方案。</li>
<li>results: 该论文通过实验表明，自动生成的错误选项和反馈信息质量仍有很大的改进空间。同时，它还提出了未来研究的方向。<details>
<summary>Abstract</summary>
Multiple-choice questions (MCQs) are ubiquitous in almost all levels of education since they are easy to administer, grade, and are a reliable format in both assessments and practices. An important aspect of MCQs is the distractors, i.e., incorrect options that are designed to target specific misconceptions or insufficient knowledge among students. To date, the task of crafting high-quality distractors has largely remained a labor-intensive process for teachers and learning content designers, which has limited scalability. In this work, we explore the task of automated distractor and corresponding feedback message generation in math MCQs using large language models. We establish a formulation of these two tasks and propose a simple, in-context learning-based solution. Moreover, we explore using two non-standard metrics to evaluate the quality of the generated distractors and feedback messages. We conduct extensive experiments on these tasks using a real-world MCQ dataset that contains student response information. Our findings suggest that there is a lot of room for improvement in automated distractor and feedback generation. We also outline several directions for future work
</details>
<details>
<summary>摘要</summary>
多选问题（MCQ）在教育中是非常普遍的，因为它们容易进行管理、评分和练习。MCQ中的错误选项（distractors）是一个重要的特点，它们需要针对学生的错误观点或知识不足进行设计。然而，制作高质量的错误选项仍然是一项劳动密集的任务，这限制了大规模应用。在这篇文章中，我们探讨了自动生成MCQ中的错误选项和相应的反馈消息，使用大型自然语言模型。我们提出了一种简单的、在场景学习中进行学习的解决方案。此外，我们还使用了两种非标准度量来评估生成的错误选项和反馈消息的质量。我们在实际的MCQ数据集上进行了广泛的实验，我们的发现表明了自动生成错误选项和反馈消息还有很大的可改进空间。我们还提出了未来工作的一些方向。
</details></li>
</ul>
<hr>
<h2 id="Average-Hard-Attention-Transformers-are-Constant-Depth-Uniform-Threshold-Circuits"><a href="#Average-Hard-Attention-Transformers-are-Constant-Depth-Uniform-Threshold-Circuits" class="headerlink" title="Average-Hard Attention Transformers are Constant-Depth Uniform Threshold Circuits"></a>Average-Hard Attention Transformers are Constant-Depth Uniform Threshold Circuits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03212">http://arxiv.org/abs/2308.03212</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lena Strobl</li>
<li>for: 这paper是为了研究transformer模型和常深度阈值电路之间的关系。</li>
<li>methods: 这paper使用了average-hard attention和logarithmic precision两个假设，以及constant-depth threshold circuits来模型语言。</li>
<li>results: 这paper证明了average-hard attention transformers可以recognizeTC0类语言，而log-precision transformers可以recognize uniform TC0类语言。这两个result都表明transformer模型可以被constant-depth threshold circuits模型。<details>
<summary>Abstract</summary>
Transformers have emerged as a widely used neural network model for various natural language processing tasks. Previous research explored their relationship with constant-depth threshold circuits, making two assumptions: average-hard attention and logarithmic precision for internal computations relative to input length. Merrill et al. (2022) prove that average-hard attention transformers recognize languages that fall within the complexity class TC0, denoting the set of languages that can be recognized by constant-depth polynomial-size threshold circuits. Likewise, Merrill and Sabharwal (2023) show that log-precision transformers recognize languages within the class of uniform TC0. This shows that both transformer models can be simulated by constant-depth threshold circuits, with the latter being more robust due to generating a uniform circuit family. Our paper shows that the first result can be extended to yield uniform circuits as well.
</details>
<details>
<summary>摘要</summary>
transformers 已经成为自然语言处理任务中广泛使用的神经网络模型。前一个研究探讨了它们与常 depth 阈值电路之间的关系，并假设了两个假设：均值困难注意力和对内部计算的对数精度相对于输入长度。Merrill et al. (2022) 证明了 average-hard 注意力 transformers 可以认出 fall 在 TC0 复杂性类中的语言，其中 TC0 表示可以通过常 depth 多项式大小阈值电路来认出的语言。另外，Merrill 和 Sabharwal (2023) 表明 log-precision transformers 可以认出 uniform TC0 类中的语言。这表明两种 transformer 模型都可以被模拟为常 depth 阈值电路，其中后一种更加稳定，因为它生成了一个 uniform 电路家族。我们的论文显示，第一个结果可以被推广到生成 uniform 电路。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/07/cs.CL_2023_08_07/" data-id="clp9qz812009wok887euuhpt5" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/07/cs.LG_2023_08_07/" class="article-date">
  <time datetime="2023-08-07T10:00:00.000Z" itemprop="datePublished">2023-08-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/07/cs.LG_2023_08_07/">cs.LG - 2023-08-07</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Improving-FHB-Screening-in-Wheat-Breeding-Using-an-Efficient-Transformer-Model"><a href="#Improving-FHB-Screening-in-Wheat-Breeding-Using-an-Efficient-Transformer-Model" class="headerlink" title="Improving FHB Screening in Wheat Breeding Using an Efficient Transformer Model"></a>Improving FHB Screening in Wheat Breeding Using an Efficient Transformer Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03670">http://arxiv.org/abs/2308.03670</a></li>
<li>repo_url: None</li>
<li>paper_authors: Babak Azad, Ahmed Abdalla, Kwanghee Won, Ali Mirzakhani Nafchi</li>
<li>For: The paper is written for the early detection of Fusarium head blight (FHB) in wheat and barley breeding programs.* Methods: The paper proposes a new Context Bridge to integrate the local representation capability of the U-Net network in the transformer model, and replaces the standard attention mechanism with Efficient Self-attention.* Results: The proposed transformer-based method is effective for FHB-disease detection, as demonstrated through extensive experiments across typical tasks for plant image segmentation.Here’s the simplified Chinese text for the three key points:* 用: 本文为小麦和麦角束缚程序中早期检测 fusarium head blight (FHB) 的研究。* 方法: 本文提出一种新的 Context Bridge，将 U-Net 网络的本地表示能力与 transformer 模型结合，并将标准的注意机制 replaced 为 Efficient Self-attention。* 结果: 提议的 transformer-based 方法在 Plant image segmentation 等 typical tasks 中得到了广泛的实验证明，表明其效果是可靠的。<details>
<summary>Abstract</summary>
Fusarium head blight is a devastating disease that causes significant economic losses annually on small grains. Efficiency, accuracy, and timely detection of FHB in the resistance screening are critical for wheat and barley breeding programs. In recent years, various image processing techniques have been developed using supervised machine learning algorithms for the early detection of FHB. The state-of-the-art convolutional neural network-based methods, such as U-Net, employ a series of encoding blocks to create a local representation and a series of decoding blocks to capture the semantic relations. However, these methods are not often capable of long-range modeling dependencies inside the input data, and their ability to model multi-scale objects with significant variations in texture and shape is limited. Vision transformers as alternative architectures with innate global self-attention mechanisms for sequence-to-sequence prediction, due to insufficient low-level details, may also limit localization capabilities. To overcome these limitations, a new Context Bridge is proposed to integrate the local representation capability of the U-Net network in the transformer model. In addition, the standard attention mechanism of the original transformer is replaced with Efficient Self-attention, which is less complicated than other state-of-the-art methods. To train the proposed network, 12,000 wheat images from an FHB-inoculated wheat field at the SDSU research farm in Volga, SD, were captured. In addition to healthy and unhealthy plants, these images encompass various stages of the disease. A team of expert pathologists annotated the images for training and evaluating the developed model. As a result, the effectiveness of the transformer-based method for FHB-disease detection, through extensive experiments across typical tasks for plant image segmentation, is demonstrated.
</details>
<details>
<summary>摘要</summary>
fusarium 头病是一种致命的疾病，每年对小麦和麦角造成重大经济损失。效率、准确性和时间检测 fusarium 头病在抗性屏选中是关键，以便为小麦和麦角推广 програм序。在最近几年，Various image processing techniques 有 been developed using supervised machine learning algorithms for the early detection of fusarium 头病。state-of-the-art convolutional neural network-based methods, such as U-Net, employ a series of encoding blocks to create a local representation and a series of decoding blocks to capture the semantic relations。However, these methods are not often capable of long-range modeling dependencies inside the input data, and their ability to model multi-scale objects with significant variations in texture and shape is limited。In order to overcome these limitations, a new Context Bridge is proposed to integrate the local representation capability of the U-Net network in the transformer model。In addition, the standard attention mechanism of the original transformer is replaced with Efficient Self-attention, which is less complicated than other state-of-the-art methods。To train the proposed network, 12,000 wheat images from an fusarium 头病-inoculated wheat field at the SDSU research farm in Volga, SD, were captured。In addition to healthy and unhealthy plants, these images encompass various stages of the disease。A team of expert pathologists annotated the images for training and evaluating the developed model。As a result, the effectiveness of the transformer-based method for fusarium 头病-disease detection, through extensive experiments across typical tasks for plant image segmentation, is demonstrated。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Model-in-Causal-Inference-with-Unmeasured-Confounders"><a href="#Diffusion-Model-in-Causal-Inference-with-Unmeasured-Confounders" class="headerlink" title="Diffusion Model in Causal Inference with Unmeasured Confounders"></a>Diffusion Model in Causal Inference with Unmeasured Confounders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03669">http://arxiv.org/abs/2308.03669</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tatsu432/BDCM">https://github.com/tatsu432/BDCM</a></li>
<li>paper_authors: Tatsuhiro Shimizu</li>
<li>For: 本研究旨在扩展Diffusion Model，以回答基于观察数据的 causal 问题，在存在隐藏 confounder 的情况下。* Methods: 我们使用 Pearl 的 Directed Acyclic Graph (DAG) 框架，并提出了一种Diffusion-based Causal Model (DCM)，即将 diffusion model  incorporated into DAG 来更准确地回答 causal 问题，假设所有 confounder 都是观察的。但在实践中，隐藏 confounder 存在，这限制了 DCM 的可应用范围。为了解决这种限制，我们提出了一种扩展模型 called Backdoor Criterion based DCM (BDCM)，其基于 Backdoor  criterion 来找出 DAG 中需要包含在 decoding 过程中的变量，以便在隐藏 confounder 的情况下扩展 DCM。* Results: 我们通过 synthetic data 实验表明，我们的提议的模型能够更 precisely 捕捉 counterfactual distribution than DCM under unmeasured confounders。<details>
<summary>Abstract</summary>
We study how to extend the use of the diffusion model to answer the causal question from the observational data under the existence of unmeasured confounders. In Pearl's framework of using a Directed Acyclic Graph (DAG) to capture the causal intervention, a Diffusion-based Causal Model (DCM) was proposed incorporating the diffusion model to answer the causal questions more accurately, assuming that all of the confounders are observed. However, unmeasured confounders in practice exist, which hinders DCM from being applicable. To alleviate this limitation of DCM, we propose an extended model called Backdoor Criterion based DCM (BDCM), whose idea is rooted in the Backdoor criterion to find the variables in DAG to be included in the decoding process of the diffusion model so that we can extend DCM to the case with unmeasured confounders. Synthetic data experiment demonstrates that our proposed model captures the counterfactual distribution more precisely than DCM under the unmeasured confounders.
</details>
<details>
<summary>摘要</summary>
我们研究如何使用扩散模型回答从观察数据中的 causal 问题，在存在未探测的干扰变量的情况下。在珍珠的框架中，使用直接径向图（DAG）捕捉 causal 干预，一种扩散基于 causal 模型（DCM）被提出，假设所有干扰变量都是观察的。然而，在实践中存在未探测的干扰变量，这限制了 DCM 的应用。为了解决 DCM 中未探测干扰变量的问题，我们提出一种扩展模型，即 Backdoor  criterion 基于 DCM（BDCM），其基于 Backdoor  criterion 来找出 DAG 中需要包含在扩散模型中的变量，以便在未探测干扰变量的情况下扩展 DCM。synthetic 数据实验表明，我们的提议的模型可以更准确地回答 counterfactual 分布，比 DCM 在未探测干扰变量的情况下。
</details></li>
</ul>
<hr>
<h2 id="Bridging-Trustworthiness-and-Open-World-Learning-An-Exploratory-Neural-Approach-for-Enhancing-Interpretability-Generalization-and-Robustness"><a href="#Bridging-Trustworthiness-and-Open-World-Learning-An-Exploratory-Neural-Approach-for-Enhancing-Interpretability-Generalization-and-Robustness" class="headerlink" title="Bridging Trustworthiness and Open-World Learning: An Exploratory Neural Approach for Enhancing Interpretability, Generalization, and Robustness"></a>Bridging Trustworthiness and Open-World Learning: An Exploratory Neural Approach for Enhancing Interpretability, Generalization, and Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03666">http://arxiv.org/abs/2308.03666</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shide Du, Zihan Fang, Shiyang Lan, Yanchao Tan, Manuel Günther, Shiping Wang, Wenzhong Guo</li>
<li>for: 提高人工智能系统的可靠性和多模式学习能力</li>
<li>methods: 使用自定义可靠网络、灵活学习正则化和开放世界认知损失来提高可靠性和多模式学习</li>
<li>results: 实现了开放世界多模式认知任务的高性能提升<details>
<summary>Abstract</summary>
As researchers strive to narrow the gap between machine intelligence and human through the development of artificial intelligence technologies, it is imperative that we recognize the critical importance of trustworthiness in open-world, which has become ubiquitous in all aspects of daily life for everyone. However, several challenges may create a crisis of trust in current artificial intelligence systems that need to be bridged: 1) Insufficient explanation of predictive results; 2) Inadequate generalization for learning models; 3) Poor adaptability to uncertain environments. Consequently, we explore a neural program to bridge trustworthiness and open-world learning, extending from single-modal to multi-modal scenarios for readers. 1) To enhance design-level interpretability, we first customize trustworthy networks with specific physical meanings; 2) We then design environmental well-being task-interfaces via flexible learning regularizers for improving the generalization of trustworthy learning; 3) We propose to increase the robustness of trustworthy learning by integrating open-world recognition losses with agent mechanisms. Eventually, we enhance various trustworthy properties through the establishment of design-level explainability, environmental well-being task-interfaces and open-world recognition programs. These designed open-world protocols are applicable across a wide range of surroundings, under open-world multimedia recognition scenarios with significant performance improvements observed.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Insufficient explanation of predictive results: It is difficult to understand why the system made a particular prediction or decision.2. Inadequate generalization for learning models: The system may not perform well when faced with new or unfamiliar situations.3. Poor adaptability to uncertain environments: The system may not be able to handle unexpected events or changes in the environment.To address these challenges, we propose a neural program that bridges trustworthiness and open-world learning, extending from single-modal to multi-modal scenarios for readers. Our approach includes the following three components:1. Design-level interpretability: We first customize trustworthy networks with specific physical meanings, making it easier to understand how the system works and why it makes certain predictions or decisions.2. Environmental well-being task-interfaces: We design flexible learning regularizers to improve the generalization of trustworthy learning, allowing the system to adapt to new situations and environments.3. Open-world recognition programs: We integrate open-world recognition losses with agent mechanisms to increase the robustness of trustworthy learning, enabling the system to handle unexpected events and changes in the environment.By enhancing various trustworthy properties through these designed open-world protocols, we observe significant performance improvements across a wide range of surroundings, under open-world multimedia recognition scenarios. These protocols are applicable to a variety of environments, including but not limited to:1. Image recognition: The system can accurately recognize objects and scenes in images, even in the presence of noise or other challenges.2. Speech recognition: The system can accurately transcribe spoken language, even in noisy or unfamiliar environments.3. Natural language processing: The system can understand and respond to natural language inputs, even in complex or ambiguous situations.Overall, our approach to trustworthy open-world learning has the potential to significantly improve the performance and reliability of artificial intelligence systems, enabling them to better serve the needs of users in a wide range of contexts.</details></li>
</ol>
<hr>
<h2 id="Distributionally-Robust-Classification-on-a-Data-Budget"><a href="#Distributionally-Robust-Classification-on-a-Data-Budget" class="headerlink" title="Distributionally Robust Classification on a Data Budget"></a>Distributionally Robust Classification on a Data Budget</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03821">http://arxiv.org/abs/2308.03821</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/penfever/vlhub">https://github.com/penfever/vlhub</a></li>
<li>paper_authors: Benjamin Feuer, Ameya Joshi, Minh Pham, Chinmay Hegde</li>
<li>for: 该论文目的是研究如何在数据有限的情况下培养可靠的深度学习模型。</li>
<li>methods: 该论文使用了一系列新的训练数据集和精心控制的调查来研究对于图像分类的Robustness的因素。</li>
<li>results: 研究发现，使用标准的ResNet-50模型，训练时使用交叉熵损失，可以在240万个图像样本上达到与CLIP模型训练400万样本后的Robustness水平。这是我们知道的第一个在有限数据预算下实现（近）顶尖分布robustness的结果。<details>
<summary>Abstract</summary>
Real world uses of deep learning require predictable model behavior under distribution shifts. Models such as CLIP show emergent natural distributional robustness comparable to humans, but may require hundreds of millions of training samples. Can we train robust learners in a domain where data is limited? To rigorously address this question, we introduce JANuS (Joint Annotations and Names Set), a collection of four new training datasets with images, labels, and corresponding captions, and perform a series of carefully controlled investigations of factors contributing to robustness in image classification, then compare those results to findings derived from a large-scale meta-analysis. Using this approach, we show that standard ResNet-50 trained with the cross-entropy loss on 2.4 million image samples can attain comparable robustness to a CLIP ResNet-50 trained on 400 million samples. To our knowledge, this is the first result showing (near) state-of-the-art distributional robustness on limited data budgets. Our dataset is available at \url{https://huggingface.co/datasets/penfever/JANuS_dataset}, and the code used to reproduce our experiments can be found at \url{https://github.com/penfever/vlhub/}.
</details>
<details>
<summary>摘要</summary>
real-world 应用需要深度学习模型在分布变化下具有预测性的行为。例如，CLIP 模型表现出了自然的分布强度性，但可能需要百万个训练样本。可以在有限数据量的领域中训练强健的学习者吗？为了彻底回答这个问题，我们介绍了 JANuS（共同注释和名称集），一个包含四个新的训练集，包括图像、标签和相应的描述，并进行了一系列严格控制的调查，以研究影响模型强度的因素。我们发现，使用权重平均损失函数，只需训练标准 ResNet-50 模型，可以在240万张图像样本上达到与 CLIP ResNet-50 模型在400万样本上的相似水平的分布强度性。我们认为这是首次在有限数据预算下实现（近）状态时的分布强度性的研究结果。我们的数据集可以在 \url{https://huggingface.co/datasets/penfever/JANuS_dataset} 上下载，并且使用来复制我们的实验代码可以在 \url{https://github.com/penfever/vlhub/} 上找到。
</details></li>
</ul>
<hr>
<h2 id="Two-stage-Early-Prediction-Framework-of-Remaining-Useful-Life-for-Lithium-ion-Batteries"><a href="#Two-stage-Early-Prediction-Framework-of-Remaining-Useful-Life-for-Lithium-ion-Batteries" class="headerlink" title="Two-stage Early Prediction Framework of Remaining Useful Life for Lithium-ion Batteries"></a>Two-stage Early Prediction Framework of Remaining Useful Life for Lithium-ion Batteries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03664">http://arxiv.org/abs/2308.03664</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dhruv Mittal, Hymalai Bello, Bo Zhou, Mayank Shekhar Jha, Sungho Suh, Paul Lukowicz</li>
<li>for: 预测 Lithium-ion 电池的有用寿命 (RUL)，以提高各种业务中的电池管理可靠性和可维护性。</li>
<li>methods: 提议的方法包括两个阶段：使用神经网络模型确定首次预测周期 (FPC)，然后使用预测衰落特征来估算剩余有用寿命。</li>
<li>results: 实验结果显示，提议的方法在 RUL 预测方面表现出色，比传统方法更准确和可靠。此外，该方法在实际应用中也表现了承诺，提供了更好的准确性和可应用性。<details>
<summary>Abstract</summary>
Early prediction of remaining useful life (RUL) is crucial for effective battery management across various industries, ranging from household appliances to large-scale applications. Accurate RUL prediction improves the reliability and maintainability of battery technology. However, existing methods have limitations, including assumptions of data from the same sensors or distribution, foreknowledge of the end of life (EOL), and neglect to determine the first prediction cycle (FPC) to identify the start of the unhealthy stage. This paper proposes a novel method for RUL prediction of Lithium-ion batteries. The proposed framework comprises two stages: determining the FPC using a neural network-based model to divide the degradation data into distinct health states and predicting the degradation pattern after the FPC to estimate the remaining useful life as a percentage. Experimental results demonstrate that the proposed method outperforms conventional approaches in terms of RUL prediction. Furthermore, the proposed method shows promise for real-world scenarios, providing improved accuracy and applicability for battery management.
</details>
<details>
<summary>摘要</summary>
早期预测电池剩余有用生命（RUL）是跨多个领域的关键，从家用电器到大规模应用。准确的RUL预测提高电池技术的可靠性和维护性。然而，现有方法有限，包括同感知数据的假设、结束生命阶段（EOL）的先知识和忽略第一预测周期（FPC）来确定开始不健康阶段。这篇论文提出了一种新的Li-ion电池RUL预测方法。该框架包括两个阶段：使用神经网络模型将衰减数据分为不同的健康状态，并预测衰减模式以估算剩余有用生命的百分数。实验结果表明，提案方法在RUL预测方面表现出了明显的优势，并在实际场景中展现出了改善的准确性和可用性。
</details></li>
</ul>
<hr>
<h2 id="Matrix-Completion-in-Almost-Verification-Time"><a href="#Matrix-Completion-in-Almost-Verification-Time" class="headerlink" title="Matrix Completion in Almost-Verification Time"></a>Matrix Completion in Almost-Verification Time</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03661">http://arxiv.org/abs/2308.03661</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan A. Kelner, Jerry Li, Allen Liu, Aaron Sidford, Kevin Tian</li>
<li>for: 这个论文提出了一个新的低级matrix completion问题的解决方案，即从random observations中approximentate矩阵\mathbf{M}的级数为r的完全问题。</li>
<li>methods: 该论文提出了一种算法，可以在没有任何假设下completes\mathbf{M}的99%行和列在 sampling的基础上。然后，假设矩阵\mathbf{M}的行和列范围满足特定的规则性质，则可以通过融合多个回归问题的解来提高完全问题的解决方案。</li>
<li>results: 论文表明，在矩阵\mathbf{M}的行和列范围满足特定的规则性质时，可以通过 sampling的基础上completes\mathbf{M}到高精度从mr^{2+o(1)} observations中，并且runtime为mr^{3+o(1)}。此外，论文还提出了一些Robust variantsof algorithms，可以在noisy环境中completes\mathbf{M}到Frobenius norm distance的approximentatelyr^{1.5}\Delta。在这些runtimes中，可以verify that a rank-$r$ decomposition $\mathbf{U}\mathbf{V}^\top$ agrees with the sampled observations。<details>
<summary>Abstract</summary>
We give a new framework for solving the fundamental problem of low-rank matrix completion, i.e., approximating a rank-$r$ matrix $\mathbf{M} \in \mathbb{R}^{m \times n}$ (where $m \ge n$) from random observations. First, we provide an algorithm which completes $\mathbf{M}$ on $99\%$ of rows and columns under no further assumptions on $\mathbf{M}$ from $\approx mr$ samples and using $\approx mr^2$ time. Then, assuming the row and column spans of $\mathbf{M}$ satisfy additional regularity properties, we show how to boost this partial completion guarantee to a full matrix completion algorithm by aggregating solutions to regression problems involving the observations.   In the well-studied setting where $\mathbf{M}$ has incoherent row and column spans, our algorithms complete $\mathbf{M}$ to high precision from $mr^{2+o(1)}$ observations in $mr^{3 + o(1)}$ time (omitting logarithmic factors in problem parameters), improving upon the prior state-of-the-art [JN15] which used $\approx mr^5$ samples and $\approx mr^7$ time. Under an assumption on the row and column spans of $\mathbf{M}$ we introduce (which is satisfied by random subspaces with high probability), our sample complexity improves to an almost information-theoretically optimal $mr^{1 + o(1)}$, and our runtime improves to $mr^{2 + o(1)}$. Our runtimes have the appealing property of matching the best known runtime to verify that a rank-$r$ decomposition $\mathbf{U}\mathbf{V}^\top$ agrees with the sampled observations. We also provide robust variants of our algorithms that, given random observations from $\mathbf{M} + \mathbf{N}$ with $\|\mathbf{N}\|_{F} \le \Delta$, complete $\mathbf{M}$ to Frobenius norm distance $\approx r^{1.5}\Delta$ in the same runtimes as the noiseless setting. Prior noisy matrix completion algorithms [CP10] only guaranteed a distance of $\approx \sqrt{n}\Delta$.
</details>
<details>
<summary>摘要</summary>
我们提出了一个新的框架来解决低阶矩阵完成问题，即确 aproximating 一个 Rank-$r$ 矩阵 $\mathbf{M} \in \mathbb{R}^{m \times n}$ (where $m \ge n$) 从Random observations 中。首先，我们提供了一个算法，可以在不进一步假设 $\mathbf{M}$ 的情况下，从 $\approx mr$ 样本中完成 $\mathbf{M}$ 的99% 的行和列。然后，假设 $\mathbf{M}$ 的行和列范围满足其他调和的特性，我们可以通过聚合这些部分完成数据来实现全矩阵完成算法。在广泛研究的设定中，其中 $\mathbf{M}$ 的行和列范围是不对称的，我们的算法可以从 $mr^{2+o(1)}$ 样本中完成 $\mathbf{M}$ 到高精度，比以前的状况下（JN15）使用 $\approx mr^5$ 样本和 $\approx mr^7$ 时间。假设 $\mathbf{M}$ 的行和列范围满足我们引入的一个假设（这个假设在高概率下成立），我们的样本缩减到 almost information-theoretically Optimal $mr^{1+o(1)}$，并且我们的时间缩减到 $mr^{2+o(1)}$。我们的时间有着愉悦的性质，与最好的知识理论时间匹配，以确认 $\mathbf{U}\mathbf{V}^\top$ 是否对样本一致。我们还提供了一些强健的算法，可以在 $\mathbf{M} + \mathbf{N}$ 中的随机样本中完成 $\mathbf{M}$，其中 $\|\mathbf{N}\|_{F} \le \Delta$。这些算法可以在同一个时间中完成这些任务，并且可以保证完成结果的 Frobenius 误差在 $\approx r^{1.5}\Delta$ 之间。与之前的噪音矩阵完成算法（CP10）相比，这些算法可以提供更好的误差保证，即 $\approx \sqrt{n}\Delta$。
</details></li>
</ul>
<hr>
<h2 id="Generative-Forests"><a href="#Generative-Forests" class="headerlink" title="Generative Forests"></a>Generative Forests</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03648">http://arxiv.org/abs/2308.03648</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AlCorreia/GeFs">https://github.com/AlCorreia/GeFs</a></li>
<li>paper_authors: Richard Nock, Mathieu Guillame-Bert</li>
<li>for: 本文主要针对Tabular数据的生成和模型化问题，旨在提出新的树状生成模型和训练算法，以解决现有方法的三大问题。</li>
<li>methods: 本文提出了一种基于树状模型的生成模型，可以快速生成高质量的Tabular数据，同时保证模型的可读性和可视化性。此外，本文还提出了一种基于树状模型的训练算法，可以简化之前的训练设定和 Display boosting-compatible convergence。</li>
<li>results: 实验表明，本文的方法可以在缺失数据补充和生成数据比较真实数据的问题上达到remarkable results，特别是在比较于state-of-the-art方法的情况下。<details>
<summary>Abstract</summary>
Tabular data represents one of the most prevalent form of data. When it comes to data generation, many approaches would learn a density for the data generation process, but would not necessarily end up with a sampler, even less so being exact with respect to the underlying density. A second issue is on models: while complex modeling based on neural nets thrives in image or text generation (etc.), less is known for powerful generative models on tabular data. A third problem is the visible chasm on tabular data between training algorithms for supervised learning with remarkable properties (e.g. boosting), and a comparative lack of guarantees when it comes to data generation. In this paper, we tackle the three problems, introducing new tree-based generative models convenient for density modeling and tabular data generation that improve on modeling capabilities of recent proposals, and a training algorithm which simplifies the training setting of previous approaches and displays boosting-compliant convergence. This algorithm has the convenient property to rely on a supervised training scheme that can be implemented by a few tweaks to the most popular induction scheme for decision tree induction with two classes. Experiments are provided on missing data imputation and comparing generated data to real data, displaying the quality of the results obtained by our approach, in particular against state of the art.
</details>
<details>
<summary>摘要</summary>
表格数据表示一种非常常见的数据形式。在数据生成方面，许多方法会学习数据生成过程中的浓度，但并不一定会得到一个抽象，更不一定是对于下面的潜在浓度准确。第二个问题是模型：虽然复杂的模型基于神经网络在图像或文本生成等领域得到了成功，但对于可质量生成模型来说，对于表格数据 menos 知之。第三个问题是表格数据的可见差异，在超vised学习中具有惊人性能的训练算法（例如，提升），而数据生成方面却缺乏保证。在这篇论文中，我们解决了这三个问题，提出了新的树状生成模型，可以增强对表格数据的模型能力，以及一种简化训练设置的训练算法，可以在前一代方法的基础上进行快速启用。这种算法可以通过对最流行的决策树生成算法中的两类训练进行一些修改来实现。我们的实验表明，我们的方法可以在缺失数据填充和生成数据与真实数据比较的情况下表现出色，特别是与现有技术相比。
</details></li>
</ul>
<hr>
<h2 id="XFlow-Benchmarking-Flow-Behaviors-over-Graphs"><a href="#XFlow-Benchmarking-Flow-Behaviors-over-Graphs" class="headerlink" title="XFlow: Benchmarking Flow Behaviors over Graphs"></a>XFlow: Benchmarking Flow Behaviors over Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03819">http://arxiv.org/abs/2308.03819</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xgraphing/xflow">https://github.com/xgraphing/xflow</a></li>
<li>paper_authors: Zijian Zhang, Zonghan Zhang, Zhiqian Chen</li>
<li>for: 本研究旨在提供一个涵盖多领域 tasks、基线模型、图 dataset 和评估工具的新的参考库，以便研究各种传播行为在不同领域的情况下。</li>
<li>methods: 本研究使用了多种方法，包括基线模型、图 Theory 和机器学习方法，以探索各种传播行为的特点和特征。</li>
<li>results: 本研究的结果显示了现有的基础模型在不同的图dataset上的优适性和缺陷，以及可能的未来研究方向。<details>
<summary>Abstract</summary>
The occurrence of diffusion on a graph is a prevalent and significant phenomenon, as evidenced by the spread of rumors, influenza-like viruses, smart grid failures, and similar events. Comprehending the behaviors of flow is a formidable task, due to the intricate interplay between the distribution of seeds that initiate flow propagation, the propagation model, and the topology of the graph. The study of networks encompasses a diverse range of academic disciplines, including mathematics, physics, social science, and computer science. This interdisciplinary nature of network research is characterized by a high degree of specialization and compartmentalization, and the cooperation facilitated by them is inadequate. From a machine learning standpoint, there is a deficiency in a cohesive platform for assessing algorithms across various domains. One of the primary obstacles to current research in this field is the absence of a comprehensive curated benchmark suite to study the flow behaviors under network scenarios.   To address this disparity, we propose the implementation of a novel benchmark suite that encompasses a variety of tasks, baseline models, graph datasets, and evaluation tools. In addition, we present a comprehensive analytical framework that offers a generalized approach to numerous flow-related tasks across diverse domains, serving as a blueprint and roadmap. Drawing upon the outcomes of our empirical investigation, we analyze the advantages and disadvantages of current foundational models, and we underscore potential avenues for further study. The datasets, code, and baseline models have been made available for the public at: https://github.com/XGraphing/XFlow
</details>
<details>
<summary>摘要</summary>
Diffusion on graphs is a common and important phenomenon, such as the spread of rumors, influenza-like viruses, and smart grid failures. Understanding the flow behavior is a challenging task due to the complex interplay between the seed distribution, propagation model, and graph topology. Network research is an interdisciplinary field that includes mathematics, physics, social science, and computer science, but this field is characterized by a high degree of specialization and compartmentalization, and cooperation between disciplines is limited. From a machine learning perspective, there is a lack of a comprehensive platform for assessing algorithms across different domains. One of the primary obstacles to current research in this field is the absence of a comprehensive curated benchmark suite to study flow behaviors under network scenarios.To address this gap, we propose the implementation of a novel benchmark suite that includes various tasks, baseline models, graph datasets, and evaluation tools. Additionally, we present a comprehensive analytical framework that provides a generalized approach to numerous flow-related tasks across diverse domains, serving as a blueprint and roadmap. Based on our empirical investigation, we analyze the advantages and disadvantages of current foundational models and highlight potential avenues for further study. The datasets, code, and baseline models have been made publicly available at: <https://github.com/XGraphing/XFlow>.
</details></li>
</ul>
<hr>
<h2 id="MedMine-Examining-Pre-trained-Language-Models-on-Medication-Mining"><a href="#MedMine-Examining-Pre-trained-Language-Models-on-Medication-Mining" class="headerlink" title="MedMine: Examining Pre-trained Language Models on Medication Mining"></a>MedMine: Examining Pre-trained Language Models on Medication Mining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03629">http://arxiv.org/abs/2308.03629</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hecta-uom/m3">https://github.com/hecta-uom/m3</a></li>
<li>paper_authors: Haifa Alrdahi, Lifeng Han, Hendrik Šuvalov, Goran Nenadic</li>
<li>for: 本研究旨在探讨现有的预训练语言模型（PLM）在自动药物检索 task 上的表现，以便为未来研究提供参考。</li>
<li>methods: 本研究使用了 Fine-tuning 方法，包括 Med7 和 XLM-RoBERTa 两种预训练模型，以便对历史药物检索 shared task 数据集进行比较。</li>
<li>results: 研究发现现有的 PLM 在不同类型的实体和药物事件上表现不均衡，并且提出了将这些模型结合使用、或者通过 ensemble learning 和数据扩展来提高总体精度的想法。<details>
<summary>Abstract</summary>
Automatic medication mining from clinical and biomedical text has become a popular topic due to its real impact on healthcare applications and the recent development of powerful language models (LMs). However, fully-automatic extraction models still face obstacles to be overcome such that they can be deployed directly into clinical practice for better impacts. Such obstacles include their imbalanced performances on different entity types and clinical events. In this work, we examine current state-of-the-art pre-trained language models (PLMs) on such tasks, via fine-tuning including the monolingual model Med7 and multilingual large language model (LLM) XLM-RoBERTa. We compare their advantages and drawbacks using historical medication mining shared task data sets from n2c2-2018 challenges. We report the findings we get from these fine-tuning experiments such that they can facilitate future research on addressing them, for instance, how to combine their outputs, merge such models, or improve their overall accuracy by ensemble learning and data augmentation. MedMine is part of the M3 Initiative \url{https://github.com/HECTA-UoM/M3}
</details>
<details>
<summary>摘要</summary>
自动药物检索从临床和生物医学文本中的检索已经成为一个流行的话题，这主要归功于它在医疗应用中的实际影响以及最近发展的强大语言模型（LM）。然而，完全自动提取模型仍然需要突破一些障碍，以便在临床实践中直接部署。这些障碍包括它们在不同实体类型和临床事件上的不均衡表现。在这项工作中，我们研究了当前状态的最佳预训练语言模型（PLM）在这些任务上，包括单语言模型Med7和多语言大型语言模型（LLM）XLM-RoBERTa。我们比较了它们的优势和缺陷，使用历史药物检索共同任务数据集。我们报告了这些精度调整实验的结果，以便将来研究如何结合它们的输出，合并这些模型，或者提高它们的总精度 mediante ensemble学习和数据扩展。MedMine是M3Initiave的一部分，详情请参考<https://github.com/HECTA-UoM/M3>。
</details></li>
</ul>
<hr>
<h2 id="A-sparse-coding-approach-to-inverse-problems-with-application-to-microwave-tomography-imaging"><a href="#A-sparse-coding-approach-to-inverse-problems-with-application-to-microwave-tomography-imaging" class="headerlink" title="A sparse coding approach to inverse problems with application to microwave tomography imaging"></a>A sparse coding approach to inverse problems with application to microwave tomography imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03818">http://arxiv.org/abs/2308.03818</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cesar F. Caiafa, Ramiro M. Irastorza</li>
<li>for: 这篇论文是关于解决具有各种科学和技术领域的不定性问题的，包括医疗诊断和天文学研究。</li>
<li>methods: 这篇论文使用了稀疏表示法，这是一种基于生物视觉系统的自然图像生成模型，可以有效地解决不定性线性逆问题。</li>
<li>results: 该论文提出了一种基于稀疏 coding的非线性和不定性问题解决方法，可能将导致现有算法的显著改进。<details>
<summary>Abstract</summary>
Inverse imaging problems that are ill-posed can be encountered across multiple domains of science and technology, ranging from medical diagnosis to astronomical studies. To reconstruct images from incomplete and distorted data, it is necessary to create algorithms that can take into account both, the physical mechanisms responsible for generating these measurements and the intrinsic characteristics of the images being analyzed. In this work, the sparse representation of images is reviewed, which is a realistic, compact and effective generative model for natural images inspired by the visual system of mammals. It enables us to address ill-posed linear inverse problems by training the model on a vast collection of images. Moreover, we extend the application of sparse coding to solve the non-linear and ill-posed problem in microwave tomography imaging, which could lead to a significant improvement of the state-of-the-arts algorithms.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "Inverse imaging problems that are ill-posed can be encountered across multiple domains of science and technology, ranging from medical diagnosis to astronomical studies. To reconstruct images from incomplete and distorted data, it is necessary to create algorithms that can take into account both, the physical mechanisms responsible for generating these measurements and the intrinsic characteristics of the images being analyzed. In this work, the sparse representation of images is reviewed, which is a realistic, compact and effective generative model for natural images inspired by the visual system of mammals. It enables us to address ill-posed linear inverse problems by training the model on a vast collection of images. Moreover, we extend the application of sparse coding to solve the non-linear and ill-posed problem in microwave tomography imaging, which could lead to a significant improvement of the state-of-the-arts algorithms." into Simplified Chinese.描述：反射图像问题可以在多个科学和技术领域中遇到，从医学诊断到天文学研究。为了从不完整和扭曲的数据中重建图像，需要创建能够考虑物理机制生成这些测量的数据，以及图像的内在特征。在这项工作中，我们评论了图像简洁表示，它是自然图像的可靠、紧凑和有效的生成模型，受蜥蜴视系统的启发。通过训练模型，可以解决线性不定的反射图像问题。此外，我们还扩展了简洁编码的应用，以解决微波Tomography影像问题，这可能会导致现有算法的显著改进。
</details></li>
</ul>
<hr>
<h2 id="A-Meta-learning-based-Stacked-Regression-Approach-for-Customer-Lifetime-Value-Prediction"><a href="#A-Meta-learning-based-Stacked-Regression-Approach-for-Customer-Lifetime-Value-Prediction" class="headerlink" title="A Meta-learning based Stacked Regression Approach for Customer Lifetime Value Prediction"></a>A Meta-learning based Stacked Regression Approach for Customer Lifetime Value Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08502">http://arxiv.org/abs/2308.08502</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karan Gadgil, Sukhpal Singh Gill, Ahmed M. Abdelmoniem</li>
<li>for: The paper is written to propose a new approach to estimating Customer Lifetime Value (CLV) that is both effective and interpretable, using a combination of bagging and boosting models.</li>
<li>methods: The proposed approach uses a meta-learning-based stacked regression model that combines the predictions from multiple bagging and boosting models to estimate CLV.</li>
<li>results: The paper shows the efficacy of the proposed approach through empirical tests on an openly available Online Retail dataset, demonstrating that it outperforms existing distribution-based and basic models.<details>
<summary>Abstract</summary>
Companies across the globe are keen on targeting potential high-value customers in an attempt to expand revenue and this could be achieved only by understanding the customers more. Customer Lifetime Value (CLV) is the total monetary value of transactions/purchases made by a customer with the business over an intended period of time and is used as means to estimate future customer interactions. CLV finds application in a number of distinct business domains such as Banking, Insurance, Online-entertainment, Gaming, and E-Commerce. The existing distribution-based and basic (recency, frequency & monetary) based models face a limitation in terms of handling a wide variety of input features. Moreover, the more advanced Deep learning approaches could be superfluous and add an undesirable element of complexity in certain application areas. We, therefore, propose a system which is able to qualify both as effective, and comprehensive yet simple and interpretable. With that in mind, we develop a meta-learning-based stacked regression model which combines the predictions from bagging and boosting models that each is found to perform well individually. Empirical tests have been carried out on an openly available Online Retail dataset to evaluate various models and show the efficacy of the proposed approach.
</details>
<details>
<summary>摘要</summary>
世界各地企业都在努力寻找高值客户，以扩大收入。这可以通过更好地了解客户来实现。客户全生命值（CLV）是指客户在企业与其交易的总财务值，在一定时间范围内，并用于估计未来客户互动。CLV在银行、保险、在线娱乐、游戏和电商等多个业务领域得到应用。现有的分布型和基本（频率、额度和时间）型模型具有处理多种输入特征的限制。此外，更先进的深度学习方法可能会添加不必要的复杂性。因此，我们提出一个能够同时具有效果、全面和简单可解释的系统。为了实现这一目标，我们开发了一种基于元学习的堆叠回归模型，该模型结合了袋型和投射型模型，每一个模型都能够单独表现出色。在一个公开的在线零售数据集上进行了实验，以评估不同模型的效果，并证明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Stock-Market-Price-Prediction-A-Hybrid-LSTM-and-Sequential-Self-Attention-based-Approach"><a href="#Stock-Market-Price-Prediction-A-Hybrid-LSTM-and-Sequential-Self-Attention-based-Approach" class="headerlink" title="Stock Market Price Prediction: A Hybrid LSTM and Sequential Self-Attention based Approach"></a>Stock Market Price Prediction: A Hybrid LSTM and Sequential Self-Attention based Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04419">http://arxiv.org/abs/2308.04419</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karan Pardeshi, Sukhpal Singh Gill, Ahmed M. Abdelmoniem</li>
<li>for: 预测股票价格，帮助投资者在最佳时间做出最佳决策。</li>
<li>methods: 使用深度学习策略，具体来说是Long Short-Term Memory（LSTM）与Sequential Self-Attention Mechanism（LSTM-SSAM）建模方法。</li>
<li>results: 对三个股票数据集（SBIN、HDFCBANK、BANKBARODA）进行了广泛的实验，实验结果表明提议的模型比现有模型更有效果和可行性。RMSE和R2评价指标得到了最佳result。<details>
<summary>Abstract</summary>
One of the most enticing research areas is the stock market, and projecting stock prices may help investors profit by making the best decisions at the correct time. Deep learning strategies have emerged as a critical technique in the field of the financial market. The stock market is impacted due to two aspects, one is the geo-political, social and global events on the bases of which the price trends could be affected. Meanwhile, the second aspect purely focuses on historical price trends and seasonality, allowing us to forecast stock prices. In this paper, our aim is to focus on the second aspect and build a model that predicts future prices with minimal errors. In order to provide better prediction results of stock price, we propose a new model named Long Short-Term Memory (LSTM) with Sequential Self-Attention Mechanism (LSTM-SSAM). Finally, we conduct extensive experiments on the three stock datasets: SBIN, HDFCBANK, and BANKBARODA. The experimental results prove the effectiveness and feasibility of the proposed model compared to existing models. The experimental findings demonstrate that the root-mean-squared error (RMSE), and R-square (R2) evaluation indicators are giving the best results.
</details>
<details>
<summary>摘要</summary>
一个非常吸引人的研究领域是股票市场，并且预测股票价格可以帮助投资者获得最佳的决策时间。深度学习策略在财务市场中得到了广泛应用。股票市场受到两个方面的影响：一是地域政治社会和全球事件的影响，这些事件可能影响股票价格走势。另一方面，我们专注于历史价格走势和季节性，以预测股票价格。在这篇论文中，我们的目标是建立一个可以预测股票价格的新模型，并且对现有模型进行比较。为了提供更好的预测结果，我们提议一种名为长期记忆（LSTM）和顺序自注意机制（SSAM）的新模型。最后，我们对三个股票数据集进行了广泛的实验：SBIN、HDFCBANK和BANKBARODA。实验结果证明了我们提出的模型的可行性和有效性，并且比现有模型更好。实验结果表明，使用Root Mean Squared Error（RMSE）和R-square（R2）评价指标，我们的模型在预测股票价格方面得到了最佳的结果。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Semi-Supervised-Segmentation-of-Brain-Vessels-with-Ambiguous-Labels"><a href="#Adaptive-Semi-Supervised-Segmentation-of-Brain-Vessels-with-Ambiguous-Labels" class="headerlink" title="Adaptive Semi-Supervised Segmentation of Brain Vessels with Ambiguous Labels"></a>Adaptive Semi-Supervised Segmentation of Brain Vessels with Ambiguous Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03613">http://arxiv.org/abs/2308.03613</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fengming Lin, Yan Xia, Nishant Ravikumar, Qiongyao Liu, Michael MacRaild, Alejandro F Frangi</li>
<li>for: 脑血管疾病诊断和治疗中，精准分割脑血管的重要性。</li>
<li>methods: 我们提出了一种适应性半supervised方法，包括进步半supervised学习、适应训练策略和边界增强。</li>
<li>results: 对3DRA数据集进行实验，我们的方法在mesh-based分割指标中显示出优于其他方法。通过利用部分和抽象标注数据，我们的方法在杂乱标注数据集上实现了出色的分割性能，展示了临床应用的潜力。<details>
<summary>Abstract</summary>
Accurate segmentation of brain vessels is crucial for cerebrovascular disease diagnosis and treatment. However, existing methods face challenges in capturing small vessels and handling datasets that are partially or ambiguously annotated. In this paper, we propose an adaptive semi-supervised approach to address these challenges. Our approach incorporates innovative techniques including progressive semi-supervised learning, adaptative training strategy, and boundary enhancement. Experimental results on 3DRA datasets demonstrate the superiority of our method in terms of mesh-based segmentation metrics. By leveraging the partially and ambiguously labeled data, which only annotates the main vessels, our method achieves impressive segmentation performance on mislabeled fine vessels, showcasing its potential for clinical applications.
</details>
<details>
<summary>摘要</summary>
精准分割脑血管是脑血管疾病诊断和治疗中的关键。然而，现有方法在捕捉小血管和处理部分或恶劣标注的数据集时遇到困难。在这篇论文中，我们提出了一种适应式半监督方法来解决这些挑战。我们的方法包括进步式半监督学习、适应性训练策略和边界增强等创新技术。在3DRA数据集上进行实验，我们的方法在基于网格的分割指标上表现出色。通过利用部分和恶劣标注的数据，我们的方法在混乱标注的细血管上具有出色的分割性能，这显示了其在临床应用中的潜力。
</details></li>
</ul>
<hr>
<h2 id="A-machine-learning-sleep-wake-classification-model-using-a-reduced-number-of-features-derived-from-photoplethysmography-and-activity-signals"><a href="#A-machine-learning-sleep-wake-classification-model-using-a-reduced-number-of-features-derived-from-photoplethysmography-and-activity-signals" class="headerlink" title="A machine-learning sleep-wake classification model using a reduced number of features derived from photoplethysmography and activity signals"></a>A machine-learning sleep-wake classification model using a reduced number of features derived from photoplethysmography and activity signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05759">http://arxiv.org/abs/2308.05759</a></li>
<li>repo_url: None</li>
<li>paper_authors: Douglas A. Almeida, Felipe M. Dias, Marcelo A. F. Toledo, Diego A. C. Cardenas, Filipe A. C. Oliveira, Estela Ribeiro, Jose E. Krieger, Marco A. Gutierrez</li>
<li>for: 这个研究的目的是开发一种基于 Photoplethysmography (PPG) 信号和活动计数的机器学习睡眠-醒目分类模型，以提高睡眠质量和全身健康。</li>
<li>methods: 该研究使用了 eXtreme Gradient Boosting (XGBoost) 算法和 PPG 信号和活动计数特征进行睡眠 stage INFERENCE。</li>
<li>results: 该研究的结果显示，使用 XGBoost 算法和 PPG 信号和活动计数特征可以达到比现有方法更高的分类性能，具体来说，敏感性为 91.15 $\pm$ 1.16%, 特征选择率为 53.66 $\pm$ 1.12%, F1 分数为 83.88 $\pm$ 0.56%, κ值为 48.0 $\pm$ 0.86%。<details>
<summary>Abstract</summary>
Sleep is a crucial aspect of our overall health and well-being. It plays a vital role in regulating our mental and physical health, impacting our mood, memory, and cognitive function to our physical resilience and immune system. The classification of sleep stages is a mandatory step to assess sleep quality, providing the metrics to estimate the quality of sleep and how well our body is functioning during this essential period of rest. Photoplethysmography (PPG) has been demonstrated to be an effective signal for sleep stage inference, meaning it can be used on its own or in a combination with others signals to determine sleep stage. This information is valuable in identifying potential sleep issues and developing strategies to improve sleep quality and overall health. In this work, we present a machine learning sleep-wake classification model based on the eXtreme Gradient Boosting (XGBoost) algorithm and features extracted from PPG signal and activity counts. The performance of our method was comparable to current state-of-the-art methods with a Sensitivity of 91.15 $\pm$ 1.16%, Specificity of 53.66 $\pm$ 1.12%, F1-score of 83.88 $\pm$ 0.56%, and Kappa of 48.0 $\pm$ 0.86%. Our method offers a significant improvement over other approaches as it uses a reduced number of features, making it suitable for implementation in wearable devices that have limited computational power.
</details>
<details>
<summary>摘要</summary>
睡眠是我们健康和养成的重要方面。它对我们的情绪、身体和肌肉健康产生重要影响，对我们的情绪、记忆和认知功能也产生很大的影响。睡眠阶段的分类是评估睡眠质量的必要步骤，可以提供评估睡眠质量的度量，以及身体在这段睡眠时的功能状况。光学抵抗检测（PPG）已经被证明可以用于睡眠阶段推断，这意味着它可以单独使用或与其他信号组合使用来确定睡眠阶段。这些信息非常重要，可以帮助发现可能存在的睡眠问题，并开发改善睡眠质量和总体健康的策略。在这项工作中，我们提出了基于XTreme Gradient Boosting（XGBoost）算法和PPG信号和活动计数的机器学习睡眠-醒目分类模型。我们的方法的性能与当前状态的方法相当，具有感知率91.15 $\pm$ 1.16%、特异性53.66 $\pm$ 1.12%、F1分数83.88 $\pm$ 0.56%和卡ปα48.0 $\pm$ 0.86%。我们的方法比其他方法更加有优势，因为它使用了减少的特征数，适合在有限计算能力的穿戴式设备中实现。
</details></li>
</ul>
<hr>
<h2 id="Generalized-Early-Stopping-in-Evolutionary-Direct-Policy-Search"><a href="#Generalized-Early-Stopping-in-Evolutionary-Direct-Policy-Search" class="headerlink" title="Generalized Early Stopping in Evolutionary Direct Policy Search"></a>Generalized Early Stopping in Evolutionary Direct Policy Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03574">http://arxiv.org/abs/2308.03574</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anonreposit/gesp">https://github.com/anonreposit/gesp</a></li>
<li>paper_authors: Etor Arza, Leni K. Le Goff, Emma Hart</li>
<li>for: 提高direct policy search任务的计算效率，尤其是在物理世界中进行评估时。</li>
<li>methods: 基于对象值的简单停止 criterion，不需要具体问题知识。</li>
<li>results: 在五个来自游戏、 роботи库和 класси控制领域的直接策略搜索环境中，可以Save up to 75% of computation time。 comparison with problem-specific stopping criteria shows that it performs comparably while being more generally applicable.<details>
<summary>Abstract</summary>
Lengthy evaluation times are common in many optimization problems such as direct policy search tasks, especially when they involve conducting evaluations in the physical world, e.g. in robotics applications. Often, when evaluating a solution over a fixed time period, it becomes clear that the objective value will not increase with additional computation time (for example, when a two-wheeled robot continuously spins on the spot). In such cases, it makes sense to stop the evaluation early to save computation time. However, most approaches to stop the evaluation are problem-specific and need to be specifically designed for the task at hand. Therefore, we propose an early stopping method for direct policy search. The proposed method only looks at the objective value at each time step and requires no problem-specific knowledge.   We test the introduced stopping criterion in five direct policy search environments drawn from games, robotics, and classic control domains, and show that it can save up to 75% of the computation time. We also compare it with problem-specific stopping criteria and demonstrate that it performs comparably while being more generally applicable.
</details>
<details>
<summary>摘要</summary>
长时间的评估时间是许多优化问题中的常见问题，如直接策略搜索任务，特别是在物理世界中进行评估，例如在二轮机器人应用中。经常情况下，当评估一解决方案在固定时间段内时，就会发现目标值不会随着计算时间增加（例如，当二轮机器人不断旋转在同一点上）。在这些情况下，可以提前结束评估以降低计算时间。然而，大多数评估结束方法是专门为特定任务设计的，需要具备专门的问题知识。因此，我们提出了一种直接策略搜索中的 early stopping 方法。该方法只需要在每个时间步骤中评估目标值，无需特定任务知识。我们在五个直接策略搜索环境中测试了引入的停止标准，这些环境来自游戏、机器人和 класси控制领域。我们发现，该方法可以将计算时间减少到75%。我们还与专门的停止标准进行比较，并证明它与专门的停止标准相比，性能相似，但更一般适用。
</details></li>
</ul>
<hr>
<h2 id="When-Federated-Learning-meets-Watermarking-A-Comprehensive-Overview-of-Techniques-for-Intellectual-Property-Protection"><a href="#When-Federated-Learning-meets-Watermarking-A-Comprehensive-Overview-of-Techniques-for-Intellectual-Property-Protection" class="headerlink" title="When Federated Learning meets Watermarking: A Comprehensive Overview of Techniques for Intellectual Property Protection"></a>When Federated Learning meets Watermarking: A Comprehensive Overview of Techniques for Intellectual Property Protection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03573">http://arxiv.org/abs/2308.03573</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammed Lansari, Reda Bellafqira, Katarzyna Kapusta, Vincent Thouvenot, Olivier Bettan, Gouenou Coatrieux</li>
<li>for: 本文提供了最新的 Federated Learning（FL） watermarking 技术的概述，包括新的挑战和机遇在FL中。</li>
<li>methods: 本文详细介绍了过去五年内关于DNN watermarking的研究，以及在FL中应用这些技术的新途径。</li>
<li>results: 本文总结了FL watermarking 技术的最新进展，并释明了在FL中保护模型所有权的新挑战和机遇。<details>
<summary>Abstract</summary>
Federated Learning (FL) is a technique that allows multiple participants to collaboratively train a Deep Neural Network (DNN) without the need of centralizing their data. Among other advantages, it comes with privacy-preserving properties making it attractive for application in sensitive contexts, such as health care or the military. Although the data are not explicitly exchanged, the training procedure requires sharing information about participants' models. This makes the individual models vulnerable to theft or unauthorized distribution by malicious actors. To address the issue of ownership rights protection in the context of Machine Learning (ML), DNN Watermarking methods have been developed during the last five years. Most existing works have focused on watermarking in a centralized manner, but only a few methods have been designed for FL and its unique constraints. In this paper, we provide an overview of recent advancements in Federated Learning watermarking, shedding light on the new challenges and opportunities that arise in this field.
</details>
<details>
<summary>摘要</summary>
受领域限制的 Federated Learning（FL）是一种技术，允许多个参与者共同训练深度神经网络（DNN），不需要中央化数据。FL possess several advantages, including privacy-preserving properties, making it suitable for applications in sensitive contexts, such as healthcare or the military. However, the training process requires sharing information about participants' models, which makes the individual models vulnerable to theft or unauthorized distribution by malicious actors. To address the issue of ownership rights protection in the context of Machine Learning（ML）, DNN watermarking methods have been developed over the past five years. Most existing works have focused on watermarking in a centralized manner, but only a few methods have been designed for FL and its unique constraints. In this paper, we provide an overview of recent advancements in Federated Learning watermarking, highlighting the new challenges and opportunities that arise in this field.
</details></li>
</ul>
<hr>
<h2 id="Provably-Efficient-Learning-in-Partially-Observable-Contextual-Bandit"><a href="#Provably-Efficient-Learning-in-Partially-Observable-Contextual-Bandit" class="headerlink" title="Provably Efficient Learning in Partially Observable Contextual Bandit"></a>Provably Efficient Learning in Partially Observable Contextual Bandit</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03572">http://arxiv.org/abs/2308.03572</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xueping Gong, Jiheng Zhang</li>
<li>for: 这项研究探讨了在半可见情况下的Contextual Bandit问题，agent有限制知识和部分隐藏变量的情况下，通过优化问题来描述或部分描述 causal effect between actions and rewards。</li>
<li>methods: 我们将原始函数约束转化为 linear 约束，通过顺序解 linear programming 来样 compatible causal models，并通过这些模型来获得 causal bounds，并考虑到估计误差。我们的抽取算法提供了可靠的抽取结果。</li>
<li>results: 我们证明了我们的 causally enhanced 算法在 action set 和函数空间大小的情况下比 classical bandit 算法更高效，并且在可以处理通用 context distribution 的情况下，我们的方法可以在函数approximation 任务中提高 regret 的速度。我们的结果还表明，我们的方法可以在实际应用中提高 contextual bandit 代理的性能。<details>
<summary>Abstract</summary>
In this paper, we investigate transfer learning in partially observable contextual bandits, where agents have limited knowledge from other agents and partial information about hidden confounders. We first convert the problem to identifying or partially identifying causal effects between actions and rewards through optimization problems. To solve these optimization problems, we discretize the original functional constraints of unknown distributions into linear constraints, and sample compatible causal models via sequentially solving linear programmings to obtain causal bounds with the consideration of estimation error. Our sampling algorithms provide desirable convergence results for suitable sampling distributions. We then show how causal bounds can be applied to improving classical bandit algorithms and affect the regrets with respect to the size of action sets and function spaces. Notably, in the task with function approximation which allows us to handle general context distributions, our method improves the order dependence on function space size compared with previous literatures. We formally prove that our causally enhanced algorithms outperform classical bandit algorithms and achieve orders of magnitude faster convergence rates. Finally, we perform simulations that demonstrate the efficiency of our strategy compared to the current state-of-the-art methods. This research has the potential to enhance the performance of contextual bandit agents in real-world applications where data is scarce and costly to obtain.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了在部分可见 контекстual bandit 中的传输学习， agents 具有其他 agents 的有限知识和隐藏的干扰因素的有限信息。我们首先将问题转化为标识或部分标识 causal 效果 между动作和奖励的优化问题。为解这些优化问题，我们将原始的函数约束Unknown  Distribution 转化为线性约束，并通过顺序解线性程序来采样兼容 causal 模型，以获得 causal 上下文的约束。我们的抽取算法提供了可靠的收敛结果。然后，我们展示了如何使用 causal 上下文来改进 classical bandit 算法，并对奖励的大小和功能空间的大小产生影响。尤其在功能适应任务中，我们的方法可以处理一般的 context 分布，我们的方法可以在功能空间大小的下降中提高奖励的顺序依赖度。我们正式证明我们的 causally 强化算法在较前文献的算法之上出perform  better，并 achiev 许多更快的收敛率。最后，我们在实验中证明了我们的策略在实际应用中比现有的方法更高效。这种研究具有提高实际应用中 contextual bandit 代理的性能的潜在可能性。
</details></li>
</ul>
<hr>
<h2 id="Partial-identification-of-kernel-based-two-sample-tests-with-mismeasured-data"><a href="#Partial-identification-of-kernel-based-two-sample-tests-with-mismeasured-data" class="headerlink" title="Partial identification of kernel based two sample tests with mismeasured data"></a>Partial identification of kernel based two sample tests with mismeasured data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03570">http://arxiv.org/abs/2308.03570</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ron Nafshi, Maggie Makar</li>
<li>for: 这个论文是关于在机器学习应用中使用非参数两个样本测试（最大均值差）时，如何处理含有错误样本的情况。</li>
<li>methods: 该论文使用了对$\epsilon$-污染情况下的MMD的估计，并研究了MMD的部分鉴定。</li>
<li>results: 该论文提出了一种方法来估计MMD的上下限，并证明这些上下限是true, unknown MMD的精确 bounds。通过三个实验 dataset，论文证明了这种方法的优越性，即它可以提供紧Binding的上下限，并且false coverage rate较低。<details>
<summary>Abstract</summary>
Nonparametric two-sample tests such as the Maximum Mean Discrepancy (MMD) are often used to detect differences between two distributions in machine learning applications. However, the majority of existing literature assumes that error-free samples from the two distributions of interest are available.We relax this assumption and study the estimation of the MMD under $\epsilon$-contamination, where a possibly non-random $\epsilon$ proportion of one distribution is erroneously grouped with the other. We show that under $\epsilon$-contamination, the typical estimate of the MMD is unreliable. Instead, we study partial identification of the MMD, and characterize sharp upper and lower bounds that contain the true, unknown MMD. We propose a method to estimate these bounds, and show that it gives estimates that converge to the sharpest possible bounds on the MMD as sample size increases, with a convergence rate that is faster than alternative approaches. Using three datasets, we empirically validate that our approach is superior to the alternatives: it gives tight bounds with a low false coverage rate.
</details>
<details>
<summary>摘要</summary>
非参数两个样本测试，如最大均值差（MMD），在机器学习应用中广泛使用以检测两个分布之间的差异。然而，现有的大部分文献假设有误无误的样本来自两个分布。我们松弛这个假设，研究在$\epsilon$-杂化下MMD的估计，其中可能存在非随机的$\epsilon$比例一个分布被误归类为另一个分布。我们表明在$\epsilon$-杂化下，通常的估计MMD是不可靠的。而我们研究了MMD的部分鉴定，并Characterize了包含真实未知MMD的尖锐上下限。我们提议了一种估计这些上下限的方法，并证明它的估计将与样本大小增加而 convergence to the sharpest possible bounds on MMD, with a convergence rate that is faster than alternative approaches。使用三个数据集，我们实验 validate that our approach is superior to the alternatives: it gives tight bounds with a low false coverage rate.
</details></li>
</ul>
<hr>
<h2 id="A-Transfer-Learning-Framework-for-Proactive-Ramp-Metering-Performance-Assessment"><a href="#A-Transfer-Learning-Framework-for-Proactive-Ramp-Metering-Performance-Assessment" class="headerlink" title="A Transfer Learning Framework for Proactive Ramp Metering Performance Assessment"></a>A Transfer Learning Framework for Proactive Ramp Metering Performance Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03542">http://arxiv.org/abs/2308.03542</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaobo Ma, Adrian Cottam, Mohammad Razaur Rahman Shaon, Yao-Jan Wu</li>
<li>for: 评估升道计划的性能时需要评估升道计划对高速公路交通流动的影响。</li>
<li>methods: 本研究提出了一种基于传输学习模型的方法，通过学习before和after情况下的车流特征，对新的高速公路段预测车流参数。</li>
<li>results: 实验结果表明，提posed方法可以作为评估升道计划性能的 alternatives。<details>
<summary>Abstract</summary>
Transportation agencies need to assess ramp metering performance when deploying or expanding a ramp metering system. The evaluation of a ramp metering strategy is primarily centered around examining its impact on freeway traffic mobility. One way these effects can be explored is by comparing traffic states, such as the speed before and after the ramp metering strategy has been altered. Predicting freeway traffic states for the after scenarios following the implementation of a new ramp metering control strategy could offer valuable insights into the potential effectiveness of the target strategy. However, the use of machine learning methods in predicting the freeway traffic state for the after scenarios and evaluating the effectiveness of transportation policies or traffic control strategies such as ramp metering is somewhat limited in the current literature. To bridge the research gap, this study presents a framework for predicting freeway traffic parameters (speed, occupancy, and flow rate) for the after situations when a new ramp metering control strategy is implemented. By learning the association between the spatial-temporal features of traffic states in before and after situations for known freeway segments, the proposed framework can transfer this learning to predict the traffic parameters for new freeway segments. The proposed framework is built upon a transfer learning model. Experimental results show that the proposed framework is feasible for use as an alternative for predicting freeway traffic parameters to proactively evaluate ramp metering performance.
</details>
<details>
<summary>摘要</summary>
（简化中文）交通机构需要评估扩展或部署匝道控制系统时，需要评估匝道控制策略的表现。匝道控制策略的效果主要是通过评估它对高速公路交通流动性的影响来评估。比较交通状态之前和之后匝道控制策略改变后的情况可以提供有价值的信息。但是，使用机器学习方法来预测高速公路交通状态的后果和评估交通政策或匝道控制策略的效果在当前文献中受限。为了填补这个研究漏洞，本研究提出了一种预测高速公路交通参数（速度、占用率和流速）的后果框架。通过学习知道的匝道段的前后交通状态之间的空间时间特征，该框架可以将这种学习转移到新的匝道段上预测交通参数。该框架基于转移学习模型。实验结果表明，该框架可以作为评估匝道控制性能的代替方法来预测高速公路交通参数。
</details></li>
</ul>
<hr>
<h2 id="On-ramp-and-Off-ramp-Traffic-Flows-Estimation-Based-on-A-Data-driven-Transfer-Learning-Framework"><a href="#On-ramp-and-Off-ramp-Traffic-Flows-Estimation-Based-on-A-Data-driven-Transfer-Learning-Framework" class="headerlink" title="On-ramp and Off-ramp Traffic Flows Estimation Based on A Data-driven Transfer Learning Framework"></a>On-ramp and Off-ramp Traffic Flows Estimation Based on A Data-driven Transfer Learning Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03538">http://arxiv.org/abs/2308.03538</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaobo Ma, Abolfazl Karimpour, Yao-Jan Wu<br>for: 用于提高路径负荷管理策略的执行和监测，以及评估高速公路交叉口的交通性能。methods: 使用数据驱动的框架，通过启用转移学习模型，可以准确地估计缺失的融合流量。results: 实验结果表明，提案的方法可以在不同的交通模式、分布和特点下提供高精度的融合流量估计，其误差值在23.90 veh&#x2F;h到40.85 veh&#x2F;h之间， root mean square error值在34.55 veh&#x2F;h到57.77 veh&#x2F;h之间。此外，比较分析表明，提案的方法在其他传统机器学习模型之上表现出优异的性能。<details>
<summary>Abstract</summary>
To develop the most appropriate control strategy and monitor, maintain, and evaluate the traffic performance of the freeway weaving areas, state and local Departments of Transportation need to have access to traffic flows at each pair of on-ramp and off-ramp. However, ramp flows are not always readily available to transportation agencies and little effort has been made to estimate these missing flows in locations where no physical sensors are installed. To bridge this research gap, a data-driven framework is proposed that can accurately estimate the missing ramp flows by solely using data collected from loop detectors on freeway mainlines. The proposed framework employs a transfer learning model. The transfer learning model relaxes the assumption that the underlying data distributions of the source and target domains must be the same. Therefore, the proposed framework can guarantee high-accuracy estimation of on-ramp and off-ramp flows on freeways with different traffic patterns, distributions, and characteristics. Based on the experimental results, the flow estimation mean absolute errors range between 23.90 veh/h to 40.85 veh/h for on-ramps, and 31.58 veh/h to 45.31 veh/h for off-ramps; the flow estimation root mean square errors range between 34.55 veh/h to 57.77 veh/h for on-ramps, and 41.75 veh/h to 58.80 veh/h for off-ramps. Further, the comparison analysis shows that the proposed framework outperforms other conventional machine learning models. The estimated ramp flows based on the proposed method can help transportation agencies to enhance the operations of their ramp control strategies for locations where physical sensors are not installed.
</details>
<details>
<summary>摘要</summary>
为了开发最佳的控制策略和监测、维护和评估高速公路叠加区域的交通表现，国家和地方交通部门需要了解每对侧线和脱离的交通流量。然而，叠加区域的流量并不总是可以向交通机构提供，而且过去很少有人尝试了估计这些缺失的流量。为了填补这一研究漏洞，我们提出了一个数据驱动的框架，可以准确地估计缺失的叠加流量，只使用高速公路主线上的循环检测器收集的数据。我们的框架采用了传输学习模型，该模型放宽了下面数据集的假设，因此我们的框架可以 garantizar高精度地估计叠加流量，无论高速公路的交通模式、分布和特征如何。根据实验结果，估计的叠加流量 Mean Absolute Error 在23.90 veh/h 到 40.85 veh/h之间，Root Mean Square Error 在34.55 veh/h 到 57.77 veh/h之间。此外，比较分析表明，我们的方法在其他传统的机器学习模型之上表现出色。根据我们的估计结果，可以帮助交通机构在没有物理感知器的情况下提高叠加控制策略的运行。
</details></li>
</ul>
<hr>
<h2 id="Deep-Feature-Learning-for-Wireless-Spectrum-Data"><a href="#Deep-Feature-Learning-for-Wireless-Spectrum-Data" class="headerlink" title="Deep Feature Learning for Wireless Spectrum Data"></a>Deep Feature Learning for Wireless Spectrum Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03530">http://arxiv.org/abs/2308.03530</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ljupcho Milosheski, Gregor Cerar, Blaž Bertalanič, Carolina Fortuna, Mihael Mohorčič</li>
<li>for: 本研究旨在实现无监督的无线传输几何分 clustering。</li>
<li>methods: 我们提出了基于卷积神经网络的自动特征表示学习方法，并证明了这种方法可以将输入数据降维至99.3%比baseline PCA少。</li>
<li>results: 我们显示了自动特征表示学习可以提取细化的传输几何shape，而baseline仅能基于背景噪音进行大致分类。<details>
<summary>Abstract</summary>
In recent years, the traditional feature engineering process for training machine learning models is being automated by the feature extraction layers integrated in deep learning architectures. In wireless networks, many studies were conducted in automatic learning of feature representations for domain-related challenges. However, most of the existing works assume some supervision along the learning process by using labels to optimize the model. In this paper, we investigate an approach to learning feature representations for wireless transmission clustering in a completely unsupervised manner, i.e. requiring no labels in the process. We propose a model based on convolutional neural networks that automatically learns a reduced dimensionality representation of the input data with 99.3% less components compared to a baseline principal component analysis (PCA). We show that the automatic representation learning is able to extract fine-grained clusters containing the shapes of the wireless transmission bursts, while the baseline enables only general separability of the data based on the background noise.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="AlphaStar-Unplugged-Large-Scale-Offline-Reinforcement-Learning"><a href="#AlphaStar-Unplugged-Large-Scale-Offline-Reinforcement-Learning" class="headerlink" title="AlphaStar Unplugged: Large-Scale Offline Reinforcement Learning"></a>AlphaStar Unplugged: Large-Scale Offline Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03526">http://arxiv.org/abs/2308.03526</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michaël Mathieu, Sherjil Ozair, Srivatsan Srinivasan, Caglar Gulcehre, Shangtong Zhang, Ray Jiang, Tom Le Paine, Richard Powell, Konrad Żołna, Julian Schrittwieser, David Choi, Petko Georgiev, Daniel Toyama, Aja Huang, Roman Ring, Igor Babuschkin, Timo Ewalds, Mahyar Bordbar, Sarah Henderson, Sergio Gómez Colmenarejo, Aäron van den Oord, Wojciech Marian Czarnecki, Nando de Freitas, Oriol Vinyals</li>
<li>For: The paper is written to advance offline reinforcement learning algorithms in the challenging environment of StarCraft II.* Methods: The paper introduces a new benchmark called AlphaStar Unplugged, which includes a dataset, standardized API, and evaluation protocol. The authors also present baseline agents, including behavior cloning and offline variants of actor-critic and MuZero.* Results: The authors achieve a 90% win rate against a previously published AlphaStar behavior cloning agent using only offline data, improving the state of the art of agents in this domain.Here is the same information in Simplified Chinese text:</li>
<li>for: 本文是为了提高星际II上的离线学习算法。</li>
<li>methods: 本文引入了一个新的标准 benchmark，即AlphaStar Unplugged，包括一个数据集、标准 API 和评估协议。作者还提供了一些基线代理，包括行为做影响和离线版actor-critic和MuZero。</li>
<li>results: 作者使用仅离线数据达到了90% 的胜率，超过了之前发表的AlphaStar行为做影响代理。<details>
<summary>Abstract</summary>
StarCraft II is one of the most challenging simulated reinforcement learning environments; it is partially observable, stochastic, multi-agent, and mastering StarCraft II requires strategic planning over long time horizons with real-time low-level execution. It also has an active professional competitive scene. StarCraft II is uniquely suited for advancing offline RL algorithms, both because of its challenging nature and because Blizzard has released a massive dataset of millions of StarCraft II games played by human players. This paper leverages that and establishes a benchmark, called AlphaStar Unplugged, introducing unprecedented challenges for offline reinforcement learning. We define a dataset (a subset of Blizzard's release), tools standardizing an API for machine learning methods, and an evaluation protocol. We also present baseline agents, including behavior cloning, offline variants of actor-critic and MuZero. We improve the state of the art of agents using only offline data, and we achieve 90% win rate against previously published AlphaStar behavior cloning agent.
</details>
<details>
<summary>摘要</summary>
星际II是一个非常挑战性的规则学习环境，它是部分可见、随机、多智能的，掌握星际II需要长期战略规划，同时在实时低级别执行。它还有活跃的职业竞赛场景。由于星际II的挑战性和Blizzard公司发布了数百万场星际II游戏记录，这使得星际II成为了提前RL算法的进步的 идеaldestination。本文利用了这些数据和API，并实现了一个基准测试（AlphaStar Unplugged），它为Offline RL算法带来了前所未有的挑战。我们定义了一个子集（Blizzard发布的数据），工具和标准API для机器学习方法，以及评价协议。我们还提供了基eline agents，包括行为做法快照、Offline变体的actor-critic和MuZero。我们使用仅Offline数据提高了代理机器的状态，并达到了以前发布的AlphaStar行为快照机器的90%赢率。
</details></li>
</ul>
<hr>
<h2 id="Worker-Activity-Recognition-in-Manufacturing-Line-Using-Near-body-Electric-Field"><a href="#Worker-Activity-Recognition-in-Manufacturing-Line-Using-Near-body-Electric-Field" class="headerlink" title="Worker Activity Recognition in Manufacturing Line Using Near-body Electric Field"></a>Worker Activity Recognition in Manufacturing Line Using Near-body Electric Field</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03514">http://arxiv.org/abs/2308.03514</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sungho Suh, Vitor Fortes Rey, Sizhen Bian, Yu-Chi Huang, Jože M. Rožanec, Hooman Tavakoli Ghinani, Bo Zhou, Paul Lukowicz</li>
<li>for: 本研究旨在提高制造业的生产效率和产品质量，通过部署先进的感知和控制系统。</li>
<li>methods: 本研究使用了融合IMU和体容感测模块的新型便携式感知原型，并对多渠道时间序列 convolutional neural networks 和深度径向LSTM进行早期和晚期整合处理。</li>
<li>results: 实验结果表明，我们提出的方法可以与基线方法相比，在实际应用中表现出较高的性能。此外，具有体容感器和特征融合方法的感知原型，对比没有体容感器和Apple Watch数据的情况，提高了6.35%，macro F1分数提高了9.38%。<details>
<summary>Abstract</summary>
Manufacturing industries strive to improve production efficiency and product quality by deploying advanced sensing and control systems. Wearable sensors are emerging as a promising solution for achieving this goal, as they can provide continuous and unobtrusive monitoring of workers' activities in the manufacturing line. This paper presents a novel wearable sensing prototype that combines IMU and body capacitance sensing modules to recognize worker activities in the manufacturing line. To handle these multimodal sensor data, we propose and compare early, and late sensor data fusion approaches for multi-channel time-series convolutional neural networks and deep convolutional LSTM. We evaluate the proposed hardware and neural network model by collecting and annotating sensor data using the proposed sensing prototype and Apple Watches in the testbed of the manufacturing line. Experimental results demonstrate that our proposed methods achieve superior performance compared to the baseline methods, indicating the potential of the proposed approach for real-world applications in manufacturing industries. Furthermore, the proposed sensing prototype with a body capacitive sensor and feature fusion method improves by 6.35%, yielding a 9.38% higher macro F1 score than the proposed sensing prototype without a body capacitive sensor and Apple Watch data, respectively.
</details>
<details>
<summary>摘要</summary>
制造业为提高生产效率和产品质量而努力，通常会使用先进的感测和控制系统。穿戴式感测器正在成为制造业中实现这一目标的有望解决方案，因为它们可以提供不间断和不干扰的工作者活动监测。本文介绍了一种新的穿戴式感测原型，该原型结合IMU和身体电容感测模块，以认识制造线上工作者的活动。为处理这些多模式感测数据，我们提出并比较了早期和晚期感测数据融合方法，用于多渠道时间序列卷积神经网络和深度卷积LSTM。我们通过使用提议的硬件和神经网络模型，对收集和标注感测数据的Apple Watch和测试准系中的感测数据进行评估。实验结果表明，我们的提议方法在比基准方法的情况下表现出色，这表明了我们的方法在实际应用中的潜在可能性。此外，结合身体电容感测器和特征融合方法的提议感测器提高了6.35%，对比没有身体电容感测器和Apple Watch数据的情况下，提议感测器的macro F1分数提高了9.38%。
</details></li>
</ul>
<hr>
<h2 id="A-data-driven-approach-to-predict-decision-point-choice-during-normal-and-evacuation-wayfinding-in-multi-story-buildings"><a href="#A-data-driven-approach-to-predict-decision-point-choice-during-normal-and-evacuation-wayfinding-in-multi-story-buildings" class="headerlink" title="A data-driven approach to predict decision point choice during normal and evacuation wayfinding in multi-story buildings"></a>A data-driven approach to predict decision point choice during normal and evacuation wayfinding in multi-story buildings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03511">http://arxiv.org/abs/2308.03511</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yan Feng, Panchamy Krishnakumari<br>for: 这项研究旨在理解和预测在多层建筑物内 pedestrian 的决策点选择行为，以确保 pedestrian 的安全。methods: 该研究使用了数据驱动的方法，首先构建了indoor网络表示，然后使用了一种已知的机器学习算法，即随机森林（RF）模型来预测 pedestrian 在路线上的决策点选择。results: 研究发现，使用 RF 模型可以高度准确预测 pedestrian 的决策点选择，其中最高的预测精度达到 96%。此外，研究还发现，个人特征不会影响决策点选择。这项研究表明了应用机器学习算法来研究 pedestrian 路线选择行为在复杂的indoor建筑物中的潜力。<details>
<summary>Abstract</summary>
Understanding pedestrian route choice behavior in complex buildings is important to ensure pedestrian safety. Previous studies have mostly used traditional data collection methods and discrete choice modeling to understand the influence of different factors on pedestrian route and exit choice, particularly in simple indoor environments. However, research on pedestrian route choice in complex buildings is still limited. This paper presents a data-driven approach for understanding and predicting the pedestrian decision point choice during normal and emergency wayfinding in a multi-story building. For this, we first built an indoor network representation and proposed a data mapping technique to map VR coordinates to the indoor representation. We then used a well-established machine learning algorithm, namely the random forest (RF) model to predict pedestrian decision point choice along a route during four wayfinding tasks in a multi-story building. Pedestrian behavioral data in a multi-story building was collected by a Virtual Reality experiment. The results show a much higher prediction accuracy of decision points using the RF model (i.e., 93% on average) compared to the logistic regression model. The highest prediction accuracy was 96% for task 3. Additionally, we tested the model performance combining personal characteristics and we found that personal characteristics did not affect decision point choice. This paper demonstrates the potential of applying a machine learning algorithm to study pedestrian route choice behavior in complex indoor buildings.
</details>
<details>
<summary>摘要</summary>
理解步行者路径选择行为在复杂的建筑物中是重要的，以确保步行者的安全。先前的研究通常使用传统的数据采集方法和精确选择模型来理解不同因素对步行者路径和出口选择的影响，特别是在简单的室内环境中。然而，关于步行者路径选择在复杂的建筑物中的研究仍然有限。本文提出了一种数据驱动的方法，用于理解和预测步行者决策点选择在正常和紧急导航中的多层建筑物中。为此，我们首先建立了一个室内网络表示，并提出了一种数据映射技术来将VR坐标映射到室内表示中。然后，我们使用一种已有的机器学习算法，即随机森林（RF）模型来预测步行者决策点选择的路径中的决策点。在一个多层建筑物中的步行者行为数据被通过虚拟现实实验收集。结果显示，使用RF模型（即93%的平均预测精度）的预测精度远高于逻辑回归模型。最高的预测精度是96%的任务3。此外，我们测试了模型性能，结果表明个人特征不会影响决策点选择。本文示范了应用机器学习算法研究步行者路径选择行为在复杂室内建筑物中的可能性。
</details></li>
</ul>
<hr>
<h2 id="Balanced-Face-Dataset-Guiding-StyleGAN-to-Generate-Labeled-Synthetic-Face-Image-Dataset-for-Underrepresented-Group"><a href="#Balanced-Face-Dataset-Guiding-StyleGAN-to-Generate-Labeled-Synthetic-Face-Image-Dataset-for-Underrepresented-Group" class="headerlink" title="Balanced Face Dataset: Guiding StyleGAN to Generate Labeled Synthetic Face Image Dataset for Underrepresented Group"></a>Balanced Face Dataset: Guiding StyleGAN to Generate Labeled Synthetic Face Image Dataset for Underrepresented Group</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03495">http://arxiv.org/abs/2308.03495</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kidist Amde Mekonnen<br>for:This paper aims to generate a robust face image dataset that is balanced among different demographic groups, using the StyleGAN model.methods:The paper uses the StyleGAN model to generate synthetic face images, and controls the generation process to achieve a balanced distribution of the dataset among different demographic groups.results:The paper achieves a balanced distribution of the dataset among different demographic groups, and demonstrates the effectiveness of using synthetic data generation and active labeling to reduce bias in machine learning.Here’s the Chinese translation of the three points:for:这篇论文的目标是使用StyleGAN模型生成一个可靠的人脸图像数据集，该数据集在不同的民族群体中具有平衡分布。methods:论文使用StyleGAN模型生成人脸synthetic图像，并控制生成过程以实现数据集中不同民族群体的平衡分布。results:论文实现了数据集中不同民族群体的平衡分布，并证明了通过生成人工数据和活动标注来减少机器学习中的偏见。<details>
<summary>Abstract</summary>
For a machine learning model to generalize effectively to unseen data within a particular problem domain, it is well-understood that the data needs to be of sufficient size and representative of real-world scenarios. Nonetheless, real-world datasets frequently have overrepresented and underrepresented groups. One solution to mitigate bias in machine learning is to leverage a diverse and representative dataset. Training a model on a dataset that covers all demographics is crucial to reducing bias in machine learning. However, collecting and labeling large-scale datasets has been challenging, prompting the use of synthetic data generation and active labeling to decrease the costs of manual labeling. The focus of this study was to generate a robust face image dataset using the StyleGAN model. In order to achieve a balanced distribution of the dataset among different demographic groups, a synthetic dataset was created by controlling the generation process of StyleGaN and annotated for different downstream tasks.
</details>
<details>
<summary>摘要</summary>
为了让机器学习模型在特定问题领域 generaleffectively，需要确保数据够大且符合实际情况。然而，实际世界数据集经常具有过度和不足的分布。一种解决偏见问题的方法是利用多样化和代表性的数据集。训练机器学习模型需要覆盖所有民族，这有助于减少偏见。然而，收集和标注大规模数据集的成本高昂，因此常用生成数据和活动标注来减少手动标注成本。这个研究的目标是通过StyleGAN模型生成一个可靠的人脸图像集。为了实现数据集的均衡分布，我们控制了StyleGAN生成过程，并对不同下游任务进行了标注。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Physical-World-Adversarial-Robustness-of-Vehicle-Detection"><a href="#Exploring-the-Physical-World-Adversarial-Robustness-of-Vehicle-Detection" class="headerlink" title="Exploring the Physical World Adversarial Robustness of Vehicle Detection"></a>Exploring the Physical World Adversarial Robustness of Vehicle Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03476">http://arxiv.org/abs/2308.03476</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Jiang, Tianyuan Zhang, Shuangcheng Liu, Weiyu Ji, Zichao Zhang, Gang Xiao</li>
<li>For: The paper is written to highlight the significance of adversarial attacks in real-world contexts and to introduce a new dataset (DCI) for evaluating the robustness of detection models under these attacks.* Methods: The paper uses an innovative instant-level data generation pipeline using the CARLA simulator to create the DCI dataset, which enables comprehensive experiments involving three detection models and three physical adversarial attacks.* Results: The paper finds that Yolo v6 demonstrates remarkable resilience to adversarial attacks, while the ASA attack yields a substantial average AP reduction of 14.51%. The study also notes that static scenes yield higher recognition AP values and that outcomes remain relatively consistent across varying weather conditions. Additionally, the study suggests that advancements in adversarial attack algorithms may be approaching their “limitation”.<details>
<summary>Abstract</summary>
Adversarial attacks can compromise the robustness of real-world detection models. However, evaluating these models under real-world conditions poses challenges due to resource-intensive experiments. Virtual simulations offer an alternative, but the absence of standardized benchmarks hampers progress. Addressing this, we propose an innovative instant-level data generation pipeline using the CARLA simulator. Through this pipeline, we establish the Discrete and Continuous Instant-level (DCI) dataset, enabling comprehensive experiments involving three detection models and three physical adversarial attacks. Our findings highlight diverse model performances under adversarial conditions. Yolo v6 demonstrates remarkable resilience, experiencing just a marginal 6.59% average drop in average precision (AP). In contrast, the ASA attack yields a substantial 14.51% average AP reduction, twice the effect of other algorithms. We also note that static scenes yield higher recognition AP values, and outcomes remain relatively consistent across varying weather conditions. Intriguingly, our study suggests that advancements in adversarial attack algorithms may be approaching its ``limitation''.In summary, our work underscores the significance of adversarial attacks in real-world contexts and introduces the DCI dataset as a versatile benchmark. Our findings provide valuable insights for enhancing the robustness of detection models and offer guidance for future research endeavors in the realm of adversarial attacks.
</details>
<details>
<summary>摘要</summary>
Note: The text has been translated into Simplified Chinese, which is the standardized form of Chinese used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="How-to-forecast-power-generation-in-wind-farms-Insights-from-leveraging-hierarchical-structure"><a href="#How-to-forecast-power-generation-in-wind-farms-Insights-from-leveraging-hierarchical-structure" class="headerlink" title="How to forecast power generation in wind farms? Insights from leveraging hierarchical structure"></a>How to forecast power generation in wind farms? Insights from leveraging hierarchical structure</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03472">http://arxiv.org/abs/2308.03472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucas English, Mahdi Abolghasemi</li>
<li>for: 预测可再生能源生产，帮助决策全球减排。</li>
<li>methods: 使用层次预测和协调，以提高预测质量。</li>
<li>results: 跨时间和空间协调预测方法可以提高预测精度，特别是在多个时间层级。 linear regression 可以在大多数水平上超过机器学习模型的性能。<details>
<summary>Abstract</summary>
Forecasting of renewable energy generation provides key insights which may help with decision-making towards global decarbonisation. Renewable energy generation can often be represented through cross-sectional hierarchies, whereby a single farm may have multiple individual generators. Hierarchical forecasting through reconciliation has demonstrated a significant increase in the quality of forecasts both theoretically and empirically. However, it is not evident whether forecasts generated by individual temporal and cross-sectional aggregation can be superior to integrated cross-temporal forecasts and to individual forecasts on more granular data. In this study, we investigate the accuracies of different cross-sectional and cross-temporal reconciliation methods using both linear regression and gradient boosting machine learning for forecasting wind farm power generation. We found that cross-temporal reconciliation is superior to individual cross-sectional reconciliation at multiple temporal aggregations. Cross-temporally reconciled machine learning base forecasts also demonstrated a high accuracy at coarser temporal granularities, which may encourage adoption for short-term wind forecasts. We also show that linear regression can outperform machine learning models across most levels in cross-sectional wind time series.
</details>
<details>
<summary>摘要</summary>
预测可再生能源生产提供关键的视野，帮助决策全球减灰化。可再生能源生产经常可以用分层结构表示，一个农场可以有多个个体发电机。层次预测通过协调可以提高预测质量， both theoretically and empirically。然而，不清楚 Whether forecasts generated by individual temporal and cross-sectional aggregation can be superior to integrated cross-temporal forecasts and to individual forecasts on more granular data.本研究 investigate 不同的横截和时间层整合预测方法的准确性，使用线性回归和梯度拟合机器学习模型预测风力农场电力生产。发现cross-temporal reconciliation 在多个时间层上胜过单个横截协调。同时，梯度拟合机器学习基础预测也在更粗细的时间层上表现出高准确性，这可能推动短期风预测的采用。此外，我们还发现了线性回归在大多数水平上超过机器学习模型的准确性。
</details></li>
</ul>
<hr>
<h2 id="Wide-Gaps-and-Clustering-Axioms"><a href="#Wide-Gaps-and-Clustering-Axioms" class="headerlink" title="Wide Gaps and Clustering Axioms"></a>Wide Gaps and Clustering Axioms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03464">http://arxiv.org/abs/2308.03464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mieczysław A. Kłopotek</li>
<li>for: 本研究旨在探讨k-means算法是否遵循克林伯格的聚类axiomaatic系统，并提出一些改进方案来使k-means更加符合这个系统。</li>
<li>methods: 本研究使用了两种新的聚类性特征：变量k-分割性和剩余k-分割性，并证明了k-means算法在欧几何或非欧几何空间中遵循克林伯格的一致性axioma。</li>
<li>results: 研究发现，k-means算法在某些情况下会violate克林伯格的一致性axioma，这是因为数据本身不符合聚类axioma。为了解决这个问题，研究提出了一些改进方案，包括一种基于变量k-分割性和剩余k-分割性的k-means算法。这些方案可以在欧几何和非欧几何空间中实现，并且可以使k-means算法更加符合克林伯格的聚类axioma。<details>
<summary>Abstract</summary>
The widely applied k-means algorithm produces clusterings that violate our expectations with respect to high/low similarity/density and is in conflict with Kleinberg's axiomatic system for distance based clustering algorithms that formalizes those expectations in a natural way. k-means violates in particular the consistency axiom. We hypothesise that this clash is due to the not explicated expectation that the data themselves should have the property of being clusterable in order to expect the algorithm clustering hem to fit a clustering axiomatic system. To demonstrate this, we introduce two new clusterability properties, variational k-separability and residual k-separability and show that then the Kleinberg's consistency axiom holds for k-means operating in the Euclidean or non-Euclidean space. Furthermore, we propose extensions of k-means algorithm that fit approximately the Kleinberg's richness axiom that does not hold for k-means. In this way, we reconcile k-means with Kleinberg's axiomatic framework in Euclidean and non-Euclidean settings. Besides contribution to the theory of axiomatic frameworks of clustering and for clusterability theory, practical contribution is the possibility to construct {datasets for testing purposes of algorithms optimizing k-means cost function. This includes a method of construction of {clusterable data with known in advance global optimum.
</details>
<details>
<summary>摘要</summary>
广泛应用的k-means算法生成的分布不符合我们对高/低相似性和密度的预期，并与克林伯格的距离基于对数据分组算法的axiomaatic系统产生冲突。k-means Violates particular consistency axiom。我们 hypothesize 这个冲突是因为数据本身不具备分组性，以至于预期k-means对数据进行分组。 To demonstrate this, we introduce two new clusterability properties, variational k-separability and residual k-separability, and show that then the Kleinberg's consistency axiom holds for k-means operating in the Euclidean or non-Euclidean space. Furthermore, we propose extensions of k-means algorithm that fit approximately the Kleinberg's richness axiom that does not hold for k-means. In this way, we reconcile k-means with Kleinberg's axiomatic framework in Euclidean and non-Euclidean settings. Besides contribution to the theory of axiomatic frameworks of clustering and for clusterability theory, practical contribution is the possibility to construct datasets for testing purposes of algorithms optimizing k-means cost function. This includes a method of construction of clusterable data with known in advance global optimum.
</details></li>
</ul>
<hr>
<h2 id="High-Resolution-Cranial-Defect-Reconstruction-by-Iterative-Low-Resolution-Point-Cloud-Completion-Transformers"><a href="#High-Resolution-Cranial-Defect-Reconstruction-by-Iterative-Low-Resolution-Point-Cloud-Completion-Transformers" class="headerlink" title="High-Resolution Cranial Defect Reconstruction by Iterative, Low-Resolution, Point Cloud Completion Transformers"></a>High-Resolution Cranial Defect Reconstruction by Iterative, Low-Resolution, Point Cloud Completion Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03813">http://arxiv.org/abs/2308.03813</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/MWod/DeepImplant_MICCAI_2023">https://github.com/MWod/DeepImplant_MICCAI_2023</a></li>
<li>paper_authors: Marek Wodzinski, Mateusz Daniol, Daria Hemmerling, Miroslaw Socha</li>
<li>for:  automatic cranial defect reconstruction</li>
<li>methods:  iterative, transformer-based method</li>
<li>results:  superior performance in terms of GPU memory consumption while maintaining high-quality of the reconstructed defectsHere’s the text in Simplified Chinese:</li>
<li>for: 自动化头部问题重建</li>
<li>methods: 迭代、基于传播器的方法</li>
<li>results:  GPU 内存消耗量下降，并保持高品质的缺陷重建I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Each year thousands of people suffer from various types of cranial injuries and require personalized implants whose manual design is expensive and time-consuming. Therefore, an automatic, dedicated system to increase the availability of personalized cranial reconstruction is highly desirable. The problem of the automatic cranial defect reconstruction can be formulated as the shape completion task and solved using dedicated deep networks. Currently, the most common approach is to use the volumetric representation and apply deep networks dedicated to image segmentation. However, this approach has several limitations and does not scale well into high-resolution volumes, nor takes into account the data sparsity. In our work, we reformulate the problem into a point cloud completion task. We propose an iterative, transformer-based method to reconstruct the cranial defect at any resolution while also being fast and resource-efficient during training and inference. We compare the proposed methods to the state-of-the-art volumetric approaches and show superior performance in terms of GPU memory consumption while maintaining high-quality of the reconstructed defects.
</details>
<details>
<summary>摘要</summary>
每年数千人都会因为不同类型的头部伤害而需要个性化嵌入式设备，但 manual 的设计是贵重时间的。因此，一个自动化、专门的系统可以大幅提高个性化头部重建的可用性。我们可以将这个问题 формули为形状完成任务，并使用专门的深度网络解决。现有的最常见方法是使用积分表示法，并应用深度网络进行图像分割。但这种方法存在一些限制，并不能扩展到高分辨率的体积，同时也不考虑数据稀缺性。在我们的工作中，我们将问题重新формализова为点云完成任务。我们提议一种迭代的变换器基本方法，可以在任何分辨率下重建头部缺陷，同时也具有快速和资源高效的训练和推理特点。我们与当前状态的积分方法进行比较，并显示我们的提议方法在 GPU 内存占用量方面具有显著优势，而无需牺牲高质量的缺陷重建。
</details></li>
</ul>
<hr>
<h2 id="Redesigning-Out-of-Distribution-Detection-on-3D-Medical-Images"><a href="#Redesigning-Out-of-Distribution-Detection-on-3D-Medical-Images" class="headerlink" title="Redesigning Out-of-Distribution Detection on 3D Medical Images"></a>Redesigning Out-of-Distribution Detection on 3D Medical Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07324">http://arxiv.org/abs/2308.07324</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anton Vasiliuk, Daria Frolova, Mikhail Belyaev, Boris Shirokikh</li>
<li>for: 本研究旨在解决验证医学影像分割中的异常样本检测问题，特别是由于缺乏明确的异常数据定义，导致许多人 искусственно设定问题而无法测量临床影响。</li>
<li>methods: 本研究提出了一种根据医学影像三维数据特点和下游任务（例如分割）重新定义异常样本检测问题。通过利用下游模型的性能来定义异常样本，我们可以无需明确ID&#x2F;OOD分类来衡量不同样本的影响。我们称这种方法为预期性能下降（EPD）。</li>
<li>results: 在11种CT和MRI异常样本检测挑战中，我们示出了EPD的效果，并证明EPD可以根据临床影响来排序方法。<details>
<summary>Abstract</summary>
Detecting out-of-distribution (OOD) samples for trusted medical image segmentation remains a significant challenge. The critical issue here is the lack of a strict definition of abnormal data, which often results in artificial problem settings without measurable clinical impact. In this paper, we redesign the OOD detection problem according to the specifics of volumetric medical imaging and related downstream tasks (e.g., segmentation). We propose using the downstream model's performance as a pseudometric between images to define abnormal samples. This approach enables us to weigh different samples based on their performance impact without an explicit ID/OOD distinction. We incorporate this weighting in a new metric called Expected Performance Drop (EPD). EPD is our core contribution to the new problem design, allowing us to rank methods based on their clinical impact. We demonstrate the effectiveness of EPD-based evaluation in 11 CT and MRI OOD detection challenges.
</details>
<details>
<summary>摘要</summary>
<<SYS>> transtable text into Simplified Chinese<</SYS>>检测非典型（OOD）样本 для信任的医学影像分割是一个重要的挑战。关键问题在于缺乏严格定义的异常数据，这经常导致人工设定的问题无法量化临床影响。在这篇论文中，我们重新设计了OOD检测问题，根据医学影像的特点和相关的下游任务（如分割）。我们提议使用下游模型的性能作为图像之间的 pseudometric，以定义异常样本。这种方法允许我们根据不同样本的性能影响加权它们，无需显式的ID/OOD分类。我们称之为预期性能下降（EPD）。 EPD是我们对新的问题设计的核心贡献，允许我们根据临床影响排名方法。我们在11个CT和MRI OOD检测挑战中展示了EPD-基于的评价效果。
</details></li>
</ul>
<hr>
<h2 id="Cross-Silo-Prototypical-Calibration-for-Federated-Learning-with-Non-IID-Data"><a href="#Cross-Silo-Prototypical-Calibration-for-Federated-Learning-with-Non-IID-Data" class="headerlink" title="Cross-Silo Prototypical Calibration for Federated Learning with Non-IID Data"></a>Cross-Silo Prototypical Calibration for Federated Learning with Non-IID Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03457">http://arxiv.org/abs/2308.03457</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qizhuang-qz/FedCSPC">https://github.com/qizhuang-qz/FedCSPC</a></li>
<li>paper_authors: Zhuang Qi, Lei Meng, Zitan Chen, Han Hu, Hui Lin, Xiangxu Meng</li>
<li>for: 这个论文目的是提出一种基于联合特征 проtotypical calibration 方法 (FedCSPC)，以在隐私保护下实现训练全球模型，并且能够对不同资料来源的数据进行一致性调整。</li>
<li>methods: 本论文使用了 Data Prototypical Modeling (DPM) 模组和 Cross-silo Prototypical Calibration (CSPC) 模组，DPM 模组可以帮助获取数据模式，而 CSPC 模组可以将不同来源的数据调整到一个共同的特征空间中，并且可以实现一致性调整。</li>
<li>results: 实验结果显示，FedCSPC 方法可以在不同资料来源上学习一致的特征，并且比起现有的方法有更好的性能。<details>
<summary>Abstract</summary>
Federated Learning aims to learn a global model on the server side that generalizes to all clients in a privacy-preserving manner, by leveraging the local models from different clients. Existing solutions focus on either regularizing the objective functions among clients or improving the aggregation mechanism for the improved model generalization capability. However, their performance is typically limited by the dataset biases, such as the heterogeneous data distributions and the missing classes. To address this issue, this paper presents a cross-silo prototypical calibration method (FedCSPC), which takes additional prototype information from the clients to learn a unified feature space on the server side. Specifically, FedCSPC first employs the Data Prototypical Modeling (DPM) module to learn data patterns via clustering to aid calibration. Subsequently, the cross-silo prototypical calibration (CSPC) module develops an augmented contrastive learning method to improve the robustness of the calibration, which can effectively project cross-source features into a consistent space while maintaining clear decision boundaries. Moreover, the CSPC module's ease of implementation and plug-and-play characteristics make it even more remarkable. Experiments were conducted on four datasets in terms of performance comparison, ablation study, in-depth analysis and case study, and the results verified that FedCSPC is capable of learning the consistent features across different data sources of the same class under the guidance of calibrated model, which leads to better performance than the state-of-the-art methods. The source codes have been released at https://github.com/qizhuang-qz/FedCSPC.
</details>
<details>
<summary>摘要</summary>
联合学习目标是在服务器端学习一个总模型，该模型在保持隐私的情况下泛化到所有客户端。现有解决方案通常是通过客户端对象函数的规范化或改进模型聚合机制来提高模型泛化能力。然而，这些方法通常受到数据偏见的限制，如不同数据分布和缺失类。为解决这个问题，本文提出了跨积 Silva 批量准备方法（FedCSPC），该方法通过客户端提供的额外原型信息在服务器端学习一个统一的特征空间。具体来说，FedCSPC首先使用数据批量模型（DPM）模块学习数据模式以帮助准备。接着，跨积 Silva 批量准备（CSPC）模块提供了一种改进了准备的批量学习方法，可以有效地将跨源特征投影到一个协调的空间中，保持清晰的决策界。此外，CSPC模块的易于实现和插件化特性使其更加吸引人。经过对四个数据集的性能比较、简洁研究、深入分析和案例研究，结果证明了 FedCSPC 能够在不同数据源之间学习一致的特征，这导致了与状态艺术法比较的更好的性能。代码已经在 GitHub 上发布，请参考 <https://github.com/qizhuang-qz/FedCSPC>。
</details></li>
</ul>
<hr>
<h2 id="Doubly-Robust-Estimator-for-Off-Policy-Evaluation-with-Large-Action-Spaces"><a href="#Doubly-Robust-Estimator-for-Off-Policy-Evaluation-with-Large-Action-Spaces" class="headerlink" title="Doubly Robust Estimator for Off-Policy Evaluation with Large Action Spaces"></a>Doubly Robust Estimator for Off-Policy Evaluation with Large Action Spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03443">http://arxiv.org/abs/2308.03443</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tatsu432/DR-estimator-OPE-large-action">https://github.com/tatsu432/DR-estimator-OPE-large-action</a></li>
<li>paper_authors: Tatsuhiro Shimizu, Laura Forastiere</li>
<li>for: Off-Policy Evaluation (OPE) in contextual bandit settings with large action spaces.</li>
<li>methods: 使用 Marginalized Inverse Propensity Scoring (MIPS) 和 Marginalized Doubly Robust (MDR)  estimator.</li>
<li>results: 提供了一种更加精度的 estimator, 并且在实验中证明了其超过了现有的 estimator.<details>
<summary>Abstract</summary>
We study Off-Policy Evaluation (OPE) in contextual bandit settings with large action spaces. The benchmark estimators suffer from severe bias and variance tradeoffs. Parametric approaches suffer from bias due to difficulty specifying the correct model, whereas ones with importance weight suffer from variance. To overcome these limitations, Marginalized Inverse Propensity Scoring (MIPS) was proposed to mitigate the estimator's variance via embeddings of an action. To make the estimator more accurate, we propose the doubly robust estimator of MIPS called the Marginalized Doubly Robust (MDR) estimator. Theoretical analysis shows that the proposed estimator is unbiased under weaker assumptions than MIPS while maintaining variance reduction against IPS, which was the main advantage of MIPS. The empirical experiment verifies the supremacy of MDR against existing estimators.
</details>
<details>
<summary>摘要</summary>
我们研究在Contextual Bandit设置下的Off-Policy评估（OPE），它们的标准估计器受到严重的偏见和方差交易的影响。参数化方法受到模型难以准确地特定的偏见，而重要性Weighted方法受到方差的影响。为了解决这些限制，我们提出了Embeddings of an action的Marginalized Inverse Propensity Scoring（MIPS）来减少估计器的方差。为了使估计器更准确，我们提出了MIPS的双重Robust（MDR）估计器。理论分析表明，我们的估计器在较弱的假设下具有不偏性，同时维持IPS的方差减少。实验证明了MDR的超越性。
</details></li>
</ul>
<hr>
<h2 id="PURL-Safe-and-Effective-Sanitization-of-Link-Decoration"><a href="#PURL-Safe-and-Effective-Sanitization-of-Link-Decoration" class="headerlink" title="PURL: Safe and Effective Sanitization of Link Decoration"></a>PURL: Safe and Effective Sanitization of Link Decoration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03417">http://arxiv.org/abs/2308.03417</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/purl-sanitizer/purl">https://github.com/purl-sanitizer/purl</a></li>
<li>paper_authors: Shaoor Munir, Patrick Lee, Umar Iqbal, Zubair Shafiq, Sandra Siby</li>
<li>for: 防止追踪浏览器新增防御策略，novel tracking方法继续出现。</li>
<li>methods: 利用机器学习方法，检测和净化链接装饰中的追踪信息。</li>
<li>results: PURL可以准确地检测和净化链接装饰，比现有Countermeasure更高效和可靠，并对常见欺骗技术有较好的鲁棒性。<details>
<summary>Abstract</summary>
While privacy-focused browsers have taken steps to block third-party cookies and browser fingerprinting, novel tracking methods that bypass existing defenses continue to emerge. Since trackers need to exfiltrate information from the client- to server-side through link decoration regardless of the tracking technique they employ, a promising orthogonal approach is to detect and sanitize tracking information in decorated links. We present PURL, a machine-learning approach that leverages a cross-layer graph representation of webpage execution to safely and effectively sanitize link decoration. Our evaluation shows that PURL significantly outperforms existing countermeasures in terms of accuracy and reducing website breakage while being robust to common evasion techniques. We use PURL to perform a measurement study on top-million websites. We find that link decorations are widely abused by well-known advertisers and trackers to exfiltrate user information collected from browser storage, email addresses, and scripts involved in fingerprinting.
</details>
<details>
<summary>摘要</summary>
“对于隐私浏览器的尝试，第三方Cookie和浏览器指纹都已经被防止，但新的追踪方法继续出现，这些方法可以跳过现有的防护措施。因为追踪者需要将信息从客户端传到服务器端，因此一个有效的对策是检测和清理链接装饰。我们提出了PURL，一种机器学习方法，利用页面执行的跨层图表示来安全地和有效地检测和清理链接装饰。我们的评估显示，PURL比现有的对策更高度精度和减少网站损坏，同时具有对常见的逃脱技术的抗性。我们使用PURL进行了顶千个网站的测量研究，发现链接装饰被知名的广告商和追踪者广泛运用，以将用户信息从浏览器储存、电子邮件地址和脚本散发扫描撷取到。”Note: The translation is in Simplified Chinese, which is the standard Chinese writing system used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="Noncompact-uniform-universal-approximation"><a href="#Noncompact-uniform-universal-approximation" class="headerlink" title="Noncompact uniform universal approximation"></a>Noncompact uniform universal approximation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03812">http://arxiv.org/abs/2308.03812</a></li>
<li>repo_url: None</li>
<li>paper_authors: Teun D. H. van Nuland</li>
<li>for: 这个论文探讨了universal approximation theorem在非 компакт输入空间 $\mathbb R^n$ 上的普遍化 convergenc。</li>
<li>methods: 这个论文使用了神经网络来对所有在 $\mathbb R^n$ 上连续函数进行uniform approximation。</li>
<li>results: 研究发现，对于所有非零 activation function $\varphi$ 和所有 $n$ 和 $l\geq2$， THEN $\mathcal{N}_\varphi^l(\mathbb R^n)$ 是一个 vector space，且对于左限和右限不同的 $\varphi$，这个 vector space 独立于 $\varphi$ 和 $l$，且等于 sigmoid  compose with one-dimensional projection 的闭 span。对于左限和右限相同的 $\varphi$，这个 vector space 等于 commutative resolvent algebra，一个 C*-algebra，且独立于 $l\geq1$。<details>
<summary>Abstract</summary>
The universal approximation theorem is generalised to uniform convergence on the (noncompact) input space $\mathbb R^n$. All continuous functions that vanish at infinity can be uniformly approximated by neural networks with one hidden layer, for all continuous activation functions $\varphi\neq0$ with asymptotically linear behaviour at $\pm\infty$. When $\varphi$ is moreover bounded, we exactly determine which functions can be uniformly approximated by neural networks, with the following unexpected results. Let $\overline{\mathcal{N}_\varphi^l(\mathbb R^n)}$ denote the vector space of functions that are uniformly approximable by neural networks with $l$ hidden layers and $n$ inputs. For all $n$ and all $l\geq2$, $\overline{\mathcal{N}_\varphi^l(\mathbb R^n)}$ turns out to be an algebra under the pointwise product. If the left limit of $\varphi$ differs from its right limit (for instance, when $\varphi$ is sigmoidal) the algebra $\overline{\mathcal{N}_\varphi^l(\mathbb R^n)}$ ($l\geq2$) is independent of $\varphi$ and $l$, and equals the closed span of products of sigmoids composed with one-dimensional projections. If the left limit of $\varphi$ equals its right limit, $\overline{\mathcal{N}_\varphi^l(\mathbb R^n)}$ ($l\geq1$) equals the (real part of the) commutative resolvent algebra, a C*-algebra which is used in mathematical approaches to quantum theory. In the latter case, the algebra is independent of $l\geq1$, whereas in the former case $\overline{\mathcal{N}_\varphi^2(\mathbb R^n)}$ is strictly bigger than $\overline{\mathcal{N}_\varphi^1(\mathbb R^n)}$.
</details>
<details>
<summary>摘要</summary>
“universal approximation theorem”被推广到非 compat 输入空间 $\mathbb R^n$ 上的 uniform convergence。所有在 infinities 处消失的连续函数可以由一层神经网络 uniform approximation，对所有非零连续激活函数 $\varphi$ 的 asymptotically linear behavior at $\pm\infty$。当 $\varphi$  moreover bounded 时，我们可以准确地确定可以 uniform approximation 的函数，并且有以下意外的结果。具有 $n$ 输入和 $l$ 层神经网络的函数空间 $\mathcal{N}_\varphi^l(\mathbb R^n)$ 被定义为可以通过神经网络 uniform approximation 的函数空间。对所有 $n$ 和 $l\geq2$，$\mathcal{N}_\varphi^l(\mathbb R^n)$ 是一个点wise product 的代数。如果左限的 $\varphi$ 不同于右限（例如，sigmoid 函数）， то $\mathcal{N}_\varphi^l(\mathbb R^n)$ ($l\geq2$) 是 $\varphi$ 和 $l$ 的独立的代数，等于sigmoid  compose with one-dimensional projection 的闭 span。如果左限的 $\varphi$ 等于右限，那么 $\mathcal{N}_\varphi^l(\mathbb R^n)$ ($l\geq1$) 是一个 commutative resolvent algebra，一个 C*-algebra，这种代数在数学方法中用于量子理论。在后者情况下，这个代数是 $l\geq1$ 的独立的，而在前者情况下，$\mathcal{N}_\varphi^2(\mathbb R^n)$ 是 $\mathcal{N}_\varphi^1(\mathbb R^n)$ 的 strictly bigger。
</details></li>
</ul>
<hr>
<h2 id="Applied-metamodelling-for-ATM-performance-simulations"><a href="#Applied-metamodelling-for-ATM-performance-simulations" class="headerlink" title="Applied metamodelling for ATM performance simulations"></a>Applied metamodelling for ATM performance simulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03404">http://arxiv.org/abs/2308.03404</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christoffer Riis, Francisco N. Antunes, Tatjana Bolić, Gérald Gurtner, Andrew Cook, Carlos Lima Azevedo, Francisco Câmara Pereira</li>
<li>for: 提高ATM simulator的计划和运作的决策支持</li>
<li>methods:  integrate active learning和SHAP值进行模拟мета模型</li>
<li>results: 比XGBoost模型具有更好的解释能力，并且可以更好地揭示输入和输出变量之间的隐藏关系。<details>
<summary>Abstract</summary>
The use of Air traffic management (ATM) simulators for planing and operations can be challenging due to their modelling complexity. This paper presents XALM (eXplainable Active Learning Metamodel), a three-step framework integrating active learning and SHAP (SHapley Additive exPlanations) values into simulation metamodels for supporting ATM decision-making. XALM efficiently uncovers hidden relationships among input and output variables in ATM simulators, those usually of interest in policy analysis. Our experiments show XALM's predictive performance comparable to the XGBoost metamodel with fewer simulations. Additionally, XALM exhibits superior explanatory capabilities compared to non-active learning metamodels.   Using the `Mercury' (flight and passenger) ATM simulator, XALM is applied to a real-world scenario in Paris Charles de Gaulle airport, extending an arrival manager's range and scope by analysing six variables. This case study illustrates XALM's effectiveness in enhancing simulation interpretability and understanding variable interactions. By addressing computational challenges and improving explainability, XALM complements traditional simulation-based analyses.   Lastly, we discuss two practical approaches for reducing the computational burden of the metamodelling further: we introduce a stopping criterion for active learning based on the inherent uncertainty of the metamodel, and we show how the simulations used for the metamodel can be reused across key performance indicators, thus decreasing the overall number of simulations needed.
</details>
<details>
<summary>摘要</summary>
使用空交通管理（ATM）模拟器进行规划和运行可能会面临模型复杂性挑战。本文介绍XALM（可解释主动学习元模型），一个三步框架，将活动学习和SHAP（SHapley Additive exPlanations）值 integrate到 simulation元模型中，以支持ATM决策。XALM能够效率地揭示ATM模拟器中输入和输出变量之间的隐藏关系，通常是政策分析中的关键点。我们的实验表明，XALM的预测性能与XGBoost元模型相当，而且XALM的解释能力比非活动学习元模型更高。使用Mercury（飞机和乘客）ATM模拟器，XALM在法国巴黎查理·德·古尔机场的一个实际场景中应用，分析了六个变量。这个案例示出了XALM在提高模拟解释性和理解变量互动方面的效果。通过解决计算挑战和提高解释性，XALM补充了传统的模拟分析。最后，我们介绍了两种实用的计算压力减轻方法：基于元模型内在不确定性的活动学习停止 criterion，以及可以将模拟用于元模型中的 simulation reuse across key performance indicators，从而降低总的模拟数量。
</details></li>
</ul>
<hr>
<h2 id="Towards-Machine-Learning-based-Fish-Stock-Assessment"><a href="#Towards-Machine-Learning-based-Fish-Stock-Assessment" class="headerlink" title="Towards Machine Learning-based Fish Stock Assessment"></a>Towards Machine Learning-based Fish Stock Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03403">http://arxiv.org/abs/2308.03403</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Lüdtke, Maria E. Pierce</li>
<li>for: 提高可持续性渔业管理中鱼类资源的准确评估</li>
<li>methods: 使用机器学习模型改进鱼类资源参数的估计和预测</li>
<li>results: 对五种不同的鱼类资源进行实验，发现预测减降率和繁殖种群质量的准确率有很大改善<details>
<summary>Abstract</summary>
The accurate assessment of fish stocks is crucial for sustainable fisheries management. However, existing statistical stock assessment models can have low forecast performance of relevant stock parameters like recruitment or spawning stock biomass, especially in ecosystems that are changing due to global warming and other anthropogenic stressors. In this paper, we investigate the use of machine learning models to improve the estimation and forecast of such stock parameters. We propose a hybrid model that combines classical statistical stock assessment models with supervised ML, specifically gradient boosted trees. Our hybrid model leverages the initial estimate provided by the classical model and uses the ML model to make a post-hoc correction to improve accuracy. We experiment with five different stocks and find that the forecast accuracy of recruitment and spawning stock biomass improves considerably in most cases.
</details>
<details>
<summary>摘要</summary>
准确评估淡水鱼资源非常重要，以实现可持续的渔业管理。然而，现有的统计鱼填评估模型可能具有低预测性能，特别是在因全球变暖和其他人类压力而变化的生态系统中。在这篇论文中，我们研究了使用机器学习模型提高鱼填评估和预测的方法。我们提议一种混合模型，结合传统的统计鱼填评估模型和监督学习，具体来说是梯度提升树。我们的混合模型利用传统模型提供的初始估计，并使用ML模型进行后续更正，以提高准确性。我们对五个不同的鱼种进行实验，发现预测准确性在大多数情况下有显著提高。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Nucleus-Segmentation-with-HARU-Net-A-Hybrid-Attention-Based-Residual-U-Blocks-Network"><a href="#Enhancing-Nucleus-Segmentation-with-HARU-Net-A-Hybrid-Attention-Based-Residual-U-Blocks-Network" class="headerlink" title="Enhancing Nucleus Segmentation with HARU-Net: A Hybrid Attention Based Residual U-Blocks Network"></a>Enhancing Nucleus Segmentation with HARU-Net: A Hybrid Attention Based Residual U-Blocks Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03382">http://arxiv.org/abs/2308.03382</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junzhou Chen, Qian Huang, Yulin Chen, Linyi Qian, Chengyuan Yu</li>
<li>for: 这个研究主要旨在提高核体像素化的精度和效率，以便于生物医学分析、诊断和分类中使用。</li>
<li>methods: 我们提出了一个基于双支分支网络的混合注意力残差U-块方法，可以同时预测目标信息和目标 outline。我们还提出了一个后处理方法，可以结合目标信息和目标 outline来区别遮蔽的核体和生成实例分割图像。</li>
<li>results: 我们的方法在各个数据集上进行了广泛的量化评估，结果显示我们的方法在与现有方法比较时表现出色，特别是在应用于不规则核体的情况下。<details>
<summary>Abstract</summary>
Nucleus image segmentation is a crucial step in the analysis, pathological diagnosis, and classification, which heavily relies on the quality of nucleus segmentation. However, the complexity of issues such as variations in nucleus size, blurred nucleus contours, uneven staining, cell clustering, and overlapping cells poses significant challenges. Current methods for nucleus segmentation primarily rely on nuclear morphology or contour-based approaches. Nuclear morphology-based methods exhibit limited generalization ability and struggle to effectively predict irregular-shaped nuclei, while contour-based extraction methods face challenges in accurately segmenting overlapping nuclei. To address the aforementioned issues, we propose a dual-branch network using hybrid attention based residual U-blocks for nucleus instance segmentation. The network simultaneously predicts target information and target contours. Additionally, we introduce a post-processing method that combines the target information and target contours to distinguish overlapping nuclei and generate an instance segmentation image. Within the network, we propose a context fusion block (CF-block) that effectively extracts and merges contextual information from the network. Extensive quantitative evaluations are conducted to assess the performance of our method. Experimental results demonstrate the superior performance of the proposed method compared to state-of-the-art approaches on the BNS, MoNuSeg, CoNSeg, and CPM-17 datasets.
</details>
<details>
<summary>摘要</summary>
核心图像分割是生物体分析、病理诊断和分类中的关键步骤，它的质量直接影响下游应用的结果。然而，核心图像分割过程面临着许多复杂的问题，如核心大小变化、核心渐圆、不均颜色、细胞堆叠和重叠细胞等。现有的核心图像分割方法主要基于核心形态或边沿基本方法。核心形态基本方法具有限定泛化能力，难以预测不规则形状的核心，而边沿基本方法在重叠细胞分割上存在困难。为解决以上问题，我们提出了一种基于双支网络的核心实例分割方法。该网络同时预测目标信息和目标极值。此外，我们引入了一种 combining 目标信息和目标极值的后处理方法，以分辨重叠的核心并生成实例分割图像。在网络中，我们提出了一种Context Fusion块（CF-块），可以有效地提取和融合网络中的Contextual信息。我们对方法进行了广泛的量化评估，并发现方法的性能在BNS、MoNuSeg、CoNSeg和CPM-17等数据集上都显著超过了现有方法。
</details></li>
</ul>
<hr>
<h2 id="A-reading-survey-on-adversarial-machine-learning-Adversarial-attacks-and-their-understanding"><a href="#A-reading-survey-on-adversarial-machine-learning-Adversarial-attacks-and-their-understanding" class="headerlink" title="A reading survey on adversarial machine learning: Adversarial attacks and their understanding"></a>A reading survey on adversarial machine learning: Adversarial attacks and their understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03363">http://arxiv.org/abs/2308.03363</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shashank Kotyan</li>
<li>for: 本研究旨在探讨和理解针对神经网络的攻击方法，以系统化的方式掌握攻击方法的类别和特点。</li>
<li>methods: 本文使用了多种攻击方法，包括随机攻击、梯度攻击、缺失攻击、噪声攻击等，以测试神经网络的抵御能力。</li>
<li>results: 本文通过对多种神经网络模型进行攻击和防御测试，发现攻击方法的多样性和神经网络模型的抵御能力强度不同，并提出了一些未来研究方向。<details>
<summary>Abstract</summary>
Deep Learning has empowered us to train neural networks for complex data with high performance. However, with the growing research, several vulnerabilities in neural networks have been exposed. A particular branch of research, Adversarial Machine Learning, exploits and understands some of the vulnerabilities that cause the neural networks to misclassify for near original input. A class of algorithms called adversarial attacks is proposed to make the neural networks misclassify for various tasks in different domains. With the extensive and growing research in adversarial attacks, it is crucial to understand the classification of adversarial attacks. This will help us understand the vulnerabilities in a systematic order and help us to mitigate the effects of adversarial attacks. This article provides a survey of existing adversarial attacks and their understanding based on different perspectives. We also provide a brief overview of existing adversarial defences and their limitations in mitigating the effect of adversarial attacks. Further, we conclude with a discussion on the future research directions in the field of adversarial machine learning.
</details>
<details>
<summary>摘要</summary>
深度学习已经赋予我们训练复杂数据的神经网络高性能。然而，随着研究的增长，许多神经网络的漏洞也被曝光。一个特定的研究分支，敌意机器学习，利用和掌握了一些导致神经网络错分的漏洞。一类称为敌意攻击的算法被提出来使神经网络错分各种任务在不同领域。随着敌意攻击的广泛和增长的研究，我们需要理解敌意攻击的分类。这将帮助我们系统地理解漏洞，并帮助我们减轻敌意攻击的影响。本文提供了现有的敌意攻击和它们的理解基于不同的角度。我们还提供了敌意防御的简要概述和其限制在减轻敌意攻击的影响。最后，我们结束 WITH 未来机器学习领域的研究方向。
</details></li>
</ul>
<hr>
<h2 id="Solving-Falkner-Skan-type-equations-via-Legendre-and-Chebyshev-Neural-Blocks"><a href="#Solving-Falkner-Skan-type-equations-via-Legendre-and-Chebyshev-Neural-Blocks" class="headerlink" title="Solving Falkner-Skan type equations via Legendre and Chebyshev Neural Blocks"></a>Solving Falkner-Skan type equations via Legendre and Chebyshev Neural Blocks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03337">http://arxiv.org/abs/2308.03337</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alireza Afzal Aghaei, Kourosh Parand, Ali Nikkhah, Shakila Jaberi</li>
<li>for: 解决非线性法克-斯坦方程</li>
<li>methods: 使用Legendre和Chebyshev神经块，利用 ortogonal  polynomials 在神经网络中增强人工神经网络的近似能力</li>
<li>results: 通过模拟不同的法克-斯坦方程配置，实现了提高计算效率和准确率的目的<details>
<summary>Abstract</summary>
In this paper, a new deep-learning architecture for solving the non-linear Falkner-Skan equation is proposed. Using Legendre and Chebyshev neural blocks, this approach shows how orthogonal polynomials can be used in neural networks to increase the approximation capability of artificial neural networks. In addition, utilizing the mathematical properties of these functions, we overcome the computational complexity of the backpropagation algorithm by using the operational matrices of the derivative. The efficiency of the proposed method is carried out by simulating various configurations of the Falkner-Skan equation.
</details>
<details>
<summary>摘要</summary>
在本文中，一种新的深度学习架构，用于解决非线性法克纳-斯坦方程，被提出。使用Legendre和Chebyshev神经块，这种方法展示了如何在神经网络中使用正交多项式增加人工神经网络的近似能力。此外，利用这些函数的数学性质，我们超越了反射算法的计算复杂性，使用操作矩阵的导数。提出的方法的效率被通过 simulate多种法克纳-斯坦方程的配置进行证明。
</details></li>
</ul>
<hr>
<h2 id="Non-Convex-Bilevel-Optimization-with-Time-Varying-Objective-Functions"><a href="#Non-Convex-Bilevel-Optimization-with-Time-Varying-Objective-Functions" class="headerlink" title="Non-Convex Bilevel Optimization with Time-Varying Objective Functions"></a>Non-Convex Bilevel Optimization with Time-Varying Objective Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03811">http://arxiv.org/abs/2308.03811</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sen Lin, Daouda Sow, Kaiyi Ji, Yingbin Liang, Ness Shroff</li>
<li>for: 本研究强调在在线应用中实现精细化优化，满足流动数据和时间变化函数的需求。</li>
<li>methods: 我们提出了一种基于单 Loop 的在线双层优化器（SOBOW），通过窗口均值来更新外层决策，不需要知道过去函数。我们还开发了一种新的分析技术，用于综合分析决策变量之间的复杂 Coupling，并且精细控制了 hypergradient 估计误差。</li>
<li>results: 我们证明 SOBOW 可以在某些条件下实现幂等级的双层本地 regret。广泛的实验结果证明 SOBOW 的效果。<details>
<summary>Abstract</summary>
Bilevel optimization has become a powerful tool in a wide variety of machine learning problems. However, the current nonconvex bilevel optimization considers an offline dataset and static functions, which may not work well in emerging online applications with streaming data and time-varying functions. In this work, we study online bilevel optimization (OBO) where the functions can be time-varying and the agent continuously updates the decisions with online streaming data. To deal with the function variations and the unavailability of the true hypergradients in OBO, we propose a single-loop online bilevel optimizer with window averaging (SOBOW), which updates the outer-level decision based on a window average of the most recent hypergradient estimations stored in the memory. Compared to existing algorithms, SOBOW is computationally efficient and does not need to know previous functions. To handle the unique technical difficulties rooted in single-loop update and function variations for OBO, we develop a novel analytical technique that disentangles the complex couplings between decision variables, and carefully controls the hypergradient estimation error. We show that SOBOW can achieve a sublinear bilevel local regret under mild conditions. Extensive experiments across multiple domains corroborate the effectiveness of SOBOW.
</details>
<details>
<summary>摘要</summary>
bilateral 优化已成为机器学习问题中的一种强大工具。然而，当前的非凸 bilateral 优化假设了一个离线数据集和静止函数，这可能不适用于新般的在线应用程序中的流动数据和时间变化函数。在这种工作中，我们研究在线 bilateral 优化（OBO），其中函数可以是时间变化的，代理人在线流动数据中不断更新决策。为了处理函数的变化和真实的梯度不可知，我们提议了一种带窗口平均（SOBOW）的单loop在线 bilateral 优化器，其在内存中保存最近的梯度估计，并基于窗口平均更新外层决策。相比现有算法，SOBOW具有计算效率和不需要知道前一个函数的优点。为了处理单 loop 更新和函数变化对 OBO 的独特技术难点，我们开发了一种新的分析技术，可以分解决策变量之间的复杂 Coupling，并且精心控制梯度估计错误。我们表明，SOBOW 可以在某些条件下 achieve 下降的 bilateral 地方 regret。广泛的实验证明了 SOBOW 的有效性。
</details></li>
</ul>
<hr>
<h2 id="Expediting-Neural-Network-Verification-via-Network-Reduction"><a href="#Expediting-Neural-Network-Verification-via-Network-Reduction" class="headerlink" title="Expediting Neural Network Verification via Network Reduction"></a>Expediting Neural Network Verification via Network Reduction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03330">http://arxiv.org/abs/2308.03330</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuyi Zhong, Ruiwei Wang, Siau-Cheng Khoo</li>
<li>for: 验证深度神经网络的安全性属性，以确保神经网络在关键应用中正常工作。</li>
<li>methods: 提出了多种验证方法，以验证神经网络的安全性。但是，许多已知的验证工具仍然无法处理复杂的网络架构和大型神经网络。本文提出了一种网络减少技术作为验证前置处理方法。</li>
<li>results: 我们在大量的 benchmark 上实验表明，提posed 的减少技术可以减少神经网络，并使现有的验证工具更快速。此外，实验结果还表明，网络减少可以提高现有验证工具对许多神经网络的可用性。<details>
<summary>Abstract</summary>
A wide range of verification methods have been proposed to verify the safety properties of deep neural networks ensuring that the networks function correctly in critical applications. However, many well-known verification tools still struggle with complicated network architectures and large network sizes. In this work, we propose a network reduction technique as a pre-processing method prior to verification. The proposed method reduces neural networks via eliminating stable ReLU neurons, and transforming them into a sequential neural network consisting of ReLU and Affine layers which can be handled by the most verification tools. We instantiate the reduction technique on the state-of-the-art complete and incomplete verification tools, including alpha-beta-crown, VeriNet and PRIMA. Our experiments on a large set of benchmarks indicate that the proposed technique can significantly reduce neural networks and speed up existing verification tools. Furthermore, the experiment results also show that network reduction can improve the availability of existing verification tools on many networks by reducing them into sequential neural networks.
</details>
<details>
<summary>摘要</summary>
深度神经网络的安全性特性的验证方法有很多已经被提出，以确保神经网络在关键应用中正确地工作。然而，许多知名的验证工具仍然无法处理复杂的网络架构和大型网络。在这种情况下，我们提出了一种网络减少技术作为预处理方法，以降低验证工具的难度。我们的方法利用稳定的ReLU神经元消除和转换为一个包含ReLU和Affine层的顺序神经网络，这种网络可以被大多数验证工具处理。我们在 alpha-beta-crown、VeriNet 和 PRIMA 等当今最佳实践中的完整和 incomplete 验证工具上实现了这种减少技术。我们对一组大型标准 benchmark 进行了实验，结果表明，我们的方法可以减少神经网络，并使现有的验证工具在许多网络上提高可用性。此外，实验结果还表明，网络减少可以提高现有验证工具对许多网络的验证能力。
</details></li>
</ul>
<hr>
<h2 id="AFN-Adaptive-Fusion-Normalization-via-Encoder-Decoder-Framework"><a href="#AFN-Adaptive-Fusion-Normalization-via-Encoder-Decoder-Framework" class="headerlink" title="AFN: Adaptive Fusion Normalization via Encoder-Decoder Framework"></a>AFN: Adaptive Fusion Normalization via Encoder-Decoder Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03321">http://arxiv.org/abs/2308.03321</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/huanranchen/ASRNorm">https://github.com/huanranchen/ASRNorm</a></li>
<li>paper_authors: Zikai Zhou, Huanran Chen</li>
<li>for: 该论文目的是提出一种统一的Normalization函数，以减少不同Normalization函数的缺点。</li>
<li>methods: 该论文使用了多种Normalization函数，并通过对比这些函数的优缺点，提出了一种新的Adaptive Fusion Normalization函数。</li>
<li>results: 实验结果显示，AFN函数在领域泛化和图像分类任务中表现较好，超过了之前的Normalization技术。<details>
<summary>Abstract</summary>
The success of deep learning is inseparable from normalization layers. Researchers have proposed various normalization functions, and each of them has both advantages and disadvantages. In response, efforts have been made to design a unified normalization function that combines all normalization procedures and mitigates their weaknesses. We also proposed a new normalization function called Adaptive Fusion Normalization. Through experiments, we demonstrate AFN outperforms the previous normalization techniques in domain generalization and image classification tasks.
</details>
<details>
<summary>摘要</summary>
深度学习的成功与 нормализацион层无可分割。研究人员已经提出了多种 нормализацион函数，每种都有其优点和缺点。为了解决这些弱点，努力设计一个统一的 нормализацион函数，既能汇集所有的 нормализацион过程，又能减轻它们的缺点。我们还提出了一种新的 нормаliasjon函数，叫做自适应融合 нормаliasjon（AFN）。经过实验，我们证明AFN在领域普适化和图像分类任务中表现出色。Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Binary-Federated-Learning-with-Client-Level-Differential-Privacy"><a href="#Binary-Federated-Learning-with-Client-Level-Differential-Privacy" class="headerlink" title="Binary Federated Learning with Client-Level Differential Privacy"></a>Binary Federated Learning with Client-Level Differential Privacy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03320">http://arxiv.org/abs/2308.03320</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lumin Liu, Jun Zhang, Shenghui Song, Khaled B. Letaief</li>
<li>for: 提高 Federated Learning 系统的隐私保护和性能。</li>
<li>methods: 使用 binary neural networks (BNNs) 和离散噪声来实现 client-level 隐私保护，并且通过减少模型参数的精度来提高通信效率。</li>
<li>results: 实验结果基于 MNIST 和 Fashion-MNIST 数据集表明，提议的训练算法可以实现客户端级隐私保护而同时具有低通信开销的优势。<details>
<summary>Abstract</summary>
Federated learning (FL) is a privacy-preserving collaborative learning framework, and differential privacy can be applied to further enhance its privacy protection. Existing FL systems typically adopt Federated Average (FedAvg) as the training algorithm and implement differential privacy with a Gaussian mechanism. However, the inherent privacy-utility trade-off in these systems severely degrades the training performance if a tight privacy budget is enforced. Besides, the Gaussian mechanism requires model weights to be of high-precision. To improve communication efficiency and achieve a better privacy-utility trade-off, we propose a communication-efficient FL training algorithm with differential privacy guarantee. Specifically, we propose to adopt binary neural networks (BNNs) and introduce discrete noise in the FL setting. Binary model parameters are uploaded for higher communication efficiency and discrete noise is added to achieve the client-level differential privacy protection. The achieved performance guarantee is rigorously proved, and it is shown to depend on the level of discrete noise. Experimental results based on MNIST and Fashion-MNIST datasets will demonstrate that the proposed training algorithm achieves client-level privacy protection with performance gain while enjoying the benefits of low communication overhead from binary model updates.
</details>
<details>
<summary>摘要</summary>
federated learning（FL）是一种隐私保护的协作学习框架，可以通过减少隐私泄露来进一步增强隐私保护。现有的FL系统通常采用联邦平均（FedAvg）作为训练算法，并通过高精度的模型权重来实现减少隐私的目的。然而，这种隐私性和用途之间的质量负担在这些系统中严重地降低了训练性能，特别是当强制实施严格的隐私预算时。此外，高精度的模型权重需要高精度的数据。为了提高通信效率和实现更好的隐私性和用途之间的质量负担，我们提议一种通信高效的FL训练算法，并且保证隐私性。具体来说，我们提议采用二进制神经网络（BNN）和在FL设定中引入离散噪声。二进制模型参数上传以提高通信效率，而离散噪声可以实现客户端级别的隐私保护。我们通过 teorema 证明了性能保证，并证明其取决于离散噪声的水平。实验结果基于 MNIST 和 Fashion-MNIST 数据集表明，提议的训练算法可以实现客户端级别的隐私保护，同时享受到低通信开销的 binary 模型更新的好处。
</details></li>
</ul>
<hr>
<h2 id="HomOpt-A-Homotopy-Based-Hyperparameter-Optimization-Method"><a href="#HomOpt-A-Homotopy-Based-Hyperparameter-Optimization-Method" class="headerlink" title="HomOpt: A Homotopy-Based Hyperparameter Optimization Method"></a>HomOpt: A Homotopy-Based Hyperparameter Optimization Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03317">http://arxiv.org/abs/2308.03317</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jeffkinnison/shadho">https://github.com/jeffkinnison/shadho</a></li>
<li>paper_authors: Sophia J. Abraham, Kehelwala D. G. Maduranga, Jeffery Kinnison, Zachariah Carmichael, Jonathan D. Hauenstein, Walter J. Scheirer</li>
<li>for: 提高机器学习模型的性能和效率，即 Hyperparameter Optimization (HPO) 问题。</li>
<li>methods: 提出一种新的数据驱动的 Hyperparameter Optimization 方法，基于 Generalized Additive Model (GAM) 函数和 homotopy 优化。</li>
<li>results: 对多种优化技术（如 Random Search、TPE、Bayes 和 SMAC）进行比较，并在多个标准机器学习Benchmark和开放集ognition任务上显示出更好的目标性能。<details>
<summary>Abstract</summary>
Machine learning has achieved remarkable success over the past couple of decades, often attributed to a combination of algorithmic innovations and the availability of high-quality data available at scale. However, a third critical component is the fine-tuning of hyperparameters, which plays a pivotal role in achieving optimal model performance. Despite its significance, hyperparameter optimization (HPO) remains a challenging task for several reasons. Many HPO techniques rely on naive search methods or assume that the loss function is smooth and continuous, which may not always be the case. Traditional methods, like grid search and Bayesian optimization, often struggle to quickly adapt and efficiently search the loss landscape. Grid search is computationally expensive, while Bayesian optimization can be slow to prime. Since the search space for HPO is frequently high-dimensional and non-convex, it is often challenging to efficiently find a global minimum. Moreover, optimal hyperparameters can be sensitive to the specific dataset or task, further complicating the search process. To address these issues, we propose a new hyperparameter optimization method, HomOpt, using a data-driven approach based on a generalized additive model (GAM) surrogate combined with homotopy optimization. This strategy augments established optimization methodologies to boost the performance and effectiveness of any given method with faster convergence to the optimum on continuous, discrete, and categorical domain spaces. We compare the effectiveness of HomOpt applied to multiple optimization techniques (e.g., Random Search, TPE, Bayes, and SMAC) showing improved objective performance on many standardized machine learning benchmarks and challenging open-set recognition tasks.
</details>
<details>
<summary>摘要</summary>
机器学习在过去几十年内取得了很大成功，经常归功于算法创新和大规模数据的可用性。然而，一个第三要 componenet是细化参数的调整，它在实现优化模型性能中扮演着关键的角色。尽管其重要性，但参数优化（HPO）仍然是一个具有挑战性的任务，主要因为以下几个原因：多数HPO技术利用粗暴的搜索方法，或者假设损失函数是连续的，这并不总是情况。传统的方法，如格里德搜索和bayesian优化，经常难以快速适应和有效地搜索损失函数的 landscape。格里德搜索 computationally expensive，而bayesian优化可能需要很长时间来 prime。由于搜索空间 дляHPOfrequently高维和非 convex，因此寻找全局最优点是有很大挑战。此外，优化参数可能会受到特定的数据集或任务的影响，这进一步增加了搜索过程的复杂性。为解决这些问题，我们提出了一种新的参数优化方法，HomOpt，使用基于通用添加模型（GAM）的数据驱动方法，并结合抽象优化。这种策略可以增强现有优化方法的性能和有效性，并在不同的域空间上提供更快的趋势。我们对HomOpt应用于多种优化技术（例如Random Search、TPE、Bayes和SMAC），并在许多标准化机器学习benchmark和开放集成任务上显示出了提高了目标性能。
</details></li>
</ul>
<hr>
<h2 id="Deep-Q-Network-for-Stochastic-Process-Environments"><a href="#Deep-Q-Network-for-Stochastic-Process-Environments" class="headerlink" title="Deep Q-Network for Stochastic Process Environments"></a>Deep Q-Network for Stochastic Process Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03316">http://arxiv.org/abs/2308.03316</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kuangheng He</li>
<li>for: 本研究用深度学习抽象环境中的束缚学习方法解决复杂问题。</li>
<li>methods: 本研究使用深度Q学习网络，并评估不同结构的网络在束缚过程环境中的性能。</li>
<li>results: 研究结果表明，使用特定网络结构可以在束缚过程环境中提高性能。<details>
<summary>Abstract</summary>
Reinforcement learning is a powerful approach for training an optimal policy to solve complex problems in a given system. This project aims to demonstrate the application of reinforcement learning in stochastic process environments with missing information, using Flappy Bird and a newly developed stock trading environment as case studies. We evaluate various structures of Deep Q-learning networks and identify the most suitable variant for the stochastic process environment. Additionally, we discuss the current challenges and propose potential improvements for further work in environment-building and reinforcement learning techniques.
</details>
<details>
<summary>摘要</summary>
“增强学习”是一种强大的方法，用于训练在给定系统中的优化策略，解决复杂的问题。这个项目的目标是通过使用“扩展 Deep Q-学习”网络和新开发的股票交易环境作为案例研究，在偏振过程环境中应用增强学习。我们评估了不同结构的 Deep Q-学习网络，并确定最适合偏振过程环境的变体。此外，我们还讨论了当前的挑战和可能的改进方法，以便进一步推进环境建设和增强学习技术。
</details></li>
</ul>
<hr>
<h2 id="Symmetry-Preserving-Program-Representations-for-Learning-Code-Semantics"><a href="#Symmetry-Preserving-Program-Representations-for-Learning-Code-Semantics" class="headerlink" title="Symmetry-Preserving Program Representations for Learning Code Semantics"></a>Symmetry-Preserving Program Representations for Learning Code Semantics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03312">http://arxiv.org/abs/2308.03312</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kexin Pei, Weichen Li, Qirui Jin, Shuyang Liu, Scott Geng, Lorenzo Cavallaro, Junfeng Yang, Suman Jana</li>
<li>for: 本研究旨在提高自动化程序理解的能力，尤其是安全任务中的核心问题。</li>
<li>methods: 我们Draw inspiration from examples of convolution layers exploiting translation symmetry，探讨如何使用代码 симметрии提高 LL M 架构。我们提出了一种正式的群理论框架，准确地定义代码 симметрии为 semantics-preserving 变换，并提供了 precisione reasoning 技术来保证在 LL M 架构中Symmetry preservation。</li>
<li>results: 我们 introduce a novel variant of self-attention that preserves program symmetries，并通过详细的实验评估，证明其在泛化和Robustness 方面的效果。总的来说，我们的代码 Symmetry 框架提供了正式和有力的理由技术，可以导向将来的特циалиzed LL M 的发展，并推动 LL M 驱动的程序理解任务的进步。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have shown promise in automated program reasoning, a crucial aspect of many security tasks. However, existing LLM architectures for code are often borrowed from other domains like natural language processing, raising concerns about their generalization and robustness to unseen code. A key generalization challenge is to incorporate the knowledge of code semantics, including control and data flow, into the LLM architectures.   Drawing inspiration from examples of convolution layers exploiting translation symmetry, we explore how code symmetries can enhance LLM architectures for program analysis and modeling. We present a rigorous group-theoretic framework that formally defines code symmetries as semantics-preserving transformations and provides techniques for precisely reasoning about symmetry preservation within LLM architectures. Using this framework, we introduce a novel variant of self-attention that preserves program symmetries, demonstrating its effectiveness in generalization and robustness through detailed experimental evaluations across different binary and source code analysis tasks. Overall, our code symmetry framework offers rigorous and powerful reasoning techniques that can guide the future development of specialized LLMs for code and advance LLM-guided program reasoning tasks.
</details>
<details>
<summary>摘要</summary>
Inspired by the use of convolution layers that exploit translation symmetry, we explore how code symmetries can enhance LLM architectures for program analysis and modeling. We provide a rigorous group-theoretic framework that defines code symmetries as semantics-preserving transformations and provides techniques for precisely reasoning about symmetry preservation within LLM architectures.Using this framework, we introduce a novel variant of self-attention that preserves program symmetries, which we demonstrate to be effective in terms of generalization and robustness through detailed experimental evaluations across different binary and source code analysis tasks. Overall, our code symmetry framework offers rigorous and powerful reasoning techniques that can guide the future development of specialized LLMs for code and advance LLM-guided program reasoning tasks.
</details></li>
</ul>
<hr>
<h2 id="Implicit-Graph-Neural-Diffusion-Based-on-Constrained-Dirichlet-Energy-Minimization"><a href="#Implicit-Graph-Neural-Diffusion-Based-on-Constrained-Dirichlet-Energy-Minimization" class="headerlink" title="Implicit Graph Neural Diffusion Based on Constrained Dirichlet Energy Minimization"></a>Implicit Graph Neural Diffusion Based on Constrained Dirichlet Energy Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03306">http://arxiv.org/abs/2308.03306</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guoji Fu, Mohammed Haroon Dupty, Yanfei Dong, Lee Wee Sun</li>
<li>for: This paper aims to address the issues of over-smoothing and limited adaptability in implicit graph neural networks (GNNs) by introducing a geometric framework for designing implicit graph diffusion layers.</li>
<li>methods: The paper proposes a parameterized graph Laplacian operator to learn the geometry of vertex and edge spaces, as well as the graph gradient operator from data. The implicit graph diffusion layer is viewed as the fixed-point solution of a Dirichlet energy minimization problem, and the authors design a solution with constraints on vertex features to trade off smoothing with the preservation of node feature information.</li>
<li>results: The paper demonstrates better performance than leading implicit and explicit GNNs on benchmark datasets for node and graph classification tasks, with substantial accuracy improvements observed for some datasets.<details>
<summary>Abstract</summary>
Implicit graph neural networks (GNNs) have emerged as a potential approach to enable GNNs to capture long-range dependencies effectively. However, poorly designed implicit GNN layers can experience over-smoothing or may have limited adaptability to learn data geometry, potentially hindering their performance in graph learning problems. To address these issues, we introduce a geometric framework to design implicit graph diffusion layers based on a parameterized graph Laplacian operator. Our framework allows learning the geometry of vertex and edge spaces, as well as the graph gradient operator from data. We further show how implicit GNN layers can be viewed as the fixed-point solution of a Dirichlet energy minimization problem and give conditions under which it may suffer from over-smoothing. To overcome the over-smoothing problem, we design our implicit graph diffusion layer as the solution of a Dirichlet energy minimization problem with constraints on vertex features, enabling it to trade off smoothing with the preservation of node feature information. With an appropriate hyperparameter set to be larger than the largest eigenvalue of the parameterized graph Laplacian, our framework guarantees a unique equilibrium and quick convergence. Our models demonstrate better performance than leading implicit and explicit GNNs on benchmark datasets for node and graph classification tasks, with substantial accuracy improvements observed for some datasets.
</details>
<details>
<summary>摘要</summary>
匿名图 neural networks (GNNs) 已经出现为一种可能的方法，以便 GNNs 可以有效地捕捉长距离依赖关系。然而，如果设计不当的匿名 GNN 层，可能会导致过滤或有限适应性，从而妨碍它们在图学习问题中表现。为了解决这些问题，我们提出了一个几何框架，用于设计基于参数化图拉普拉斯运算符的匿名图扩散层。我们的框架允许学习顶点和边空间的几何结构，以及图的梯度运算符从数据中学习。此外，我们还证明了匿名 GNN 层可以视为 Dirichlet 能量最小化问题的固定点解，并给出了避免过滤的条件。为了超越过滤问题，我们设计了一种基于顶点特征的 Dirichlet 能量最小化问题的约束，使得匿名图扩散层能够平衡平滑与保留顶点特征信息之间的权衡。在适当的超参数设置为大于最大 eigenvalues of 参数化图拉普拉斯运算符时，我们的框架保证唯一的平衡点和快速收敛。我们的模型在 benchmark 数据集上 для 节点和图分类任务中表现出色，与领先的匿名 GNN 和Explicit GNN 相比，具有显著的准确率提高。
</details></li>
</ul>
<hr>
<h2 id="Do-You-Remember-Overcoming-Catastrophic-Forgetting-for-Fake-Audio-Detection"><a href="#Do-You-Remember-Overcoming-Catastrophic-Forgetting-for-Fake-Audio-Detection" class="headerlink" title="Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection"></a>Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03300">http://arxiv.org/abs/2308.03300</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cecile-hi/regularized-adaptive-weight-modification">https://github.com/cecile-hi/regularized-adaptive-weight-modification</a></li>
<li>paper_authors: Xiaohui Zhang, Jiangyan Yi, Jianhua Tao, Chenglong Wang, Chuyuan Zhang</li>
<li>for: 这个论文的目的是解决伪音标注检测算法在不同数据集上表现下降的问题。</li>
<li>methods: 我们提出了一种持续学习算法，叫做Regularized Adaptive Weight Modification（RAWM），可以避免伪阳性检测算法中的溃败性忘记。当调整检测网络时，我们的方法会根据伪音和真音的比例进行适应的 modificaitondirection。</li>
<li>results: 我们在多个数据集上进行了跨数据集实验，结果表明我们的方法可以提高伪音标注检测的表现。另外，我们还引入了一个规化因素，以保持网络对于不同的音响环境中的真音标注的记忆。<details>
<summary>Abstract</summary>
Current fake audio detection algorithms have achieved promising performances on most datasets. However, their performance may be significantly degraded when dealing with audio of a different dataset. The orthogonal weight modification to overcome catastrophic forgetting does not consider the similarity of genuine audio across different datasets. To overcome this limitation, we propose a continual learning algorithm for fake audio detection to overcome catastrophic forgetting, called Regularized Adaptive Weight Modification (RAWM). When fine-tuning a detection network, our approach adaptively computes the direction of weight modification according to the ratio of genuine utterances and fake utterances. The adaptive modification direction ensures the network can effectively detect fake audio on the new dataset while preserving its knowledge of old model, thus mitigating catastrophic forgetting. In addition, genuine audio collected from quite different acoustic conditions may skew their feature distribution, so we introduce a regularization constraint to force the network to remember the old distribution in this regard. Our method can easily be generalized to related fields, like speech emotion recognition. We also evaluate our approach across multiple datasets and obtain a significant performance improvement on cross-dataset experiments.
</details>
<details>
<summary>摘要</summary>
When fine-tuning a detection network, our approach adaptively computes the direction of weight modification based on the ratio of genuine utterances and fake utterances. This ensures that the network can effectively detect fake audio on the new dataset while preserving its knowledge of the old model, thus mitigating catastrophic forgetting.In addition, we introduce a regularization constraint to force the network to remember the old distribution of genuine audio in terms of feature distribution, even when faced with new audio from quite different acoustic conditions. Our method can easily be applied to related fields such as speech emotion recognition.We evaluate our approach across multiple datasets and observe a significant improvement in performance on cross-dataset experiments.
</details></li>
</ul>
<hr>
<h2 id="Studying-Large-Language-Model-Generalization-with-Influence-Functions"><a href="#Studying-Large-Language-Model-Generalization-with-Influence-Functions" class="headerlink" title="Studying Large Language Model Generalization with Influence Functions"></a>Studying Large Language Model Generalization with Influence Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03296">http://arxiv.org/abs/2308.03296</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Roger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner, Dustin Li, Esin Durmus, Ethan Perez, Evan Hubinger, Kamilė Lukošiūtė, Karina Nguyen, Nicholas Joseph, Sam McCandlish, Jared Kaplan, Samuel R. Bowman</li>
<li>for: 了解和 mitigate Machine Learning 模型中关联的风险</li>
<li>methods: 使用 Influence Functions 来回答一个 counterfactual：如果一个序列被添加到训练集中，如何改变模型的参数和输出？</li>
<li>results: 使用 Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) 方法可以在大型语言模型 (LLMs) 中扩展 Influence Functions，并且可以在几乎实时内计算 inverse-Hessian-vector product (IHVP)。我们的实验表明，EK-FAC 可以达到类似于传统的 Influence Functions 估计器的准确性，即使 IHVP 计算在下面的许多 órders of magnitude 快。<details>
<summary>Abstract</summary>
When trying to gain better visibility into a machine learning model in order to understand and mitigate the associated risks, a potentially valuable source of evidence is: which training examples most contribute to a given behavior? Influence functions aim to answer a counterfactual: how would the model's parameters (and hence its outputs) change if a given sequence were added to the training set? While influence functions have produced insights for small models, they are difficult to scale to large language models (LLMs) due to the difficulty of computing an inverse-Hessian-vector product (IHVP). We use the Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) approximation to scale influence functions up to LLMs with up to 52 billion parameters. In our experiments, EK-FAC achieves similar accuracy to traditional influence function estimators despite the IHVP computation being orders of magnitude faster. We investigate two algorithmic techniques to reduce the cost of computing gradients of candidate training sequences: TF-IDF filtering and query batching. We use influence functions to investigate the generalization patterns of LLMs, including the sparsity of the influence patterns, increasing abstraction with scale, math and programming abilities, cross-lingual generalization, and role-playing behavior. Despite many apparently sophisticated forms of generalization, we identify a surprising limitation: influences decay to near-zero when the order of key phrases is flipped. Overall, influence functions give us a powerful new tool for studying the generalization properties of LLMs.
</details>
<details>
<summary>摘要</summary>
当尝试更好地了解一个机器学习模型以便理解和避免相关风险时，一个有价值的证据来源是：哪些训练示例最大程度地对模型的行为做出贡献？影响函数的目的是回答一个Counterfactual问题：如果给定的序列添加到训练集中， THEN 模型的参数（以及其输出）如何改变？虽然影响函数已经生成了一些启示，但是它们难以扩展到大型自然语言模型（LLM），因为计算 inverse-Hessian-vector product（IHVP）的困难。我们使用Eigenvalue-corrected Kronecker-Factored Approximate Curvature（EK-FAC）的方法来扩展影响函数到 LLM 中，并在520亿参数下实现了类似的准确率。我们运行了两种算法技术来减少计算候选训练序列的梯度的成本：TF-IDF 筛选和查询批处理。我们使用影响函数来调查大语言模型的泛化模式，包括泛化 Patterns的稀缺性、逐渐增加的抽象级别、数学和编程能力、跨语言泛化和角色扮演行为。尽管有很多复杂的泛化形式，但我们发现一个Surprising limitation：影响的 decay 到 near-zero 当键phrase 的顺序被反转。总之，影响函数为我们研究大语言模型的泛化性质提供了一个强大的新工具。
</details></li>
</ul>
<hr>
<h2 id="DOMINO-Domain-invariant-Hyperdimensional-Classification-for-Multi-Sensor-Time-Series-Data"><a href="#DOMINO-Domain-invariant-Hyperdimensional-Classification-for-Multi-Sensor-Time-Series-Data" class="headerlink" title="DOMINO: Domain-invariant Hyperdimensional Classification for Multi-Sensor Time Series Data"></a>DOMINO: Domain-invariant Hyperdimensional Classification for Multi-Sensor Time Series Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03295">http://arxiv.org/abs/2308.03295</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyao Wang, Luke Chen, Mohammad Abdullah Al Faruque</li>
<li>for: 这个研究是为了解决智能网络的资料驱动机器学学习方法中的分布偏移问题。</li>
<li>methods: 这个研究使用了脑海算法（HDC）来解决分布偏移问题，并且提出了一个名为DOMINO的新的学习框架。</li>
<li>results: 这个研究的结果显示，DOMINO比前一代的预测方法高出2.04%的精度，并且在训练和测试过程中比前一代的预测方法快得多了16.34倍和2.89倍。此外，DOMINO在部分标签和高度不均的资料上进行学习时表现特别出色，与硬件噪音相比，DOMINO的Robustness提高了10.93倍。<details>
<summary>Abstract</summary>
With the rapid evolution of the Internet of Things, many real-world applications utilize heterogeneously connected sensors to capture time-series information. Edge-based machine learning (ML) methodologies are often employed to analyze locally collected data. However, a fundamental issue across data-driven ML approaches is distribution shift. It occurs when a model is deployed on a data distribution different from what it was trained on, and can substantially degrade model performance. Additionally, increasingly sophisticated deep neural networks (DNNs) have been proposed to capture spatial and temporal dependencies in multi-sensor time series data, requiring intensive computational resources beyond the capacity of today's edge devices. While brain-inspired hyperdimensional computing (HDC) has been introduced as a lightweight solution for edge-based learning, existing HDCs are also vulnerable to the distribution shift challenge. In this paper, we propose DOMINO, a novel HDC learning framework addressing the distribution shift problem in noisy multi-sensor time-series data. DOMINO leverages efficient and parallel matrix operations on high-dimensional space to dynamically identify and filter out domain-variant dimensions. Our evaluation on a wide range of multi-sensor time series classification tasks shows that DOMINO achieves on average 2.04% higher accuracy than state-of-the-art (SOTA) DNN-based domain generalization techniques, and delivers 16.34x faster training and 2.89x faster inference. More importantly, DOMINO performs notably better when learning from partially labeled and highly imbalanced data, providing 10.93x higher robustness against hardware noises than SOTA DNNs.
</details>
<details>
<summary>摘要</summary>
With the rapid evolution of the Internet of Things, many real-world applications utilize heterogeneously connected sensors to capture time-series information. Edge-based machine learning (ML) methodologies are often employed to analyze locally collected data. However, a fundamental issue across data-driven ML approaches is distribution shift. It occurs when a model is deployed on a data distribution different from what it was trained on, and can substantially degrade model performance. Additionally, increasingly sophisticated deep neural networks (DNNs) have been proposed to capture spatial and temporal dependencies in multi-sensor time series data, requiring intensive computational resources beyond the capacity of today's edge devices. While brain-inspired hyperdimensional computing (HDC) has been introduced as a lightweight solution for edge-based learning, existing HDCs are also vulnerable to the distribution shift challenge. In this paper, we propose DOMINO, a novel HDC learning framework addressing the distribution shift problem in noisy multi-sensor time-series data. DOMINO leverages efficient and parallel matrix operations on high-dimensional space to dynamically identify and filter out domain-variant dimensions. Our evaluation on a wide range of multi-sensor time series classification tasks shows that DOMINO achieves on average 2.04% higher accuracy than state-of-the-art (SOTA) DNN-based domain generalization techniques, and delivers 16.34x faster training and 2.89x faster inference. More importantly, DOMINO performs notably better when learning from partially labeled and highly imbalanced data, providing 10.93x higher robustness against hardware noises than SOTA DNNs.
</details></li>
</ul>
<hr>
<h2 id="SynJax-Structured-Probability-Distributions-for-JAX"><a href="#SynJax-Structured-Probability-Distributions-for-JAX" class="headerlink" title="SynJax: Structured Probability Distributions for JAX"></a>SynJax: Structured Probability Distributions for JAX</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03291">http://arxiv.org/abs/2308.03291</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/deepmind/synjax">https://github.com/deepmind/synjax</a></li>
<li>paper_authors: Miloš Stanojević, Laurent Sartran</li>
<li>for: 这个论文是为了提高深度学习模型中的结构化对象处理而写的。</li>
<li>methods: 这篇论文使用的方法是为了提供高效的 вектор化实现归一化算法，以便在现代硬件加速器上实现大规模可导模型。</li>
<li>results: 这篇论文通过SynJax库实现了大规模可导模型，并且可以高效地处理结构化对象，如树和分割。<details>
<summary>Abstract</summary>
The development of deep learning software libraries enabled significant progress in the field by allowing users to focus on modeling, while letting the library to take care of the tedious and time-consuming task of optimizing execution for modern hardware accelerators. However, this has benefited only particular types of deep learning models, such as Transformers, whose primitives map easily to the vectorized computation. The models that explicitly account for structured objects, such as trees and segmentations, did not benefit equally because they require custom algorithms that are difficult to implement in a vectorized form.   SynJax directly addresses this problem by providing an efficient vectorized implementation of inference algorithms for structured distributions covering alignment, tagging, segmentation, constituency trees and spanning trees. With SynJax we can build large-scale differentiable models that explicitly model structure in the data. The code is available at https://github.com/deepmind/synjax.
</details>
<details>
<summary>摘要</summary>
深度学习软件库的发展允许用户专注于模型设计，让库负责处理现代硬件加速器的繁琐和耗时任务。然而，这只有某些深度学习模型，如转换器，得到了利用。这些模型的基本 primitives 可以直接映射到 вектор化计算。不同的模型，如树和分割，因为它们需要特定的算法，很难以在 вектор化形式下实现。SynJax 直接解决了这个问题，提供了高效的 вектор化实现方式，用于推理算法，包括对适配、标记、分割、树和 span 的支持。通过 SynJax，我们可以构建大规模的可导 diferenciable 模型，直接模型数据中的结构。代码可以在 GitHub 上找到：https://github.com/deepmind/synjax。
</details></li>
</ul>
<hr>
<h2 id="FLIQS-One-Shot-Mixed-Precision-Floating-Point-and-Integer-Quantization-Search"><a href="#FLIQS-One-Shot-Mixed-Precision-Floating-Point-and-Integer-Quantization-Search" class="headerlink" title="FLIQS: One-Shot Mixed-Precision Floating-Point and Integer Quantization Search"></a>FLIQS: One-Shot Mixed-Precision Floating-Point and Integer Quantization Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03290">http://arxiv.org/abs/2308.03290</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jordan Dotzel, Gang Wu, Andrew Li, Muhammad Umar, Yun Ni, Mohamed S. Abdelfattah, Zhiru Zhang, Liqun Cheng, Martin G. Dixon, Norman P. Jouppi, Quoc V. Le, Sheng Li</li>
<li>for: 这个研究旨在提出一种一击混合精度搜寻方法，以实现高品质且低成本的模型。</li>
<li>methods: 这个方法使用混合精度搜寻，将数据分成integer和low-precision floating point两部分，并对它们进行一击搜寻。</li>
<li>results: 这个方法可以实现高品质的模型，并且比先前的方法更高的精度和更低的成本。在ImageNet上，这个方法可以提高ResNet-18的精度 by 1.31% points和ResNet-50的精度 by 0.90% points，而且与先前的方法相比，这个方法的模型成本相同。此外，这个方法还可以对MobileNetV2进行改进，实现更高的精度和更低的成本。最后，这个方法还可以同时搜寻模型的架构和量化空间，实现ImageNet的精度提高 by 2.69% points，并且与相似的模型成本。<details>
<summary>Abstract</summary>
Quantization has become a mainstream compression technique for reducing model size, computational requirements, and energy consumption for modern deep neural networks (DNNs). With the improved numerical support in recent hardware, including multiple variants of integer and floating point, mixed-precision quantization has become necessary to achieve high-quality results with low model cost. Prior mixed-precision quantization methods have performed a post-training quantization search, which compromises on accuracy, or a differentiable quantization search, which leads to high memory usage from branching. Therefore, we propose the first one-shot mixed-precision quantization search that eliminates the need for retraining in both integer and low-precision floating point models. We evaluate our floating-point and integer quantization search (FLIQS) on multiple convolutional networks and vision transformer models to discover Pareto-optimal models. Our approach discovers models that improve upon uniform precision, manual mixed-precision, and recent integer quantization search methods. With the proposed integer quantization search, we increase the accuracy of ResNet-18 on ImageNet by 1.31% points and ResNet-50 by 0.90% points with equivalent model cost over previous methods. Additionally, for the first time, we explore a novel mixed-precision floating-point search and improve MobileNetV2 by up to 0.98% points compared to prior state-of-the-art FP8 models. Finally, we extend FLIQS to simultaneously search a joint quantization and neural architecture space and improve the ImageNet accuracy by 2.69% points with similar model cost on a MobileNetV2 search space.
</details>
<details>
<summary>摘要</summary>
快照量化已成为现代深度神经网络（DNN）的主流压缩技术，以降低模型大小、计算需求和能耗。随着当前硬件的改进 numerical support，包括多种整数和浮点数的多种变体，杂音精度量化已成为实现高质量结果的低成本模型的必要手段。先前的杂音精度量化方法通常会进行训练后量化搜索，这会妥协准确性，或者使用可导量化搜索，这会导致高内存使用率。因此，我们提出了首次一shot杂音精度量化搜索，这将消除重新训练的需要，并在整数和低精度浮点数模型中实现高质量结果。我们对多个卷积神经网络和视Transformer模型进行了评估，并发现了Pareto优质模型。我们的方法可以提高ResNet-18在ImageNet上的准确率by 1.31%点和ResNet-50上的准确率by 0.90%点，与先前方法相当。此外，我们首次探索了一种新的杂音精度浮点数搜索，并提高了MobileNetV2的性能，相比先前的FP8模型。最后，我们将FLIQS扩展到同时搜索一个量化和神经网络架构的空间，并在MobileNetV2搜索空间上提高ImageNet的准确率by 2.69%点，与相同的模型成本相似。
</details></li>
</ul>
<hr>
<h2 id="High-rate-discretely-modulated-continuous-variable-quantum-key-distribution-using-quantum-machine-learning"><a href="#High-rate-discretely-modulated-continuous-variable-quantum-key-distribution-using-quantum-machine-learning" class="headerlink" title="High-rate discretely-modulated continuous-variable quantum key distribution using quantum machine learning"></a>High-rate discretely-modulated continuous-variable quantum key distribution using quantum machine learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03283">http://arxiv.org/abs/2308.03283</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qin Liao, Jieyu Liu, Anqi Huang, Lei Huang, Zhuoying Fei, Xiquan Fu</li>
<li>for: 该研究旨在提出一种高速率的 continuous-variable quantum key distribution（CVQKD）分类器，用于提高CVQKD系统的安全性和效率。</li>
<li>methods: 该研究使用了量子机器学习技术，将CVQKD系统分为三部分：初始化部分，预测部分和数据处理部分。在预测部分，使用了低复杂度的量子k-最近邻（QkNN）分类器，用于预测Bob方向的失去拟合干扰 states。</li>
<li>results: 研究发现，提出的QkNN-based CVQKD具有较高的安全性和效率，并且可以通过增加拟合干扰的幅度来进一步提高秘密密钥率。数值仿真结果表明，相比现有的DM CVQKD协议，该方案的秘密密钥率明显高于现有的协议。<details>
<summary>Abstract</summary>
We propose a high-rate scheme for discretely-modulated continuous-variable quantum key distribution (DM CVQKD) using quantum machine learning technologies, which divides the whole CVQKD system into three parts, i.e., the initialization part that is used for training and estimating quantum classifier, the prediction part that is used for generating highly correlated raw keys, and the data-postprocessing part that generates the final secret key string shared by Alice and Bob. To this end, a low-complexity quantum k-nearest neighbor (QkNN) classifier is designed for predicting the lossy discretely-modulated coherent states (DMCSs) at Bob's side. The performance of the proposed QkNN-based CVQKD especially in terms of machine learning metrics and complexity is analyzed, and its theoretical security is proved by using semi-definite program (SDP) method. Numerical simulation shows that the secret key rate of our proposed scheme is explicitly superior to the existing DM CVQKD protocols, and it can be further enhanced with the increase of modulation variance.
</details>
<details>
<summary>摘要</summary>
我们提出了一种高率方案 для离散Modulated kontinuierliche variable Quantum Key Distribution（DM CVQKD），使用量子机器学习技术，将整个CVQKD系统分成三部分：初始化部分用于训练和估计量子分类器，预测部分用于生成高度相关的Raw密钥，以及数据处理部分用于生成最终由Alice和Bob共享的密钥串。为此，我们设计了一种低复杂度的量子k-最近邻（QkNN）分类器，用于预测Bob方面的失去离散Modulated coherent states（DMCSs）。我们分析了提议的QkNN-based CVQKD的性能，包括机器学习指标和复杂度，并使用半definite Program（SDP）方法证明其理论安全性。numerical simulation表明，我们提议的方案的秘密密钥率明显高于现有的DM CVQKD协议，并可以通过增加模拟幅度进一步提高。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Distilled-Ensemble-Model-for-sEMG-based-Silent-Speech-Interface"><a href="#Knowledge-Distilled-Ensemble-Model-for-sEMG-based-Silent-Speech-Interface" class="headerlink" title="Knowledge Distilled Ensemble Model for sEMG-based Silent Speech Interface"></a>Knowledge Distilled Ensemble Model for sEMG-based Silent Speech Interface</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06533">http://arxiv.org/abs/2308.06533</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenqiang Lai, Qihan Yang, Ye Mao, Endong Sun, Jiangnan Ye</li>
<li>for: 这个论文是为了解决语音疾病的问题而写的。</li>
<li>methods: 这个论文使用了深度学习知识压缩组合模型（KDE-SSI）来解决surface electromyography-based Silent Speech Interface（sEMG-based SSI）的限制。</li>
<li>results: 该模型可以在26个NATO音素字符集中正确地类别3900个数据样本，允许不ambiguous地生成任何英语单词。测试准确率达85.9%。<details>
<summary>Abstract</summary>
Voice disorders affect millions of people worldwide. Surface electromyography-based Silent Speech Interfaces (sEMG-based SSIs) have been explored as a potential solution for decades. However, previous works were limited by small vocabularies and manually extracted features from raw data. To address these limitations, we propose a lightweight deep learning knowledge-distilled ensemble model for sEMG-based SSI (KDE-SSI). Our model can classify a 26 NATO phonetic alphabets dataset with 3900 data samples, enabling the unambiguous generation of any English word through spelling. Extensive experiments validate the effectiveness of KDE-SSI, achieving a test accuracy of 85.9\%. Our findings also shed light on an end-to-end system for portable, practical equipment.
</details>
<details>
<summary>摘要</summary>
声音疾病影响全球数百万人。基于表面电 MYography（sEMG）的无声朗读界面（SSI）已经被研究了几十年。然而，之前的工作受到小词汇和从原始数据手动提取的特征所限制。为了解决这些限制，我们提议一种轻量级深度学习知识填充ensemble模型 для sEMG-based SSI（KDE-SSI）。我们的模型可以 классифицировать一个 NATO phonetic alphabet dataset，包含 3900 个数据样本，允许不 ambiguous 地生成任何英语单词 through spelling。广泛的实验证明了 KDE-SSI 的效果，实现了 85.9% 的测试精度。我们的发现还 shed light on 一个端到端系统 for portable, practical equipment.
</details></li>
</ul>
<hr>
<h2 id="DSformer-A-Double-Sampling-Transformer-for-Multivariate-Time-Series-Long-term-Prediction"><a href="#DSformer-A-Double-Sampling-Transformer-for-Multivariate-Time-Series-Long-term-Prediction" class="headerlink" title="DSformer: A Double Sampling Transformer for Multivariate Time Series Long-term Prediction"></a>DSformer: A Double Sampling Transformer for Multivariate Time Series Long-term Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03274">http://arxiv.org/abs/2308.03274</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengqing Yu, Fei Wang, Zezhi Shao, Tao Sun, Lin Wu, Yongjun Xu</li>
<li>for: 预测多变量时间序列长期变化，提供决策参考。</li>
<li>methods: 提议使用双重采样变换器（DSformer），包括双重采样（DS）块和时间变量注意（TVA）块。DS块使用下采样和分割采样将原始序列转换为具有全球信息和本地信息注意的特征向量。然后，TVA块使用时间注意和变量注意来挖掘这些特征向量的不同维度信息，并提取关键信息。</li>
<li>results: 实验结果表明，DSformer可以在九个真实世界数据集上超过八个基elines。<details>
<summary>Abstract</summary>
Multivariate time series long-term prediction, which aims to predict the change of data in a long time, can provide references for decision-making. Although transformer-based models have made progress in this field, they usually do not make full use of three features of multivariate time series: global information, local information, and variables correlation. To effectively mine the above three features and establish a high-precision prediction model, we propose a double sampling transformer (DSformer), which consists of the double sampling (DS) block and the temporal variable attention (TVA) block. Firstly, the DS block employs down sampling and piecewise sampling to transform the original series into feature vectors that focus on global information and local information respectively. Then, TVA block uses temporal attention and variable attention to mine these feature vectors from different dimensions and extract key information. Finally, based on a parallel structure, DSformer uses multiple TVA blocks to mine and integrate different features obtained from DS blocks respectively. The integrated feature information is passed to the generative decoder based on a multi-layer perceptron to realize multivariate time series long-term prediction. Experimental results on nine real-world datasets show that DSformer can outperform eight existing baselines.
</details>
<details>
<summary>摘要</summary>
多变量时间序列长期预测，目的是预测数据在长期内的变化，可以提供决策参考。虽然基于转换器模型在这个领域已经取得了进步，但它们通常不充分利用多变量时间序列的三个特征：全局信息、本地信息和变量相关性。为了有效利用这些特征并建立高精度预测模型，我们提议了双重采样变换器（DSformer）。DSformer包括双重采样（DS）块和时间变量注意（TVA）块。首先，DS块使用下采样和分割采样将原始序列转化为特征向量，其中专注于全局信息和本地信息。然后，TVA块使用时间注意和变量注意来挖掘这些特征向量从不同维度，提取关键信息。最后，基于并行结构，DSformer使用多个TVA块来挖掘和集成不同维度的特征信息，并将其传递给基于多层感知机器的生成解码器，实现多变量时间序列长期预测。实验结果表明，DSformer可以在九个真实世界数据集上超过八个基准值。
</details></li>
</ul>
<hr>
<h2 id="Local-Structure-aware-Graph-Contrastive-Representation-Learning"><a href="#Local-Structure-aware-Graph-Contrastive-Representation-Learning" class="headerlink" title="Local Structure-aware Graph Contrastive Representation Learning"></a>Local Structure-aware Graph Contrastive Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03271">http://arxiv.org/abs/2308.03271</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kai Yang, Yuan Liu, Zijuan Zhao, Peijin Ding, Wenqian Zhao</li>
<li>for: 本研究提出了一种Local Structure-aware Graph Contrastive representation Learning方法（LS-GCL），用于模型节点的多视图结构信息。</li>
<li>methods: 本方法首先构建了各个目标节点的含义子图，不限于首领相邻节点。然后，对每个目标节点，使用共享GNN编码器获取子图级别节点嵌入。最后，使用pooling函数生成子图级别图像嵌入。</li>
<li>results: 对五个数据集进行实验，结果表明， compared to现有图像学习方法，LS-GCL方法在节点分类和链接预测任务中表现出色。<details>
<summary>Abstract</summary>
Traditional Graph Neural Network (GNN), as a graph representation learning method, is constrained by label information. However, Graph Contrastive Learning (GCL) methods, which tackle the label problem effectively, mainly focus on the feature information of the global graph or small subgraph structure (e.g., the first-order neighborhood). In the paper, we propose a Local Structure-aware Graph Contrastive representation Learning method (LS-GCL) to model the structural information of nodes from multiple views. Specifically, we construct the semantic subgraphs that are not limited to the first-order neighbors. For the local view, the semantic subgraph of each target node is input into a shared GNN encoder to obtain the target node embeddings at the subgraph-level. Then, we use a pooling function to generate the subgraph-level graph embeddings. For the global view, considering the original graph preserves indispensable semantic information of nodes, we leverage the shared GNN encoder to learn the target node embeddings at the global graph-level. The proposed LS-GCL model is optimized to maximize the common information among similar instances at three various perspectives through a multi-level contrastive loss function. Experimental results on five datasets illustrate that our method outperforms state-of-the-art graph representation learning approaches for both node classification and link prediction tasks.
</details>
<details>
<summary>摘要</summary>
传统的图形神经网络（GNN）在图像学习中受标签信息的限制。然而，图像对比学习（GCL）方法，它们可以有效地解决标签问题，主要集中在全图或小部分图结构（例如，第一邻居）的特征信息上。在文章中，我们提出了一种本地结构意识 graph contrastive representation learning方法（LS-GCL），用于模型节点的多视图结构信息。具体来说，我们构建了不限于第一邻居的语义子图。对于本地视图，每个目标节点的语义子图输入到共享GNNEncoder中，以获得目标节点的子图级别嵌入。然后，我们使用一种池化函数生成子图级别图像嵌入。对于全球视图，因为原始图保留了节点的不可或缺semantic信息，我们利用共享GNNEncoder来学习目标节点的全图级别嵌入。我们提出的LS-GCL模型通过最大化三个不同视点上相似实例的共同信息来优化多级对比损失函数。实验结果表明，我们的方法在五个 dataset上较前state-of-the-art图形学习方法出色地进行节点类别和连接预测任务。
</details></li>
</ul>
<hr>
<h2 id="Simple-Rule-Injection-for-ComplEx-Embeddings"><a href="#Simple-Rule-Injection-for-ComplEx-Embeddings" class="headerlink" title="Simple Rule Injection for ComplEx Embeddings"></a>Simple Rule Injection for ComplEx Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03269">http://arxiv.org/abs/2308.03269</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haodi Ma, Anthony Colas, Yuejie Wang, Ali Sadeghian, Daisy Zhe Wang</li>
<li>for: 本研究旨在结合逻辑规则和知识图embedding以获得优化的知识图推理结果。</li>
<li>methods: 本研究提出了一种名为InjEx的机制，可以通过简单的约束来插入多种逻辑规则，以捕捉Definite Horn规则。</li>
<li>results: 实验结果表明，InjEx可以在知识图完成(KGC)和少量shot知识图完成(FKGC)任务中具有更高的性能和可扩展性，并且可以带来更加可读性的知识图 Representation。<details>
<summary>Abstract</summary>
Recent works in neural knowledge graph inference attempt to combine logic rules with knowledge graph embeddings to benefit from prior knowledge. However, they usually cannot avoid rule grounding, and injecting a diverse set of rules has still not been thoroughly explored. In this work, we propose InjEx, a mechanism to inject multiple types of rules through simple constraints, which capture definite Horn rules. To start, we theoretically prove that InjEx can inject such rules. Next, to demonstrate that InjEx infuses interpretable prior knowledge into the embedding space, we evaluate InjEx on both the knowledge graph completion (KGC) and few-shot knowledge graph completion (FKGC) settings. Our experimental results reveal that InjEx outperforms both baseline KGC models as well as specialized few-shot models while maintaining its scalability and efficiency.
</details>
<details>
<summary>摘要</summary>
近期研究在神经知识图推理中尝试将逻辑规则与知识图嵌入结合以获得优势，但通常无法避免规则固化，并尚未全面探讨多种规则的混合。在这种工作中，我们提出了InjEx机制，可以通过简单的约束来注入多种类型的规则，这些规则捕捉了幂等规则。首先，我们理论上证明了InjEx可以注入这些规则。然后，我们通过在知识图完成(KGC)和少量知识图完成(FKGC)设置中评估InjEx，发现InjEx可以充分吸收明确的先验知识，并在缺少数据时保持高效性和扩展性。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Different-Time-series-Transformer-TST-Architectures-A-Case-Study-in-Battery-Life-Prediction-for-Electric-Vehicles-EVs"><a href="#Exploring-Different-Time-series-Transformer-TST-Architectures-A-Case-Study-in-Battery-Life-Prediction-for-Electric-Vehicles-EVs" class="headerlink" title="Exploring Different Time-series-Transformer (TST) Architectures: A Case Study in Battery Life Prediction for Electric Vehicles (EVs)"></a>Exploring Different Time-series-Transformer (TST) Architectures: A Case Study in Battery Life Prediction for Electric Vehicles (EVs)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03260">http://arxiv.org/abs/2308.03260</a></li>
<li>repo_url: None</li>
<li>paper_authors: Niranjan Sitapure, Atharva Kulkarni</li>
<li>for: The paper aims to develop accurate battery life prediction models for electric vehicles (EVs) using a data-driven approach and novel transformer-based architectures.</li>
<li>methods: The paper uses time-series-transformers (TSTs) and long short-term memory (LSTM) models to predict battery state-of-charge (SOC) and temperature in EVs, incorporating environmental, battery, vehicle driving, and heating circuit data.</li>
<li>results: The paper explores and compares novel TST architectures, including encoder TST + decoder LSTM and a hybrid TST-LSTM, to create accurate battery life prediction models for EVs.<details>
<summary>Abstract</summary>
In recent years, battery technology for electric vehicles (EVs) has been a major focus, with a significant emphasis on developing new battery materials and chemistries. However, accurately predicting key battery parameters, such as state-of-charge (SOC) and temperature, remains a challenge for constructing advanced battery management systems (BMS). Existing battery models do not comprehensively cover all parameters affecting battery performance, including non-battery-related factors like ambient temperature, cabin temperature, elevation, and regenerative braking during EV operation. Due to the difficulty of incorporating these auxiliary parameters into traditional models, a data-driven approach is suggested. Time-series-transformers (TSTs), leveraging multiheaded attention and parallelization-friendly architecture, are explored alongside LSTM models. Novel TST architectures, including encoder TST + decoder LSTM and a hybrid TST-LSTM, are also developed and compared against existing models. A dataset comprising 72 driving trips in a BMW i3 (60 Ah) is used to address battery life prediction in EVs, aiming to create accurate TST models that incorporate environmental, battery, vehicle driving, and heating circuit data to predict SOC and battery temperature for future time steps.
</details>
<details>
<summary>摘要</summary>
近年来，电动汽车（EV）的电池技术受到了重点关注，开发新的电池材料和化学组合也受到了重视。然而，正确预测电池参数，如充电状态（SOC）和温度，仍然是构建高级电池管理系统（BMS）的挑战。现有的电池模型没有完全覆盖所有影响电池性能的参数，包括非电池相关因素，如外部温度、车辆温度、海拔和在EV运行时的回生制动。由于将这些辅助参数 incorporated into traditional models 是困难的，一种数据驱动的方法被建议。时序列转换器（TST），利用多头注意力和并行化友好的架构，与LSTM模型一起被探讨。 Novel TST架构，包括编码TST+解码LSTM和混合TST-LSTM，也被开发并与现有模型进行比较。使用了72次开放驱动记录（BMW i3，60 Ah），用于预测EV电池寿命，目标是创建准确的TST模型， incorporating environmental, battery, vehicle driving, and heating circuit data to predict SOC and battery temperature for future time steps。
</details></li>
</ul>
<hr>
<h2 id="Optimal-Approximation-and-Learning-Rates-for-Deep-Convolutional-Neural-Networks"><a href="#Optimal-Approximation-and-Learning-Rates-for-Deep-Convolutional-Neural-Networks" class="headerlink" title="Optimal Approximation and Learning Rates for Deep Convolutional Neural Networks"></a>Optimal Approximation and Learning Rates for Deep Convolutional Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03259">http://arxiv.org/abs/2308.03259</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shao-Bo Lin</li>
<li>for: 这篇论文主要针对深度卷积神经网络的抽象和学习性能分析。</li>
<li>methods: 论文使用了零填充和最大池化来分析深度卷积神经网络的抽象和学习性能。</li>
<li>results: 论文证明了，用于模型$ r $-光滑函数的近似率，深度卷积神经网络的深度$ L $ 的近似率是$ (L^2&#x2F;\log L)^{-2r&#x2F;d} $，这是最佳准确性因子。此外，论文还提出了几乎最佳的学习率来实现深度卷积神经网络上的实际风险函数最小化。<details>
<summary>Abstract</summary>
This paper focuses on approximation and learning performance analysis for deep convolutional neural networks with zero-padding and max-pooling. We prove that, to approximate $r$-smooth function, the approximation rates of deep convolutional neural networks with depth $L$ are of order $ (L^2/\log L)^{-2r/d} $, which is optimal up to a logarithmic factor. Furthermore, we deduce almost optimal learning rates for implementing empirical risk minimization over deep convolutional neural networks.
</details>
<details>
<summary>摘要</summary>
Here's the translation in Simplified Chinese:这篇论文关注深度卷积神经网络（CNN）的近似和学习性能分析，包括零填充和最大池化。我们证明，用于近似$r$-光滑函数的深度$L$的CNN的近似率是$(L^2/\log L)^{-2r/d}$，即优化到对数因子。此外，我们还得出了对深度CNN进行实际风险最小化的几乎最佳学习速率。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Adversarial-Detection-without-Extra-Model-Training-Loss-Should-Change"><a href="#Unsupervised-Adversarial-Detection-without-Extra-Model-Training-Loss-Should-Change" class="headerlink" title="Unsupervised Adversarial Detection without Extra Model: Training Loss Should Change"></a>Unsupervised Adversarial Detection without Extra Model: Training Loss Should Change</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03243">http://arxiv.org/abs/2308.03243</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cyclebooster/unsupervised-adversarial-detection-without-extra-model">https://github.com/cyclebooster/unsupervised-adversarial-detection-without-extra-model</a></li>
<li>paper_authors: Chien Cheng Chyou, Hung-Ting Su, Winston H. Hsu</li>
<li>for: 提高深度学习模型对抗攻击的可靠性</li>
<li>methods: 提出新的训练损失函数和无需依赖于攻击类型的检测方法</li>
<li>results: 检测率高于93.9%，false positive率低于2.5%，在所有攻击类型下都有良好表现<details>
<summary>Abstract</summary>
Adversarial robustness poses a critical challenge in the deployment of deep learning models for real-world applications. Traditional approaches to adversarial training and supervised detection rely on prior knowledge of attack types and access to labeled training data, which is often impractical. Existing unsupervised adversarial detection methods identify whether the target model works properly, but they suffer from bad accuracies owing to the use of common cross-entropy training loss, which relies on unnecessary features and strengthens adversarial attacks. We propose new training losses to reduce useless features and the corresponding detection method without prior knowledge of adversarial attacks. The detection rate (true positive rate) against all given white-box attacks is above 93.9% except for attacks without limits (DF($\infty$)), while the false positive rate is barely 2.5%. The proposed method works well in all tested attack types and the false positive rates are even better than the methods good at certain types.
</details>
<details>
<summary>摘要</summary>
深度学习模型在实际应用中面临严重的敌对Robustness挑战。传统的敌对训练和监督检测方法需要严格的优化目标和训练数据，这并不现实。现有的无监督敌对检测方法可以判断目标模型是否正常工作，但它们因使用共同克服极值损失函数，导致检测精度差。我们提出了新的训练损失函数，以减少无用的特征，并提出了不需要先知 adversarial 攻击的检测方法。对于所有给出的白盒攻击，检测率（真正阳性率）高于93.9%，只有不受限制的攻击（DF($\infty$））的检测率较低， False Positive率只有2.5%。我们的方法在所有攻击类型上都很好，而且对于某些类型的检测率更高。
</details></li>
</ul>
<hr>
<h2 id="Asynchronous-Decentralized-Q-Learning-Two-Timescale-Analysis-By-Persistence"><a href="#Asynchronous-Decentralized-Q-Learning-Two-Timescale-Analysis-By-Persistence" class="headerlink" title="Asynchronous Decentralized Q-Learning: Two Timescale Analysis By Persistence"></a>Asynchronous Decentralized Q-Learning: Two Timescale Analysis By Persistence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03239">http://arxiv.org/abs/2308.03239</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bora Yongacoglu, Gürdal Arslan, Serdar Yüksel</li>
<li>for: 这篇论文主要探讨了多智能体学习（MARL）中的非站点性挑战，以及如何使用不同的方法来解决这个挑战。</li>
<li>methods: 这篇论文使用了 asynchronous variant of the decentralized Q-learning algorithm，并提供了 suficient conditions 以 garantuee that the asynchronous algorithm drives play to equilibrium with high probability。</li>
<li>results: 研究发现，使用constant learning rates在Q-factor更新中是关键的，可以relaxing the synchrony assumptions of earlier work。此外，这种方法还可以应用于 asynchronous generalizations of a number of other algorithms from the regret testing tradition。<details>
<summary>Abstract</summary>
Non-stationarity is a fundamental challenge in multi-agent reinforcement learning (MARL), where agents update their behaviour as they learn. Many theoretical advances in MARL avoid the challenge of non-stationarity by coordinating the policy updates of agents in various ways, including synchronizing times at which agents are allowed to revise their policies. Synchronization enables analysis of many MARL algorithms via multi-timescale methods, but such synchrony is infeasible in many decentralized applications. In this paper, we study an asynchronous variant of the decentralized Q-learning algorithm, a recent MARL algorithm for stochastic games. We provide sufficient conditions under which the asynchronous algorithm drives play to equilibrium with high probability. Our solution utilizes constant learning rates in the Q-factor update, which we show to be critical for relaxing the synchrony assumptions of earlier work. Our analysis also applies to asynchronous generalizations of a number of other algorithms from the regret testing tradition, whose performance is analyzed by multi-timescale methods that study Markov chains obtained via policy update dynamics. This work extends the applicability of the decentralized Q-learning algorithm and its relatives to settings in which parameters are selected in an independent manner, and tames non-stationarity without imposing the coordination assumptions of prior work.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="AdaER-An-Adaptive-Experience-Replay-Approach-for-Continual-Lifelong-Learning"><a href="#AdaER-An-Adaptive-Experience-Replay-Approach-for-Continual-Lifelong-Learning" class="headerlink" title="AdaER: An Adaptive Experience Replay Approach for Continual Lifelong Learning"></a>AdaER: An Adaptive Experience Replay Approach for Continual Lifelong Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03810">http://arxiv.org/abs/2308.03810</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingyu Li, Bo Tang, Haifeng Li</li>
<li>for: 这 paper 的目的是解决机器学习框架中的持续学习问题，即学习者需要不断获得新的知识，但是流动数据的非站点性带来了快速忘记之问题。</li>
<li>methods: 这 paper 使用了一种新的算法叫做adaptive-experience replay（AdaER），它包括两个阶段：记忆重播和记忆更新。在记忆重播阶段，AdaER 使用了一种 Contextually-cued memory recall（C-CMR）策略，选择在当前输入数据和任务上表现最大的冲突的记忆进行重播。此外，AdaER 还包括一种 Entropy-balanced reservoir sampling（E-BRS）策略来增强记忆缓存的性能。</li>
<li>results: 经过实验表明，AdaER 比现有的持续学习基elines表现更高，highlighting its efficacy in mitigating catastrophic forgetting and improving learning performance。<details>
<summary>Abstract</summary>
Continual lifelong learning is an machine learning framework inspired by human learning, where learners are trained to continuously acquire new knowledge in a sequential manner. However, the non-stationary nature of streaming training data poses a significant challenge known as catastrophic forgetting, which refers to the rapid forgetting of previously learned knowledge when new tasks are introduced. While some approaches, such as experience replay (ER), have been proposed to mitigate this issue, their performance remains limited, particularly in the class-incremental scenario which is considered natural and highly challenging. In this paper, we present a novel algorithm, called adaptive-experience replay (AdaER), to address the challenge of continual lifelong learning. AdaER consists of two stages: memory replay and memory update. In the memory replay stage, AdaER introduces a contextually-cued memory recall (C-CMR) strategy, which selectively replays memories that are most conflicting with the current input data in terms of both data and task. Additionally, AdaER incorporates an entropy-balanced reservoir sampling (E-BRS) strategy to enhance the performance of the memory buffer by maximizing information entropy. To evaluate the effectiveness of AdaER, we conduct experiments on established supervised continual lifelong learning benchmarks, specifically focusing on class-incremental learning scenarios. The results demonstrate that AdaER outperforms existing continual lifelong learning baselines, highlighting its efficacy in mitigating catastrophic forgetting and improving learning performance.
</details>
<details>
<summary>摘要</summary>
AdaER包括两个阶段：记忆回顾和记忆更新。在记忆回顾阶段，AdaER使用了Contextually-cued Memory Recall（C-CMR）策略，选择ively回顾与当前输入数据和任务相关的记忆。此外，AdaER还将Entropy-balanced Reservoir Sampling（E-BRS）策略添加到记忆缓存中，以提高记忆缓存的性能，并 Maximizing information entropy。为了评估AdaER的有效性，我们对已有的超vised continual lifelong learning Benchmark进行实验，特别是对维度增量学习scenario。结果显示，AdaER在减少忘却和提高学习性能方面表现出色，较以前的持续性学习基eline高效。
</details></li>
</ul>
<hr>
<h2 id="G-Mix-A-Generalized-Mixup-Learning-Framework-Towards-Flat-Minima"><a href="#G-Mix-A-Generalized-Mixup-Learning-Framework-Towards-Flat-Minima" class="headerlink" title="G-Mix: A Generalized Mixup Learning Framework Towards Flat Minima"></a>G-Mix: A Generalized Mixup Learning Framework Towards Flat Minima</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03236">http://arxiv.org/abs/2308.03236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingyu Li, Bo Tang</li>
<li>for: 提高深度神经网络（DNN）的通用能力，特别是在有限的训练数据available时。</li>
<li>methods: combines Mixup和SAM（Sharpness-Aware Minimization）技术，以提高DNN训练过程中的泛化能力。</li>
<li>results: 提出了两种新算法：Binary G-Mix和Decomposed G-Mix，可以进一步优化DNN性能。实验结果表明，这两种算法可以在多个数据集和模型上提高模型的泛化性能，达到状态 искусственный智能水平。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) have demonstrated promising results in various complex tasks. However, current DNNs encounter challenges with over-parameterization, especially when there is limited training data available. To enhance the generalization capability of DNNs, the Mixup technique has gained popularity. Nevertheless, it still produces suboptimal outcomes. Inspired by the successful Sharpness-Aware Minimization (SAM) approach, which establishes a connection between the sharpness of the training loss landscape and model generalization, we propose a new learning framework called Generalized-Mixup, which combines the strengths of Mixup and SAM for training DNN models. The theoretical analysis provided demonstrates how the developed G-Mix framework enhances generalization. Additionally, to further optimize DNN performance with the G-Mix framework, we introduce two novel algorithms: Binary G-Mix and Decomposed G-Mix. These algorithms partition the training data into two subsets based on the sharpness-sensitivity of each example to address the issue of "manifold intrusion" in Mixup. Both theoretical explanations and experimental results reveal that the proposed BG-Mix and DG-Mix algorithms further enhance model generalization across multiple datasets and models, achieving state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Deep neural networks" is translated as "深度神经网络" (shēn dào shén zhī wǎng luò)* "Mixup" is translated as "混合" (hùn hé)* "Sharpness-Aware Minimization" is translated as "锐度意识化最小化" (yǐ shi zhī yì xiǎng zuì xiǎo)* "Generalized-Mixup" is translated as "通用混合" (gōng yòng hùn hé)* "Binary G-Mix" is translated as "二进制G-Mix" (èr jì zhì G-Mix)* "Decomposed G-Mix" is translated as "分解G-Mix" (fēn jiě G-Mix)
</details></li>
</ul>
<hr>
<h2 id="Analysis-of-the-Evolution-of-Advanced-Transformer-Based-Language-Models-Experiments-on-Opinion-Mining"><a href="#Analysis-of-the-Evolution-of-Advanced-Transformer-Based-Language-Models-Experiments-on-Opinion-Mining" class="headerlink" title="Analysis of the Evolution of Advanced Transformer-Based Language Models: Experiments on Opinion Mining"></a>Analysis of the Evolution of Advanced Transformer-Based Language Models: Experiments on Opinion Mining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03235">http://arxiv.org/abs/2308.03235</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zekaouinoureddine/Opinion-Transformers">https://github.com/zekaouinoureddine/Opinion-Transformers</a></li>
<li>paper_authors: Nour Eddine Zekaoui, Siham Yousfi, Maryem Rhanoui, Mounia Mikram</li>
<li>for: 本研究的目的是研究Transformer型语言模型在情感分析领域的表现，并对这些模型进行比较，以便为生产工程师和研究人员提供指导。</li>
<li>methods: 本研究使用了Transformer型语言模型进行情感分析，并对这些模型进行了比较。</li>
<li>results: 研究发现，Transformer型语言模型在情感分析任务上具有出色的表现，具有较快的处理速度和更高的准确率。<details>
<summary>Abstract</summary>
Opinion mining, also known as sentiment analysis, is a subfield of natural language processing (NLP) that focuses on identifying and extracting subjective information in textual material. This can include determining the overall sentiment of a piece of text (e.g., positive or negative), as well as identifying specific emotions or opinions expressed in the text, that involves the use of advanced machine and deep learning techniques. Recently, transformer-based language models make this task of human emotion analysis intuitive, thanks to the attention mechanism and parallel computation. These advantages make such models very powerful on linguistic tasks, unlike recurrent neural networks that spend a lot of time on sequential processing, making them prone to fail when it comes to processing long text. The scope of our paper aims to study the behaviour of the cutting-edge Transformer-based language models on opinion mining and provide a high-level comparison between them to highlight their key particularities. Additionally, our comparative study shows leads and paves the way for production engineers regarding the approach to focus on and is useful for researchers as it provides guidelines for future research subjects.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Imbalanced-Large-Graph-Learning-Framework-for-FPGA-Logic-Elements-Packing-Prediction"><a href="#Imbalanced-Large-Graph-Learning-Framework-for-FPGA-Logic-Elements-Packing-Prediction" class="headerlink" title="Imbalanced Large Graph Learning Framework for FPGA Logic Elements Packing Prediction"></a>Imbalanced Large Graph Learning Framework for FPGA Logic Elements Packing Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03231">http://arxiv.org/abs/2308.03231</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhixiong Di, Runzhe Tao, Lin Chen, Qiang Wu, Yibo Lin</li>
<li>for: 预测FPGA嵌入式逻辑元素是否被压缩，以优化设计优化和加速设计关闭。</li>
<li>methods: 提议一种大图学习框架ImLG，通过独特的特征提取和特征聚合方法来提高层次图表示学习。另外，针对不均衡分布的逻辑元素压缩和未压缩的情况，提出了图像增量和小批训练等技术来解决这种学习任务。</li>
<li>results: 实验结果表明，我们的框架可以提高FPGA嵌入式逻辑元素预测的F1分数比最近的泊Sdk-based预测方法提高42.82%。物理设计结果表明，提议的方法可以帮助分配器提高已经路由的电缆长度0.93%和SLICE占用0.89%。<details>
<summary>Abstract</summary>
Packing is a required step in a typical FPGA CAD flow. It has high impacts to the performance of FPGA placement and routing. Early prediction of packing results can guide design optimization and expedite design closure. In this work, we propose an imbalanced large graph learning framework, ImLG, for prediction of whether logic elements will be packed after placement. Specifically, we propose dedicated feature extraction and feature aggregation methods to enhance the node representation learning of circuit graphs. With imbalanced distribution of packed and unpacked logic elements, we further propose techniques such as graph oversampling and mini-batch training for this imbalanced learning task in large circuit graphs. Experimental results demonstrate that our framework can improve the F1 score by 42.82% compared to the most recent Gaussian-based prediction method. Physical design results show that the proposed method can assist the placer in improving routed wirelength by 0.93% and SLICE occupation by 0.89%.
</details>
<details>
<summary>摘要</summary>
Packing 是FPGA CAD流程中的一个必需步骤，它对FPGA的地点和路径产生了高度的影响。 early prediction of packing results can guide design optimization and expedite design closure. 在这种工作中，我们提出了一种大图学习框架，ImLG，用于预测逻辑元素是否将被packed After placement。specifically，我们提出了专门的特征提取和特征聚合方法，以增强环 Graph的节点表示学习。 With imbalanced distribution of packed and unpacked logic elements, we further propose techniques such as graph oversampling and mini-batch training for this imbalanced learning task in large circuit graphs. Experimental results demonstrate that our framework can improve the F1 score by 42.82% compared to the most recent Gaussian-based prediction method. Physical design results show that the proposed method can assist the placer in improving routed wirelength by 0.93% and SLICE occupation by 0.89%.
</details></li>
</ul>
<hr>
<h2 id="Tractability-of-approximation-by-general-shallow-networks"><a href="#Tractability-of-approximation-by-general-shallow-networks" class="headerlink" title="Tractability of approximation by general shallow networks"></a>Tractability of approximation by general shallow networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03230">http://arxiv.org/abs/2308.03230</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hrushikesh Mhaskar, Tong Mao</li>
<li>for: 本文提出了一种更加锐利的 bounds 方法，用于 approximating 函数 $ x\mapsto\int_{\mathbb{Y} G( x, y)d\tau( y) $ 的形式，where $\mathbb{X}$ 和 $\mathbb{Y}$ 是紧密的 метри空间。</li>
<li>methods: 本文使用了 $G$-网络，即 $ x\mapsto \sum_{k&#x3D;1}^n a_kG( x, y_k) $，where $ y_1,\cdots, y_n\in\mathbb{Y} $ 和 $ a_1,\cdots, a_n\in\mathbb{R} $。</li>
<li>results: 本文提出了基于维度的独立 bounds 方法，可以用于评估 $G$-网络的度量准确性，并且这些 bounds 中的常数都是几乎只依赖于维度的 polynomial 方式增长。<details>
<summary>Abstract</summary>
In this paper, we present a sharper version of the results in the paper Dimension independent bounds for general shallow networks; Neural Networks, \textbf{123} (2020), 142-152. Let $\mathbb{X}$ and $\mathbb{Y}$ be compact metric spaces. We consider approximation of functions of the form $ x\mapsto\int_{\mathbb{Y} G( x, y)d\tau( y)$, $ x\in\mathbb{X}$, by $G$-networks of the form $ x\mapsto \sum_{k=1}^n a_kG( x, y_k)$, $ y_1,\cdots, y_n\in\mathbb{Y}$, $a_1,\cdots, a_n\in\mathbb{R}$. Defining the dimensions of $\mathbb{X}$ and $\mathbb{Y}$ in terms of covering numbers, we obtain dimension independent bounds on the degree of approximation in terms of $n$, where also the constants involved are all dependent at most polynomially on the dimensions. Applications include approximation by power rectified linear unit networks, zonal function networks, certain radial basis function networks as well as the important problem of function extension to higher dimensional spaces.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一个更为精细的版本的result，来自文章《独立维度 bounds for general shallow networks》；Neural Networks, \textbf{123} (2020), 142-152。假设 $\mathbb{X}$ 和 $\mathbb{Y}$ 是两个 компакт度度量空间。我们考虑了将函数 $x\mapsto\int_{\mathbb{Y} G( x, y)d\tau( y)$, $x\in\mathbb{X}$, 近似为 $G$-网络的形式 $x\mapsto \sum_{k=1}^n a_kG( x, y_k)$, $y_1,\cdots, y_n\in\mathbb{Y}$, $a_1,\cdots, a_n\in\mathbb{R}$.我们使用covering数量来定义 $\mathbb{X}$ 和 $\mathbb{Y}$ 的维度，得到了独立于维度的度量约束，其中涉及的常数都是仅仅受到 polynomial 幂级的影响。应用包括power rectified linear unit网络、zonal function网络、certain radial basis function网络以及高维空间中函数扩展的重要问题。
</details></li>
</ul>
<hr>
<h2 id="Why-Linguistics-Will-Thrive-in-the-21st-Century-A-Reply-to-Piantadosi-2023"><a href="#Why-Linguistics-Will-Thrive-in-the-21st-Century-A-Reply-to-Piantadosi-2023" class="headerlink" title="Why Linguistics Will Thrive in the 21st Century: A Reply to Piantadosi (2023)"></a>Why Linguistics Will Thrive in the 21st Century: A Reply to Piantadosi (2023)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03228">http://arxiv.org/abs/2308.03228</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jordan Kodner, Sarah Payne, Jeffrey Heinz</li>
<li>for: 本文批判Piantadosi（2023）认为现代语言模型推翻了昌斯基的语言方法，从四个主要方面进行批判。</li>
<li>methods: 本文使用了大量语言模型（LLMs）的印象和实用性来评估昌斯基的语言方法。</li>
<li>results: 本文 conclude that 大量语言模型（LLMs）无法解决语言学习中的主要谜团，即儿童在获得 native语言后的语言能力增长。此外，LLMs 无法提供对语言和语言学习的科学理论，因为它们只提供了预测而不是可解释的结果。<details>
<summary>Abstract</summary>
We present a critical assessment of Piantadosi's (2023) claim that "Modern language models refute Chomsky's approach to language," focusing on four main points. First, despite the impressive performance and utility of large language models (LLMs), humans achieve their capacity for language after exposure to several orders of magnitude less data. The fact that young children become competent, fluent speakers of their native languages with relatively little exposure to them is the central mystery of language learning to which Chomsky initially drew attention, and LLMs currently show little promise of solving this mystery. Second, what can the artificial reveal about the natural? Put simply, the implications of LLMs for our understanding of the cognitive structures and mechanisms underlying language and its acquisition are like the implications of airplanes for understanding how birds fly. Third, LLMs cannot constitute scientific theories of language for several reasons, not least of which is that scientific theories must provide interpretable explanations, not just predictions. This leads to our final point: to even determine whether the linguistic and cognitive capabilities of LLMs rival those of humans requires explicating what humans' capacities actually are. In other words, it requires a separate theory of language and cognition; generative linguistics provides precisely such a theory. As such, we conclude that generative linguistics as a scientific discipline will remain indispensable throughout the 21st century and beyond.
</details>
<details>
<summary>摘要</summary>
我们提供了对Piantadosi（2023）的批判性评估，关注四个主要点。首先，虽然大型语言模型（LLM）具有印象力和实用性，但人类通过相对许多更少的数据获得语言能力是语言学习的中心谜题，Piantadosi最初吸引到了注意。LLMs current show little promise of solving this mystery. Second, what can the artificial reveal about the natural? In other words, the implications of LLMs for our understanding of the cognitive structures and mechanisms underlying language and its acquisition are like the implications of airplanes for understanding how birds fly. Third, LLMs cannot constitute scientific theories of language for several reasons, not least of which is that scientific theories must provide interpretable explanations, not just predictions. This leads to our final point: to even determine whether the linguistic and cognitive capabilities of LLMs rival those of humans requires explicating what humans' capacities actually are. In other words, it requires a separate theory of language and cognition; generative linguistics provides precisely such a theory. As such, we conclude that generative linguistics as a scientific discipline will remain indispensable throughout the 21st century and beyond.
</details></li>
</ul>
<hr>
<h2 id="Local-Consensus-Enhanced-Siamese-Network-with-Reciprocal-Loss-for-Two-view-Correspondence-Learning"><a href="#Local-Consensus-Enhanced-Siamese-Network-with-Reciprocal-Loss-for-Two-view-Correspondence-Learning" class="headerlink" title="Local Consensus Enhanced Siamese Network with Reciprocal Loss for Two-view Correspondence Learning"></a>Local Consensus Enhanced Siamese Network with Reciprocal Loss for Two-view Correspondence Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03217">http://arxiv.org/abs/2308.03217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linbo Wang, Jing Wu, Xianyong Fang, Zhengyi Liu, Chenjie Cao, Yanwei Fu</li>
<li>for: 提高两视对匹配学习框架的精度和稳定性。</li>
<li>methods: 提议一个Local Feature Consensus（LFC）插件块来增强现有模型的特征，以及扩展现有模型到siames网络，使用对称损失来利用对方向投影的信息。</li>
<li>results: 通过实验，在标准 benchmark 数据集上达到了状态当前的性能。<details>
<summary>Abstract</summary>
Recent studies of two-view correspondence learning usually establish an end-to-end network to jointly predict correspondence reliability and relative pose. We improve such a framework from two aspects. First, we propose a Local Feature Consensus (LFC) plugin block to augment the features of existing models. Given a correspondence feature, the block augments its neighboring features with mutual neighborhood consensus and aggregates them to produce an enhanced feature. As inliers obey a uniform cross-view transformation and share more consistent learned features than outliers, feature consensus strengthens inlier correlation and suppresses outlier distraction, which makes output features more discriminative for classifying inliers/outliers. Second, existing approaches supervise network training with the ground truth correspondences and essential matrix projecting one image to the other for an input image pair, without considering the information from the reverse mapping. We extend existing models to a Siamese network with a reciprocal loss that exploits the supervision of mutual projection, which considerably promotes the matching performance without introducing additional model parameters. Building upon MSA-Net, we implement the two proposals and experimentally achieve state-of-the-art performance on benchmark datasets.
</details>
<details>
<summary>摘要</summary>
最近的两视匹配学习研究通常建立一个端到端网络，同时预测匹配可靠性和相对pose。我们从两个方面提高了这种框架：首先，我们提出了一个本地特征共识（LFC）插件块，用于增强现有模型的特征。给一个匹配特征，该块将其周围的特征与相互邻域共识，并将它们积累到生成一个强化特征。由于匹配点遵循均匀的双视变换，并且分享更一致的学习特征，因此特征共识强化匹配点之间的相互关系，降低干扰物的影响，使输出特征更有特征性，以便将匹配点分类为匹配/干扰物。其次，现有方法在网络训练时使用真实匹配和 Essential matrix projecting一个图像到另一个图像，而不考虑反向映射的信息。我们将现有模型扩展为siamese网络，使用相互抽象的损失函数，以便利用反向映射的supervision，大幅提高匹配性能，而无需添加更多的模型参数。基于MSA-Net，我们实现了两个提议，并在benchmark datasets上实验ally achieve state-of-the-art performance。
</details></li>
</ul>
<hr>
<h2 id="The-Effect-of-SGD-Batch-Size-on-Autoencoder-Learning-Sparsity-Sharpness-and-Feature-Learning"><a href="#The-Effect-of-SGD-Batch-Size-on-Autoencoder-Learning-Sparsity-Sharpness-and-Feature-Learning" class="headerlink" title="The Effect of SGD Batch Size on Autoencoder Learning: Sparsity, Sharpness, and Feature Learning"></a>The Effect of SGD Batch Size on Autoencoder Learning: Sparsity, Sharpness, and Feature Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03215">http://arxiv.org/abs/2308.03215</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikhil Ghosh, Spencer Frei, Wooseok Ha, Bin Yu</li>
<li>for: 这个论文研究了使用梯度下降法（SGD）训练单神经 autoencoder 的动态性，并研究了不同批处理大小对于非对称问题的影响。</li>
<li>methods: 该论文使用了随机初始化的 SGD 算法，并研究了不同批处理大小对于解的影响。</li>
<li>results: 研究发现，无论批处理大小，SGD 都可以成功找到全球最小值，但是特定的全球最小值取决于批处理大小。在全部批处理情况下，解是 dense 的（即不含杂的），并且与初始向量高度相似，表明在这种情况下，Feature learning 不太多。相反，任何小于样本数的批处理大小都可以找到一个 sparse 的全球最小值，这种 “特征选择” 是由梯度randomness 引起的。此外，我们还发现，使用全部批处理的SGD 找到的最小值比使用更小的批处理大小找到的最小值更平滑（即距离初始向量更远），这与之前的研究不同。Note: “梯度randomness” is a term used to describe the randomness of the gradients in the stochastic gradient descent algorithm.<details>
<summary>Abstract</summary>
In this work, we investigate the dynamics of stochastic gradient descent (SGD) when training a single-neuron autoencoder with linear or ReLU activation on orthogonal data. We show that for this non-convex problem, randomly initialized SGD with a constant step size successfully finds a global minimum for any batch size choice. However, the particular global minimum found depends upon the batch size. In the full-batch setting, we show that the solution is dense (i.e., not sparse) and is highly aligned with its initialized direction, showing that relatively little feature learning occurs. On the other hand, for any batch size strictly smaller than the number of samples, SGD finds a global minimum which is sparse and nearly orthogonal to its initialization, showing that the randomness of stochastic gradients induces a qualitatively different type of "feature selection" in this setting. Moreover, if we measure the sharpness of the minimum by the trace of the Hessian, the minima found with full batch gradient descent are flatter than those found with strictly smaller batch sizes, in contrast to previous works which suggest that large batches lead to sharper minima. To prove convergence of SGD with a constant step size, we introduce a powerful tool from the theory of non-homogeneous random walks which may be of independent interest.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们研究了使用单 neuron autoencoder 的梯度下降法（SGD）在正交数据上的动态。我们发现，对于这个非 convex 问题，随机初始化的SGD  WITH 常数步长能够成功找到全局最小值，但这个全局最小值与批处理大小有关。在完整批处理设置下，我们发现的解是密集的（即不是稀疏），与其初始方向高度相似，表明在这个设置下，相对较少的特征学习发生。相反，任何小于样本数的批处理大小都会使SGD找到全局最小值，这个全局最小值是稀疏的和初始方向几乎垂直的，这表明随机梯度的Randomness 会导致一种totally different的"特征选择"现象。此外，我们还证明了SGD WITH 常数步长的收敛性，并引入了非同homogeneous random walk 的理论工具，这可能有独立的意义。
</details></li>
</ul>
<hr>
<h2 id="Average-Hard-Attention-Transformers-are-Constant-Depth-Uniform-Threshold-Circuits"><a href="#Average-Hard-Attention-Transformers-are-Constant-Depth-Uniform-Threshold-Circuits" class="headerlink" title="Average-Hard Attention Transformers are Constant-Depth Uniform Threshold Circuits"></a>Average-Hard Attention Transformers are Constant-Depth Uniform Threshold Circuits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03212">http://arxiv.org/abs/2308.03212</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lena Strobl</li>
<li>for: This paper explores the relationship between transformer models and constant-depth threshold circuits, and demonstrates that transformers can be simulated by constant-depth threshold circuits.</li>
<li>methods: The paper uses two assumptions: average-hard attention and logarithmic precision for internal computations relative to input length.</li>
<li>results: The paper shows that both transformer models can be simulated by constant-depth threshold circuits, with the latter being more robust due to generating a uniform circuit family. Additionally, the paper extends the first result to yield uniform circuits as well.Here’s the information in Simplified Chinese text:</li>
<li>for: 这篇论文研究了 transformer 模型与常深度阈值电路之间的关系，并证明 transformer 可以被模拟为常深度阈值电路。</li>
<li>methods: 该论文使用了两个假设：平均困难的注意力和对输入长度的对数精度。</li>
<li>results: 该论文显示了 transformer 模型可以被模拟为常深度阈值电路，其中第二个更加稳定，因为它生成了一个固定深度电路家族。此外，论文还将第一个结果推广到生成固定深度电路。<details>
<summary>Abstract</summary>
Transformers have emerged as a widely used neural network model for various natural language processing tasks. Previous research explored their relationship with constant-depth threshold circuits, making two assumptions: average-hard attention and logarithmic precision for internal computations relative to input length. Merrill et al. (2022) prove that average-hard attention transformers recognize languages that fall within the complexity class TC0, denoting the set of languages that can be recognized by constant-depth polynomial-size threshold circuits. Likewise, Merrill and Sabharwal (2023) show that log-precision transformers recognize languages within the class of uniform TC0. This shows that both transformer models can be simulated by constant-depth threshold circuits, with the latter being more robust due to generating a uniform circuit family. Our paper shows that the first result can be extended to yield uniform circuits as well.
</details>
<details>
<summary>摘要</summary>
transformers 已经成为自然语言处理任务中广泛使用的神经网络模型。前期研究探讨了它们与常深度阈值电路之间的关系，假设了平均困难注意力和对内部计算的对数精度相对于输入长度。Merill et al. (2022)证明了average-hard attention transformers 可以认出TC0复杂性类型的语言，这些语言可以被表示为常深度的多阶度阈值电路。Merill 和 Sabharwal (2023)表明，log-precision transformers 可以认出 uniform TC0 类型的语言。这表明这两种 transformers 模型都可以被模拟为常深度阈值电路，其中后者更加稳定，因为它生成了一个固定深度的多阶度阈值电路家族。我们的论文表明，上一个结果可以被推广到生成固定深度的多阶度阈值电路。
</details></li>
</ul>
<hr>
<h2 id="Time-Parameterized-Convolutional-Neural-Networks-for-Irregularly-Sampled-Time-Series"><a href="#Time-Parameterized-Convolutional-Neural-Networks-for-Irregularly-Sampled-Time-Series" class="headerlink" title="Time-Parameterized Convolutional Neural Networks for Irregularly Sampled Time Series"></a>Time-Parameterized Convolutional Neural Networks for Irregularly Sampled Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03210">http://arxiv.org/abs/2308.03210</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chrysoula Kosma, Giannis Nikolentzos, Michalis Vazirgiannis</li>
<li>for: 这篇论文主要关注于如何对不规则数据进行模型化和预测，尤其是在多变量时间序列中。</li>
<li>methods: 本文提出了一种名为时间参数化卷积神经网（TPCNN）的新型神经网络模型，它运用时间参数化的卷积 kernel 来实现不规则数据的模型化。</li>
<li>results: 根据实验结果，TPCNN 模型在测量 interpolating 和类别任务中表现了竞争力和效率，并且可以对不规则数据进行有效地模型化和预测。<details>
<summary>Abstract</summary>
Irregularly sampled multivariate time series are ubiquitous in several application domains, leading to sparse, not fully-observed and non-aligned observations across different variables. Standard sequential neural network architectures, such as recurrent neural networks (RNNs) and convolutional neural networks (CNNs), consider regular spacing between observation times, posing significant challenges to irregular time series modeling. While most of the proposed architectures incorporate RNN variants to handle irregular time intervals, convolutional neural networks have not been adequately studied in the irregular sampling setting. In this paper, we parameterize convolutional layers by employing time-explicitly initialized kernels. Such general functions of time enhance the learning process of continuous-time hidden dynamics and can be efficiently incorporated into convolutional kernel weights. We, thus, propose the time-parameterized convolutional neural network (TPCNN), which shares similar properties with vanilla convolutions but is carefully designed for irregularly sampled time series. We evaluate TPCNN on both interpolation and classification tasks involving real-world irregularly sampled multivariate time series datasets. Our experimental results indicate the competitive performance of the proposed TPCNN model which is also significantly more efficient than other state-of-the-art methods. At the same time, the proposed architecture allows the interpretability of the input series by leveraging the combination of learnable time functions that improve the network performance in subsequent tasks and expedite the inaugural application of convolutions in this field.
</details>
<details>
<summary>摘要</summary>
不规则时间序列是多种应用领域中的普遍现象，导致不同变量之间的观察记录稀缺、不完全观察和不对称。标准的序列神经网络架构，如循环神经网络（RNN）和卷积神经网络（CNN），假设时间序列的均匀采样，对于不规则时间序列模型 pose significant challenges。大多数提议的架构包括RNN变体来处理不规则时间间隔，但是卷积神经网络在不规则采样 Setting 中尚未得到了充分的研究。在这篇论文中，我们将时间序列中的卷积层参数化，使用时间explicitly初始化的kernel。这种通用时间函数可以增强隐藏时间序列的学习过程，并可以高效地被包含到卷积核心的 weights 中。因此，我们提出了时间参数化卷积神经网络（TPCNN），它与普通的卷积神经网络 sharing similar properties，但是特别地设计 для不规则时间序列。我们在实验中使用TPCNN进行 interpolate 和 classification 任务，并对实际的不规则时间序列多变量数据进行评估。我们的实验结果表明，提议的 TPCNN 模型在竞争性和效率两个方面具有竞争力，而且可以更好地利用输入序列的学习可能性，通过组合学习时间函数来提高网络性能，并且可以更快地在这一领域中应用卷积神经网络。
</details></li>
</ul>
<hr>
<h2 id="Communication-Free-Distributed-GNN-Training-with-Vertex-Cut"><a href="#Communication-Free-Distributed-GNN-Training-with-Vertex-Cut" class="headerlink" title="Communication-Free Distributed GNN Training with Vertex Cut"></a>Communication-Free Distributed GNN Training with Vertex Cut</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03209">http://arxiv.org/abs/2308.03209</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaidi Cao, Rui Deng, Shirley Wu, Edward W Huang, Karthik Subbian, Jure Leskovec</li>
<li>for: 加速图 neural network（GNN）在实际图中的训练，以便应对实际图中的巨量数据和复杂结构。</li>
<li>methods: 提出了一种新的分布式训练框架CoFree-GNN，通过减少交互 communication来加速训练过程，并采用骨架剖分法保持图结构。</li>
<li>results: 在实际网络上进行了广泛的实验，demonstrating that CoFree-GNN can speed up GNN training by up to 10 times compared to existing state-of-the-art methods.<details>
<summary>Abstract</summary>
Training Graph Neural Networks (GNNs) on real-world graphs consisting of billions of nodes and edges is quite challenging, primarily due to the substantial memory needed to store the graph and its intermediate node and edge features, and there is a pressing need to speed up the training process. A common approach to achieve speed up is to divide the graph into many smaller subgraphs, which are then distributed across multiple GPUs in one or more machines and processed in parallel. However, existing distributed methods require frequent and substantial cross-GPU communication, leading to significant time overhead and progressively diminishing scalability. Here, we introduce CoFree-GNN, a novel distributed GNN training framework that significantly speeds up the training process by implementing communication-free training. The framework utilizes a Vertex Cut partitioning, i.e., rather than partitioning the graph by cutting the edges between partitions, the Vertex Cut partitions the edges and duplicates the node information to preserve the graph structure. Furthermore, the framework maintains high model accuracy by incorporating a reweighting mechanism to handle a distorted graph distribution that arises from the duplicated nodes. We also propose a modified DropEdge technique to further speed up the training process. Using an extensive set of experiments on real-world networks, we demonstrate that CoFree-GNN speeds up the GNN training process by up to 10 times over the existing state-of-the-art GNN training approaches.
</details>
<details>
<summary>摘要</summary>
训练图 neural network（GNN）在实际图中包含数百亿个节点和边的情况下是非常困难的，主要是因为需要很大的内存来存储图和其间途节点和边特征的存储。随着图的规模的增长，训练过程的速度变得非常重要。现有的分布式方法需要频繁的跨GPU通信，导致训练过程中的时间开销很大，并且随着图的规模的增长，缓存的缺省值逐渐减少。在这里，我们介绍了CoFree-GNN，一种新的分布式GNN训练框架，可以快速加速GNN训练过程。该框架使用顶点割分法，而不是将图分成多个分区，然后在多个GPU上并行处理。此外，框架还保持了高精度模型，通过对填充的节点数据进行重新权重来处理受损的图分布。我们还提出了一种修改后 DropEdge 技术，以进一步加速训练过程。通过对实际网络进行了广泛的实验，我们证明了CoFree-GNN可以在实际图中加速GNN训练过程，并且可以达到现有状态的 искусственный智能训练方法的10倍速度。
</details></li>
</ul>
<hr>
<h2 id="Microvasculature-Segmentation-in-Human-BioMolecular-Atlas-Program-HuBMAP"><a href="#Microvasculature-Segmentation-in-Human-BioMolecular-Atlas-Program-HuBMAP" class="headerlink" title="Microvasculature Segmentation in Human BioMolecular Atlas Program (HuBMAP)"></a>Microvasculature Segmentation in Human BioMolecular Atlas Program (HuBMAP)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03203">http://arxiv.org/abs/2308.03203</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youssef Sultan, Yongqiang Wang, James Scanlon, Lisa D’lima</li>
<li>for: 这个研究旨在为 HuBMAP 项目提供细胞分割技术，以创建详细的人体细胞地图。</li>
<li>methods: 该研究使用 FastAI U-Net 模型作基础，并对其进行了多种变化，包括不同的后处理架构、更深的模型和特征峰网络。</li>
<li>results: 研究对不同方法的性能进行了严格评估，并提供了有价值的探索和Future研究方向。<details>
<summary>Abstract</summary>
Image segmentation serves as a critical tool across a range of applications, encompassing autonomous driving's pedestrian detection and pre-operative tumor delineation in the medical sector. Among these applications, we focus on the National Institutes of Health's (NIH) Human BioMolecular Atlas Program (HuBMAP), a significant initiative aimed at creating detailed cellular maps of the human body. In this study, we concentrate on segmenting various microvascular structures in human kidneys, utilizing 2D Periodic Acid-Schiff (PAS)-stained histology images. Our methodology begins with a foundational FastAI U-Net model, upon which we investigate alternative backbone architectures, delve into deeper models, and experiment with Feature Pyramid Networks. We rigorously evaluate these varied approaches by benchmarking their performance against our baseline U-Net model. This study thus offers a comprehensive exploration of cutting-edge segmentation techniques, providing valuable insights for future research in the field.
</details>
<details>
<summary>摘要</summary>
Image segmentation  serves as a critical tool across a range of applications, encompassing autonomous driving's pedestrian detection and pre-operative tumor delineation in the medical sector. Among these applications, we focus on the National Institutes of Health's (NIH) Human BioMolecular Atlas Program (HuBMAP), a significant initiative aimed at creating detailed cellular maps of the human body. In this study, we concentrate on segmenting various microvascular structures in human kidneys, utilizing 2D Periodic Acid-Schiff (PAS)-stained histology images. Our methodology begins with a foundational FastAI U-Net model, upon which we investigate alternative backbone architectures, delve into deeper models, and experiment with Feature Pyramid Networks. We rigorously evaluate these varied approaches by benchmarking their performance against our baseline U-Net model. This study thus offers a comprehensive exploration of cutting-edge segmentation techniques, providing valuable insights for future research in the field.Here's the word-for-word translation of the text into Simplified Chinese:图像分割 serves as a critical tool across a range of applications, including autonomous driving的 pedestrian detection和医疗领域的 pre-operative tumor delineation。在这些应用程序中，我们关注国家医疗研究所（NIH）的人类生物分子图库计划（HuBMAP），这是一项创建详细人体细胞地图的重要 iniciative。在这种研究中，我们关注人类肾脏中的微血管结构分割，使用2D Periodic Acid-Schiff（PAS）染色 histology 图像。我们的方法开始于基础 FastAI U-Net 模型，然后我们 investigate alternative backbone architectures、 deeper models、和 Feature Pyramid Networks。我们严格评估这些不同的方法，对比我们的基eline U-Net 模型。这种研究 thus offers a comprehensive exploration of cutting-edge segmentation techniques, providing valuable insights for future research in the field。
</details></li>
</ul>
<hr>
<h2 id="Source-free-Domain-Adaptive-Human-Pose-Estimation"><a href="#Source-free-Domain-Adaptive-Human-Pose-Estimation" class="headerlink" title="Source-free Domain Adaptive Human Pose Estimation"></a>Source-free Domain Adaptive Human Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03202">http://arxiv.org/abs/2308.03202</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/davidpengucf/sfdahpe">https://github.com/davidpengucf/sfdahpe</a></li>
<li>paper_authors: Qucheng Peng, Ce Zheng, Chen Chen</li>
<li>for: 本研究旨在解决人体姿态估计（HPE）中数据隐私和安全问题，提出了一种新的任务：无源频道适应HPE。</li>
<li>methods: 本研究提出了一种新的框架，包括三个模型：源模型、中间模型和目标模型，从源保护和目标相关两个角度解决了问题。 source-protect模块更好地保持源信息，而target-relevant模块减少了空间表示的稀疏性，通过建立一个新的空间概率空间和姿势特异性学习和信息增强来解决这个问题。</li>
<li>results: 对多个频道适应HPEbenchmark进行了广泛的实验，结果表明，提出的方法在与现有方法进行比较时得到了显著的改进。代码可以在<a target="_blank" rel="noopener" href="https://github.com/davidpengucf/SFDAHPE%E4%B8%AD%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/davidpengucf/SFDAHPE中下载。</a><details>
<summary>Abstract</summary>
Human Pose Estimation (HPE) is widely used in various fields, including motion analysis, healthcare, and virtual reality. However, the great expenses of labeled real-world datasets present a significant challenge for HPE. To overcome this, one approach is to train HPE models on synthetic datasets and then perform domain adaptation (DA) on real-world data. Unfortunately, existing DA methods for HPE neglect data privacy and security by using both source and target data in the adaptation process. To this end, we propose a new task, named source-free domain adaptive HPE, which aims to address the challenges of cross-domain learning of HPE without access to source data during the adaptation process. We further propose a novel framework that consists of three models: source model, intermediate model, and target model, which explores the task from both source-protect and target-relevant perspectives. The source-protect module preserves source information more effectively while resisting noise, and the target-relevant module reduces the sparsity of spatial representations by building a novel spatial probability space, and pose-specific contrastive learning and information maximization are proposed on the basis of this space. Comprehensive experiments on several domain adaptive HPE benchmarks show that the proposed method outperforms existing approaches by a considerable margin. The codes are available at https://github.com/davidpengucf/SFDAHPE.
</details>
<details>
<summary>摘要</summary>
人体姿势估计（HPE）在多个领域得到广泛应用，包括运动分析、医疗和虚拟现实。然而，实际世界数据集的高成本成为HPE的一大挑战。为了解决这个问题，一种方法是在HPE模型上训练 synthetic 数据集，然后进行领域适应（DA）操作实际世界数据。然而，现有的 DA 方法 для HPE 忽视了数据隐私和安全性，使用了源和目标数据在适应过程中。为此，我们提出了一个新的任务，即无源领域适应 HPE，旨在解决 HPE 的跨领域学习问题，不需要源数据的访问 during adaptation process。我们还提出了一个新的框架，包括三个模型：源模型、中间模型和目标模型，该框架从源保护和目标相关两个角度出发，探讨了这个任务。源保护模块更好地保留源信息，同时抗抗噪，目标相关模块通过建立一个新的空间概率空间，减少了空间表示的稀疏性，并通过基于这个空间的姿势特异性学习和信息最大化来提高姿势估计的精度。通过对多个领域适应 HPE benchmark 进行广泛的实验，我们发现了提议方法在现有方法之上得到了较大的提升。代码可以在 <https://github.com/davidpengucf/SFDAHPE> 中下载。
</details></li>
</ul>
<hr>
<h2 id="Automatically-Correcting-Large-Language-Models-Surveying-the-landscape-of-diverse-self-correction-strategies"><a href="#Automatically-Correcting-Large-Language-Models-Surveying-the-landscape-of-diverse-self-correction-strategies" class="headerlink" title="Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies"></a>Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03188">http://arxiv.org/abs/2308.03188</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/teacherpeterpan/self-correction-llm-papers">https://github.com/teacherpeterpan/self-correction-llm-papers</a></li>
<li>paper_authors: Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, William Yang Wang</li>
<li>for: This paper aims to provide a comprehensive review of techniques for self-correction in large language models (LLMs) to address undesired behaviors such as hallucination, unfaithful reasoning, and toxic content.</li>
<li>methods: The paper reviews and taxonomizes recent work utilizing self-correction techniques, including training-time, generation-time, and post-hoc correction methods.</li>
<li>results: The paper summarizes the major applications of self-correction techniques in LLMs and discusses future directions and challenges in this emerging area of research.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目的是为大语言模型（LLM）提供全面的审查，以解决它们的不良行为，如幻见、不负责任的思维和恶势感。</li>
<li>methods: 论文回顾和分类了最近几年利用自我修复技术的研究，包括训练时、生成时和后续修复方法。</li>
<li>results: 论文总结了自我修复技术在LLM中的主要应用场景，并讨论未来的发展和挑战。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated remarkable performance across a wide array of NLP tasks. However, their efficacy is undermined by undesired and inconsistent behaviors, including hallucination, unfaithful reasoning, and toxic content. A promising approach to rectify these flaws is self-correction, where the LLM itself is prompted or guided to fix problems in its own output. Techniques leveraging automated feedback -- either produced by the LLM itself or some external system -- are of particular interest as they are a promising way to make LLM-based solutions more practical and deployable with minimal human feedback. This paper presents a comprehensive review of this emerging class of techniques. We analyze and taxonomize a wide array of recent work utilizing these strategies, including training-time, generation-time, and post-hoc correction. We also summarize the major applications of this strategy and conclude by discussing future directions and challenges.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Large language models" (LLMs) is translated as "大型语言模型" (dàxìng yǔyán módelǐ)* "Natural language processing" (NLP) is translated as "自然语言处理" (zìrán yǔyán xùzhì)* "Hallucination" is translated as "幻见" (hèngjiàn)* "Unfaithful reasoning" is translated as "不寻常的理解" (bù zhǎngcháng de lǐjiě)* "Toxic content" is translated as "毒害内容" (dāo hài nèirong)* "Self-correction" is translated as "自动修复" (zìdòng xiūgòng)* "Automated feedback" is translated as "自动反馈" (zìdòng fāngxiàn)* "Training-time" is translated as "训练时间" (xùnlí shíjiān)* "Generation-time" is translated as "生成时间" (shēngchǎng shíjiān)* "Post-hoc correction" is translated as "后续修复" (hòu xiāng xiūgòng)* "Major applications" is translated as "主要应用" (zhǔyào yìngyòu)* "Future directions" is translated as "未来方向" (wèilái fāngdìng)* "Challenges" is translated as "挑战" (tiǎozhàn)
</details></li>
</ul>
<hr>
<h2 id="A-Lightweight-Method-for-Modeling-Confidence-in-Recommendations-with-Learned-Beta-Distributions"><a href="#A-Lightweight-Method-for-Modeling-Confidence-in-Recommendations-with-Learned-Beta-Distributions" class="headerlink" title="A Lightweight Method for Modeling Confidence in Recommendations with Learned Beta Distributions"></a>A Lightweight Method for Modeling Confidence in Recommendations with Learned Beta Distributions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03186">http://arxiv.org/abs/2308.03186</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nkny/confidencerecsys2023">https://github.com/nkny/confidencerecsys2023</a></li>
<li>paper_authors: Norman Knyazev, Harrie Oosterhuis</li>
<li>for: 提供一种简单实用的推荐方法，可以提供推荐结果的信息度量。</li>
<li>methods: 使用学习 beta 分布来预测用户喜好，该方法可以简单实现，同时可以提供明确的信息度量。</li>
<li>results: 对比 existed 方法，本方法可以保持竞争性的准确率，同时信息度量和准确率之间存在显著正相关性。此外，在高精度目标推荐任务中，本方法的表现更高。<details>
<summary>Abstract</summary>
Most Recommender Systems (RecSys) do not provide an indication of confidence in their decisions. Therefore, they do not distinguish between recommendations of which they are certain, and those where they are not. Existing confidence methods for RecSys are either inaccurate heuristics, conceptually complex or computationally very expensive. Consequently, real-world RecSys applications rarely adopt these methods, and thus, provide no confidence insights in their behavior. In this work, we propose learned beta distributions (LBD) as a simple and practical recommendation method with an explicit measure of confidence. Our main insight is that beta distributions predict user preferences as probability distributions that naturally model confidence on a closed interval, yet can be implemented with the minimal model-complexity. Our results show that LBD maintains competitive accuracy to existing methods while also having a significantly stronger correlation between its accuracy and confidence. Furthermore, LBD has higher performance when applied to a high-precision targeted recommendation task. Our work thus shows that confidence in RecSys is possible without sacrificing simplicity or accuracy, and without introducing heavy computational complexity. Thereby, we hope it enables better insight into real-world RecSys and opens the door for novel future applications.
</details>
<details>
<summary>摘要</summary>
大多数推荐系统（RecSys）没有提供决策的信任度指示。因此，它们不能分辨出它们是否具有信任度。现有的信任方法对RecSys是 Either inaccurate heuristics, conceptually complex or computationally very expensive。因此，现实世界中的RecSys应用 rare adopt these methods，并因此无法提供信任信息。在这项工作中，我们提议使用学习beta分布（LBD）作为简单而实用的推荐方法，具有显式的信任度度量。我们的主要发现是，beta分布预测用户喜好的概率分布，自然地模型信任的闭合区间，但可以实现最小的模型复杂度。我们的结果表明，LBD与现有方法具有相似的精度，而且其准确性和信任度之间存在显著的相关性。此外，LBD在高精度目标推荐任务中表现更高。因此，我们的工作显示了信任度在RecSys中是可能的，不需要牺牲简单性或准确性，也不需要承受重量的计算复杂度。这有助于提供更好的察看实际RecSys的信息，并开启了未来应用的新可能性。
</details></li>
</ul>
<hr>
<h2 id="A-Critical-Review-of-Physics-Informed-Machine-Learning-Applications-in-Subsurface-Energy-Systems"><a href="#A-Critical-Review-of-Physics-Informed-Machine-Learning-Applications-in-Subsurface-Energy-Systems" class="headerlink" title="A Critical Review of Physics-Informed Machine Learning Applications in Subsurface Energy Systems"></a>A Critical Review of Physics-Informed Machine Learning Applications in Subsurface Energy Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04457">http://arxiv.org/abs/2308.04457</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdeldjalil Latrach, Mohamed Lamine Malki, Misael Morales, Mohamed Mehana, Minou Rabiei</li>
<li>For: The paper is written for researchers and practitioners in the field of machine learning, particularly in the area of physics-informed machine learning (PIML), to provide a comprehensive review of its applications in subsurface energy systems, such as the oil and gas industry.* Methods: The paper uses a literature review to discuss the current state of PIML techniques and their applications in various fields, including seismic applications, reservoir simulation, hydrocarbons production forecasting, and intelligent decision-making in the exploration and production stages.* Results: The paper highlights the successful utilization of PIML for tasks related to subsurface energy systems, demonstrating its ability to provide more accurate and reliable predictions for resource management and operational efficiency. Additionally, it shows the potential of PIML to revolutionize the oil and gas industry and other emerging areas of interest, such as carbon and hydrogen storage, and geothermal systems.<details>
<summary>Abstract</summary>
Machine learning has emerged as a powerful tool in various fields, including computer vision, natural language processing, and speech recognition. It can unravel hidden patterns within large data sets and reveal unparalleled insights, revolutionizing many industries and disciplines. However, machine and deep learning models lack interpretability and limited domain-specific knowledge, especially in applications such as physics and engineering. Alternatively, physics-informed machine learning (PIML) techniques integrate physics principles into data-driven models. By combining deep learning with domain knowledge, PIML improves the generalization of the model, abidance by the governing physical laws, and interpretability. This paper comprehensively reviews PIML applications related to subsurface energy systems, mainly in the oil and gas industry. The review highlights the successful utilization of PIML for tasks such as seismic applications, reservoir simulation, hydrocarbons production forecasting, and intelligent decision-making in the exploration and production stages. Additionally, it demonstrates PIML's capabilities to revolutionize the oil and gas industry and other emerging areas of interest, such as carbon and hydrogen storage; and geothermal systems by providing more accurate and reliable predictions for resource management and operational efficiency.
</details>
<details>
<summary>摘要</summary>
Translation Notes:* "unravel" 解释 (jiě jiě)* "hidden patterns" 隐藏的模式 (hìn zì de mó shì)* "revolutionize" 革命化 (gé mín huà)* "domain knowledge" 领域知识 (zhōng yì zhī shī)* "physics-informed" 物理学习 (wù lǐ xué xí)* "abides by" 遵循 (zhèng xiǎng)* "interpretability" 可解释性 (kě jì e xiǎng xìng)* "subsurface energy systems" 地层能源系统 (dì céng néng yuán xìng zhì)* "oil and gas industry" 石油和天然气业 (shí yóu hé tiān nàng qì yè)* "seismic applications" 地震应用 (dì zhèn yìng yòu)* "reservoir simulation" 储量模拟 (chuī liàng mó xiǎng)* "hydrocarbons production forecasting" 矿物质生产预测 (kuàng wù zhì shēng chéng yù jì)* "intelligent decision-making" 智能决策 (zhì néng jí suī)* "resource management" 资源管理 (yùn xīn guǎn lí)* "operational efficiency" 运营效率 (yùn yìng xiǎng jì)* "carbon and hydrogen storage" 碳和氢存储 (dàn hé hóu cè yù)* "geothermal systems" 地热系统 (dì rè xìng zhì)
</details></li>
</ul>
<hr>
<h2 id="Adapting-Machine-Learning-Diagnostic-Models-to-New-Populations-Using-a-Small-Amount-of-Data-Results-from-Clinical-Neuroscience"><a href="#Adapting-Machine-Learning-Diagnostic-Models-to-New-Populations-Using-a-Small-Amount-of-Data-Results-from-Clinical-Neuroscience" class="headerlink" title="Adapting Machine Learning Diagnostic Models to New Populations Using a Small Amount of Data: Results from Clinical Neuroscience"></a>Adapting Machine Learning Diagnostic Models to New Populations Using a Small Amount of Data: Results from Clinical Neuroscience</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03175">http://arxiv.org/abs/2308.03175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rongguang Wang, Guray Erus, Pratik Chaudhari, Christos Davatzikos</li>
<li>for: 这篇论文目的是为了解决机器学习在医疗领域中的可重现性问题，特别是在医学中。</li>
<li>methods: 这篇论文使用了权重的机制实现零偏好的学习方法，将来自不同来源的数据结合来预测目标群的结果。</li>
<li>results: 这篇论文的结果显示，这种方法可以在多来源数据上建立更好的预测模型，并且可以在不同的扫描器、实验室和人口特征下进行可重现性的预测。<details>
<summary>Abstract</summary>
Machine learning (ML) has shown great promise for revolutionizing a number of areas, including healthcare. However, it is also facing a reproducibility crisis, especially in medicine. ML models that are carefully constructed from and evaluated on a training set might not generalize well on data from different patient populations or acquisition instrument settings and protocols. We tackle this problem in the context of neuroimaging of Alzheimer's disease (AD), schizophrenia (SZ) and brain aging. We develop a weighted empirical risk minimization approach that optimally combines data from a source group, e.g., subjects are stratified by attributes such as sex, age group, race and clinical cohort to make predictions on a target group, e.g., other sex, age group, etc. using a small fraction (10%) of data from the target group. We apply this method to multi-source data of 15,363 individuals from 20 neuroimaging studies to build ML models for diagnosis of AD and SZ, and estimation of brain age. We found that this approach achieves substantially better accuracy than existing domain adaptation techniques: it obtains area under curve greater than 0.95 for AD classification, area under curve greater than 0.7 for SZ classification and mean absolute error less than 5 years for brain age prediction on all target groups, achieving robustness to variations of scanners, protocols, and demographic or clinical characteristics. In some cases, it is even better than training on all data from the target group, because it leverages the diversity and size of a larger training set. We also demonstrate the utility of our models for prognostic tasks such as predicting disease progression in individuals with mild cognitive impairment. Critically, our brain age prediction models lead to new clinical insights regarding correlations with neurophysiological tests.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Two-Sides-of-Miscalibration-Identifying-Over-and-Under-Confidence-Prediction-for-Network-Calibration"><a href="#Two-Sides-of-Miscalibration-Identifying-Over-and-Under-Confidence-Prediction-for-Network-Calibration" class="headerlink" title="Two Sides of Miscalibration: Identifying Over and Under-Confidence Prediction for Network Calibration"></a>Two Sides of Miscalibration: Identifying Over and Under-Confidence Prediction for Network Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03172">http://arxiv.org/abs/2308.03172</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aoshuang92/miscalibration_ts">https://github.com/aoshuang92/miscalibration_ts</a></li>
<li>paper_authors: Shuang Ao, Stefan Rueger, Advaith Siddharthan</li>
<li>for: 本研究旨在解决深度神经网络的准确预测问题中的可靠性问题，即训练数据集中的模型可靠性问题。</li>
<li>methods: 本研究提出了一种新的评估方法，即误差评估指标，以评估模型的整体和分类准确率。此外，本研究还提出了一种基于分类误差评估指标的calibration技术，可以解决模型过于自信和不够自信的问题。</li>
<li>results: 本研究的实验结果表明，提出的误差评估指标和calibration技术可以substantially outperform现有的calibration技术。此外，在一个自动失败检测任务中，我们的方法也提高了模型的失败检测和可靠性。<details>
<summary>Abstract</summary>
Proper confidence calibration of deep neural networks is essential for reliable predictions in safety-critical tasks. Miscalibration can lead to model over-confidence and/or under-confidence; i.e., the model's confidence in its prediction can be greater or less than the model's accuracy. Recent studies have highlighted the over-confidence issue by introducing calibration techniques and demonstrated success on various tasks. However, miscalibration through under-confidence has not yet to receive much attention. In this paper, we address the necessity of paying attention to the under-confidence issue. We first introduce a novel metric, a miscalibration score, to identify the overall and class-wise calibration status, including being over or under-confident. Our proposed metric reveals the pitfalls of existing calibration techniques, where they often overly calibrate the model and worsen under-confident predictions. Then we utilize the class-wise miscalibration score as a proxy to design a calibration technique that can tackle both over and under-confidence. We report extensive experiments that show our proposed methods substantially outperforming existing calibration techniques. We also validate our proposed calibration technique on an automatic failure detection task with a risk-coverage curve, reporting that our methods improve failure detection as well as trustworthiness of the model. The code are available at \url{https://github.com/AoShuang92/miscalibration_TS}.
</details>
<details>
<summary>摘要</summary>
深度神经网络的正确信度调整是关键 для可靠的预测结果。不当的信度调整可能导致模型过度自信或者不足自信，即模型对其预测结果的自信度高于或低于实际精度。当前的研究主要关注过度自信的问题，并已经在不同的任务上展现了成功。然而，下降自信的问题尚未得到充分的注意。在这篇论文中，我们强调了对下降自信的需要，并提出了一种新的评价指标——信度混乱分数，用于评估模型的总体和分类准确程度。我们的提议的评价指标 revelas了现有的准则化技术的缺陷，它们经常对模型进行过度准则化，从而使下降自信的预测结果更加差。然后，我们使用分类准确度下降的指标作为代理，设计了一种能够杜绝过度和下降自信的准则化技术。我们进行了广泛的实验，并证明了我们的提议方法在现有的准则化技术之上显著超越。此外，我们还验证了我们的提议准则化技术在自动故障检测任务中的可靠性和信任性，通过发布了一个风险覆盖曲线。代码可以在 \url{https://github.com/AoShuang92/miscalibration_TS} 中找到。
</details></li>
</ul>
<hr>
<h2 id="Detection-of-Anomalies-in-Multivariate-Time-Series-Using-Ensemble-Techniques"><a href="#Detection-of-Anomalies-in-Multivariate-Time-Series-Using-Ensemble-Techniques" class="headerlink" title="Detection of Anomalies in Multivariate Time Series Using Ensemble Techniques"></a>Detection of Anomalies in Multivariate Time Series Using Ensemble Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03171">http://arxiv.org/abs/2308.03171</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anastasios Iliopoulos, John Violos, Christos Diou, Iraklis Varlamis</li>
<li>for: 这篇论文主要关注于多变量时间序列异常探测，以解决许多领域中的问题。</li>
<li>methods: 本论文提出了一种基于深度神经网络的方法，包括LSTM、自动Encoder和嵌入式自动Encoder等。这些方法在具有偏差数据的情况下表现出色。然而，当应用到多变量时间序列时，异常可能从一小subset的特征集中发生。为了提高这些基本模型的表现，我们提出了一种特征袋包技术，将特征集分成多个子集，并对每个子集进行适当的变数转换。</li>
<li>results: 本论文的实验结果显示，提出的组合技术可以对SKAB资料集进行异常探测，并且在不监控和半监控情况下都有着良好的表现。具体来说，这篇论文的数据显示，使用组合技术可以对SKAB资料集进行异常探测，并且在不监控情况下，异常探测精度提高了2%，而在半监控情况下，异常探测精度提高了10%以上。<details>
<summary>Abstract</summary>
Anomaly Detection in multivariate time series is a major problem in many fields. Due to their nature, anomalies sparsely occur in real data, thus making the task of anomaly detection a challenging problem for classification algorithms to solve. Methods that are based on Deep Neural Networks such as LSTM, Autoencoders, Convolutional Autoencoders etc., have shown positive results in such imbalanced data. However, the major challenge that algorithms face when applied to multivariate time series is that the anomaly can arise from a small subset of the feature set. To boost the performance of these base models, we propose a feature-bagging technique that considers only a subset of features at a time, and we further apply a transformation that is based on nested rotation computed from Principal Component Analysis (PCA) to improve the effectiveness and generalization of the approach. To further enhance the prediction performance, we propose an ensemble technique that combines multiple base models toward the final decision. In addition, a semi-supervised approach using a Logistic Regressor to combine the base models' outputs is proposed. The proposed methodology is applied to the Skoltech Anomaly Benchmark (SKAB) dataset, which contains time series data related to the flow of water in a closed circuit, and the experimental results show that the proposed ensemble technique outperforms the basic algorithms. More specifically, the performance improvement in terms of anomaly detection accuracy reaches 2% for the unsupervised and at least 10% for the semi-supervised models.
</details>
<details>
<summary>摘要</summary>
异常检测在多变量时间序列中是许多领域的主要问题。由于异常事件罕见，因此对分类算法来说是一个困难的问题。基于深度神经网络的方法，如LSTM、Autoencoder、Convolutional Autoencoder等，在这样的不均衡数据中显示出了正面的效果。然而，在多变量时间序列中，异常可能来自一小部分特征集。为了提高基本模型的性能，我们提议一种特征袋包技术，该技术只考虑特定的子集特征，并应用基于Principal Component Analysis（PCA）的嵌入式旋转变换来提高效果和泛化性。此外，我们还提议一种集成技术，将多个基本模型的输出集成到最终决策中。此外，我们还提出了一种半监督方法，使用Logistic Regressor将基本模型的输出集成到最终决策中。我们对Skoltech异常数据集（SKAB）进行了实验，该数据集包含关于水流在关闭环circuit中的时间序列数据，实验结果显示，我们的ensemble方法在异常检测精度方面与基本算法相比，提高了2%（不监督）和至少10%（半监督）。
</details></li>
</ul>
<hr>
<h2 id="FireFly-A-Synthetic-Dataset-for-Ember-Detection-in-Wildfire"><a href="#FireFly-A-Synthetic-Dataset-for-Ember-Detection-in-Wildfire" class="headerlink" title="FireFly A Synthetic Dataset for Ember Detection in Wildfire"></a>FireFly A Synthetic Dataset for Ember Detection in Wildfire</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03164">http://arxiv.org/abs/2308.03164</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ergowho/firefly2.0">https://github.com/ergowho/firefly2.0</a></li>
<li>paper_authors: Yue Hu, Xinan Ye, Yifei Liu, Souvik Kundu, Gourav Datta, Srikar Mutnuri, Namo Asavisanu, Nora Ayanian, Konstantinos Psounis, Peter Beerel</li>
<li>for: 本研究写作了一个名为FireFly的人工数据集，用于发现灯火。</li>
<li>methods: 本研究使用Unreal Engine 4（UE4）创建了一个自动生成的统计数据集，并使用自动化的标签工具来生成标签数据。</li>
<li>results: 本研究在四个流行的物体检测模型上进行了评估，并获得了8.57%的平均精度提升（mAP）在实际的野火enario中，相比于仅以小型实际数据进行训练。<details>
<summary>Abstract</summary>
This paper presents "FireFly", a synthetic dataset for ember detection created using Unreal Engine 4 (UE4), designed to overcome the current lack of ember-specific training resources. To create the dataset, we present a tool that allows the automated generation of the synthetic labeled dataset with adjustable parameters, enabling data diversity from various environmental conditions, making the dataset both diverse and customizable based on user requirements. We generated a total of 19,273 frames that have been used to evaluate FireFly on four popular object detection models. Further to minimize human intervention, we leveraged a trained model to create a semi-automatic labeling process for real-life ember frames. Moreover, we demonstrated an up to 8.57% improvement in mean Average Precision (mAP) in real-world wildfire scenarios compared to models trained exclusively on a small real dataset.
</details>
<details>
<summary>摘要</summary>
这份论文提出了“火萝虫”，一个使用Unreal Engine 4（UE4）创建的人工数据集，用于缺乏ember特有训练资源的缺陷。为创建这个数据集，我们提供了一个自动生成Synthetic标注数据集的工具，可以根据用户需求进行自定义，以实现数据集的多样性和自定义。我们总共生成了19273帧，用于评估FireFly在四种流行的物体检测模型上。此外，我们还利用一个已经训练的模型来创建一种半自动的标注过程，以便为真实的萝虫框架进行标注。此外，我们还证明了在实际野外爆发火情况下，FireFly比只在小型实际数据集上训练的模型提高了8.57%的平均准确率。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/07/cs.LG_2023_08_07/" data-id="clp9qz86b00pmok88djqi2ler" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/07/eess.IV_2023_08_07/" class="article-date">
  <time datetime="2023-08-07T09:00:00.000Z" itemprop="datePublished">2023-08-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/07/eess.IV_2023_08_07/">eess.IV - 2023-08-07</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="SoilNet-An-Attention-based-Spatio-temporal-Deep-Learning-Framework-for-Soil-Organic-Carbon-Prediction-with-Digital-Soil-Mapping-in-Europe"><a href="#SoilNet-An-Attention-based-Spatio-temporal-Deep-Learning-Framework-for-Soil-Organic-Carbon-Prediction-with-Digital-Soil-Mapping-in-Europe" class="headerlink" title="SoilNet: An Attention-based Spatio-temporal Deep Learning Framework for Soil Organic Carbon Prediction with Digital Soil Mapping in Europe"></a>SoilNet: An Attention-based Spatio-temporal Deep Learning Framework for Soil Organic Carbon Prediction with Digital Soil Mapping in Europe</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03586">http://arxiv.org/abs/2308.03586</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nafiseh Kakhani, Moien Rangzan, Ali Jamali, Sara Attarchi, Seyed Kazem Alavipanah, Thomas Scholten</li>
<li>for: 这个研究旨在精确地描述土壤属性的空间分布，并且运用深度学习技术来预测土壤碳数量。</li>
<li>methods: 本研究提出了一个新的架构， combining 类神经网络（CNN）模型和时间缓冲机制，以及一个长期记忆（LSTM）网络，用于预测欧洲各地的土壤碳数量。</li>
<li>results: 研究结果显示，提案的架构比普遍使用的机器学习方法（如随机森林）精度高，具有较低的根平方差误差（RMSE）。这个模型可以作为预测土壤碳数量的robust工具，并且可以应用到其他土壤特性的预测中。<details>
<summary>Abstract</summary>
Digital soil mapping (DSM) is an advanced approach that integrates statistical modeling and cutting-edge technologies, including machine learning (ML) methods, to accurately depict soil properties and their spatial distribution. Soil organic carbon (SOC) is a crucial soil attribute providing valuable insights into soil health, nutrient cycling, greenhouse gas emissions, and overall ecosystem productivity. This study highlights the significance of spatial-temporal deep learning (DL) techniques within the DSM framework. A novel architecture is proposed, incorporating spatial information using a base convolutional neural network (CNN) model and spatial attention mechanism, along with climate temporal information using a long short-term memory (LSTM) network, for SOC prediction across Europe. The model utilizes a comprehensive set of environmental features, including Landsat-8 images, topography, remote sensing indices, and climate time series, as input features. Results demonstrate that the proposed framework outperforms conventional ML approaches like random forest commonly used in DSM, yielding lower root mean square error (RMSE). This model is a robust tool for predicting SOC and could be applied to other soil properties, thereby contributing to the advancement of DSM techniques and facilitating land management and decision-making processes based on accurate information.
</details>
<details>
<summary>摘要</summary>
《数字化土壤地图（DSM）是一种高级方法，通过统计模型和先进技术，包括机器学习（ML）方法，准确描述土壤属性和其空间分布。土壤有机碳（SOC）是一项重要的土壤特征，提供了有价值的信息关于土壤健康、营养循环、温室气体排放和生态系统产生力。本研究强调了在 DSM 框架中使用深度学习（DL）技术的重要性。本文提出了一种新的架构，其包括使用基本的卷积神经网络（CNN）模型和空间注意机制，以及使用长期短 памяouss术（LSTM）网络，为欧洲地区的 SOC 预测。该模型使用了包括 Landsat-8 图像、地形、遥感指数和气候时间序列等环境特征作为输入特征。结果表明，提议的框架在比Random Forest 常用于 DSM 的 ML 方法下，得到了更低的根平均方差误差（RMSE）。这种模型是一种可靠的 SOC 预测工具，可以应用于其他土壤属性预测，从而为土地管理和决策过程提供准确信息的支持。
</details></li>
</ul>
<hr>
<h2 id="Quantitative-MR-Image-Reconstruction-using-Parameter-Specific-Dictionary-Learning-with-Adaptive-Dictionary-Size-and-Sparsity-Level-Choice"><a href="#Quantitative-MR-Image-Reconstruction-using-Parameter-Specific-Dictionary-Learning-with-Adaptive-Dictionary-Size-and-Sparsity-Level-Choice" class="headerlink" title="Quantitative MR Image Reconstruction using Parameter-Specific Dictionary Learning with Adaptive Dictionary-Size and Sparsity-Level Choice"></a>Quantitative MR Image Reconstruction using Parameter-Specific Dictionary Learning with Adaptive Dictionary-Size and Sparsity-Level Choice</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03460">http://arxiv.org/abs/2308.03460</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andreas Kofler, Kirsten Miriam Kerkering, Laura Göschel, Ariane Fillmer, Cristoph Kolbitsch</li>
<li>for: 提出一种方法用于量化磁共振成像（QMRI）中的参数地图重建。</li>
<li>methods: 使用字典学习（DL）和稀疏编码（SC）算法自动计算参数地图的最佳字典大小和稀疏程度，并对每个参数地图进行自适应调整。</li>
<li>results: 相比MAP方法和其他基于稀疏性的方法（TV、Wl、Sh），提出的方法在PSNR和RMSE上表现更好，同时能够加速重建过程约七倍。Here’s the full text in Simplified Chinese:</li>
<li>for: 本文提出了一种方法，用于量化磁共振成像（QMRI）中的参数地图重建。</li>
<li>methods: 该方法使用字典学习（DL）和稀疏编码（SC）算法自动计算参数地图的最佳字典大小和稀疏程度，并对每个参数地图进行自适应调整。</li>
<li>results: 相比MAP方法和其他基于稀疏性的方法（TV、Wl、Sh），提出的方法在PSNR和RMSE上表现更好，同时能够加速重建过程约七倍。I hope that helps!<details>
<summary>Abstract</summary>
Objective: We propose a method for the reconstruction of parameter-maps in Quantitative Magnetic Resonance Imaging (QMRI).   Methods: Because different quantitative parameter-maps differ from each other in terms of local features, we propose a method where the employed dictionary learning (DL) and sparse coding (SC) algorithms automatically estimate the optimal dictionary-size and sparsity level separately for each parameter-map. We evaluated the method on a $T_1$-mapping QMRI problem in the brain using the BrainWeb data as well as in-vivo brain images acquired on an ultra-high field 7T scanner. We compared it to a model-based acceleration for parameter mapping (MAP) approach, other sparsity-based methods using total variation (TV), Wavelets (Wl) and Shearlets (Sh), and to a method which uses DL and SC to reconstruct qualitative images, followed by a non-linear (DL+Fit).   Results: Our algorithm surpasses MAP, TV, Wl and Sh in terms of RMSE and PSNR. It yields better or comparable results to DL+Fit by additionally significantly accelerating the reconstruction by a factor of approximately seven.   Conclusion: The proposed method outperforms the reported methods of comparison and yields accurate $T_1$-maps. Although presented for $T_1$-mapping in the brain, our method's structure is general and thus most probably also applicable for the the reconstruction of other quantitative parameters in other organs.   Significance: From a clinical perspective, the obtained $T_1$-maps could be utilized to differentiate between healthy subjects and patients with Alzheimer's disease. From a technical perspective, the proposed unsupervised method could be employed to obtain ground-truth data for the development of data-driven methods based on supervised learning.+
</details>
<details>
<summary>摘要</summary>
Methods: 因为不同的量化参数地图之间的本地特征不同，我们提议使用自适应词库学习（DL）和稀疏编码（SC）算法自动计算参数地图的优化词库大小和稀疏性水平。我们对BrainWeb数据集和7T磁共振成像机上实验取得的生物体内部图像进行评估。我们与参数映射（MAP）方法、总变量（TV）、波лет（Wl）和扭变（Sh）等其他稀疏方法进行比较，以及使用DL和SC重建质量图像，然后使用非线性（DL+Fit）方法。Results: 我们的算法在RMSE和PSNR方面都高于MAP、TV、Wl和Sh，并且与DL+Fit相比，同时提供了约七倍的加速。Conclusion: 我们提出的方法在$T_1$-mapping问题上表现出色，并且可以在脑部其他参数的重建中使用。尽管我们只对脑部的$T_1$-mapping进行了评估，但我们的方法结构是通用的，因此可能适用于其他器官的量化参数重建。Significance: 从临床角度来看，获得的$T_1$-地图可能用于识别健康人群和患有阿尔茨海默病的患者。从技术角度来看，我们提出的无监督方法可以用于获得数据驱动学习方法的基准数据。
</details></li>
</ul>
<hr>
<h2 id="Lighting-Every-Darkness-in-Two-Pairs-A-Calibration-Free-Pipeline-for-RAW-Denoising"><a href="#Lighting-Every-Darkness-in-Two-Pairs-A-Calibration-Free-Pipeline-for-RAW-Denoising" class="headerlink" title="Lighting Every Darkness in Two Pairs: A Calibration-Free Pipeline for RAW Denoising"></a>Lighting Every Darkness in Two Pairs: A Calibration-Free Pipeline for RAW Denoising</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03448">http://arxiv.org/abs/2308.03448</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/srameo/led">https://github.com/srameo/led</a></li>
<li>paper_authors: Xin Jin, Jia-Wen Xiao, Ling-Hao Han, Chunle Guo, Ruixun Zhang, Xialei Liu, Chongyi Li</li>
<li>for: 提高低光环境下 RAW 图像减噪性能，不需要干扰式减噪方法。</li>
<li>methods: 基于减噪策略，对目标摄像机进行几次匹配数据和微调，以适应不同摄像机和数字增量。同时，通过特殊的结构修改，解决synthetic noise和实际噪声之间的域距离问题。</li>
<li>results: 与其他干扰式减噪方法相比，本方法在6对增量数据和0.5%迭代后，在低光环境下达到了更高的性能。<details>
<summary>Abstract</summary>
Calibration-based methods have dominated RAW image denoising under extremely low-light environments. However, these methods suffer from several main deficiencies: 1) the calibration procedure is laborious and time-consuming, 2) denoisers for different cameras are difficult to transfer, and 3) the discrepancy between synthetic noise and real noise is enlarged by high digital gain. To overcome the above shortcomings, we propose a calibration-free pipeline for Lighting Every Drakness (LED), regardless of the digital gain or camera sensor. Instead of calibrating the noise parameters and training repeatedly, our method could adapt to a target camera only with few-shot paired data and fine-tuning. In addition, well-designed structural modification during both stages alleviates the domain gap between synthetic and real noise without any extra computational cost. With 2 pairs for each additional digital gain (in total 6 pairs) and 0.5% iterations, our method achieves superior performance over other calibration-based methods. Our code is available at https://github.com/Srameo/LED .
</details>
<details>
<summary>摘要</summary>
准确性基于方法在极低照度环境中对RAW图像净化得到了广泛应用。然而，这些方法受到以下主要缺点的影响：1）准备和实施准化过程是时间consuming和劳动密集的；2）描述器对不同的相机难以传递；3）高度数字增强会使 synthetic 噪声与实际噪声之间的差距变大。为了缓解以上缺点，我们提出了不需要准化的激光照明每个点（LED）管道，无论相机感知器或数字增强。而不是在每次训练中重复准化噪声参数，我们的方法可以适应目标相机只需要几个数据对和微调。此外，我们在两个阶段中设计了结构修改，以避免噪声生成器和实际噪声之间的领域差距，无需额外计算成本。使用2对每个附加的数字增强（总共6对）和0.5%迭代，我们的方法可以在其他准化基于方法上达到更高的性能。我们的代码可以在https://github.com/Srameo/LED 上找到。
</details></li>
</ul>
<hr>
<h2 id="Energy-Guided-Diffusion-Model-for-CBCT-to-CT-Synthesis"><a href="#Energy-Guided-Diffusion-Model-for-CBCT-to-CT-Synthesis" class="headerlink" title="Energy-Guided Diffusion Model for CBCT-to-CT Synthesis"></a>Energy-Guided Diffusion Model for CBCT-to-CT Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03354">http://arxiv.org/abs/2308.03354</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linjie Fu, Xia Li, Xiuding Cai, Dong Miao, Yu Yao, Yali Shen</li>
<li>for: 提高CBCT图像质量和Hounsfield单位精度，以便更好地用于放射治疗</li>
<li>methods: 基于能量导向扩散模型(EGDiff)，从CBCT图像生成synthetic CT(sCT)</li>
<li>results: 对胸腔肿瘤数据集进行实验，得到了具有较高精度和视觉质量的sCT图像，并且超过了现有无监督合成方法的性能。<details>
<summary>Abstract</summary>
Cone Beam CT (CBCT) plays a crucial role in Adaptive Radiation Therapy (ART) by accurately providing radiation treatment when organ anatomy changes occur. However, CBCT images suffer from scatter noise and artifacts, making relying solely on CBCT for precise dose calculation and accurate tissue localization challenging. Therefore, there is a need to improve CBCT image quality and Hounsfield Unit (HU) accuracy while preserving anatomical structures. To enhance the role and application value of CBCT in ART, we propose an energy-guided diffusion model (EGDiff) and conduct experiments on a chest tumor dataset to generate synthetic CT (sCT) from CBCT. The experimental results demonstrate impressive performance with an average absolute error of 26.87$\pm$6.14 HU, a structural similarity index measurement of 0.850$\pm$0.03, a peak signal-to-noise ratio of the sCT of 19.83$\pm$1.39 dB, and a normalized cross-correlation of the sCT of 0.874$\pm$0.04. These results indicate that our method outperforms state-of-the-art unsupervised synthesis methods in accuracy and visual quality, producing superior sCT images.
</details>
<details>
<summary>摘要</summary>
cone beam CT (CBCT) 在适应性辐射疗法 (ART) 中发挥关键作用，准确地提供辐射治疗当器官 анатомиче变化时。然而， CBCT 图像受到扰干噪和artefacts的影响，使凭借 CBCT  alone 准确地计算辐射剂量和精确地地标定组织结构困难。因此，需要改进 CBCT 图像质量和温迪尔单位 (HU) 准确性，同时保持器官结构。为了提高 CBCT 在 ART 中的应用价值，我们提议一种能量导向扩散模型 (EGDiff) ，并在胸腔肿瘤数据集上进行实验，将 CBCT 转换成Synthetic CT (sCT)。实验结果表明，我们的方法可以准确地生成高质量的 sCT 图像，其中平均绝对错误为 26.87$\pm$6.14 HU，结构相似度指数为 0.850$\pm$0.03，峰值响应信号强度为 19.83$\pm$1.39 dB，和正常化交叉相似度为 0.874$\pm$0.04。这些结果表明，我们的方法在精度和视觉质量方面都有出色的表现，生成的 sCT 图像较为优秀。
</details></li>
</ul>
<hr>
<h2 id="A-Hybrid-CNN-Transformer-Architecture-with-Frequency-Domain-Contrastive-Learning-for-Image-Deraining"><a href="#A-Hybrid-CNN-Transformer-Architecture-with-Frequency-Domain-Contrastive-Learning-for-Image-Deraining" class="headerlink" title="A Hybrid CNN-Transformer Architecture with Frequency Domain Contrastive Learning for Image Deraining"></a>A Hybrid CNN-Transformer Architecture with Frequency Domain Contrastive Learning for Image Deraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03340">http://arxiv.org/abs/2308.03340</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng Wang, Wei Li</li>
<li>for: 图像恢复（image restoration），即修复受损图像中的雨线 Streaks 的问题。</li>
<li>methods: 该论文使用了 Deep Learning 技术，特别是 Convolutional Neural Networks (CNNs) 和 Generative Adversarial Networks (GANs)，以实现图像恢复。</li>
<li>results: 该论文实现了高效的图像恢复，可以减少或完全消除雨线 Streaks，并保持图像的原始细节和颜色彩虹。<details>
<summary>Abstract</summary>
Image deraining is a challenging task that involves restoring degraded images affected by rain streaks.
</details>
<details>
<summary>摘要</summary>
图像推Clearing是一项具有挑战性的任务，旨在修复受到雨批 streaks 的图像。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/07/eess.IV_2023_08_07/" data-id="clp9qz8df018gok88044ahiqe" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/71/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/70/">70</a><a class="page-number" href="/page/71/">71</a><span class="page-number current">72</span><a class="page-number" href="/page/73/">73</a><a class="page-number" href="/page/74/">74</a><span class="space">&hellip;</span><a class="page-number" href="/page/97/">97</a><a class="extend next" rel="next" href="/page/73/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">66</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">81</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">140</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
